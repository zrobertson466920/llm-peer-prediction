[
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDOUBLE DYNAMIC SPARSE TRAINING FOR GANS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe past decade has witnessed a drastic increase in modern deep neural networks (DNNs) size, especially for generative adversarial networks (GANs). Since GANs usually suffer from high computational complexity, researchers have shown an increased interest in applying pruning methods to reduce the training and inference costs of GANs. Among different pruning methods invented for supervised learning, dynamic sparse training (DST) has gained increasing attention recently as it enjoys excellent training efficiency with comparable performance to post-hoc pruning. Hence, applying DST on GANs, where we train a sparse GAN with a fixed parameter count throughout training, seems to be a good candidate for reducing GAN training costs. However, a few challenges, including the degrading training instability, emerge due to the adversarial nature of GANs. Hence, we introduce a quantity called balance ratio (BR) to quantify the balance of the generator and the discriminator. We conduct a series of experiments to show the importance of BR in understanding sparse GAN training. Building upon single dynamic sparse training (SDST), where only the generator is adjusted during training, we propose double dynamic sparse training (DDST) to control the BR during GAN training. Empirically, DDST automatically determines the density of the discriminator and greatly boosts the performance of sparse GANs on multiple datasets.\n\n1\n\nINTRODUCTION\n\nIn the past decade, the training and inference costs of modern deep neural networks (DNNs) are gradually becoming prohibitive (He et al., 2016; Dosovitskiy et al., 2020; Liu et al., 2021d), especially for large language models (Brown et al., 2020). Among all these large models, generative adversarial networks (GANs) (Goodfellow et al., 2020) have been widely investigated for years and achieved remarkable results. However, similar to other giant models, GANs are notably computationally intensive. For example, BigGAN (Brock et al., 2018) trained on 8 NVIDIA V100 GPUs with full precision will take 15 days. Consequently, to train GANs in broader resource-constrained scenarios, this computational bottleneck of training needs to be resolved urgently.\n\nNeural network pruning has recently emerged as a powerful tool to reduce training and inference costs of DNNs for supervised learning. There are mainly three genres of pruning methods, namely pruning-at-initialization, pruning-during-training, and post-hoc pruning methods. Post-hoc pruning (Janowsky, 1989; LeCun et al., 1989; Han et al., 2015) can date back to the 1980s, which was first introduced for reducing inference time and memory requirements; hence does not align with our purpose of efficient training. Later, pruning-at-initialization (Lee et al., 2018; Wang et al., 2020a; Tanaka et al., 2020) and pruning-during-training methods (Louizos et al., 2017; Wen et al., 2016) were introduced to prune the networks before training and throughout the training, respectively. Most early pruning-during-training algorithms (Savarese et al., 2020) gradually decrease the density of the neural networks and hence do not bring much training efficiency compared to posthoc pruning. However, recent advances in dynamic sparse training (DST) (Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018) for the first time show that pruning-during-training methods can have comparable training FLOPs as pruning-at-initialization methods while having competing performance with respect to post-hoc pruning. Therefore, applying DST on GANs seems to be a promising choice.\n\nAlthough DST has attained remarkable achievements in supervised learning, the application of DST on GANs is less explored due to newly emerged challenges. The main difficulty stems from the fact that the training procedure of GANs is notoriously brittle. To ensure successful training, we\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nusually need carefully chosen architectures and finely-tuned hyper-parameters. One possible cause is the difficulty of balancing the generator and the discriminator throughout training (Bai et al., 2018; Arora et al., 2017). Specifically, an overly-strong discriminator will lead to overfitting, while a weak discriminator will result in mode collapse. As a consequence, the requirement of balanced training brings even more challenges to sparse GAN training. On the one hand, we find that performance degradation caused by the unbalance of GANs is even more severe when sparsity is introduced. On the other hand, for directly applying DST to the generator (or both) like the pioneering work STUGAN (Liu et al., 2022), it is unclear how to determine a reasonable density of the discriminator. To this end, we propose a metric called balance ratio (BR), which measures the degree of balance of the two components, to study sparse GAN training.\n\nWe find that BR is useful in (1) understanding the interaction between the discriminator and the generator, (2) identifying the cause of training failure, and (3) helping stabilize sparse GAN training as an indicator. To our best knowledge, this is the first study to investigate the balance of sparse GANs and may even provide new insights into dense GAN training.\n\nUsing BR as an indicator, we further propose double dynamic sparse training (DDST) to adjust the density and the connections of the discriminator automatically during training.\n\nOur contributions are summarized below:\n\n• We introduce a quantity named balance ratio to quantify the degree of balance in GAN\n\ntraining, which also helps understand the cause of some training failure cases.\n\n• We first consider single dynamic sparse training (SDST), which is a generalization of STUGAN (Liu et al., 2022): applying DST to only the generator with varying discriminator density ratios. We show that SDST does not necessarily outperform the static sparse training baselines.\n\n• We provide two strategies to determine the discriminator density for SDST, and we find\n\nthat using a relatively larger density usually generates stable and better performance.\n\n• Using the balance ratio as an indicator, we propose double dynamic sparse training (DDST), which makes the discriminator dynamic both in density level and parameter level. Empirically, DDST outperforms baselines with reasonable computational cost on several datasets.\n\n2 RELATED WORKS\n\n2.1 NEURAL NETWORK PRUNING\n\nBased on the smallest granularity of pruned units, neural network pruning can be categorized into structured (Liu et al., 2017; 2018; Huang & Wang, 2018; Luo et al., 2017) and unstructured pruning (Frankle & Carbin, 2018; Han et al., 2015). In this work, we mainly focus on unstructured pruning where individual weight is the finest resolution.\n\nPost-hoc pruning. Post-hoc pruning prunes weights of a fully-trained neural network, and they usually have high computation cost due to the multiple rounds of train-prune-retrain procedure (Han et al., 2015; Renda et al., 2020). Some use specific criteria (Han et al., 2015; LeCun et al., 1989; Hassibi et al., 1993; Molchanov et al., 2019; Dai et al., 2018; Guo et al., 2016; Dong et al., 2017; Yu et al., 2018) to remove weights, while others perform extra optimization iterations (Verma & Pesquet, 2021). Post-hoc pruning was initially proposed to reduce the inference time, while later work on lottery ticket works (Frankle & Carbin, 2018; Renda et al., 2020) aimed to mine trainable sub-networks.\n\nPruning-at-initialization methods. SNIP (Lee et al., 2018) is one of the pioneering works which aim to find trainable sub-networks without any training. Some following works (Wang et al., 2020a; Tanaka et al., 2020; de Jorge et al., 2020; Alizadeh et al., 2022) aim to propose different metrics to prune networks at initialization. Among them, Synflow (Tanaka et al., 2020), SPP (Lee et al., 2019), and FORCE (de Jorge et al., 2020) try to address the problem of layer collapse during pruning. Neural tangent transfer (Liu & Zenke, 2020) learns a sub-network by aligning the empirical neural tangent kernel and network output to the dense counterpart.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nPruning-during-training methods. Another genre of pruning algorithms gradually prunes dense DNNs throughout training. To mitigate performance drop after pruning, early works add explicit l0 (Louizos et al., 2017) or l1 (Wen et al., 2016) regularization terms to encourage sparse solution. Later works learn the subnetworks structures through projected gradient descent (Zhou et al., 2021) or trainable masks (Kang & Han, 2020; Kusupati et al., 2020; Liu et al., 2020; Savarese et al., 2020; Srinivas et al., 2017; Xiao et al., 2019). However, these pruning-during-training methods often do not enjoy memory sparsity during training. As a remedy, DST methods (Bellec et al., 2017; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018; Mostafa & Wang, 2019; Graesser et al., 2022) were introduced to train the neural networks under a given parameter budget while mask change is allowed during training.\n\n2.2 GENERATIVE ADVERSARIAL NETWORKS\n\nGenerative adversarial networks (GANs). GANs (Goodfellow et al., 2020) have drawn considerable attention and have been widely investigated for years. Various architectures have been proposed to enhance the capability of GANs. Deep convolutional GANs (Radford et al., 2015) replace fullyconnected layers in the generator and the discriminator. After that, follow-up works (Brock et al., 2018; Gulrajani et al., 2017; Karras et al., 2017; Zhang et al., 2019) employed more advanced methods to improve the fidelity of generated samples. Due to the difficulty of finding Nash Equilibrium, training of GAN is highly unstable. Therefore, several novel loss functions (Mao et al., 2017; Arjovsky et al., 2017; Salimans et al., 2016; Gulrajani et al., 2017; Sun et al., 2020), normalization and regularization methods (Miyato et al., 2018; Wu et al., 2021; Terj ́ek, 2019) were proposed to stabilize the adversarial training. Besides the efforts devoted to the training of GAN, image-to-image translation is also extensively explored. Specifically, this direction includes semantic image synthesis (Zhu et al., 2017b), style transfer (Karras et al., 2020b; Choi et al., 2018; Zhu et al., 2017a), super resolution (Ledig et al., 2017; Wang et al., 2018) etc.\n\nGAN compression and pruning. Like other deep neural networks, the training and inference process of GANs requires massive resource consumption and memory. One of the promising ways is based on neural architecture search and distillation algorithm (Li et al., 2020; Fu et al., 2020; Hou et al., 2021). Another part of the work applied prune-based methods for GANs’ generator compression (Shu et al., 2019; Jin et al., 2021; Yu & Pool, 2020). Yet, they only focus on the pruning of generators, thus potentially posing a negative influence on Nash Equilibrium between generators and discriminators. Later, works by (Wang et al., 2020b) presented a unified framework by combing the methods mentioned above. Follow-up work by Li et al. (2021) compresses both components of GANs by letting the student GANs also learn the losses. Another line of work (Kalibhat et al., 2021; Chen et al., 2021) tries to test the existence of lottery tickets in GAN. However, most mentioned methods are not prepared for training efficiency and require over-parameterized GAN models in advance. Directly training sparse GANs has been less explored so far. To the best of our knowledge, STU-GAN Liu et al. (2022) is the only work that tries to apply DST to GANs.\n\n3 BALANCE RATIO: QUANTIFYING THE BALANCE OF SPARSE GANS\n\n3.1 PRELIMINARY AND SETUPS\n\n; θG) Generative adversarial networks (GANs) have two fundamental components, a generator G( ·\n; θD). Specifically, the generator maps a sampled noise z from a multivariate and a discriminator D( ·\nnormal distribution p(z) into a fake image to cheat the discriminator, whereas the discriminator distinguishes the generator’s output and the real images xr from the distribution q(x). Formally, the optimization objective of the two-player game defined in JS-GAN (Goodfellow et al., 2020) is defined as follows:\n\n(θD, θG) = Exr∼q(x) [log(D(xr; θD))] + Ez∼p(z) [log(1\n\nL\n\nD(G(z; θG)))] .\n\n(1)\n\n−\n\nTo be more specific, different loss can be used, including Wasserstein loss (Gulrajani et al., 2017) and hinge loss (Miyato et al., 2018). In this work, we use hinge loss for all GANs.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nGAN sparse training. In this work, we are interested in sparse training for GANs. Specifically, the objective of sparse GAN training can be formulated as:\n\ns.t. mD\n\n0, 1\n\n}\n\n∈ {\n\nmin θG pD , mG\n\nmax θD L 0, 1\n\n∈ {\n\n(θD\n\nmD, θG\n\nmG)\n\n⊙\n\npG ,\n\n}\n\nmD\n\n∥\n\n∥\n\n⊙ 0/pD\n\ndD,\n\n≤\n\nmG\n\n∥\n\n0/pG\n\n∥\n\n≤\n\ndG,\n\n(2)\n\n⊙\n\nis the Hadamard product; θD, mD, pD, dD are the sparse solution, mask, number of where parameters, and target density for the discriminator, respectively. The corresponding variables for the generator are denoted with subscript G. For pruning-at-initialization methods, masks m are determined before training whereas m are dynamically adjusted for dynamic sparse training (DST) methods.\n\n3.2 BALANCE OF GAN DURING TRAINING\n\nAs discussed in Section 1, it is essential to maintain the balance of generator and discriminator during GAN training. As pointed out by Bai et al. (2018) and Arora et al. (2017), discriminators that are too strong lead to over-fitting, whereas weak discriminators are unable to detect mode collapse. When it comes to sparse GAN training, the consequences caused by the unbalance can be further amplified. Specifically, different from dense GAN training, densities of generators and discriminators can be varied significantly, leading to a more unbalanced worst-case scenario.\n\nTo support our claim, we conduct experiments with SNGAN (Miyato et al., 2018) on the CIFAR-10 dataset. Following Liu et al. (2022), we start with static sparse training where densities of generators and discriminators are chosen from . Layer-wise sparsity ratio and masks mG, mD are determined using Erd ̋os-R ́enyi-Kernel (ERK) graph topology (Evci et al., 2020) and are fixed throughout the training. More experiment details can be found at Appendix A.\n\n10%, 20%, 30%, 50%, 100%\n\n}\n\n{\n\nExperiment results. Results are reported in Figure 2. First three plots in Figure 2 show the results when varying density of discriminator dD for weak generators (dG ). We observe FID first decreases then increases. Specifically, neither overly-weak discriminators nor overly-strong discriminators can provide satisfactory performance. Similarly, for stronger generators (dG ), only the dense discriminator with dG = 100% is not too weak to have satisfactory FID result. Hence, to ensure a balanced training of GAN, it is crucial to find the suitable sparsity ratio for the discriminator.\n\n10%, 20%, 30%\n\n50%, 100%\n\n∈ {\n\n∈ {\n\n}\n\n}\n\n3.3 BALANCE RATIO\n\nThe observation in Section 3.2 raises a fundamental question: is there a way to quantify the degree of balance between the generator and the discriminator? To answer the question, we introduce balance ratio (BR), which is, to the best of our knowledge, the first quantity that measures the balance of sparse generators and discriminators.\n\nFigure 1: Illustration of balance ratio.\n\nAt each training iteration, we draw random noise z from a multivariate normal distribution and real images xr from the training set. We denote the discriminator after gradient descent up- ; θD). We denote generator before and after gradient descent training as Gpre( ; θG) and date as D( ·\n· ; θ′ Gpost( ·\n\nG), respectively. Then the balance ratio is defined as:\n\nBR =\n\nD(Gpost(z)) D(xr)\n\nD(Gpre(z))\n\n− D(Gpre(z))\n\nα β\n\n−\n\n=\n\n.\n\n(3)\n\nFigure 1 also provides an illustration of BR. Specifically, BR measures how much improvement the generator can achieve in the scale measured by the discriminator for a specific random noise z. When BR is small (e.g., BR< 30%), it means that the updated generator is too weak to trick the discriminator as the generated images are still considered fake. Similarly, for the case where BR is large (e.g., BR> 80%), the discriminator is considered too weak hence it will not provide useful information to the generator.\n\n4\n\n−1.00−0.75−0.50−0.250.000.250.500.751.00X−1.00−0.75−0.50−0.250.000.250.500.751.00D(X)αβ(xr,D(xr)))(Gpre(z),D(Gpre(z)))(Gpost(z),D(Gpost(z)))Balanceratio=αβUnder review as a conference paper at ICLR 2023\n\nFigure 2: FID ( different sparsity ratio combinations. The shaded areas denote the standard deviation.\n\n) of static sparsely trained SNGAN with and without DA on CIFAR-10 with\n\n↓\n\nFigure 3: Balance ratio of static sparsely trained SNGAN on CIFAR-10 with different sparsity ratio combinations.\n\nWe now visualize the BR evolution throughout the training for the experiments in Section 3.2. The results are shown in Figure 3.\n\nThe effectiveness of BR. We first check if BR can reflect the density increase (hence representation power increase) of the discriminator. In Figure 3 we can see that for larger discriminator density dD, the BR is much lower throughout the training. Furthermore, the best density as indicated by Figure 2 has overall BR in the range [0.3, 0.7].\n\nOverly weak discriminators lead to training failure. For the cases where the discriminators are too weak compared to the generators, e.g., all cases where dD = 10%, we are able to observe the strongly oscillatory behavior of BR. More precisely, BR starts to oscillate after it reaches a value that is higher than 1.0. During the experiments, we also empirically observe that the FID gradually increases after such a turning point. As also shown in Figure 2, FID of overly-strong discriminators (e.g., dD = 100%) are lower than overly-weak discriminators (e.g., dD = 10%). The such phenomenon seems to imply that performance degradation caused by overly-strong discriminators is better than failure caused by overly-weak discriminators.\n\n3.4 DYNAMIC DENSITY ADJUSTMENT OF THE DISCRIMINATORS\n\nWe have shown in Section 3.3 that BR is able to capture the degree of balance of the generators and discriminators. Hence, it is natural to leverage BR to dynamically adjust the density of discriminators during sparse GAN training. Specifically, we initialize the initial density of the discriminator dinit D = dG. After a specific training iteration interval ∆TD, we will adjust the density of the discriminator based on the time-averaged BR over last a few iterations with a pre-defined density increment ∆d. With a pre-defined BR bounds [B−, B+], we decrease dD by ∆d when BR is smaller than B−, and vise versa. Notice that the DA algorithm is in spirit very similar to StyleGAN2-ADA (Karras et al., 2020a) which adjusts augmentation probability with ADA. Out of simplicity, we increase the density by growing the connections with the largest gradient magnitude (Evci et al., 2020). Global magnitude pruning is used to drop connections so as to decrease the density. The algorithm is shown in Appendix C Algorithm 1.\n\nWe test our proposed methods dynamic density adjust (DA) with two target BR intervals, namely DA-strong ([B−, B+] = [0.3, 0.4]), DA-mild ([B−, B+] = [0.45, 0.55]). DA-strong tends to find a relatively stronger discriminator, which results in a lower BR throughout the training, whereas DA-mild tends to make the discriminator and the generator relatively balanced, i.e., BR\n\n0.5.\n\n≈\n\n5\n\n10%20%30%50%100%DensityofdiscriminatordD182022242628FIDDensityofgeneratordG=10%10%20%30%50%100%DensityofdiscriminatordD15.017.520.022.525.027.530.032.5FIDDensityofgeneratordG=20%10%20%30%50%100%DensityofdiscriminatordD152025303540FIDDensityofgeneratordG=30%10%20%30%50%100%DensityofdiscriminatordD1020304050607080FIDDensityofgeneratordG=50%10%20%30%50%100%DensityofdiscriminatordD102030405060708090FIDDensityofgeneratordG=100%StaticDA-strongDA-mild020000400006000080000100000Trainingiteration0.00.20.40.60.81.01.21.41.6BalanceratioDensityofgeneratordG=10%020000400006000080000100000Trainingiteration0.00.20.40.60.81.01.21.41.6BalanceratioDensityofgeneratordG=20%020000400006000080000100000Trainingiteration0.00.20.40.60.81.01.21.41.6BalanceratioDensityofgeneratordG=30%020000400006000080000100000Trainingiteration0.00.20.40.60.81.01.21.41.6BalanceratioDensityofgeneratordG=50%020000400006000080000100000Trainingiteration0.00.20.40.60.81.01.21.41.6BalanceratioDensityofgeneratordG=100%dD=10%dD=20%dD=30%dD=50%dD=100%Under review as a conference paper at ICLR 2023\n\nFigure 4: FID ( with different sparsity ratio combinations. The shaded areas denote the standard deviation.\n\n) comparison of SDST against static sparse training for SNGAN on CIFAR-10\n\n↓\n\n∈ {\n\n30%, 50%, 100%\n\nExperiment results. Results are shown in Figure 2 with dashed lines. For stronger generators ), both DA-strong and DA-mild are able to identify reasonable dis- (dG }\n), DA-mild shows a more criminator densities. While for weak generators (dG stable performance. The experiments show the significant benefits brought by BR. Furthermore, they again support our claims that neither overly-strong nor weak discriminators can lead to balanced and successful GAN training.\n\n10%, 20%\n\n∈ {\n\n}\n\n4\n\nIS ONLY ADJUSTING THE GENERATOR ENOUGH FOR SPARSE GANS?\n\nIn this section, we are going to test DST on GANs. We first test SDST, a direct application of DST method on GAN where only the generator dynamically adjusts masks during the training. We do not consider naively applying DST on both generators and discriminators, as in STU-GAN (Liu et al., 2022), it is empirically shown that adjusting both components simultaneously generates worse performance with more severe training instability. We name such method single dynamic sparse training (SDST) as only one component of the GAN, i.e., the generator, is dynamic. Hence, STUGAN is a special case for SDST, which grows connections based on gradients.1\n\nWe follow the same setting considered in Section 3.2 where the densities of the generators dG and . Detailed DST procedure and discriminators dD are chosen from corresponding hyper-parameters can be found in Appendix B.\n\n10%, 20%, 30%, 50%, 100%\n\n}\n\n{\n\nExperiment results. We show the experiment results in Figure 4. The first observation is that the performance of RigL and SET does not vary much in general. The second observation is that SDST is better than static sparse training when the discriminator is strong enough. More , SDST method is worse than static sparse training when the specifically, for dG density of the discriminator is weak (dD = 10%). On the contrary, when the discriminator is strong , we see a great performance boost brought by SDST. The enough, dD reason is that the in-time over-parameterization induced by DST increases the representation power of the generator. Such a boost is beneficial only when the discriminator has matching or better representation power.\n\n20%, 30%, 50%, 100%\n\n10%, 20%\n\n∈ {\n\n∈ {\n\n}\n\n}\n\nDespite the superior performance of STU-GAN (or SDST in general) at higher density ratios, there exist some limitations for SDST, which are summarized as follows: ➊ When using SDST, dD is manually chosen before training. However, it is unclear what is a good choice. In real-world scenarios, it is not practical to search for the optimal dD for each dG. ➋ The issue of GAN unbalance is unresolved during training since the density of the discriminator is fixed. As shown in Figure 4, the best performance is not always obtained with the maximum dD = 100%. If we are using an overly-strong discriminator, we are wasting extra computational cost for a worse performance.\n\nHence, STU-GAN (or SDST in general), which directly applies DST to the generator, may only be useful when the corresponding discriminator is strong enough. In this sense, to deal with more complicated scenarios, obtaining balanced training in an automatic way is essential in GAN dynamic sparse training.\n\n1Notice that STU-GAN is almost identical to SDST(RigL) with EMA tailored for DST.\n\n6\n\n10%20%30%50%100%DensityofdiscriminatordD15.017.520.022.525.027.530.0FIDDensityofgeneratordG=10%10%20%30%50%100%DensityofdiscriminatordD1520253035FIDDensityofgeneratordG=20%10%20%30%50%100%DensityofdiscriminatordD1020304050FIDDensityofgeneratordG=30%10%20%30%50%100%DensityofdiscriminatordD102030405060708090FIDDensityofgeneratordG=50%10%20%30%50%100%DensityofdiscriminatordD102030405060708090FIDDensityofgeneratordG=100%StaticSDST(RigL)SDST(SET)Under review as a conference paper at ICLR 2023\n\n) of different sparse training methods on CIFAR-10 and STL-10 datasets with no Table 1: FID ( constraint on the density of the discriminator. Best results are in bold; second-best results are underlined.\n\n↓\n\nCIFAR-10\n\nSTL-10\n\nGenerator density\n\n10% 20 % 30 % 50 % 10% 20 % 30 % 50 %\n\n(Dense Baseline)\n\nStatic-Balance Static-Strong\n\nDST-balance (SET) DST-balance (RigL)\n\nSDST-Balance (SET) SDST-Strong (SET) SDST-Balance (RigL) SDST-Strong (RigL)\n\nR-DDST (SET) R-DDST (RigL)\n\n10.74\n\n29.71\n\n26.73 26.60\n\n32.02 24.56\n\n27.80 17.04 30.38 16.95\n\n13.58 13.77\n\n18.04 19.47\n\n18.54 15.53\n\n18.13 14.58 17.89 14.26\n\n12.54 12.33\n\n14.38 14.60\n\n14.74 13.62\n\n14.15 12.29 14.95 12.36\n\n11.71 11.46\n\n12.22 11.28\n\n13.23 12.51\n\n12.32 11.47 12.09 11.47\n\n10.97 11.18\n\n50.08 52.03\n\n49.91 66.90\n\n63.57 78.34 46.17 48.04\n\n63.59 42.72\n\n44.19 44.04\n\n33.71 50.34\n\n49.05 54.31 38.12 34.24\n\n56.15 33.12\n\n43.96 42.53\n\n32.92 44.57\n\n43.74 41.77 31.88 32.67\n\n45.48 32.44\n\n37.21 38.33\n\n31.75 32.63\n\n31.29 32.32 31.30 30.40\n\n31.71 30.88\n\nTable 2: FID ( on CIFAR-10 dataset. Best results are in bold; second-best results are underlined.\n\n) and normalized training FLOPs of different sparse training methods with BigGAN\n\n↓\n\nFID (\n\n)\n\nNormalized training FLOPs\n\n↓\n\nGenerator density\n\n(Dense Baseline)\n\nStatic-Balance Static-Strong\n\nSDST-Balance (RigL) SDST-Strong (RigL) R-DDST (RigL)\n\n10% 20 % 30 % 50 %\n\n10%\n\n20 %\n\n30 %\n\n50 %\n\n8.43\n\n6.80\n\n1017 (100%)\n\n×\n\n17.46 22.96 11.98 10.79 9.58\n\n13.13 13.54 9.58 9.30 8.77\n\n10.90 11.54 8.96 8.82 8.11\n\n8.54 9.02 7.92 8.30 8.17\n\n9.78% 19.04% 28.68% 49.12% 83.90% 84.93% 86.32% 90.15% 9.91% 19.41% 28.90% 48.38% 84.04% 85.22% 86.54% 89.56% 9.77% 24.85% 40.00% 77.13%\n\n5 DOUBLE DYNAMIC SPARSE TRAINING FOR GANS\n\nSTU-GAN (or SDST in general) considered in the last section cannot generate stable and satisfying performance. This implies that we should utilize the discriminator in a better way rather than just directly applying DST to the discriminator. Consequently, DA (Section 3.4), which adjusts the discriminator density to stabilize GAN training, is a favorable candidate to address the issue. We name the proposed method double dynamic sparse training (DDST), which adjusts the density of the discriminator during training with BR as the indicator while the generator performs DST. We propose two DDST methods, namely R-DDST and S-DDST based on whether we give constraints on the maximum density of the discriminator. We present them in Section 5.1 and Section 5.2. We use the word double for the following two reasons: ➊ both the generators and the discriminators are dynamic (both R-DDST and S-DDST); ➋ the discriminator enjoys two levels of dynamic flexibility, namely density level and parameter level (S-DDST). Such a method has much more flexibility and generates more stable performance compared to SDST.\n\n5.1 RELAXED DOUBLE DYNAMIC SPARSE TRAINING\n\nWe first investigate the direct combination of SDST with DA. Specifically, the generator is adjusted using SDST as mentioned in Section 4 while the density of the generator is dynamically adjusted with DA as mentioned in Section 3.4. We call such a combination relaxed double dynamic sparse training (R-DDST) as it does not necessarily introduce sparsity to the discriminator, and the density of the discriminator can be as high as 100% (hence dense discriminator). For a fair comparison, baseline methods can use the discriminator with arbitrary sparsity ratio, i.e., dD [dmin, dmax] = [0%, 100%].\n\n∈\n\nComparison to STU-GAN (SDST). Compared to STU-GAN (or SDST in general) which predefines the discriminator density before training, the difference is that for R-DDST, the density of the discriminator is adjusted during the training process automatically through DA. Given the initial discriminator density dint D = dG, R-DDST automatically increases the discriminator density if a stronger discriminator is needed, and vice versa.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nDatasets, architectures, and target sparsity ratios. We first conduct experiments on SNGAN with ResNet architectures on CIFAR-10 (Krizhevsky et al., 2009) and STL-10 (Coates et al., 2011) datasets. Target density ratios of the generators dG are chosen from . Please see Appendix A for more experiment details.\n\n10%, 20%, 30%, 50%\n\n{\n\n}\n\nBaseline methods and R-DDST. We use static (Section 3.2) and SDST (Section 4) as our baselines. Since these baselines use pre-defined discriminator density ratios, we propose two strategies to define the discriminator density ratios based on the results from Section 3.2 and Section 4: ➊ balance strategy, where we set the density of the discriminator dD the same as the density of the generator dG; ➋ strong strategy, where we set the density of the discriminator as large as possible, i.e., dD = dmax. In this section, we use dmax = 100% for the strong strategy. For SDST methods, we test both grow methods, i.e., SDST(SET) (Mocanu et al., 2018) which grows connections randomly and SDST(RigL) (Evci et al., 2020) which grows connections via gradient.\n\nSimilar to SDST, we again consider R-DDST(SET) and R-DDST(RigL) which differ based on how R-DDST grows connections. One thing to notice is that we use the same growth criterion for the generator and the discriminator out of simplicity. More training details can be found in Appendix B. FID results on the training set are shown in Table 1. More results of SNGAN on CIFAR-10 test set can be found in Appendix E. Training costs comparison can be found in Appendix G.\n\nThe strong strategy and the balance strategy. For almost all density ratios of SNGAN (CIFAR-10) experiments, using the strong strategy is always comparable to or better than the balance strategy. The difference between the two is almost negligible when applied to static methods. However, for SDST methods, using stronger discriminators always leads to a large performance gain.\n\nFor SNGAN on the STL-10 dataset, the advantage of the strong strategy over the balance strategy is no longer obvious. Precisely, for 3 out of 8 cases, using the strong strategy is better than using the balance strategy. The explanation is that the size difference between generators and discriminators is larger for STL-10. Hence, the degree of unbalance is more severe and leads to more detrimental effects.\n\nR-DDST identifies reasonable discriminator density. For the CIFAR-10 dataset, we find that R-DDST consistently performs better than the corresponding baselines with the same grow methods. This illustrates that R-DDST is flexible and able to find suitable discriminator density compared to the two baseline strategies, i.e., the strong and the balance strategy.\n\nFor the STL-10 dataset, R-DDST(RigL) performs consistently better than R-DDST(SET) and baselines, whereas R-DDST(SET) is not competitive any more. We postulate that under such a setting where the dataset scales up and the training is more difficult, gradient growth not only identifies important connections of the generator but also provides efficient representation power growth of the discriminator to balance the growth of the generator. Please also see Appendix D for the time evolution of BR and the discriminator density during training for R-DDST methods.\n\nLarger GAN model experiments. We have also conducted experiments with BigGAN (Brock et al., 2018) on CIFAR-10 datasets. Based on the SNGAN results, we compare all RigL variants with static baselines. FID and normalized training FLOPs with respect to dense training are shown in Table 2. The results show that R-DDST shows stable performance and outperforms other baselines most of the time. Moreover, compared to the second best method SDST-Strong, R-DDST not only shows lower FID but also requires much less training cost.\n\nMain takeaway. In this section, we compared R-DDST with sparse training baselines. We find that RigL and strong strategy are preferred compared to SET and balance strategy. SDST(RigL) with strong strategy generally generates better performance compared to other sparse training baselines. Finally, R-DDST(RigL) beats SDST(RigL) with much less computational cost and always ranked top two among all methods.\n\n5.2 STRICT DOUBLE DYNAMIC SPARSE TRAINING\n\nR-DDST introduced in the previous section does not necessarily introduce sparsity for the discriminator, which provides less memory/training resources saving for larger generator density ratios. Hence, we further present strict double dynamic sparse training (S-DDST) in this section which dmax < 100%. In this section, we assume that enforces the discriminator to be sparse, i.e., dD\n\n≤\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n) of different sparse training methods on CIFAR-10 and STL-10 datasets. The density Table 3: FID ( of the discriminator is constrained to be lower than 50%. Best results are in bold; second-best results are underlined.\n\n↓\n\nCIFAR-10\n\nSTL-10\n\nGenerator density\n\n10% 20 % 30 % 50 % 10% 20 % 30 % 50 %\n\n(Dense Baseline)\n\nStatic-balance Static-strong\n\nDST-balance (SET) DST-balance (RigL)\n\nSDST-balance (SET) SDST-strong (SET) SDST-balance (RigL) SDST-strong (RigL)\n\nS-DDST (SET) S-DDST (RigL)\n\n10.74\n\n29.71\n\n26.73 22.35\n\n32.02 24.56\n\n27.80 16.00 30.38 15.66\n\n14.22 14.13\n\n18.04 16.57\n\n18.54 15.53\n\n18.13 13.31 17.89 13.20\n\n13.30 12.87\n\n14.38 13.47\n\n14.74 13.62\n\n14.15 13.17 14.95 12.99\n\n12.39 12.15\n\n12.22 12.22\n\n13.23 12.51\n\n12.32 12.32 12.09 12.09\n\n11.97 12.17\n\n50.08 50.28\n\n49.91 66.90\n\n63.57 48.40 46.17 63.65\n\n51.72 44.28\n\n44.19 44.95\n\n33.71 50.34\n\n49.05 33.56 38.12 33.45\n\n35.74 32.84\n\n43.96 42.12\n\n32.92 44.57\n\n43.74 32.19 32.48 32.09\n\n42.36 32.00\n\n37.21 37.21\n\n31.75 32.63\n\n31.29 31.29 31.30 31.30\n\n31.68 30.28\n\n∈\n\n[dmin, dmax] = [0%, 50%]. Compared with we can use the discriminator with sparsity ratio dD R-DDST, the learning process will be more challenging with the introduced constraints on the maximal discriminator density. S-DDST consists of two phases and works as follows: ➊ Density exploration of the discriminator. During the first phase, S-DDST performs just like dmax < 100%. Concretely, S-DDST R-DDST, with the exception that we apply the constraint dD aims to find a suitable discriminator density d∗ ➋ Paramter exploration of the discriminator. During the second phase, both the generator and discriminator are adjusted using DST with fixed discriminator density d∗\n\nD with DA algorithm in the first half of training.\n\nD found in the first phase.\n\n≤\n\nBaseline methods and S-DDST. We use the same baselines and adopt the same general setup in Section 5.1. We divide the training iterations evenly for two phases. For a comprehensive comparison, we continue to report FID results from two growth methods, i.e., S-DDST(SET) and S-DDST(RigL) in Table 3. IS and other results can be found in Appendix E.\n\nS-DDST shows stable and superior performance. For the CIFAR-10 dataset, we notice that S-DDST stably surpasses its corresponding baselines regardless of grow methods and initial density of discriminators and generators. Even with a further constraint on the discriminator, DA is still able to improve GANs training and can explore more reasonable density than the strong and the balance strategy. For STL-10 dataset, S-DDST(RigL) again shows the most promising performance. Please also see Figure 7 in Appendix D for discriminator density and BR evolution during training.\n\nMain takeaway. In this section, we report the results from S-DDST(RigL) with its baselines. Generally, RigL still demonstrates encouraging results compared with SET in most experiments when extra sparsity is introduced. While the strong strategy shows favorable performance in the CIFAR-10 dataset, the gain is not salient when the size of the backbone increase and the training S-DDST(RigL) is able to have comparable dataset scales up to STL-10. Most importantly, performance in some cases when compared to R-DDST(RigL) and outperforms SDST(RigL) after we restrict the maximal density of discriminators.\n\n6 CONCLUSION\n\nIn this paper, we study DST for GANs. We find that simply applying DST methods to the generator is not sufficient to improve the performance of sparse GANs. Hence, we propose to use BR to measure the degree of unbalance between the generator and the discriminator. We find that the application of DST only on the generator is beneficial when the discriminator is relatively stronger. Furthermore, we propose two methods, namely R-DDST and S-DDST, to dynamically adjust the discriminator in both parameter and density levels. Both of these methods demonstrate encouraging results. Our study may help researchers have a better understanding of the balance of GAN training and encourage more researchers to investigate sparse training for generative models.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY STATEMENT\n\nTo ensure reproducibility, we will include a link to an anonymous repository after the discussion forums are open. All the experiment details can be found in Section 4, Section 5.1, Section 5.2, Appendix A and Appendix B.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMilad Alizadeh, Shyam A Tailor, Luisa M Zintgraf, Joost van Amersfoort, Sebastian Farquhar, Nicholas Donald Lane, and Yarin Gal. Prospect pruning: Finding trainable weights at initialization using meta-gradients. arXiv preprint arXiv:2202.08132, 2022.\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn International conference on machine learning, pp. 214–223. PMLR, 2017.\n\nSanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International Conference on Machine Learning, pp. 224– 232. PMLR, 2017.\n\nYu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in\n\ngans. arXiv preprint arXiv:1806.10586, 2018.\n\nGuillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training\n\nvery sparse deep networks. arXiv preprint arXiv:1711.05136, 2017.\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\n\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nXuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen. Gans can play lottery tickets too.\n\narXiv preprint arXiv:2106.00134, 2021.\n\nYunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8789–8797, 2018.\n\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.\n\nBin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational information bottleneck. In International Conference on Machine Learning, pp. 1135–1144. PMLR, 2018.\n\nPau de Jorge, Amartya Sanyal, Harkirat S Behl, Philip HS Torr, Gregory Rogez, and Puneet K Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. arXiv preprint arXiv:2006.09081, 2020.\n\nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing\n\nperformance. arXiv preprint arXiv:1907.04840, 2019.\n\nXin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise\n\noptimal brain surgeon. Advances in Neural Information Processing Systems, 30, 2017.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n\nnetworks. arXiv preprint arXiv:1803.03635, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, and Zhangyang Wang. arXiv preprint\n\nAutogan-distiller: Searching to compress generative adversarial networks. arXiv:2006.08198, 2020.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n\nLaura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. The state of sparse training in deep reinforcement learning. In International Conference on Machine Learning, pp. 7766–7792. PMLR, 2022.\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017.\n\nYiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. Advances\n\nin neural information processing systems, 29, 2016.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015.\n\nBabak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network\n\npruning. In IEEE international conference on neural networks, pp. 293–299. IEEE, 1993.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nLiang Hou, Zehuan Yuan, Lei Huang, Huawei Shen, Xueqi Cheng, and Changhu Wang. Slimmable generative adversarial networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 7746–7753, 2021.\n\nZehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In\n\nProceedings of the European conference on computer vision (ECCV), pp. 304–320, 2018.\n\nSteven A Janowsky. Pruning versus clipping in neural networks. Physical Review A, 39(12):6600,\n\n1989.\n\nQing Jin, Jian Ren, Oliver J Woodford, Jiazhuo Wang, Geng Yuan, Yanzhi Wang, and Sergey Tulyakov. Teachers do more than teach: Compressing image-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13600–13611, 2021.\n\nNeha Mukund Kalibhat, Yogesh Balaji, and Soheil Feizi. Winning lottery tickets in deep generative models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8038– 8046, 2021.\n\nMinsoo Kang and Bohyung Han. Operation-aware soft channel pruning using differentiable masks.\n\nIn International Conference on Machine Learning, pp. 5122–5131. PMLR, 2020.\n\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-\n\nproved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33:12104–12114, 2020a.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020b.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pp. 5544–5555. PMLR, 2020.\n\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information\n\nprocessing systems, 2, 1989.\n\nChristian Ledig, Lucas Theis, Ferenc Husz ́ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4681–4690, 2017.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning\n\nbased on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip HS Torr. A signal propagation perspective for pruning neural networks at initialization. arXiv preprint arXiv:1906.06307, 2019.\n\nMuyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Efficient architectures for interactive conditional gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5284–5294, 2020.\n\nShaojie Li, Jie Wu, Xuefeng Xiao, Fei Chao, Xudong Mao, and Rongrong Ji. Revisiting discriminator in gan compression: A generator-discriminator cooperative compression scheme. Advances in Neural Information Processing Systems, 34:28560–28572, 2021.\n\nJunjie Liu, Zhe Xu, Runbin Shi, Ray CC Cheung, and Hayden KH So. Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers. arXiv preprint arXiv:2005.06870, 2020.\n\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration. Advances in Neural Information Processing Systems, 34:9908–9922, 2021a.\n\nShiwei Liu, Decebal Constantin Mocanu, Yulong Pei, and Mykola Pechenizkiy. Selfish sparse rnn\n\ntraining. In International Conference on Machine Learning, pp. 6893–6904. PMLR, 2021b.\n\nShiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In International Conference on Machine Learning, pp. 6989–7000. PMLR, 2021c.\n\nShiwei Liu, Yuesong Tian, Tianlong Chen, and Li Shen. Don’t be so dense: Sparse-to-sparse gan\n\ntraining without sacrificing performance. arXiv preprint arXiv:2203.02770, 2022.\n\nTianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent trans-\n\nfer. In International Conference on Machine Learning, pp. 6336–6347. PMLR, 2020.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021d.\n\nZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. LearnIn Proceedings of the IEEE\n\ning efficient convolutional networks through network slimming. international conference on computer vision, pp. 2736–2744, 2017.\n\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\n\nnetwork pruning. arXiv preprint arXiv:1810.05270, 2018.\n\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\n\nl 0 regularization. arXiv preprint arXiv:1712.01312, 2017.\n\nJian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pp. 5058–5066, 2017.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nXudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2794–2802, 2017.\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization\n\nfor generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.\n\nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018.\n\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11264–11272, 2019.\n\nHesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In International Conference on Machine Learning, pp. 4646–4655. PMLR, 2019.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nAlex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural\n\nnetwork pruning. arXiv preprint arXiv:2003.02389, 2020.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\nPedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsification.\n\nAdvances in Neural Information Processing Systems, 33:11380–11390, 2020.\n\nHan Shu, Yunhe Wang, Xu Jia, Kai Han, Hanting Chen, Chunjing Xu, Qi Tian, and Chang Xu. Co-evolutionary compression for unpaired image translation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3235–3244, 2019.\n\nSuraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. Training sparse neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 138–145, 2017.\n\nRuoyu Sun, Tiantian Fang, and Alexander Schwing. Towards a better global loss landscape of gans.\n\nAdvances in Neural Information Processing Systems, 33:10186–10198, 2020.\n\nHidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in Neural Information Processing Systems, 33:6377–6389, 2020.\n\nD ́avid Terj ́ek. Adversarial lipschitz. arXiv preprint arXiv:1907.05681, 2019.\n\nSagar Verma and Jean-Christophe Pesquet. Sparsifying networks via subdifferential inclusion. In\n\nInternational Conference on Machine Learning, pp. 10542–10552. PMLR, 2021.\n\nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by\n\npreserving gradient flow. arXiv preprint arXiv:2002.07376, 2020a.\n\nHaotao Wang, Shupeng Gui, Haichuan Yang, Ji Liu, and Zhangyang Wang. Gan slimming: All-inone gan compression by a unified optimization framework. In European Conference on Computer Vision, pp. 54–73. Springer, 2020b.\n\nXintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings of the European conference on computer vision (ECCV) workshops, pp. 0–0, 2018.\n\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\n\ndeep neural networks. Advances in neural information processing systems, 29, 2016.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nYi-Lun Wu, Hong-Han Shuai, Zhi-Rui Tam, and Hong-Yu Chiu. Gradient normalization for generative adversarial networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6373–6382, 2021.\n\nXia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran. Autoprune: Automatic network pruning by regularizing auxiliary parameters. Advances in neural information processing systems, 32, 2019.\n\nYasin Yaz, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, Vijay Chandrasekhar, et al. The unusual effectiveness of averaging in gan training. In International Conference on Learning Representations, 2018.\n\nChong Yu and Jeff Pool. Self-supervised generative adversarial compression. Advances in Neural\n\nInformation Processing Systems, 33:8235–8246, 2020.\n\nRuichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao, ChingYung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance score propagation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9194– 9203, 2018.\n\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In International conference on machine learning, pp. 7354–7363. PMLR, 2019.\n\nShengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. Advances in Neural Information Processing Systems, 33:7559–7570, 2020.\n\nXiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang. Effective sparsification of neural networks with global sparsity constraint. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3599–3608, 2021.\n\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2223–2232, 2017a.\n\nJun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. Advances in neural information processing systems, 30, 2017b.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENTAL SETUP\n\nOur code is mainly based on the original code of ITOP (Liu et al., 2021c) and GAN ticket (Chen et al., 2021).\n\nA.1 ARCHITECTURE DETAILS\n\nWe use ResNet-32 (He et al., 2016) for CIFAR-10 dataset and ResNet-48 for STL-10 dataset. See Table 4 and Table 5 for detailed architectures. Spectral normalization is applied for all fullyconnected layers and convolutional layers of the discriminators.\n\nFor BigGAN architecture, we use the implementation used in Zhao et al. (2020). 2\n\nA.2 DATASETS\n\nWe use the training set of CIFAR-10 and unlabeled partition of STL-10 for GAN training. Training images are resized to 32 48 for CIFAR-10 and STL-10 datasets, respectively. Augmentation methods for both datasets are random horizontal flip and per-channel normalization.\n\n32 and 48\n\n×\n\n×\n\nA.3 TRAINING HYPERPARAMETERS\n\n10−4 for both SNGAN on the CIFAR-10 and STL-10 datasets. We use a learning rate of 2 generators and discriminators. The discriminator is updated five times for every generator update. We adopt Adam optimizer with β1 = 0 and β2 = 0.9. The batch size of the discriminator and the generator is set to be 64 and 128, respectively. Hinge loss is used following Brock et al. (2018); Chen et al. (2021). We use exponential exponential moving average (EMA) (Yaz et al., 2018) with β = 0.999. The generator is trained for a total of 100k iterations.\n\n×\n\n10−4 for both generators and BigGAN on the CIFAR-10 dataset. We use a learning rate of 2 discriminators. The discriminator is updated four times for every generator update. We adopt Adam optimizer with β1 = 0 and β2 = 0.999. The batch size of both the discriminator and the generator is set to be 50. Hinge loss is used following Brock et al. (2018); Wu et al. (2021). We use EMA with β = 0.9999. The generator is trained for a total of 200k iterations.\n\n×\n\nA.4 EVALUATION METRIC\n\nSNGAN on the CIFAR-10 and the STL-10 datasets. We compute Fr ́echet inception distance (FID) and Inception score (IS) for 50k generated images every 2000 iterations. Best FID and IS are reported. For the CIFAR-10 dataset, we report both FID for the training set and test set, whereas, for the STL-10 dataset, we report the FID of the unlabeled partition.\n\nBigGAN on the CIFAR-10 dataset. We compute Fr ́echet inception distance (FID) and Inception score (IS) for 10k generated images every 5000 iterations. Best FID and IS are reported.\n\nB DYNAMIC SPARSE TRAINING DETAILS\n\nB.1 GENERAL DST HYPERPARAMETERS\n\nFollowing Evci et al. (2020), we specify the hyper-parameters of DST through sparsity distribution, update schedule, drop criterion, and grow criterion. We explain the details of DST below.\n\nSparsity distribution at initialization. Following Evci et al. (2020); Liu et al. (2021c), only parameters of fully connected layers and convolutional layers will be pruned. At initialization, we use the commonly adopted Erd ̋os-R ́enyi-Kernel (ERK) strategy (Evci et al., 2020; Dettmers & Zettlemoyer, 2019; Liu et al., 2021c) to allocates higher sparsity to larger layers. Specifically, the sparsity of nl−1+nl+wl+hl , where nl denotes the number of channels convolutional layers l is scaled with 1 nl−1nlwlhl\n\n−\n\n2https://github.com/mit-han-lab/data-efficient-gans/tree/master/\n\nDiffAugment-biggan-cifar.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nof layer l while wl and hl are the width and the height of the corresponding kernel in that layer. For fully connected layers, Erd ̋os-R ́enyi (ER) strategy is used, where the sparsity is scaled with 1\n\nnl−1+nl nl−1nl .\n\n−\n\nDrop and grow. After ∆T training iterations, we update the mask mG by dropping/pruning fdecay(γ, T )pGdG number of connections with the lowest magnitude, where pG, dG are the number of parameters and target density for the generator, fdecay(γ, T ) is the update schedule, which will be explained in the next part. Right after the connection drop, we regrow the same amount of connections.\n\nFor the growing criterion, we test both random growth SDST(SET) (Mocanu et al., 2018; Liu et al., 2021c) and gradient-based growth SDST(RigL) (Evci et al., 2020). Concretely, gradient- (cid:12) based methods find newly-activated connections θ with highest gradient magnitude (cid:12) (cid:12), while random based methods explore connections in a random fashion. All the newly-activated connections are set to 0. One thing that should be noticed is that while previous works consider layer-wise connections drop and growth, we grow and drop connections globally as it grants more flexibility to the SDST method.\n\n(cid:12) ∂L\n\n∂θ\n\nUpdate schedule. The update schedule can be specified by the number of training iterations between sparse connectivity updates ∆T , the initial fraction of connections adjusted γ, and decaying schedule fdecay(γ, T ) for γ.\n\nEMA for sparse GAN. EMA (Yaz et al., 2018) is well-known for its ability to alleviate the nonconvergence of GAN. We also implement EMA for sparse GAN training. Specifically, we zero out the moving average of dropped weights whenever there is a mask change.\n\nB.2 DST HYPERPARAMETERS FOR SDST\n\nSNGAN on the CIFAR-10 and the STL-10 datasets. The connection update frequency of the generator ∆T is set to 500 and 1000 for the CIFAR-10 dataset and STL-10 dataset, respectively. The initial γ is set to 0.5 and we use a cosine annealing function fdecay following RigL and ITOP.\n\nBigGAN on the CIFAR-10 dataset. The connection update frequency of the generator ∆T is set to be 1000. The initial γ is set to 0.5 and we use a cosine annealing function fdecay following RigL and ITOP.\n\nB.3 DYNAMIC ADJUST AND DST HYPERPARAMETERS FOR DDST\n\nR-DDST. For R-DDST, only the generator is adjusted using DST while the discriminator is adjusted using dynamic adjust (DA). The DA bounds are chosen to be [0.475, 0.525], [0.45, 0.55], and [0.45, 0.55] for SNGAN (CIFAR-10), SNGAN (STL-10) and BigGAN (CIFAR-10), respectively. ∆d is set to be 0.05, 0.025, 0.05 for SNGAN (CIFAR-10), SNGAN (STL-10) and BigGAN (CIFAR10), respectively. The density of the discriminator is adjusted every 1000, 2000, and 5000 iterations for the three settings, respectively. Time-averaged BR over 1000 iterations is used as the indicator. We use the same setting used in Section B.2 for the generator.\n\nS-DDST. For S-DDST, the discriminator is adjusted using DA in the first half of training, i.e., the first 50,000 iterations. In the second half of the training, the discriminator is adjusted using DST. The generator is only adjusted with DST. For the DA bounds, they are set as [0.45, 0.55] and [0.475, 0.525] for CIFAR-10 and STL-10 dataset, respectively. The density of the discriminator is adjusted every 2000 iterations for each dataset. The density of the generator is adjusted every 1000 iterations.\n\nWe compute BR for every iteration to visualize the BR evolution, whereas one should note that such computational cost can be greatly decreased if BR is computed every ∆T iterations.\n\nC ALGORITHMS\n\nIn this section, we present the detailed algorithms for both DA and S-DDST. We do not present the algorithm of R-DDST as it is a combination of DA and SDST.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: ResNet architecture for CIFAR-10.\n\nTable 5: ResNet architecture for STL-10.\n\n(a) Generator\n\n(b) Discriminator\n\nR128\n\nz\n\n∈\n\ndense, 4\n\n(0, I)\n\n256\n\n∼ N 4\n\n× ResBlock up 256\n\n×\n\nResBlock up 256\n\nResBlock up 256\n\nBN, ReLU, 3\n\n3 conv, Tanh\n\n×\n\nimage x\n\n[\n\n1, 1]32×32×3\n\n∈\n\n− ResBlock down 128\n\nResBlock down 128\n\nResBlock down 128\n\nResBlock down 128\n\nReLU 0.1\n\nGlobal sum pooling\n\ndense\n\n1\n\n→\n\n(a) Generator\n\n(b) Discriminator\n\nR128\n\nz\n\n∈\n\ndense, 6\n\n(0, I)\n\n512\n\n∼ N 6\n\n× ResBlock up 256\n\n×\n\nResBlock up 128\n\nResBlock up 64\n\nimage x\n\n[\n\n1, 1]48×48×3\n\n∈\n\n− ResBlock down 64\n\nResBlock down 128\n\nResBlock down 256\n\nResBlock down 512\n\nBN, ReLU, 3\n\n×\n\n3 conv, Tanh\n\nResBlock down 1024\n\nReLU 0.1\n\nGlobal avg pooling\n\ndense\n\n1\n\n→\n\nC.1 DYNAMIC ADJUST ALGORITHM\n\nWe first present DA in Algorithm 1.\n\nAlgorithm 1 Dynamic density adjust (DA) for the discriminator.\n\nRequire: Generator G, discriminator D, DA upper bound B+ and lower bound B−, DA interval ∆TD, density\n\nincrement ∆d, grow method A, drop method B, iteration t.\n\nCompute time-averaged BR with Equation 3 if BR is greater or equal to B+ then\n\nelse if BR is less or equal to B− then\n\n1: if t mod ∆TD == 0 then 2: 3: 4: 5: 6: 7: 8: end if\n\nend if\n\nIncrease the density of discriminator from dD to dD + ∆d using given grow method A.\n\nDecrease the density of discriminator from dD to dD − ∆d using given drop method B.\n\nC.2 STRICT DOUBLE DYNAMIC SPARSE TRAINING ALGORITHM\n\nDetails of S-DDST algorithm is presented in Algorithm 2.\n\nAlgorithm 2 Strict double dynamic sparse training (S-DDST) for GANs.\n\nRequire: Generator G, discriminator D, total number of iterations T , number of training steps for discrimina-\n\ntor in each iteration N , maximal density of discriminator dmax.\n\nCompute the loss of discriminator LD(θD) LD(θD).backward()\n\nend for Compute the loss of generator LG(θG) LG(θG).backward() if t is less than 0.5 ∗ T and current density of discriminator dD is less than dmax then\n\nApply DA in Algorithm 1 to D\n\nfor n in [1, · · · , N ] do\n\n1: for t in [1, · · · , T ] do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for\n\nend if Apply DST to G\n\nApply DST to D\n\nelse\n\nD DDST BALANCE RATIO EVOLUTION\n\nIn this section, we show that DDST methods are able to maintain a BR throughout training. We show the time evolution of BR and discriminator density for CIFAR-10 and STL-10 datasets.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Balance ratio and discriminator density evolution during training for R-DDST(RigL) on CIFAR-10. Dashed lines represent BR values of 0.45 and 0.55.\n\nFigure 6: Balance ratio and discriminator density evolution during training for R-DDST(RigL) on STL-10. Dashed lines represent BR values of 0.45 and 0.55.\n\nD.1 R-DDST\n\nResults are shown in Figure 5 and Figure 6. It clearly illustrates the ability of R-DDST(RigL) to keep the BR controlled during GAN training.\n\nD.2 S-DDST\n\nResults of S-DDST are shown in Figure 7 and Figure 8. S-DDST(RigL) to keep the BR controlled during GAN training.\n\nIt clearly illustrates the ability of\n\n19\n\n020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=10%0.20.40.6Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=20%0.20.40.60.81.0Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=30%0.40.60.81.0Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=50%0.50.60.70.80.91.0Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=10%0.040.060.080.10Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=20%0.150.200.250.300.35Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=30%0.30.40.50.6Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=50%0.50.60.70.80.91.0DensityofthediscriminatorUnder review as a conference paper at ICLR 2023\n\nFigure 7: Balance ratio and discriminator density evolution during training for S-DDST(RigL) on CIFAR-10. Dashed lines represent BR values of 0.45 and 0.55.\n\nFigure 8: Balance ratio and discriminator density evolution during training for S-DDST(RigL) on STL-10. Dashed lines represent BR values of 0.45 and 0.55.\n\nE MORE EXPERIMENT RESULTS\n\nIn this section, we present IS scores results for Table 1 and Table 3. The corresponding results are shown in Table 6 and Table 7, respectively. We also include FID results of CIFAR-10 test set in Table 8.\n\n20\n\n020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=10%0.10.20.30.40.5Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=20%0.20.30.40.5Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=30%0.300.350.400.450.50Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=50%0.480.490.500.510.52Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=10%0.00.10.20.30.4Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=20%0.10.20.30.40.5Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=30%0.20.30.40.5Densityofthediscriminator020000400006000080000100000Trainingiteration−0.250.000.250.500.751.001.25BalanceratioDensityofthegeneratordG=50%0.400.420.440.460.480.50DensityofthediscriminatorUnder review as a conference paper at ICLR 2023\n\nTable 6: IS (higher is better) of different sparse training methods on CIFAR-10 and STL-10 datasets. There is no constraint on the density of the discriminator, i.e., dmax = 100%.\n\nDataset\n\nCIFAR-10\n\nSTL-10\n\nGenerator density\n\n10% 20 % 30 % 50 % 10% 20 % 30 % 50 %\n\n(Dense Baseline)\n\nStatic-Balance Static-Strong\n\nSDST-Balance (SET) SDST-Strong (SET) SDST-Balance (RigL) SDST-Strong (RigL)\n\n8.48\n\n9.16\n\n7.18 7.49\n\n6.94 8.27 6.81 8.20\n\n7.76 8.00\n\n7.79 8.46 7.77 8.38\n\n8.01 8.31\n\n8.05 8.51 8.08 8.55\n\n8.31 8.54\n\n8.20 8.43 8.30 8.48\n\n7.84 7.74\n\n8.40 8.10 8.85 8.25\n\n8.07 8.29\n\n8.54 8.67 8.74 9.30\n\n8.35 8.38\n\n9.20 8.89 9.19 9.01\n\n8.60 8.83\n\n9.12 9.31 9.14 9.37\n\nR-DDST (SET) R-DDST (RigL)\n\n8.40 8.49 Table 7: IS (higher is better) of different sparse training methods on CIFAR-10 and STL-10 datasets. The density of the discriminator is constrained to be lower than dmax = 50%.\n\n8.33 8.79\n\n8.50 8.61\n\n8.62 9.25\n\n8.55 8.32\n\n9.34 9.27\n\n9.04 9.30\n\n8.56 8.55\n\nDataset\n\nCIFAR-10\n\nSTL-10\n\nGenerator density\n\n10% 20 % 30 % 50 % 10% 20 % 30 % 50 %\n\n(Dense Baseline)\n\nStatic-Balance Static-Strong\n\nSDST-Balance (SET) SDST-Strong (SET) SDST-Balance (RigL) SDST-Strong (RigL)\n\nS-DDST (SET) S-DDST (RigL)\n\n8.48\n\n9.16\n\n7.18 7.86\n\n6.94 8.22 6.81 8.24\n\n8.08 8.16\n\n7.76 8.21\n\n7.79 8.36 7.77 8.51\n\n8.25 8.47\n\n8.01 8.35\n\n8.05 8.56 8.08 8.20\n\n8.45 8.29\n\n8.31 8.28\n\n8.20 8.35 8.30 8.18\n\n8.23 8.32\n\n7.84 7.81\n\n8.40 8.40 8.85 7.70\n\n8.07 8.45\n\n8.07 8.05\n\n8.54 9.29 8.74 9.32\n\n8.91 9.24\n\n8.35 8.26\n\n9.20 9.21 9.19 9.19\n\n9.11 9.16\n\n8.60 8.37\n\n9.12 9.22 9.14 9.33\n\n9.50 9.03\n\nTable 8: FID of test set ( are in bold; second-best results are underlined.\n\n↓\n\n) of different sparse training methods on CIFAR-10 dataset. Best results\n\nMaximal discriminator density dmax\n\n100 %\n\n50 %\n\nGenerator density\n\n(Dense Baseline)\n\nStatic-Balance Static-Strong\n\nSDST-Balance (SET) SDST-Strong (SET) SDST-Balance (RigL) SDST-Strong (RigL)\n\nR-DDST (SET) R-DDST (RigL) S-DDST (SET) S-DDST (RigL)\n\n10% 20 % 30 % 50 % 10% 20 % 30 % 50 %\n\n13.32\n\n29.53 29.15\n\n30.34 19.95 33.25 19.67\n\n16.34 16.65 -\n-\n\n20.83 22.17\n\n21.00 17.05 20.74 15.79\n\n15.29 14.87 -\n-\n\n17.09 17.37\n\n16.84 15.16 17.78 13.89\n\n14.30 14.49 -\n-\n\n14.21 14.04\n\n15.53 14.10 14.75 14.36\n\n13.85 14.05 -\n-\n\n29.53 21.98\n\n30.34 18.83 33.25 18.60\n\n- -\n17.75 17.07\n\n20.83 19.35\n\n21.00 15.96 20.74 16.01\n\n- -\n15.74 15.50\n\n17.09 16.52\n\n16.84 15.61 17.78 15.84\n\n- -\n15.07 15.02\n\n14.21 14.84\n\n15.53 14.53 14.75 14.67\n\n- -\n14.91 14.67\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Training FLOPs (\n\n×\n\n1017) of different sparse training methods on CIFAR-10 dataset.\n\nDataset\n\nGenerator density\n\n(Dense Baseline)\n\ndmax = 100%\n\ndmax = 50%\n\nStatic-Strong SDST-Strong R-DDST\n\nStatic-Strong SDST-Strong S-DDST\n\n×\n\nCIFAR-10\n\n10%\n\n20 %\n\n30 %\n\n50 %\n\n(1.74, 1.00\n\n) ×\n\n(0.63, 0.36 (0.63, 0.36 (0.63, 0.36\n\n× ×\n×\n\n(0.36, 0.21 (0.36, 0.21 (0.36, 0.21\n\n) )\n)\n\n) )\n)\n\n(0.70, 0.40 (0.70, 0.40 (0.70, 0.40\n\n(0.43, 0.25 (0.43, 0.25 (0.43, 0.25\n\n× ×\n×\n\n) )\n)\n\n) )\n)\n\n× ×\n×\n\n× ×\n×\n\n(0.80, 0.46 (0.80, 0.46 (0.80, 0.46\n\n(0.53, 0.30 (0.53, 0.30 (0.53, 0.30\n\n) ×\n) ×\n) ×\n) ×\n) ×\n) ×\n\n(1.07, 0.61 (1.07, 0.61 (1.07, 0.61\n\n(0.79, 0.46 (0.79, 0.46 (0.79, 0.46\n\n) )\n)\n\n) )\n)\n\n× ×\n×\n\n× ×\n×\n\nTable 10: Training FLOPs (\n\n1017) of different sparse training methods on STL-10 dataset.\n\nDataset\n\nGenerator density\n\n(Dense Baseline)\n\nSTL-10\n\n10%\n\n20 %\n\n30 %\n\n50 %\n\n(1.85, 1.00\n\n) ×\n\ndmax = 100%\n\ndmax = 50%\n\nStatic-Strong SDST-Strong R-DDST\n\nStatic-Strong SDST-Strong S-DDST\n\n(1.30, 0.75 (1.30, 0.75 (1.30, 0.75\n\n× ×\n×\n\n(1.07, 0.62 (1.07, 0.62 (1.07, 0.62\n\n× ×\n×\n\n) )\n)\n\n) )\n)\n\n(1.34, 0.77 (1.34, 0.77 (1.34, 0.77\n\n(1.11, 0.63 (1.11, 0.63 (1.11, 0.63\n\n) )\n)\n\n) )\n)\n\n× ×\n×\n\n× ×\n×\n\n(1.36, 0.78 (1.36, 0.78 (1.36, 0.78\n\n(1.13, 0.65 (1.13, 0.65 (1.13, 0.65\n\n) ×\n) ×\n) ×\n) ×\n) ×\n) ×\n\n(1.41, 0.81 (1.41, 0.81 (1.41, 0.81\n\n(1.18, 0.68 (1.18, 0.68 (1.18, 0.68\n\n) )\n)\n\n) )\n)\n\n× ×\n×\n\n× ×\n×\n\nF A ROUGH ESTIMATION OF COMPUTATIONAL COSTS ON SNGAN\n\nIn this section, we provide a very rough estimation on the computational cost of different sparse training methods in terms of training FLOPs. Please see Appendix G for a more accurate comparison. We approximate the number of backward FLOPs with two times the number of forward FLOPs. We compare the following methods under two settings where dmax\n\n100%, 50%\n\n:\n\n∈ {\n\n}\n\n• Dense training. • static-Strong. • SDST-Strong. • R-DDST. • S-DDST.\n\nWe choose static-Strong and SDST-Strong as they perform relatively better than their counterparts with the balance strategy. To simplify our calculation, we compute the FLOPs of R-DDST and S-DDST assuming the discriminator density dD = dmax. We also assume that DST may not cause the change of FLOPs. The results are shown in Table 9 and Table 10.\n\nIt can be seen that the extra computational cost introduced by DA3, which computes BR, and RigL, which computes gradient magnitude for connection growth, is negligible compared to the total training cost as they only happen every several hundred iterations.\n\nG A DETAILED COMPARISON OF TRAINING COSTS\n\nIn this section, we compute the computational cost of RigL vairants and static baselines more accurately. More specifically, we take into account the density redistribution over different layers in this section. Also, we neglect the computational overhead introduced by computing BR.\n\n3In our experiment, we compute BR for every iteration to visualize its evolution. However, BR only needs\n\nto be calculated for every several hundred iterations to compute the time-averaged BR.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Training FLOPs ( of different sparse training methods with SNGAN on CIFAR-10 dataset.\n\n1017) and normalized training FLOPs with respect to dense training\n\n×\n\nDataset\n\nGenerator density\n\n(Dense Baseline)\n\nStatic-Balance Static-Strong\n\nSDST-Balance (RigL) SDST-Strong (RigL) R-DDST (RigL)\n\nCIFAR-10\n\n10%\n\n20 %\n\n30 %\n\n50 %\n\n(2.67, 100%)\n\n(0.24, 9.00%) (1.56, 58.29%) (0.24, 9.14%) (1.57, 58.80%) (0.49, 18.20%)\n\n(0.46, 17.08%) (1.63, 60.94%) (0.46, 17.19%) (1.64, 61.42%) (1.14, 42.73%)\n\n(0.70, 26.29% ) (1.72, 64.57%) (0.68, 25.62%) (1.71, 64.04%) (1.63, 60.94%)\n\n(1.26, 47.34%) (1.99, 74.64%) (1.16, 43.45%) (1.90, 71.16%) (1.85, 69.33%)\n\nTable 12: Training FLOPs ( of different sparse training methods with BigGAN on CIFAR-10 dataset.\n\n1017) and normalized training FLOPs with respect to dense training\n\n×\n\nDataset\n\nGenerator density\n\n(Dense Baseline)\n\nStatic-Balance Static-Strong\n\nSDST-Balance (RigL) SDST-Strong (RigL) R-DDST (RigL)\n\nCIFAR-10\n\n10%\n\n20 %\n\n30 %\n\n50 %\n\n(6.80, 100%)\n\n(0.67, 9.78%) (5.71, 83.90%) (0.67, 9.91%) (5.72, 84.04%) (0.67, 9.77%)\n\n(1.30, 19.04%) (5.78, 83.90%) (1.32, 19.41%) (5.80, 85.22%) (1.69, 24.85%)\n\n(1.95, 28.69%) (5.87, 86.34%) (1.96, 28.82%) (5.89, 86.54%) (2.72, 40.00%)\n\n(3.34, 49.09% ) (6.14, 90.26%) (3.29, 48.38%) (6.09, 89.56%) (5.25, 77.13%)\n\nG.1 SNGAN ON THE CIFAR-10 DATASET\n\nWe first show the results of SNGAN (CIFAR-10) in Table 11. Combined with the results shown in Table 1, it shows that generally R-DDST is able to achieve promising performance with reasonable computational costs. More precisely, R-DDST outperforms SDST-Strong (RigL) with much fewer training FLOPs. The reason is that SDST-Strong (RigL) uses unnecessarily strong (dense) discriminators.\n\nG.2 BIGGAN ON THE CIFAR-10 DATASET\n\nIn this subsection, we show the results of BigGAN (CIFAR-10). We have included the simplified version in the Table 2. Here we give more detailed results in Table 12. The results are similar to SNGAN on the CIFAR dataset.\n\nH ONE-SHOT PRUNING AFTER TRAINING WITHOUT FINE-TUNING\n\nIn this section, we perform one-shot pruning after training for GANs without any fine-tuning. The results of SNGANs on the CIFAR-10 and STL-10 datasets are shown in Table 13.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nTable 13: FID ( ) of different sparse training methods on CIFAR-10 and STL-10 datasets. The density of the discriminator is constrained to be lower than 50%. Best results are in bold; secondbest results are underlined.\n\n↓\n\nCIFAR-10\n\nSTL-10\n\nGenerator density\n\n10%\n\n20 % 30 % 50 %\n\n10%\n\n20 %\n\n30 % 50 %\n\n(Dense Baseline)\n\n10.74\n\n29.71\n\nPF without fine-tuning\n\n305.81\n\n247.99\n\n89.29\n\n30.03\n\n339.95\n\n195.69\n\n156.29\n\n66.66\n\nStatic-balance Static-strong\n\nDST-balance (SET) DST-balance (RigL)\n\nSDST-balance (SET) SDST-strong (SET) SDST-balance (RigL) SDST-strong (RigL)\n\nS-DDST (SET) S-DDST (RigL)\n\n26.73 22.35\n\n32.02 24.56\n\n27.80 16.00 30.38 15.66\n\n14.22 14.13\n\n18.04 16.57\n\n18.54 15.53\n\n18.13 13.31 17.89 13.20\n\n13.30 12.87\n\n14.38 13.47\n\n14.74 13.62\n\n14.15 13.17 14.95 12.99\n\n12.39 12.15\n\n12.22 12.22\n\n13.23 12.51\n\n12.32 12.32 12.09 12.09\n\n11.97 12.17\n\n50.08 50.28\n\n49.91 66.90\n\n63.57 48.40 46.17 63.65\n\n51.72 44.28\n\n44.19 44.95\n\n33.71 50.34\n\n49.05 33.56 38.12 33.45\n\n35.74 32.84\n\n43.96 42.12\n\n32.92 44.57\n\n43.74 32.19 32.48 32.09\n\n42.36 32.00\n\n37.21 37.21\n\n31.75 32.63\n\n31.29 31.29 31.30 31.30\n\n31.68 30.28\n\n24",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose a new dynamic sparse training method for GANs. The key idea is to balance the performance of the generator and discriminator during training by developing a quantity called balance ratio. Based on this quantity, the authors adjust the densities of the generator and discriminator during training to keep balance. This quantity can be integrated with existing sparse training algorithms flexibly. Some experiments are conducted to evaluate the performance of the proposed method.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. The proposed quantity can be integrated with existing sparse training algorithms flexibly to stabilize the training process. \n\n2. The experimental results show that the proposed method can stabilize the training process. \n\nWeaknesses:\n\n1. The performance in Table 2 shows that the improvements achieved by the proposed method over SDST are marginal in some tasks. The authors are recommended to look into this phenomenon.\n\n2. Some theoretical analysis is needed for the proposed balance ratio. I understand it is difficult to prove the convergence of training.  But some analysis is essential to show the insights and properties of the proposed balance ratio, this paper looks too heuristic. \n\n3. This paper is not well organized. For example, the authors present SDST for more than 1 page, which can potentially make the readers confused. \n\n4. Some notations are abused. For example, the $\\alpha$ in balance ratio and $f_{decay}$ should be different.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee the strength and weaknesses.\n\n# Summary Of The Review\n\nThe performance of the proposed method is marginal in some tasks.\n\nThis paper looks too heuristic since no theoretical analysis are provided for the proposed balance ratio.  \n\nThis paper is also not well organized.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCONTRASTIVE META-LEARNING FOR PARTIALLY OBSERVABLE FEW-SHOT LEARNING\n\nAdam Jelley1, Amos Storkey1, Antreas Antoniou1, 1School of Informatics, University of Edinburgh,\n\nSam Devlin2 2 Microsoft Research, Cambridge\n\nABSTRACT\n\nMany contrastive and meta-learning approaches learn representations by identifying common features in multiple views. However, the formalism for these approaches generally assumes features to be shared across views to be captured coherently. We consider the problem of learning a unified representation from partial observations, where useful features may be present in only some of the views. We approach this through a probabilistic formalism enabling views to map to representations with different levels of uncertainty in different components; these views can then be integrated with one another through marginalisation over that uncertainty. Our approach, Partial Observation Experts Modelling (POEM), then enables us to meta-learn consistent representations from partial observations. We evaluate our approach on an adaptation of a comprehensive few-shot learning benchmark, Meta-Dataset, and demonstrate the benefits of POEM over other meta-learning methods at representation learning from partial observations. We further demonstrate the utility of POEM by meta-learning to represent an environment from partial views observed by an agent exploring the environment.1\n\nFigure 1: Standard contrastive (meta-) learners minimise a relative distance between representations. This encourages the learning of features that are consistent in all views; in the above example this corresponds to the pattern on the bird’s wing. To better handle partial observability, where features may be disjoint between views, we propose Partial Observation Experts Modelling (POEM). POEM instead maximises consistency between multiple views, by utilising representation uncertainty to learn which features of the entity are captured by a view, and then combining these representations together by weighting features by their uncertainty via a product of experts model (Hinton, 2002).\n\nINTRODUCTION\n\n1 Modern contrastive learning methods (Radford et al., 2021; Chen et al., 2020; He et al., 2020; Oord et al., 2019), and embedding-based meta-learning methods such as Prototypical Networks (Snell et al., 2017; Vinyals et al., 2016; Sung et al., 2018; Edwards & Storkey, 2017), learn representations by minimizing a relative distance between representations of related items compared with unrelated\n\n1Implementation code is available at https://github.com/AdamJelley/POEM\n\n1\n\nMinimise DistanceMaximise Consistency(a) Standard Contrastive (Meta-) Learners(b) Partial Observation Experts Model (POEM)Published as a conference paper at ICLR 2023\n\nitems (Ericsson et al., 2021). However, we argue that these approaches may learn to disregard potentially relevant features from views that only inform part of the representation in order to achieve better representational consistency, as demonstrated in Figure 1. We refer to such partially informative views as partial observations. The difficulty with partial observations occurs because distances computed between representations must include contributions from all parts of the representation vector. If the views provided are diverse, and therefore contain partially disjoint features, their representations may appear different to a naive distance metric. For example, two puzzle pieces may contain different information about the whole picture. We call this the problem of integrative representation learning, where we wish to obtain a representation that integrates different but overlapping information from each element of a set.\n\nIn this paper, we provide a probabilistic formalism for a few-shot objective that is able to learn to capture representations in partially observable settings. It does so by building on a product of experts (Hinton, 2002) to utilise representation uncertainty: a high variance in a representation component indicates that the given view of the data poorly informs the given component, while low variance indicates it informs it well. Given multiple views of the data, the product of experts component in POEM combines the representations, weighting by the variance, to get a maximally informative and consistent representation from the views.\n\nTo comprehensively evaluate our approach, we adapt a large-scale few-shot learning benchmark, Meta-Dataset (Triantafillou et al., 2020), to evaluate representation learning from partial observations. We demonstrate that our approach, Partial Observation Experts Modelling (POEM), is able to outperform standard few-shot baselines on our adapted benchmark, Partially Observed MetaDataset (PO-Meta-Dataset), while still matching state-of-the-art on the standard benchmark. Finally, we demonstrate the potential for our approach to be applied to meta-learn representations of environments from the partial views observed by an agent exploring that environment.\n\nThe main contributions of this work are: 1) A probabilistic formalism, POEM, that enables representation learning under partial observability; 2) Comprehensive experimental evaluation of POEM on an adaptation of Meta-Dataset designed to evaluate representation learning under partial observability, demonstrating that this approach outperforms standard baselines in this setting while still matching state-of-the-art on the standard fully observed benchmark; 3) A demonstration of a potential application of POEM to meta-learn representations of environments from partial observations.\n\n2 RELATED WORK\n\n2.1 CONTRASTIVE LEARNING\n\nContrastive learning extracts features that are present in multiple views of a data item, by encouraging representations of related views to be close in an embedding space (Ericsson et al., 2021). In computer vision and natural language applications these views typically consist of different augmentations of data items, which are carefully crafted to preserve semantic features, and thereby act as an inductive bias to encourage the contrastive learner to retain these consistent features (Le-Khac et al., 2020). A challenge in this approach is to prevent representational ‘collapse’, where all views are mapped to the same representation. Standard contrastive approaches such as Contrastive Predictive Coding (Oord et al., 2019), MoCo (He et al., 2020), and SimCLR (Chen et al., 2020) handle this by computing feature space distance measures relative to the distances for negative views – pairs of views that are encouraged to be distinct in the embedding space. In this work we take a similar approach, where the negative views are partial observations of distinct items, but we aim to learn to unify features from differing views, not just retain the consistent features. We learn to learn a contrastive representation from partial views. We note that state-of-the-art representation learning approaches such as CLIP (Radford et al., 2021), which leverage contrastive learning across modalities, also suffer from extracting only a limited subset of features (F ̈urst et al., 2022) due to using an embedding-based approach (Vinyals et al., 2016) to match image and text representations.\n\n2.2 EMBEDDING-BASED META-LEARNING\n\nEmbedding-based meta-learners similarly learn representations of classes by extracting features that are consistently present in the data samples (generally referred to as shots in the meta-learning literature) provided for each class, such that the class of new samples can be identified with a similarity\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nmeasure (Hospedales et al., 2020). These methods generally differ in terms of their approach to combine features, and the distance metric used. Prototypical Networks (Snell et al., 2017) use a Euclidian distance between the query representation and the average over the support representations for a class (referred to as a prototype). Relation Networks (Sung et al., 2018) use the same prototype representation as Prototypical Networks, but use a parameterised relation module to learn to compute the similarity between the query and the prototype rather than using a Euclidian distance. Matching Networks (Vinyals et al., 2016) use a Cosine distance between the query sample and each support sample as a weighting over the support labels, and so perform few-shot classification without unifying the support representations. None of these approaches are designed to unify partially informative support samples. The approach closest to that proposed in this paper is by Edwards & Storkey (2017), where the authors map the different views to a statistic with an associated covariance through a variational approach. However there is no control of the contribution of each view to the variance, and the covariance is spherical, so the approach is also unsuitable for partial observation.\n\n2.3 OPTIMISATION-BASED META-LEARNING\n\nThe few-shot classification task can also be solved without learning embeddings. One sensible baseline, fine-tuning of a previously pre-trained large model, simply treats each few-shot task as a standard classification problem (Nakamura & Harada, 2019). For each task, one or more additional output layers are added on top of a pre-trained embedding network and trained to predict the classes of the support set (alongside optionally finetuning the embedding network). This can then be utilised to predict the classes of the query set.\n\nTaking this approach a step further, Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) learns the initialisation of the embedding network, such that it can be rapidly fine-tuned on a new few-shot task. Given the generality of this approach, many variants of this method now exist, such as MAML++, Antoniou et al. (2018), Meta-SGD (Li et al., 2017), CAVIA (Zintgraf et al., 2019) and fo-Proto-MAML (Triantafillou et al., 2020). One variant, LEO (Rusu et al., 2019), performs the meta-optimisation on a latent representation of the embedding parameters, learned using a relational network (Sung et al., 2018). However, none of these variants of this fundamental optimisation based approach to few-shot learning (referred to as ’MAML’ for the remainder of this work) have a mechanism for integrating partial information from the entire support set at inference time, or for comparison with a partial query observation.\n\n2.4 OTHER META-LEARNING APPROACHES\n\nProbabilisitic meta-learning methods, such as VERSA (Gordon et al., 2019), DKT (Patacchiola et al., 2020) and Amortised Bayesian Prototype Meta-Learning (Sun et al., 2021), often unify both embedding-based and optimisation based meta-learning by learning to output a posterior distribution that captures uncertainty in predictions, but do not use uncertainty in features to optimally combine support set information. Other recent work, such as DeepEMD (Zhang et al., 2022), has considered the use of attention mechanisms or transformers with image patches (Hiller et al., 2022; Dong et al., 2020), or augmentations (Chen et al., 2021a). However, the purpose of these approaches is to identify informative patches or features within each support example, to improve fine-grained few-shot learning performance or interpretability where relevant features may occupy only a small region of the samples. As far as we know, there are no existing meta-learning methods that aim to integrate partial information from across the support set for comparison with a partially informative query.\n\n2.5 PARTIAL OBSERVABILITY AND PRODUCT OF EXPERTS\n\nFactor analysis is the linear counterpart to modern representation learners, but where partial observability is inherently expressed in the model. The inferential model for the latent space in factor analysis is a product of each of the conditional Gaussian factors. In general, this form of inferential model can be captured as a product of experts (Hinton, 2002). When those experts are Gaussian distributions (Williams et al., 2001), this product of experts is fully tractable. By focusing on the inferential components rather than the linear model, it is possible to generalise factor analysis inference to nonlinear mappings (Tang et al., 2012). However, when only an inferential component is required (as with representation learning), the product of experts can be used more flexibly, as in our approach below.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 THEORETICAL FORMALISM\n\nIn this section, we introduce POEM, which incorporates a product of experts model for combining different views with a prior representation, and then uses that representation to classify a query view.\n\n3.1 PRODUCT OF EXPERT PROTOTYPES\n\n1 , xm\n\n2 , . . . , xm\n\nLet us consider data corresponding to partial observations, or views, of a set of items. In common with most few-shot frameworks, we arrange the data into support sets and query sets. Each support set consists of M data items: S = {Xm|m = 1, 2, . . . , M }, where the mth item Xm collects V m views, where V may vary with m. Let xm v denote the vth view of the mth data item, such that Xm = {xm V m}. The items in the support set are sampled randomly from the training dataset. The query point, denoted x∗, here consists of a single different view corresponding to one and only one of the M items in the support set (although in general we may consider N query points simultaneously). We cast our representation learning problem as a meta-learning task. We must learn a unified representation derived from the support set that can be compared with a representation of the query view. We want that comparison to enable us to infer which support set item m = m∗ the query view belongs to.\n\nIn this paper we are concerned with partial observability; that is, not every data view will inform the whole representation. So instead of mapping each view to a deterministic point representation, we map each view to a distributional representation where each component is a normalised density that indicates the uncertainty in that component (called a factor). We denote this conditional density φ, and on implementation parameterise the parameters of the distribution φ with a neural network. We combine the corresponding factors for each view together using a product of experts, which integrates a prior distribution along with the different views such that views with low variance in a component strongly inform that component.\n\nFor a given support set, we compute a product of experts distribution for the representation zm:\n\np(zm|Xm) =\n\np(zm) (cid:81)V m (cid:82) dz′ p(z′) (cid:81)V m\n\nv=1 φ(zm|xm v ) v=1 φ(z′|xm v )\n\n,\n\n(1)\n\nwhere p(z) is a prior density over the latent space. Now for a query point with a view that matches other views from e.g. data item m, we can use Bayes rule to compute the probability that the query point would be generated from the corresponding representation zm by\n\np(x∗|zm) =\n\np(x∗)φ(zm|x∗) p(zm)\n\n,\n\n(2)\n\nwhere, again, p(z) = (cid:82) dx p(x)φ(z|x) is the prior. We put Eq.2 and Eq.1 together and marginalise over zm to get the marginal predictive distribution\n\np(x∗|Xm) =\n\n(cid:90)\n\ndzm\n\n= p(x∗)\n\nwhere\n\n(cid:32)\n\np(zm) (cid:81)V m (cid:82) dz′ p(z′) (cid:81)V m (cid:32) (cid:82) dzm φ(zm|x∗) (cid:81)V m\n\nv=1 φ(zm|xm v ) v=1 φ(z′|xm v )\n\nv=1 φ(zm|xm v )\n\n(cid:82) dz′ p(z′) (cid:81)V m\n\nv=1 φ(z′|xm v )\n\n(cid:33) (cid:18) p(x∗)φ(zm|x∗)\n\n(cid:19)\n\np(zm)\n\n(cid:33)\n\n= p(x∗)\n\nλ(x∗, Xm) λ′(Xm)\n\n(cid:90)\n\n(cid:90)\n\nλ(y, X) =\n\nλ′(X) =\n\ndz φ(z|y)\n\nV (cid:89)\n\nv=1\n\nφ(z|xv),\n\nand\n\ndz p(z)\n\nV (cid:89)\n\nv=1\n\nφ(z|xv).\n\n(3)\n\n(4)\n\n(5)\n\n(6)\n\nThe marginal predictive p(x∗|Xm) is used to form the training objective. In our few shot task, we wish to maximize the likelihood for the correct match of query point to support set, accumulated across all support/query selections indexed with t from the dataset. This provides a complete\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nnegative log marginal likelihood objective to be minimized, as derived in appendix A.2:\n\nL({St}, {x∗\n\nt }) = −\n\n(cid:34)\n\nlog\n\n(cid:88)\n\nt\n\nλ(x∗, Xm∗ λ′(Xm∗ )\n\n)\n\n− log\n\nλ(x∗, Xm) λ′(Xm)\n\n(cid:88)\n\nm\n\n(cid:35)\n\n(7)\n\nFull pseudocode for training POEM with this objective is provided in appendix A.3.\n\n3.2\n\nINTERPRETATION OF OBJECTIVE\n\nWhile the normalised factors φ can be chosen from any distribution class, we take φ to be Gaussian with parameterised mean and precision for the remainder of this paper, rendering the integral in Eq. 5 analytic. Approximating the prior p(z) by a Gaussian also renders Eq. 6 analytic. 2 We note that other distributions with analytic products, such as Beta distributions, may also be of interest in certain applications, but we leave an investigation of other distributional forms for φ to further work.\n\nIf the representations from each view for a support point are aligned with each other and the query view (the means of all the Gaussians are similar), they will have a greater overlap and the integral of the resulting product of Gaussians will be larger, leading to a greater value of λ(y, X). Furthermore, increasing the precisions for aligned Gaussian components leads to greater λ(y, X), while, up to a limit, decreasing the precisions for non-aligned Gaussian components leads to greater λ(y, X).\n\nWhile the numerator in Eq. 4, λ(y, X), quantifies the overlap of the support set with the query, the denominator λ′(X) contrasts this with the overlap of the support set representation with the prior. Together, this factor is enhanced if it is beneficial in overlap terms to replace the prior with the query representation, and reduced if such a replacement is detrimental. A greater consistency between query and combined support set representations intuitively leads to a greater probability that the query belongs to the class of the corresponding support set, effectively extending Prototypical Networks to a probabilistic latent representation space (Snell et al., 2017).\n\nAs a result, this objective is a generalisation of a Prototypical Network that allows for (a) learnable weighted averaging over support examples based on their informativeness to a given component; (b) learnable combinations of features from subsets of support examples (via differing relative precisions of components within support representations), and (c) partial comparison of the query sample with the support samples (via differing relative precisions within the query). With all precisions fixed to 1, this approach reproduces Prototypical Networks, neglecting small differences in scaling factors that arise with varying numbers of views. This relationship is derived in Appendix A.4.\n\n4 EXPERIMENTAL EVALUATION\n\nThere is a lack of established benchmarks specifically targeted at the evaluation of representation learning under partial observability. To design a comprehensive benchmark for few-shot representation learning under partial observability, we leverage Meta-Dataset (Triantafillou et al., 2020), a recently proposed collection of few-shot learning benchmarks. We selected Meta-Dataset as the basis for our adapted benchmark as it consists of diverse datasets involving natural, human-made and text-based visual concepts, with a variety of fine-grained classification tasks that require learning from varying and unbalanced numbers of samples and classes. As a result, our derived benchmark inherits these properties to provide a robust measure of the ability of a learning approach to learn representations from partial observations.\n\nTo extend Meta-Dataset to incorporate partial observability, we take multiple views of each sample and divide these views into support and query sets. Our adapted few-shot classification task is to predict which sample a query view comes from, given a selection of support views of that sample, as demonstrated in Figure 2.\n\nIn keeping with the spirit of Meta-Dataset, we vary the number of ways in the task (now the number of images) from 5 to 25, taken from between 1 to 5 classes. Views are generated by applying the standard augmentation operations used in SimCLR (Chen et al., 2020) and most other selfsupervised learning methods. However, to emphasise the focus on partial observability, the size of\n\n2In reality, p(z) is typically flat over the region of non-negligible density of the product (cid:81)V\n\ndoes not affect the value of λ′ in Eq. 6 and can be neglected, as described in appendix A.1.\n\nv=1 φ(z|xv) so\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Standard few-shot learning requires the prediction of an image class from a sample. Our adapted task evaluates representation learning under partial observability by instead requiring prediction of the underlying image from partial views. Views are generated with the standard contrastive augmentations, with stronger cropping. We call the resulting benchmark Partially Observable MetaDataset (PO-Meta-Dataset).\n\nthe random crops and the number of views was fixed, such that the entire support set for a sample contains a maximum of 50% of the image. We also maintain a constant number of query views per sample. Viewpoint information consisting of the coordinates of the view is provided to make it possible for learners to understand where a view fits into a representation even in the absence of overlapping views. Full details of the definition of the task are provided in appendix A.5.\n\nWe apply our proposed evaluation procedure to all datasets included in Meta-Dataset with a few exceptions. ILSVRC (ImageNet, Russakovsky et al. (2015)) was not included since our network backbones were pre-trained on this dataset, including the standard few-shot test classes (which is also why this dataset was subsequently removed from the updated benchmark, MetaDataset-v2 (Dumoulin et al., 2021)). Traffic Signs (Stallkamp et al., 2011) and MSCOCO (Lin et al., 2015) were not included since these datasets are fully reserved for evaluation by Meta-Dataset and so do not have a training set specified. Quick Draw (Fernandez-Fernandez et al., 2019) was also not included since this dataset was found to be too large to use within the memory constraints of standard RTX2080 GPUs. This leaves six diverse datasets: Aircraft (Maji et al., 2013), Birds (Wah et al., 2011), Flowers (Nilsback & Zisserman, 2008), Fungi (Schroeder, Brigit, 2018), Omniglot (Lake et al., 2015) and Textures (Cimpoi et al., 2014), on all of which our models were trained, validated and tested on according to the data partitions specified by the Meta-Dataset benchmark.\n\nThe resulting benchmark, Partially Observed Meta-Dataset (PO-Meta-Dataset), therefore requires that the learner coherently combine the information from the support views into a consistent representation of the sample, such that the query view can be matched to the sample it originated from. Since a maximum of 50% of each sample is seen in the support set, the task also requires generalisation to correctly match and classify query views.\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nWe utilise a re-implementation of Meta-Dataset benchmarking in PyTorch (Paszke et al., 2019) which closely replicates the Meta-Dataset sampling procedure of uniformly sampling classes, followed by a balanced query set (since all classes are considered equally important) and unbalanced support sets (to mirror realistic variations in the appearances of classes). The experimental implementation, including full open-source code and data will be available on publication.\n\nFollowing the MD-Transfer procedure used in Meta-Dataset, we leverage a single ResNet-18 (He et al., 2015) classifier pre-trained on ImageNet (Russakovsky et al., 2015) at 126 × 126 resolution. Since both a mean and precision must be learned to fully specify the model φv(z|xn v ), we add two simple 3-layer MLP heads onto this backbone for POEM, each maintaining an embedding size of\n\n6\n\nMeta-DatasetDatasetClassesSamplesViewsVGG Flowers010. Globe ThistleMeta-DatasetMeta-Dataset: Few-Shot Classification of Classes from SamplesPO-Meta-Dataset: Few-Shot Classification of Samples from ViewsPublished as a conference paper at ICLR 2023\n\n512. For fair comparison, we also add the same 3-layer MLP head onto the backbone for the baselines. Using a larger embedding for the baselines was not found to be beneficial. During training, gradients are backpropagated through the entire network such that both the randomly initialised heads and pre-trained backbones are learned/fine-tuned.\n\nWe use a representative selection of meta-learning baselines utilised by Meta-Dataset for our reimplementation. This includes a strong naive baseline (Finetuning, Nakamura & Harada (2019)), an embedding-based approach (Prototypical Network, Snell et al. (2017)) and an optimisation-based approach (MAML, Finn et al. (2017)), all modernised to use the ResNet-18 backbone as described above. Recent competitions, such as the NeurIPS 2021 MetaDL Challenge (Baz et al., 2022; 2021), have demonstrated that these fundamental approaches, updated to use modern pre-trained backbones that are finetuned on the meta-task (exactly as in our experiments below) are still generally stateof-the-art for novel datasets (Chen et al., 2021b), and so form strong baselines. In addition, our re-implementation enables us to ensure that all learners are optimised for Meta-Dataset and that comparisons between learners are fair, utilising the same benchmark parameters, model architectures and where applicable, hyperparameters. Crucially, given the close connection between POEM and Prototypical Networks, we ensure that all hyperparameters, including learning rates, scheduling and architectures are identical for both methods.\n\n4.2 RESULTS\n\nOur results on this novel representation learning benchmark, PO-Meta-Dataset, are given in table 1.\n\nTest Source Aircraft Birds Flowers Fungi Omniglot Textures\n\nFinetune 46.5 ± 0.6 62.6 ± 0.7 48.5 ± 0.4 61.0 ± 0.2 71.3 ± 0.1 83.2 ± 0.4\n\nProtoNet 48.5 ± 1.0 67.4 ± 1.2 46.4 ± 0.7 61.4 ± 0.4 87.8 ± 0.1 76.7 ± 1.6\n\nMAML 37.5 ± 0.3 52.5 ± 0.6 33.5 ± 0.3 46.1 ± 0.4 47.4 ± 1.0 73.1 ± 0.4\n\nPOEM 55.3 ± 0.7 71.1 ± 0.1 49.2 ± 1.5 64.8 ± 0.3 89.2 ± 0.7 81.4 ± 0.6\n\nTable 1: Few-shot classification accuracies on our adapted Meta-Dataset benchmark, PO-MetaDataset. All learners use a ResNet-18 model pre-trained on ImageNet, with MLP heads to incorporate view information. POEM outperforms the baselines across the range of datasets, demonstrating the benefits of the approach to learn and match representations from partial observations.\n\nThe results show that POEM outperforms the baselines at identifying views of images across a diverse range of datasets, demonstrating the benefits of the approach to learn and match representations from partial observations. The only exception is the Textures dataset, for which the finetuning baseline performs particularly strongly. We hypothesise that this is because the images in the Textures dataset are relatively uniform compared to the other datasets, so capturing the relative location of views is less important than identifying very fine grained features that distinguish the samples, which optimisation-based approaches are particularly effective at.\n\n4.3 ABLATION: META-DATASET\n\nTo demonstrate that the observed benefit of POEM over the baselines is due to the requirement of the task to learn coherent representations from partial observations, we also evaluate our approach against the baselines on the established Meta-Dataset benchmark. We now follow the standard fewshot learning procedure as described in the paper (Triantafillou et al., 2020), but keep all learners identical to those used in the evaluation above.\n\nOur results on the standard Meta-Dataset benchmark are provided in table 2. As expected, we find that POEM performs comparably with the baselines. Although Meta-Dataset provides realistic few-shot learning tasks in terms of diversity of visual concepts, fine-grained classes and variable shots and ways, each sample generally contains complete information including all relevant features for the visual concept in question. Correctly classifying query samples does not generally require any unification of views from support examples, but simply the identification of common features. As a result, we see that the additional capacity of POEM to learn to weight support examples and\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTest Source Aircraft Birds Flowers Fungi Omniglot Textures\n\nFinetune 56.2 ± 1.1 52.6 ± 1.8 80.1 ± 2.0 33.6 ± 1.7 89.6 ± 3.3 60.4 ± 1.0\n\nProtoNet 47.2 ± 1.2 78.3 ± 0.5 84.2 ± 0.7 84.7 ± 0.2 98.7 ± 0.1 65.3 ± 1.2\n\nMAML 35.9 ± 1.8 65.2 ± 0.3 70.4 ± 0.4 18.9 ± 0.2 94.7 ± 0.1 56.1 ± 0.3\n\nPOEM 46.5 ± 1.5 79.4 ± 0.3 83.6 ± 1.3 81.0 ± 0.1 98.6 ± 0.1 65.7 ± 0.8\n\nTable 2: Few-shot classification accuracies on Meta-Dataset, all using a ResNet-18 backbone pretrained on ImageNet, with a 3 layer MLP head. POEM is comparable with the baselines.\n\ncombine partial features does not provide a significant performance improvement over the baselines at few-shot classification in this fully observable benchmark.\n\nIn support of our hypothesis that feature uncertainty is not useful on this benchmark, we find that the variance in the precisions relative to the means output by the POEM model generally decreases during training and becomes negligible for all datasets, indicating that the precisions are not being utilised to improve performance and that the POEM objective is reducing to the Prototypical Network objective, as discussed in section 3.2. This is further evidenced by the very similar performances of POEM and the Prototypical Network across the entire set of datasets. However, on PO-Meta-Dataset, we find that the relative variance in the precisions to the means is much larger on convergence, which leads to the improved performance of POEM over the Prototypical Network observed in Table 1. This is shown in appendix A.6.\n\n5 DEMONSTRATION OF LEARNING REPRESENTATIONS OF ENVIRONMENTS\n\nWe now apply POEM to the equivalent task of learning a representation of an environment from the partial observations collected by an agent exploring that environment.\n\nTo do so, we utilise the 2D gridworld environment, MiniGrid (Chevalier-Boisvert et al., 2018). We consider the 11 × 11 Simple Crossing environment, which consists of a procedurally generated maze where the agent is required to traverse from the top left corner to the goal in the bottom right corner. The MiniGrid environment provides an agent-centric viewpoint at each step in the trajectory, consisting of a 7 × 7 window of the environment in front of the agent, taking into account the agent’s current direction and where the line of sight is blocked by walls.\n\n5.1 META-LEARNING ENVIRONMENT REPRESENTATIONS VIA FEW-SHOT CLASSIFICATION\n\nTo generate few-shot episodes, we utilise two agents: an optimal agent that takes the optimal trajectory from the start to the goal, and an exploratory agent that is incentivised to explore all possible views in the environment. The support set for each environment is generated by running the optimal agent in the environment and collecting the partial observations of this agent at each step during its trajectory. The query set is similarly generated by running the exploratory agent in the environment, filtering out any observations that are contained within the support set, and then randomly sampling the desired number of queries from the remaining observations.\n\nWe generate these few-shot episodes dynamically, and train POEM to combine the support samples (partial observations from the optimal trajectory) into a representation of the environment, such that it can classify which environment a novel query observation has been collected from. A set of sample environments and observations from those environments are shown in figures 3 and 4.\n\nFigure 3: Sample environments.\n\nFigure 4: Sample queries labelled with targets corresponding to the environment which they were observed in.\n\n8\n\n1234567891039577Published as a conference paper at ICLR 2023\n\nAll observations are provided as pixels to a standard convolutional backbone, with the corresponding agent location and direction appended to this representation and passed through an MLP head, equivalent to the procedure utilised for the adapted Meta-Dataset experiments. As a baseline comparison, we also train a Prototypical Network with an identical architecture on this task. Additionally, we train an equivalent recurrent network architecture typically applied to POMDP tasks such as this (Hausknecht & Stone, 2017), by adding a GRU layer (Cho et al., 2014; Chung et al., 2014) where the hidden state of the GRU is updated at each timestep and then extracted as the unified representation of the agent. We find that POEM trains more quickly and reaches almost 10% higher final environment recognition performance than both the Prototypical Network and GRU-based approach over 100 test episodes (81.1% vs 72.4% and 72.1%), as shown in appendix A.8. This is a result of POEM’s capacity to associate each observation with only part of the representation.\n\n5.2 RECONSTRUCTING ENVIRONMENTS FROM PARTIAL OBSERVATION TRAJECTORIES\n\nHaving learned an environment encoder using the few-shot learning procedure above, we now investigate the extent to which our representations can be used to reconstruct the environment. As above, we generate trajectories with the optimal agent and feed these through the encoder to generate a representation of the environment. An MLP decoder is then trained to reconstruct the original environment layout from the learned environment representation. The decoder attempts to predict a one-hot representation of each possible grid cell, with a mean squared error loss. Given the trained encoder and decoder, we are now able to generate a map of the environment the optimal agent has traversed, solely from the agent’s partial observations, and without ever having seen the environment as a whole. A sample of environments alongside their reconstructions are shown in figure 5.\n\nFigure 5: Left: Ground truth environments explored by the agent. Right: Reconstructions of the corresponding environments from POEM’s unified representation, encoded from the partial observations of the agent.\n\nWe see that the reconstructions clearly capture the approximate structure of each environment, demonstrating that the agent has been able to integrate its observations from along its trajectory into a single consistent representation. Since POEM enables the representation to be updated incrementally with each partial observation of the environment at inference time, it would be possible for an agent to update an internal environment representation at each step in its trajectory. There is potential for utilising this approach for learning environment representations to be beneficial in the context of exploration for reinforcement learning, but we leave such an investigation to future work.\n\n6 CONCLUSION\n\nIn this work, we have introduced Partial Observation Experts Modelling (POEM), a contrastive meta-learning approach for few-shot learning in partially-observable settings. Unlike other standard contrastive and embedding-based meta-learning approaches, POEM utilises representational uncertainty to enable observations to inform only part of a representation vector. This probabilistic formalism enables consistent representation learning from multiple observations with a few-shot learning objective. We have demonstrated that POEM is comparable to the state-of-the-art baselines on a comprehensive few-shot learning benchmark, and outperforms these baselines when this benchmark is adapted to evaluate representation learning from partial observations. We have also demonstrated a promising potential application for POEM to learn representations of an environment from an agent’s partial observations. We hope that this research inspires further work into the challenging task of learning representations under partial observability and the creation of more realistic partial observability benchmarks.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nAdam Jelley was kindly supported by Microsoft Research and EPSRC through Microsoft’s PhD Scholarship Programme. Antreas Antoniou was supported by a Huawei DDMPLab Innovation Research Grant. The Meta-Dataset experiments in this work were partly funded by Google Research Compute Credits, and we thank Hugo Larochelle for his support in acquiring these compute credits.\n\nREPRODUCIBILITY STATEMENT\n\nAll results reported in this paper are reproducible. The implementation code is available at: https://github.com/AdamJelley/POEM.\n\nETHICS STATEMENT\n\nThis work considers the problem of learning representations from partial observations. The authors are not aware of any specific ethical concerns associated with improving the quality of learned representations from partial observations, other than those already associated with any work in the field of machine learning.\n\nREFERENCES\n\nAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. October 2018.\n\ndoi: 10.48550/arXiv.1810.09502. URL https://arxiv.org/abs/1810.09502v3.\n\nAdrian El Baz, Isabelle Guyon, Zhengying Liu, Jan N. van Rijn, Sebastien Treguer, and Joaquin Vanschoren. Advances in MetaDL: AAAI 2021 Challenge and Workshop. In AAAI Workshop on Meta-Learning and MetaDL Challenge, pp. 1–16. PMLR, August 2021. URL https:// proceedings.mlr.press/v140/el-baz21a.html. ISSN: 2640-3498.\n\nAdrian El Baz, Ihsan Ullah, Edesio Alcobac ̧a, Andr ́e C. P. L. F. Carvalho, Hong Chen, Fabio Ferreira, Henry Gouk, Chaoyu Guan, Isabelle Guyon, Timothy Hospedales, Shell Hu, Mike Huisman, Frank Hutter, Zhengying Liu, Felix Mohr, Ekrem ̈Ozt ̈urk, Jan N. van Rijn, Haozhe Sun, Xin Wang, and Wenwu Zhu. Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning dominates for few-shot learning image classification, July 2022. URL http://arxiv.org/abs/2206.08138. arXiv:2206.08138 [cs].\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. arXiv:2002.05709 [cs, stat], June 2020. URL http://arxiv.org/abs/2002.05709. arXiv: 2002.05709.\n\nWentao Chen, Chenyang Si, Wei Wang, Liang Wang, Zilei Wang, and Tieniu Tan. Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images, May 2021a. URL http://arxiv.org/abs/2105.11874. arXiv:2105.11874 [cs].\n\nYudong Chen, Chaoyu Guan, Zhikun Wei, Xin Wang, and Wenwu Zhu. MetaDelta: A MetaLearning System for Few-shot Image Classification, February 2021b. URL http://arxiv. org/abs/2102.10744. arXiv:2102.10744 [cs].\n\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment\n\nfor OpenAI gym, 2018. URL https://github.com/maximecb/gym-minigrid.\n\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, October 2014. URL http:// arxiv.org/abs/1409.1259. arXiv:1409.1259 [cs, stat].\n\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, December 2014. URL http: //arxiv.org/abs/1412.3555. arXiv:1412.3555 [cs].\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. DeIn Proceedings of the IEEE conference on computer vision and\n\nscribing textures in the wild. pattern recognition, pp. 3606–3613, 2014.\n\nChuanqi Dong, Wenbin Li, Jing Huo, Zheng Gu, and Yang Gao. Learning Task-aware LoIn Proceedings of the Twenty-Ninth International cal Representations for Few-shot Learning. Joint Conference on Artificial Intelligence, pp. 716–722, Yokohama, Japan, July 2020. International Joint Conferences on Artificial Intelligence Organization. ISBN 978-0-9992411-6-5. doi: 10.24963/ijcai.2020/100. URL https://www.ijcai.org/proceedings/2020/100.\n\nVincent Dumoulin, Neil Houlsby, Utku Evci, Xiaohua Zhai, Ross Goroshin, Sylvain Gelly, and Hugo Larochelle. Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot Classification Benchmark. Technical Report arXiv:2104.02638, arXiv, April 2021. URL http: //arxiv.org/abs/2104.02638. arXiv:2104.02638 [cs] type: article.\n\nHarrison Edwards and Amos Storkey. Towards a Neural Statistician. arXiv:1606.02185 [cs, stat],\n\nMarch 2017. URL http://arxiv.org/abs/1606.02185. arXiv: 1606.02185.\n\nLinus Ericsson, Henry Gouk, Chen Change Loy, and Timothy M. Hospedales. Self-Supervised Representation Learning: Introduction, Advances and Challenges. arXiv:2110.09327 [cs, stat], October 2021. URL http://arxiv.org/abs/2110.09327. arXiv: 2110.09327.\n\nRaul Fernandez-Fernandez, Juan G. Victores, David Estevez, and Carlos Balaguer. Quick, Stat!: A Statistical Analysis of the Quick, Draw! Dataset. In EUROSIM 2019 Abstract Volume, 2019. doi: 10.11128/arep.58. URL http://arxiv.org/abs/1907.06417. arXiv:1907.06417 [cs, eess].\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adapta-\n\ntion of Deep Networks. Technical report, 2017.\n\nAndreas F ̈urst, Elisabeth Rumetshofer, Johannes Lehner, Viet Tran, Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, G ̈unter Klambauer, Angela Bitto-Nemling, and Sepp Hochreiter. CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP, November 2022. URL http://arxiv.org/abs/2110.11316. arXiv:2110.11316 [cs].\n\nJonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E. Turner. MetaLearning Probabilistic Inference For Prediction, August 2019. URL http://arxiv.org/ abs/1805.09921. arXiv:1805.09921 [cs, stat].\n\nMatthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs. Technical Report arXiv:1507.06527, arXiv, January 2017. URL http://arxiv.org/abs/ 1507.06527. arXiv:1507.06527 [cs] type: article.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/ 1512.03385. arXiv: 1512.03385.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http: //arxiv.org/abs/1911.05722. arXiv: 1911.05722.\n\nMarkus Hiller, Rongkai Ma, Mehrtash Harandi, and Tom Drummond. Rethinking Generalization in Few-Shot Classification, October 2022. URL http://arxiv.org/abs/2206.07267. arXiv:2206.07267 [cs].\n\nGeoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural\n\ncomputation, 14(8):1771–1800, 2002. Publisher: MIT Press.\n\nTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-Learning in Neural Networks: A Survey. April 2020. doi: 10.48550/arXiv.2004.05439. URL https://arxiv. org/abs/2004.05439v2.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science (New York, N.Y.), 350(6266):1332–1338, 2015. Publisher: American Association for the Advancement of Science.\n\nPhuc H. Le-Khac, Graham Healy, and Alan F. Smeaton. Contrastive Representation Learning: A Framework and Review. ISSN 2169-3536. doi: 10.1109/ACCESS.2020.3031549. URL http://arxiv.org/abs/2010.05113. arXiv: 2010.05113.\n\nIEEE Access, 8:193907–193934, 2020.\n\nZhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-\n\nshot learning. arXiv preprint arXiv:1707.09835, 2017.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll ́ar. Microsoft COCO: Common Objects in Context, February 2015. URL http://arxiv.org/abs/1405.0312. arXiv:1405.0312 [cs].\n\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-Grained Visual Classification of Aircraft, June 2013. URL http://arxiv.org/abs/1306.5151. arXiv:1306.5151 [cs].\n\nAkihiro Nakamura and Tatsuya Harada. Revisiting Fine-tuning for Few-shot Learning, October\n\n2019. URL http://arxiv.org/abs/1910.00216. arXiv:1910.00216 [cs, stat].\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian conference on computer vision, graphics and image processing, December 2008.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. arXiv:1807.03748 [cs, stat], January 2019. URL http://arxiv.org/abs/ 1807.03748. arXiv: 1807.03748.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ̈opf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, December 2019. URL http://arxiv.org/abs/1912.01703. arXiv:1912.01703 [cs, stat].\n\nMassimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O’Boyle, and Amos Storkey. Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels, October 2020. URL http: //arxiv.org/abs/1910.05199. arXiv:1910.05199 [cs, stat].\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, February 2021. URL http://arxiv.org/abs/2103.00020. arXiv:2103.00020 [cs].\n\nRoweis, Sam. Gaussian Identities, 1999. URL https://cs.nyu.edu/ ̃roweis/notes/\n\ngaussid.pdf.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li FeiFei. ImageNet Large Scale Visual Recognition Challenge. arXiv:1409.0575 [cs], January 2015. URL http://arxiv.org/abs/1409.0575. arXiv: 1409.0575.\n\nAndrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-Learning with Latent Embedding Optimization, March 2019. URL http://arxiv.org/abs/1807.05960. arXiv:1807.05960 [cs, stat].\n\nSchroeder, Brigit. FGVC5 - Fungi, 2018. URL https://sites.google.com/view/\n\nfgvc5/competitions/fgvcx/fungi.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs], August 2017. URL http://arxiv.org/ abs/1707.06347. arXiv: 1707.06347.\n\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical Networks for Few-shot Learning. arXiv:1703.05175 [cs, stat], June 2017. URL http://arxiv.org/abs/1703.05175. arXiv: 1703.05175.\n\nJohannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German traffic sign recognition benchmark: a multi-class classification competition. In The 2011 international joint conference on neural networks, pp. 1453–1460, 2011. tex.organization: IEEE.\n\nZhuo Sun, Jijie Wu, Xiaoxu Li, Wenming Yang, and Jing-Hao Xue. Amortized Bayesian Prototype Meta-learning: A New Probabilistic Meta-learning Approach to Few-shot Image Classification. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pp. 1414–1422. PMLR, March 2021. URL https://proceedings.mlr.press/v130/ sun21a.html. ISSN: 2640-3498.\n\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales. Learning to Compare: Relation Network for Few-Shot Learning. Technical Report arXiv:1711.06025, arXiv, March 2018. URL http://arxiv.org/abs/1711.06025. arXiv:1711.06025 [cs] version: 2 type: article.\n\nYichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton. Deep Mixtures of Factor Analysers,\n\nJune 2012. URL http://arxiv.org/abs/1206.4635. arXiv:1206.4635 [cs, stat].\n\nEleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. MetaDataset: A Dataset of Datasets for Learning to Learn from Few Examples. Technical Report arXiv:1903.03096, arXiv, April 2020. URL http://arxiv.org/abs/1903.03096. arXiv:1903.03096 [cs, stat] type: article.\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html.\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\n\nbirds-200-2011 dataset. 2011. Publisher: California Institute of Technology.\n\nChristopher Williams, Felix Agakov, and Stephen Felderhof. Products of gaussians. In T. Dietterich, S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing Systems, volume 14. MIT Press, 2001. URL https://proceedings.neurips.cc/paper/2001/ file/8232e119d8f59aa83050a741631803a6-Paper.pdf.\n\nChi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. DeepEMD: Differentiable Earth Mover’s Distance for Few-Shot Learning, January 2022. URL http://arxiv.org/abs/2003. 06777. arXiv:2003.06777 [cs, eess].\n\nLuisa M. Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast Context Adaptation via Meta-Learning. arXiv:1810.03642 [cs, stat], June 2019. URL http: //arxiv.org/abs/1810.03642. arXiv: 1810.03642.\n\nA APPENDIX\n\nA.1 GAUSSIAN PRODUCT RULES\n\nAssuming the latent variable model φ(z|x) to be a diagonal covariance multivariate Gaussian, the resulting integrals over latent variables become integrals over Gaussian products. This allows both λ(y, X) (Equation 5) and λ′(X) (Equation 6) in the marginal predictive distribution (Equation 4) to\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nbe evaluated analytically using the following univariate Gaussian product rules on each independent dimension (Roweis, Sam, 1999). Since a product of Gaussians (cid:81) SN (μ, τ −1), where\n\n) is itself a Gaussian, we have (cid:81)\n\ni N (μi, τ −1\n\ni N (μi, τ −1\n\n) =\n\ni\n\ni\n\nτ =\n\n(cid:88)\n\nτi\n\nμ =\n\ni 1\nτ\n\n(cid:88)\n\nτiμi\n\ni\n\nS = (2π)\n\n(1−n) 2\n\n(cid:81)\n\ni τ 1/2 i\nτ 1/2\n\n(cid:32)\n\n1 2\n\nexp\n\nτ μ2 −\n\n(cid:33)\n\nτiμ2\n\ni\n\n.\n\n1 2\n\n(cid:88)\n\ni\n\nTherefore the integral of a Gaussian product is given by the resulting normalisation S.\n\n(8)\n\n(9)\n\n(10)\n\nIn the case of evaluating the marginal predictive distribution p(x∗|Xm) (equation 4), this gives S′S = S∗ S∗S S′ where S is the normalisation constant of the support product, S∗ is the normalisation constant of the product of the query and normalised support product, and S′ is the normalisation constant of the product of the prior p(z) and normalised support product. In reality, p(z) generally has little impact as it is typically flat (τ → 0) over the region of non-negligible density of the product (cid:81)V λ′ in the objective can be approximated by S∗, as in the simplified pseduocode in appendix A.3.\n\nv=1 φ(z|xv) and so S′ ≈ 1 and we find S∗\n\nS′ ≈ S∗ so the ratios λ\n\nA.2 DERIVATION OF OBJECTIVE FROM MARGINAL PREDICTIVE DISTRIBUTION\n\nIn section 3, we derived the marginal predictive distribution:\n\np(x∗|Xm) = p(x∗)\n\nλ(x∗, Xm) λ′(Xm)\n\nwhere\n\n(cid:90)\n\n(cid:90)\n\nλ(y, X) =\n\nλ′(X) =\n\ndz φ(z|y)\n\ndim(X) (cid:89)\n\nv=1\n\nφ(z|xv),\n\nand\n\ndz p(z)\n\ndim(X) (cid:89)\n\nv=1\n\nφ(z|xv).\n\n(11)\n\n(12)\n\n(13)\n\nIn our few shot task, the support data is chosen and then the query view is chosen uniformly at random to match the views of one of the support data items. Let the hypothesis Hm indicate the event that the query view x∗ comes from support point m. Then\n\nP (Hm|S, x∗) =\n\n=\n\nP (Hm)P (S, x∗|Hm) m′ P (Hm′)P (S, x∗|Hm′) (1/M )p(x∗|Xm) m′(1/M )p(x∗|Xm′)\n\n=\n\n(cid:80)\n\n(cid:80)\n\np(x∗|Xm) m′ p(x∗|Xm′)\n\n.\n\n(cid:80)\n\n=\n\n(1/M )p(x∗|S, Hm) m′(1/M )p(x∗|S, Hm′)\n\n(cid:80)\n\nFrom this we can formulate the training task: we wish to maximize the likelihood for the correct match of query point to support set, accumulated across all support/query selections from the dataset. Denote the tth support set by St, the tth query point by x∗ t , and let mt denote the support point with views that match the view of the query point. Then the complete negative log marginal likelihood objective to be minimized is:\n\nL({St}, {x∗\n\nt }) = −\n\n= −\n\n= −\n\n(cid:88)\n\nt\n\n(cid:88)\n\nt\n\n(cid:88)\n\nt\n\nlog\n\n(cid:34)\n\nlog\n\nlog P (Hmt|St, x∗ t )\n\np(x∗|Xm) m′ p(x∗|Xm′)\n\n(cid:80)\n\nλ(x∗, Xm∗ λ′(Xm∗ )\n\n)\n\n− log\n\nλ(x∗, Xm) λ′(Xm)\n\n(cid:88)\n\nm\n\n(cid:35)\n\n14\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\nPublished as a conference paper at ICLR 2023\n\nA.3 PSEUDOCODE\n\nAlgorithm 1 Pytorch-Style Pseudocode: Gaussian Partial Observation Experts Modelling\n\n# phi: dual-headed encoder network with shared backbone and output heads for mean and\n\nprecision of Gaussian embedding # M: Number of items/classes in task # V: Number of views of each item/class (in general can vary with m in range(M)) # N: Number of query views # D: Embedding dimension\n\n# Load augmented partial views with view information for (support_views, query_views, query_targets) in loader:\n\n# support_views.shape = (M, V, ...) # query_views.shape=(N, ...) # query_targets.shape = (N,)\n\n# Encode each support and query views support_means, support_precisions = phi(support_views) # (M, V, D) query_means, query_precisions = phi(query_views) # (N, D)\n\n# Combine support views into unified representation of each item # Gaussian products computed using equations in appendix A.1 # Optionally include prior Gaussian here (neglected for simplified implementation) environment_means, environment_precisions, log_environment_normalisation =\n\ninner_gaussian_product(support_means, support_precisions) # Outputs: (M, D)\n\n# Combine each query view with each unified support representation env_query_mean, env_query_precisions, log_env_query_normalisation =\n\nouter_gaussian_product(support_means, support_precisions, query_means, query_precisions) # Outputs: (N, M, D)\n\n# Predictions correspond to unified support with maximum overlap with query _, predictions = log_env_query_normalisation.sum(2).max(1) # (N,)\n\n# Cross entropy loss normalises with softmax and computes negative log-likelihood loss = F.cross_entropy(log_env_query_normalisation, query_targets, reduction=’mean’)\n\n# Optimization step loss.backwards() optimizer.step()\n\nAlgorithm 2 Language Agnostic Pseudocode: Gaussian Partial Observation Experts Modelling Require: Training meta-set Dtrain ∈ T Require: Learning rate α\n\n▷ Heads correspond to mean μ and precision τ of Gaussian embedding z\n\nSample task instance Ti = (X, x∗) ∼ Dtrain\n\n1: Initialise dual-headed network φθ(z|x) 2: 3: while not converged do 4: 5: 6: 7: 8: 9:\n\nEncode each view in support set X into Gaussian z using φ(z|X) Encode each query view in x∗ into Gaussian z∗ using φ(z∗|x∗) for m ∈ {1, ..., M } do\n\n▷ Support set X consists of V m views of item m ∈ {1, ..., M }. ▷ Query set x∗ consists of N queries, each one view from any one item.\n\n10: 11: 12:\n\n13: 14: 15: 16:\n\nCompute Gaussian product over views (cid:81)V m\n\nv=1 φ(z|xm ▷ This gives unified support representation (global environment representation)\n\nv ) (using results in A.1)\n\nfor n ∈ {1, ..., N } do\n\nend for\n\nCompute Gaussian product of query with support product φ(z∗\n\nn|x∗\n\nn) (cid:81)V m\n\nv=1 φ(z|xm v )\n\nend for Normalise resulting query-support normalisation constants Sm\n\nn = Sm for correct support as loss L({Dt}, {x∗\n\n(cid:80)\n\nn\n\nn\n\nm Sm t }) (eq. 7)\n\nacross items\n\nCompute negative log of Sm∗\n\nn\n\n▷ Negative log likelihood for correct support\n\nPerform gradient step w.r.t. θ: θ ← φ − α∇θL({Dt}, {x∗\n\nt })\n\n17: 18: 19: 20: end while\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA.4 EQUIVALENCE OF PROTOTYPICAL NETWORK OBJECTIVE TO POEM OBJECTIVE WITH\n\nFIXED PRECISIONS\n\nThe probability of a query x∗ belonging to class n using the POEM objective is given by:\n\nP (Hm|S, x∗) =\n\nλ(x∗; Xn) λ′(Xn)\n\n(cid:30) (cid:88)\n\nm\n\nλ(x∗; Xm) λ′(Xm)\n\nas defined in equation 15, where\n\n(cid:90)\n\n(cid:90)\n\nλ(y, X) =\n\nλ′(X) =\n\ndz φ(z|y)\n\nV (cid:89)\n\nv=1\n\nφ(z|xv),\n\nand\n\ndz p(z)\n\nV (cid:89)\n\nv=1\n\nφ(z|xv).\n\n(19)\n\n(20)\n\n(21)\n\nTaking the precisions of the all Gaussian factors φ in λ and λ′ to be 1, we can apply the Gaussian product rules given in appendix A.1 to calculate λ and λ′ analytically. We find that this gives:\n\npn =\n\n(cid:18)\n\nVn Vn+1\n\n1\n\n2 exp\n\n− Vn\n\n2(Vn+1)\n\n(cid:16)\n\nμ −\n\n(cid:80)\n\ni μni Vn\n\n(cid:17)2(cid:19)\n\n(cid:80)\n\nm\n\nVm Vm+1\n\n1\n\n2 exp\n\n(cid:18)\n\n− Vm\n\n2(Vm+1)\n\n(cid:16)\n\nμ −\n\n(cid:80)\n\ni μmi Vm\n\n(cid:17)2(cid:19)\n\n(22)\n\nwhere μ is the representation mean of the query, and μni is the representation mean of support sample i for class n, and Vn is the number of support samples for class n.\n\nEquivalently, the probability of a query with representation vector μ belonging to a class n using a Prototypical Network objective is given by:\n\npn =\n\n(cid:18)\n\nexp\n\n−\n\n(cid:16)\n\nμ −\n\n(cid:80)\n\ni μni Vn\n\n(cid:17)2(cid:19)\n\n(cid:80)\n\nm exp\n\n(cid:18)\n\n(cid:16)\n\n−\n\nμ −\n\n(cid:80)\n\ni μmi Vm\n\n(cid:17)2(cid:19)\n\n(23)\n\n(2)(Vm+1) which only have a (sigWe find that these are equivalent aside from the scaling factors nificant) effect when there are varying numbers of samples by class, and a greater effect when the number of samples is smaller. Experimentally, we find that these scaling factors make little difference, as demonstrated in table 2 of section 4.3.\n\nVm\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA.5 PARTIALLY OBSERVABLE META-DATASET BENCHMARK ADDITIONAL DETAILS\n\nParameters used for adapted PO-Meta-Dataset are provided in Table A.5. All parameters not listed chosen to match Meta-Dataset defaults. All augmentations are applied using Torchvision, with parameters specified.\n\nTable 3: Partially Observable Meta-Dataset Parameters\n\nPARAMETER\n\nVALUE\n\nClasses per Task Samples per Task Support Views per Sample Query Views per Sample Image Size Crop Size Color Jitter Random Greyscale Random Horizontal Flip Gaussian Blur\n\n[1, 5] [5, 25] 18 2\n(84, 84) (except Omniglot, (28, 28)) (14, 14) (1/6 in each dim, except Omniglot, random placement) (0.8, 0.8, 0.8, 0.2), p(apply) = 0.3 0.2 0.5 ((3, 3), (1, 0, 2.0)), p(apply) = 0.2\n\nAll results computed over three runs. The Finetuning, Prototypical Network and POEM baselines were run on on-premise RTX2080 GPUs. MAML required more memory and compute than available, so was run on cloud A100s.\n\nA.6 RELATIVE VARIANCE OF PRECISIONS DURING TRAINING ON META-DATASET AND\n\nPARTIALLY OBSERVABLE-META-DATASET\n\nThe plot below shows the evolution of the variance in the representation precisions relative to the variance in the representation means learned by POEM on two distinct datasets, Aircraft and VGG Flowers. We see that for standard few-shot learning on Meta-Dataset, the variance in precisions is negligible relative to the variance in the means, demonstrating that the representational uncertainty is not useful in this task. Meanwhile, we see the variance in the precisions relative to the variance in the means becoming large before converging to a value of O(100) on the PO-Meta-Dataset task, demonstrating that learning relative precisions is useful in this setting since each support sample only informs part of the representation.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nA.7 LEARNING REPRESENTATIONS OF ENVIRONMENTS ADDITIONAL DETAILS\n\nAdditional details about the parameters used for learning environment representations from agent observations are provided in Table 4\n\nTable 4: Environment Representation Learning Parameters\n\nPARAMETER\n\nVALUE\n\nAgent Training Algorithm Optimal Agent Reward Exploratory Agent Reward Encoder Conv Backbone Layers Encoder MLP Head Layers Encoder Embedding Dim Decoder MLP Layers\n\nPPO (Schulman et al., 2017) (default hyperparameters) 1 for reaching goal, -0.01 per timestep 1/N count exploration bonus (state defined by agent location and direction) 5\n3 128 (corresponding ∼ 11 × 11 environment size) 4\n\nA.8 ENVIRONMENT RECOGNITION ACCURACY DURING TRAINING\n\nPOEM trains more quickly on the environment recognition task and reaches a higher final performance than an equivalent Prototypical Network or Recurrent Network (GRU) (81.1% vs 72.4% and 72.1% ) over a subsequent 100 test episodes.\n\n18\n\n010002000300040005000Episodes0.10.20.30.40.50.60.70.8AccuracyEnvironment Recognition AccuracyPOEMPrototypical NetworkRecurrent Network (GRU)",
    "reference": "# Summary Of The Paper\n\nThis paper proposes POEM, a generalization of prototypical networks designed to handle few-shot learning when each support or query example is only a partial view/observation of the underlying concept. The derivation follows from modeling the joint distribution of support representations as the product of conditionally independent Gaussians. Prediction for a query example is done via Bayes's rule and marginalizing over the representation. For their experiments, the authors adapt a subset of Meta-Dataset so that each sample exhibits substantial partial observability. POEM is shown to improve upon baselines that do not model the partial observability. The authors also compare POEM and prototypical networks on a map reconstruction task based on the MiniGrid environment, finding that POEM enables better reconstructions faster.\n\n# Strength And Weaknesses\n\nStrengths\n\nThe work studies partial observability, a realistic property of high-dimensional embodied observations, in the context of few-shot learning. The proposed method is reasonable and generalizes a standard method from prior work. Steps are taken to ensure fair empirical comparisons. The authors also contribute a new benchmark based on adapting an existing dataset, and experiment with an embodied toy mapping task.\n\nWeaknesses\n\nThe baseline algorithms used in the PO-Meta-Dataset are rather weak, and it would be interesting to see to what extent more expressive models such as latent embedding optimization [A] are able to meta-learn strategies to handle the partial observability in comparison to the more explicit modeling done in POEM. Some relevant references also seem to be missing; it would also be good to discuss POEM in relation to amortized Bayesian prototype meta-learning [B], which also considers a probabilistic model of semi-parametric few-shot learning for classification. The authors also don't discuss whether the RoamingX datasets proposed in [C] could be suitable for evaluation, given the close relationship between embodiment and partial observability (as evidenced by the MiniGrid experiment).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity\n\nIt would be good to provide the concrete instantiation of POEM in algorithm box form, i.e. provide the missing link between the mathematical statement of the objective (Eqs. 8 and 9) and the reduced form afforded by simplifying assumptions, e.g. Gaussianity of each expert. \n\nI'm also not sure that emphasizing the \"contrastive\" aspect of the work (e.g. in the title) is particularly important. You could say that few-shot learning has always been contrastive in the sense that different samples of the same class are \"augmentations\" of one another. The method doesn't seem to have particularly strong connections to standard contrastive learning objectives.\n\nQuality\n\nThere are several minor weaknesses as detailed above. Otherwise, the technical contribution seems correct and the empirical evaluation sound.\n\nNovelty\n\nThere is some technical novelty in the proposed method, and some empirical novelty in the new dataset.\n\nReproducibility\n\nCode is promised upon publication. As mentioned above, it would be good to have a concise description of the objective as pseudocode to aid in long-term reproducibility.\n\n# Summary Of The Review\n\nOverall, the work is interesting and of good quality, but is currently mired by a few individually minor weaknesses as detailed above. As such, I currently recommend borderline rejection.\n\nReferences\n\n[A] Rusu et al., Meta-Learning with Latent Embedding Optimization, ICLR 2019.\n\n[B] Sun et al., Amortized Bayesian Prototype Meta-learning: A New Probabilistic Meta-learning Approach to Few-shot Image Classification, AISTATS 2021.\n\n[C] Ren et al., Wandering Within a World: Online Contextualized Few-Shot Learning, ICLR 2021.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTHE INFLUENCE OF LEARNING RULE ON REPRESENTATION DYNAMICS IN WIDE NEURAL NETWORKS\n\nBlake Bordelon & Cengiz Pehlevan School of Engineering and Applied Science Harvard University Cambridge, MA 02138, USA {blake bordelon,cpehlevan}@g.harvard.edu\n\nABSTRACT\n\nIt is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologicallyplausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel’s evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy limit, we find that DFA and Hebb can only learn using the last layer features, while full FA can utilize earlier layers with a scale determined by the initial correlation between feedforward and feedback weight matrices. In the rich regime, DFA and FA utilize a temporally evolving and depthdependent NTK. Counterintuitively, we find that FA networks trained in the rich regime exhibit more feature learning if initialized with smaller correlation between the forward and backward pass weights. GLNs admit a very simple formula for their lazy limit kernel and preserve conditional Gaussianity of their preactivations under gating functions. Error modulated Hebb rules show very small task-relevant alignment of their kernels and perform most task relevant learning in the last layer.\n\n1\n\nINTRODUCTION\n\nDeep neural networks have now attained state of the art performance across a variety of domains including computer vision and natural language processing (Goodfellow et al., 2016; LeCun et al., 2015). Central to the power and transferability of neural networks is their ability to flexibly adapt their layer-wise internal representations to the structure of the data distribution during learning.\n\nIn this paper, we explore how the learning rule that is used to train a deep network affects its learning dynamics and representations. Our primary motivation for studying different rules is that exact gradient descent (GD) training with the back-propagation algorithm is thought to be biologically implausible (Crick, 1989). While many alternatives to standard GD training were proposed (Whittington & Bogacz, 2019), it is unclear how modifying the learning rule changes the functional inductive bias and the learned representations of the network. Further, understanding the learned representations could potentially offer more insight into which learning rules account for representational changes observed in the brain (Poort et al., 2015; Kriegeskorte & Wei, 2021; Schumacher et al., 2022). Our current study is a step towards these directions.\n\nThe alternative learning rules we study are error modulated Hebbian learning (Hebb), Feedback alignment (FA) (Lillicrap et al., 2016) and direct feedback alignment (DFA) (Nøkland, 2016). These rules circumvent one of the biologically implausible features of GD: the weights used in the backward pass computation of error signals must be dynamically identical to the weights used on the\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nforward pass, known as the weight transport problem. Instead, FA and DFA algorithms compute an approximate backward pass with independent weights that are frozen through training. Hebb rule only uses a global error signal. While these learning rules do not perform exact GD, they are still able to evolve their internal representations and eventually fit the training data. Further, experiments have shown that FA and DFA can scale to certain problems such as view-synthesis, recommendation systems, and small scale image problems (Launay et al., 2020), but they do not perform as well in convolutional architectures with more complex image datasets (Bartunov et al., 2018). However, significant improvements to FA can be achieved if the feedback-weights have partial correlation with the feedforward weights (Xiao et al., 2018; Moskovitz et al., 2018; Boopathy & Fiete, 2022).\n\nWe also study gated linear networks (GLNs), which use frozen gating functions for nonlinearity (Fiat et al., 2019). Variants of these networks have bio-plausible interpretations in terms of dendritic gates (Sezener et al., 2021). Fixed gating can mitigate catastrophic forgetting (Veness et al., 2021; Budden et al., 2020) and enable efficient transfer and multi-task learning Saxe et al. (2022).\n\nHere, we explore how the choice of learning rule modifies the representations, functional biases and dynamics of deep networks at the infinite width limit, which allows a precise analytical description of the network dynamics in terms of a collection of evolving kernels. At infinite width, the network can operate in the lazy regime, where the feature embeddings at each layer are constant through time, or the rich/feature-learning regime (Chizat et al., 2019; Yang & Hu, 2021; Bordelon & Pehlevan, 2022). The richness is controlled by a scalar parameter related to the initial scale of the output function.\n\nIn summary, our novel contributions are the following:\n\n1. We identify a class of learning rules for which function evolution is described by a dynamical effective Neural Tangent Kernel (eNTK). We provide a dynamical mean field theory (DMFT) for these learning rules which can be used to compute this eNTK. We show both theoretically and empirically that convergence to this DMFT occurs at large width N with error O(N −1/2).\n\n2. We characterize precisely the inductive biases of infinite width networks in the lazy limit by computing their eNTKs at initialization. We generalize FA to allow partial correlation between the feedback weights and initial feedforward weights and show how this alters the eNTK.\n\n3. We then study the rich regime so that the features are allowed to adapt during training. In this regime, the eNTK is dynamical and we give a DMFT to compute it. For deep linear networks, the DMFT equations close algebraically, while for nonlinear networks we provide a numerical procedure to solve them.\n\n4. We compare the learned features and dynamics among these rules, analyzing the effect of richness, initial feedback correlation, and depth. We find that rich training enhances gradientpseudogradient alignment for both FA and DFA. Counterintuitively, smaller initial feedback correlation generates more dramatic feature evolution for FA. The GLN networks have dynamics comparable to GD, while Hebb networks, as expected, do not exhibit task relevant adaptation of feature kernels, but rather evolve according to the input statistics.\n\n1.1 RELATED WORKS\n\nGLNs were introduced by Fiat et al. (2019) as a simplified model of ReLU networks, allowing the analysis of convergence and generalization in the lazy kernel limit. Veness et al. (2021) provided a simplified and biologically-plausible learning rule for deep GLNs which was extended by Budden et al. (2020) and provided an interpretation in terms of dendritic gating Sezener et al. (2021). These works demonstrated benefits to continual learning due to the fixed gating. Saxe et al. (2022) derived exact dynamical equations for a GLN with gates operating at each node and each edge of the network graph. Krishnamurthy et al. (2022) provided a theory of gating in recurrent networks.\n\nLillicrap et al. (2016) showed that, in a two layer linear network the forward weights will evolve to align to the frozen feedback weights under the FA dynamics, allowing convergence of the network to a loss minimizer. This result was extended to deep networks by Frenkel et al. (2019), who also introduced a variant of FA where only the direction of the target is used. Refinetti et al. (2021) studied DFA in a two-layer student-teacher online learning setup, showing that the network first undergoes an alignment phase before converging to one the degenerate global minima of the loss. They argued that FA’s worse performance in CNNs is due to the inability of the forward pass gradients to align under the block-Toeplitz connectivity strucuture that arises from enforced weight sharing (d’Ascoli et al., 2019). Garg & Vempala (2022) analyzed matrix factorization with FA,\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nproving that, when overparameterized, it converges to a minimizer under standard conditions, albeit more slowly than GD. Cao et al. (2020) analyzed the kernel and loss dynamics of linear networks trained with learning rules from a space that includes GD, contrastive Hebbian, and predictive coding rules, showing strong dependence of hierarchical representations on learning rule.\n\nRecent works have utilized DMFT techniques to analyze typical performance of algorithms trained on high-dimensional random data (Agoritsas et al., 2018; Mignacco et al., 2020; Celentano et al., 2021; Gerbelot et al., 2022). In the present work, we do not average over random datasets, but rather over initial random weights and treat data as an input to the theory. Wide NNs have been analyzed at infinite width in both lazy regimes with the NTK (Jacot et al., 2018; Lee et al., 2019) and rich feature learning regimes (Mei et al., 2018). In the feature learning limit, the evolution of kernel order parameters have been obtained with both Tensor Programs framework (Yang & Hu, 2021) and with DMFT (Bordelon & Pehlevan, 2022). Song et al. (2021) recently analyzed the lazy infinite width limit of two layer networks trained with FA and weight decay, finding that only one layer effectively contributes to the two-layer NTK. Boopathy & Fiete (2022) proposed alignment based learning rules for networks at large width in the lazy regime, which performs comparably to GD and outperform standard FA. Their Align-Ada rule corresponds to our ρ-FA with ρ = 1 in lazy large width networks.\n\n2 EFFECTIVE NEURAL TANGENT KERNEL FOR A LEARNING RULE\n\nWe denote the output of a neural network for input xμ ∈ RD as fμ. For concreteness, in the main text we will focus on scalar targets fμ ∈ R and MLP architectures. Other architectures such as multiclass outputs and CNN architectures with infinite channel count can also be analyzed as we show in the Appendix C. For the moment, we let the function be computed recursively from a collection of weight matrices θ = Vec{W 0, W 1, ..., wL} in terms of preactivation vectors hl\n\nμ ∈ RN where,\n\nfμ =\n\n1 γ0N\n\nwL · φ(hL\n\nμ ) , hl+1\n\nμ =\n\n1 √\n\nN\n\nW lφ(hl\n\nμ) , h1\n\nμ =\n\n1 √\n\nD\n\nW 0xμ\n\n(1)\n\nwhere nonlinearity φ is applied element-wise. The scalar parameter γ0 controls how rich the network training is: small γ0 corresponds to lazy learning while large γ0 generates large changes to the features (Chizat et al., 2019). For gated linear networks, we follow Fiat et al. (2019) and modify the μ) with a multiplicative gating function ̇φ(ml forward pass equations by replacing φ(hl μ where gating variables ml M lxμ are fixed through training with Mij ∼ N (0, 1). To minimize loss L = (cid:80)\n\nμ l(fμ, yμ), we consider learning rules to the parameters θ of the form\n\nμ = 1√\n\nμ)hl\n\nD\n\nd dt\n\nwL = γ0\n\n(cid:88)\n\nμ\n\nφ(hL\n\nμ (t))∆μ ,\n\nd dt\n\nW l =\n\nγ0√ N\n\n(cid:88)\n\nμ\n\n∆μ ̃gl+1\n\nμ\n\nφ(hl\n\nμ)⊤ ,\n\nd dt\n\nW 0 =\n\nγ0√ D\n\n(cid:88)\n\nμ\n\n∆μ ̃g1\n\nμx⊤\n\nμ\n\nwhere the error signal is ∆μ(t) = − ∂L |fμ(t). The last layer weights wL are always updated with ∂fμ their true gradient. This corresponds to the biologically-plausible and local delta-rule, which merely correlates the error signals ∆μ and the last layer features φ(hL μ ) (Widrow & Hoff, 1960). In intermediate layers, the pseudo-gradient vectors ̃gl μ are determined by the choice of the learning rule. For concreteness, we provide below the recursive definitions of ̃gl for our five learning rules of interest.\n\n(2)\n\n ̃gl\n\nμ =\n\n\n\n \n\n ̇φ(hl\n\nμ) ⊙\n\n ̇φ(hl\n\nμ) ⊙\n\n(cid:104) 1√ (cid:20)\n\n1√\n\nN\n\nN\n\nμ) ⊙ ̃zl , ̃zl μ) ⊙\n\n ̇φ(hl ̇φ(ml ∆μ(t)φ(hl\n\n(cid:104) 1√\n\nμ(t))\n\nN\n\n(cid:105)\n\nW l(t)⊤ ̃gl+1 (cid:16)\n\nμ = ̇φ(hL ρW l(0) + (cid:112)1 − ρ2 ̃W l(cid:17)⊤\n\n, ̃gL\n\nμ\n\nμ ) ⊙ wL (cid:21)\n\n ̃gl+1\n\nGD\n\n, ̃W l\n\nij ∼ N (0, 1) ρ-FA\n\ni ∼ N (0, 1) W l(t)⊤ ̃gl+1\n\nμ\n\n(cid:105)\n\n, ̃gL = ̇φ(ml\n\nμ) ⊙ wL(t)\n\n(3)\n\nDFA\n\nGLN\n\nHebb\n\nWhile GD uses the instantaneous feedforward weights on the backward pass, ρ-FA uses the weight matrices which do not evolve throughout training. These weights have correlation ρ with the initial forward pass weights W l(0). This choice is motivated by the observation that partial correlation between forward and backward pass weights at initialization can improve training (Liao et al., 2016;\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nXiao et al., 2018; Moskovitz et al., 2018), though the cost is partial weight transport at initialization. However, we consider partial correlation at initialization more biologically plausible than the demanding weight transport at each step of training, like in GD. For DFA, the weight vectors ̃zl are sampled randomly at initialization and do not evolve in time. For GLN, the gating variables ml μ\nare frozen through time but the exact feedforward weights are used in the backward pass. Lastly, we modify the classic Hebb rule (Hebb, 1949) to get ∆W l ∝ (cid:80) μ)⊤, which weighs each example by its current error. Unlike standard Hebbian updates, this learning rule gives stable dynamics without regularization (App. G). For all rules, the evolution of the function is determined by a time-dependent eNTK Kμν which is defined as\n\nμ ∆μ(t)2φ(hl+1\n\nμ )φ(hl\n\n∂fμ ∂t\n\n=\n\n∂fμ ∂θ\n\n·\n\ndθ dt\n\n(cid:88)\n\n=\n\nν\n\n∆νKμν(t, t) , Kμν(t, s) =\n\nL (cid:88)\n\nl=0\n\n ̃Gl+1\n\nμν (t, s)Φl\n\nμν(t, s)\n\n ̃Gl\n\nμν(t, s) =\n\n1 N\n\ngl\n\nμ(t) · ̃gl\n\nν(s) , Φl\n\nμν(t, s) =\n\n1 N\n\nφ(hl\n\nμ(t)) · φ(hl\n\nν(s)),\n\n(4)\n\nμν (t, s) = 1 and Φ0\n\nD xμ · xν are time-invariant. The kernel ̃Gl where the base cases ̃GL+1 computes an inner product between the true gradient signals gl and the pseudo-gradient ν which is set by the chosen learning rule. We see that because ̃Gl is not necessarily symmetric, K ̃gl is also not necessarily symmetric. The matrix ̃Gl quantifies pseudo-gradient / gradient alignment.\n\nμ = γ0N ∂fμ\n\nμν(t, s) = 1\n\n∂hl μ\n\n3 DYNAMICAL MEAN FIELD THEORY FOR VARIOUS LEARNING RULES\n\nFor each of these learning rules considered, the infinite width N → ∞ limit of network learning can be described by a dynamical mean field theory (DMFT) (Bordelon & Pehlevan, 2022). At infinite width, the dynamics of the kernels Φl and ̃Gl become deterministic over random Gaussian initialization of parameters θ. The activity of neurons in each layer become i.i.d. random variables drawn from a distribution defined by these kernels, which themselves are averages over these singlesite distributions. Below, we provide DMFT formulas which are valid for all of our learning rules\n\nhl\n\nμ(t) = ul\n\nμ(t) + γ0\n\nμ(t) = rl zl\n\nμ(t) + γ0\n\n(cid:90) t\n\n0\n\n(cid:90) t\n\n0\n\nds\n\nds\n\nP (cid:88)\n\nν=1\n\nP (cid:88)\n\nν=1\n\n(cid:2)Al−1\n\nμν (t, s)gl\n\nν(s) + C l−1\n\nμν (t, s) ̃gl\n\nν(s) + Φl−1\n\nμν (t, s)∆ν(s) ̃gl\n\nν(s)(cid:3)\n\n(cid:104)\n\nBl\n\nμν(t, s) + ̃Gl+1\n\n(cid:105) μν (t, s)∆ν(s)\n\nφ(hl\n\nν(s)), gl\n\nμ(t) = ̇φ(hl\n\nμ(t))zl\n\nμ(t)\n\n{ul\n\nμ(t)} ∼ GP(0, Φl−1), Φl\n\nμν(t, s) = (cid:10)φ(hl\n\nμ(t))φ(hl\n\nν(s))(cid:11) , Al\n\nμν(t, s) = γ−1\n\n0\n\n{rl\n\nμ(t)} ∼ GP(0, Gl+1), ̃Gl\n\nμν(t, s) = (cid:10)gl\n\nμ(t) ̃gl\n\nν(s)(cid:11) , Bl\n\nμν(t, s) = γ−1\n\n0\n\n(cid:28)\n\n(cid:28) δ δrl\n\nν(s)\n\n(cid:29)\n\nφ(hl\n\nμ(t))\n\nδ δul+1\n\nν\n\n(s)\n\ngl+1\n\nμ (t)\n\n(cid:29)\n\n(5)\n\nThe definitions of ̃gl pre-gradient field defined so that gl on data comes from the base case Φ0\n\nμ(t) depend on the learning rule and are described in Table 1. The zl\n\nμ(t) is the μ(t). The dependence of these DMFT equations\n\nμ(t) = ̇φ(hl\n\nμ(t))zl\n\nμν(t, s) = 1\n\nD xμ · xν and error signal ∆μ = − ∂L\n\n∂fμ\n\n.\n\nRule ̃gl μ(t)\n\nGD μ(t))zl\n\nμ(t)\n\n ̇φ(hl\n\nρ-FA μ(t)) ̃zl\n\nμ(t)\n\n ̇φ(hl\n\nDFA μ(t)) ̃zl\n\n ̇φ(hl\n\nGLN μ)zl\n\nHebb μ(t) ∆μ(t)φ(hl\n\n ̇φ(ml\n\nμ(t))\n\nμ(t) + γ0\n\nTable 1: The field definitions for each learning rule. For ρ-FA, the field has definition ̃zl μ(t) + (cid:112)1 − ρ2 ̃ζ l ρvl with (cid:10)rl ν(s)(cid:11) = ̃Gl+1 μ(t)vl (cid:68) ̃ζ l (cid:69) = (cid:10) ̃gl+1 μ(t) ̃ζ l ν(s) For GLN, we use {ml\n\nμ(t) = μ(t)} are Gaussian μν (t, s). The ̃ζ l field is an independent Gaussian with correlation μν (t, s). For DFA, the ̃zl field is static ̃zl ∼ N (0, 1).\n\nμ (t) ̃gl+1 μ} ∼ N (0, Kx) as a gating variable. C l = 0 except for ρ-FA with ρ > 0.\n\nν(s)) where {vl\n\n(s)(cid:11) = ̃ ̃Gl+1\n\nμν(t, s)φ(hl\n\n(cid:82) t 0 ds (cid:80)\n\nμ(t), ̃ζ l\n\nν Dl\n\nν\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nWe see that, for {GD, ρ-FA, DFA, Hebb} the distribution of hl μ(t) are Gaussian throughout training only in the lazy γ0 → 0 limit for general nonlinear activation functions φ(h). However, μ}, the {hl, zl} fields are all Gaussian for GLNs. For all algorithms except ρ-FA, conditional on {ml μ(t))\n\nC l = 0. For ρ-FA we have C l\n\nμα(t, s) = γ−1\n\nμ(t), zl\n\n(cid:68) δ\n\n(cid:69) .\n\n0\n\nν (s) φ(hl\n\nδvl\n\n(a) Loss Dynamics\n\n(b) NTK-Target Alignment\n\n(c) ̃G Dynamics\n\n(d) Final h Distributions\n\n(e) Final Φ Kernels\n\n(f) Final ̃G kernels\n\nFigure 1: The DMFT predicts feature dynamics of wide networks trained with gradient descent (GD), feedback alignment (FA) with ρ = 0, gated linear network (GLN), and a error-modulated β = 1 Hebb rule (Hebb) in the feature learning regime. (a) The loss dynamics in a two layer (L = 1, N = 2000) network trained with these learning rules at richness γ0 = 2. The network is trained on a collection of P = 10 random vectors in D = 50 dimensions. (b) The cosine similarity of the eNTK with the targets A(K, yy⊤) = y⊤Ky |K|F |y|2 reveals increasing alignment for all algorithms. Though FA starts with the lowest alignment, its final NTK task alignment exceeds that of GD. (c) The dynamics of the gradient-pseudogradient kernel ̃G also reveals increasing correlation of g with ̃g. FA starts with ̃G = 0 but ̃G increases to non-zero value. (d) The distribution of hidden layer preactivations after training reveals non-Gaussian statistics for both GD and FA, but approximately Gaussian statistics for GLN. (e)-(f) The final Φ and ̃G kernels from theory and experiment.\n\nμν(t, s), ̃Gl\n\nAs described in prior results on the GD case (Bordelon & Pehlevan, 2022), the above equations can be solved self-consistently in polynomial (in train-set size P and training steps T ) time. With an estimate of the dynamical kernels {Φl μν(t, s)}, one computes the eNTK Kμν(t) and error dynamics ∆μ(t). From these objects, we can sample the stochastic processes {hl, zl, ̃zl} which can then be used to derive new refined estimates of the kernels. This procedure is repeated until convergence. This algorithm can be found in App. A. An example of such a solution is provided in Figure 1 for two layer ReLU networks trained with GD, FA, GLN, and Hebb. We show that our self-consistent DMFT accurately predicts training and kernel dynamics, as well as the density of preactivations {hμ(t)} and final kernels {Φμν, ̃Gμν} for each learning rule. We observe substantial differences in the learned representations (Figure 1e), all predicted by our DMFT.\n\nμν(t, s), Gl\n\n3.1 LAZY OR EARLY TIME STATIC-KERNEL LIMITS When γ0 → 0, we see that the fields hl μ(0) and rl μ(0). In this limit, the eNTK Kμν remains static and has the form summarized in Table 2 in terms of the initial feature kernels Φl and gradient kernels Gl. We derive these kernels in Appendix D.\n\nμ(t) are equal to the Gaussian variables ul\n\nμ(t) and zl\n\nThe feature P × P matrices Φl, Gl in Table 2 are computed recursively as (cid:68) ̇φ(u) ̇φ(u)⊤(cid:69)\n\nΦl = (cid:10)φ(u)φ(u)⊤(cid:11)\n\nu∼N (0,Φl−1) , Gl = Gl+1 ⊙\n\nu∼N (0,Φl−1)\n\n(6)\n\n5\n\n0100200300400500t0.00.20.40.60.81.0tGDFAGLNHebbDMFT0100200300400500t0.20.30.40.5A(K,yy)0100200300400500t01234Tr G42024h102101100p(h) NN Expt.BPFAGLNHebb DMFTG NN Expt.BPFAGLNHebbG DMFTPublished as a conference paper at ICLR 2023\n\n(a) ReLU FA varying ρ\n\n(b) ReLU FA varying L\n\n(c) ReLU GLN varying L\n\n(d) Φl convergence\n\n(e) Gl convergence\n\n(f) eNTK convergence\n\nFigure 2: The lazy infinite width limits of the various learning rules can be fully summarized with their initial eNTK. (a) The kernels of ρ-aligned ReLU FA and ReLU GLN for inputs separated by angle θ. (a) The kernels for varying ρ in ρ-aligned FA. Larger ρ has a sharper peak in the kernel around θ = 0. The ρ → 0 limit recovers the NNGP kernel ΦL while the ρ → 1 limit gives the backprop NTK. (b) Deeper networks with partial alignment ρ = 0.5. (c) ReLU-GLN kernel sharpens with depth. (d)-(e) The relative error of the infinite width Φl, Gl kernels in a width N ReLU neural network. The late layer Φl and early layer Gl kernels have highest errors since finite size effects accumulate on forward and backward passes respectively. (f) Finite width corrections to eNTK are larger for small ρ and large depth L. All square errors go as |KN − K∞|2 ∼ ON (1/N ).\n\nRule\n\nKμν\n\nGD\n\nρ-FA\n\nDFA\n\n(cid:80)L\n\nl=0 Gl+1\n\nμν Φl\n\nμν\n\n(cid:80)L\n\nl=0 ρL−lGl+1\n\nμν Φl\n\nμν ΦL\n\nμν\n\nGLN\n\nHebb\n\n(cid:104)(cid:68) ̇φ(mμ) ̇φ(mν)\n\n(cid:69)(cid:105)L\n\nK x μν\n\nΦL μν\n\nTable 2: The initial eNTK Kμν for each learning rule. The GD kernel is the usual initial NTK of Jacot et al. (2018). For ρ-aligned FA, each layer l’s contribution to the eNTK is suppressed by a factor ρL−l. For DFA and Hebb, only the last layer feature kernel ΦL contributes to the NTK. For GLN, each layer has an identical contribution.\n\nwith base cases Φ0 = Kx and GL+1 = 11⊤. We provide interpretations of this result below.\n\nμν Φl\n\nl Gl+1\n\n• Backpropagation (GD) and ρ = 1 FA recover the usual depth L NTK, with contributions from μν at initialization. This kernel governs both training dynamics and\n\nevery layer Kμν = (cid:80) test predictions in the lazy limit γ0 → 0 (Jacot et al., 2018; Lou et al., 2022). • ρ = 0 FA, DFA and Hebb are equivalent to using the NNGP kernel Kμν ∼ ΦL\n\nμν, giving the Bayes posterior mean (Matthews et al., 2018; Lee et al., 2018; Hron et al., 2020). In the γ0, ρ → 0 limit, only the dynamics of the readout weights wL contribute to the evolution of fμ since error signals cannot successfully propagate backward and gradients cannot align with pseudo-gradients (App D). The standard ρ = 0 FA will be indistinguishable from merely training wL with the delta-rule unless the network is trained in the rich feature learning regime γ0 > 0, where ̃Gl can evolve. This effect was also noted in two layer networks by Song et al. (2021).\n\n• ρ-FA weighs each layer l with scale ρL−l, since each layer’s pseudo-gradient is only partially correlated with the true gradient, giving recursion ̃Gl = ρ ̃Gl+1 with base case ̃GL+1 = GL+1. μ} ∼ N (0, Kx).\n\n• GLN’s kernel in lazy limit is determined by the Gaussian gating variables {ml\n\nWe visualize these kernels for deep ReLU networks and ReLU GLNs for normalized inputs |x|2 = |x′|2 = D, by plotting the kernel as a function of the angle θ separating two inputs cos(θ) =\n\n6\n\n32101230.20.40.60.81.0K()FA L=4=0.00=0.25=0.50=1.0032101230.000.250.500.751.00K()FA =0.5L=1L=2L=3L=432101230.000.250.500.751.00K()GLNL=1L=2L=3L=4101102103N103102101100|N|2/||2N11234101102103N103102101100101|GNG|2/|G|2N1G1G2G3G4101102103N103102101100101|KNK|2/|K|2L=2,=0.0L=2,=1.0L=4L=8Published as a conference paper at ICLR 2023\n\n1\n\nD x⊤x′. We find that the kernels develop a sharp discontinuity at the origin θ = 0, which becomes more exaggerated as ρ and L increase. We further show that the square difference of width N kenels and infinite width kernels go as O(N −1). We derive this scaling with a perturbative argument in App. H, which enables analytical prediction of leading order finite size effects (Figure 7). In the lazy γ0 → 0 limit, these kernels define the eNTK and the network prediction dynamics.\n\n3.2 FEATURE LEARNING ENABLES GRADIENT/PSEUDO-GRADIENT ALIGNMENT AND\n\nKERNEL/TASK ALIGNMENT\n\nμ(t), zl\n\nμ(t) and pseudo-gradients ̃gl\n\nIn the last section, we saw that, in the γ0 → 0 limit, all algorithms have frozen preactivations and pregradient features {hl μ(t)}. A consequence of this fact is that FA and DFA cannot increase their gradient-pseudogradient alignment throughout training in the lazy limit γ0 = 0. However, if we increase γ0, then the gradient features gl μ(t) evolve in time and can increase their alignment. In Figure 3, we show the effect of increasing γ0 on alignment dynamics in a depth 4 tanh network trained with DFA. In (b), we see that larger γ0 is associated with high taskalignment of the last layer feature kernel ΦL, which becomes essentially rank one and aligned to yy⊤. The asympotic cosine similarity between gradients and pseudogradients also increase with γ0. The eNTK also becomes aligned with the task relevant directions (shown in Figure 3 c), like has been observed in GD training (Baratin et al., 2021; Shan & Bordelon, 2021; Geiger et al., 2021; Atanasov et al., 2022). We see that width N networks have a dynamical eNTK KN (t) which deviates from the DMFT eNTK K∞(t) by O(1/N ) in square loss. DMFT is more predictive for larger γ0 networks, suggesting a reduction in finite size variability due to task-relevant feature evolution.\n\n(a) DFA Train Loss\n\n(b) ΦL Alignment\n\n(c) gl, ̃gl Correlation\n\n(d) Final NTK Aligns to Task\n\n(e) Dynamical NTK Convergence\n\nFigure 3: Feature Learning enables alignment for a depth 4 (L = 3 hidden layers) tanh network trained with direct feedback alignment (DFA) with varying γ0. (a) Training loss for DFA networks with width N = 4000 with varying richness γ0 shows that feature learning accelerates training, as predicted by DMFT (black). (b) The alignment (cosine similarity) of the last layer kernel ΦL with the target function reveals successful task depedent feature learning at large γ0. (c) The dynamics of pseudo-grad./grad. correlation corr(g, ̃g) = 1 μ(t)| averaged over layers l and datapoints μ. Larger γ0 generates more significant alignment between pseudogradients (d) The final NTKs as a function of γ0 reveals increasing clustering of the data and gradients. points by class. (e) The error of the DMFT approximation for K’s dynamics as a function of N : ⟨|KN (t)−K∞(t)|2⟩t ⟨|K∞(t)|2⟩t\n\n∼ O(N −1), where the averages are computed over the time interval of training.\n\ngl μ(t)· ̃gl |gl μ(t)|| ̃gl\n\nμ(t)\n\n(cid:80)\n\nl,μ\n\nLP\n\nThis error is smaller for larger feature learning strength γ0.\n\n7\n\n050100150200250t0.00.20.40.60.81.0t0=0.20=0.50=1.00=2.0050100150200250t0.40.60.81.0A(L,yy)050100150200250t0.00.10.20.30.4Corr(g,g)K(T) Expt0=0.2K(T) DMFT0=0.50=1.00=2.0101102103N103102101100|KNK|2/|K|20=0.20=0.50=1.00=2.0N1Published as a conference paper at ICLR 2023\n\n3.3 DEEP LINEAR NETWORK KERNEL DYNAMICS\n\nWhen γ0 > 0 the kernels and features in the network evolve according to the DMFT equations. For deep linear networks we can analyze the equations for the kernels in closed form without sampling since the correlation functions close algebraically (App. E). In Figure 4, we utilize our algebraic DMFT equations to explore ρ-FA dynamics in a depth 4 linear network. Networks with larger ρ train faster, which can be intuited by noting that the initial function time derivative dt |t=0 ∼ (cid:80)L df is an increasing function of ρ. We observe higher final gradient pseudogradient alignment in each layer with larger ρ, which is also intuitive from the initial condition ̃Gl(0) = ρL−l. However, surprisingly, for large initial correlation ρ, the NTK achieves lower task alignment, despite having larger ̃Gl(t). We show that this is caused by smaller overlap of each layer’s feature kernel H l(t) with yy⊤. Though this phenomenon is counterintuitive, we gain more insight in the next section by studying an even simpler two layer model.\n\nl=0 ρL−l ∼ 1−ρL+1\n\n1−ρ\n\n(a) ρ-Aligned Loss Dynamics\n\n(b) Gradient-Pseudogradient Kernel Dynamics\n\n(c) NTK-Task Alignment\n\n(d) Feature Kernel Task Overlap\n\nFigure 4: The initial feedback correlation ρ alters alignment dynamics in on the FA dynamics in a depth 4 (L = 3 hidden layer) linear network. (a) Larger ρ leads to faster initial training since the scale of the eNTK is larger. (b) Further, larger ρ leads to larger scales of ̃G(t) = 1 N gl(t) · ̃gl(t). (c) However, smaller ρ leads to more alignment of the NTK K(t) with the task-relevant subspace, measured with cosine similarity A(K, yy⊤). (d) The feature kernel H(t) overlaps with y reveal that H l(t) aligns more significantly in the small ρ networks.\n\n3.3.1 EXACTLY SOLVEABLE DYNAMICS IN TWO LAYER LINEAR NETWORK We can provide exact solutions to the infinite width GD and ρ-FA dynamics in the setting of Saxe et al. (2013), specifically a two layer linear network trained with whitened data K x μν = δμν. Unlike Saxe et al. (2013)’s result, however, we do not demand small initialization scale (or equivalently large γ0), but rather provide the exact solution for all positive γ0. We will establish that large initial correlation ρ results in higher gradient/pseudogradient alignment but lower alignment of the hidden feature kernel H(t) with the task relevant subspace yy⊤.\n\nWe first note that when Kx = I, the GD or FA hidden feature kernel H(t) only evolves in the rank-one yy⊤ subspace. It thus suffices to track the projection of H(t) on this rank one subspace, which we call Hy(t). In the App. F we derive dynamics for Hy for GD and ρ-FA\n\nHy(t) =\n\n(cid:40) ̃G(t) = (cid:112)1 + γ2\n\n0 (y − ∆(t))2 , d∆\n\ndt = −(cid:112)1 + γ2\n\n0 (y − ∆(t))2∆(t) GD\n\n2 ̃G(t) + 1 − 2ρ = 1 + a2 , da\n\ndt = γ0y − 1\n\n2 a3 − (1 + ρ)a\n\n(7)\n\nρ-FA\n\nWe illustrate these dynamics in Figure 5. The fixed points are Hy = (cid:112)1 + γ2 0 y2 for GD and for ρ-FA, Hy = 1 + a2 where a is the smallest positive root of 1 2 a3 + (1 + ρ)a = γ0y. For both GD and FA, we see that increasing γ0 results in larger asymptotic values for Hy and ̃G. For ρ-FA the fixed point of a’s dynamics is a strictly decreasing function of ρ since da dρ < 0, showing that the final\n\n8\n\n0100200300400t0.00.20.40.60.81.0Train Loss=0.00=0.25=0.50=0.75010020030040002468G(t)=1=0.00=0.25=0.50=0.750100200300400012345=201002003004000.00.51.01.52.02.5=30100200300400t0.20.30.40.5A(K,yy)=0.00=0.25=0.50=0.7501002003004000.60.81.01.21.41.6yH(t)y=1=0.00=0.25=0.50=0.7501002003004001234=20100200300400051015=3Published as a conference paper at ICLR 2023\n\nvalue of Hy is smaller for larger ρ. On the contrary, we have that the final ̃G = ρ + 1 2 a2 is a strictly increasing function of ρ since d 2 a2+(1+ρ) > 1 3 > 0. Thus, this simple model replicates the phenomenon of increasing ̃G and decreasing Hy as ρ increases. For the Hebb rule with Kx = I, the story is different. Instead of aligning H along the rank-one task relevant subpace, the dynamics instead decouple over samples, giving the following P separate equations\n\n ̃G = 1 −\n\na2\n\ndρ\n\n3\n\nd dt\n\n∆μ = −[Hμμ(t) + γ0∆μ(yμ − ∆μ)]∆μ(t) ,\n\nd dt\n\nHμμ = 2γ0∆μ(t)2Hμμ.\n\n(8)\n\nFrom this perspective, we see that the hidden feature kernel does not align to the task, but rather increases its entries in overall scale as is illustrated in Figure 5 (b).\n\n(a) Loss Dynamics\n\n(b) Kernel-Task Alignment\n\n(c) Feature Learning vs γ0\n\n(b) The alignment of the kernel to the target function Hy(t) = 1\n\nFigure 5: The feature kernel dynamics and scaling with γ2 0 for GD, ρ-FA, and Hebbian rules in an exactly solveable two layer linear network. (a) The loss dynamics for all algorithms reveals that ρ = 0 FA and Hebb rules have same early time dynamics and that ρ = 1 FA and GD have same early-time dynamics. However all loss curves become distinct at late times due to different eNTK |y|2 y⊤Hy/TrH(t) dynamics. increases significantly for GD, and FA, but not for Hebb, reflecting the task-independence of the learned representation. (c) The movement of the feature kernel ∆Hy = limt→∞ Hy(t) − Hy(0) as a function of γ0 for GD, and ρ = 0, 1 FA. At small feature learning strength, all algorithms have 0 . At large γ0, GD has ∆Hy ∼ γ0 while FA has ∆Hy ∼ γ2/3 updates on the order of ∆Hy ∼ γ2 .\nThe ρ = 1 FA (green) has lower ∆Hy than the ρ = 0 FA across all γ0.\n\n0\n\n4 DISCUSSION\n\nWe provided an analysis of the training dynamics of a wide range of learning rules at infinite width. This set of rules includes (but is not limited to) GD, ρ-FA, DFA, GLN and Hebb as well as many others. We showed that each of these learning rules has an dynamical effective NTK which concentrates over initializations at infinite width. In the lazy γ0 → 0 regime, it suffices to compute the the initial NTK, while in the rich regime, we provide a dynamical mean field theory to compute the NTK’s dynamics. We showed that, in the rich regime, FA learning rules do indeed align the network’s gradient vectors to their pseudo-gradients and that this alignment improves with γ0. We show that initial correlation ρ between forward and backward pass weights alters the inductive bias In the rich regime, larger ρ networks have smaller eNTK of FA in both lazy and rich regimes. evolution. Overall, our study is a step towards understanding learned representations in neural networks, and the quest to reverse-engineer learning rules from observations of evolving neural representations during learning in the brain.\n\nMany open problems remain unresolved with the present work. We currently have only implemented our theory in MLPs. An implementation in CNNs could explain some of the observed advantages of partial initial alignment in ρ-FA (Xiao et al., 2018; Moskovitz et al., 2018; Bartunov et al., 2018; Refinetti et al., 2021). In addition, our framework is sufficiently flexible to propose and test new learning rules by providing new ̃gl μ(t) formulas. Our DMFT gives a recipe to compute their initial kernels, function dynamics and analyze their learned representations. The generalization performance of these learning rules at varying γ0 is yet to be explored. Lastly, our DMFT is numerically expensive for large datasets and training intervals, making it difficult to scale up to realistic datsets. Future work could provide theoretical convergence guarantees for our DMFT solver.\n\n9\n\n02004006008001000t0.00.20.40.60.81.0tGDFA =0FA =1DMFTHebb02004006008001000t0.100.150.200.250.300.35Hy(t) / Tr H(t)1011001011020103102101100101102103Hy2/30200Published as a conference paper at ICLR 2023\n\nREFERENCES\n\nElisabeth Agoritsas, Giulio Biroli, Pierfrancesco Urbani, and Francesco Zamponi. Out-ofequilibrium dynamical mean-field equations for the perceptron model. Journal of Physics A: Mathematical and Theoretical, 51(8):085002, 2018.\n\nAlexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: In International Conference on Learning Representations, 2022.\n\nThe silent alignment effect. URL https://openreview.net/forum?id=1NvflqAdoom.\n\nAristide Baratin, Thomas George, C ́esar Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 2269–2277. PMLR, 13–15 Apr 2021. URL https://proceedings.mlr.press/v130/ baratin21a.html.\n\nSergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey E Hinton, and Timothy Lillicrap. Assessing the scalability of biologically-motivated deep learning algorithms and architectures. Advances in neural information processing systems, 31, 2018.\n\nCarl M Bender, Steven Orszag, and Steven A Orszag. Advanced mathematical methods for scientists and engineers I: Asymptotic methods and perturbation theory, volume 1. Springer Science & Business Media, 1999.\n\nAkhilan Boopathy and Ila Fiete. How to train your wide neural network without backprop: An input-weight alignment perspective. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2178–2205. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ boopathy22a.html.\n\nSelf-consistent dynamical field theory of kernel evoBlake Bordelon and Cengiz Pehlevan. lution in wide neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=sipwrPCrIS.\n\nDavid Budden, Adam Marblestone, Eren Sezener, Tor Lattimore, Gregory Wayne, and Joel Veness. Gaussian gated linear networks. Advances in Neural Information Processing Systems, 33:16508– 16519, 2020.\n\nYinan Cao, Christopher Summerfield, and Andrew Saxe. Characterizing emergent representations in a space of candidate learning rules for deep networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8660–8670. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/6275d7071d005260ab9d0766d6df1145-Paper.pdf.\n\nMichael Celentano, Chen Cheng, and Andrea Montanari. The high-dimensional asymptotics of first\n\norder methods with random data. arXiv preprint arXiv:2112.07572, 2021.\n\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n\nFrancis Crick. The recent excitement about neural networks. Nature, 337(6203):129–132, 1989.\n\nA Crisanti and H Sompolinsky. Path integral approach to random neural networks. Physical Review\n\nE, 98(6):062120, 2018.\n\nSt ́ephane d’Ascoli, Levent Sagun, Giulio Biroli, and Joan Bruna. Finding the needle in the haystack with convolutions: on the benefits of architectural bias. Advances in Neural Information Processing Systems, 32, 2019.\n\nJonathan Fiat, Eran Malach, and Shai Shalev-Shwartz. Decoupling gating from linearity. arXiv\n\npreprint arXiv:1906.05032, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nCharlotte Frenkel, Martin Lefebvre, and David Bol. Learning without feedback: direct random target projection as a feedback-alignment algorithm with layerwise feedforward training. arXiv preprint arXiv:1909.01311, 10, 2019.\n\nShivam Garg and Santosh Vempala. How and when random feedback works: A case study of low-rank matrix factorization. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera (eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 4070–4108. PMLR, 28–30 Mar 2022. URL https://proceedings.mlr.press/v151/garg22a.html.\n\nMario Geiger, Leonardo Petrini, and Matthieu Wyart. Landscape and training regimes in deep\n\nlearning. Physics Reports, 924:1–18, 2021.\n\nCedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zdeborova. Rigorous dynamical mean field theory for stochastic gradient descent methods, 2022. URL https://arxiv.org/abs/2210.06591.\n\nGabriel Goh. Why momentum really works. Distill, 2(4):e6, 2017.\n\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.\n\nMIT Press, 2016.\n\nDonald O. Hebb. The organization of behavior: A neuropsychological theory. Wiley, New York,\n\nJune 1949. ISBN 0-8058-4300-0.\n\nJiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, and Jascha Sohl-Dickstein. Exact posterior distributions of wide bayesian neural networks. arXiv preprint arXiv:2006.10541, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\nMehran Kardar. Statistical physics of fields. Cambridge University Press, 2007.\n\nNikolaus Kriegeskorte and Xue-Xin Wei. Neural tuning and representational geometry. Nature\n\nReviews Neuroscience, 22(11):703–718, 2021.\n\nKamesh Krishnamurthy, Tankut Can, and David J Schwab. Theory of gating in recurrent neural\n\nnetworks. Physical Review X, 12(1):011011, 2022.\n\nJulien Launay, Iacopo Poli, Franc ̧ois Boniface, and Florent Krzakala. Direct feedback alignment scales to modern deep learning tasks and architectures. Advances in neural information processing systems, 33:9346–9360, 2020.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,\n\n2015.\n\nJaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and In International ConferYasaman Bahri. Deep neural networks as gaussian processes. ence on Learning Representations, 2018. URL https://openreview.net/forum?id= B1EA-M-0Z.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems, 32, 2019.\n\nJaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. Advances in Neural Information Processing Systems, 33:15156–15172, 2020.\n\nQianli Liao, Joel Leibo, and Tomaso Poggio. How important is weight symmetry in backpropaga-\n\ntion? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.\n\nTimothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature communications, 7(1): 1–10, 2016.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYizhang Lou, Chris E Mingard, and Soufiane Hayou. Feature learning and signal propagation in deep neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 14248–14282. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lou22a.html.\n\nAlessandro Manacorda, Gr ́egory Schehr, and Francesco Zamponi. Numerical solution of the dynamical mean field theory of infinite-dimensional equilibrium liquids. The Journal of chemical physics, 152(16):164506, 2020.\n\nPaul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. Physical\n\nReview A, 8(1):423, 1973.\n\nAlexander G.D.G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=H1-nGgWC-.\n\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.\n\nFrancesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborov ́a. Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification. Advances in Neural Information Processing Systems, 33:9540–9550, 2020.\n\nTheodore H Moskovitz, Ashok Litwin-Kumar, and LF Abbott. Feedback alignment in deep convo-\n\nlutional networks. arXiv preprint arXiv:1812.06488, 2018.\n\nArild Nøkland. Direct feedback alignment provides learning in deep neural networks. Advances in\n\nneural information processing systems, 29, 2016.\n\nJasper Poort, Adil G Khan, Marius Pachitariu, Abdellatif Nemri, Ivana Orsolic, Julija Krupic, Marius Bauza, Maneesh Sahani, Georg B Keller, Thomas D Mrsic-Flogel, et al. Learning enhances sensory and multiple non-sensory representations in primary visual cortex. Neuron, 86(6):1478– 1490, 2015.\n\nMaria Refinetti, St ́ephane d’Ascoli, Ruben Ohana, and Sebastian Goldt. Align, then memorise: In International Conference on Machine\n\nthe dynamics of learning with feedback alignment. Learning, pp. 8925–8935. PMLR, 2021.\n\nAndrew Saxe, Shagun Sodhani, and Sam Jay Lewallen. The neural race reduction: Dynamics of In International Conference on Machine Learning, pp. 19287–\n\nabstraction in gated networks. 19309. PMLR, 2022.\n\nAndrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-\n\nics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.\n\nJoseph W Schumacher, Matthew K McCann, Katherine J Maximov, and David Fitzpatrick. Selective enhancement of neural coding in v1 underlies fine-discrimination learning in tree shrew. Current Biology, 32(15):3245–3260, 2022.\n\nEren Sezener, Agnieszka Grabska-Barwi ́nska, Dimitar Kostadinov, Maxime Beau, Sanjukta Krishnagopal, David Budden, Marcus Hutter, Joel Veness, Matthew Botvinick, Claudia Clopath, et al. A rapid and efficient learning rule for biological neural circuits. BioRxiv, 2021.\n\nHaozhe Shan and Blake Bordelon. A theory of neural tangent kernel alignment and its influence on\n\ntraining. arXiv e-prints, pp. arXiv–2105, 2021.\n\nGanlin Song, Ruitu Xu, and John Lafferty. Convergence and alignment of gradient descent with random backpropagation weights. Advances in Neural Information Processing Systems, 34:19888– 19898, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJoel Veness, Tor Lattimore, David Budden, Avishkar Bhoopchand, Christopher Mattern, Agnieszka Grabska-Barwinska, Eren Sezener, Jianan Wang, Peter Toth, Simon Schmitt, et al. Gated linear In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. networks. 10015–10023, 2021.\n\nJames CR Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends\n\nin cognitive sciences, 23(3):235–250, 2019.\n\nBernard Widrow and Marcian E Hoff. Adaptive switching circuits. Technical report, Stanford Univ\n\nCa Stanford Electronics Labs, 1960.\n\nWill Xiao, Honglin Chen, Qianli Liao, and Tomaso Poggio. Biologically-plausible learning algo-\n\nrithms can scale to large datasets. arXiv preprint arXiv:1811.03567, 2018.\n\nGreg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural netIn Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conworks. ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11727–11737. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/ v139/yang21c.html.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA ALGORITHM TO SOLVE NONLINEAR DMFT EQUATIONS\n\nAlgorithm 1: Alternating Monte-Carlo Solution to Saddle Point Equations Data: Kx, y, Initial Guesses {Φl, Gl, ̃Gl, ̃ ̃Gl}L\n\nl=1, {Al, Bl, Cl, Dl}L−1\n\nl=1 , Sample count S,\n\nUpdate Speed β\n\nResult: Network predictions through training fμ(t), correlation functions l=1, response functions {Al, Bl, Cl, Dl}L−1 l=1 ,\n\n{Φl, Gl, ̃Gl, ̃ ̃Gl}L\n\n1 Φ0 = Kx ⊗ 11⊤, GL+1 = 11⊤ ; 2 while Kernels Not Converged do\n\ndt fμ(t) = (cid:80)\n\nα ∆α(t)K N T K\n\nμα\n\n(t, t);\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\nFrom {Φl, Gl} compute KN T K(t, t) and solve d l = 1; while l < L + 1 do\n\nDraw S samples {ul\n\nμ,n(t)}S\n\nn=1 ∼ GP(0, Φl−1),\n\n{rl\n\nμ,n(t), vl\n\nμ,n(t)}S\n\nn=1 ∼ GP\n\n0,\n\n(cid:32)\n\n(cid:34)\n\n(cid:35)(cid:33) ;\n\n ̃Gl+1 Gl+1 ̃Gl+1⊤ ̃ ̃Gl+1 μ,n(t), zl μ,n(t)}S\n\nn=1;\n\nμν\n\n(cid:80)\n\n(cid:80)\n\n(cid:80)\n\nn=1;\n\nν,n(s)) ,\n\nμ,n(t)}S\n\nμ,n(t), ̃gl\n\nμ,n(t))φ(hl ν,n(s) , ν,n(s),\n\n(t, s) = 1 S\n(t, s) = 1 S\n(t, s) = 1 S\n(t, s) = 1 S\n\nn∈[S] φ(hl n∈[S] gl n∈[S] gl n∈[S] ̃gl\n\nSolve equation 5 for each sample to get {hl Use learning rule (Table 1) to compute { ̃gl Compute new correlation function {Φl, Gl, ̃Gl, ̃ ̃Gl} estimates: Φl,new μν Gl,new μν ̃Gl,new ̃ ̃Gl,new ν,n(s) ; Solve for Jacobians on each sample ∂φ(hl n) Compute new response functions {Al, Bl−1, Cl, Dl−1} estimates: Al,new = 1 S\nCl,new = 1 S\nl ← l + 1;\n\n, Bl−1,new = 1 S\n, Dl−1,new = 1 S\n\nμ,n(t)gl μ,n(t) ̃gl μ,n(t) ̃gl\n\n∂φ(hl ∂rl⊤ n\n∂φ(hl ∂vl⊤ n\n\n∂gl n\n∂ul⊤ ∂ ̃gl n\n∂ul⊤\n\n, ∂φ(hl n)\n\n, ∂ ̃gl\n\n, ∂gl\n\nn ∂ul⊤\n\nn ∂ul⊤\n\n∂vl⊤ n\n\n∂rl⊤ n\n\nn∈[S]\n\nn∈[S]\n\nn∈[S]\n\nn∈[S]\n\n(cid:80)\n\n(cid:80)\n\n(cid:80)\n\n(cid:80)\n\n(cid:80)\n\nn)\n\nn)\n\nμν\n\n;\n\n;\n\n;\n\nn\n\nn\n\nn\n\nn\n\nend l = 1; while l < L + 1 do\n\nUpdate correlation functions Φl ← (1 − β)Φl + βΦl,new, Gl ← (1 − β)Gl + βGl,new ; ̃ ̃Gl ← (1 − β) ̃ ̃Gl + β ̃ ̃Gl,new ; ̃Gl ← (1 − β) ̃Gl + β ̃Gl,new, if l < L then\n\nUpdate response functions Al ← (1 − β)Al + βAl,new, Bl ← (1 − β)Bl + βBl,new Cl ← (1 − β)Cl + βCl,new, Dl ← (1 − β)Dl + βDl,new\n\nend l ← l + 1\n\nend\n\n31 32 end 33 return {fμ(t)}P\n\nμ=1, {Φl, Gl, ̃Gl, ̃ ̃Gl}L\n\nl=1, {Al, Bl, Cl, Dl}L−1\n\nl=1\n\nThe sample-and-solve procedure we developed and describe below for nonlinear networks is based on numerical recipes used in the dynamical mean field simulations in computational physics Manacorda et al. (2020) and is similar to recent work in the GD case Bordelon & Pehlevan (2022). The basic principle is to leverage the fact that, conditional on order parameters, we can easily draw\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nμ(t), rl\n\nsamples {ul μ(t)} from their appropriate GPs. From these sampled fields, we can identify the kernel order parameters by simple estimation of the appropriate moments. The algorithm is provided in Algorithm 1.\n\nμ(t), ζ l\n\nμ(t), ̃ζ l\n\nThe parameter β controls recency weighting of the samples obtained at each iteration. If β = 1, then the rank of the kernel estimates is limited to the number of samples S used in a single iteration, but with β < 1 smaller sample sizes S can be used to still obtain accurate results. We used β = 0.6 in our deep network experiments.\n\nB DERIVATION OF DMFT EQUATIONS\n\nIn this section, we derive the DMFT description of infinite network dynamics. The path integral theory we develop is based on the Martin-Siggia-Rose-De Dominicis-Janssen (MSRDJ) framework Martin et al. (1973). A useful review of this technique applied to random recurent networks can be found here Crisanti & Sompolinsky (2018). This framework was recently extended for deep learning with GD in (Bordelon & Pehlevan, 2022).\n\nB.1 WRITING EVOLUTION EQUATIONS IN FEATURE SPACE\n\nN\n\nW l(t)φ(hl\n\nμ(t)), pre-gradient features zl\n\nFirst, we will express all of the learning dynamics in terms of preactivation features hl 1√\n\nμ(t) = μ(t) = 1√ W l(t)⊤gl+1 and pseudogradient features ̃gl μ(t). Since we would like to understand typical behavior over random initializations of weights θ(0) = {W 0(0), W 1(0), ..., wL(0)}, we want to isolate the dependence of our evolution equations by W l(0). We achieve this separation by using our learning dynamics for W l(t)\n\nN\n\nW l(t) = W l(0) +\n\nγ0√ N\n\n(cid:90) t\n\n0\n\nds\n\nP (cid:88)\n\nμ=1\n\n∆μ(s) ̃gl+1\n\nμ (s)φ(hl\n\nμ(s))⊤.\n\n(9)\n\nThe inclusion of the prefactor γ0√ dt hl = Oγ0,N (γ0) at initialization (Chizat et al., 2019; Bordelon & Pehlevan, 2022). Using the forward and backward pass equations, we find the following evolution equations for our feature vectors\n\nin the weight dynamics ensures that d\n\ndt f = Oγ0,N (1) and d\n\nN\n\nhl\n\nμ(t) = χl\n\nμ(t) + γ0\n\nzl\n\nμ(t) = ξl\n\nμ(t) + γ0\n\n0\n\n(cid:90) t\n\n0\n\n(cid:90) t\n\nds\n\nP (cid:88)\n\nν=1\n\n∆ν(s) ̃gl+1\n\nμ (s)Φl−1\n\nμν (t, s) , χl\n\nμ(t) =\n\nds\n\nP (cid:88)\n\nν=1\n\n∆ν(s)φ(hl\n\nμ(s)) ̃Gl+1\n\nμν (t, s) , ξl\n\nμ(t) =\n\n1 √\n\nN\n\nW l(0)φ(hl\n\nμ(t))\n\n1 √\n\nN\n\nW l(0)⊤gl+1\n\nμ (t) , (10)\n\nwhere we introduced the following feature and gradient/pseudo-gradient kernels\n\nΦl\n\nμν(t, s) =\n\n1 N\n\nφ(hl\n\nμ(t)) · φ(hl\n\nν(s)) , ̃Gl\n\nμν(t, s) =\n\n1 N\n\ngl\n\nμ(t) · ̃gl\n\nν(s).\n\n(11)\n\nfor all\n\nThe particular learning rule defines the definition of the pseudo-gradient ̃gl that, fields {hl {Φl, ̃Gl}. The additional fields have definitions\n\nμ(t). We note the i,μ(t)}μ∈[P ],t∈R+, conditional on the value of the kernels\n\nlearning rules considered, iμ(t), ζ l\n\ni,μ(t) is a function of\n\nthe pseudogradient ̃gl\n\ni,μ(t), ̃ζ l\n\niμ(t), ml\n\ni,μ(t), zl\n\nζl\n\nμ(t) =\n\n1 √\n\nN\n\nW l(0)⊤ ̃gl+1\n\nμ (t) , ̃ζl+1\n\nμ\n\n(t) =\n\n1 √\n\n ̃W l⊤ ̃gl+1\n\nμ (t)\n\nN μ(t) = ρ ̇φ(hl\n\n(12)\n\nand are specifically required for ρ-FA with ρ > 0 since ̃gl (cid:112)1 − ρ2 ̇φ(hl μ(t). The fields ml\n\nμ(t)) ⊙ ̃ζl\n\nμ = 1√\n\nM lxμ are required for GLNs.\n\nD\n\nμ(t)) ⊙ ζl\n\nμ(t) +\n\nμ(t), ξl\n\nAll of the necessary fields {hl μ(t)} are thus causal functions of the stochastic fields μ(t)} and the kernels {Φl, ̃Gl}. It thus suffices to characterize the dis- {χl tribution of these latter objects over random initialization of θ(0) in the N → ∞ limit, which we study in the next section.\n\nμ(t), ml\n\nμ(t), ̃ζl\n\nμ(t), zl\n\nμ(t), ̃gl\n\nμ, ζl\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB.2 MOMENT GENERATING FUNCTIONAL\n\nWe will now attempt to characterize the probability density of the random fields\n\nχl+1\n\nμ (t) =\n\nζl\n\nμ(t) =\n\n1 √\n\nN 1\n√\n\nN\n\nW l(0)φ(hl\n\nμ(t)) , ξl\n\nμ(t) =\n\nW l(0)⊤ ̃gl+1\n\nμ (t) , ̃ζl\n\nμ(t) =\n\n1 √\n\nN 1\n√\n\nN\n\nW l(0)⊤gl+1\n\nμ (t) , ml\n\nμ =\n\n1 √\n\nD\n\nM lxμ\n\n ̃W l⊤ ̃gl+1\n\nμ (t).\n\n(13)\n\nIt is readily apparent that the fields ml μ are independent of the others and have a Gaussian distribution over random Gaussian M l. These fields, therefore do not can be handled independently from the others, which are statistically coupled through the initial conditions. We will thus characterize the moment generating functional of the remaining fields {χl μ(t)} over random initial condition and random backward pass weights\n\nμ(t), ̃ζl\n\nμ(t), ζl\n\nμ(t), ξl\n\nZ[{jl\n\nμ(t), kl\n\nμ(t), nl\n\n= E\n\nθ(0),{ ̃W l} exp\n\nμ(t), pl (cid:32) P\n\nμ(t)}] (cid:90) ∞\n\n(cid:88)\n\nμ=1\n\n0\n\n(cid:104)\n\nμ(t) · χl jl\n\nμ(t) + kl\n\nμ(t) · ξl\n\nμ(t) + nl\n\nμ(t) · ζl\n\nμ(t) + pl\n\nμ(t) · ̃ζl\n\n(cid:105) μ(t)\n\n(cid:33)\n\ndt\n\nwhere χl, ξ, ζ, ̃ζ are regarded as functions of θ(0), { ̃W l}. Arbitrary moments of these random variables can be computed by differentiation of Z near zero source. For example, a two-point correlation function can be obtained as\n\n(14)\n\n(cid:68)\n\nχl\n\ni,μ(t)ζ l′\n\ni′,ν(s)\n\n(cid:69)\n\n= lim\n\nj,k,n,p→0\n\nδ i,μ(t)\n\nδjl\n\nδ δnl′ i′ν(s)\n\nZ[{jl\n\nμ(t), kl\n\nμ(t), nl\n\nμ(t), pl\n\nμ(t)}].\n\n(15)\n\nMore generally, we let μ = (i, μ, t) be a tuple containing the neuron, time, and sample index for an entry of one of these fields so that χl i,μ(t). Further, we let Nχl, Nξl, Nζl , N ̃ζl be index sets which contain sample and time indices as well as neuron indices Nχ = {μχ |Nχ|} for all of the indices we wish to compute an average over. Then arbitrary moments can be computed with the formula\n\n1 , ..., |μχ\n\nμ = χl\n\n(cid:42)\n\n(cid:89)\n\n\n\n\n\n(cid:89)\n\nχl\n\nμ\n\n(cid:89)\n\nξl\n\nν\n\n(cid:89)\n\nζ l\n\nα\n\n(cid:89)\n\n\n\n(cid:43)\n\n ̃ζ l\n\nβ\n\n\n\nl\n\nμ∈Nχl\n\nν∈Nξl\n\nα∈Nζl\n\nβ∈Nξl\n\n= lim\n\nj,k,n,p→0\n\n(cid:89)\n\n\n\n\n\n(cid:89)\n\nl\n\nμ∈Nχl\n\nδ δjl μ\n\n(cid:89)\n\nν∈Nξl\n\nδ δkl ν\n\n(cid:89)\n\nα∈Nζl\n\nδ δnl α\n\n(cid:89)\n\nβ∈Nξl\n\nδ δpl μ\n\n  Z[{jl\n\nμ(t), kl\n\nμ(t), nl\n\nμ(t), pl\n\nμ(t)}].\n\nWe now to study this moment generating functional Z in the large width N → ∞ limit.\n\nB.3 PATH INTEGRAL FORMULATION AND INTEGRATION OVER WEIGHTS\n\n(16)\n\nTo enable the average over the weights, we multiply Z by an integral representation of unity that enforces the relationship between χl+1\n\nμ (t), W l(0), φ(hl\n\nμ(t))\n\n(cid:90)\n\n1 =\n\nRN\n\ndχl+1\n\nμ (t) δ\n\n(cid:18)\n\nχl+1\n\nμ (t) −\n\n1 √\n\nN\n\nW l(0)φ(hl\n\nμ(t))\n\n(cid:19)\n\n(cid:90)\n\n(cid:90)\n\n=\n\nRN\n\nRN\n\ndχl+1\n\nμ (t)d ˆχl+1 (2π)N\n\nμ (t)\n\n(cid:18)\n\ni ˆχl+1\n\nμ (t) ·\n\nexp\n\n(cid:20)\n\nχl+1\n\nμ (t) −\n\nW l(0)φ(hl\n\nμ(t))\n\n(cid:21)(cid:19)\n\n.\n\n(17)\n\n1 √\n\nN\n\nIn the second line, we used the Fourier representation of the Dirac-Delta function for each of the N neuron indices δ(r) = (cid:82) ∞ 2π exp (iˆrr). We repeat this procedure for the other fields μ(t), ζl ξl μ(t) at each time t and each sample μ. After inserting these delta functions, we find\n\nμ(t), ̃ζl\n\n−∞\n\ndˆr\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nthe following form of the moment generating functional\n\nZ =\n\n(cid:90) (cid:89)\n\nlμt\n\ndχl\n\nμ(t)d ˆχl (2π)N\n\nμ(t)\n\ndξl\n\nμ(t)\n\ndζl\n\nμ(t)d ˆξl (2π)N\n\nμ(t)\n\nμ(t)d ˆζl (2π)N\n\nd ̃ζl\n\nˆ ̃ζl μ(t)d μ(t) (2π)N\n\n(cid:104)\n\nχl\n\nμ(t) · (jl\n\nμ(t) + i ˆχl\n\nμ(t)) + ξl\n\nμ(t) · (kl\n\nμ(t) + i ˆξl\n\nμ(t))\n\n(cid:105)\n\n\n\n\n\n(cid:104)\n\nμ(t) · (nl ζl\n\nμ(t) + i ˆζl\n\nμ(t)) + ̃ζl\n\nμ(t) · (pl\n\nμ(t) + i\n\n(cid:105)\n\n\n\n\n\nˆ ̃ζl μ(t))\n\n× exp\n\n× exp\n\n\n\n\n\n\n\n\n\n(cid:90) ∞\n\n0\n\n(cid:90) ∞\n\n0\n\n(cid:88)\n\ndt\n\nl,μ\n\n(cid:88)\n\ndt\n\nl,μ (cid:32)\n\n(cid:89)\n\n×\n\nl\n\nEW l(0) exp\n\n−\n\nTr W l(0)⊤\n\n(cid:34)(cid:90)\n\n(cid:88)\n\ndt\n\nμ\n\nˆχl+1\n\nμ (t)φ(hl\n\nμ(t))⊤ + gl+1\n\nμ (t) ˆξl\n\nμ(t)⊤\n\ni √\n\nN (cid:34)(cid:90)\n\n(cid:32)\n\n× exp\n\n−\n\ni √\n\nN\n\nW l(0)⊤\n\n(cid:88)\n\ndt\n\n ̃gl+1\n\nμ (t)ζl\n\nμ(t)⊤\n\n(cid:35)(cid:33)\n\n(cid:32)\n\nE ̃W l exp\n\n−\n\ni √\n\nN\n\nTr ̃W l⊤\n\n(cid:89)\n\n×\n\nl\n\nμ\n\n(cid:34)(cid:90)\n\n(cid:88)\n\ndt\n\n ̃gl+1\n\nμ (t)\n\nˆ ̃ζl μ(t)⊤\n\n(cid:35)(cid:33)\n\n.\n\nμ\n\n(cid:35)(cid:33)\n\n(18)\n\nμ aμbμ = (cid:82) ∞\n\nWe see that we often have simultaneous integrals over time t and sums over samples μ so we will again adopt a shorthand notation for indices μ = (μ, t) and define a summmation convention (cid:80) μ=1 aμ(t)bμ(t). To perform the averages over weights, we note that for a (cid:1). Using this fact for each standard normal variable Wij, that EWij exp (iWijaibj) = exp (cid:0)− 1 of the weight matrix averages, we have (cid:34)\n\n0 dt (cid:80)P\n\n2 a2\n\ni b2\n\n(cid:35)(cid:33)\n\n(cid:32)\n\ni\n\ni √\n\nN\n\nEW l(0) exp\n\n−\n\n\n\n= exp\n\n−\n\n(cid:32)\n\n= exp\n\n−\n\nTr W l(0)⊤\n\nˆχl+1\n\nμ φ(hl\n\nμ)⊤ + gl+1\n\nμ\n\nˆξl⊤ μ + ̃gl+1\n\nμ\n\nˆζl⊤\n\nμ\n\n(cid:88)\n\nμ\n\n1 2N\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nμ\n\nˆχl+1\n\nμ φ(hl\n\nμ)⊤ + gl+1\n\nμ\n\nˆξl⊤ μ + ̃gl+1\n\nμ\n\nˆζl⊤\n\nμ\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n2\n\n\n\nF\n\n(cid:104)\n\nˆχl+1\n\nμ · ˆχl+1\n\nν Φl\n\nμ,ν + ˆξl\n\nμ · ˆξl\n\nν Gl+1\n\nμν + ˆζl\n\nμ · ˆζl\n\nν\n\n1 2\n\n(cid:88)\n\nμ,ν\n\n(cid:32)\n\n× exp\n\n−i\n\n(cid:88)\n\nμν\n\n(cid:2) ˆχl+1\n\nμ · gl+1\n\nν Al\n\nμν + ˆχl+1\n\nμ · ̃gl+1\n\nν C l\n\nμν\n\n(cid:33)\n\n(cid:3)\n\n.\n\n ̃ ̃Gl+1\n\nμ,ν + ˆξl\n\nμ · ˆζl\n\nν\n\n(cid:33)\n\n(cid:105)\n\n ̃Gl+1\n\nμ,ν\n\n(19)\n\nIn the above, we introduced a collection of order parameters {Φ, G, ̃G, ̃ ̃G, A, C}, which will correspond to correlation and response functions of our DMFT. These are defined as\n\nΦl\n\nμ,ν =\n\n ̃ ̃Gl+1\n\nμ,ν =\n\n1 N\n1 N\n\nφ(hl\n\nμ) · φ(hl\n\nμ,ν =\n\nμ · gl\n\nν , ̃Gl\n\nμν =\n\n ̃gl\n\nμ · ̃gl\n\nν , iAl\n\nφ(hl\n\nν , iC l\n\nμν =\n\nν ) , Gl 1\nN\n\nμν =\n\ngl\n\n1 N\nμ) · ˆξl\n\n1 N\n\ngl\n\nμ · ̃gl\n\nν\n\nφ(hl\n\nμ) · ˆζl\n\nν .\n\n1 N\n\nWe perform a similar average over ̃W l can be obtained directly (cid:32)\n\n(cid:35)(cid:33)\n\n(cid:32)\n\n(cid:34)\n\nE ̃W l exp\n\n−\n\nTr ̃W l⊤\n\n(cid:88)\n\n ̃gl+1\n\nμ\n\nˆ ̃ζl⊤\n\nμ\n\n= exp\n\n−\n\ni √\n\nN\n\nμ\n\n1 2\n\n(cid:88)\n\nˆ ̃ζl μ ·\n\nˆ ̃ζl\n\nν\n\n ̃ ̃Gl+1\n\nμν\n\nμν\n\n(cid:33)\n\n.\n\n(20)\n\n(21)\n\nNow that we have defined our collection of order parameters, we enforce their definitions with Dirac-Delta functions by multiplying by one. For example,\n\n(cid:90)\n\n1 = N\n\ndΦl\n\nμν δ (cid:0)N Φl\n\nμν − φ(hl\n\nμ) · φ(hl\n\nν )(cid:1)\n\n=\n\n(cid:90) dΦl\n\nμν d ˆΦl 2πN −1\n\nμν\n\n(cid:16)\n\nexp\n\nN ˆΦl\n\nμν Φl\n\nμν − ˆΦl\n\nμν φ(hl\n\nμ) · φ(hl\n\nν )\n\n(cid:17)\n\n.\n\n(22)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nthe\n\nWe enforce these definitions for all order parameters {Φl μν , Gl We of { ˆΦl for the moment generating function and take the N → ∞ limit to derive our DMFT equations.\n\nIn the next section we show the resulting formula\n\nμ,ν , C l parameters\n\ncorresponding Fourier\n\nlet μν , ˆGl\n\nμ,ν , −Dl\n\nμν , −Bl\n\nμν , ˆ ̃Gl\n\nμ,ν }. be\n\nμν , Al\n\nμ,ν }.\n\nduals\n\norder\n\nthese\n\nˆ ̃ ̃Gl\n\neach\n\nμν ,\n\nfor\n\nμν , ̃Gl\n\nμν , ̃ ̃Gl\n\nB.4 DMFT ACTION\n\ninserting the Dirac-Delta functions\n\nAfter rameters, we derive the following moment generating functional {Φ, ˆΦ, G, ˆG, ̃G, ˆ ̃G, ̃ ̃G,\n\nˆ ̃ ̃G, A, B, C, D, j, k, n, p}\n\nto enforce the definitions of\n\nthe order pain terms of q =\n\n(cid:90) (cid:89)\n\nZ =\n\nl,μ,ν\n\ndΦl\n\nμν d ˆΦl 2πN −1\n\nμν\n\ndGl\n\nμν d ˆGl 2πN −1\n\nμν\n\nd ̃Gl\n\nμν d ˆ ̃Gl 2πN −1\n\nμν\n\nd ̃ ̃Gl\n\nˆ ̃ ̃Gl μν d 2πN −1\n\nμν\n\ndAl\n\nμν dBl 2πN −1\n\nμν\n\ndC l\n\nμν dDl 2πN −1\n\nμν\n\nexp (N S[q])\n\nwhere S[q] is the ON (1) DMFT action which takes the form\n\nS[q] =\n\n(cid:88)\n\nlμν\n\n(cid:20)\n\nΦl\n\nμ,ν\n\nˆΦl\n\nμ,ν + Gl\n\nμν\n\nˆGl\n\nμν + ̃Gl\n\nμν\n\nˆ ̃Gl\n\nμν + ̃ ̃Gl\n\nμν\n\nˆ ̃ ̃Gl\n\nμν − Al\n\nμν Bl\n\nμν − C l\n\nμν Dl\n\nμν\n\n+\n\n1 N\n\nN (cid:88)\n\nL (cid:88)\n\ni=1\n\nl=1\n\nln Z l\n\ni [q].\n\n(cid:21)\n\n(23)\n\ni , kl\n\ni , nl\n\nThe single-site moment generating functionals (MGF) Z l i involve only the integrals with sources {jl i } for neuron i ∈ [N ] in layer l. For a given set of order parameters q at zero source, these functionals become identical across all neuron sites i. Concretely, for any l ∈ [L], i ∈ [N ], the single site MGF takes the form\n\ni , pl\n\n(cid:90) (cid:89)\n\nZ l\n\ni =\n\ndχl\n\nμ\n\nμd ˆχl 2π\n\ndξl\n\nμd ˆξl 2π\n\nμ\n\ndζ l\n\nμdˆζ l 2π\n\nμ\n\nˆ ̃ζ l\n\nμ\n\nd ̃ζ l\n\nμd 2π\n\nμ\n\n(cid:32)\n\nexp\n\n−\n\n(cid:32)\n\nexp\n\n−\n\n(cid:32)\n\nexp\n\n−\n\n(cid:104)\n\nˆχl+1\n\nμ ˆχl+1\n\nν Φl\n\nμ,ν + ˆξl\n\nμ\n\nˆξl ν Gl+1\n\nμν +\n\nˆ ̃ζ l\n\nμ\n\nˆ ̃ζ l\n\nν\n\n ̃ ̃Gl+1\n\nμν\n\n(cid:33)\n\n(cid:105)\n\n(cid:104)ˆζ l\n\nμ\n\nˆζ l\n\nν\n\n ̃ ̃Gl+1\n\nμ,ν + 2 ˆξl\n\nμ\n\n(cid:33)\n\n(cid:105)\n\nˆζ l\n\nν\n\n ̃Gl+1\n\nμ,ν\n\n1 2\n\n1 2\n\n(cid:88)\n\nμ,ν\n\n(cid:88)\n\nμν\n\n(cid:20)\n\nˆΦl\n\nμν φ(hl\n\nμ)φ(hl\n\nν ) + ˆGl\n\nμν gl\n\nμgl\n\nν + ˆ ̃Gl\n\nμν gl\n\nμ ̃gl\n\nν +\n\n(cid:21)(cid:33)\n\nˆ ̃ ̃Gl\n\nμν ̃gl\n\nμ ̃gl\n\nν\n\n(cid:88)\n\nμν\n\n(cid:32)\n\nexp\n\n−i\n\n(cid:88)\n\nμν\n\n(cid:104)\n\nˆχl+1\n\nμ gl+1\n\nν Al\n\nμν + ˆχl+1\n\nμ ̃gl+1\n\nν C l\n\nμν + φ(hl\n\nμ) ˆξl\n\nν Bl\n\nμν + φ(hl\n\nμ)ˆζ l\n\nν Dl\n\nμν\n\n(cid:33)\n\n(cid:105)\n\n(cid:32)\n\n(cid:88)\n\n(cid:104)\n\nexp\n\nμ\n\nχl\n\nμ(jl\n\ni,μ + i ˆχl\n\nμ) + ξl\n\nμ(kl\n\ni,μ + i ˆξl\n\nμ) + ζ l\n\nμ(nl\n\ni,μ + iˆζ l\n\nμ) + ̃ζ l\n\nμ(pl\n\ni,μ + i\n\n(cid:105) ˆ ̃ζ l μ)\n\n(24)\n\n(cid:33)\n\n.\n\nAs promised, the only terms in Zi which vary over site index i are the sources {j, k, n, p}. To simplify our later saddle point equations, we will abstract the notation for the single site MGF, letting\n\nZ l\n\ni =\n\n(cid:90) (cid:89)\n\nμ\n\ndχμd ˆχμ 2π\n\ndξμd ˆξμ 2π\n\ndζμdˆζμ 2π\n\nd ̃ζμd 2π\n\nˆ ̃ζμ\n\nexp\n\n(cid:16)\n\n−Hl\n\ni [χ, ˆχ, ξ, ˆξ, ζ, ˆζ, ̃ζ,\n\n(cid:17) ˆ ̃ζ]\n\n(25)\n\nwhere Hl Hl\n\ni are identical for all i ∈ [N ].\n\ni is the single site effective Hamiltonian for neuron i and layer l. Note that at zero source,\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nB.5 SADDLE POINT EQUATIONS\n\nLetting the full collection of concatenated order parameters q be indexed by b. We now take the N → ∞ limit, using the method of steepest descent\n\n(cid:90) (cid:89)\n\nZ =\n\nb\n\n√ N dqb√ 2π\n\nexp (N S[q]) ∼ exp (N S[q∗]) , ∇S[q]|q∗ = 0 , N → ∞.\n\n(26)\n\nWe see that the integral over q is exponentially dominated by the saddle point where ∇S[q] = 0. We thus need to solve these saddle point equations for the q∗. To do this, we need to introduce some ˆ ̃ζ) be an arbitrary function of the single site stochastic processes. notation. Let O(χ, ˆχ, ξ, ˆξ, ζ, ˆζ, ̃ζ,\n\nWe define the l-th layer i-th single site average, denoted by\n\n(cid:68)\n\nO(χ, ˆχ, ξ, ˆξ, ζ, ˆζ, ̃ζ,\n\n(cid:69)\n\nˆ ̃ζ)\n\nas\n\nl,i\n\n(cid:68)\n\nO(χ, ˆχ, ξ, ˆξ, ζ, ˆζ, ̃ζ,\n\nˆ ̃ζ)\n\n(cid:69)\n\nl,i\n\n=\n\n1 Z l i\n\n(cid:90) (cid:89)\n\nμ\n\ndχμd ˆχμ 2π\n\ndξμd ˆξμ 2π\n\ndζμdˆζμ 2π\n\nˆ ̃ζμ\n\nd ̃ζμd 2π\n\n(cid:16)\n\nexp\n\n−Hl\n\ni [χ, ˆχ, ξ, ˆξ, ζ, ˆζ, ̃ζ,\n\n(cid:17) ˆ ̃ζ]\n\nO(χ, ˆχ, ξ, ˆξ, ζ, ˆζ, ̃ζ,\n\nˆ ̃ζ) (27)\n\nwhich can be interpreted as an average over the Gibbs measure defined by energy Hl i . With this notation, we now set about computing the saddle point equations which define the primal order parameters {Φ, G, ̃G, ̃ ̃G}.\n\n(cid:10)φ(hl\n\nμ)φ(hl\n\nν )(cid:11)\n\n= 0\n\nl,i\n\n(cid:10)gl\n\nμgl\n\nν\n\n(cid:10)gl\n\nμ ̃gl\n\nν\n\n(cid:10) ̃gl\n\nμ ̃gl\n\nν\n\n(cid:11)\n\n(cid:11)\n\n(cid:11)\n\n= 0\n\nl,i\n\n= 0\n\nl,i\n\n= 0\n\nl,i\n\n∂S ∂ ˆΦl\n\nμν\n\n∂S ∂ ˆGl\n\nμν\n\n∂S ∂ ˆ ̃Gl\n\nμν\n\n∂S\n\nˆ ̃ ̃Gl\n\nμν\n\n∂\n\n= Φl\n\nμν −\n\n= Gl\n\nμν −\n\n= ̃Gl\n\nμν −\n\n= ̃ ̃Gl\n\nμν −\n\n1 N\n\n1 N\n\n1 N\n\n1 N\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nWe further compute the saddle point equations for the dual order parameters\n\nN (cid:88)\n\n(cid:68)\n\n[ˆζ l−1\n\nμ\n\nˆζ l−1 ν +\n\nˆ ̃ζ l−1\n\nμ\n\nˆ ̃ζ l−1\n\nν\n\n(cid:69) ]\n\n= 0\n\nl−1,i\n\n∂S ∂Φl\n\nμν\n\n∂S ∂Gl\n\nμν\n\n∂S ∂ ̃Gl\n\nμν\n\n∂S ∂ ̃ ̃Gl\n\nμν\n\n∂S ∂Al\n\nμν\n\n∂S ∂Bl\n\nμν\n\n∂S ∂C l\n\nμν\n\n∂S ∂Dl\n\nμν\n\n= ˆΦl\n\nμν −\n\n= ˆGl\n\nμν −\n\n1 2N\n\n1 2N\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n(cid:10) ˆχl+1\n\nμ ˆχl+1\n\nν\n\n(cid:11)\n\nl+1,i\n\n= 0\n\n(cid:68) ˆξl−1\n\nμ\n\nˆξl−1\n\nν\n\n(cid:69)\n\nl−1,i\n\n= 0\n\n= ˆ ̃Gl\n\nμν −\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:68) ˆξl−1\n\nμ\n\nˆζ l−1\n\nν\n\n(cid:69)\n\nl−1,i\n\n= 0\n\ni=1\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\nˆ ̃ ̃Gl\n\nμν −\n\n=\n\n1 2N\n\n= −Bl\n\nμν −\n\n= −Al\n\nμν −\n\n= −Dl\n\nμν −\n\n= −C l\n\nμν −\n\ni N\n\ni N\n\ni N\n\ni N\n\n(cid:10) ˆχl+1\n\nμ gl+1\n\nν\n\n(cid:11)\n\n= 0\n\nl+1,i\n\n(cid:68)\n\nφ(hl\n\nμ) ˆξl\n\nν\n\n(cid:69)\n\nl,i\n\n= 0\n\n(cid:10) ˆχl+1\n\nμ ̃gl+1\n\nν\n\n(cid:11)\n\nl+1,i\n\n= 0\n\ni=1\n\nN (cid:88)\n\n(cid:68)\n\ni=1\n\nφ(hl\n\nμ)ˆζ l\n\nν\n\n(cid:69)\n\nl,i\n\n= 0 .\n\n(28)\n\nThe correlation functions involving real variables {h, g, ̃g} have a straightforward interpetation. However, it is not immediately clear what to do with terms involving the dual fields { ˆχ, ˆξ, ˆζ}. As a (cid:11). We make progress starting example, let’s consider one of the terms for Bl−1 by inserting another fictitious source term ul\n\nμν , namely −i (cid:10) ˆχl\n\nμ and differentiating near zero source\n\nν gl\n\nν\n\n−i (cid:10) ˆχl\n\nν gl\n\nν\n\n(cid:11) i = lim\n\n{uμ}→0\n\n∂ ∂ul ν\n\n(cid:42)\n\n(cid:32)\n\ngl\n\nν exp\n\n−i\n\nuν ′ ˆχl\n\nμ′\n\n(cid:88)\n\nν ′\n\n(cid:33)(cid:43)\n\n.\n\ni\n\n(29)\n\nIntroducing a vectorization notation ul = Vec{ul Mat{Φl−1\n\nμ,ν }μ,ν , we can perform the internal integrals over ˆχl\n\nμ}μ, ˆχl = Vec{ ˆχl\n\nμ}μ and Φl−1 =\n\n(cid:90) (cid:89)\n\nμ\n\nd ˆχl μ√ 2π\n\n(cid:18)\n\nexp\n\n−\n\n1 2\n\nˆχl⊤Φl−1 ˆχl + i ˆχl · (χl − ul − Al−1gl − Cl−1 ̃gl)\n\n(cid:19)\n\n(cid:18)\n\n= exp\n\n−\n\n(cid:18)\n\nexp\n\n−\n\n1 2\n\n1 2\n\n(χl − ul − Al−1gl − Cl−1 ̃gl)[Φl−1]−1(χl − ul − Al−1gl − Cl−1 ̃gl)\n\n(cid:19)\n\nln det Φl−1\n\n(cid:19)\n\n.\n\n(30)\n\nWe thus need to compute a derivative of the above function with respect to ul at ul = 0, which gives\n\n−i (cid:10) ˆχlgl⊤(cid:11)\n\ni = [Φl−1]−1 (cid:10)(χl − Al−1gl − Cl−1 ̃gl)gl⊤(cid:11)\n\ni .\n\n(31)\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFrom the above reasoning, we can also easily obtain ˆΦl−1 using\n\n∂2\n\n∂ul∂ul⊤ |u=0\n\n(cid:10)exp (cid:0)−iul · ˆχl(cid:1)(cid:11)\n\n(cid:10) ˆχl ˆχl⊤(cid:11) = −\n\n(cid:90)\n\n= −\n\ndχl...\n\n∂2\n\n∂ul∂u⊤ |u=0\n\n(cid:18)\n\n× exp\n\n−\n\n1 2\n\n(χl − ul − Al−1gl − Cl−1 ̃gl)[Φl−1]−1(χl − ul − Al−1gl − Cl−1 ̃gl) − ...\n\n(cid:19)\n\n= [Φl−1]−1 − [Φl−1]−1 (cid:10)(χl − Al−1gl − Cl−1 ̃gl)(χl − Al−1gl − Cl−1 ̃gl)⊤(cid:11) [Φl−1]−1.\n\nPerforming a similar analysis, we insert source fields r+ =\n\n(cid:34)\n\n(cid:35)\n\n ̃Gl+1 Gl+1 ̃Gl+1⊤ ̃ ̃Gl+1 the same technique\n\n, and Bl\n\n+ = (cid:2)Bl Dl(cid:3) and then we can compute the necessary averages using\n\n(32)\n\n(cid:21)\n\n(cid:20)rl vl\n\nfor ˆξl\n\n+ =\n\n(cid:21)\n\n(cid:20) ˆξl ˆζl\n\nand define Gl+1\n\n+ =\n\n(cid:68)\n\nφ(hl) ˆξl⊤\n\n+\n\n−i\n\n(cid:69)\n\n=\n\n∂ ∂rl +\n\n|rl\n\n+=0\n\n(cid:68)\n\nφ(hl) exp\n\n(cid:16)\n\n−irl\n\n+ · ˆξl\n\n+\n\n(cid:17)(cid:69)\n\n= (cid:10)φ(hl)(ξl\n\n+ − Bl⊤\n\n+ φ(hl))⊤(cid:11)\n\n(cid:68) ˆξl\n\n+\n\nˆξl⊤\n\n+\n\n(cid:69)\n\n= −\n\n∂2 +∂rl⊤\n\n|rl\n\n+=0\n\n∂rl + ]−1 − [Gl+1 = [Gl+1\n\n+\n\n(cid:68)\n\nexp\n\n(cid:16)\n\n−irl\n\n+ · ˆξl\n\n+\n\n(cid:17)(cid:69)\n\n+ ]−1 (cid:10)(ξl\n\n+ − Bl⊤\n\n+ φ(hl))(ξl\n\n+ − Bl⊤\n\n+ φ(hl))⊤(cid:11) [Gl+1\n\n+ ]−1.\n\nWe now have formulas for all the necessary averages entirely in terms of the primal fields {χ, ξ, ζ, ̃ζ}.\n\nB.6 LINEARIZING WITH THE HUBBARD TRICK\n\nNow, using the fact that in the N → ∞ limit q concentrates around q∗, we will simplify our single ˆ ̃ ̃G}. To do so, site stochastic processes so we can obtain a final formula for {A, B, C, D, ˆΦ, ˆG, ˆ ̃G, we utilize the Hubbard-Stratanovich identity\n\n(33)\n\n(cid:18)\n\nexp\n\n−\n\nσ2 2\n\n(cid:19)\n\nk2\n\n=\n\n(cid:90)\n\n√\n\ndu\n\n2πσ2\n\n(cid:18)\n\nexp\n\n−\n\n(cid:19)\n\n1\n\n2σ2 u2 − iku\n\n,\n\n(34)\n\nwhich is merely a consequence of the Fourier transform of the Gaussian distribution. This is often referred to as “linearizing” the action since the a term quadratic in k was replaced with an average of an action which is linear in k. In our setting, we perform this trick on a collection of variables which appear in the quadratic forms of our single site MGFs Z l i . For example, for the ˆχl+1 fields, we have\n\n(cid:32)\n\nexp\n\n−\n\n1 2\n\n(cid:88)\n\nμν\n\nˆχl+1\n\nμ ˆχl+1\n\nν Φl\n\nμν\n\n(cid:33)\n\n(cid:42)\n\n(cid:32)\n\n=\n\nexp\n\n−i\n\n(cid:88)\n\nμ\n\nˆχl+1\n\nμ ul+1\n\nμ\n\n(cid:33)(cid:43)\n\n.\n\n(35)\n\n{ul+1\n\nμ }∼N (0,Φl)\n\nSimilarly, we perform a joint decomposition for the { ˆξl, ˆζ l} fields which gives\n\n(cid:32)\n\nexp\n\n−\n\n1 2\n\n(cid:88)\n\n(cid:104) ˆξl\n\nμ\n\nμν\n\nˆξl ν Gl+1\n\nμ,ν + 2 ˆξl\n\nμ\n\nˆζ l\n\nν\n\n ̃Gl+1\n\nμν + ˆζ l\n\nμ\n\n(cid:33)\n\n(cid:105)\n\nˆζ l\n\nν\n\n ̃ ̃Gl+1\n\nμν\n\n(36)\n\n(cid:42)\n\n(cid:32)\n\n=\n\nexp\n\n−i\n\n(cid:88)\n\n[rl\n\nμ\n\n(cid:33)(cid:43)\n\nˆξl μ + vl\n\nμ\n\nˆζ l μ]\n\nμ\n\n{rl\n\nμ,vl\n\nμ}∼N (0,Gl+1 + )\n\n, Gl+1\n\n+\n\n(cid:34)\n\nGl+1 ̃Gl+1\n\n=\n\n(cid:35)\n\n.\n\n ̃Gl+1 ̃ ̃Gl+1\n\nWe thus see that the Gaussian sources {rl μ}μ are mean zero with correlation given by Σl+1. Now that we have linearized the quadratic components involving each of the dual fields\n\nμ}μ and {vl\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n{ ˆχ, ˆξ, ˆζ}, we now perform integration over these variables, giving\n\n(cid:90) (cid:89)\n\nμ\n\nd ˆχl μ\n2π\n\n(cid:32) i\n\nexp\n\n(cid:88)\n\nˆχl\n\nμ\n\nμ\n\n(cid:32)\n\nχl\n\nμ − ul\n\nμ −\n\n(cid:88)\n\nν\n\nAl−1\n\nμν gl\n\nν −\n\n(cid:33)(cid:33)\n\n(cid:88)\n\nν\n\nC l−1\n\nμν ̃gl\n\nν\n\n(cid:89)\n\nδ\n\n=\n\n(cid:32)\n\nχl\n\nμ − ul\n\nμ −\n\n(cid:88)\n\nAl−1\n\nμν gl\n\nν −\n\nμ\n\n(cid:90) (cid:89)\n\nμ\n\n(cid:90) (cid:89)\n\nμ\n\nd ˆξl μ\n2π\n\ndˆζ l μ\n2π\n\n(cid:32) i\n\n(cid:32) i\n\nexp\n\nexp\n\nν\n\nˆξl\n\nμ\n\n(cid:88)\n\nμ\n\n(cid:88)\n\nˆζ l\n\nμ\n\nμ\n\n(cid:32)\n\nμ − rl ξl\n\nμ −\n\n(cid:32)\n\nμ − vl ζ l\n\nμ −\n\n(cid:88)\n\nν\n\n(cid:88)\n\nν\n\n(cid:88)\n\nν\n\nC l−1\n\nμν ̃gl\n\nν\n\n(cid:33)\n\n(cid:33)(cid:33)\n\nBl\n\nνμφ(hl\n\nν )\n\n(cid:33)(cid:33)\n\nDl\n\nνμφ(hl\n\nν )\n\n(cid:89)\n\nδ\n\n=\n\nμ\n\n(cid:89)\n\nδ\n\n=\n\nμ\n\n(cid:32)\n\nμ − rl ξl\n\nμ −\n\n(cid:32)\n\nμ − vl ζ l\n\nμ −\n\n(cid:88)\n\nν\n\n(cid:88)\n\nν\n\n(cid:33)\n\nBl\n\nνμφ(hl\n\nν )\n\n(cid:33)\n\nDl\n\nνμφ(hl\n\nν )\n\n.\n\nThis reveals the following set of identities\n\nχl\n\nμ = ul\n\nμ +\n\nμ = rl ξl\n\nμ +\n\nAl−1\n\nμν gl\n\nν +\n\n(cid:88)\n\nν\n\nC l−1\n\nμν ̃gl\n\nν\n\nBl\n\nνμφ(hl\n\nν ) , ζ l\n\nμ = vl\n\nμ +\n\n(cid:88)\n\nν (cid:88)\n\nν\n\n(37)\n\n(38)\n\n(cid:88)\n\nν\n\nDl−1\n\nνμ φ(hl\n\nν ).\n\nSince we know by construction that ul = χl − Al−1gl − Cl−1 ̃gl is a zero mean Gaussian with covariance Φl−1, we can simplify our expressions for Bl−1 and ˆΦl−1 using Stein’s Lemma\n\nBl−1 =\n\nDl−1 =\n\n1 N\n\n1 N\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n[Φl−1]−1 (cid:10)ulgl⊤(cid:11)\n\ni =\n\n[Φl−1]−1 (cid:10)ul ̃gl⊤(cid:11)\n\ni =\n\n1 N\n\n1 N\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n(cid:29)\n\n(cid:28) ∂gl⊤ ∂ul\n\n(cid:29)\n\n(cid:28) ∂ ̃gl⊤ ∂ul\n\ni\n\ni\n\nˆΦl−1 =\n\n1 2\n\n[Φl−1]−1 −\n\n1 2N\n\nN (cid:88)\n\n[Φl−1]−1 (cid:10)ulul⊤(cid:11)\n\ni [Φl−1]−1 = 0.\n\ni=1\n\n(39)\n\nSimilarly, using the Gaussianity of rl, ζl, ˆζl we have\n\nAl =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:29)\n\n(cid:28) ∂φ(hl) ∂rl⊤\n\ni\n\n, Cl =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:29)\n\n(cid:28) ∂φ(hl) ∂vl⊤\n\ni\n\n, ˆGl+1 = ˆ ̃Gl+1 =\n\nˆ ̃ ̃Gl+1 = 0.\n\nB.7 FINAL DMFT EQUATIONS\n\nWe now take the limit of zero source jl, kl, nl, pl → 0. In this limit, all single site averages ⟨⟩i become identical so we can simplify the expressions for the order parameters. To “symmetrize” the equations we will also make the substitution B → B⊤, D → D⊤. Next, we also rescale all of the response functions {Al, Bl, C l, Dl} by γ−1 so that they are Oγ0(1) at small γ0. This gives us the following set of equations for the order parameters\n\n0\n\nΦl\n\nμν(t, s) = (cid:10)φ(hl\n\nμ(t))φ(hl\n\nν(s))(cid:11) , Gl\n\nμν(t, s) = (cid:10)gl\n\nμ(t)gl\n\nν(s)(cid:11) , ̃Gl\n\nμν(t, s) = (cid:10)gl\n\n ̃ ̃Gl\n\nμν(t, s) = (cid:10) ̃gl\n\nμ(t) ̃gl\n\nν(s)(cid:11) , Al\n\nμν(t, s) = γ−1\n\n0\n\n(cid:42)\n\n, C l\n\nμν(t, s) = γ−1\n\n0\n\nν(s)(cid:11)\n\nμ(t) ̃gl (cid:42)\n\nδφ(hl δvl\n\nμ(t)) ν(s)\n\n(cid:43)\n\nBl\n\nμν(t, s) = γ−1\n\n0\n\n(cid:43)\n\n(cid:42)\n\nδgl+1 δul+1\n\nμ (t) (s)\n\nν\n\n(cid:43)\n\nδφ(hl μ(t)) δrl ν(s) (cid:42)\n\nδ ̃gl+1 δul+1\n\nμ (t) (s)\n\nν\n\n(cid:43)\n\n.\n\n, Dl\n\nμν(t, s) = γ−1\n\n0\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(a) ρ = 0 Final Kernel\n\n(b) ρ = 0 Kernel Dynamics\n\n(c) ρ = 1.0 Final Kernel\n\n(d) ρ = 1.0 Kernel Dynamics\n\nFigure 6: Feature kernels Φl and their dynamics predicted by solving full set of saddle point equations equation 40 for ρ-FA in depth 3 tanh network with γ0 = 1.0. Solving deep nonlinear ρ-FA requires sampling the full triplet of Gaussian sources {ul, rl, vl} for each layer and computing all four response functions {Al, Bl, C l, Dl}. DMFT theoretical predictions are compared to a width N = 3000 neural network.\n\nFor the fields hl\n\nμ(t), zl\n\nμ(t), ̃zl\n\nμ(t), we have the following equations\n\nhl\n\nμ(t) = ul\n\nμ(t) + γ0\n\nμ(t) = rl zl\n\nμ(t) + γ0\n\n(cid:90) t\n\n0\n\n(cid:90) t\n\nP (cid:88)\n\nds\n\n0\n\nν=1\n\nP (cid:88)\n\nds\n\n[Al−1\n\nμν (t, s)gl\n\nν(s) + C l−1\n\nμν (t, s) ̃gl\n\nμ(s) + ∆ν(s)Φl−1\n\nμν (t, s) ̃gl\n\nν(s)]\n\nν=1\n\n[Bl\n\nμν(t, s)φ(hl\n\nν(s)) + ∆ν(s) ̃Gl+1\n\nμν (t, s)φ(hl\n\nν(s))]\n\n ̃gl\n\nμ(t) =\n\n\n\n \n\n ̇φ(hl μ(t))zl μ(t) (cid:104)(cid:112)1 − ρ2 ̃ζ l ̇φ(hl μ(t)) ̇φ(hl μ(t)) ̃zl , ̃zl ∼ N (0, 1) ̇φ(ml μ(t))zl ∆μ(t)φ(hl\n\nμ(t) μ(t))\n\nμ(t) + ρvl\n\nμ(t) + ργ0\n\n(cid:82) t 0 ds (cid:80)P\n\nν=1 Dl\n\nμν(t, s)φ(hl\n\nν(s))\n\n{ul\n\nμ(t)} ∼ GP(0, Φl−1) , {rl μ(t), vl (cid:35) ̃Gl+1 Gl+1 ̃Gl+1,⊤ ̃ ̃Gl+1\n\n+ =\n\nGl+1\n\n(cid:34)\n\n.\n\nμ(t)} ∼ GP(0, Gl+1\n\n+ ) , { ̃ζ l\n\nμ(t)} ∼ GP(0, ̃ ̃Gl+1)\n\n(cid:105)\n\nGD\n\nρ-FA\n\nDFA GLN Hebb\n\n(40)\n\nC EXTENSION TO OTHER ARCHITECTURES AND OPTIMIZERS\n\nIn this section, we consider the effect of changing architectural details (multiple output channels and convolutional structure) and also optimization choices (momentum, regularization).\n\n23\n\nExpt.1,DMFT.2,3,Expt.Tr 1(t,s)DMFT.Tr 2(t,s)Tr 3(t,s)Expt.1,DMFT.2,3,Expt.Tr 1(t,s)DMFT.Tr 2(t,s)Tr 3(t,s)(41)\n\n· ̃gl\n\nν(s)(cid:3)\n\nPublished as a conference paper at ICLR 2023\n\nC.1 MULTIPLE OUTPUT CLASSES\n\nSimilar to pre-existing work on the GD case (Bordelon & Pehlevan, 2022), our new generalized DMFT can be easily extended to C output channels, provided the number of channels C is not simultaneously taken to infinity with network width N . We note that the outputs of the network are now vectors fμ ∈ RC and that each eNTK entry is now a C × C matrix Kμν(t, s) ∈ RC×C. The relevant true gradient fields are vectors gl\n\n. We construct pseudo-gradients ̃gl\n\nc,μ = ∂fc,μ\n\nc,μ\n\nas before using each of our learning rules. The gradient-pseudogradient kernel Gl ̃Gl dynamics\n\nμν ∈ RC×C is μν can be used to derive the function\n\nc′,ν. The eNTK Kμν = (cid:80)\n\nc,c′,μν = 1\n\nc,μ · gl\n\nμν Φl\n\n ̃Gl+1\n\nN gl\n\nl\n\n∂hl μ\n\n∂fμ ∂t\n\n(cid:88)\n\n=\n\nν\n\nKμν∆μ , ∆μ = −\n\n∂L ∂fμ\n\n.\n\nAt infinite width N → ∞, the field dynamics for hl\n\nμ(t) ∈ R, gl\n\nμ(t) ∈ RC satisfy\n\nhl\n\nμ(t) = ul\n\nμ(t) + γ0\n\nzl\n\nμ(t) = rl\n\nμ(t) + γ0\n\n(cid:90) t\n\n0\n\n(cid:90) t\n\n0\n\nds\n\nds\n\nP (cid:88)\n\nν=1\n\nP (cid:88)\n\nν=1\n\n(cid:2) ̃gl\n\nν(s) · ∆ν(s)Φl−1\n\nμν (t, s) + Al−1\n\nμν (t, s) · gl\n\nν(s) + Cl−1\n\nμν\n\nφ(hl\n\nν(s))\n\nμν (t, s)∆ν(s) + Bl\n\n(cid:104) ̃Gl+1\n\n(cid:105) μν(t, s)\n\n,\n\n(42)\n\nwhere Al\n\nμν(t, s) = γ0\n\n(cid:28) δφ(hl\n\nμ(t)) ν (s)\n\nδrl\n\n(cid:29)\n\n∈ RC, Al\n\nμν(t, s) = γ0\n\n(cid:28) δφ(hl\n\nμ(t)) ν (s)\n\nδvl\n\n(cid:29)\n\n∈ RC, and Bμα(t, s) =\n\n∂gl+1 ∂ul\n\nμ (t)\n\nν (s) ∈ RC. The feature kernels are the same as before Φl μ(t) ̃gl⊤\n\nthe gradient-pseudogradient kernel is ̃Gl fields ̃gl are defined analogously for each learning rule as in the single class setting.\n\nν(s))(cid:11) while μν(t, s) = (cid:10)φ(hl ν (s)(cid:11) ∈ RC×C. The pseudogradient\n\nμν(t, s) = (cid:10)gl\n\nμ(t))φ(hl\n\nμ(t) + (cid:112)1 − ρ2 ̃ζl\n\n ̇φ(hl μ(t))zl μ(t) (cid:104) ̇φ(hl ρvl μ(t)) ̇φ(hl μ(t)) ̃zl , ̃zl ∼ N (0, I) ̇φ(ml μ(t))zl 1∆μ(t)φ(hl\n\nμ(t) μ(t))\n\nμ(t) + ργ0\n\n ̃gl\n\nμ(t) =\n\n\n\n \n\nC.2 CNN\n\n(cid:82) t 0 ds (cid:80)P\n\nν=1 Dl\n\nν(t, s)φ(hl\n\nν(s))\n\n(cid:105)\n\nGD\n\nρ-FA\n\nDFA GLN Hebb\n\n(43)\n\nThe DMFT described for each of these learning rules can also be extended to CNNs with infinitely many channels. Following the work of Bordelon & Pehlevan (2022) Appendix G on the GD DMFT limit for CNNs, we let W l ij,a represent the value of the filter at spatial displacement a from the center of the filter, which maps relates activity at channel j of layer l to channel i of layer l + 1. The fields hl\n\nμ,i,a satisfy the recursion\n\nhl+1\n\nμ,i,a =\n\n1 √\n\nN\n\nN (cid:88)\n\n(cid:88)\n\nj=1\n\nb∈S l\n\nW l\n\nij,bφ(hl\n\nμ,j,a+b) , i ∈ [N ],\n\n(44)\n\nwhere S l is the spatial receptive field at layer l. For example, a (2k + 1) × (2k + 1) convolution will have S l = {(i, j) ∈ Z2 : −k ≤ i ≤ k, −k ≤ j ≤ k}. The output function is obtained from the last layer is defined as fμ = 1 i,aφ(hL μ,i,a). The true gradient fields have the same γ0N definition as before gl\n\n∈ RN , which as before enjoy the following recursion\n\na wL\n\nμ,a = γ0N ∂fμ\n\n(cid:80)N\n\n(cid:80)\n\ni=1\n\n∂hl\n\nμ,a\n\ngl\n\nμ,a = γ0N\n\n(cid:88)\n\nb\n\n∂fμ ∂hl+1\n\nμ,b\n\n·\n\nμ,b\n\n∂hl+1 ∂hl\n\nμ,a\n\nW l⊤\n\nb gl+1\n\nμ,a−b\n\n\n\n .\n\n(45)\n\n= ̇φ(hl\n\nμ,a) ⊙\n\n\n\n\n\n1 √\n\nN\n\nN (cid:88)\n\n(cid:88)\n\nj=1\n\nb∈S l\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nWe consider the following learning dynamics for the filters\n\nd dt\n\nW l\n\nb =\n\nγ0√ N\n\n(cid:88)\n\nμ,c\n\n∆μ ̃gl+1\n\nμ,c φ(hl\n\nμ,c+b)⊤\n\n(46)\n\nwhere as before ̃gl is determined by the learning rule. The relevant kernel order parameters now have spatial indices. For instance the feature kernel at each layer has form Φl μ,a(t)) · φ(hl\n\nν,b(s)). At the infinite width N → ∞, the order parameters and field dynamics have the form\n\nμ,ν,ab = 1\n\nN φ(hl\n\nhl\n\nμ,a(t) = ul\n\nμ,a(t) + γ0\n\n+ γ0\n\n(cid:90) t\n\n0\n\n(cid:88)\n\nds\n\nν,b\n\n(cid:90) t\n\n0\n\n(cid:88)\n\nds\n\nν,b,c\n\n∆ν(s)Φl−1\n\nμν,a+b,b+c(t, s) ̃gl\n\nν,c(s)\n\n[Al−1\n\nμν,ab(t, s)gl\n\nν,b(s) + C l−1\n\nμν,ab(t, s) ̃gl\n\nνb(s)]\n\nμ,a(t) = rl zl\n\nμ,a(t) + γ0\n\n(cid:90) t\n\n0\n\n(cid:88)\n\nds\n\nν,b,c\n\n ̃Gl+1\n\nμν,a−b,c−b(t, s)φ(hl\n\nν,c(s))\n\n+ γ0\n\n(cid:90) t\n\n0\n\n(cid:88)\n\nds\n\nν,b\n\nBl\n\nμν,ab(t, s)φ(hl\n\nν,b(s))\n\nwhere correlation and response functions have the usual definitions\n\n(47)\n\n(48)\n\nΦl\n\nμα,ab(t, s) = (cid:10)φ(hl (cid:42)\n\nAl\n\nμα,ab(t, s) =\n\n1 γ0\n\nμa(t))φ(hl δφ(hl δrl\n\nμa(t)) αb(s)\n\n(cid:43)\n\nαb(s))(cid:11) , Gl\n\nμα,ab(t, s) = (cid:10)gl (cid:42)\n\n, Bl\n\nμα,ab(t, s) =\n\nμa(t)gl δgl+1 δul+1\n\nμa (t) αb (s)\n\n1 γ0\n\nαb(s)(cid:11) , ̃Gl\n\nμν,ab(t, s) = (cid:10)gl\n\nμa(t) ̃gl\n\nαb(s)(cid:11)\n\n(cid:43)\n\n.\n\n(49)\n\nC.3 L2 REGULARIZATION (WEIGHT DECAY)\n\nL2 regularization on the weights W l (weight decay) can also be modeled within DMFT. We start by looking at the weight dynamics\n\nd dt\n\nW l =\n\nγ0√ N\n\nP (cid:88)\n\nμ=1\n\n∆μ ̃gl+1\n\nμ φ(hl\n\nμ)⊤ − λW l\n\n=⇒ W l(t) = e−λtW l(0) +\n\nγ0√ N\n\n(cid:90) t\n\n0\n\nds e−λ(t−s) (cid:88)\n\nμ\n\n∆μ(s) ̃gl+1\n\nμ (s)φ(hl\n\nμ(s))⊤\n\n(50)\n\nIn the second line we used an integrating factor eλt. We can thus arrive at the following feature dynamics in the DMFT limit\n\nhl\n\nμ(t) = e−λtχl\n\nμ(t) + γ0\n\nμ(t) = e−λtξl zl\n\nμ(t) + γ0\n\n(cid:90) t\n\n0 (cid:90) t\n\n0\n\nds e−λ(t−s) (cid:88)\n\nν\n\n∆ν(s)Φl−1\n\nμν (s) ̃gl\n\nν(s)\n\nds e−λ(t−s) (cid:88)\n\nν\n\n∆ν(s) ̃Gl+1\n\nμν (s)φ(hl\n\nν(s)).\n\nThe ̃g dynamics are also modified appropriately with factors of e−λt and e−λ(t−s) for each of our learning rules. We see that the contribution from the initial conditions χ, ξ are suppressed at late times while the feature learning update which is O(γ0/λ) in the first layer dominates scale of the final features.\n\nC.4 MOMENTUM\n\nMomentum uses a low-pass filtered version of the gradients to update the weights (Goh, 2017). A continuous time limit of momentum dynamics on the trainable parameters {W l} would give the\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nfollowing differential equations\n\nW l(t) = Ql(t)\n\n∂ ∂t d\ndt\n\nτ\n\nQl(t) = −Ql +\n\nγ0√ N\n\n(cid:88)\n\nμ\n\n∆μ(t) ̃gl+1\n\nμ (t)φ(hl\n\nμ(t))⊤.\n\n(51)\n\nWe write the expression this way so that the small time constant τ → 0 limit corresponds to classic gradient descent. Integration of the Ql(t) dynamics gives the following integral expression for W l\n\nW l(t) = W l(0) +\n\nγ0√ N τ\n\n(cid:90) t′\n\n(cid:90) t\n\ndt′\n\n0\n\n0\n\ndt′′e−(t′−t′′)/τ (cid:88)\n\nμ\n\n∆μ(t′′) ̃gl+1\n\nμ (t′′)φ(hl\n\nμ(t′′))⊤.\n\nThese weight dynamics give rise to the following field evolution\n\nhl+1\n\nμ (t) = χl+1\n\nμ (t) +\n\nγ0 τ\n\n(cid:90) t′\n\n(cid:90) t\n\ndt′\n\n0\n\n0\n\ndt′′e−(t′−t′′)/τ (cid:88)\n\n∆ν(t′′) ̃gl+1\n\nν\n\n(t′′)Φl\n\nμν(t, t′′)\n\nν\n\nμ(t) = ξl zl\n\nμ(t) +\n\nγ0 τ\n\n(cid:90) t′\n\n(cid:90) t\n\ndt′\n\n0\n\n0\n\ndt′′e−(t′−t′′)/τ (cid:88)\n\nν\n\ndt′′∆α(t′′) ̃Gl+1\n\nμν (t, t′′)φ(hl\n\nν(t′′)).\n\n(52)\n\n(53)\n\nWe see that in the τ → 0 limit, the t′′ integral is dominated by the contribution at t′′ ∼ t′ recovering usual gradient descent dynamics. For τ ≫ 0, we see that the integral accumulates additional contributions from the past values of fields and kernels.\n\nD LAZY LIMITS\n\nIn this section we discuss the lazy γ0 → 0 limit. zl(t) = rl(t) for all time t. Since the input data gram matrix Φ0 the sources in the first hidden layer u1 kernel is constant in time since\n\nIn this limit we see that hl(t) = ul(t) and D xμ · xν is a constant in time μ are constant in time. Consequently, the first layer feature\n\nμν = 1\n\nΦ1\n\nμν(t, s) = (cid:10)φ(h1\n\nμ(t))φ(h1\n\nν(s))(cid:11) = (cid:10)φ(u1\n\nμ)φ(u1\n\nν)(cid:11)\n\nu1∼N (0,Φ0)\n\n.\n\n(54)\n\nNow, we see that this argument can proceed inductively. Since Φ1 is time-independent, the second layer fields h2 = u2 ∼ N (0, Φ1) are also constant in time, implying Φ2 is constant in time. This argument is repeated for all layer l ∈ [L]. Similarly, we can analyze the backward pass fields zl. Since zL ∼ N (0, GL+1) are constant, then zl are time-independent for all l. It thus suffices to compute the static kernels {Φl, Gl, ̃Gl} at initialization Φl = (cid:10)φ(ul)φ(ul)⊤(cid:11)\n\nul∼N (0,Φl−1)\n\n(cid:68)\n\nGl =\n\n[ ̇φ(ul) ⊙ rl][ ̇φ(ul) ⊙ rl]⊤(cid:69)\n\nul∼N (0,Φl−1),rl∼N (0,Gl+1)\n\n= Gl+1 ⊙ ̇Φl ,\n\n ̇Φl =\n\n(cid:68) ̇φ(ul) ̇φ(ul)⊤(cid:69)\n\nul∼N (0,Φl−1)\n\n.\n\n(55)\n\nwhere in the last line we utilized the independence of ul, rl. These above equations give a forward pass recursion for the Φl kernels and the backward pass recursion for Gl. Lastly, depending on the learning rule, we arrive at the following definitions for ̃Gl for l ∈ {1, ..., L}\n\n ̃Gl =\n\n(cid:68)\n\n[ ̇φ(ul) ⊙ rl] ̃gl⊤(cid:69)\n\n=\n\n\n\n\n\n\n\nGl+1 ⊙ ̇Φl ρ ̃Gl+1 ⊙ ̇Φl 0\nGl+1 ⊙\n\n(cid:68) ̇φ(ml) ̇φ(ml)⊤(cid:69)\n\nGD ρ-FA DFA,Hebb\n\nGLN\n\n(56)\n\nUsing these results for ̃G, we can compute the initial eNTK K = (cid:80)L prediction dynamics.\n\nl=0\n\n ̃Gl+1 ⊙ Φl which governs\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nD.1 LAZY LIMIT PERFORMANCES ON REALISTIC TASKS\n\nWe note that, while the DMFT equations on P datapoints and T timesteps require O(P 3T 3) time complexity to solve in the rich regime, the lazy limit gives neural network predictions in O(P 3) time, since the predictor can be obtained by solving a linear system of P equations. The performance of these lazy limit kernels on realistic tasks would match the performances reported by Lee et al. (2020). Specifically, GD and ρ = 1 FA would match the test accuracy reported for “infinite width GD”, while ρ = 0 FA, DFA, and Hebbian rules would match “infinite width Bayesian” networks in Figure 1 of Lee et al. (2020).\n\nE DEEP LINEAR NETWORKS\n\nIn deep linear networks, the DMFT equations close without needing any numerical sampling procedure, as was shown in prior work on the GD case (Yang & Hu, 2021; Bordelon & Pehlevan, 2022). The key observation is that for all of the following learning rules, the fields {h, g, ̃g} are linear combinations of the Gaussian sources {u, r, v}, and are thus Gaussian themselves. Concretely, we introduce a vector notation hl = Vec{hl\n\nμ(t)}, etc. We have in each layer\n\nμ(t)} and gl = Vec{gl\n\nhl = Rh,uul + Rh,rrl + Rh,vvl + Rh, ̃ζ gl = Rg,uul + Rg,rrl + Rg,vvl + Rg, ̃ζ ̃gl = R ̃g,uul + R ̃g,rrl + R ̃g,vvl + R ̃g, ̃ζ\n\n ̃ζl\n\n ̃ζl\n\n ̃ζl\n\nwhere the matrices R depend on the learning rule and the data. The necessary kernels H l = (cid:10)hlhl⊤(cid:11) can thus be closed algebraically since all of the correlation statistics of the sources {u, r, v} have known two-point correlation statistics.\n\nE.1 LINEAR NETWORK TRAINED WITH GD\n\nThe R matrices for GD were provided in (Bordelon & Pehlevan, 2022). We start by noting the following DMFT equations for hl, gl\n\nhl = ul + γ0(Al−1 + H l−1\n\n∆ )gl , gl = rl + γ0(Bl + Gl+1\n\n∆ )hl\n\n(57)\n\nwhere [H l−1 have\n\n∆ ]μν,ts = H l\n\nμν(t, s)∆ν(s). Isolating the dependence of these equations on u and r, we\n\n(cid:2)I − γ2 (cid:2)I − γ2\n\n0 (Al−1 + H l−1 0 (Bl + Gl+1\n\n∆ )(Bl + Gl+1 ∆ )(Al−1 + H l−1\n\n∆ )(cid:3) hl = ul + γ2 ∆ )(cid:3) gl = rl + γ2\n\n0 (Al−1 + H l−1 0 (Bl + Gl+1\n\n∆ )(Bl + Gl+1 ∆ )(Al−1 + H l−1\n\n∆ )rl ∆ )rl.\n\nThese equations can easily be closed for H l and Gl.\n\nE.2 ρ-ALIGNED FEEDBACK ALIGNMENT\n\nIn ρ-FA we define the following pseudo-gradient fields\n\nNext, we note that, at initialization, the ̃Gl can be computed recursively\n\n(cid:112)\n\n ̃gl =\n\n1 − ρ2 ̃ζl + ρvl + ργ0Dlhl\n\n ̃Gl = ρ ̃Gl+1\n\n(58)\n\n(59)\n\n(60)\n\n∂\n\n∂r1 h1 = 0 which implies A1 = 0. Similarly we have\n\nWe note that 0. Proceeding inductively, we find Al = 0. Similarly, we note that ∂ ̃gL Inductively, we have Dl = 0 for all l. Using these facts, we thus find the following equations\n\n∂r2 h2 = 0. Thus A2 = ∂uL = 0 so DL−1 = 0.\n\n∂\n\nhl = ul + γ0(Cl−1 + H l−1 gl = rl + γ0(Bl + Gl+1 ∆ )hl ̃gl =\n\n1 − ρ2 ̃ζl + ρvl\n\n(cid:112)\n\n∆ ) ̃gl\n\n27\n\n(61)\n\n(62)\n\n(63)\n\nPublished as a conference paper at ICLR 2023\n\nWe can close these equations for H l and ̃Gl\n\nH l = H l−1 + γ2\n\n0\n\n ̃Gl = ρ ̃Gl+1 + γ2 Cl = ρ (cid:0)Cl−1 + H l−1\n\n0\n\n(cid:0)Cl−1 + H l−1 (cid:0)Bl + Gl+1\n\n∆\n\n(cid:1) ̃ ̃Gl+1 (cid:0)Cl−1 + H l−1 (cid:1) ̃ ̃Gl+1\n\n∆\n\n∆\n\n(cid:1) (cid:0)Cl−1 + H l−1 (cid:1) , Bl = Bl+1 + Gl+2 ∆ .\n\n∆\n\n∆\n\n(cid:1)⊤\n\n(64)\n\nThe matrices (cid:0)Cl−1 + H l−1\n\n∆\n\n ̃ ̃Gl = 11⊤ are all rank one. Thus it suffices to compute the vectors cl = (cid:1) 1. Further, it suffices to consider dl = ̃Gl1/|1|2. With this formalism we have\n\nH l = H l−1 + γ2\n\n0 clcl⊤ , dl = ρdl+1 + γ2\n\n0 (Bl + Gl+1\n\n∆ )cl.\n\n(65)\n\nThe analysis for DFA and Hebb rules is very similar.\n\nF EXACTLY SOLVEABLE 2 LAYER LINEAR MODEL\n\nF.1 GRADIENT FLOW\n\nBased on the prior results from (Bordelon & Pehlevan, 2022), the Hy = y⊤Hy/|y|2 dynamics for GD are coupled to the dynamics for the error ∆(t) = 1\n\n|y| y · ∆(t) have the form\n\nd dt\n\nHy(t) = 2γ2\n\n0 (y − ∆)∆ ,\n\nd dt\n\n∆ = −2Hy∆.\n\n(66)\n\nThese dynamics have the conservation law d law from time 0 to time t, we find Hy(t)2 = 1 + γ2 ODE for ∆(t), giving the following simplified dynamics\n\ny = γ2\n\ndt H 2\n\n0\n\nd\n\ndt (y − ∆)2. Integrating this conservation 0 (y − ∆(t))2. We can therefore solve a single\n\nd dt\n\n∆ = −2\n\n(cid:113)\n\n1 + γ2\n\n0 (y − ∆)2∆ , Hy =\n\n(cid:113)\n\n1 + γ2\n\n0 (y − ∆)2.\n\n(67)\n\nThese dynamics interpolate between exponential convergence (at small γ0) and a logistic convergence (at large γ0) of ∆(t) to zero. Since ∆ → 0 at late time, the final value of the kernel alignment is Hy = (cid:112)1 + γ2\n\n0 y2.\n\nF.2 ρ-ALIGNED FA\n\nFor the two layer linear network, the ρ-FA field dynamics are\n\nd dt\n\nhμ(t) = γ0\n\n(cid:88)\n\nν\n\n ̃gν(t)∆ν(s)K x\n\nμν ,\n\nd dt\n\ngμ(t) = γ0\n\n(cid:88)\n\nν\n\n∆ν(t)hν(t).\n\n(68)\n\nFA we have ̃gμ(t) = ̃g ∼ N (0, 1) which is a constant standard normal. We let aμ(t) = ⟨ ̃ghμ(t)⟩. The dynamics for Hμν and aμ are coupled\n\nd dt\n\nHμν = γ0aν(t)\n\n(cid:88)\n\nν\n\n∆ν(t)K x\n\nμν + γ0aμ(t)\n\n(cid:88)\n\nν\n\n∆ν(t)K x\n\nμν\n\nd dt\n\naμ(t) = γ0\n\nd dt\n\n ̃G(t) = γ0\n\n(cid:88)\n\nν (cid:88)\n\nμ\n\n∆νK x\n\nμν\n\n∆μ(t)aμ(t) ,\n\nd dt\n\n∆μ(t) = −\n\n(cid:88)\n\n[Hμν(t) + ̃G(t)K x\n\nμν]∆ν(t).\n\n(69)\n\nν\n\nWhitening the dataset Kx = I and projecting all dynamics on ˆy subspace gives the reduced dynamics\n\nd dt\n\nH = 2γ0a∆ ,\n\nd dt\n\na = γ0∆ ,\n\nd dt\n\n ̃G = γ0∆a ,\n\nd dt\n\n∆ = −[H + ̃G]∆.\n\n(70)\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nFrom these dynamics we identify the following set of conservation laws\n\n2\n\nd dt\n\n ̃G =\n\nd dt\n\na2 =\n\nd dt\n\nH\n\n=⇒ 2 ̃G − 2ρ = a2 = H − 1.\n\n(71)\n\nWriting everything in terms of ∆, a we have\n\nd dt\n\na = γ0∆ ,\n\nd dt\n\n∆ = −\n\n(cid:20) 3 2\n\n(cid:21)\n\na2 + (1 + ρ)\n\n∆ = −γ−1\n\n0\n\nd dt\n\nIntegrating both sides of this equation from 0 to t gives ∆ = y − γ−1 a dynamics now one dimensional, giving\n\n0\n\na3 + (1 + ρ)a\n\n(cid:21)\n\n(cid:20) 1 2\n(cid:2) 1\n\n2 a3 + (1 + ρ)a(cid:3). Thus, the\n\nd dt\n\na = γ0y −\n\na3 − (1 + ρ)a.\n\n1 2\nWhen run from initial condition a = 0, this will converge to the smallest positive root of the cubic 2 a3 + (1 + ρ)a = γ0y. This implies that, for small γ0 we have a ∼ γ0y equation 1 1+ρ so that ∆H = 2∆ ̃G ∼ γ2 (1+ρ)2 and so that larger initial alignment ρ leads to smaller changes in the feature kernel and pseudo-gradient alignment kernel. At large γ0y, we have that a ∼ (2γ0y)1/3 so that ∆H = 2∆ ̃G ∼ (2γ0y)2/3.\n\n(72)\n\n0 y2\n\nF.3 HEBB\n\nFor the Hebb rule, ̃Gμ = ⟨ghμ⟩ ∆μ = γ0fμ∆μ = γ0(yμ−∆μ)∆μ. Under the whitening assumption K x\n\nμν = δμν, the dynamics decouples over samples\n\nd dt\n\nHμ,μ = 2γ0Hμμ∆2\n\nμ ,\n\nd dt\n\n∆μ = −[Hμμ + γ0(yμ − ∆μ)∆μ]∆μ.\n\n(73)\n\n(cid:104)\n\n(cid:105)\n\n(cid:113)\n\nyμ ±\n\n0 Hμμ\n\nμ + γ−1 y2\n\nWe see that Hμμ strictly increases. The possible fixed points for ∆μ are ∆μ = 0 or ∆μ = 1\n. One of these roots shares a sign with yμ and has larger absolute value. 2\nThe other root has the opposite sign from yμ. From the initial condition ∆μ = yμ and Hμμ = 1, ∆μ is initially approaching decreasing in absolute value so that |∆μ| ∈ (0, |yμ|) and will have the same sign as yμ. In this regime d dt |∆μ| < 0. Thus, the system will eventually reach the fixed point at ∆μ = 0, rather than increasing in magnitude to the root which shares a sign with yμ or continuing to the root with the opposite sign as yμ.\n\nG DISCUSSION OF MODIFIED HEBB RULE\n\nWe chose to modify the traditional Hebb rule to include a weighing of each example by its instantaneous error. In this section we discuss this choice and provide a brief discussion of alternatives\n\n• Traditional Hebb Learning: d\n\nμ φ(hl+1)φ(hl)⊤. In the absence of regularization or normalization, this learning rule will continue to update the weights even once the task is fully learned, leading to divergences at infinite time t → ∞.\n\ndt W l ∝ (cid:80)\n\ndt W l ∝ (cid:80)\n\n• Single Power of the Error: d\n\nμ ∆μφ(hl+1)φ(hl)⊤. While this rule may naively appear plausible, it can only learn training points with positive target values yμ in a linear network if γ0 > 0. Further this rule only gives Hebbian updates when ∆μ > 0. dt W l ∝ (cid:80)\n\nμφ(hl+1)φ(hl)⊤. This was our error modified Hebb rule. We note that the update always has the correct sign for a Hebbian update and the updates stop when the network converges to zero error, preventing divergence of the features at late time.\n\n• Two Powers of the Error: d\n\nμ ∆2\n\nH FINITE SIZE EFFECTS\n\nWe can reason about the fluctuations of q around the saddle point q∗ at large but finite N using a Taylor expansion of the DMFT action S around the saddle point. This argument will show that\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nat large but finite N , we can treat q as fluctuating over initializations with mean q∗ and variance O(N −1). We will first illustrate the mechanics of this computation of an arbitrary observable with a scalar example before applying this to the DMFT.\n\nH.1 SCALAR EXAMPLE\n\nSuppose we have a scalar variable q with a distribution defined by Gibbs measure\n\ne−N S[q]\n\n(cid:82) dqe−N S[q] for\n\naction S. We consider averaging some arbitrary observable O(q) over this distribution\n\n⟨O(q)⟩ =\n\n(cid:82) dq exp (−N S[q]) O(q) (cid:82) dq exp (−N S[q])\n\n.\n\n(74)\n\nWe Taylor expand S around its saddle point q∗ giving S[q] = S[q∗] + 1 (cid:80)∞\n\n2 S′′[q∗](q − q∗)2 +\n\nk=3 S(k)[q∗](q − q∗)k. This gives (cid:82) dq exp (cid:0)−N [ 1\n\n⟨O(q)⟩ =\n\n2 S′′[q∗](q − q∗)2 − (cid:80)∞\n\n(cid:82) dq exp (cid:0)−N [ 1\n\n2 S′′[q∗](q − q∗)2 − (cid:80)∞\n\nk=3 S(k)[q∗](q − q∗)k](cid:1) O(q) k=3 S(k)[q∗](q − q∗)k](cid:1)\n\n.\n\n(75)\n\nThe exp(N S[q∗]) terms canceled in both numerator and denominator. We let the variable q − q∗ = 1√\n\nδ. After this change of variable, we have\n\nN\n\n⟨O(q)⟩ =\n\n(cid:82) dδ exp (cid:0)− 1\n\n2 S′′[q∗]δ2 − (cid:80)∞\n\nk=3 N 1−k/2S(k)[q∗]δk(cid:1) O(q∗ + N −1/2δ) k=3 N 1−k/2S(k)[q∗]δk(cid:1)\n\n2 S′′[q∗]δ2 − (cid:80)∞\n\n(cid:82) dδ exp (cid:0)− 1\n\n.\n\n(76)\n\nWe note that all the higher order derivatives (k ≥ 3) are suppressed by at least N −1/2 compared to the quadratic term. Letting U = (cid:80)∞ k=3 N 1−k/2S(k)[q∗]δk represent the perturbed potential, we can 2 δ2S′′[q∗](cid:1). We Taylor expand the exponential around the Gaussian unperturbed potential exp (cid:0)− 1 let ⟨O(δ)⟩0 = Eq∼N (0,S′′[q∗]−1)O(δ) represent an average over this unperturbed potential\n\n⟨O(q)⟩ =\n\n=\n\n(cid:82) dδ exp (cid:0)− 1\n\n2 S′′[q∗]δ2(cid:1) [1 − U + 1\n\n(cid:82) dδ exp (cid:0)− 1\n\n2 S′′[q∗]δ2(cid:1) [1 − U + 1 (cid:10)O(q)U 2(cid:11)\n\n⟨O(q)⟩0 − ⟨O(q)U ⟩0 + 1\n\n2\n\n2 U 2 + ...]O(q) 2 U 2 + ...] 0 + ...\n\n.\n\n1 − ⟨U ⟩0 + 1\n\n2 ⟨U 2⟩0 + ...\n\n(77)\n\n(78)\n\nTruncating each series in numerator and denominator at a certain order in 1/N gives a PadeApproximant to the full observable average (Bender et al., 1999). Alternatively, this can be expressed in terms of a cumulant expansion (Kardar, 2007)\n\n⟨O⟩ =\n\n∞ (cid:88)\n\nk=0\n\n(−1)k k!\n\n(cid:10)OU k(cid:11)c 0 ,\n\n(79)\n\nwhere (cid:10)OU k(cid:11)c nected correlations have the form\n\n0 are the connected correlations, or alternatively the cumulants. The first two con-\n\n⟨OU ⟩c (cid:10)OU 2(cid:11)c\n\n0 = ⟨OU ⟩0 − ⟨O⟩ ⟨U ⟩0 0 = (cid:10)OU 2(cid:11)\n\n0 − 2 ⟨OU ⟩0 ⟨U ⟩0 − ⟨O⟩0\n\n(cid:10)U 2(cid:11)\n\n0 + 2 ⟨O⟩ ⟨U ⟩2 0 .\n\n(80)\n\nUsing Stein’s lemma, we can now attempt to extract the leading O(N −1) behavior from each of these terms. First, we will note the following useful identity which follows from Stein’s Lemma\n\n(cid:10)O(q)δk(cid:11) = N k/2 (cid:10)O(q)(q − q∗)k(cid:11)\n\n= N k/2−1[S′′]−1[(k − 1) (cid:10)O(q)(q − q∗)k−2(cid:11) + (cid:10)O′(q)(q − q∗)k−1(cid:11)] 1\n√\n\n= (k − 1)[S′′]−1 (cid:10)O(q)δk−2(cid:11) +\n\n[S′′]−1 (cid:10)O′(q)δk−1(cid:11) .\n\nN\n\n(81)\n\n(82)\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nUsing these this fact, we can find the first few correlation functions of interest\n\n⟨O(q)U ⟩ = 3N −1S(3)[S′′]−2 ⟨O′(q)⟩0 + 3N −1S(4)[S′′]−2 ⟨O(q)⟩0 + O(N −2)\n\n(cid:10)O(q)U 2(cid:11) = 15N −1[S(3)]2[S′′]−3 ⟨O(q)⟩ + O(N −1).\n\n(83)\n\nThus, the leading order Pade-Approximant has the form\n\n⟨O(q)⟩ =\n\n⟨O⟩0 − 3\n\nN S(3)[S′′]−2 ⟨O′(q)⟩0 − 3\n\nN S(4)[S′′]−2 ⟨O(q)⟩0 + 15\n\n2N [S(3)]2[S′′]−3 ⟨O(q)⟩\n\n1 − 3\n\nN S(3)[S′′]−2 − 3\n\nN S(4)[S′′]−2 + 15\n\n2N [S(3)]2[S′′]−3\n\n.\n\n(84)\n\nH.1.1 DMFT ACTION EXPANSION\n\nThe logic of the previous section can be extended to our DMFT. We first redefine the action as its negation S → −S to simplify the argument. Concretely, this action S[q] defines a Gibbs measure over the order parameters q which we can use to compute observable averages\n\n⟨O(q)⟩ =\n\n(cid:82) exp (−N S[q]) O(q) (cid:82) exp (−N S[q])\n\nAs before, one can Taylor expand the action around the saddle point q∗\n\nS[q] ∼ S[q∗] +\n\n1 2\n\n(q − q∗)∇2S[q∗](q − q∗) + ...\n\n(85)\n\n(86)\n\nAs before, the linear term vanishes since ∇qS[q∗] = 0 at the saddle point q∗. We again change variables to δ =\n\nN (q − q∗) and express the average as\n\n√\n\n⟨O⟩ =\n\n(cid:82) dδ exp (cid:0)− 1\n\n(cid:82) dδ exp (cid:0)− 1\n\n2 δ⊤∇2Sδ + U (δ)(cid:1) O(δ) 2 δ⊤∇2Sδ + U (δ)(cid:1)\n\n=\n\n∞ (cid:88)\n\nk=0\n\n(−1)k k!\n\n(cid:10)OU k(cid:11)c\n\n0\n\n(87)\n\nwhere ⟨⟩0 denotes a Gaussian average over q ∼ N (q∗, 1\n\nN [∇2S]−1).\n\nH.2 HESSIAN COMPONENTS OF DMFT ACTION\n\nTo gain insight into the Hessian, we will first restrict our attention to the subset of Hessian entries related to Φl, ˆΦl. We again adopt a multi-index notation μ = (μ, ν, t, s) so that Φl\n\nμ = Φl\n\nμν(t, s)\n\nμ′\n\n∂2S μ∂Φl′ ∂2S μ∂ ˆΦl′ ∂2S μ∂ ˆΦl′\n\nμ′\n\nμ′\n\n∂Φl\n\n∂Φl\n\n∂ ˆΦl\n\n= 0\n\n= δl,l′δμ,μ′ − δl′,l+1\n\n∂ ∂Φl μ\n\nΦl+1 μ′\n\n= δl,l′\n\n(cid:2)(cid:10)φ(hl\n\nμ(t))φ(hl\n\nν(s))φ(hl\n\nμ′(t′))φ(hl\n\nν′(s′))(cid:11) − Φl\n\nμΦl\n\nμ′\n\n(cid:3) .\n\nThe first equation follows from the fact that ˆχ has vanishing moments due to the normalization of the probability distribution induced by Z l. Similarly, for the G, ˆG kernels we have\n\nμ′\n\n∂2S μ∂Gl′ ∂2S μ∂ ˆGl′ ∂2S μ∂ ˆΦl′\n\nμ′\n\nμ′\n\n∂Gl\n\n∂Gl\n\n∂ ˆΦl\n\n= 0\n\n= δl,l′δμ,μ′ − δl′,l−1\n\n∂ ∂Gl μ\n\nGl−1 μ′\n\n= δl,l′\n\n(cid:2)(cid:10)gl\n\nμ(t)gl\n\nν(s)gl\n\nμ′(t′)gl\n\nν′(s′)(cid:11) − Gl\n\nμGl\n\nμ′\n\n(cid:3) .\n\nProceeding in a similar manner, we can compute all off-diagonal components such as ∂2S ∂Φ∂ ˆG ∂2S ∂ ˆΦ∂ ˆG of the order parameters.\n\n. Once all entries are computed, one can seek an inverse of the Hessian to obtain the covariance\n\nand\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nH.3 SINGLE SAMPLE NEXT-TO-LEADING ORDER PERTURBATION THEORY\n\nIn order to obtain exact analytical expressions, we will consider L-hidden layer ReLU and linear neural networks in the lazy regime trained on a single sample with K x = |x|2 D = 1. To ensure preservation of norm, we will use φ(h) = 2hΘ(h) for ReLU and φ(h) = h for linear networks. First, we note that in either case, the infinite width saddle point equations give\n\n√\n\nΦl = (cid:10)φ(h)2(cid:11)\n\nh∼N (0,Φl−1) = Φl−1 , Φ0 = 1\n\nGl =\n\n(cid:68) ̇φ(h)2z2(cid:69)\n\nh,z\n\n= Gl+1 , GL+1 = 1\n\n=⇒ Φl = 1 , Gl = 1 , ∀l ∈ [1, ..., L].\n\n(88)\n\nAt large but finite width, the kernels therefore fluctuate around this typical mean value of Φl = 1 and Gl = 1. We now compute the necessary ingredients to invert the Hessian\n\nVφ = (cid:10)φ(h)4(cid:11) − Φ2 =\n\n(cid:68) ̇φ(h)4z4(cid:69)\n\n− G2 =\n\nVg =\n\n(cid:26)5 ReLU 2 Linear (cid:26)5 ReLU 2 Linear\n\n.\n\nNext, we compute the sensitivity of each layer’s kernel to the previous layer\n\n∂\n\n∂Φl Φl+1 = 1 ,\n\n∂\n\n∂Gl+1 Gl = 1.\n\n(89)\n\n(90)\n\nFirst, let’s analyze the marginal covariance statistics for Φ = Vec{Φl}L We note that the DMFT action has Hessian components\n\nl=1 and ˆΦ = Vec{ ˆΦl}L\n\nl=1.\n\nHΦ =\n\n(cid:20) ∇2 ∇2\n\nΦS ∇2 S ∇2 ˆΦ\n\nS Φ ˆΦ S\n\nˆΦΦ\n\n(cid:21)\n\n=\n\n(cid:21)\n\n(cid:20) 0 U\nU ⊤ VφI\n\n, U =\n\n\n\n \n \n\n\n...\n\n0 0\n\n1 −1 0\n\n0 1 −1 . . . 0\n0\n\n0 0\n\n...\n\n... ... . . . ... ...\n\n\n\n \n \n\n\n.\n\n0 0\n\n...\n\n0 0\n\n...\n\n1 −1 1\n0\n\n(91)\n\nWe seek a (physical) inverse C which has vanishing lower diagonal entry, indicating zero variance in the dual order parameters ˆΦ. This gives us the following linear equations (cid:20) 0 U\nU ⊤ VφI\n\n(cid:21) (cid:20)C11 C12 0\n\nHΦC =\n\n(cid:20)I 0\n\n0 I\n\n=\n\n(cid:21)\n\n(cid:21)\n\nC⊤ 12\n\n=⇒ U C ⊤\n\n12 = I , U ⊤C11 + VφC⊤\n\n12 = 0.\n\nThe relevant entry is C11 = −Vφ[U ⊤]−1U −1. This matrix has the form \n\n\n\n\n\n\n\n\n\n1 1\n1\n\n...\n\n1\n\n \n \n\n\n0 1\n1\n\n...\n\n1\n\n0 0\n1 . . . 1\n\n... ... ... . . . ...\n\n0 0\n0\n\n...\n\n \n \n\n\n \n \n\n\n1 0\n0\n\n...\n\n1\n\n0\n\n1 1\n0\n\n...\n\n0\n\n1 1\n1 . . . 0\n\n... ... ... . . . ...\n\n1 1\n1\n\n...\n\n \n \n\n\n= −Vφ\n\n1\n\n1\n\n1 1\n1\n\n...\n\n \n \n\n\nC11 = −Vφ\n\n(92)\n\n.\n\n(93)\n\n1 2\n2\n\n...\n\n2\n\n1 2\n3 . . . 3\n\n\n\n \n \n\n\n1 2\n3\n\n... ... ... . . . ... L\n\n...\n\nUsing the fact that the covariance is the negative of the Hessian inverse multiplied by 1/N , we have the following covariance structure for {Φl}\n\n1 N\nThis result can be interpreted as the covariance of Brownian motion. Following an identical argument, we find\n\nVφ min{l, l′}.\n\nCov(Φl, Φl′\n\n(94)\n\n) =\n\nCov(Gl, Gl′\n\n) =\n\n1 N\n\nVg min{L + 1 − l, L + 1 − l′}.\n\n(95)\n\nWe verify these scalings against experiments below in Figure 7.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\n(a) Cross-Layer Φl Covariance\n\n(b) Cross-Layer Gl Covariance\n\n(c) NLO-DMFT Φl Variance Scaling\n\n(d) NLO-DMFT Gl Variance Scaling\n\nFigure 7: Verification of kernel fluctuations through next-to-leading-order (NLO) perturbation theory within DMFT formalism. (a) The cross layer covariance structure of {Φl} in a L = 10 hidden layer ReLU MLP. The empirical covariance was estimated by initializing a large number (500) of random networks and computing their Φl kernels. We see that variance for Φl increases as l increases. (b) The cross-layer covariance structure of {Gl}. The variance of Gl is larger for smaller l. (c) The predicted variance of Φl for different layer l and widths N . All layers have variance scaling as 1/N , consistent with NLO perturbation theory. (d) The scaling of Gl variance.\n\n33\n\n024681024682Empirical Covariance024681024682DMFT Covariance024681024682Empirical G Covariance024681024682DMFT G Covariance101102103N102101100Var DMFT =1DMFT =2DMFT =3DMFT =4DMFT =5DMFT =6101102103N102101100101Var GDMFT =1DMFT =2DMFT =3DMFT =4DMFT =5DMFT =6",
    "reference": "# Summary Of The Paper\n\nThis paper studies the learning dynamics of infinite-width networks trained using various biologically plausible variations to gradient descent, including feedback alignment, direct feedback alignment, error modulated Hebbian learning, and gated linear networks. The paper derives an effective Neural Tangent Kernel (NTK) that governs the learning dynamics. Through theory and simulation, the paper identifies a number of interesting phenomena about the learning dynamics of these biologically plausible learning rules.\n\n# Strength And Weaknesses\n\nStrengths:\n- Nice use of the theory of infinite width neural networks to study learning dynamics in networks trained with variants of gradient descent.\n- Identifies a number of interesting phenomena regarding similarities and differences between the representations learned using the different learning rules.\n\nWeaknesses:\n-  Analysis is limited to MLP networks\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOverall, the paper is clearly written. To my knowledge the contributions are novel.\n\n# Summary Of The Review\n\nOverall, I think this paper is an important contribution in our understanding of the learning dynamics of networks trained with biologically plausible (or at least biologically inspired) learning rules. Feedback Alignment and subsequent variants are an intriguing learning rule because they seem so far from gradient descent; this paper helps us understand when and why they work.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLEARNING SPARSE GROUP MODELS THROUGH BOOLEAN RELAXATION\n\nYijie Wang1∗ Yuan Zhou2∗ Xiaoqing Huang3 Kun Huang3 Jie Zhang4 Jianzhu Ma5 1Computer Science Department, Indiana University Bloomington 2Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University 3Department of Biostatistics & Health Data Science, Indiana University 4Department of Medical and Molecular Genetics, Indiana University 5Institute for AI Industry Research, Tsinghua University yijwang@iu.edu {yuan-zhou, majianzhu}@tsinghua.edu.cn;(∗ Equal contribution)\n\nABSTRACT We introduce an efficient algorithmic framework for learning sparse group models formulated as the natural convex relaxation of a cardinality-constrained program with Boolean variables. We provide theoretical techniques to characterize the equivalent condition when the relaxation achieves the exact integral optimal solution, as well as a rounding algorithm to produce a feasible integral solution once the optimal relaxation solution is fractional. We demonstrate the power of our equivalent condition by applying it to two ensembles of random problem instances that are challenging and popularly used in literature and prove that our method achieves the exactness with overwhelming probability and the nearly optimal sample complexity. Empirically, we use synthetic datasets to demonstrate that our proposed method significantly outperforms the state-of-the-art group sparse learning models in terms of individual and group support recovery when the number of samples is small. Furthermore, we show the out-performance of our method in cancer drug response prediction.\n\n1\n\nINTRODUCTION\n\nSparsity is one of the most important concepts in statistical machine learning, which strongly connects to the data & computational efficiency, generalizability, and interpretability of the model. Traditional sparse estimation tasks aim at selecting sparse features at the individual level Tibshirani (1996); Negahban et al. (2012). However, in many real-world scenarios, structural properties among the individual features are assumed thanks to prior knowledge, and leveraging these structures may improve both model accuracy and learning efficiency Gramfort & Kowalski (2009); Kim & Xing (2012). In this paper, we focus on learning the sparse group models for intersection-closed group sparsity, where groups of variables are either selected or discarded together. The general task of learning the sparse group models has been investigated quite a lot in literature, where most of the prior studies are based on the structured sparsity-inducing norm regularization Friedman et al. (2010); Huang et al. (2011); Zhao et al. (2009); Simon et al. (2013), which stems from Lasso Tibshirani (1996), the traditional and popular technique for a sparse estimate at the individual feature level. As reviewed in Bach et al. (2012); Jenatton et al. (2011), the structured sparsity-inducing norm is quite general and can encode structural assumptions such as trees Kim & Xing (2012); Liu & Ye (2010), contiguous groups Rapaport et al. (2008), directed-acyclic-graphs Zheng et al. (2018), and general overlapping groups Yuan et al. (2011). Another type of approach for learning the sparse group models is to view the task as a cardinalityconstrained program, where the constraint set encodes the group structures as well as restricts the number of groups of variables being selected. Baldassarre et al. Baldassarre et al. (2013) investigate the projection onto such cardinality-constrained set. However, due to the combinatorial nature of the projection, directly applying the projected gradient descent with the projection Baldassarre et al. (2013) to solve general learning problems with typical loss functions might not have good results Kyrillidis et al. (2015). Recent work Pilanci et al. (2015) studies the Boolean relaxation of the learning problem with cardinality constraints on the individual variables. This work Pilanci et al. (2015) can be viewed as a special case of sparse group models, where each group contains only one variable. Both the original work of Pilanci et al. (2015) and several follow-up papers Bertsimas & Parys (2020); Bertsimas et al. (2020) show that the Boolean relaxation empirically outperforms the sparse estimation methods using sparse-inducing norms (Lasso Tibshirani (1996) and elastic net Zou & Hastie (2005)), especially when the sample size is small and the feature dimension is\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nlarge. However, the results in Pilanci et al. (2015) cannot be applied to the sparse group models with arbitrary group structures. To fill the gap, in this paper, we study the sparse group models through a cardinality-constrained program. We first propose the Boolean relaxation for sparse group models. We further establish an analytical and algorithmic framework for our Boolean relaxation which includes a theorem stating the equivalent condition for the relaxation to achieve the exactness (i.e., the optimal integral solution) and a rounding scheme that produces an integral solution when the optimal relaxation solution is fractional. We demonstrate the power of our equivalent condition theorem by applying it to two ensembles of random problem instances that are challenging and popularly used in literature and proving that our Boolean relaxation achieves the exactness with high probability and the nearly optimal sample complexity. Our contributions are threefold: 1) We propose a novel framework that uses constraints to induce intersection-closed group sparsity. Baldassarre et al.Baldassarre et al. (2013) investigate the projection on the group sparsity constraints. But our framework extends to any convex loss function with the group sparsity constraints.\n\n2) We prove our framework is tight and can achieve the exactness with high probability and the nearly optimal sample complexity for two ensembles of random problem instances. This result is inspired by Pilanci et al. (2015) but our derivations and proofs are not straightforward extensions (e.g., due to the group structure, we need to analyze more complex feature-group matrices, prove new matrix concentration properties, and carefully choose different regularization parameters).\n\n3) Empirically, we perform extensive experiments to demonstrate that our framework significantly outperforms the state-of-the-art methods when the sample size is small on simulated datasets. Furthermore, we show the out-performance of our framework in cancer drug response prediction.\n\n1.1 RELATED WORKS\n\nConvex programming relaxations and their rounding techniques have been widely used for approximating many combinatorial optimization problems that are computationally intractable (see, e.g., Williamson & Shmoys (2011)). The specific algorithmic technique in this work is inspired by the Boolean relaxation method introduced in Pilanci et al. (2015) for learning sparsity at the individual feature level. However, the additional group structure in our problem raises new algorithmic challenges, and both our Boolean relaxation formulation and its theoretical analysis (e.g., the equivalent condition for the exactness) are different from their counterparts Pilanci et al. (2015). As mentioned before, sparse estimation using structured sparsity-inducing norms were thoroughly studied for learning structured sparsity under different structure assumptions motivated by various practical scenarios Friedman et al. (2010); Huang et al. (2011); Zhao et al. (2009); Simon et al. (2013); Tibshirani (1996); Bach et al. (2012); Kim & Xing (2012); Liu & Ye (2010); Rapaport et al. (2008); Zheng et al. (2018); Yuan et al. (2011); Jenatton et al. (2011). However, none of these algorithms provides the rigorous theoretical techniques as in this work to verify whether the algorithm has produced the exact optimal solution. Also, as we will show in the experiments section, our proposed method outperforms these algorithms on both synthetic and real-world datasets. In our experiments, we also compare with the elastic net method Zou & Hastie (2005), which can only control the sparsity at the individual feature level. There exist another family of structured sparsity-inducing norms Jacob et al. (2009) that aim to model the union-closed families of supports, where the support of the solution is a union of groups. Different from our proposed models, in which the support of the solution is the intersection of the complements of some of groups considered (intersection-closed group sparsity) Jenatton et al. (2009). Another approach is to learn sparse group models by introducing the penalty functions for the constraints and applying the convex relaxation to them. Bach Bach (2010) investigate to design norms from submodular set-functions. Halabi et al. El Halabi & Cevher (2015); Halabi et al. (2018) study to induce group sparsity using tight convex relaxation of linear matrix inequalities and combinatorial penalties. Note that these works use convex regularizers to induce group sparsity while we use constraints. Halabi et al. El Halabi & Cevher (2015); Halabi et al. (2018) study general equivalent conditions to characterize the tightness of their relaxations while our theoretical results works for specific distributions where their general conditions cannot be easily verified. We use different analytical frameworks and thus the theoretical results cannot be directly compared.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 BOOLEAN RELAXATION FOR SPARSE GROUP MODEL\n\nThe organization of this section is as follows. In section 2.1, we introduce the original problem and its exact boolean representation. In section 2.2, we propose the boolean relaxed program and provide the condition under which the relaxed program is guaranteed to have an integer solution and hence be tight. In section 2.3, we propose the rounding strategy if the relaxed program does not generate integral solutions.\n\n2.1 SPARSE GROUP MODEL AND ITS FORMULATION VIA BOOLEAN CONSTRAINTS We consider a learning problem for a collection of n samples {(xi, yi) ∈ Rd × Y}n i=1 and define i ∈ Rd is the i-th row of X. This setup is flexible to the design matrix as X ∈ Rn×d, where x⊤ model various problems including binary classification (where the label space Y = {−1, +1}) and regression problems (where the label space Y = R). For a linear model x (cid:55)→ w⊤x, our goal is to learn a sparse weight vector w ∈ Rd whose support encodes certain structures reflecting the relationships among the features which are usually defined by the prior knowledge. More formally, we need to solve the following mathematical program.\n\n(cid:40)\n\nP ∗ = min\n\nw∈Θ\n\nF (w) :=\n\nn (cid:88)\n\ni=1\n\nf (w⊤xi; yi) +\n\n1 2\n\n(cid:41)\n\nρ∥w∥2\n\n2\n\n.\n\n(1)\n\nHere, the loss function f (·; ·) measures the prediction error by our linear model, where the common choices include the squared loss for least-squares regression, the log loss for the logistic regression, and the hinge loss for the support vector machine. The regularization term 1 2 in (1) makes sure that the objective function is strongly convex and therefore has a unique optimal solution w∗ ∈ Rd. Finally, the constraint set Θ encodes the sparsity requirements for both individual features and groups of features. We use gi to denote the set of the indices of features in the i-th group and for any vector w ∈ Rd, we use wgi to denote the vector containing all entries of w corresponding to the indices in gi. We further assume that we have b predefined groups and then the cardinality constraint set Θ can be written as\n\n2 ρ∥w∥2\n\nΘ =\n\nw ∈ Rd\n\n∥w∥0 ≤ k,\n\n1 (cid:2)(cid:13)\n\n(cid:13)wgj\n\n(cid:13)\n\n(cid:13)0 > 0(cid:3) ≤ h\n\n,\n\n(2)\n\n \n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n \n\n\n\nb (cid:88)\n\nj=1\n\nwhere ∥ · ∥0 denotes the l0 norm and 1[·] denotes the indicator variable that takes the value 1 when the corresponding condition holds and 0 otherwise. The first constraint enforces the number of contributing features to be less than k, and the second constraint makes sure the number of groups that contain those selected features is less than h. We also remark that the structured sparsity constraints defined by Θ in equation 2 is very flexible. First, the ∥w∥0 ≤ k constraint imposes the feature-level sparsity requirement and encompasses the unstructured sparsity model (as investigated in Pilanci et al. (2015)) as a special case. Second, the group-level sparsity constraint is introduced by 1 (cid:2)(cid:13) sparsity arising from many practical scenarios such as neuroimaging Gramfort & Kowalski (2009); Xi et al. (2009), genomic analysis Rapaport et al. (2008); Kim & Xing (2012), and wavelet-based denoising Zhao et al. (2009); Huang et al. (2011). The groups {gi}i∈{1,2,...,b} can be arbitrary sets of the features and may model not only non-overlapping structured sparsity when gi ∩ gj = ∅, ∀i, j but also various overlapping patterns including the contiguous pattern, the block pattern, and the hierarchical pattern as reviewed in Bach et al. (2012); Jenatton et al. (2011). Note that the first term in equation 2 can be absorbed into the second term, which however will not have the sparsity control at the individual level. Note that the structured sparse group learning problem P ∗ defined in equation 1 involves only real variables. In the following theorem, we show that the problem can be reformulated as a convex program with additional Boolean variables and constraints, which will naturally lead to the Boolean relaxation algorithm in the later sections.\n\n(cid:13)0 > 0(cid:3) ≤ h covers the needs for structured\n\n(cid:13)wgj\n\n(cid:13)\n\nTheorem 2.1 (Exact representation with Boolean constraints). Suppose that for each y ∈ Y, the function t (cid:55)→ f (t; y) is closed and convex. The Legendre-Fenchel conjugate of f is f ∗(s; y) := supt∈R{st − f (t : y)}. Then for any ρ > 0, the structured sparse learning problem P ∗ in equation 1 can be represented by the following Boolean convex program\n\nP ∗ = min\n\n(u,z)∈Γ\n\nmax v∈Rn\n\n(cid:124)\n\n(cid:40)\n\nn (cid:88)\n\ni=1\n\nf ∗(vi; yi)\n\n−\n\n1 2ρ\n\nv⊤XD(u)X ⊤v −\n\n(cid:123)(cid:122) H(u)\n\n3\n\n(cid:41) ,\n\n(cid:125)\n\n(3)\n\nPublished as a conference paper at ICLR 2023\n\nwhere D(u) := diag(u) is a diagonal matrix with u ∈ Rd on its diagonal, and Γ is the constraint set for u and v, defined as the follows,\n\nΓ =\n\n \n\n\n\n(u, z)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nd (cid:88)\n\ni=1\n\nui ≤ k,\n\nb (cid:88)\n\nj=1\n\nzj ≤ h,\n\nui ≤ zj,\n\n∀i ∈ gj,\n\nu ∈ {0, 1}d,\n\nz ∈ {0, 1}b\n\n \n\n\n\n.\n\nThe proof of Theorem 2.1 can be found in the supplementary materials Section A. In the theorem statement, u is a vector of the Boolean indicators for the supports of the individual features and z is also a vector of the Boolean indicators for the supports of the group features. H(u) in equation 3 is convex in u because it is the maximum of a family of functions that are linear with u. However, the whole program is still computationally difficult due to the Boolean constraints u ∈ {0, 1}d and z ∈ {0, 1}b. In the next subsection, we will relax these Boolean constraints which leads to a convex program that can be efficiently solved for many popular loss functions f .\n\n2.2 CONVEX PROGRAM THROUGH BOOLEAN RELAXATION AND THEORETICAL CONDITIONS\n\nFOR EXACTNESS\n\nWe apply interval relaxation to both Boolean vector variables u and z, and obtain the Boolean relaxation for P ∗ as follows.\n\nPBR = min\n\n(u,z)∈Ω\n\nmax v∈Rn\n\n(cid:40)\n\n−\n\n1 2ρ\n\nv⊤XD(u)X ⊤v −\n\nn (cid:88)\n\ni=1\n\n(cid:41)\n\nf ∗(vi; yi)\n\n,\n\n(4)\n\nwhere\n\nΩ =\n\n \n\n\n\n(u, z)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nd (cid:88)\n\ni=1\n\nui ≤ k,\n\nb (cid:88)\n\nj=1\n\nzj ≤ h,\n\nui ≤ zj,\n\n∀i ∈ gj,\n\nu ∈ [0, 1]d,\n\nz ∈ [0, 1]b\n\n \n\n\n\n.\n\nPBR is a convex program and can be solved by the sub-gradient-based optimization algorithm Nesterov (2009) if the inner maximization problem can be solved efficiently. In general, PBR can also be converted into a minimax optimization problem and solved by methods in Lin et al. (2020). We now investigate when PBR achieves the exact solution of P ∗, under the assumption that the groups are non-overlapping. Note that PBR is a relaxation of P ∗ as defined in equation 3. The relaxation is exact if and only if the optimal solution in PBR also happens to be integral and therefore feasible in P ∗. The following theorem (proved in the supplementary materials Section B.1) characterizes the equivalent condition for the exactness.\n\nTheorem 2.2. Suppose that each feature belongs to only one group and the optimal integral solution (ˆu, ˆz) for P ∗ as defined in equation 3 selects k features and h groups, then the optimal solution of PBR also recovers (ˆu, ˆz) if and only if there exists non-negative λk and λh, such that\n\nˆv ∈ arg max v∈Rn\n\n(cid:40)\n\n−\n\n1 2ρ\n\nv⊤XD(ˆu)X ⊤v −\n\nn (cid:88)\n\ni=1\n\nf ∗(vi; yi)\n\n(cid:41)\n\n1. For each group i such that ˆzi = 1, it holds that\n\n∀p ∈ gi, ˆup = 1 ⇒ (X ⊤ ∀p ∈ gi, ˆup = 0 ⇒ (X ⊤\n\np ˆv)2 > λk p ˆv)2 ≤ λk.\n\nand\n\n2. For each group i such that ˆzi = 1, it holds that\n\n(cid:88)\n\n((X ⊤\n\np ˆv)2 − λk) > λh.\n\np∈gi,ˆup=1\n\n3. For each group i such that ˆzi = 0,\n\n(cid:88)\n\nmax{(X ⊤\n\np ˆv)2 − λk, 0} ≤ λh.\n\nHere, Xp denotes the p-th column of the design matrix X.\n\np∈gi\n\nThe special case of least-squares regression. Among all candidate choices of the loss function f , the squared loss f (t; y) = 1 2 (t − y)2 for least-squares regression is the most important and popular one with many real-world applications Kim et al. (2007); Nguyen & Rocke (2002); Boulesteix & Strimmer (2007); Fort & Lambert-Lacroix (2004), and the corresponding Legendre-Fenchel conjugate becomes f ∗(s; y) = s2 2 + sy. In this special case of the structured sparse learning for least-squares\n\n4\n\n(5)\n\n(6)\n\n(7)\n\n(8)\n\n(9)\n\nPublished as a conference paper at ICLR 2023\n\nregression, the relaxed convex program PBR becomes the following form. (cid:40) (cid:19)−1\n\nLBR = min\n\n(u,z)∈Ω\n\nG(u) := y⊤\n\nXD(u)X ⊤ + I\n\n(cid:18) 1 ρ\n\n(cid:41)\n\ny\n\n.\n\n(10)\n\nThe detailed derivation of equation 10 can be found in the supplementary materials Section B.2. We let S denote the support of the unique optimal solution u∗ to the original Boolean program L∗ := min\n\n{G(u)} and define the n × n matrix M by M := (cid:0)In + ρ−1XSX ⊤ (12) Now we are ready to apply Theorem 2.2 to the squared loss function and derive the sufficient and necessary condition for the exactness of LBR (assuming non-overlapping groups), as follows. Corollary 2.3. Suppose that each feature belongs to only one group and the optimal integral solution (ˆu, ˆz) selects k features and h groups, then LBR = L∗ if and only if there exist non-negative λk and λh, such that\n\n(u,z)∈Γ\n\n(11)\n\n(cid:1) .\n\nS\n\n1. For each group i such that ˆzi = 1, it holds that\n\n∀p ∈ gi, ˆup = 1 ⇒ (X ⊤ ∀p ∈ gi, ˆup = 0 ⇒ (X ⊤\n\np M y)2 > λk p M y)2 ≤ λk.\n\nand\n\n2. For each group i such that ˆzi = 1, it holds that\n\n(cid:88)\n\np∈gi,ˆup=1\n\n((X ⊤\n\np M y)2 − λk) > λh.\n\n3. For each group i such that ˆzi = 0,\n\n(cid:88)\n\nmax{(X ⊤\n\np M y)2 − λk, 0} ≤ λh.\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\np∈gi\n\nThe proof of Corollary 2.3 can be found in the supplementary materials Section B.3. Corollary 2.3 creates an analysis framework for the exactness of the Boolean relaxation LBR where one only has to construct two scalars λk and λh and verify the conditions in equation 14, equation 15, and equation 16. In Section 3, we will follow this framework to theoretically prove the exactness of LBR for several classes of problem instances that are popularly studied in the literature, demonstrating the power of Corollary 2.3.\n\n2.3 RANDOMIZED ROUNDING\n\nWhen the Boolean relaxation is not exact (i.e., the optimal solution of the Boolean relaxation turns out to be fractional), we describe in this section a rounding method to recover an integral solution. We will apply randomized rounding, a state-of-the-art technique for converting fractional solutions to integer solutions with provable approximation guarantees Pilanci et al. (2015), to the solution of the relaxed problem PBR. Given the fractional solution ̄u ∈ [0, 1]d and ̄z ∈ [0, 1]b, our goal is to recover a feasible Boolean solution u ∈ {0, 1}d and z ∈ {0, 1}b. For simplicity of exposition, we show the rounding scheme for the case when each feature belongs to exactly one group. However, the algorithm could be easily generalized to the cases of overlapping groups. In our rounding algorithm, we first generate a feasible Boolean solution at the group level z ∈ {0, 1}b. For each group j, we independently set zj according to the following probability distribution:\n\n(17) Once the groups are decided, the ui is set to 0 for each feature i that belongs to a non-selected group. For each selected group gj and for each feature i that belongs to the group, we independently set ui according to the following probability distribution:\n\nand Pr[zj = 0] = 1 − ̄zj.\n\nPr[zj = 1] = ̄zj\n\nPr[ui = 1] =\n\nand Pr[ui = 0] = 1 −\n\n.\n\n(18)\n\n ̄ui ̄zj\n\n ̄ui ̄zj\n\nIt is easy to verify that the Boolean solution generated by the method above matches the fractional solution in the sense of expectation E[z] = ̄z and E[u] = ̄u, and furthermore their expected l0 norms are given by E [∥z∥0] = (cid:80)b j=1 ̄zj ≤ h, and E [∥u∥0] = (cid:80)d\n\nj=1 Pr [zj = 1] = (cid:80)b\n\ni=1 (Pr[ ̃ui = 1, ̃zj = 1] + Pr[ ̃ui = 1, ̃zj = 0]) = (cid:80)d\n\ni=1 ̄ui ≤ k.\n\nWith these expectation bounds in hand, applying standard concentration inequalities, we can show that if we let G = maxj |gj| be the size of the largest group, for any δ ∈ (0, 1/3), with probability at least (1 − exp(Ω(−hδ2)) − exp(−Ω(k2δ2/(bG2)))), we have that ∥z∥0 ≤ (1 + δ)h and ∥u∥0 ≤ (1 + δ)k. This means that when the group sizes are relatively small, our randomized rounding scheme produces a nearly optimal solution with high probability. Finally, once we have obtained\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nthe integral solution u, the weight vector w for the original problem equation 1 can be computed by w := arg minw∈Rd F (D( ̃u)w).\n\nValue guarantees for least-squares regression. For least-squares loss, we are also able to establish theoretical guarantees for the value (i.e., H(u) as defined in equation 3) of our rounding scheme. Without loss of generality, let us assume the columns of the design matrix X are normalized, i.e., ∥Xp∥2 ≤ 1 for p ∈ {1, 2, . . . , d} and ∥y∥2 = 1. We have the following theorem and its proof is in the supplementary materials Section C.\n\nTheorem 2.4. Let ( ̄u, ̄z) be the optimal solution to the relaxed program. Let rz be the number of fractional entries in ̄z and let ru be the number of fractional entries in ̄u. Let (u, z) be the integral solution returned by our rounding scheme. For any δ > 0, with probability (1 − δ), it holds that H(u) − P ∗ ≤ O\n\n(cid:16)(cid:112)rz log(rz/δ)G + (cid:112)ru log(ru/δ)\n\n(cid:16) 1\n\n(cid:17)(cid:17)\n\n.\n\nρ\n\n3 THEORETICAL GUARANTEES OF LBR ON ENSEMBLES OF RANDOM\n\nINSTANCES\n\nIn this section, we apply Corollary 2.3 to prove our relaxed program is tight and can achieves the exactness with high probability and the nearly optimal sample complexity for two ensembles of random problem instances. We focus on the case of least-squares regression and its corresponding relaxation LBR. We will introduce two ensembles of random problem instances and theoretically analyze the performance of our LBR on them. The first random ensemble has been popularly used in literature to evaluate the l1 Sparse Group Lasso algorithms Simon et al. (2013); Friedman et al. (2010). The second random ensemble is designed by us. It is more challenging compared to the first ensemble as there is more than one “optimal” weight vector w at the feature level. However, the algorithm has to figure out the one with the most group sparsity. For both ensembles, we will show that our LBR will successfully recover both the group and feature sparsity with overwhelming probability and almost optimal sample complexity (i.e., n – the number of observations).\n\n3.1 RANDOM ENSEMBLE I\n\nThe first class of random instances can be generated as follows (illustrated in Fig. 1(a)). We first generate the random design matrix X ∈ Rn×d with i.i.d. N (0, 1) entries. The d features are divided into b groups where the size of each group is d/b. We will construct the regression weight vector w ∈ Rd such that the first h groups will have non-zero coefficients and the coefficients in the rest of the groups are 0. The number of non-zero coefficients in the j-th group (for j ∈ {1, 2, . . . , h}) is kj and we have that (cid:80)h j=1 kj = k, where k is the total number of non-zero coefficients in w. For each group j ∈ {1, 2, . . . , h}, we arbitrarily choose kj features in the group and randomly set the . Finally, y = Xw + ε, where the noise vector ε ∈ Rn has corresponding coefficient in w to be ± 1√ i.i.d. N (0, γ2) entries. The goal is to identify the support of w and the corresponding coefficients. The following theorem provides the theoretical guarantee that given a sufficient amount of observations, LBR recovers the individual and group level sparsity for this random ensemble. Theorem 3.1. Consider the random instance described above with parameters (n, d, k, γ, b, h) and let y = Xw + ε be the observed response vector. Suppose that γ ≥ 1. Let ρ = n1/2+δ (δ ∈ (0, 1/2)). With probability at least (1 − d exp(−Ω(n2δ/(γ2k))) − d exp(−Ω(n1−2δ)), the relaxed program LBR admits the optimal solution u∗ and z∗ where u∗\n\nj = 1[j ∈ {1, 2, . . . , h}].\n\ni = 1[wi ̸= 0] and z∗\n\nk\n\n√\n\nWe remark that the regularization parameter ρ is set to n1/2+δ in our theorem, while in contrast, ρ n in Pilanci et al. (2015) for the sparse learning problem without group constraints. In was set to fact, if we are only looking for (1 − o(1)) success probability, we can set δ to be as large (close to 1/2) as possible. For example, if we set δ = 1/2 − log log n log n . The success probability is (1 − o(1)) as long as n/(γ2k log n) = ω(1), which means that we only need n = ω((k/γ2) log(k/γ2)) to achieve the (1 − o(1)) success probability, which almost matches the information theoretic lower bound Wainwright (2007) up to the logarithmic factor.\n\n3.2 RANDOM ENSEMBLE II\n\nWe now describe the another class of random instances which is more challenging due to the multiple candidate regression vectors (illustrated in Fig. 2(a)). We first generate a random design matrix X ∈ Rn×d with two candidate k-sparse cross-fit regression vectors w(1), w(2) ∈ Rd, such that both\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nvectors lead to the same expected response (i.e., Xw(1) = Xw(2)). The response vector is generated by y = Xw(1) + ε where ε ∈ Rn has i.i.d. N (0, γ2) entries. The d feature dimensions are divided into b groups of the same size d/b (for simplicity we assume that d is a multiple of b). The groups are arranged in a way so that the non-zero coefficients of w(1) span h groups (where h · d/b ≥ k), and the non-zero coefficients of w(2) span ζh groups (ζ > 1). Given (X, y) (after randomly permuting the indices of the coordinates and the groups), the goal is to identify the support of w(1) since it is sparser at the group level. We next specifically describe how we generate w(1), w(2) and design the groups. For any vector w ∈ Rk with no non-zero entries, we let the first k entries of w(1) filled by w and the rest filled by 0; we also let the (k + 1)-th to the 2k-th entries of w(2) filled by w and the rest filled 0. We let each of the first h groups contain k/h non-zero identical coordinates of w(1), and let each of the next ζh groups contain k/(ζh) non-zero identical coordinates of w(2). We finally fill the b groups with the coordinates numbered from (2k + 1) to d so that each group has the same size d/b. We next describe how we generate the design matrix X. We first generate a random orthogonal matrix Q such that Qw = w. This can be done by first fixing an arbitrary orthogonal matrix with w as its first column (i.e., letting P = [ w , β1, β2, . . . , βk−1]), generating a random (k − 1) × (k − 1) orthogonal matrix Q′, and finally letting Q = P · diag(1, Q′) · P ⊤. We then generate the matrices X1 ∈ Rn×k and X3 ∈ Rn×(d−2k) with i.i.d. N (0, 1) entries. Let X2 = X1Q. Since Q is orthogonal, it is easy to see that in the the marginal distribution of X2, each entry is also i.i.d. N (0, 1). We finally let X = [X1, X2, X3]. One can verify that Xw(1) = X1w as well as Xw(2) = X1Qw = X1w. For the theoretical guarantee of LBR on our second random ensemble, we have the following theorem.\n\n∥w∥2\n\n∥w∥2\n\nTheorem 3.2. Let X = [X1, X2, X3] and y = Xw(1) + ε be a random instance described above with parameters (n, d, k, γ, b, h, ζ, w). Suppose there exists ξ > 0 such that ξ ≤ |wi| ≤ ζ 1/4ξ for all i ∈ {1, 2, . . . , k}. Also suppose that γ ≥ 1. Let ρ = n1/2+δ (δ ∈ (0, 1/2)). For large enough constant ζ, with probability at least (1 − d exp(−Ω(n2δξ2/γ2)) − d exp(−Ω(n1−2δ))), the relaxed program LBR admits the optimal solution u∗ and z∗ where u∗ ̸= 0] and g = 1[∃i ∈ g : w(1) z∗ features included in the group.\n\n̸= 0]. Here, we use g to denote both the index of a group and the set of the\n\ni = 1[w(1)\n\ni\n\ni\n\nWe first remark that the smallest possible value of ζ for the theorem to hold can be made arbitrarily close to 1 (but greater than 1). This relaxation would only affect the constant coefficients in the Ω(·) notations in the success probability bound. Also, similarly to the remark in Section 3.1, we may set δ = 1/2 − log log n log n . The success probability is (1 − o(1)) as long as nξ2/(γ2 log n) = ω(1). Since ξ2 is usually Θ(1/k), again, we only need n = ω((k/γ2) log(k/γ2)) to achieve the (1 − o(1)) success probability, almost matching the information theoretic lower bound Wainwright (2007) up to the logarithmic factor.\n\n4 EXPERIMENTS\n\nIn this section, we perform extensive experiments to investigate the effectiveness of the proposed sparse group models under the setting of l2 2-regularized least-squares regression specified in equation 10. We use both simulated datasets (non-overlapping groups) introduced in Sections 3.1 and 3.2 and a real-world application (overlapping groups) in cancer to evaluate the performance. To access the performance on simulated data, we compute the recovery accuracy of both individual supports and group supports, which are defined as AI (w) and AG(w), respectively:\n\ni\n\ni\n\n,\n\n̸= 0}|\n\nAI (w) :=\n\n|{i : wi ̸= 0, wtrue |{i : wtrue\n\n|{j : wgj ̸= 0, wtrue |{j : wtrue gj is the ith element of the ground-truth vector wtrue and wtrue\n\nAG(w) :=\n\ni ̸= 0}|\n\n̸= 0}|\n\ngj ̸= 0}|\n\nHere wtrue is the weight vector for group gj. We apply the projected Quasi-Newton method Schmidt et al. (2009) to efficiently solve equation 10. The details related to the optimization could be found in supplementary materials Section E. If the solution of equation 10 is not integral, we use the rounding scheme proposed in section 2.3. For the non-overlapping setting, we compare the performance of our method with Sparse Group Lasso (SGL) Simon et al. (2013), Sparse G-group cover (SGCover) El Halabi & Cevher (2015), and SGL∞(group level sparsity using l∞ norm and individual level sparsity using l1 norm); for the overlapping group setting, we compare with (SGL-Overlap) Yuan et al. (2011) and SGCover El Halabi & Cevher (2015). We also compare our method with Elastic Net (ENet) Zou &\n\n.\n\n(19)\n\ngj\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: (a) Illustration of the data generation process for Random Ensemble I. (b) AI as n increases. (c) AG as n increases. We average results over 100 datasets and the error bar means 95% confidence interval. (d)–(g) Estimate k and h by out-of-sample of MSE by different methods. The black vertical line indicates the true k. The results are averaged over 10 datasets and the error bar means standard deviation. Hastie (2005) which is the state-of-the-art method for detecting sparse features at the individual level. All experiments run on a computer with 8 cores 3.7GHz Intel CPU and 32 GB RAM. 4.1 SIMULATION EVALUATION OF RANDOM ENSEMBLE I\n\nWe first consider the simulation setting described in Section 3.1 in which we use d = 1000, b = 10, k = 50 (ki = 10, ∀i ∈ {1, 2, . . . , 5}), h = 5, and γ = 2.5 to generate the simulation data whose signal-noise-ratio (SNR) is around 4. We evaluate the performance of all the methods on recovering the ground truth weight vector wtrue with k = 50 contributing features distributed in h = 5 groups. Feature selection with given support sizes k and h: We first consider the case when k and h are given and equal to the ground-truth for all methods, while all other hyper-parameters are selected by cross-validation. It is hard to let SGL, SGCover, and SGL∞ to select exact k individual features and h groups of features so we indirectly control k and h by sweeping the regularization parameters. For cases where these methods do not yield exact k individual features and h groups of features, we rank their results and get the top k individual features and h groups of features instead. The same procedure was adopted to ENet to select k features. In this setting, we do not need to worry about false discovery rate (FDR), because it is complementary to the accuracy defined in equation 19. As shown in Fig. 1(b) and (c), AI and AG of all competing methods converge to 1 with the increasing sample size n. However, our proposed method converges the fastest among all the methods, indicating its effectiveness on recovering the structured sparsity when the sample size is small. We further conduct similar experiments with larger γ and show the results in the supplementary materials Section G. Based on these results, we could confirm that our proposed method outperforms conventional methods in selecting contributing features at both the individual level and group level. Estimation of support sizes k and h: In the real-world scenario, we might not know the number of contributing features and groups. Typically, they could be selected by cross-validation based on the out-of-sample mean square error (MSE). Motivated by this practical need, in this study, we also investigate whether the competing methods could accurately recover the ground truth k and h based on the out-of-sample of MSE. As shown in Fig. 1(d)–(g), only our proposed method is able to provide the smallest MSE at h = 5. In addition, for the number of contributing feature k, only our method achieves the smallest MSE around the number of features k = 50. Interpreting the superior performance of our method. All the competing methods are norm-based and they rely on the regularization factor before the penalty norm to adjust to the support size (group number and feature number) requirements. However, this connection is not explicit as our Boolean relaxation where we directly require that (cid:80)d i=1 zi ≤ h. Together with our randomized rounding scheme, our Boolean relaxation method might be at an advantageous position to utilize the prior knowledge on support sizes and/or recover the support sizes from the out-of-sample MSE. 4.2 SIMULATION EVALUATION OF RANDOM ENSEMBLE II Next, we consider a more challenging simulated data generated from the procedure introduced in Section 3.2. We set d = 500, k = 80, h = 10, ζ = 4, and γ = 0.1 as described in Section 3.2. As shown in Fig. 2(a), both w(1) and w(2) are “optimal” for the linear regression problem. However, the\n\ni=1 ui ≤ k and (cid:80)b\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n7\n\n46\n\nENet\n\nk(s.d.)\n\nMethod\n\n92 (5.4)\n\n19 (0.5)\n\nProposed\n\n32.6± 2.2\n\nSGL-Overlap\n\nOut-of-sample MSE ±95%CI\n\nFigure 2: (a) Illustration of the data generation process for Random Ensemble II. (b) AI as n increases. (c) AG as n increases. We average results over 100 datasets and the error bar means 95% confidence interval. supports of w(1) are in h = 10 groups but the supports of w(2) are in ζh = 40 groups. The goal is to test whether each method can recover the solution w(1), which is more sparse on the group level. We control the parameters of each method to make it select k = 80 features in h = 10 groups. As shown in Fig. 2(b) and (c), the recovery accuracy (AG and AI ) of the proposed method rapidly converges to 1 and 0.93 when more training samples are provided. Surprisingly, the recovery accuracy of all the rest of the competing methods converge very slowly. Note that here we do not need to consider false discovery rate (FDR) because each method selects exact k features in h groups, therefore, FDR is complementary to the accuracy. Overall, under the setting of Random Ensemble II, the performance improvement of our method over the-state-of-art algorithms is significant in terms of both recovery accuracy. 4.3 CANCER DRUG RESPONSE PREDICTION We further adapt our model and algorithm on a real-world application to predict the drug response. The goal of the task is to find the essential genes and pathways that are responsible for the ineffectiveness of cancer therapy. For this task, we chose l2 2-regularized least-square regression model to predict the continuous value of drug response. Table 1: Result comparison for IMATNIB. We collect drug response data from h(s.d.) the Cancer Therapeutics Response Portal (CTRP) v2 and the Genomics of Drug Sensitivity in Cancer (GDSC) database Seashore-Ludlow et al. (2015); Yang et al. (2013) with 684 chemotherapy and targeted therapy drugs. For each drug, we create a separate machine learning task to predict the drug response from the expression value of each gene for different tumor samples. In total, we include 1,225 tumor samples and use the gene expression value of 2,369 genes. We focus on the signal transduction pathways which are mined and collected by the Reactome database Jassal et al. (2020). We only consider pathways that contain more than 10 genes and less than 80 genes. We collect 207 pathways (gene groups), in which the average number of genes in each group is 28.9. The gene expression data are extracted from CCLE Barretina et al. (2012). For each drug (machine learning task), we hold 20% of the samples as the test set and used the remaining samples as training and validation set. For each competing method, we use standard cross-validation to determine the hyper-parameter based on the out-of-sample square error MSE on the validation set. Here we only show the performance of the drug IMATNIB as a representative and put the performance of other drugs in the supplementary materials Section I. Table 1 illustrates the estimation of k and h and the out-of-sample MSE on the test set for drug IMATNIB with 10 bootstrap samples. We do not compare with SGL∞ because the SpaSM package Sjöstrand et al. (2018) we used to solve SGL∞ cannot handle overlapping groups. We find that our proposed method achieves the smallest out-of-sample MSE as well as selects the smallest number of genes and pathways. More importantly, as shown in supplementary materials Table S3, the selected pathways are all well associated with the drug response and functional mechanisms of IMATNIB in different types of tumor cells supported by rich literature. We also make predictions for other drugs and show the results in Table. S4. 5 CONCLUSION In this paper, we propose a novel convex framework for learning structured sparsity. We provide theoretical tools to verify the exactness of the solution of the relaxation, and a rounding algorithm to produce the feasible integral solution when the relaxation solution is fractional. For the case of least-squares loss, we perform extensive experiments to demonstrate the effectiveness of the proposed framework.\n\n39.6 ± 4.2\n\n55.4 ± 6.9\n\n321 (10.5)\n\n46.9±3.7\n\nSGCover\n\n13 (1.7)\n\n18 (2.3)\n\n60 (8.2)\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nRudolf Ahlswede and Andreas Winter. Strong converse for identification via quantum channels.\n\nIEEE Transactions on Information Theory, 48(3):569–579, 2002.\n\nFrancis Bach. Structured sparsity-inducing norms through submodular functions. In NIPS 2010 : Twenty-Fourth Annual Conference on Neural Information Processing Systems, pp. NIPS, Vancouver, Canada, 2010. URL https://hal.archives-ouvertes.fr/hal-00511310.\n\nFrancis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Structured Sparsity through Convex Optimization. Statistical Science, 27(4):450–468, November 2012. ISSN 08834237. doi: 10.1214/12-sts394. URL http://dx.doi.org/10.1214/12-sts394.\n\nLuca Baldassarre, Nirav Bhan, Volkan Cevher, Anastasios Kyrillidis, and Siddhartha Satpathi. Group-\n\nsparse model selection: Hardness and relaxations. arXiv preprint arXiv:1303.3207, 2013.\n\nJ. Barretina, G. Caponigro, N. Stransky, K. Venkatesan, A. A. Margolin, S. Kim, C. J. Wilson, J. Lehár, G. V. Kryukov, D. Sonkin, A. Reddy, M. Liu, L. Murray, M. F. Berger, J. E. Monahan, P. Morais, J. Meltzer, A. Korejwa, J. Jané-Valbuena, F. A. Mapa, J. Thibault, E. Bric-Furlong, P. Raman, A. Shipway, I. H. Engels, J. Cheng, G. K. Yu, J. Yu, P. Aspesi, M. de Silva, K. Jagtap, M. D. Jones, L. Wang, C. Hatton, E. Palescandolo, S. Gupta, S. Mahan, C. Sougnez, R. C. Onofrio, T. Liefeld, L. MacConaill, W. Winckler, M. Reich, N. Li, J. P. Mesirov, S. B. Gabriel, G. Getz, K. Ardlie, V. Chan, V. E. Myer, B. L. Weber, J. Porter, M. Warmuth, P. Finan, J. L. Harris, M. Meyerson, T. R. Golub, M. P. Morrissey, W. R. Sellers, R. Schlegel, and L. A. Garraway. The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity. Nature, 483(7391): 603–607, Mar 2012.\n\nDimitris Bertsimas and Bart Van Parys. Sparse high-dimensional regression: Exact scalable doi:\n\nalgorithms and phase transitions. The Annals of Statistics, 48(1):300 – 323, 2020. 10.1214/18-AOS1804. URL https://doi.org/10.1214/18-AOS1804.\n\nDimitris Bertsimas, Jean Pauphilet, and Bart Van Parys. Sparse Regression: Scalable Algorithms and Empirical Performance. Statistical Science, 35(4):555 – 578, 2020. doi: 10.1214/19-STS701. URL https://doi.org/10.1214/19-STS701.\n\nA. L. Boulesteix and K. Strimmer. Partial least squares: a versatile tool for the analysis of high-\n\ndimensional genomic data. Brief Bioinform, 8(1):32–44, Jan 2007.\n\nB. S. Braun and K. Shannon. Targeting Ras in myeloid leukemias. Clin Cancer Res, 14(8):2249–2252,\n\nApr 2008.\n\nS. C. Chen, T. T. Liao, and M. H. Yang. Emerging roles of epithelial-mesenchymal transition in\n\nhematological malignancies. J Biomed Sci, 25(1):37, Apr 2018.\n\nY. J. Chung, T. M. Kim, D. W. Kim, H. Namkoong, H. K. Kim, S. A. Ha, S. Kim, S. M. Shin, J. H. Kim, Y. J. Lee, H. M. Kang, and J. W. Kim. Gene expression signatures associated with the resistance to imatinib. Leukemia, 20(9):1542–1550, Sep 2006.\n\nA. M. Coluccia, A. Vacca, M. Duñach, L. Mologni, S. Redaelli, V. H. Bustos, D. Benati, L. A. Pinna, and C. Gambacorti-Passerini. Bcr-Abl stabilizes beta-catenin in chronic myeloid leukemia through its tyrosine phosphorylation. EMBO J, 26(5):1456–1466, Mar 2007.\n\nS. Corrêa, R. Binato, B. Du Rocher, M. T. Castelo-Branco, L. Pizzatti, and E. Abdelhay. Wnt/bcatenin pathway regulates ABCB1 transcription in chronic myeloid leukemia. BMC Cancer, 12: 303, Jul 2012.\n\nKenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach\n\nspaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.\n\nDamek Davis. Lecture notes for mathematical programming, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMarwa El Halabi and Volkan Cevher. A totally unimodular view of structured sparsity. In Guy Lebanon and S. V. N. Vishwanathan (eds.), Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pp. 223–231, San Diego, California, USA, 09–12 May 2015. PMLR. URL https://proceedings.mlr.press/v38/elhalabi15.html.\n\nGersende Fort and Sophie Lambert-Lacroix. Classification using partial least squares with penalized logistic regression. Bioinformatics, 21(7):1104–1111, 11 2004. ISSN 1367-4803. doi: 10.1093/ bioinformatics/bti114. URL https://doi.org/10.1093/bioinformatics/bti114.\n\nJ. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group lasso, 2010.\n\nAlexandre Gramfort and Matthieu Kowalski. Improving m/eeg source localization with an intercondition sparse prior. In 2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro, pp. 141–144, 2009. doi: 10.1109/ISBI.2009.5193003.\n\nJ. J. Gu, J. R. Ryu, and A. M. Pendergast. Abl tyrosine kinases in T-cell signaling. Immunol Rev, 228\n\n(1):170–183, Mar 2009.\n\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.\n\ngurobi.com.\n\nMarwa El Halabi, Francis Bach, and Volkan Cevher. Combinatorial Penalties: Which structures In AISTATS 2018 - 22nd International Conference on are preserved by convex relaxations? Artificial Intelligence and Statistics, Canary Islands, Spain, April 2018. URL https://hal. archives-ouvertes.fr/hal-01652151.\n\nC. H. Heldin. Targeting the PDGF signaling pathway in tumor treatment. Cell Commun Signal, 11:\n\n97, Dec 2013.\n\nT. C. Hoang, T. K. Bui, T. Taguchi, T. Watanabe, and Y. Sato. All-trans retinoic acid inhibits KIT activity and induces apoptosis in gastrointestinal stromal tumor GIST-T1 cell line by affecting on the expression of survivin and Bax protein. J Exp Clin Cancer Res, 29:165, Dec 2010.\n\nF. F. Huang, L. Zhang, D. S. Wu, X. Y. Yuan, Y. H. Yu, X. L. Zhao, F. P. Chen, and H. Zeng. PTEN regulates BCRP/ABCG2 and the side population through the PI3K/Akt pathway in chronic myeloid leukemia. PLoS One, 9(3):e88298, 2014.\n\nJunzhou Huang, Tong Zhang, and Dimitris Metaxas. Learning with structured sparsity. Journal of Machine Learning Research, 12(103):3371–3412, 2011. URL http://jmlr.org/papers/ v12/huang11b.html.\n\nY. Huang, E. O. Comiskey, R. S. Dupree, S. Li, A. J. Koleske, and J. K. Burkhardt. The c-Abl tyrosine\n\nkinase regulates actin remodeling at the immune synapse. Blood, 112(1):111–119, Jul 2008.\n\nLaurent Jacob, Guillaume Obozinski, and Jean-Philippe Vert. Group lasso with overlap and graph lasso. In Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09. ACM Press, 2009. doi: 10.1145/1553374.1553431. URL https://doi.org/10.1145/ 1553374.1553431.\n\nB. Jassal, L. Matthews, G. Viteri, C. Gong, P. Lorente, A. Fabregat, K. Sidiropoulos, J. Cook, M. Gillespie, R. Haw, F. Loney, B. May, M. Milacic, K. Rothfels, C. Sevilla, V. Shamovsky, S. Shorser, T. Varusai, J. Weiser, G. Wu, L. Stein, H. Hermjakob, and P. D’Eustachio. The reactome pathway knowledgebase. Nucleic Acids Res, 48(D1):D498–D503, 01 2020.\n\nRodolphe Jenatton, Jean-Yves Audibert, and Francis Bach. Structured variable selection with sparsityinducing norms. 2009. doi: 10.48550/ARXIV.0904.3523. URL https://arxiv.org/abs/ 0904.3523.\n\nRodolphe Jenatton, Jean-Yves Audibert, and Francis Bach. Structured Variable Selection with Sparsity-Inducing Norms. Journal of Machine Learning Research, 12:2777–2824, 2011. URL https://hal.inria.fr/inria-00377732.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJ. Kim, D. G. Bates, I. Postlethwaite, P. Heslop-Harrison, and K. H. Cho. Least-squares methods for identifying biochemical regulatory networks from noisy measurements. BMC Bioinformatics, 8:8, Jan 2007.\n\nSeyoung Kim and Eric P. Xing. Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping. The Annals of Applied Statistics, 6(3), Sep 2012. ISSN 1932-6157. doi: 10.1214/12-aoas549. URL http://dx.doi.org/10.1214/ 12-AOAS549.\n\nAnastasios Kyrillidis, Luca Baldassarre, Marwa El Halabi, Quoc Tran-Dinh, and Volkan Cevher. Structured sparsity: Discrete and convex approaches. CoRR, abs/1507.05367, 2015. URL http://arxiv.org/abs/1507.05367.\n\nE. Leo, M. Mancini, M. Aluigi, S. Luatti, F. Castagnetti, N. Testoni, S. Soverini, M. A. Santucci, and G. Martinelli. BCR-ABL1-associated reduction of beta catenin antagonist Chibby1 in chronic myeloid leukemia. PLoS One, 8(12):e81425, 2013.\n\nL. Li, D. K. Blumenthal, T. Masaki, C. M. Terry, and A. K. Cheung. Differential effects of imatinib on PDGF-induced proliferation and PDGF receptor signaling in human arterial and venous smooth muscle cells. J Cell Biochem, 99(6):1553–1563, Dec 2006.\n\nTianyi Lin, Chi Jin, and Michael. I. Jordan. Near-optimal algorithms for minimax optimization, 2020.\n\nIn J. Lafferty, C. Williams,\n\nJun Liu and Jieping Ye. Moreau-Yosida regularization for grouped tree structure learnand A. Culotta ing. (eds.), Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010. URL https://proceedings.neurips.cc/paper/2010/file/ d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf.\n\nJ. Shawe-Taylor, R. Zemel,\n\nC. J. Malavaki, A. E. Roussidis, C. Gialeli, D. Kletsas, T. Tsegenidis, A. D. Theocharis, G. N. Tzanakakis, and N. K. Karamanos. Imatinib as a key inhibitor of the platelet-derived growth factor receptor mediated expression of cell surface heparan sulfate proteoglycans and functional properties of breast cancer cells. FEBS J, 280(10):2477–2489, May 2013.\n\nSahand N. Negahban, Pradeep Ravikumar, Martin J. Wainwright, and Bin Yu. A Unified Framework for High-Dimensional Analysis of M -Estimators with Decomposable Regularizers. Statistical Science, 27(4):538 – 557, 2012. doi: 10.1214/12-STS400. URL https://doi.org/10. 1214/12-STS400.\n\nYurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):221–259, 2009. doi: 10.1007/s10107-007-0149-x. URL https://doi.org/10. 1007/s10107-007-0149-x.\n\nDanh V. Nguyen and David M. Rocke. Tumor classification by partial least squares using microarray gene expression data . Bioinformatics, 18(1):39–50, 01 2002. ISSN 1367-4803. doi: 10.1093/ bioinformatics/18.1.39. URL https://doi.org/10.1093/bioinformatics/18.1. 39.\n\nC. Nishioka, T. Ikezoe, J. Yang, K. Udaka, and A. Yokoyama. Imatinib causes epigenetic alterations of PTEN gene via upregulation of DNA methyltransferases and polycomb group proteins. Blood Cancer J, 1(12):e48, Dec 2011.\n\nRoberto Oliveira et al. Sums of random hermitian matrices and an inequality by Rudelson. Electronic\n\nCommunications in Probability, 15:203–212, 2010.\n\nC. Peng, Y. Chen, Z. Yang, H. Zhang, L. Osterby, A. G. Rosmarin, and S. Li. PTEN is a tumor suppressor in CML stem cells and BCR-ABL-induced leukemias in mice. Blood, 115(3):626–635, Jan 2010.\n\nMert Pilanci, Martin J Wainwright, and Laurent El Ghaoui. Sparse learning via boolean relaxations.\n\nMathematical Programming, 151(1):63–87, 2015.\n\nF. Rapaport, E. Barillot, and J. P. Vert. Classification of arrayCGH data using fused SVM. Bioinfor-\n\nmatics, 24(13):i375–382, Jul 2008.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMark Schmidt, Ewout van den Berg, Michael Friedlander, and Kevin Murphy. Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm. In David van Dyk and Max Welling (eds.), Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, volume 5 of Proceedings of Machine Learning Research, pp. 456–463, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 16–18 Apr 2009. PMLR. URL http://proceedings.mlr.press/v5/schmidt09a.html.\n\nB. Seashore-Ludlow, M. G. Rees, J. H. Cheah, M. Cokol, E. V. Price, M. E. Coletti, V. Jones, N. E. Bodycombe, C. K. Soule, J. Gould, B. Alexander, A. Li, P. Montgomery, M. J. Wawer, N. Kuru, J. D. Kotz, C. S. Hon, B. Munoz, T. Liefeld, V. Danˇcík, J. A. Bittker, M. Palmer, J. E. Bradner, A. F. Shamji, P. A. Clemons, and S. L. Schreiber. Harnessing Connectivity in a Large-Scale Small-Molecule Sensitivity Dataset. Cancer Discov, 5(11):1210–1223, Nov 2015.\n\nNoah Simon, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. A sparse-group lasso. Journal of\n\nComputational and Graphical Statistics, 2013.\n\nKarl Sjöstrand, Line Harder Clemmensen, Rasmus Larsen, Gudmundur Einarsson, and Bjarne Ersbøll. Spasm: A matlab toolbox for sparse statistical modeling. Journal of Statistical Software, 84(10), 2018. doi: 10.18637/jss.v084.i10. URL https://doi.org/10.18637/jss.v084.i10.\n\nR. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\n\nSociety (Series B), 58:267–288, 1996.\n\nMartin Wainwright. Information-theoretic bounds on sparsity recovery in the high-dimensional and noisy setting. In 2007 IEEE International Symposium on Information Theory, pp. 961–965. IEEE, 2007.\n\nDavid P Williamson and David B Shmoys. The design of approximation algorithms. Cambridge\n\nuniversity press, 2011.\n\nYongxin Xi, Uri Hasson, Peter J Ramadge, and Zhen Xiang. Boosting with spatial regularization. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta (eds.), Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2009/file/ f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf.\n\nW. Yang, J. Soares, P. Greninger, E. J. Edelman, H. Lightfoot, S. Forbes, N. Bindal, D. Beare, J. A. Smith, I. R. Thompson, S. Ramaswamy, P. A. Futreal, D. A. Haber, M. R. Stratton, C. Benes, U. McDermott, and M. J. Garnett. Genomics of Drug Sensitivity in Cancer (GDSC): a resource for therapeutic biomarker discovery in cancer cells. Nucleic Acids Res, 41(Database issue):D955–961, Jan 2013.\n\nLei Yuan, Jun Liu, and Jieping Ye.\n\nIn J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances Information Processing Systems, volume 24. Curran Associates, URL https://proceedings.neurips.cc/paper/2011/file/ Inc., 03c6b06952c750899bb03d998e631860-Paper.pdf.\n\nEfficient methods for overlapping group lasso.\n\nin Neural\n\n2011.\n\nH. Zhang, Y. Wang, H. Yang, Z. Huang, X. Wang, and W. Feng. TCF7 knockdown inhibits the imatinib resistance of chronic myeloid leukemia K562/G01 cells by neutralizing the Wnt/b-catenin/TCF7/ABC transporter signaling axis. Oncol Rep, 45(2):557–568, Feb 2021.\n\nPeng Zhao, Guilherme Rocha, and Bin Yu. The composite absolute penalties family for grouped and hierarchical variable selection. The Annals of Statistics, 37(6A):3468 – 3497, 2009. doi: 10.1214/07-AOS584. URL https://doi.org/10.1214/07-AOS584.\n\nXun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous optimization for structure learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/ paper/2018/file/e347c51419ffb23ca3fd5050202f9c3d-Paper.pdf.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nL. Zhou, N. An, R. C. Haydon, Q. Zhou, H. Cheng, Y. Peng, W. Jiang, H. H. Luu, P. Vanichakarn, J. P. Szatkowski, J. Y. Park, B. Breyer, and T. C. He. Tyrosine kinase inhibitor STI-571/Gleevec down-regulates the beta-catenin signaling activity. Cancer Lett, 193(2):161–170, Apr 2003.\n\nHui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the\n\nRoyal Statistical Society, Series B, 67:301–320, 2005.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nSupplementary Materials\n\nA PROOF OF THEOREM 2.1\n\nProof Theorem 2.1. Let ui ∈ {0, 1} indicate whether the i-th feature is selected and ugj be a vector containing ui, i ∈ gj. We then define D(u) := diag(u) and D(ugj ) := diag(ugj ). Considering the change of variable w = D(u)w, we find the original problem equation 1 is equivalent to (cid:41)\n\n(cid:40) n\n\n(cid:88)\n\nf (w⊤D(u)xi; yi) +\n\nρ∥D(u)w∥2\n\n2\n\n.\n\n(20)\n\n1 2\n\nP ∗ =\n\nmin ∥D(u)w∥0 ≤ k (cid:104)(cid:13) (cid:13) (cid:13)D(ugj )wgj\n\n(cid:13) (cid:13) (cid:13)0\n\n(cid:80)b\n\nj=1 1\n\n(cid:105)\n\n> 0\n\n≤ h\n\ni=1\n\nWe further introduce zj ∈ {0, 1} to indicate whether the group of features gj is selected and obtain the following equivalent formulation\n\n(cid:40) n\n\n(cid:88)\n\ni=1\n\nf (w⊤D(u)xi; yi) +\n\n1 2\n\n(cid:41)\n\nρ∥D(u)w∥2\n\n2\n\n.\n\n(21)\n\nP ∗ =\n\nmin ∥D(u)w∥0 ≤ k\n\n(cid:13) (cid:13) (cid:13)D(ugj )wgj (cid:80)b\n\n(cid:13) (cid:13) (cid:13)0 j=1 zj ≤ h\n\n≤ zj , ∀j\n\n(cid:41)\n\nf ∗(vi; yi)\n\n.\n\n(23)\n\n(24)\n\nWe further split w and u and then equation 21 becomes\n\nP ∗ = min\n\n(u,z)∈Γ\n\nmin w∈Rd\n\nf (w⊤D(u)xi; yi) +\n\n(cid:40) n\n\n(cid:88)\n\ni=1\n\n(cid:41)\n\nρ∥w∥2\n\n2\n\n,\n\n1 2\n\nwhere\n\nΓ =\n\n \n\n\n\n(cid:12) (cid:12) (u, z) (cid:12) (cid:12)\n\nd (cid:88)\n\ni=1\n\nui ≤ k,\n\nb (cid:88)\n\nj=1\n\nzj ≤ h,\n\nui ≤ zj, ∀i ∈ gj,\n\nu ∈ {0, 1}d,\n\nz ∈ {0, 1}b\n\n(22)\n\n \n\n\n\n.\n\nIt is easy to verify that equation 22 achieves the same objective function value of equation 1 at the same unique optimal solution w∗. It remains to prove the inner minimization is equivalent to\n\n(cid:40) n\n\n(cid:88)\n\ni=1\n\nmin w∈Rd\n\nf (w⊤D(u)xi; yi) +\n\n1 2\n\nρ∥w∥2\n\n2\n\n(cid:41)\n\n(cid:40)\n\n= max v∈Rn\n\n−\n\n1 2ρ\n\nv⊤XD(u)X ⊤v −\n\nn (cid:88)\n\ni=1\n\nReplacing f by its Legendre-Fenchel conjugate f ∗, we have\n\nmin w∈Rd\n\nmax v∈Rn\n\n(cid:40) n\n\n(cid:88)\n\ni=1\n\nw⊤D(u)xi · vi − f ∗(vi : yi) +\n\n(cid:41)\n\nρ∥w∥2\n\n2\n\n.\n\n1 2\n\nUnder the stated assumptions, strong duality must hold and therefore minimum and maximum can be exchanged.\n\nmax v∈Rn\n\nmin w∈Rd\n\n(cid:40) n\n\n(cid:88)\n\ni=1\n\nw⊤D(u)xi · vi − f ∗(vi : yi) +\n\n(cid:41)\n\nρ∥w∥2\n\n2\n\n.\n\n1 2\n\n(25)\n\nThe objective function is strongly convex with respect to w. Hence, we can obtain the unique minimizer w∗ = 1\n\ni=1 D(u)xivi. Substituting w∗ yields equation 23.\n\n(cid:80)n\n\nρ\n\nB PROOFS AND DEVIATIONS FOR THE CONVEX FORMULATION\n\nB.1 PROOF OF THEOREM 2.2\n\nTo prove Theorem 2.2, let us first provide the sufficient and necessary conditions for PBR to have integral solutions in the following Lemma.\n\nLemma B.1. Suppose that each feature belongs to exactly one group. Then suppose that the integral solution (ˆu, ˆz) of equation 3 selects exactly k features and h groups. This solution is also the optimal solution of the relaxed program PBR if and only if there exist non-negative {λup≤1, λup≥0}p∈[d], {λzi≤1, λzi≥0}i∈[b], {λup≤zi}∀p∈gi, λk, and λh, such that\n\nˆv ∈ arg max v∈Rn\n\n(cid:40)\n\n−\n\n1 2ρ\n\nv⊤XD(ˆu)X ⊤v −\n\nn (cid:88)\n\ni=1\n\nf ∗(vi; yi)\n\n15\n\n(cid:41)\n\n(26)\n\nPublished as a conference paper at ICLR 2023\n\nλup≤1 − λup≥0 + λk = (X ⊤ (cid:88)\n\nλzi≤1 − λzi≥0 + λh =\n\nλup≤zi,\n\np ˆv)2 − λup≤zi ,\n\np∈gi\n\nλup≤1 = 0, λup≥0 = 0, λzi≤1 = 0, λzi≥0 = 0, λup≤zi = 0,\n\n∀p ∈ gi;\n\n∀i ∈ [b];\n\n∀p : ˆup = 0; ∀p : ˆup = 1; ∀i : ˆzi = 0; ∀i : ˆzi = 1; ∀p ∈ gi : ˆup < ˆzi.\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32) (33)\n\nTo prove Lemma B.1, we need to use the following two theorems.\n\nTheorem B.2 (Davis (2020)). Suppose ̄x is a local minimizer of f : Rd → R on a closed convex set X ⊆ Rd. If f is differentiable at ̄x, it holds that\n\n− ∇f ( ̄x) ∈ NX ( ̄x).\n\n(34)\n\nTheorem B.3 (Davis (2020)). Let A ∈ Rm×n and let β ∈ Rm. Consider the polyhedron Q(A, β) = {x|Ax ≤ β}. Suppose x ∈ Q(A, β), then the normal cone at x is NQ(A,β)(x) = {A⊤y|y ∈ Rm such that y ≥ 0 and y⊤(β − Ax) = 0}.\n\nProof of Lemma B.1. PBR is\n\nPBR = min\n\n(u,z)∈Ω\n\nmax v∈Rn\n\n(cid:124) j uj ≤ k; (cid:80)\n\n(cid:40)\n\n−\n\n1 2ρ\n\nv⊤XD(u)X ⊤v −\n\n(cid:123)(cid:122) F (u,z)\n\nn (cid:88)\n\ni=1\n\nf ∗(vi; yi)\n\n(cid:41) ,\n\n(cid:125)\n\n(35)\n\nwhere Ω = {(u, z)| (cid:80) i zi ≤ h; ui ≤ zj; ∀i ∈ gj; u ∈ [0, 1]d; z ∈ [0, 1]b} is the feasible set for (u, z). By Theorem B.2, we know (ˆu, ˆz) is optimal if and only if the following inclusion holds:\n\n− ∇F (ˆu, ˆz) ∈ NΩ(ˆu, ˆz),\n\n(36)\n\nwhere NΩ(ˆu, ˆz) is the normal cone at (ˆu, ˆz). Regarding the Left-Hand-Side of equation 36, by standard calculation, we have that ∂uiF (ˆu) = −(X ⊤\n\ni ˆv)2 (ˆv is defined in equation 26) and ∂zi F (ˆz) = 0. Therefore,\n\n− ∇F (ˆu, ˆz) =\n\n\n\n1 ˆv)2 (X ⊤ 2 ˆv)2 (X ⊤ ... d ˆv)2 (X ⊤ 0\n0\n\n\n\n \n \n \n \n \n \n\n...\n\n0\n\n.\n\n \n \n \n \n \n \n\n(37)\n\nRegarding the Right-Hand-Side of equation 36, we obtain the normal cone NΩ(ˆu, ˆz) using Theorem B.3. Specifically, the feasible set Ω is a polyhedron that can be presented by Ω(A, b) = (cid:26)(cid:20) ˆu ˆz\n\n, where the A matrix and β vector are constructed as follows.\n\n(cid:21) (cid:12) (cid:12) (cid:12) A\n\n(cid:20) ˆu ˆz\n\n≤ β\n\n(cid:27)\n\n(cid:21)\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nfeature block: d columns (cid:125)(cid:124)\n\n(cid:122) 1\n−1\n\ngroup block: b columns (cid:125)(cid:124)\n\n(cid:123)\n\n(cid:122)\n\n(cid:123)\n\n1 −1\n\n. . .\n\n1\n\n1\n\n· · ·\n\n1 −1 1\n\n1 −1\n\n1 −1\n\n. . .\n\n1\n\n1\n\n· · ·\n\n1 −1 1\n\nA′\n\n(cid:123)(cid:122) A\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n(cid:124)\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n(cid:125)\n\n\n\n \n \n \n \n \n \n\n≤\n\n\n\n \n \n \n \n \n \n\nˆu1 ˆu2 ... ˆud ˆz1 ˆz2 ... ˆzb\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n1 0\n1 0\n\n...\n\n1 0\nk 1\n0 1\n0\n\n...\n\n1 0\nh 0\n\n...\n\n0 (cid:124)(cid:123)(cid:122)(cid:125) β\n\n.\n\n(38)\n\nHere, A′ is the feature-group relation matrix where for each feature group relation p ∈ gi, there is a row in A′ where the p-th entry in the feature block is 1, the i-th entry in the group block is −1, and all the rest entries are 0. According to Theorem B.3, we know the normal cone NΩ(ˆu, ˆz) =\n\nNΩ(A,b)(ˆu, ˆz) =\n\n(cid:26)\n\nA⊤λ\n\n(cid:12) (cid:12)λ ∈ (R≥0) (cid:12)\n\nc\n\n(cid:18)\n\n: λ⊤\n\nb − A\n\n(cid:20) ˆu ˆz\n\n(cid:21)(cid:19)\n\n(cid:27)\n\n= 0\n\n, where c denotes the number of\n\nconstraints (i.e., the row dimension of A). We identify the entries of λ as follows: λup≤1 denotes the dual parameter corresponding to the constraint up ≤ 1 for each feature p; λup≥0 denotes the dual parameter corresponding to the constraint up ≥ 0 for each feature p; λk denotes the dual parameter corresponding to the constraint (cid:80) j uj ≤ k; λzi≤1 denotes the dual parameter corresponding to the constraint zi ≤ 1 for each group i; λzi≥0 denotes the dual parameter corresponding to the constraint zi ≥ 0 for each group i; λh denotes the dual parameter corresponding to the constraint (cid:80) i zi ≤ h; λup≤zi denotes the dual parameter corresponding to the constraint up ≤ zi for each feature p and group gi such that p ∈ gi. Finally, we conclude that the equivalent condition of equation 36 is there exists λ ∈ (R≥0)c (cid:20) ˆu ˆz\n\nsuch that −∇F (ˆu, ˆz) = A⊤λ and λ⊤\n\n= 0. By equation 37, we obtain equa-\n\nb − A\n\n(cid:21)(cid:19)\n\n(cid:18)\n\ntion 27 and equation 28 as the equivalent condition of −∇F (ˆu, ˆz) = A⊤λ. We also obtain equation 29, equation 30, equation 31, equation 32, and equation 33 as the equivalent condition of\n\n(cid:18)\n\nλ⊤\n\nb − A\n\n(cid:21)(cid:19)\n\n(cid:20) ˆu ˆz\n\n= 0.\n\nWith Lemma B.1 in hand, we can prove Theorem 2.2 in the following.\n\nProof of Theorem 2.2. We first prove the sufficient condition. Given λk and λh, we only need to construct non-negative {λup≤1, λup≥0}p∈[d], {λzi≤1, λzi≥0}i∈[b], {λup≤zi}∀p∈gi to satisfy equation 27-equation 33. By Lemma B.1, this will establish the optimality of (ˆu, ˆz) in the relaxed program PBR. We first construct {λup≤1, λup≥0}p∈[d] and {λup≤zi}∀p∈gi as follows.\n\n1. For each group i such that ˆzi = 1, and for each p ∈ gi and ˆup = 1, set λup≤1 = λup≥0 = 0 and λup≤zi = (X ⊤ p ˆv)2 and λup≤1 = λup≤zi = 0. By equation 14, one may verify that all constructed values in this step are non-negative.\n\np ˆv)2 − λk. For each p ∈ gi and ˆup = 0, set λup≥0 = λk − (X ⊤\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n2. For each group i such that ˆzi = 1, set λzi≥0 = 0 and λzi≤1 = (cid:80)\n\np∈gi,ˆup=1((X ⊤\n\np ˆv)2 −\n\nλk) − λh, which is non-negative due to equation 15.\n\n3. For each group i such that ˆzi = 0, and for each p ∈ gi and (X ⊤\n\np ˆv)2 > λk, set λup≤1 = p ˆv)2 ≤ λk, set p ˆv)2 − λk. For each p ∈ gi and (X ⊤ p ˆv)2 and λup≤1 = λup≤zi = 0. Observe that we always have λup≤zi =\n\nλup≥0 = 0 and λup≤zi = (X ⊤ λup≥0 = λk − (X ⊤ max{(X ⊤\n\np ˆv)2 − λk, 0} for each p ∈ gi.\n\n4. For each group i such that ˆzi = 0, set λzi≤1 = 0 and λzi≥0 = λh − (cid:80)\n\nmax{(X ⊤\n\np ˆv)2 −\n\np∈gi\n\nλk, 0}, which is non-negative due to equation 16.\n\nFinally, it is straightforward to verify that equation 27-equation 33 are satisfied by our constructed λ, and therefore, by Lemma B.1, we conclude that (ˆu, ˆz) is the optimal solution of the relaxed program PBR. We next prove the necessary condition. Given PBR and P ∗ have the same integral solution, we only need to show there exist λk and λh that satisfy equation 14–equation 16. By Lemma B.1 and ˆu is the integral solution, we have\n\n1. For each group i such that ˆzi = 1 and ∀p ∈ gi, ˆup = 1, we have\n\n(X ⊤\n\np ˆv)2 = λk + λup≤1 + λup≤zi ⇒ (X ⊤\n\np ˆv)2 > λk.\n\n(39)\n\nFor each group i such that ˆzi = 1 and ∀p ∈ gi, ˆup = 0, we have\n\n(40) 2. For each group i such that ˆzi = 1 and all p ∈ gi, ˆup = 1, we apply equation 39 and equa-\n\np ˆv)2 = λk − λup≥0 ⇒ (X ⊤\n\np ˆv)2 ≤ λk.\n\n(X ⊤\n\ntion 28 and have (cid:88)\n\n(cid:0)(X ⊤\n\na ˆv)2 − λk\n\n(cid:1) = λh + λzi≤1 ⇒\n\n(cid:88)\n\n(cid:0)(X ⊤\n\na ˆv)2 − λk\n\n(cid:1) > λh.\n\n(41)\n\np∈gi,ˆup=1\n\np∈gi,ˆup=1\n\n3. For each group i such that ˆzi = 0, we apply equation 40 and equation 28 and have\n\n(cid:88)\n\n(cid:0)(X ⊤\n\np ˆv)2 − λk\n\n(cid:1) = λh − λzi≥0 −\n\n(cid:88)\n\nλup≥0\n\n⇒\n\n⇒\n\np∈gi (cid:88)\n\np∈gi (cid:88)\n\np∈gi\n\nmax{(X ⊤\n\np ˆv)2 − λk, 0} ≤ λh − λzi≥0 −\n\np∈gi\n\n(cid:88)\n\np∈gi\n\nλup≥0\n\nmax{(X ⊤\n\np ˆv)2 − λk, 0} ≤ λh.\n\n(42)\n\nB.2 DERIVATION OF EQUATION 10\n\nIn the case of least-square regression, the Legendre-Fenchel conjugate of the least-square loss f (t; y) = 1 2 + sy. Substituting this conjugate function into H(u) in equation 3 in Theorem 2.1, we have\n\n2 (t − y)2 is given by f ∗(s; y) = s2\n\nH(u) = max v∈Rn\n\n(cid:26)\n\n−\n\n1 2ρ\n\nv⊤XD(u)X ⊤v −\n\n1 2\n\n∥v∥2\n\n2 − v⊤y\n\n(cid:27)\n\n.\n\nWe can verify that the unique optimal solution of H(u) is\n\nˆv = −\n\n(cid:18) XD(u)X ⊤ ρ\n\n(cid:19)−1\n\n+ I\n\ny.\n\nSubstituting ˆv back into equation 3 and applying Theorem 2.1 yield the representation\n\nLBR = min\n\n(u,z)∈Ω\n\nB.3 PROOF OF COROLLARY 2.3\n\n(cid:40)\n\ny⊤\n\n(cid:18) 1 ρ\n\nXD(u)X ⊤ + I\n\n(cid:19)−1\n\n(cid:41)\n\ny\n\n.\n\n(43)\n\n(44)\n\n(45)\n\non equation 5, we know ˆv = −\n\nProof of Corollary 2.3. Under the least-square regression setting, we apply Theorem 2.2 and based (cid:17)−1 (cid:1), where S is the support set indicated by ˆu and then ˆv = −M y. Substituting ˆv by ˆv = −M y in Theorem 2.2 proves the Corollary.\n\ny. We further define M := (cid:0)In + ρ−1XSX ⊤\n\n(cid:16) XD(ˆu)X ⊤ ρ\n\n+ I\n\nS\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nC PROOF OF THEOREM 2.4\n\nProof of Theorem 2.4. For least-squares loss f (t, y) = 1\n\n2 (t − y)2, we have that\n\nH(u) = max v∈Rn\n\n−\n\n(cid:26)\n\n1 2ρ\n\nv⊤XD(u)X ⊤v −\n\n∥v∥2\n\n2 − v⊤y\n\n(cid:27)\n\n.\n\n1 2\n\nSince the optimal value if non-negative, the optimal dual parameter v ∈ Rn must satisfy that ∥v∥2 ≤ 2∥y∥2 ≤ 2. Note that\n\nH(u) − P ∗ ≤ H(u) − H( ̄u) = H(u) − H( ̃u) + H( ̃u) − H( ̄u),\n\n(46)\n\nwhere we set ̃ui = ̄ui/ ̄zj if the feature i belongs to group j and zj = 1, otherwise we set ̃ui = 0. For H(u) − H( ̃u), we have that\n\nH(u) − H( ̃u) (cid:26)\n\n= max v∈Rn\n\n−\n\n1 2ρ\n\nv⊤XD(u)X ⊤v −\n\n1 2\n\n∥v∥2\n\n2 − v⊤y\n\n(cid:27)\n\n− max v∈Rn\n\n(cid:26)\n\n−\n\n1 2ρ\n\nv⊤XD( ̃u)X ⊤v −\n\n(cid:27)\n\n∥v∥2\n\n2 − v⊤y\n\n1 2\n\n≤ max v∈Rn\n\n(cid:26)\n\n−\n\n1 2ρ\n\nv⊤X(D(u) − D( ̃u))X ⊤v\n\n(cid:27)\n\n≤\n\n1 ρ\n\nσmax\n\n(cid:0)X(D(u) − D( ̃u))X ⊤(cid:1) ,\n\n(47)\n\nwhere σmax(·) denotes the maximum singular value of the matrix. Similarly, for H( ̃u) − H( ̄u), we have that\n\nσmax\n\n(cid:0)X(D( ̃u) − D( ̄u))X ⊤(cid:1) .\n\n(48)\n\n1 ρ\nFor X(D( ̃u) − D( ̄u))X ⊤, we rewrite it as\n\nH( ̃u) − H( ̄u) ≤\n\nX(D( ̃u) − D( ̄u))X ⊤ =\n\n(cid:88)\n\n(cid:88)\n\n(cid:18)\n\nj\n\ni∈gj\n\nzj ×\n\n ̄ui ̄zj\n\n(cid:19)\n\n− ̄ui\n\nXiX ⊤ i .\n\n(cid:16)\n\n(cid:17)\n\n√\n\nNote that by our assumption, the operator norm of (cid:80) is at most |gj| ≤ G and the mean of the random matrix is 0. By the Ahlswede-Winter matrix concentration bound Ahlswede & Winter (2002); Oliveira et al. (2010), we have that\n\nzj × ̄ui ̄zj\n\nXiX ⊤\n\n− ̄ui\n\ni∈gj\n\ni\n\nrzGt(cid:3) ≤ rz exp(−Ω(t2)),\n\nPr (cid:2)σmax\n\n(cid:0)X(D( ̃u) − D( ̄u))X ⊤(cid:1) ≥\n\n√\n\n(49) where rz is the number of fractional entries in ̄z. The standard matrix concentration bound (e.g., matrix Hoeffding) states that for n-dimensional random matrices X1, X2, . . . , XM , we have that P r[σmax(X1 + · · · + XM ) ≥ α] ≤ 2n exp(−α2/(8M σ2)), where σ upper bounds the operator norms of X1, X2, . . . , XM almost surely. In the context of Eq. (49), we have that σ = G and we set α = rzGt. Using the matrix concentration inequality, we upper bound the Left-Hand-Side of Eq. (49) by 2n exp(−rzt2/8M ) ≤ 2n exp(−t2/8) (since rz ≥ M ). Note that this bound is already good enough since the only difference from the Right-Hand-Side of Eq. (49) is the factor 2n instead of rz. These factors would go into the logarithmic factor in the final error bound of Theorem 2.4 so they do not make much difference. To improve the factor 2n to rz in Eq. (49), we need to show that there exists rz dimensional subspace whose basis vector denoted by the columns of Q ∈ Rn×rz so that X(D( ̃u) − D( ̄u))X ⊤ can be written as Q(X ′ M are rz-dimensional matrices and 1, . . . , X ′ their operator norms are also bounded by G). The construction of Q and X ′ M is possible because each matrix associated with a fractional variable ̄zj is low rank and there are only rz fractional ̄zj’s. We then apply the above matrix concentration bound to X ′ For X(D(u) − D( ̃u))X ⊤, we have that\n\nM )Q⊤ almost surely (where X ′\n\nM to derive Eq. (49).\n\n1 + . . . X ′\n\n1 + . . . X ′\n\n1, . . . , X ′\n\nX(D(u) − D( ̃u))X ⊤ =\n\n(ui − ̃ui)XiX ⊤ i .\n\nAgain, by our assumption, the operator norm of (ui − ̃ui)XiX ⊤ is at most 1, and the mean of the random matrix is 0 (even when conditioned on z). Therefore, by the Ahlswede-Winter matrix concentration bound Ahlswede & Winter (2002); Oliveira et al. (2010), we have that\n\ni\n\nPr (cid:2)σmax\n\n(cid:0)X(D(u) − D( ̃u))X ⊤(cid:1) ≥\n\nrut|z(cid:3) ≤ ru exp(−Ω(t2)),\n\n(50)\n\n(cid:88)\n\ni\n\n√\n\nwhere ru is the number of fractional entries in ̄u.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nCombining equation 46, equation 47, equation 48, equation 49, and equation 50, we have that with probability at least (1 − δ), it holds that\n\nH(u) − P ∗ ≤ O\n\nproving the theorem.\n\n(cid:32) (cid:112)rz log(rz/δ)G + (cid:112)ru log(ru/δ)\n\n(cid:33)\n\n,\n\nρ\n\nD ANALYSIS OF THE PERFORMANCE OF LBR ON SYNTHETIC RANDOM\n\nENSEMBLES\n\nD.1 PROOF OF THEOREM 3.1\n\nProof of Theorem 3.1. Let\n\nM =\n\n(cid:18) 1 ρ\n\nXD(u∗)X ⊤ + I\n\n(cid:19)−1\n\n.\n\nFor each feature index i ∈ {1, 2, . . . , n}, by y = M w + ε, we have that i X ⊤M ε.\n\n(51) We first bound e⊤ i X ⊤M Xw as follows. Applying Lemma D.1 to each feature index i such that wi ̸= 0, and via a union bound, we have that there exists a universal constant c1 ∈ (0, 1/3200), such that with probability at least (1 − 3k exp(−c1n1−2δ)), it holds that\n\ni X ⊤M Xy = e⊤ e⊤\n\ni X ⊤M Xw + e⊤\n\n∀i : wi ̸= 0,\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 ρ\n\ni X ⊤M Xw e⊤\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n√\n\n√\n\n∈ [0.9/\n\nk, 1.1/\n\nk].\n\n(52)\n\nAlso, applying Lemma D.2 to each feature index i such that wi = 0, and via a union bound, we have that with probability at least (1 − 4(d − k) exp(−c1n/k)), it holds that\n\n∀i : wi = 0,\n\n(cid:12) (cid:12) (cid:12) (cid:12) i X ⊤M ε as follows. Note that M ≼ I. Therefore, applying Lemma D.3 to each k), and via a union bound, we have that with probability at least\n\ni X ⊤M Xw e⊤\n\n≤ 0.1/\n\n(53)\n\n1 ρ\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n√\n\n√\n\nk.\n\nWe then bound e⊤\n\nfeature index i (with τ = 0.1ρ/ (1 − 2d exp(−n/8) − 2d exp(−n2δ/(400γ2k))), it holds that\n\n∀i ∈ {1, 2, . . . , d}, (cid:12)\n\n(cid:12)e⊤\n\ni X ⊤M ε(cid:12)\n\n(cid:12) ≤ 0.1ρ/\n\n√\n\nk.\n\n(54)\n\nNow we condition on the event that all of equation 52, equation 53, and equation 54 happen. By equation 51, we have that\n\n∀i : wi ̸= 0 : (cid:12) ∀i : wi = 0 : (cid:12) √\n\n(cid:12)e⊤ (cid:12)e⊤\n\ni X ⊤M Xy(cid:12) i X ⊤M Xy(cid:12)\n\n(cid:12) ∈ [0.8ρ/\n\n√\n\n√\n\nk, 1.2ρ/ √\n\n(56) k)2 − λk) · kmin (where we let kmin = We now set λk = (0.2ρ/ minj∈{1,2,...,h}{kj} be the size of the smallest non-empty group (in terms of non-zero coefficients)), and verify the conditions in Corollary 2.3 as follows:\n\nk)2, λh = ((0.79ρ/\n\n(cid:12) ∈ [0, 0.2ρ/\n\nk].\n\n√\n\nk],\n\n(55)\n\n1. Fix any group g such that z∗\n\ng = 1. For each i ∈ g such that u∗\n\ni X ⊤M Xy)2 ≥ (0.8ρ/ i = 0, we have that wi = 0 and therefore (e⊤\n\nand therefore (e⊤ that u∗ equation 56.\n\n√\n\ni = 1, we have that wi ̸= 0 k)2 > λk by equation 55. For each i ∈ g such k)2 ≤ λk by\n\ni X ⊤M Xy)2 ≤ (0.2ρ/\n\n√\n\n2. Fix any group g such that z∗\n\ng = 1. By equation 55, we have (cid:80)\n\ni∈g:u∗\n\ni =1((e⊤\n\ni X ⊤M y)2 −\n\n√\n\nλk) ≥ ((0.8ρ/\n\nk)2 − λk) · kmin > λh.\n\n3. Fix any group g such that z∗\n\ng = 0. By equation 55 and equation 56, we have\n\n(cid:80)\n\ni∈g max{(e⊤\n\ni X ⊤M y)2 − λk, 0} ≤ 0 < λh.\n\nFinally, the theorem is proved by collecting the failure probabilities of the desired events (equation 52, equation 53, and equation 54).\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nD.2 PROOF OF THEOREM 3.2\n\nProof of Theorem 3.2. Let\n\nM =\n\n(cid:18) 1 ρ\n\nXD(u∗)X ⊤ + I\n\n(cid:19)−1\n\n=\n\n(cid:18) 1 ρ\n\nX1X ⊤\n\n1 + I\n\n(cid:19)−1\n\n=\n\n(cid:18) 1 ρ\n\nX2X ⊤\n\n2 + I\n\n(cid:19)−1\n\n.\n\nFor each feature index i ∈ {1, 2, . . . , n}, by y = M w(1) + ε, we have that i X ⊤M ε. i X ⊤M Xy = e⊤ e⊤\n\ni X ⊤M Xw(1) + e⊤\n\n(57) i X ⊤M Xw(1) as follows. Applying Lemma D.1 to each feature index i such that 1 ∈ (0, 1/3200),\n\n̸= 0, and via a union bound, we have that there exists a universal constant c′\n\ni\n\nWe first bound e⊤ w(1) such that with probability at least (1 − 3k exp(−c′ (cid:12) (cid:12) (cid:12) (cid:12)\n\ni X ⊤M Xw(1) e⊤\n\n∀i : w(1)\n\n̸= 0,\n\n1 ρ\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni\n\n1n1−2δ)), it holds that |, 1.1|w(1)\n\n∈ [0.9|w(1)\n\ni\n\ni\n\n|] ⊆ [0.9ξ, 1.1ζ 1/4ξ].\n\n(58)\n\nSimilarly, applying Lemma D.1 to each feature index i such that w(2) we have that with probability at least (1 − 3k exp(−c′\n\ni\n\n∀i : w(2)\n\ni\n\n̸= 0,\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 ρ\n\ni X ⊤M Xw(1) e⊤\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 ρ\n\n=\n\ni X ⊤M Xw(2) e⊤\n\n1n1−2δ)), it holds that ∈ [0.9|w(2)\n\n|, 1.1|w(2)\n\ni\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n̸= 0, and via a union bound,\n\n|] ⊆ [0.9ξ, 1.1ζ 1/4ξ].\n\n(59)\n\nFinally, applying Lemma D.2 to each feature index i such that both w(1) union bound, we have that with probability at least (1 − 4(d − 2k) exp(−c′\n\ni = 0 and w(2)\n\ni = 0, and via a\n\n1nξ2)), it holds that\n\n∀i : w(1)\n\ni = 0 ∧ w(2)\n\ni = 0,\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 ρ\n\ni X ⊤M Xw(1) e⊤\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 0.1ξ.\n\n(60)\n\ni X ⊤M ε as follows. Note that M ≼ I. Therefore, applying Lemma D.3 to each We then bound e⊤ feature index i (with τ = 0.1ξρ), and via a union bound, we have that with probability at least (1 − 2d exp(−n/8) − 2d exp(−ξ2n2δ/(400γ2))), it holds that\n\n∀i ∈ {1, 2, . . . , d}, (cid:12)\n\n(cid:12)e⊤\n\ni X ⊤M ε(cid:12)\n\n(cid:12) ≤ 0.1ξρ.\n\n(61)\n\nNow we condition on the event that all of equation 58, equation 59, equation 60, and equation 61 happen. By equation 57, we have that ̸= 0 ∨ w(2) i = 0 ∧ w(2)\n\n(63) We now set λk = (0.2ξρ)2, λh = ((0.79ξρ)2 − λk) · (k/h), and verify the conditions in Corollary 2.3 as follows:\n\ni X ⊤M Xy(cid:12) i X ⊤M Xy(cid:12)\n\n̸= 0 : (cid:12) i = 0 : (cid:12)\n\n(cid:12) ∈ [0.8ξρ, 1.2ζ 1/4ξρ],\n\n(cid:12) ∈ [0, 0.2ξρ].\n\n∀i : w(1)\n\n∀i : w(1)\n\n(cid:12)e⊤ (cid:12)e⊤\n\n(62)\n\ni\n\ni\n\n1. Fix any group g such that z∗\n\nand therefore (e⊤ i = 0, we have that w(1) u∗ equation 63.\n\ng = 1. For each i ∈ g such that u∗\n\n̸= 0 i X ⊤M Xy)2 ≥ (0.8ξρ)2 > λk by equation 62. For each i ∈ g such that i X ⊤M Xy)2 ≤ (0.2ξρ)2 ≤ λk by\n\ni = 0 and therefore (e⊤\n\ni = 1, we have that w(1)\n\ni = w(2)\n\ni\n\n2. Fix any group g such that z∗\n\ng = 1. By equation 62, we have (cid:80)\n\nλk) = ((0.8ξρ)2 − λk) · (k/h) > λh.\n\ni∈g:u∗\n\ni =1((e⊤\n\ni X ⊤M y)2 −\n\n3. Fix any group g such that z∗\n\ni X ⊤M y)2 − λk, 0} ≤ ((1.44ξρ)2√\n\ng = 0. By equation 62 and equation 63, we have ζ − λk) · (k/(ζh)) ≤ ((1.44ξρ)2 − ζ < λh, where the last inequality holds for large\n\nζh)) = 2.0336(ξρ)2 · (k/h)/\n\n√\n\n(cid:80)\n\ni∈g max{(e⊤\n\n√\n\nλk) · (k/( enough constant ζ > 1.\n\nFinally, the theorem is proved by collecting the failure probabilities of the desired events (equation 58, equation 59, equation 60, and equation 61).\n\nD.3 TECHNICAL LEMMAS\n\nLemma D.1. Suppose k ≤ n/4. Let X ∈ Rn×k be a matrix with i.i.d. N (0, 1) entries. Let ρ = n1/2+δ (δ ≥ 0), and M = (I + 1 ρ XX ⊤)−1. For any ε ≥ 32nδ−1/2, any fixed vector z ∈ Rk such that ∥z∥∞ ≤ 1 and any fixed index i ∈ {1, 2, . . . , k}, with probability at least\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n(1 − 3 exp (cid:0)−n1−2δε2/2048(cid:1)), it holds that\n\n(cid:18) 1 ρ\nwhere ei is the i-th (column) basis vector.\n\ne⊤\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni\n\nX ⊤M X − I\n\n(cid:19)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nz\n\n≤ ε,\n\nProof. The proof follows the similar lines of Part (1) of the proof of Lemma 2 in Pilanci et al. (2015). However, we adopt a different regularization parameter ρ. We write X = U DV ⊤ for the singular decomposition of X. By standard results on the singular value of Gaussian random matrices (e.g., Davidson & Szarek (2001)), for each t ≥ 0, it holds that\n\nPr[∀j ∈ {1, 2, . . . , k} :\n\nn −\n\nk − t ≤ Djj ≤\n\nn +\n\nk + t] ≥ 1 − 2 exp(−t2/2).\n\n√\n\n√\n\n√\n\n√\n\n√\n\n√\n\n√\n\nIn particular, if we set t =\n\nn/4, we have that\n\nPr[∀j ∈ {1, 2, . . . , k} :\n\n(64) The rest of the proof will be carried out by conditioning on the successful event in equation 64. Note that\n\nn/4] ≥ 1 − 2 exp(−n/32).\n\nn/4 ≤ Djj ≤ 7\n\nX ⊤M X = V (ρI + D2)−1D2V ⊤,\n\nand therefore,\n\n1 ρ\n\nX ⊤M X − I = V [(ρI + D2)−1D2 − I]V ⊤ = V ̃DV ⊤,\n\nwhere we let ̃D := diag({\n\nD2 jj ρ+D2 jj\n\n− 1}j∈{1,2,...,k}), and have that ̃Djj ≥ 0 and ̃Djj ≤ 16nδ−1/2 for\n\nall j ∈ {1, 2, . . . , k} due to the successful event in equation 64. Note that\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ne⊤\n\ni\n\n(cid:18) 1 ρ\n\nX ⊤M X − I\n\n(cid:19)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nz\n\n=\n\n(cid:12) (cid:12)e⊤ (cid:12)\n\ni V ̃DV ⊤z\n\n(cid:12) (cid:12) (cid:12) =\n\n(cid:88)\n\nVij ̃Djj\n\n(cid:88)\n\nVqjzq\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nj\n\nV 2\n\nij\n\n ̃Djjzi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\nj\n\n(cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12)\n\nj\n\nq\n\n(cid:88)\n\nq:q̸=i\n\nVij ̃Djj\n\nVqjzq\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\n(65)\n\nIt is easy to bound the first term in equation 65 by (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Let ̃V be the (k − 1) × k matrix obtained by removing the i-th row from V , and let ̃z be the (k − 1)- dimensional vector by removing the i-th entry of z. We can rewrite the second term in equation 65 as\n\n(cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 16nδ−1/2.\n\n ̃Djjzi\n\n(66)\n\nV 2\n\nij\n\nj\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nVij ̃Djj\n\n(cid:88)\n\nVqjzq\n\nj\n\nq:q̸=i\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n=\n\n(cid:12) (cid:12)e⊤ (cid:12)\n\n(cid:12) i V ̃D ̃V ⊤ ̃z (cid:12) (cid:12) .\n\n(67)\n\nObserve that even when conditioned on D (and therefore ̃D), e⊤ i V ̃D ̃V ⊤ is a (k − 1)-dimensional vector pointing towards a uniform random direction, and its 2-norm is at most 16nδ−1/2. On the other hand, ̃z is a fixed (k − 1)-dimensional vector with ∥ ̃z∥2 ≤ k − 1. Therefore, by standard spherical concentration inequality, we have that (cid:12) i V ̃D ̃V ⊤ ̃z (cid:12)\n\n−n(ε − 16nδ−1/2)2/512\n\n(cid:12) ≤ ε − 16nδ−1/2(cid:105)\n\n(cid:104)(cid:12) (cid:12)e⊤ (cid:12)\n\nPr\n\n√\n\n(cid:17)\n\n(cid:16)\n\n(68) Combining equation 65, equation 66, equation 67, equation 68, and collecting the probabilities, we prove the desired result.\n\n≥ 1 − exp ≥ 1 − exp (cid:0)−n1−2δε2/2048(cid:1) .\n\nLemma D.2. Suppose k ≤ n/4. Let X ∈ Rn×k be a matrix with i.i.d. N (0, 1) entries. Let u ∈ Rk be a column vector with i.i.d. N (0, 1) entries. Let ρ = n1/2+δ (δ ≥ 0), and M = (I + 1 ρ XX ⊤)−1. For any ε ∈ (0, 1) and any fixed vector z ∈ Rk such that ∥z∥2 ≤ 1 , with probability at least (1 − 4 exp (cid:0)−nε2/32(cid:1)), it holds that\n\n≤ ε.\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 ρ\n\nu⊤M Xz\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nProof. The following proof is based on the standard calculation, which also appeared in Part (2) of the proof of Lemma 2 in Pilanci et al. (2015). Write X = U DV ⊤ for the singular decomposition of X. Again, we have equation 64 and will condition on the successful event in equation 64 for the rest of the proof. Note that\n\n1 ρ\n\nM Xz = U (ρD−1 + D)−1V ⊤z,\n\nand therefore\n\n(cid:13) (cid:13) (cid:13) (cid:13)2 by the successful event in equation 64. Thus, 1 ρ u⊤M Xz is a centered Gaussian with standard (cid:12) (cid:12) (cid:12) > ε is at most 2 exp(−ε2n/32) by the standard\n\n(cid:12) (cid:12) (cid:12) Gaussian tail bound. The lemma is proved by collecting the failure probabilities.\n\nn, and the probability that\n\nρ u⊤M Xz\n\ndeviation 4/\n\nM Xz\n\n≤ 4/\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n1 ρ\n\n√\n\n√\n\nn\n\n1\n\nLemma D.3. Let u ∈ Rn be a column vector with i.i.d. N (0, 1) entries. Let M ∈ Rn×n be any PSD matrix (which might depend on u) such that its eigenvalues are at most 1. Let ε ∈ Rn be an independent column noise vector with i.i.d. N (0, γ2) entries. For any τ > 0, with probability at least (1 − 2 exp(−n/8) − 2 exp(−τ 2/(4γ2n))), it holds that (cid:12) ≤ τ.\n\n(cid:12)u⊤M ε(cid:12) (cid:12)\n\nProof. By standard χ2 concentration results, we have that with probability at least (1−2 exp(−n/8)), it holds that ∥u∥2 Note that ∥M u∥2 ≤ ∥u∥2. Therefore, u⊤M ε ∼ N (0, γ2∥u∥2 we have that\n\n2 ≤ 2n. The rest of the proof will be carried out conditioning on this event.\n\n2). By the standard Gaussian tail bound,\n\nPr[|u⊤M ε| ≤ τ ] ≥ 1 − 2 exp(−τ 2/(2γ2∥u∥2\n\n2)) ≥ 1 − 2 exp(−τ 2/(4γ2n)).\n\nThe lemma is proved by collecting the failure probabilities.\n\nE OPTIMIZATION\n\nWe use the projected Quasi-Newton (PQN) method to solve the optimization LBR defined in equation 10. The details of PQN is elaborated in Schmidt et al. (2009) Algorithm 1, therefore, we refer the interested audiences to Schmidt et al. (2009) for more details. To apply PQN, we need to know the gradient of the objective function in equation 10. The partial gradient of G(u) w.r.t ui can be written as\n\n∂G(u) ∂ui\n\n= −\n\n1 ρ\n\n(cid:32)\n\nX ⊤\n\ni\n\n(cid:18) 1 ρ\n\n(cid:19)−1\n\n(cid:33)\n\nXD(u)X ⊤ + I\n\ny\n\n.\n\n(69)\n\nComputing such a gradient requires the solution of a rank-∥u∥0 linear system of size n, which can be calculated in time O(∥u∥3 0) + O(nd) via the QR decomposition. When the sparsity level k is relatively small, such computation is not expensive. We also need to do the following projection in PQN.\n\nmin x∈Ω\n\n: ∥x − y∥2 2,\n\n(70)\n\nwhere Ω is defined in Section 2.2. The projection on the relaxed constraint set Ω can be efficiently obtained by a commercial solver (we use Gurobi Gurobi Optimization, LLC (2022)).\n\nF NUMERICAL VERIFICATION OF COROLLARY 2.3\n\nCorollary 2.3 states that under the least-squares regression setting, under certain conditions, the solution of the relaxed program is integral and consistent with the optimal solution of the problem with Boolean constraints. In this section, we numerically verify the equivalence established in Corollary 2.3. We consider to generate synthetic data follow Random Ensemble I. we set d = 200, b = 10, k = 50 (ki = 10, ∀i ∈ {1, 2, . . . , 5}), h = 5. We vary sample size n and SNR (controlled by γ) to see whether the solution of the relaxed program is the optimal integral solution of the program with Boolean constraints. For each n and SNR, we randomly generate 10 datasets. We run our relaxed program on these 10 datasets and count the number of solutions that are the same as the integral solutions of the program with Boolean constraints. As shown in the Table S1, the proposed equivalence can be achieved when the sample size and SNR are large.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure S1: Performance comparison for different γs. (a) AI as n increases when γ = 3.5. (b) AG as n increases when γ = 3.5. We average results over 10 datasets and the error bar means 95% confidence interval.\n\nTable S1: Number of solutions that is the integral solutions of the program with Boolean constraints .\n\nSNR n = 100 n = 200 n = 300 n = 1, 000 40 30 20\n\n10/10 10/10 0/10\n\n10/10 10/10 0/10\n\n10/10 10/10 10/10\n\n0/10 0/10 0/10\n\nG ADDITIONAL EXPERIMENTS FOR RANDOM ENSEMBLE I\n\nWe compare our proposed method with other state-of-the-art methods on simulation data generated from Random Ensemble I with different γs. We first set γ = 3.5 and compare the performance of the competing methods on recovering the individual and group supports. Note that when γ = 3.5, the signal to noise ratio is around 3. We compare the competing methods on 10 different data sets generated from Random Ensemble I with γ = 3.5. As illustrated in Fig. S1 (a) and (b), our proposed method still outperforms all the competing methods in terms of both AI and AG with the increasing number of samples.\n\nH EXPERIMENTS FOR SYNTHETIC DATA SATISFYING MUTUAL\n\nINCOHERENCE CONDITION\n\nIn Random Ensemble I, we generate xi ∼ N (0, I) as i.i.d features. In this experiments, we aims to evaluate the performance of the competing methods in the presence of correlation between features. We follow the way we generate the data in Random Ensemble I, where we set d = 1000, b = 10, k = 50 (ki = 10, ∀i ∈ {1, 2, . . . , 5}), h = 5, and γ = 0.1. Then only different is that we generate xi ∼ N (0, Σ), where Σ is the Toeplitz covariance matrix Σij = p|i−j|. Such matrices satisfy the mutual incoherence condition, required by l1-regularized estimators to be statistically consistent. We consider p = 0.2 and p = 0.7 for Σ to evaluate the competing methods’ performance under different correlations. The performance is shown in Fig. S2. We find that when p = 0.2, support recovery (both individual level Fig. S2(a) and group level Fig. S2(b)) can be easily achieved by most of the models. And our model outperforms other competing methods. However, when p = 0.7, all models have difficulties to accurately recover the support at the individual level (Fig. S2(c)), which makes sense because the correlation between features are high so that finding the correct features becomes more challenge. Our model still outperforms other competing methods at both individual level (Fig. S2(c)) and group level (Fig. S2(d)).\n\nI ADDITIONAL EXPERIMENTS FOR CANCER DRUG RESPONSE PREDICTION\n\nWe first show Table. S3, which illustrates the pathways and genes identified by the proposed method for drug IMATNIB and the corresponding researches that support the findings. We further find the targeted pathways and genes for three other drugs: GEFITINIB, BEXAROTENE, and BOSUTINIB. In Table. S4, we show the number of targeted pathways and genes identified by\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nFigure S2: Performance comparison for different ps. (a) AI as n increases when p = 0.2. (b) AG as n increases when p = 0.2. (c) AI as n increases when p = 0.7. (d) AG as n increases when p = 0.y. We average results over 10 datasets and the error bar means 95% confidence interval for (a)-(d).\n\neach competing method and the out-of-sample MSE. As shown our method achieves the smallest out-of-sample MSE and the fewest number of pathways and genes.\n\nJ TIME COMPARISON FOR RANDOM ENSEMBLE I\n\nWe compared the running time for experiments of Random Ensemble I when the sample size is 1,000 in the Table. S2.\n\nTable S2: Time comparison (mean ± s.d.) for Random Ensemble I when the sample size is 1000. Method Time (s)\n\nSGL_∞ SGCover 20.8±0.9 4.2± 0.2\n\nENEt 0.2±0.03\n\nProposed 24.4±2.1\n\nSGL 1.4±0.1\n\nK IMPLEMENTATION DETAILS\n\nFor section 4.1 subsection “Feature selection with given support sizes k and h” and section 4.2, we select the parameters as follows. For our method, because k and h are given, we only have one parameter ρ in (1) left, we select ρ by the 5-fold CV in terms of MSE. For the result of the methods, which cannot control k and h, we just sweep the parameters to let them yield the desired k and h. For the real-world application, we select the parameters in terms of out-of-sample MSE.\n\nL CODE AVAILABILITY The codes for the proposed method can be found here: https://anonymous.4open. science/r/L0GL-F107/Readme\n\nTable S3: Pathways and genes identified by the proposed methods for IMATNIB.\n\nPathway RHO GTPases Activate WASPs and WAVEs\n\nRegulation of PTEN gene transcription\n\nSignaling by PDGF\n\nRetinoid metabolism and transport\n\nTCF transactivating complex\n\nGenes ARPC1B WASF1 ARPC5 WASL CYFIP1 ACTG1 ACTR3 LAMTOR3 LAMTOR4 SNAI1 RPTOR RRAGA RRAGB MBD3 RRAGD PHC3 GATAD2A RCOR1 MECOM CBX8 LAMTOR2\n\nReference Gu et al. (2009); Huang et al. (2008); Chen et al. (2018)\n\nNishioka et al. (2011); Peng et al. (2010); Huang et al. (2014)\n\nPDGFC COL4A3 COL6A2 COL6A3 COL9A3\n\nMalavaki et al. (2013); Li et al. (2006); Heldin (2013)\n\nCLPS LRP8 APOC3 SDC4 LPL LRP10 LRP12 APOA2\n\nHoang et al. (2010)\n\nRBBP5 KAT5 PYGO1 PYGO2 BCL9\n\nZhang et al. (2021); Coluccia et al. (2007); Corrêa et al. (2012)\n\nDeactivation of the beta-catenin transactivating complex\n\nRBBP5 SOX3 SRY PYGO1 PYGO2 CBY1 BCL9\n\nZhou et al. (2003); Leo et al. (2013)\n\nRAS processing\n\nZDHHC9 GOLGA7 BCL2L1 ABHD17B\n\nChung et al. (2006); Braun & Shannon (2008)\n\n25\n\n5001000150000.20.40.60.81Sample sizeAccuracy of group support recoeryp=0.7Accuracy of group Sample sizep=0.2Accuracy of Individual support recovery5001000150000.20.40.60.81(a)ProposedSGLENetSGL∞SGCover50010001500Sample size0.50.60.70.80.91support recoery(b)(c)(d)SGLProposedSGL∞SGCoverSGLProposedSGL∞SGCoverp=0.2ProposedSGLENetSGL∞SGCover5001000150000.20.40.60.81Sample sizeAccuracy of Individual support recoveryp=0.7Published as a conference paper at ICLR 2023\n\nTable S4: Result comparison for three other drugs.\n\nDurg BOSUTINIB\n\nGEFITINIB\n\nMethod Proposed method SGL-Overlap ENet SGCover Proposed method SGL-Overlap ENet SGCover\n\nBEXAROTENE Proposed method\n\nSGL-Overlap ENet SGCover\n\nk (s.d.) 36 64 ( 4.3) 48 (5.3) 240 (10.4) 42 78 (4.6) 49 (5.6) 278 (13.4) 52 86 (3.8) 64 (5.2) 312 (16.3)\n\nh (s.d.) Out-of-sample MSE ± 95% CI\n\n5 9 (0.9) 18(3.2) 15 (1.8) 6\n13 (1.3) 15 (2.1) 12 (2.4) 8\n15 (1.5) 17 (3.5) 18 (2.3)\n\n29.4±2.1 42.6 ± 2.3 35.4 ± 3.1 52.4 ± 4.2 35.4 ± 1.4 4.78 ± 2.1 38.7 ± 2.9 59.6 ± 3.6 36.9 ± 1.8 61.8 ± 2.7 38.4 ± 2.3 58.2 ± 3.1\n\n26",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose learning group structured sparsity using explicit constraints on the feature and group sparsity. They derive a reformulation of the proposed problem in terms of boolean variables for the selected features and groups, which is then converted into a convex problem by relaxing the boolean constraints to lie within [0, 1]. Theoretical results are presented to (a) give conditions when the relaxed problem recovers the true model structure, (b) special cases when the model and design matrix are generated according to a random ensemble protocol. \nThe experimental results illustrate the superiority of the proposed scheme against similar methods such as SGL, SGL_\\infty.\n\n# Strength And Weaknesses\n\nStrengths\n1. The paper is written well and easy to follow. \n2. Presents theoretical results on support recovery and convergence\n\nQuestions / Comments\n1. The idea has some similarity to (I) \"Convex relaxation for combinatorial penalties - Obozinski. et. al, 2012\" and (ii)\"Identifying Groups of Strongly Correlated Variables through Smoothed Ordered Weighted 𝐿1-norms - Sankaran et.al 2017\". The above works consider L_p relaxations of combinatorial (submodular) penalties, the special case of which is L_2 relaxation of the penalties. The formulation proposed in this submitted paper may be compared to (i) and (ii) after converting the constraints into a penalty. \n\n2. Most/All the results are presented for non-overlapping groups only, even though the problem setup initially studied is in general. It may be better to stress this in the paper to avoid misrepresentation.  \n\n3. The random ensembles are studied for iid design matrices. May be interesting to understand how the algorithms compare in the presence of correlation within columns of X. \n\n4. What are the regimes where the other compared algorithms such as SGL, SGL_\\infty may do better in the structure recovery ? Or, is the proposed formulation expected to perform better than them in many other settings than the ones considered in this paper. If so, how do we reason out on the performance ?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity - The paper is easy to follow\n\nQuality / Novelty - this is an interesting take on learning sparse group structured models. May be worth investigating the similarities to the references quoted earlier in this review.\n\n# Summary Of The Review\n\nOverall, this paper presents an approch for learning group structured sparse models. The theoretical results and evaluation are convincing.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nENHANCING META LEARNING VIA MULTI-OBJECTIVE SOFT IMPROVEMENT FUNCTIONS\n\nRunsheng Yu1, Weiyu Chen1, Xinrun Wang2, James T. Kwok1 1Department of Computer Science and Engineering, Hong Kong University of Science and Technology 2School of Computer Science and Engineering, Nanyang Technological University runshengyu@gmail.com, wchenbx@cse.ust.hk, xinrun.wang@ntu.edu.sg, jamesk@cse.ust.hk\n\nABSTRACT\n\nMeta-learning tries to leverage information from similar learning tasks. In the commonly-used bilevel optimization formulation, the shared parameter is learned in the outer loop by minimizing the average loss over all tasks. However, the converged solution may be compromised in that it only focuses on optimizing on a small subset of tasks. To alleviate this problem, we consider meta-learning as a multi-objective optimization (MOO) problem, in which each task is an objective. However, existing MOO solvers need to access all the objectives’ gradients in each iteration, and cannot scale to the huge number of tasks in typical meta-learning settings. To alleviate this problem, we propose a scalable gradient-based solver with the use of mini-batch. We provide theoretical guarantees on the Pareto optimality or Pareto stationarity of the converged solution. Empirical studies on various machine learning settings demonstrate that the proposed method is efficient, and achieves better performance than the baselines, particularly on improving the performance of the poorly-performing tasks and thus alleviating the compromising phenomenon.\n\n1\n\nINTRODUCTION\n\nMeta-learning, also known as “learning to learn\", aims to enable models to learn more effectively by leveraging information from many similar learning tasks (Hospedales et al., 2020). In recent years, meta-learning has received much attention for its fast adaptation to new learning scenarios with limited data (Kao et al., 2021; Finn et al., 2017; Snell et al., 2017; Lee et al., 2019; Nichol et al., 2018; Deleu et al., 2022; Rajeswaran et al., 2019; Vilalta & Drissi, 2002). It is usually formulated as a bi-level optimization problem (Franceschi et al., 2018; Hong et al., 2020), which finds task-specific parameters in the inner level and minimizes the average loss over tasks in the outer level.\n\nRecently, Wang et al. (2021) reformulate meta-learning as a multi-task learning problem. From this perspective, minimizing the average loss in the outer level using (stochastic) gradient descent may not always be desirable. Specifically, it may suffer from the compromising (or conflicting) phenomenon, in which the converged solution only focuses on minimizing the losses of a small subset of tasks while ignoring the others (Yu et al., 2020; Liu et al., 2021a; Sener & Koltun, 2018). This compromised solution may thus lead to poor performance.\n\nTo alleviate this problem, we propose reformulating meta-learning as a multi-objective optimization (MOO) problem, in which each task is an objective. The performance of all tasks (objectives) are then considered during optimization (Emmerich & Deutz, 2018). A popular class of MOO solvers is the gradient-based approach (Liu et al., 2021a; Yu et al., 2020; Sener & Koltun, 2018; Navon et al., 2022; Liu et al., 2021b), with prominent examples such as the multiple-gradient descent algorithm (MGDA) (Désidéri, 2012; Sener & Koltun, 2018), PCGard (Yu et al., 2020), and CAGard (Liu et al., 2021a). In each iteration, they find a common descent direction among all objective gradients, instead of simply optimizing the average performance over all objectives.\n\nExisting gradient-based MOO methods require using gradients from all the objectives. However, when formulating meta-learning as a MOO problem with each task being an objective, computing all these gradients in each iteration can become very expensive, as the number of objectives (i.e., tasks)\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ncan be huge. For example, in 5-way 1-shot classification on the miniImageNet data, the total number of meta-training tasks is (cid:0)64\n\n(cid:1) ≈ 7 × 106.\n\n5\n\nTo address this challenge, we propose a scalable MOO solver by using the improvement function (Miettinen & Mäkelä, 1995; Mäkelä et al., 2016; Montonen et al., 2018) with the help of mini-batch. On the other hand, we show that a trivial extension of existing gradient-based MOO methods with the use of mini-batch does not guarantee Pareto optimality and has poor performance in practice.\n\nOur main contributions are as follows: (i) To alleviate the compromising phenomenon, we reformulate meta-learning as a multi-objective optimization problem in which each task is an objective; (ii) To handle the possibly huge number of tasks, we propose a scalable gradient-based solver. (iii) We provide theoretical guarantees on the Pareto optimality or Pareto stationarity of the converged solution. (iv) Empirical studies on few-shot regression, few-shot classification, and reinforcement learning demonstrate that the proposed method achieves better performance, particularly in improving the performance of the poorly-performing tasks and thus alleviating the compromising phenomenon.\n\n2 BACKGROUND\n\nMulti-Objective Optimization (MOO). m ≥ 2 objectives f1(x), . . . , fm(x):\n\nIn MOO (Marler & Arora, 2004), one aims to minimize1\n\nmin x\n\n[f1(x), . . . , fm(x)].\n\n(1)\n\nDefinition 2.1. (Global Pareto optimality) (Miettinen, 2012; Mäkelä et al., 2016) x∗ is global Pareto optimal if there does not exist another x such that fτ (x∗) ≥ fτ (x) for all τ ∈ {1, . . . , m}, and fτ ′(x∗) > fτ ′(x) for at least one τ ′ ∈ {1, . . . , m}.\n\nThe Pareto front (PF) is the set of multi-objective values of all global Pareto-optimal solutions. Definition 2.2. (Pareto stationarity) (Miettinen, 2012; Désidéri, 2012) x∗ is Pareto-stationary if there exist {uτ }m\n\nτ =1 uτ ∇xfτ (x)∥ = 0, uτ ≥ 0 ∀τ and (cid:80)m\n\nτ =1 such that ∥ (cid:80)m\n\nτ =1 uτ = 1.\n\nNote that global Pareto optimal solutions are also Pareto stationary (Désidéri, 2012). Analogous to the extension from a stationary point to an ε-stationary point (Lin et al., 2020), we extend Pareto stationarity to ε-Pareto stationarity. Obviously, 0-Pareto stationarity reduces to Pareto stationarity. Definition 2.3. (ε-Pareto stationarity). For a given ε, x is ε-Pareto-stationary iff there exist {uτ }m such that ∥ (cid:80)m Definition 2.4. (Improvement function) (Montonen et al., 2018) The improvement function of problem (1) is: H(x, x′) = maxτ =1...,m {fτ (x) − fτ (x′)}.\n\nτ =1 uτ ∇xfτ (x)∥ ≤ ε, uτ ≥ 0 ∀τ and (cid:80)m\n\nτ =1 uτ = 1.\n\nτ =1\n\nNote that x∗ satisfying x∗ = arg minx H(x, x∗) (intuitively, x∗ cannot be further improved) is Pareto stationary (Montonen et al., 2018). To find x∗, one can perform steepest descent on H:\n\nxs+1 = xs + βd∗, d∗ = arg min\n\nd\n\nH(xs + d, xs) +\n\nλ′ 2\n\n∥d∥2,\n\n(2)\n\nwhere xs is the iterate at iteration s, β is the learning rate satisfying H(xs + βd, xs) < H(xs, xs), and λ′ is a hyper-parameter. It can be shown that when s → ∞, xs is Pareto stationary (Montonen et al., 2018).\n\nIn this paper, we focus on gradient-based MOO methods, including MGDA (Désidéri, 2012; Sener & Koltun, 2018), PCGard (Yu et al., 2020), and CAGard (Liu et al., 2021a). They assign weights to each objective’s gradient and find a common descent direction that decreases the losses of all objectives. For example, MGDA finds the direction g∗(x) = (cid:80)m\n\nτ ∇xfτ (x) in each iteration, where\n\nτ =1 γ∗\n\n{γ∗\n\nτ } = arg min {γτ }\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:88)m\n\nτ =1\n\nγτ ∇xfτ (x)\n\n(cid:13) (cid:13) (cid:13)\n\n2\n\ns.t.\n\n(cid:88)m\n\nτ =1\n\nγτ = 1, γτ ≥ 0, ∀τ.\n\n(3)\n\nMeta-Learning. Meta-learning aims to achieve good performance with limited data and computation (Hospedales et al., 2020). Most of them are gradient-based (Nichol et al., 2018; Deleu et al., 2022; Rajeswaran et al., 2019; Zhou et al., 2019; Shu et al., 2019) or metric-based (Snell et al., 2017;\n\n1Without loss of generality, we consider minimization in this paper.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nLee et al., 2019; Vinyals et al., 2016). Let T be the set of all m tasks, and w be the shared model parameter. For a task τ ∈ T , let Dτ be its dataset and Lτ the corresponding loss function. It tries to obtain task-specific parameter wτ from the shared w as w∗ τ (w). Meta-learning is usually formulated as the following bilevel optimization problem (Ji et al., 2021):\n\nminw\n\n(cid:88)m\n\nτ =1\n\nLτ (wτ ) s.t. wτ = w∗\n\nτ (w).\n\n(4)\n\nThe inner subproblem learns the task-specific parameter wτ for each τ , while the outer subproblem learns w by minimizing the average loss over tasks in T . As m can be very large, usually a mini-batch B of tasks are uniformly sampled from T , and w is then updated as ws+1 = ws − β 1 |B|\n\nτ (w)) (Finn et al., 2017).\n\nτ ∈B Lτ (w∗\n\n(cid:80)\n\nBy taking each Lτ (w∗ τ (w)) in (4) as an objective, this can be regarded as a weighted sum in multi-task learning (Wang et al., 2021). As observed in (Sener & Koltun, 2018; Yu et al., 2020), gradient descent on this weighted sum can suffer from the compromising phenomenon, in which the loss obtained on some task τ ′ can be much larger than the losses on the other tasks.\n\n3 SOFT IMPROVEMENT MULTI-OBJECTIVE META-LEARNING (SIMOL)\n\nWe take the view of meta-learning as multi-task learning in (Wang et al., 2021) one step further and consider the meta-learning problem as the following multi-objective optimization (MOO) problem:\n\nminw(L1(w∗\n\n1(w)), . . . , Lm(w∗\n\nm(w))),\n\n(5)\n\nin which each task corresponds to an objective. This considers all the individual tasks instead of simply considering the total loss over all tasks (Liu et al., 2021a). Recently, Ye et al. (2021) also use multi-objective learning into meta-learning. However, their focus is not on addressing the compromising phenomenon and they do not treat each task as an objective. Instead, besides minimizing the average task loss in (4), they consider adding some other objectives such as robustness to adversarial attacks. Moreover, MGDA is still used to find the Pareto optimal solution. However, as in other gradient-based MOO methods (Yu et al., 2020; Liu et al., 2021a), MGDA requires collecting gradients from all m objectives in each iteration (as can be seen from its optimization problem (3)). This is computationally feasible only when there are a small number of objectives.2 When each task is treated as an objective, the number of objectives can easily be in the millions (as in performing 5-way 1-shot classification on miniImageNet).\n\nAnother widely adopted MOO based methods are the Chebyshev methods (Miettinen, 2012; Mao et al., 2020; Momma et al., 2022), which leverage the weighted Chebyshev problem to find the Pareto front. However, these methods also cannot handle a huge number of tasks, as the computational complexity per epoch for these methods is O(m2) , where m is the number of tasks.\n\nTo alleviate this problem, one solution is to use only a mini-batch of objectives in each iteration. For example, when a subset B of objectives is used, MGDA’s optimization problem in (3) becomes:\n\nmin{γτ }\n\n(cid:88)\n\n(cid:13) (cid:13) (cid:13)\n\nτ ∈B\n\nγτ ∇τ (Lτ (w∗\n\nτ (w))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\ns.t.\n\n(cid:88)\n\nτ ∈B\n\nγτ = 1, γτ ≥ 0, ∀τ.\n\nτ =1 γτ = 1 in (3) is also changed to (cid:80)\n\nHowever, the descent direction then only considers objectives in B, and the original normalization constraint (cid:80)m τ ∈B γτ = 1. The obtained solution may no longer Pareto optimal. In the following, we demonstrate this by using a simple toy example with two objectives (f1(x) and f2(x), where x ∈ R2) from (Liu et al., 2021a; Navon et al., 2022).3 As can be seen from Figure 1, mini-batch MGDA (with a mini-batch of 1) cannot converge to the Pareto front.\n\n3.1 SOFT IMPROVEMENT FUNCTION In this section, we propose a scalable MOO solver with the use of the improvement function (Montonen et al., 2018). The proposed solver is agnostic to the number of tasks, while still theoretically guaranteeing that the solution is Pareto optimal.\n\nUsing Definition 2.4, the improvement function for problem (5) is:\n\nH(w, w′) = maxτ =1...,m {Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))} .\n\n(6)\n\n2In the meta-learning experiments of (Ye et al., 2021), they only consider two objectives. 3Definitions for f1, f2 and the environment setup are in Appendix A.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(b) mini-batch MGDA. Figure 1: Convergence on a two-objective toy dataset with mini-batch size 1. The Pareto front is shown in black.\n\n(c) proposed SIMOL.\n\n(a) MGDA.\n\nConsider the optimization problem\n\nmaxπ Eτ ∼π [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))] ,\n\n(7)\n\nwhere π is a probability density function on τ . The following Lemma shows that (6) and (7) are equivalent when π is the Dirac delta distribution concentrated on the task corresponding to the maximum in (6). All the proofs are in Appendix C. Lemma 3.1. H(w, w′) = maxπ Eτ ∼π [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))].\n\nUsing (2) and Lemma 3.1, w can be updated as\n\nws+1 = ws + βd∗,\n\nd∗ = arg mind (maxπ Eτ ∼π [Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))]) +\n\nTaking the first-order approximation\n\nLτ (w∗\n\nτ (ws + d)) ≃ Lτ (w∗\n\nτ (ws)) + ∇wLτ (w∗\n\nτ (ws))⊤d,\n\nλ′ 2\n\n∥d∥2.\n\n(8)\n\n(9)\n\n(10)\n\nthe minimax theorem (Simons, 1995) can be used to swap the min and max operators in (9), as\n\n(π∗, d∗) = arg maxπ\n\n(cid:18)\n\nmind Eτ ∼π [Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))] +\n\n(cid:19)\n\n.\n\n∥d∥2\n\nλ′ 2\n\n(11)\n\nThe following Proposition shows that the inner minimization problem has a closed-form solution.\n\nProposition 2λ′ ∥Eτ ∼π∇wLτ (w∗ −1\n\n3.2. mind Eτ ∼π [Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗ τ (w))|w=ws∥2, and the optimal d is d∗ = − 1\n\nτ (ws))] + λ′ Eτ ∼π[∇wLτ (w∗\n\n2 ∥d∥2 τ (w)|w=ws ].\n\nλ′\n\n=\n\nThe expectation in Proposition 3.2 requires sampling tasks from π. An easier alternative is to sample tasks from the uniform distribution U (·) over the set T of all tasks, and then weighting each sampled task τ with r(τ ) ≡ π(τ )/U (τ ). Note that\n\nEτ ∼U r(τ ) =\n\n(cid:88)\n\nτ\n\nU (τ )π(τ )/U (τ ) =\n\n(cid:88) τ\n\nπ(τ ) = 1.\n\n(12)\n\nWe further parameterize r as a neural network rθ with parameter θ. Using Proposition 3.2, we can then rewrite (11) as\n\nθ∗ = arg max\n\nθ\n\n2λ′ ||Eτ ∼U rθ(τ )∇wLτ (w∗\n\nτ (w))|w=ws ||2 −\n\n−1\n\nλ′′ 2\n\n(Eτ ∼U rθ(τ ) − 1)2,\n\n(13)\n\nwhere the last term (with another hyper-parameter λ′′) is a penalty for enforcing the constraint in (12). For notational simplicity, we denote the objective in (13) by K(θ).\n\nIn principle, θ∗ can be obtained from (13) by gradient ascent. However, problem (13) involves an expectation over tasks. Recall that we have a total of m tasks, and m can be huge. Hence, using all of them to compute this expectation may not be feasible. Instead, Let B be a mini-batch of k tasks, and denote the the mini-batched version of the objective in (13) as:\n\n ̃KB(θ) ≡\n\n−1 2ˆλ′\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 |B|\n\n(cid:88)\n\nτ ∈B\n\nrθ(τ )∇wLτ (w∗\n\nτ (w))|w=ws\n\n(cid:32)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n−\n\nˆλ′′ 2\n\n1 |B|\n\n(cid:88)\n\nτ ∈B\n\n(cid:33)2\n\nrθ(τ ) − 1\n\n,\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nwhere ˆλ′, ˆλ′′ are another set of hyper-parameters (which will be set in Proposition 3.3) corresponding to λ′, λ′′ in (13). Note that ̃KT (θ) = K(θ). Let B be the set of all size-k mini-batches (with k > 1). The following Proposition bounds the difference between K(θ), the original objective in (13), and the version 1\n\n ̃KB(θ) based on mini-batches.\n\n(cid:80)\n\n|B|\n\nB∈B\n\nλ′ C1|B|\n\nand\n\nˆλ′′\n\n=\n\nλ′′C1|B|k2.\n\nWe\n\nhave:\n\nProposition 3.3. (cid:16)\n\nK(θ) − 1 |B|\n\n(cid:80)\n\nB∈B\n\nˆλ′ (cid:17)2\n\nSet\n\n ̃KB(θ)\n\n=\n\n≤\n\nC2k|B|G1 λ′\n\nk2 k−2 )m2 , (m−2 1\n(cid:88) k|B|\n\nB∈B\n\n(cid:88)\n\nτ ∈B\n\nand C2\n\n≡\n\n∥rθ(τ )∇wLτ (w∗\n\n(cid:19)\n\n(cid:20)(cid:18)m − 1 k − 1\n\n(cid:18)m − 2 k − 2 τ (w)|w=ws ∥2, and G2 ≡ 1\n\n1 2\n\n−\n\nk|B|\n\n+ C2k|B|λ′′G2, (cid:19)\n\n(cid:19)(cid:21)\n\n(cid:18)m − 1 /\nk − 1 (cid:80)\n\n(cid:80)\n\nB∈B\n\nwhere C1 (cid:18)m − 2 k − 2\n\n(cid:19)\n\n,G1\n\nm2\n\n≡\n\n≡\n\nτ ∈B[rθ(τ ) − 1]2.\n\nCorollary 3.3.1. When k ≪ m,\n\n(cid:16)\n\nK(θ) − 1 |B|\n\n(cid:80)\n\nB∈B\n\n(cid:17)2\n\n ̃KB(θ)\n\n≤ G1\n\nkλ′ + G2\n\nk λ′′.\n\n(cid:80)\n\n1 |B|\n\n ̃KB(θ) ∈ [−103, −1] during training, and (K(θ) − 1\n\nIn the experiments, m ≥ 106, k ≈ 102, and G1, G2 ≤ 104. When λ′ ≥ 102, λ′′ ≤ 10−2, we have ̃KB(θ))2 ≤ 0.2 is small. (cid:80) ̃KB(θ).\n\nThus, Proposition 3.3 shows that K(θ) can be decomposed into mini-batches as 1 This allows us to update θ by SGD over the task mini-batches as:\n\nB∈B\n\nB∈B\n\nB∈B\n\n(cid:80)\n\n|B|\n\n|B|\n\nθs+1 = θs + β′∇θ ̃KB(θs),\n\n(14)\n\nwhere β′ is the learning rate. Similarly, we approximate d∗ by its mini-batch approximation ̃d∗ = τ (w))|w=ws] and update w as ws+1 = ws + β ̃d∗. − 1\n\nτ ∈B ∇wLτ (w∗\n\nλ′|B| [(cid:80)\n\nThe whole procedure, which will be called Soft Improvement Multi-Objective Meta Learning (SIMOL), is shown in Algorithm 1. Step 4 trains the base learner. In the experiments, we use two popular meta-learning algorithms: MAML (Finn et al., 2017) and prototypical network (PN) (Snell et al., 2017). For MAML, the base learner is updated as\n\nw∗\n\nτ (w) = w − α∇wLτ (w).\n\n(15)\n\n(cid:80)\n\nτ (w) =\n\n1 |Qτ |NC\n\nexp(−∥fw(x)−ck∥2) For the PN, w∗ k′ exp(−∥fw(x)−ck′ ∥2) , where Qτ is the set of query examples for task τ , NC is the number of classes per epoch, fw is the model with parameter w, ck = fw(xi), and Sk is the set of examples belonging to class k. Pseudo-codes for\n\n1 |Sk| SIMOL-based MAML and PN are shown in Algorithms 2 and 3 of Appendix B, respectively.\n\n(xi,yi)∈Sk\n\nx∈Qτ\n\n(cid:80)\n\n(cid:80)\n\nAlgorithm 1: Soft Improvement Multi-Objective Meta Learning (SIMOL) Input: T , batch size k, learning rates β and β′ for w, ̃d∗ = 0 and θ, respectively.\n\n1 for s = 1, 2, . . . , S do Reset ̃d∗ = 0; for τ = 1, 2, . . . , k do\n\n3\n\n2\n\n4\n\n5\n\n6\n\n7\n\nobtain rθs(τ )∇ws Lτ (w∗ ̃d∗ = ̃d∗ − rθs(τ )∇ws Lτ (w∗ ̃d∗; 2ˆλ′ ∇θs τ ∈B rθs(τ ) − 1(cid:1)2\n\nws+1 = ws + β 1 k\nθs+1 = θs + β′ −1 (cid:0) 1 k\n\n(cid:13) (cid:13) 1\n\n∇θs\n\nˆλ′′ 2\n\n(cid:80)\n\n(cid:80)\n\nk\n\n;\n\nτ (ws)) for task τ ; τ (ws));\n\nτ ∈B rθs(τ )∇wLτ (w∗\n\nτ (w))|w=ws+1\n\n(cid:13) 2\n(cid:13)\n\n−\n\n4 CONVERGENCE ANALYSIS\n\nLemma 4.1. Define\n\nR(θ, w) ≡ Eτ ∼U [⊥ (rθ(τ ))[Lτ (w∗\n\nτ (w)]] + ∆(θ) −\n\nλ′′ 2\n\n(Eτ ∼U rθ(τ ) − 1)2,\n\n(16)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwhere ∆(θ) ≡ − 1 d∗) − Lτ (w∗ ∇θR(θ, ws) = ∇θK(θ).\n\nλ′ Eτ ∼U rθ(τ ) ⊥ [(Lτ (w∗\n\nτ (w) + τ (w))]], and ⊥ is the stop gradient operator.4 Then, ∇wR(θ, w)|w=ws = −d∗, and\n\nτ (w))]· ⊥ [Eτ ∼U rθ(τ )[Lτ (w∗\n\nτ (w) + d∗) − Lτ (w∗\n\nThis allows interpreting the updates in (14) and (8) as performing Gradient Descent Ascent (GDA) (Singh et al., 2000) on (16). Thus, we can leverage game theoretical tools (Lin et al., 2020) in the analysis.\n\n(cid:80)\n\n(cid:80)\n\n|B|\n\n|B|λ′\n\n(cid:104) 1\n\nτ (w))]· ⊥\n\n(cid:105) τ (w))]\n\nτ ∈B rθ(τ )[Lτ (w∗\n\nτ ∈B[rθ(τ )][Lτ (w∗\n\nτ ∈B rθ(τ )[⊥ [(Lτ (w∗\n\nLet ̃R(θ, w; B) ≡⊥ 1 |B| (cid:80)\n\nτ (w)] − 1 τ (w) + d∗) − Lτ (w∗\n\nτ (w) + d∗) − Lτ (w∗ τ ∈B rθ(τ ) − 1)2 be the mini-batch version of R, and U (B) be the uniform distribution over task mini-batches. The following Theorem shows that Algorithm 1 converges to an ε-Pareto stationary point of (5). Theorem 4.2. Assume that (i) Lτ (w∗ τ (·)) is L-smooth and rθ(·) is μ-strongly concave. (ii) The domain of θ is a convex and bounded set with diameter D > 0, (iii) EB∼U (B)[∇θ ̃R(θ, w; B)−∇θR(θ, w)] = 0, and EB∼U (B)∥∇θ ̃R(θ, w, B) − ∇θR(θ, w)∥2 ≤ σ2. Assume the first-order approximation in (10), and take β = Θ (cid:0)1/D2σ2(L2 + σ2)(cid:1), β′ = Θ(1/Lσ2). Algorithm 1 converges to an ε-Pareto τ (w)) is also μ′-convex w.r.t. w and ε = 0, the stationary point of (5) with a rate of O(1/ε8). If Lτ (w∗ 0-Pareto stationary point is also global Pareto optimal.\n\n( 1 |B|\n\n− λ 2\n\n(cid:80)\n\n′′\n\nAssumption (i) is commonly used in the literature (Collins et al., 2020; Zhou et al., 2021; Finn et al., 2019); while (ii) and (iii) are from (Lin et al., 2020). Theorem 4.2 shows that the proposed method can obtain an ε-Pareto stationary point (or global Pareto optimal point for convex objectives) regardless of m, the number of tasks/objectives. Corollary 4.2.1. Consider the MAML base learner update in (15). Assume that ∇wLτ (w) is HessianLipschitz continuous, bounded, Lipschitz-continuous, and w is bounded. Then, Algorithm 1 converges to an ε-Pareto stationary point of (5) with a rate of O(1/ε8).\n\nCorollary 4.2.1 is an application of Theorem 4.2 revealing that SIMOL with MAML can also converge to a Pareto point. Convergence of the outer loop is slower than the O(1/ε2) rate of standard MAML (Fallah et al., 2020). However, standard MAML only guarantees convergence to stationary points of w but not to Pareto-stationary points. Moreover, as will be seen in Section 5.1, empirically, the proposed method has comparable or even slightly faster convergence speed than MAML and other meta learning baselines. Besides, most the gradient-based MOO approaches (except CAGrad (Liu et al., 2021a)) do not provide convergence rate analysis; while CAGrad requires that all task gradients are available in each epoch, which is very expensive (as will be demonstrated in Section 5.2).\n\n5 EXPERIMENTS\n\nIn this section, we perform experiments on few-shot regression (Section 5.1), few-shot classification (Section 5.2), and reinforcement learning (Section 5.3). All experiments are run on a GeForce RTX 2080 Ti GPU and Intel(R) Xeon(R) CPU E5-2680. Our implementations are based on the popular open-source meta-learning library Learn2Learn (Arnold et al., 2020).\n\n5.1 FEW-SHOT REGRESSION Setup. We follow the setup in (Finn et al., 2017; Li et al., 2017). The target function for task τ is y = aτ sin(x + bτ ), where aτ and bτ are sampled uniformly from [0.1, 5.0] and [0, π], respectively. We generate 160, 000 meta-training tasks and 1, 000 meta-testing tasks. A multilayer perceptron with 2 fully-connected (FC) layers (each of size 32) and ReLU activation is used as meta-learner and re-weighting network. The re-weighting network uses all mini-batch instances as input. To ensure that the re-weighting network output is positive, we take the square of its last layer’s output as output.\n\nThe backbone meta-learning algorithm is MAML (Finn et al., 2017). We use Adam (Kingma & Ba, 2014), with an initial learning rate of 0.01, to update the base learners for 5 steps in the inner-loop. For the outer-loop, we compare (i) minimizing the (single objective) of overall task loss as in MAML; versus performing MOO with (ii) mini-batch MGDA (Désidéri, 2012; Ye et al., 2021), (iii) mini-batch CAGrad, using the same hyper-parameters as in (Liu et al., 2021a), (iv) mini-batch PCGrad, using the\n\n4The stop gradient operator satisfies ⊥ (h(x)) = h(x), ∇x ⊥ (h(x)) = 0, where h(·) is any differentiable\n\nfunction.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nsame hyper-parameters as in (Yu et al., 2020), (v) the proposed SIMOL, and (vi) updating of (6) with a mini-batch version of the improvement function in (6). Hyperparameters for MAML, mini-batch MGDA, and mini-batch CAGrad follow (Finn et al., 2017), while that for the proposed SIMOL are in Appendix D.\n\nThe mini-batch size is 16. We do not compare with the batch versions of MGDA/CAGrad, as computing all task gradients takes very large memory and time. The initial learning rate for the outer loop is 0.001. The experiment is repeated three times with different random seeds. For performance evaluation, we use the mean-squared-error (MSE) over all meta-testing tasks. We also report the worst-10% MSE, which is the average MSE for the 10% worst-performing meta-testing tasks.\n\nResults. Table 1 shows the MSE and its 95% confidence interval (computed as in (Finn et al., 2017; Li et al., 2017)). As can be seen, SIMOL consistently outperforms MAML, and the mini-batch versions of MGDA, CAGrad, PCGrad and improvement function in terms of both the overall and worst-10% MSEs. Indeed, the mini-batch versions of MGDA, CAGard, PCGrad and improvement function are even worse than the original MAML. Figure 2 shows the convergence of MSE with the number of training epochs. As can be seen, SIMOL converges slightly faster than the other baselines.\n\nTable 1: MSE (with 95% confidence interval) for few-shot regression. The best results are in bold. The ∗ denotes that the improvement over the second-best is statistically significant (at a significance level of 0.1 using the paired t-test).\n\noverall\n\nworst-10%\n\n5-shot\n\n2-shot\n\n5-shot\n\n2-shot\n\nmin average loss (MAML) mini-batch MGDA mini-batch CAGrad mini-batch PCGrad mini-batch improvement function SIMOL\n\n0.43 ± 0.11 0.60 ± 0.02 1.90 ± 0.24 1.89 ± 0.12 0.69 ± 0.03 0.34* ± 0.04\n\n1.70 ± 0.11 1.73 ± 0.10 1.82 ± 0.44 1.80 ± 0.11 1.79 ± 0.09 1.24* ± 0.08\n\n2.13 ± 0.21 2.62 ± 0.10 8.18 ± 0.63 6.64 ± 0.47 2.63 ± 0.21 1.69* ± 0.18\n\n7.75 ± 0.48 7.04 ± 0.51 8.23 ± 2.33 7.73 ± 0.54 8.23 ± 0.36 5.66* ± 0.33\n\n(a) 5-shot.\n\n(b) 2-shot.\n\nFigure 2: Convergence of MSE with the number of training epochs.\n\n5.2 FEW-SHOT IMAGE CLASSIFICATION Setup. In this section, we perform 5-way-1-shot and 5-way-5-shot classification on the miniImageNet (Ravi & Larochelle, 2016) and tieredImageNet data (Ren et al., 2018). Following (Finn et al., 2017), we split the miniImageNet dataset into a meta-training set with 64 classes, a meta-validation set with 16 classes, and a meta-testing set with 20 classes. The total number of meta-training tasks (cid:1) ≈ 7.6 × 106. Similarly, as in (Zhou et al., 2019), we split the tieredImageNet dataset into a is (cid:0)64 meta-training set with 351 classes, a meta-validation set with 97 classes, and a meta-test set with 160 classes. The total number of meta-training tasks is (cid:0)351 (cid:1) ≈ 4.3 × 1010. For both datasets, we randomly select 1, 000 meta-testing tasks for evaluation.\n\n5\n\n5\n\nWe use two backbone meta-learning algorithms, MAML and prototypical network (PN) (Snell et al., 2017), with hyper-parameters following the original papers. Following (Finn et al., 2017; Li et al., 2017; Zintgraf et al., 2019), we use the CNN45 (LeCun et al., 2015) as the meta-learner, and a CNN4\n\n5The CNN4 consists of four 3 × 3 convolution networks with batch normalization, 2 × 2 max-pooling and a\n\nReLU activation layer.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nwith a 3-layer FC as the reweighting network. The optimizer is Adam. The learning rates for the meta networks are 0.003 for MAML-based methods and 0.001 for PN-based methods, respectively. The learning rate of the reweighting network is 0.08 for SIMOL and 0.0008 for SIMOL+PN. The mini-batch size is 32. More details on the hyperparameters are in Appendix D.\n\nThe proposed SIMOL is compared with (i) minimizing the overall task loss as in standard MAML, (ii) mini-batch MGDA, and (iii) mini-batch CAGrad. We also compare with MAML variants including (iv) Reptile (Nichol & Schulman, 2018), (v) FOMAML (Finn et al., 2017), (vi) MetaMinibatchProx (Zhou et al., 2019), (vii) TSA-MAML (Zhou et al., 2021), (viii) IMAML (Rajeswaran et al., 2019)) (ix) MTL (Wang et al., 2021), a multi-task learning based maml approach, and the standard prototypical network (Snell et al., 2017). The evaluation metrics are similar to those in Section 5.1, but with accuracy instead of MSE. Experiments are repeated three times with different random seeds.\n\nResults. Tables 2 and 3 show the meta-testing accuraccies and 95% confidence intervals on miniImageNet and tieredImageNet, respectively. For MAML and its variants, SIMOL consistently outperforms all the other baselines in terms of both the overall and worst-10% accuracies. The same is also observed on the meta-learning algorithm prototypical network. This demonstrates that SIMOL is useful for both gradient-based and metric-based meta-learning approaches.\n\nNote that (mini-batch) MGDA and CAGrad do not perform good in terms of both overall and worst10% accuracies, showing that they cannot be straightforwardly extended to the use of mini-batch. On the other hand, the batch versions of MGDA and CAGrad are computationally impractical. Table 4 compares the per-epoch running time in training stage of standard MAML, SIMOL, and batch MGDA and CAgrad. Experiment is performed on 5-way-5-shot classification with the MAML algorithm on miniImageNet. As can be seen, while SIMOL has comparable per-epoch running time as MAML, batch MGDA and CAgrad are much more computationally expensive (around 432, 000 times slower).\n\nTable 2: 5-way classification accuracies on miniImageNet (with 95% confidence interval). Results of Reptile, FOMAML, and Meta-MinibatchProx are from (Zhou et al., 2019), IMAML from (Deleu et al., 2022), and TSA-MAML from (Zhou et al., 2021). Results not reported in the original papers are denoted “-”. The best results are in bold.\n\noverall\n\nworst-10%\n\n1-shot\n\n5-shot\n\n1-shot\n\n5-shot\n\n(MAML)\n\n(MAML variants)\n\nmin average loss mini-batch MGDA mini-batch CAGrad SIMOL Reptile FOMAML IMAML Meta-MinibatchProx TS-MAML MTL\n\n49.24 ± 0.78 46.08 ± 0.78 44.67 ± 0.75 50.62* ± 1.39 47.07 ± 0.26 45.53 ± 1.58 49.30 ± 1.88 48.51 ± 0.92 48.44 ± 0.91 49.87 ± 0.41\n\n62.13 ± 0.72 60.15 ± 0.41 60.05 ± 0.67 65.83* ± 0.86 62.74 ± 0.37 61.02 ± 1.12 59.77 ± 0.41 64.15 ± 0.92 65.52 ± 0.68 65.81 ± 0.33\n\n13.33 ± 1.07 10.60 ± 1.33 11.33 ± 1.12 14.99* ± 1.72 -\n- -\n- -\n13.64 ± 1.45\n\n41.71 ± 1.02 39.67 ± 0.55 40.01 ± 0.88 44.81*± 0.58 -\n- -\n- -\n43.42 ± 0.47\n\n(PN)\n\nStandard SIMOL\n\n48.25 ± 0.95 50.45*± 0.93\n\n65.29 ± 0.48 66.67* ± 0.47\n\n13.10 ± 1.32 15.00* ± 1.21\n\n45.03 ± 0.69 46.27* ± 0.70\n\n5.3 REINFORCEMENT LEARNING In this experiment, experiments are performed on the HalfCheetach-Dir and Walker-Dir environments in Mujoco (Todorov et al., 2012). In both environments, each task corresponds to a random direction in the XY-plane, and the agent (Walker/ HalfCheetach) learns to run in that direction as far as possible. The reward is the average velocity minus control costs. We again use MAML as the meta-learning algorithm, and the base reinforcement learning algorithm is vanilla policy gradient (VPG) (Sutton et al., 1999). Following (Rothfuss et al., 2018), the policy network has two 64 × 64 FC layers with tanh activation, while the critic is a linear state-value function whose parameters are obtained by minimizing least-square. The re-weighting network has two 64 × 64 FC layers with ReLU activation. Following (Zintgraf et al., 2019), we use MAML as the baseline.\n\nFigure 3 shows the convergence of the accumulated reward with the number of training iterations. As can be seen, SIMOL consistently outperforms MAML. Moreover, the convergence of MAML is less\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: 5-way classification accuracies on tieredImageNet (with 95% confidence interval).\n\noverall\n\nworst-10%\n\n1-shot\n\n5-shot\n\n1-shot\n\n5-shot\n\n(MAML)\n\n(MAML variants)\n\nmin average loss mini-batch MGDA mini-batch CAGrad SIMOL\n\n50.58 ± 1.44 22.92 ± 1.04 49.04 ± 0.93 51.42* ± 1.50\n\n69.33 ± 0.74 53.41 ± 0.74 65.43 ± 0.73 70.13* ± 0.74\n\n11.60 ± 1.96 7.12 ± 2.11 11.40 ± 1.97 12.00* ± 1.95\n\n46.95 ± 1.14 32.79 ± 0.88 42.63 ± 0.95 47.51* ± 1.46\n\nReptile FOMAML IMAML Meta-MinibatchProx TS-MAML MTL\n\n49.12 ± 0.43 45.53 ± 1.58 38.54 ± 1.37 50.14 ± 1.37 48.82 ± 0.88 51.02 ± 0.46\n\n65.99 ± 0.42 61.02 ± 1.12 60.24 ± 0.76 68.30 ± 0.91 67.82 ± 0.72 66.47 ± 0.39\n\n- -\n- -\n- 13.60 ± 1.59\n\n- -\n- -\n- 49.45 ± 0.58\n\n(PN)\n\nStandard SIMOL\n\n47.63 ± 0.93 50.09* ± 0.96\n\n68.92 ± 0.52 71.51* ± 0.79\n\n14.40 ± 1.25 15.10* ± 1.20\n\n46.67 ± 0.78 49.85* ± 0.79\n\nstable, as also reported by (Rothfuss et al., 2018). On the other hand, the convergence of SIMOL is smoother and more stable.\n\n(a) HalfCheetah-Dir\n\n(b) Walker-Dir\n\nFigure 3: Returns for SIMOL-VPG and MAML-VPG. Results are averaged over 3 trials.\n\n5.4 ABLATION STUDY\n\nIn this experiment, we use the setup in Section 5.1, and vary the number of FC layers in SIMOL’s meta-learner. The number of training epochs is always fixed to 10, 000. Table 5 shows the MSE’s on 2-shot regression. as can be seen, the use of 2 FC layers has the best overall MSE and worst-10% MSE. the deeper networks may not be sufficiently trained with the fixed number of training epochs, leading to worse performance.\n\nTable 4: Per-epoch running time for 5-way 5shot classification on miniImageNet.\n\nTable 5: Performance for SIMOL with different numbers of layers in 2-shot regression.\n\nMAML\n\nSIMOL\n\nbatch MGDA batch CAGrad\n\n2.0 sec\n\n2.3 sec\n\n6.9 days\n\n5.6 days\n\n#layers\n\noverall MSE worst-10 % MSE\n\n2 3\n4\n\n1.24 ± 0.08 1.36 ± 0.09 1.42 ± 0.08\n\n5.66 ± 0.33 6.01 ± 0.38 6.22 ± 0.34\n\n6 CONCLUSION\n\nIn this paper, we propose to avoid the compromising phenomenon in meta-learning by reformulating it as a multi-objective optimization (MOO) problem, in which each task is an objective. However, current gradient-based MOO solvers cannot scale to a large number of objectives. With the use of improvement function and mini-batch, we propose a scalable gradient-based solver with theoretical guarantees to Pareto-optimality. Empirical studies on few-shot regression, few-shot classification, and reinforcement learning demonstrate that the proposed method is efficient, and has good generalization in terms of both overall performance and performance on the poorly-performing tasks.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nThis research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 16200021).\n\nREFERENCES\n\nSébastien MR Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner, and Konstantinos Saitas Zarkias. Learn2learn: A library for meta-learning research. Preprint arXiv:2008.12284, 2020.\n\nDW Bolton. The multinomial theorem. The Mathematical Gazette, 52(382):336–342, 1968.\n\nLiam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Task-robust model-agnostic meta-learning.\n\nNeurIPS, 33:18860–18871, 2020.\n\nTristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua Bengio, Guillaume Lajoie, and Pierre-Luc Bacon. Continuous-time meta-learning with forward mode differentiation. In ICLR, 2022.\n\nJean-Antoine Désidéri. Multiple-gradient descent algorithm (MGDA) for multiobjective optimization.\n\nCRM, 350(5-6):313–318, 2012.\n\nMichael Emmerich and André H Deutz. A tutorial on multiobjective optimization: fundamentals and\n\nevolutionary methods. NC, 17(3):585–609, 2018.\n\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-based\n\nmodel-agnostic meta-learning algorithms. In AISTATS, pp. 1082–1092, 2020.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\n\ndeep networks. In ICML, pp. 1126–1135, 2017.\n\nChelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In\n\nICML, pp. 1920–1930, 2019.\n\nLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In ICML, pp. 1568–1577, 2018.\n\nMingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. Preprint arXiv:2007.05170, 2020.\n\nTimothy M Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J Storkey. Meta-learning in\n\nneural networks: A survey. PAMI, 2020.\n\nKaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced\n\ndesign. In ICML, pp. 4882–4892, 2021.\n\nChia Hsiang Kao, Wei-Chen Chiu, and Pin-Yu Chen. MAML is a noisy contrastive learner in\n\nclassification. In ICLR, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Preprint\n\narXiv:1412.6980, 2014.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,\n\n2015.\n\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with\n\ndifferentiable convex optimization. In CVPR, pp. 10657–10665, 2019.\n\nZhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot\n\nlearning. Preprint arXiv:2008.12284, 2017.\n\nTianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax\n\nproblems. In ICML, pp. 6083–6093, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for\n\nmulti-task learning. NeurIPS, 2021a.\n\nLiyang Liu, Yi Li, Zhanghui Kuang, J Xue, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne\n\nZhang. Towards impartial multi-task learning. In ICLR, 2021b.\n\nMarko M Mäkelä, Napsu Karmitsa, and Outi Wilppu. Proximal bundle method for nonsmooth and\n\nnonconvex multiobjective optimization. In MMOCS, pp. 191–204. 2016.\n\nYuren Mao, Shuang Yun, Weiwei Liu, and Bo Du. Tchebycheff procedure for multi-task text\n\nclassification. In ACL, pp. 4217–4226, 2020.\n\nR Timothy Marler and Jasbir S Arora. Survey of multi-objective optimization methods for engineering.\n\nStructural and Multidisciplinary Optimization, 26(6):369–395, 2004.\n\nK Miettinen and MM Mäkelä. Interactive bundle-based method for nondifferentiable multiobjeective\n\noptimization: NIMBUS. Optimization, 34(3):231–246, 1995.\n\nKaisa Miettinen. Nonlinear Multiobjective Optimization, volume 12. Springer Science & Business\n\nMedia, 2012.\n\nMichinari Momma, Chaosheng Dong, and Jia Liu. A multi-objective/multi-task learning framework induced by pareto stationarity. In International Conference on Machine Learning, pp. 15895–15907. PMLR, 2022.\n\nO Montonen, N Karmitsa, and MM Mäkelä. Multiple subgradient descent bundle method for convex\n\nnonsmooth multiobjective optimization. Optimization, 67(1):139–158, 2018.\n\nAviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik, and\n\nEthan Fetaya. Multi-task learning as a bargaining game. In ICML, 2022.\n\nAlex Nichol and John Schulman. Reptile: A scalable metalearning algorithm.\n\nPreprint\n\narXiv:1803.02999, 2018.\n\nAlex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. Preprint\n\narXiv:1803.02999, 2018.\n\nAravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit\n\ngradients. NeurIPS, 2019.\n\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR, 2016.\n\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. In ICLR, 2018.\n\nJonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal\n\nmeta-policy search. In ICLR, 2018.\n\nOzan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. NeurIPS, 2018.\n\nJun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-\n\nnet: Learning an explicit mapping for sample weighting. NeurIPS, 2019.\n\nStephen Simons. Minimax theorems and their proofs. In Minimax and Applications, pp. 1–23.\n\nSpringer, 1995.\n\nSatinder Singh, Michael J Kearns, and Yishay Mansour. Nash convergence of gradient dynamics in\n\ngeneral-sum games. In UAI, pp. 541–548, 2000.\n\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. NeurIPS,\n\n2017.\n\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods\n\nfor reinforcement learning with function approximation. NeurIPS, 1999.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\n\nIn ICIRS, pp. 5026–5033, 2012.\n\nRicardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. AIJ, 18(2):\n\n77–95, 2002.\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-\n\ning networks for one shot learning. NeurIPS, 2016.\n\nHaoxiang Wang, Han Zhao, and Bo Li. Bridging multi-task learning and meta-learning: Towards\n\nefficient training and effective adaptation. In ICML, pp. 10991–11002, 2021.\n\nFeiyang Ye, Baijiong Lin, Zhixiong Yue, Pengxin Guo, Qiao Xiao, and Yu Zhang. Multi-objective\n\nmeta learning. NeurIPS, 2021.\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\n\nGradient surgery for multi-task learning. NeurIPS, 2020.\n\nPan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via\n\nminibatch proximal update. NeurIPS, 2019.\n\nPan Zhou, Yingtian Zou, Xiao-Tong Yuan, Jiashi Feng, Caiming Xiong, and Steven Hoi. Task similarity aware meta learning: Theory-inspired improvement on MAML. In UAI, pp. 23–33, 2021.\n\nLuisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context\n\nadaptation via meta-learning. In ICML, pp. 7693–7702, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA TOY EXAMPLE\n\nThe definitions of f1 and f2 are:\n\nf1(x) = c1(x)l1(x) + c2(x)g1(x), f2(x) = c1(x)l2(x) + c2(x)g2(x),\n\nwhere\n\nl1(x) = log (max (|0.5 (−x1 − 7) − tanh (−x2)| , 0.000005)) + 6, l2(x) = log (max (|0.5 (−x1 + 3) − tanh (−x2) + 2| , 0.000005)) + 6,\n\ng1(x) =\n\ng2(x) =\n\n(cid:16)\n\n(cid:16)\n\n(−x1 + 7)2 + 0.1 ∗ (−x2 − 8)2(cid:17) (−x1 − 7)2 + 0.1 ∗ (−x2 − 8)2(cid:17)\n\n/10 − 20,\n\n/10 − 20,\n\nc1(x) = max(tanh(0.5x2), 0), c2(x) = max(tanh(−0.5x2), 0).\n\nFor mini-batch MGDA, and SIMOL the probability to sample task 1 is 2/3 and 1/3 for task 2. The learning rates for MGDA and mini-batch MGDA are both 0.01. The learning rate for SIMOL’s meta-learner is 0.01, while that for its re-weighting network is 0.1. Note that the meta-learner and the re-weighting network are represented by learnable vectors.\n\nB PSEUDO-CODES\n\nAlgorithms 2 and 3 show the pseudo-codes for SIMOL-based MAML and PN, respectively. The key differences between SIMOL and MAML/PN are highlighted in blue.\n\nAlgorithm 2: SIMOL for MAML. Input: T , and total epoch S. B is the batch size, β and β′ are learning rates for w and θ. α is the\n\nlearning rates for inner loop.\n\nτ (w) = ws − α∇wLτ (ws);\n\n3\n\n4\n\n5\n\n6\n\n7\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n1 for epoch s = 1, 2, 3, . . . , S do 2\n\nSample tasks 1, 2, . . . B; for Every task τ do Receive rθs (τ ); Compute adapted parameters with gradient descent w∗\n\nws+1 = ws − βs 1 B\nθs+1 = θs + β\n\n(cid:80)B ′s ̃K(θ, B);\n\nτ =1 rθs(τ )∇wLτ (w∗\n\nτ (w));\n\nAlgorithm 3: SIMOL for PN.\n\n1 for epoch s = 1, 2, 3, . . . , S do 2\n\nSample tasks 1, 2, . . . B; for Every task τ do\n\nfor Every class c do\n\nSelect the support Sc Compute prototype cτ = 1 NC\n\nτ and query set Qτ ; (cid:80)\n\n(xi,yi)∈Sc τ\n\nfθ (xi);\n\nCalculate w∗ Receive rθs (τ );\n\nτ (w) =\n\n1 |Qτ |NC\n\n(cid:80)\n\nx∈Qτ\n\nexp(−∥fw(x)−ck∥2) k′ exp(−∥fw(x)−ck′ ∥2) ;\n\n(cid:80)\n\nws+1 = ws − βs 1 B\nθs+1 = θs + β\n\n(cid:80)B ′s ̃K(θ, B);\n\nτ =1 rθs(τ )∇wLτ (w∗\n\nτ (w));\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nC PROOFS\n\nC.1 PROOF OF LEMMA 3.1\n\nProof. Note that\n\nmax τ =1...,m\n\nwhere\n\nFirst, we have\n\n{Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))} =\n\n(cid:88)\n\nτ\n\np(τ )Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′)),\n\np(τ ) =\n\n(cid:26)1 0\n\nτ = arg maxτ Lτ (w∗ otherwise\n\nτ (w)) − Lτ (w∗\n\nτ (w′))\n\n.\n\nmax π(τ )\n\nEπ(τ ) [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))] −\n\n(cid:88)\n\nτ\n\np(τ )Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′)) ≥ 0.\n\nThis can be done by setting π(τ ) = p(τ ). Next, we show that\n\nmax π(τ )\n\nEπ(τ ) [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))] −\n\n(cid:88)\n\nτ\n\nThis is established since\n\np(τ )Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′)) ≤ 0.\n\nmax π(τ )\n\nEτ ∼π(τ ) [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))] −\n\n(cid:88)\n\nτ\n\np(τ )Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))\n\n(cid:88)\n\n≤\n\nτ\n\n[π(τ ) − p(τ )] [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))]\n\n= [π(τ ′) − 1] [Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))] +\n\n(cid:88)\n\nτ ∈{1,...,m}\\τ ′\n\n[π(τ )] [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))]\n\n(a)\n\n≤ [π(τ ′) − 1] [Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))] +\n\n(cid:88)\n\nτ ∈{1,...,m}\\τ ′\n\n[π(τ )] [Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))]\n\n≤ π(τ ′) [Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))] +\n\n(cid:88)\n\nτ ∈{1,...,m}\\τ ′\n\n[π(τ )] [Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))]\n\n− [Lτ ′(w∗ = [Lτ ′(w∗ = 0,\n\nτ ′(w)) − Lτ ′(w∗ τ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))] τ ′(w′))] − [Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))]\n\nwhere τ ′\n\n=\n\narg max\n\nτ =1...,m\n\n{Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))}.\n\n(a)\n\nis due to the fact\n\nthat\n\n[Lτ ′(w∗ Therefore, we have\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))] ≥ [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))] , ∀τ based on the property of τ ′.\n\nand\n\nThus,\n\nmax τ =1...,m\n\n[Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))] ≥ [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))]\n\n[Lτ ′(w∗\n\nτ ′(w)) − Lτ ′(w∗\n\nτ ′(w′))] ≤ [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))] .\n\n{Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))} = max\n\nπ(τ )\n\nEτ ∼π(τ ) [Lτ (w∗\n\nτ (w)) − Lτ (w∗\n\nτ (w′))] .\n\nC.2 PROOF OF PROPOSITION 3.2\n\nProof. Using the first-order Taylor expansion,\n\narg min\n\nd\n\nEU (τ )[rθ(τ )[Lτ (w∗\n\nτ (ws + d)) − (Lτ (w∗\n\nτ (ws))] +\n\nλ′ 2\n\n∥d∥2\n\n= arg min\n\nd\n\nEU (τ )\n\n(cid:2)rθ(τ ) (cid:2)∇wLτ (w∗\n\nτ (ws))⊤d(cid:3)(cid:3) +\n\nλ′ 2\n\n∥d∥2.\n\n(17)\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTaking the derivatives w.r.t. d,\n\n∇dEU (τ )\n\n(cid:2)rθ(τ ) (cid:2)∇wLτ (w∗\n\nτ (ws))⊤d (cid:3)(cid:3) +\n\nλ′ 2\n\n∥d∥2 = EU (τ )[rθ(τ )[∇wLτ (w∗\n\nτ (ws))]] + λ′d.\n\nSetting the above to zero, we have EU (τ )[rθ(τ )∇wLDτ (w∗\n\nτ (ws))] + d = 0, and\n\nd∗ =\n\n−1 λ′\n\nEU (τ )[rθ(τ )[∇wLτ (w∗\n\nτ (ws))]].\n\nPutting d∗ back to the objective in (17), we have:\n\n∥d∥2\n\nλ′ 2\n(cid:13) (cid:13) (cid:13) (cid:13) 1\n\n−\n\n1 λ′\n\nEτ ∼π[∇wLτ (w∗\n\nτ (w)|w=ws ]\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n2λ′ ∥Eτ ∼π[∇wLτ (w∗ w=ws, Eτ ∼U rθ(τ )∇wLτ (w∗\n\nτ (w))|⊤\n\nτ (w)|w=ws]∥2\n\nτ (w))|⊤\n\nw=ws ⟩\n\nEτ ∼U rθ(τ )[Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))] +\n\n= (cid:2)Eτ ∼U rθ(τ )∇wLτ (w∗\n\nτ (w))|⊤\n\nw=ws d(cid:3) +\n\nλ′ 2\n\n= ⟨Eτ ∼U rθ(τ )∇wLτ (w∗\n\nτ (w))|⊤\n\nw=ws , d⟩ +\n\n1\n\n= −\n\n+\n\n1\n\nλ′ ⟨Eτ ∼U rθ(τ )∇wLτ (w∗ 2λ′ ∥Eτ ∼π[∇wLτ (w∗ λ′ ∥Eτ ∼π[∇wLτ (w∗\n\n−1\n\nτ (w)|w=ws]∥2\n\nτ (w)|w=ws]||2.\n\n=\n\nNext, we have the following two Lemmas. Lemma C.1. When Lτ (w∗ d and concave. w.r.t. rθ(τ ).\n\nτ (ws + d)) ≈ Lτ (w∗\n\nτ (ws) + ∇wLτ (w∗\n\nτ (ws))⊤d, Eq. (9) is convex w.r.t.\n\nProof. Putting Lτ (w∗\n\nτ (ws + d)) ≈ Lτ (w∗\n\nτ (ws) + ∇wLτ (w∗\n\nτ (ws))⊤d into Eq. (9), we have:\n\nmin d\n\nmax r(τ )\n\nEU (τ )[r(τ )[Lτ (w∗\n\nτ (ws + d)) − (Lτ (w∗\n\nτ (ws))] +\n\nλ′ 2\n\n∥d∥2 −\n\nλ′′ 2\n\n(EU (τ )[r(τ )] − 1)2]\n\n= min\n\nd λ′ 2\n\n+\n\nmax r(τ )\n\nEU (τ )[r(τ )[Lτ (w∗\n\nτ (ws)) + ∇wLτ (w∗\n\nτ (ws))⊤d − (Lτ (w∗\n\nτ (ws))]\n\n∥d∥2 −\n\nλ′′ 2\n\n(EU (τ )[r(τ )] − 1)2].\n\n2 ||d||2 is convex w.r.t. d. Then, ∇wLτ (w∗\n\nτ (ws))⊤d + λ′\n\nFor ∇wLτ (w∗ and λ′ of two convex functions ∇wLτ (w∗ w.r.t. d. Similarly, since − λ′′ concave w.r.t. rθ(τ ).\n\n2 ||d||2, since ∇wLτ (w∗\n\nτ (ws))⊤d is both convex and concave w.r.t. d, 2 ∥d∥2 is convex w.r.t d. Thus, the sum 2 ∥d∥2 is convex w.r.t. d. Thus, Eq. (9) is convex 2 (EU (τ )[rθ(τ )] − 1)2 is concave w.r.t. rθ(τ ). Therefore, Eq. (9) is also\n\nτ (ws))⊤d + λ′\n\nτ (ws))⊤d + λ′\n\nLemma C.2. When Lτ (w∗\n\nτ (ws + d)) ≈ Lτ (w∗\n\nτ (ws) + ∇wLτ (w∗\n\nτ (ws))⊤d,\n\nmin d\n\nmax rθ(τ )\n\nEU (τ )[rθ(τ )[Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))] +\n\nλ′ 2\n\n∥d∥2 −\n\nλ′′ 2\n\n(Ep[rθ(τ )] − 1)2]\n\n= max rθ(τ )\n\nmin d\n\nEU (τ )[rθ(τ )[Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))] +\n\nλ′ 2\n\n∥d∥2 −\n\nλ′′ 2\n\n(Ep[rθ(τ )] − 1)2].\n\nProof. The above equation is established by the minimax theorem (Simons, 1995) when Eq. (9) is convex w.r.t. d and concave w.r.t. rθ(τ ). This holds by using Lemma C.1.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nLemma C.3. If every B ∈ B is a set consists of a unique selection of k (m > k > 1) tasks out of m (the total number of tasks) tasks without replacement. For any continuous function f (·), we have:\n\n(cid:34)\n\n1 m\n\nm (cid:88)\n\nτ =1\n\n(cid:35)2\n\nf (τ )\n\n= C1\n\n(cid:34)\n\n(cid:88)\n\n(cid:88)\n\n(cid:35)2\n\nf (τ )\n\n− C2\n\n(cid:88)\n\n(cid:88)\n\n[f (τ )]2,\n\nB∈B\n\nτ ∈B\n\nB∈B\n\nτ ∈B\n\n(cid:19)(cid:21) (cid:88)\n\nf (τ )2\n\nτ ∈T\n\n(cid:88)\n\nτ ∈B\n\n\n\n\n\nf (τ )2\n\n. (b) is due to the\n\n(cid:104)(cid:0)m−1\n\nk−1\n\n(cid:1) − 1\n\n2\n\n(cid:1)(cid:105)\n\n(cid:0)m−2 k−2\n\n/(cid:0)m−1\n\nk−1\n\n(cid:1)m2(cid:0)m−2\n\nk−2\n\nwhere C1 =\n\nk2 k−2 )m2 , C2 = (m−2\n\nProof. Note that\n\n(cid:34)\n\n1 m\n\nm (cid:88)\n\nτ =1\n\n(cid:35)2\n\nf (τ )\n\n(cid:88)\n\n(cid:88)\n\n1(τ, τ ′)f (τ )f (τ ′)\n\n(a) =\n\n(b) =\n\n=\n\n=\n\nτ ′\n\nτ\n\n1 (cid:0) n−2 B−2\n\n(cid:1)\n\n1(τ, τ ′)f (τ )f (τ ′) −\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nτ ′∈B\n\nτ ∈B\n\nB∈B \n\n1(τ, τ ′)f (τ )f (τ ′) −\n\n1 (cid:0) n−2 B−2\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\n\n\n(cid:1)\n\nτ ′∈B\n\nτ ∈B\n\nB∈B \n\n1 (cid:1)m2 (cid:0) n−2 B−2\n\n(cid:88)\n\n(cid:88)\n\n[\n\n\n\nB∈B\n\nτ ∈B\n\nf (τ )]2 −\n\n(cid:19)\n\n(cid:20)(cid:18) n − 1 B − 1 (cid:104)(cid:0) n−1\n\nB−1\n\n(cid:18) n − 2 B − 2 (cid:1)(cid:105)\n\n(cid:0) n−2 B−2\n\n(cid:1) − 1 2\n(cid:0) n−1 B−1\n\n(cid:1)\n\n(cid:88)\n\nB∈B \n\n(cid:104)(cid:0) n−1\n\nB−1\n\n(cid:1) − 1\n\n2\n\n(cid:1)(cid:105)\n\n(cid:0) n−2 B−2\n\n(cid:0) n−1 B−1\n\n(cid:1)m2\n\n(cid:88)\n\n(cid:88)\n\nB∈B\n\nτ ∈B\n\nf (τ )2\n\n\n\n(cid:1).\n\n−\n\n1 2\n\n(cid:34)\n\n(cid:88)\n\n(cid:88)\n\n(cid:35)2\n\nf (τ )\n\n− C2\n\n= C1[\n\n(cid:88)\n\n(cid:88)\n\n[f (τ )]2.\n\nB∈B\n\nτ ∈B\n\nB∈B\n\nτ ∈B\n\n(a) is due to the multinomial theorem (Bolton, 1968) and 1(τ, τ ′) =\n\n(cid:26)2 1\n\nτ ̸= τ ′ otherwise\n\nfact that every task τ has occurred exactly (cid:0) n−1 (cid:0) n−2 B−2\n\n(cid:1) times.\n\nB−1\n\n(cid:1) times, and every tuple (τ, τ ′) has occurred exactly\n\nIf every B ∈ B is a set consists of a unique selection of k (m > k > 1) tasks out of m Lemma C.4. (the total number of tasks) tasks without replacement. Let g(τ ) = [g1(τ ), g2(τ ), . . . , gm(τ )], where gi(·)’s are continuous functions. We have\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 m\n\nm (cid:88)\n\nτ =1\n\ng(τ )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n= C1\n\n(cid:88)\n\nB∈B\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nτ ∈B\n\ng(τ )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n− C2\n\n(cid:88)\n\n(cid:88)\n\n∥g(τ )∥2,\n\nB∈B\n\nτ ∈B\n\nwhere C1 =\n\nk2 k−2 )m2 , and C2 = (m−2\n\n(cid:104)(cid:0)m−1\n\nk−1\n\n(cid:1) − 1\n\n2\n\n(cid:1)(cid:105)\n\n(cid:0)m−2 k−2\n\n/(cid:0)m−1\n\nk−1\n\n(cid:1)m2(cid:0)m−2\n\n(cid:1).\n\nk−2\n\nProof. Observe that ∥g(τ )∥2 = (cid:80)\n\ni gi(τ )2. The remaining follows from Lemma C.3.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nProof. (of Proposition 3.3) Using Lemmas C.3 and C.4, we have:\n\n ̃KT (θ) −\n\n1 |B|\n\n(cid:88)\n\nB\n\n ̃KB(θ)\n\n(cid:20)\n\n−\n\n(cid:88)\n\nB∈B\n\n(cid:21)\n\n3C1 2λ′ +\n\n3 2λ′|B|\n\n|| ̃d∗||2 −\n\n(cid:20)\n\nλ′′C1 −\n\nλ′′ |B|k2\n\n(cid:88)\n\nB∈B\n\n(cid:21) (cid:32)\n\n(cid:88)\n\nτ ∈B\n\n(cid:33)2\n\n(rθ(τ ) − 1)\n\n||rθ(τ )∇wLτ (w∗\n\nτ (w))|w=ws||2 − ˆλ′′[rθ(τ ) − 1]2\n\n(cid:19)(cid:35)\n\n(cid:18)\n\n−\n\n1 ˆλ′\n\n1\n\n−C2\n\nλ′ ∥rθ(τ )∇wLτ (w∗\n\nτ (w))|w=ws∥2 − C2λ′′[rθ(τ ) − 1]2\n\n(cid:19)(cid:35)\n\n,\n\n=\n\n=\n\n(cid:34)\n\n(cid:88)\n\n(cid:88)\n\n−C2\n\nB∈B\n\nτ ∈B (cid:18)\n\n(cid:34)\n\n(cid:88)\n\n(cid:88)\n\nτ ∈B\n\nB∈B (cid:80)\n\nB∈B\n\nwhere ̃d∗ := 1\n\n|B|\n\nrθ(τ )∇wLτ (w∗\n\nτ (w))|w=ws . Recall ˆλ′ and ˆλ′′ are defined in Sec. 3. Then,\n\n( ̃KT (θ) −\n\n1 |B)\n\n(cid:88)\n\nB\n\n ̃KB(θ))2\n\n(cid:32)(cid:34)(cid:32)\n\n≤\n\n−\n\nC2k|B| λ′\n\n1 k|B|\n\n(cid:88)\n\n(cid:88)\n\nB∈B\n\nτ ∈B\n\n||rθ(τ )∇wLτ (w∗\n\nτ (w))|w=ws ||2\n\n+C2k|B|λ′′ 1\n\nk|B|\n\n(cid:88)\n\n(cid:88)\n\nB∈B\n\nτ ∈B\n\n[rθ(τ ) − 1]2\n\n(cid:33)(cid:35)(cid:33)2\n\n≤\n\nC2k|B|G1 λ′\n\n+ C2k|B|λ′′G2.\n\nBy noting that ̃KT (θ) is exactly K(θ), we obtain the first claim.\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\nRegarding the second claim, note that when m is large, (cid:1) − 1\n\n(cid:104)(cid:0)m−1\n\nk\n\nC2k|B| =\n\n≈\n\n=\n\n=\n\n(cid:0)m−2 k−2\n\n(cid:1)(cid:105) (cid:0)m\n\nk\n\n(cid:1)\n\n2\n\nk−1 (cid:0)m−1 k−1\n\nk(cid:0)m−1\n\nk−1\n\n(cid:1) (cid:1)m2(cid:0)m−2 (cid:1)(cid:0)m k\n\nk−2\n\n(cid:1)\n\n(cid:1) =\n\n(cid:1)m2(cid:0)m−2 (cid:1) m k\n\nk−2\n\n(cid:1) =\n\n(cid:0)m−1 k−1\n\nk−1\n\nk(cid:0)m−1 m2(cid:0)m−2 k−2 m − 1 (k − 1)m\n\n≈\n\nk\n\nk(cid:0)m (cid:1) m2(cid:0)m−2 (cid:1) (cid:1) m k\n\nk−2\n\nk−2\n\nk(cid:0)m−2\n\nm−1 k−1 (cid:1) m2(cid:0)m−2 1\nk − 1\n\nk−2\n\n.\n\nThe first approximation is due to the fact that (cid:0)m−1 due to the property of combinatorics ((cid:0)m the results.\n\n(cid:1) = m\n\nk−1 (cid:0)m−1 k−1\n\nk\n\nk\n\n(cid:1) ≫ 1\n\n(cid:0)m−2 k−2 (cid:1)). Putting 1\n\n(cid:1). The third and forth equations are k−1 back into Eq. (18), we obtain\n\n2\n\nC.3 PROOF OF LEMMA 4.1\n\nProof. We have:\n\n∇θR(θ, w) = ∇θ∆(θ) − ∇θ\n\nλ′′ 2\n\n(Eτ ∼U rθ(τ ) − 1)2\n\n1 λ′\n\n= −\n\nEτ ∼U ∇θrθ(τ ) ⊥ [(Lτ (w∗\n\nτ (w) + d∗) − Lτ (w∗\n\nτ (w))]\n\n⊥ [Eτ ∼U rθ(τ )[Lτ (w∗\n\nτ (w) + d∗) − Lτ (w∗\n\nτ (w))]]\n\n= ∇θ∥Eτ ∼U ∇θrθ(τ )∇wLτ (w∗\n\nτ (w)|w=ws||2 − ∇θ\n\nλ′′ 2\n\n(Eτ ∼U rθ(τ ) − 1)2\n\n= ∇θK(θ).\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nAlso notice that:\n\n∇wR(θ, w)\n\n= Eτ ∼U [⊥ (rθ(τ )) [∇wLτ (w∗\n\nτ (w)]] + ∇w∆(θ) − ∇w\n\n= Eτ ∼U rθ(τ )[∇wLτ (w∗ Therefore, ∇wR(θ, w)|w=ws = −d∗.\n\nτ (w))].\n\nC.4 PROOF OF THEOREM 4.2\n\nλ′′ 2\n\n(Eτ ∼U rθ(τ ) − 1)2]\n\nDefinition C.5. (Weak Global Pareto Optimality) (Mäkelä et al., 2016) A solution x∗ of problem (5) is weakly global Pareto optimal if there does not exist another x such that fτ (x∗) ≥ fτ (x) for τ ∈ {1, . . . , m}.\n\nTheorem C.6. (Theorem 5 in (Miettinen & Mäkelä, 1995)) For a multi-objective optimization problem [f1(w), f2(w), . . . , fm(w)] and its corresponding improvement function H (w, w∗). A necessary min w\ncondition for w∗ ∈ Rn to be weakly global Pareto optimal is that w∗ = arg minw H(w, w∗). Moreover, if fi(w) is convex ∀i, then it is a sufficient condition.\n\nWe observe the following.\n\n1) By replacing fτ = Lτ ◦ w∗ setting.\n\nτ (w), ∀τ , the above theorem can be directly applied to the MAML\n\n2) f in our setting can be a neural network. Thus, the convexity condition for fτ (w) can be hard to satisfy. Here, we give the a more relaxed version of the above theorem: Theorem C.7. w∗ := arg minwH (w, w∗) is Pareto stationary.\n\nProof. Using Theorem 2 in (Miettinen & Mäkelä, 1995), we have ∂wH (w, w∗) ⊂ conv (cid:83) convex set.\n\ni ∂w (fi(w) − fi (w∗)), where ∂ is reloaded as sub-gradient, and conv is the\n\nNote that 0 ∈ ∂wH (w, w∗) due to the fact that the element in the convex set of the union of subgradients is still a sub-gradient, and for w ∈ {w|0 ∈ ∂wH (w, w∗)}, the sub-gradient ∂wH (w, w∗) is zero. Then, we always have (cid:80) i wi = 1, based by definition of a convex set, where wi ∈ [0, 1], ∀i is a real number. By simplifying the above term, we have (cid:80) i wi∂wfi(w) = 0. which is exactly the definition of Pareto sta-\n\ni wi∂w (fi(w) − fi (w∗)) = 0, (cid:80)\n\ni wi∂ (fi(w) − fi (w∗)) = (cid:80)\n\ntionary point.\n\nWe now show convergence of Algorithm 1.\n\nLemma C.8. In each epoch s of Algorithm 1, set ˆλ′′ =\n\n(cid:26)0\n\nEp[rθ(τ )] = 1\n\n∞ otherwise\n\n, we have:\n\nH(ws + d, ws) ≤ H(ws, ws) .\n\nWhen equality holds, ws is a stationary point.\n\nProof. By Lemma C.2, the solutions of\n\nmaxθ mind EU (τ )[rθ(τ ) [Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))] + ˆλ′\n\n2 ||d||2 − ˆλ′′\n\n2 (Ep[rθ(τ )] − 1)2]\n\nare the same as those from\n\nmind maxθ EU (τ )[rθ(τ ) [Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))] + ˆλ′\n\n2 ||d||2 − ˆλ′′\n\n2 (Ep[rθ(τ )] − 1)2].\n\nSolving θ, the above minimax problem degenerates to\n\nmin d\n\nmax τ =1...,m\n\n[Lτ (w∗\n\nτ (ws + d)) − Lτ (w∗\n\nτ (ws))] +\n\nˆλ′ 2\n\n∥d∥2.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nNote that the above optimization problem is exactly mind H(ws + d, ws) + ˆλ′ 2 ||d||2, where d is the steepest descent direction. Then, due to the property of steepest descent direction, we always have H(ws + d, ws) ≤ H(ws, ws), and equality holds iff ||d||2 = 0, where ws a stationary point according to the definition.\n\nLemma C.9. Algorithm 1 converges to a Pareto stationary point. Moreover, if Lτ (w∗ Algorithm 1 converges to a global weak Pareto optimal point.\n\nτ (w)) is convex,\n\nProof. From Lemma C.8, the sequence {H(ws+d, ws, ws+1 1:m)}s is decreasing. Also obviously, H(ws + d, ws, ws+1 1:m) has a lower bound. Thus, using the monotone convergence theorem, Algorithm 1 converges. Using Theorem C.7, it is Pareto stationary. Together with Theorem C.6, it is global Pareto optimal if Lτ (w∗\n\n1:m , ws\n\n1:m , ws\n\nτ (w)) is convex.\n\nLemma C.10. (i) If Lτ (w∗ concave, rθ(τ ) and Lτ (w∗ then R(θ, w) is L + Cmaxμ′ convex w.r.t. w and 2L-smooth w.r.t. w, θ and C ′\n\nτ (w + d)) is L-smooth and μ′-convex w.r.t. w and d, and rθ(τ ) is μτ (ws+1)) ≤ C ′ max), maxμ concave w.r.t. θ. τ (w + d)) is L-smooth w.r.t. w and d, and rθ(τ ) is μ-concave, then ̃R(θ, w, B) is\n\nτ (ws+1)) are bounded (i.e., rθ(τ ) ≤ Cmax and Lτ (w∗\n\n(ii) If Lτ (w∗ L-smooth w.r.t. w, θ, and C ′\n\nmaxμ-concave w.r.t. θ.\n\nProof. For claim (i), we first prove that for any differentiable functions f, g, ⊥ [f (x)] ⊥ [g(x)] is 0-smooth and convex w.r.t. x. Note that\n\n||∇x ⊥ [f (x)]⊥ [g(x)] − ∇x′ ⊥ [f (x′)]⊥ [g(x′)] ||2 = 0\n\ndue to the property of the stop gradient operator. Thus,\n\n⊥ [f (x)]⊥ [g(x)]− ⊥ [f (x′)]⊥ [g(x′)] = −∇x′(⊥ [f (x′)]⊥ [g(x′)])(⊥ [f (x)] ⊥ [g(x)]) = 0.\n\nFor convexity, by using the property of 0-smoothness, we have\n\n⊥ [f (x′)]⊥ [g(x′)]− ⊥ [f (x)]⊥ [g(x)] ≥ ∇x′(⊥ [f (x′)]⊥ [g(x′)])(⊥ [f (x)]⊥ [g(x)]). Thus, ⊥ [f (x)] ⊥ [g(x)] is convex due to the definition of convexity. Therefore, ⊥ [f (x)] ⊥ [g(x)] is also 0-smooth and convex w.r.t. x.\n\nA direct application of the above, we obtain ⊥ [f (x)] is 0-smooth and convex w.r.t. x by setting g(x) ≡ 1.\n\nτ (w))]], ⊥ [(LDT (w∗\n\nUsing the above, we have ⊥ [(Lτ (w∗ d∗) − Lτ (w∗ d∗) − LDT (w∗ LDT (w∗ L ≥ 0).\n\nT (w))]] ⊥ [ET ∼U rθ(T )[LDT (w∗\n\nT (w))] are also 0-smooth and convex. Therefore rθ(T ) ⊥ [(LDT (w∗\n\nτ (w) + d) − Lτ (w∗\n\nT (w) + d∗) − LDT (w∗\n\nT (w) + d∗) − LDT (w∗\n\nτ (w))], ⊥ [Eτ ∼U rθ(τ )[Lτ (w∗ T (w))]] ⊥ ET ∼U rθ(T )[LDT (w∗\n\nτ (w) + T (w) + T (w) + d∗) − T (w))]] is convex and L-smooth (as\n\nτ (w)]] is Cmaxμ′-convex and L-smooth due to our assumption.\n\nτ (w)]] is Cmaxμ′-convex and L-smooth since the addition of convex τ (w)]] is Cmaxμ′-convex, and the adding of smooth funcτ (w)]] is L-smooth, and taking the average does not affect the\n\nAlso, we have ⊥ (rθ(τ ))[Lτ (w∗ Then, Eτ ∼U [⊥ (rθ(τ ))[Lτ (w∗ functions implies Eτ ∼U [⊥ (rθ(τ ))[Lτ (w∗ tions implies Eτ ∼U [⊥ (rθ(τ ))[Lτ (w∗ results. Thus, R(θ, w) is Cmaxμ′-convex and L-smooth. For rθ, since − λ′ (cid:80) λ′′ 2 ( 1 The proof of claim (ii) is similar.\n\nτ [rθ(τ )] − 1)2 is also C ′\n\nB\n\n2 ∇θ(Ep[rθs (τ )] − 1)2 + ∆(θ) is concave, ∆(θ) is 2μC ′\n\nmax-concave. Then ∆(θ) −\n\nmaxμ-concave. Thus R(θ, w) is C ′\n\nmaxμ-concave.\n\nProof. (of Theorem 4.2) Recall Lemma C.10 on the properties of convex and smooth for ̃R. Combine it with the assumptions in Theorem 4.2, and use Theorem 4.9 in (Lin et al., 2020), we obtain the bound of O( 1\n\nε8 ) when B = 1.\n\nWhen B > 1, note that using Lemma A.2 in (Lin et al., 2020), we have:\n\nE\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 B\n\nB (cid:88)\n\ni=1\n\n ̃R(θ, w, B)\n\n2  ≤ ∥∇wR(θ, w)∥2 +\n\nσ2 B\n\n.\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTherefore, let σ2′\n\n= σ2\n\nB and use Theorem 4.9 in (Lin et al., 2020) shows the bound of O( 1\n\nε8 ).\n\nThe remaining part that the fixed point w of maxθ ̃R(θ, w, B) is global Pareto optimal (resp. Pareto stationary) can be obtained by using Lemma C.9, which says that the fixed point of Algorithm 1 is global Pareto optimal (resp. Pareto stationary).\n\nFinally, we show that using SIMOL on MAML can guarantee convergence.\n\nProof. (of Corollary 4.2.1) First, we show that if ∇wLτ (w) is Hessian-Lipschitz continuous, bounded and Lipschitz-continuous, and ∥w∥ is bounded, then w∗ τ (w) is also Hessian-Lipschitz continuous, bounded and Lipschitz-continuous. Note that\n\nww∗ ∥∇2 ∥∇ww∗\n\nw′w∗ τ (w) − ∇2 τ (w) − ∇w′w∗ ∥w∗\n\nτ (w′)∥ = α∥∇2 w′(∇w′Lτ (w′))∥, w(∇wLτ (w)) − ∇2 τ (w′)∥ = α∥∇w(∇wLτ (w)) − ∇w′(∇w′Lτ (w′))∥, τ (w)∥ = ∥w − α∇wLτ (w)∥ = ∥w∥ + α∥∇wLτ (w)∥.\n\nIt is easy to see that w∗\n\nτ (w) is also Hessian-Lipschitz continuous, bounded and Lipschitz continuous.\n\nApplying Lemma 3 in (Collins et al., 2020), we obtain that Lτ (w∗ positive. Then setting C = L and using Theorem 4.2, we get the desire result.\n\nτ (w)) is C-smooth, where C is\n\nD HYPER-PARAMETER SELECTION OF SIMOL\n\nFor the few-shot regression experiment (section 5.1), the regularization parameters ˆλ′, ˆλ′′ are selected from {0.001, 0.01, 0.1, 1}, and learning rate β′ is selected from {0.01, 0.03, 0.1, 0.3, 1} based on the validation set. For the few-shot classification experiments (section 5.2), we use the ˆλ′, ˆλ′′) combination selected from few-shot regression, while the learning rate β′ is selected from {0.0003, 0.0008, 0.01, 0.03, 0.08, 0.1} for the 1-shot miniImageNet task based on the validation set. this is then also used in the other few-shot classification experiments.\n\n20",
    "reference": "# Summary Of The Paper\n\nThis paper proposes to formulate the meta-learning as a multi-objective optimization (MOO) problem instead of the traditional single objective problem where the sum of each task’s loss is optimized. By doing do the compromising phenomenon can be avoided, but it also incurs a new problem — the current gradient-based MOO solvers cannot scale to a large number of objectives. Then the authors proposes a scalable gradient-based solver via the use of improvement functions. Besides, this algorithm is guaranteed to have theoretical convergence. Experimental results on few-shot regression, few-shot classification and reinforcement learning show that the proposed method, called SIMOL, outperforms several existing algorithms in terms of either efficiency or generalization.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. This work is well motivated that reformulating the original problem to a MOO can avoid the compromising phenomenon.\n2. The overall logic of deriving the solution along the journey is quite clear: 1) The improvement function can provide a closed-form gradient for the original problem; 2) The closed-form requires  uniform sampling of the tasks, which can be parameterized as a neural network; 3) Optimizing the network requires another sampling of tasks to compute the gradient of the NN; 4) Theoretically, the gap between the full batch and mini-batch of tasks can be bounded, so we can safely adopt the SGD to optimize the NN and finally solve the MOO problem with pareto optimality.\n3. Empirical studies looks very promising that the proposed method works quite well on multiple tasks and is better than the competitors.\n\nWeaknesses:\n\n1. There are tons of math equations. The author may need to simplify or condense them a bit, and give timely interpretation for them. It is important to clearly extract and convey the message behind the equations to the readers.\n2. The fonts of Figure 1 and 2 are quite small that it is hard to read.\n3. Typos: e.g., “the converged solution may be comprised” —> “the converged solution may be compromised”\n4. Immediately after Equation (7), it is logically not correct to say “We first rewrite the above as:”, since Lemma 3.1 is to prove that (7) and (8) are equivalent, but before the proof, we don’t know if that’s true.\n5. Definition 2.1. and 2.2. may need to cite some classical literatures rather than those new ones, as these definitions should exist long time ago.\n6. In Table 3, SIMOL has much large variance than others in the MAML category. Would that be a concern?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe overall idea is clearly conveyed, though the massive equations hurts the readability a bit.\n\nThe idea of converting the mete-learning problem as a multi-objective optimization sounds new, and the derived algorithm with theoretical guarantee is a solid contribution too.\n\nAlgorithm 1 seems easy to reimplement, so the results could be reproducible with some effort.\n\n# Summary Of The Review\n\nThe paper seems to be the first to reformulate the meta-learning problem as a MOO to avoid the compromising problem. Then a gradient-based algorithm guaranteed to converge to pareto optimal solution is proposed. Overall, this is a solid contribution.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBOPTFORMER: BEYOND TRANSFORMER FOR BLACKBOX OPTIMIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe design a novel Transformer for continuous unconstrained black-box optimization, called BOptformer. Inspired by the similarity between Vision Transformer and evolutionary algorithms (EAs), we modify Tansformer’s multi-head self-attention layer, feed-forward network, and residual connection to implement the functions of crossover, mutation, and selection operators. Moreover, we devise an iterated mode to generate and survive potential solutions like EAs. BOptformer learns the optimization strategies from the target task automatically without human intervention, which addresses the poor generalization of human-designed EAs when given a new task. Compared to baselines, such as EAs, Bayesian optimization, and the learning-to-optimize (L2O) method, BOptformer shows the top performance in six black-box functions and two real-world applications. We also find that untrained BOptformer can achieve good performance on the simple tasks. Deep BOptformer performs better than shallow ones. We bring a new and efficient Transformer-based black-box optimization framework for the L2O and EA communities.\n\n1\n\nINTRODUCTION\n\nMany tasks, such as neural architecture search (Elsken et al., 2019) and hyperparameter optimization (Hutter et al., 2019; Golovin et al., 2017), can be abstracted as black-box optimization problems, which means that although we can evaluate f (x) for any x ∈ X, we have no access to any other information about f , such as the Hessian and gradients. A series of hand-designed algorithms, such as evolutionary algorithms (EAs) (Mitchell, 1998; Khadka & Tumer, 2018; Zhang & Li, 2007), Bayesian optimization (Snoek et al., 2012; Mutny & Krause, 2018; Li et al., 2017; Kandasamy et al., 2015; Balandat et al., 2020), and evolutionary strategies (ES) (Wierstra et al., 2014; Hansen & Ostermeier, 2001; Auger & Hansen, 2005; Salimans et al., 2017), have been designed to solve black-box optimization.\n\nRecently, the learning to optimize (L2O) framework (Chen et al., 2022) gives an new insight on optimization by leveraging the recurrent neural network (RNN), long short-term memory architecture (LSTM) (Chen et al., 2020; Andrychowicz et al., 2016; Chen et al., 2017; Li & Malik, 2016; Wichrowska et al., 2017; Bello et al., 2017) or multilayer perceptron (MLP) (Metz et al., 2019) as the optimizer to develop optimization methods, aiming at reducing the laborious iterations of hand engineering (Sun et al., 2018; Vicol et al., 2021; Flennerhag et al., 2021; Li & Malik, 2016; Sun et al., 2018). They don’t concentrate on issues with black-box optimization. The core of L2O is constructing a strong mapping from the initial solutions to the optimal solution. Although several efforts like (Cao et al., 2019; Chen et al., 2017) have coped with the black-box problems, their effectiveness may be hindered by the limited representational capabilities of RNN, LSTM, and MLP.\n\nIn EAs, the hand-designed crossover, mutation, and selection operators make the initial population move near the optimal solution. This updated model has stood the test of time. Because the evolutionary operators must be modified to maximize their performance on the target task, humandesigned EAs have a low generalization ability to a new black-box problem. Most notably, the limited use of target function information in EA design due to expert knowledge limitations makes it difficult to adapt to the target task. Learning the optimization strategies from the taget task is the key step to overcome this limitation.\n\nThis paper designs a novel L2O framework based on the advantages of Vision Transformer (Dosovitskiy et al., 2021) and EAs to overcome the above limitations, termed BOptformer. Moreover,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTransformer (Han et al., 2022) owns a strong representation ability, and there is currently no work to use Transformer for optimization. Inspired by the similarity of EAs and Transformer (Zhang et al., 2021; 2022), BOptformer revised the critical part of Transformer to realize the mapping from the random and optimal populations. To generate potential individuals to approach the optimal solution, we first design an self-attention (SA)-based crossover module (SAC) to simulate the crossover operator of EA, and then the output of this module is input into the proposed feed-forward network (FFN)-based mutation module (FM) to perform mutation. Moreover, the residual and selection module (RSSM) is designed to survive the fittest individuals. RSSM is a pairwise comparison between the output of SAC, FM, and the input population regarding their fitness. We design an BOptformer Block (OB) consisting of SAC, FM, and RSSM. Finally, we construct BOptformer by stacking OBs to simulate generations of EAs.\n\nMoreover, to cope with black-box optimization, we establish a function set to train BOptformer under an unsupervised mode. We construct a set of differentiable functions with similar properties to the targeted black-box optimization problems. This training set contains the pair of the initial population and the designed function. Thus, we can use gradient-based methods to train BOptformer.\n\nWe tested BOptformer on six standard functions, the protein docking (Cao & Shen, 2020) problem, and the planar mechanic arm problem (Wang et al., 2021). The experimental results demonstrate the top rank of BOptformer and the strong representation compared with three population-based baselines, Bayesian optimization, and one learning-to-optimize method (Cao et al., 2019). Moreover, we also analyze the effect of learning rate, deep structure, and weight sharing between OBs. The highlights of this paper are summarized as follows:\n\n1) We propose a solid Transformer-based L2O framework addressing black-box problems to the L2O community. We have demonstrated its benefit when compared with standard black-box optimization methods, particularly for the L2O-based method.\n\n2) BOptformer efficiently uses the target black-box function’s information to aid in the development of the optimization strategy. Compared to the human-designed EA, BOptformer has a substantially greater degree of task fit.\n\n2 RELATED WORK\n\nTransformer Transformer structure achieves significant progress for machine translation task (Vaswani et al., 2017), computer vision task (Dosovitskiy et al., 2021), time series task (Zhou et al., 2021), and so on. Many improved models are proposed and obtain great achievements (Han et al., 2022). There are no Transformer-based efforts for handling optimization problems, which is crucial in the machine learning community. (Vaswani et al., 2017) proposed the meta-learning hyperparameter optimization framework with Transformers to learn both policy and function priors from data across different search spaces. However, the BOptformer proposed in this paper expands the application scope of Transformer and can effectively deal with this case. The basic modules of Transformer are shown in Appendix A.1.\n\nEvolutionary Algorithm Inspired by the evolution of species, EAs have provided surprising performance for black-box optimization (Mitchell, 1998). The basic modules of EAs are shown in Appendix A.2. Many influential variants have been proposed to deal with different problems (Das & Suganthan, 2010; Wu & Liu, 2019), but at their core they are: 1) recombination and mutation, how to produce the excellent solution; 2) selection, how to choose the best individuals between the parents and offspring. Thus, many algorithmic components have been designed for different tasks. The performance of algorithms varies towards various tasks, as different optimization strategies may be required given diverse landscapes. Current methods manually adjust genetic operators’ hyperparameters and design the combination between them (Kerschke et al., 2019; Tian et al., 2020) to map the random population to the optimal solution. We require an expert to design or choose the evolutionary operations when given a new black-box optimization task to maximize its performance on the target task, which negatively impacts generalization ability. Most notably, the limited use of target function information in EA design due to expert knowledge limitations makes it difficult to adapt to the target task. The suggested BOptformer uses a Transformer framework instead of the manually designed crossover, mutation, and selection operators. The genetic operator is then designed automatically by the built Transformer rather than by a human designer. BOptformer efficiently\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nuses the target black-box function’s information to aid in developing the optimization strategy. In comparison to the human-designed EA, BOptformer has a substantially greater degree of task fit.\n\n3 BOPTFORMER\n\n3.1 PROBLEM DEFINITION\n\nA black-box optimization problem can be transformed as a minimization problem, as shown in Equation (1), and constraints may exist for corresponding solutions:\n\nmin f (x), s.t. xi ∈ [li, ui]\n\n(1)\n\nwhere x = (x1, x2, · · · , xd) represents the solution of optimization problem f , the lower and upper bounds l = (l1, l2, · · · , ld) and u = (u1, u2, · · · , ud), and d is the dimension of x. Suppose n individuals of one population be X1 = (X1,1, X1,2, · · · , X1,d), X2 = (X2,1, X2,2, · · · , X2,d), · · · , Xn = (Xn,1, Xn,2, · · · , Xn,d), then BOptformer are required to find the population near the optimal solution ˆx. We suppose that X 0 is the initial population and X t is the output population.\n\n3.2 SELF-ATTENTION CROSSOVER MODULE\n\nSimilar to the crossover operator in EAs, we propose a new module based on SA to generate the potential solutions by maximizing information interaction among individuals in a population. The crossover operator generates a new individual by (cid:80)n i=1 XiW c is the diagonal matrix. If W c i is full of zeros, the ith individual has no contribution. Suppose a population X is arranged in a non-descending order of fitness, and F ∈ Rn×1 be the fitness matrix of X. Then, this module can be represented as follows:\n\ni (Zhang et al., 2021). W c\n\ni\n\nX c = SAC(X, F )\n\n(2)\n\nwhere X c is the output population of the proposed SAC module.\n\nSince the object processed by BOptformer is the population, and the order of individuals in the population does not affect the population distribution, SA does not require position coding. Standard SA projects the input sequence X into a d-dimensional space via the queries (Q), keys (K), and values (V ). These three mappings enable the SA module to capture better the characteristics of the problems encountered during training. In other words, these three mappings strengthen the ability of SA to focus on specific problems but do not necessarily make SA have good transferability between different problems. Therefore, we consider removing these three mappings for enhanced transferability, and X c = AX. A ∈ Rn×n is a self-attention matrix that can be learned to maximize inter-individual information interaction based on individual ranking information. This is why the population needs to be sorted in non-descending order.\n\nHowever, designing crossover operations based solely on population ranking information is a coarsegrained approach. Because this method only considers the location information of individuals in the population, but does not consider the fitness relationship between individuals. Therefore, we further introduce fitness information to assist in learning crossover operators:\n\nAF = SA(F ) = Sof tmax (cid:0)F W Q(F W K)T /sqrt(dk)(cid:1)\n\nThus, X c = AX + AF X. To better balance the roles of A and AF , we introduce two learnable 2 ∈ Rn×1. Therefore, the final crossover operation is shown as follows: weights W c\n\n1 ∈ Rn×1 and W c\n\n1 ) ⊙ (AX) + tile(W c where X c ∈ Rn×d is the population obtained by X through the SAC module; ⊙ represents Hadamard product; the tile copy function extends the vector to a matrix.\n\n2 ) ⊙ (AF X)\n\nX c = tile(W c\n\n(3)\n\n3.3 FFN-BASED MUTATION MODULE\n\nThe mutation operator brings random changes into the population. Specifically, an individual Xi in the population goes through the mutation operator to form the new individual ˆXi, formulated\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Overall architecture of BOptformer and OB. N x stands for BOptformer is composed of N x stacked OBs. These OBs can be set to share weights with each other or not share weights with each other.\n\ni\n\ni\n\n. W m\n\nas ˆXi = XiW m is the diagonal matrix. In Transformer, each patch embedding carries on directional feature transformation through the FFN module. We take one linear layer as an example: X = XW F , where W F is the weight of the linear layer, and it is applied to each embedding separately and identically. This equation and the mutation operator have the same formula format, which inspires us to design a learnable mutation module FM based on FFN with ReLU activation function:\n\nX m = F M (X c) = (ReLU (XW F\n\n2 + b2 where X m is the population after the mutation of X c. W F 1 represent the weight of the second layer of FFN and the weight of the first layer of FFN, respectively. b2 and b1 represent the bias of the second layer and the first layer of FFN, respectively.\n\n1 + b1))W F 2 and W F\n\n(4)\n\n3.4 SELECTION MODULE\n\nThe residual connection in the transformer can be analogized to the selection operation in EA (Zhang et al., 2021). We combine the residual structure and selection module (SM) (Anonymous, 2023) to design a learnable selection module RSSM. The RSSM generates the offspring population according to the following equation:\n\nˆX = RSSM (X, X c, X m) = Sort(SM (X, tile(W s\n\n1 ) ⊙ X + tile(W s\n\n2 ) ⊙ X c + tile(W s\n\n3 ) ⊙ X m))\n\n(5)\n\n2 ∈ Rn×1, and W s\n\nwhere ˆX is the fittest population for the next generation; the learnable weights W s 1 ∈ Rn×1, 3 ∈ Rn×1 are the weights for X, X c, and X m, respectively. Sort(X) W s represents that X is sorted in non-descending order of fitness. We use quicksort to sort the population. These three learnable weight matrices realize the weighted summation of residual connections, thereby simulating a learnable selection strategy. Meanwhile, the introduction of residual structure also enhances the model’s representation ability, enabling BOptformer to form a deep architecture.\n\nSM updates individuals based on a pairwise comparison between the offspring and input population regarding their fitness. Suppose that X and X are the input populations of SM. We compare the quality of individuals from X and X pairwise based on fitness. A binary mask matrix indicating the selected individual can be obtained based on the indicator function lx>0(x), where lx>0(x) = 1 if x > 0 and lx>0(x) = 0 if x < 0. SM forms a new population ˆX by employing Equation (6).\n\n′\n\n′\n\nˆX = tile(lx>0(MF ′ − MF )) ⊙ X + tile(1 − lx>0(MF ′ − MF )) ⊙ X\n\n′\n\n(6)\n\nwhere the tile copy function extends the indication vector to a matrix, MF (MF ′) denotes the fitness matrix of X(X\n\n).\n\n′\n\n3.5 STRUCTURE OF BOPTFORMER\n\nBOptformer comprises basic t BOptformer blocks (OBs), and parameters can be shared among these t OBs or not. The overall architecture of BOptformer and OB is shown in Figure 1. Each OB consists of SAC, FM, and RSSM. X 0 ∈ Rn×d represents the initial population input into BOptformer, which\n\n4\n\nPopulationSorting FMFitness MatrixOrdered Population X0SACOBiRSSMNxXcXmInput Population OutputPopulation 1−iXiXUnder review as a conference paper at ICLR 2023\n\nneeds to be sorted in non-descending order of fitness. In Equation 7, X i−1 is fed into OBt to get X i, where i ∈ [1, t]. BOptformer realizes the mapping from the random initial population to the target population by stacking t OBs.\n\nX i = OB(X i−1); X c = SAC(X i−1, F ); X m = F M (X c); X i = RSSM (X i−1, X c, X m)\n\n(7)\n\n3.6 TRAINING OF BOPTFORMER\n\nTraining Dataset Before introducing the details of the training dataset, fidelity (Kandasamy et al., 2016) is defined as follows: Suppose the differentiable surrogate functions f1, f2, · · · , fm are the continuous exact approximations of the black-box function f . We call these approximations fidelity, which satisfies the following conditions: 1) f1, · · · , fi, · · · , fm approximate f . ||f − fi||∞ ≤ ζm, where the fidelity bound ζ1 > ζ2 > · · · ζm. 2) Estimating approximation fi is cheaper than estimating f . Suppose the query cost at fidelity is λi, and λ1 < λ2 < · · · λm.\n\nTraining data is a crucial factor beyond the objective functions. This paper establishes the training set by constructing a set of differentiable functions related to the optimization objective. This training dataset only contains (X0, fi(x|ω)), the initial population and objective function, respectively. The variance of ω causes the shift in landscapes. The training dataset is designed as follows: 1) Randomly initialize the input population X0; 2) Randomly produce a shifted objective function fi(x|ω) by adjusting the parameter ω; 3) Evaluate X0 by fi(x|ω); 4) Repeat Steps 1)-3) to generate the corresponding dataset. We show the designed training and testing datasets as follows:\n\nF train = {f1(x|ωtrain\n\n1,i\n\n), · · · , fm(x|ωtrain\n\nm,i )}\n\n(8)\n\nwhere ωtrain\n\nm,i\n\nrepresents the ith different values of ω in mth function fm.\n\nLoss Function BOptformer attempts to search for individuals with high quality based on the available information. The loss function tells how to obtain the parameters of BOptformer to generate individuals closer to the optimal solution by maximizing the difference between the initial population and the output population of BOptformer. The following loss function is employed (Anonymous, 2023),\n\n(cid:80)\n\n1 |X 0|\n\nx∈X 0\n\nfi(x|ω) −\n\n1 |Eθ(X 0)|\n\n(cid:80)\n\nfi(x|ω)\n\nx∈Eθ(X 0)\n\nli(X 0, f (x|ω)) =\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) where θ denotes parameters of BOptformer (E). Equation (9) calculates the average fitness difference between the input and output, further normalized within [0, 1]. To encourage BOptformer to explore the fitness landscape, for example, the constructed Bayesian posterior distribution over the global optimum (Cao & Shen, 2020) can be added to Equation (9). Since the derivatives of functions in the training dataset are available, we can obtain the gradient information of Equation (9) for the training process. Also, we can employ REINFORCE (Williams, 1992) to approximate these derivatives.\n\n(cid:12) (cid:12) (cid:12) fi(x|ω) (cid:12) (cid:12)\n\n1 |X 0|\n\nx∈X 0\n\n(cid:80)\n\n(9)\n\nTraining BOptformer We then train BOptformer under a supervised mode. Since the gradient is unnecessary during the test process, BOptformer can solve black-box optimization problems. To prepare BOptformer to learn a balanced performance upon different optimization problems, we design a loss function formulated as follows:\n\nlΩ = −\n\n1 K\n\n(cid:88)\n\nX 0∈Ω\n\nli(X 0, fi(x|ωtrain\n\ni\n\n))\n\n(10)\n\nWe employ Adam (Kingma & Ba, 2014) method with a minibatch Ω to train BOptformer upon the constructed training dataset.\n\nDetailed Training Process The goal of the training algorithm is to search for parameters θ∗ of the BOptformer. Before training starts, BOptformer is randomly initialized to get initial parameters θ. Then the algorithm will perform the following three steps in a loop until the training termination condition is satisfied: Step 1, randomly initialize a minibatch Ω comprised of K populations X 0;\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nStep 2, for each fi ∈ F train, given training data (X 0, fi), update θ by minimizing the lΩ; Step 3, given X 0, update θ by minimizing −1/m (cid:80) i lΩ, where m is the number of functions in F train. After completing the training process, the algorithm will output θ∗.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\n4.1.1 DATASETS\n\nSynthetic Functions This paper first employs nine commonly used functions to show the effectiveness of the proposed BOptformer. The characteristics of these nine functions are shown in Tables 6 and 7 (Appendix). Here, BOptformer is trained on F train is generated based on functions in Table 6, and the target functions are shown in Table 7 (Appendix). Here, d = {10, 100}.\n\nProtein Docking We also handle the problem of Ab initio protein docking (Cao & Shen, 2020), which optimizes a noisy and costly function in a high-dimensional conformational space. Mathematically, this problem is formulated as optimizing the Gibbs binding free energy f (x) for conformation x. We calculate the energy function in a CHARMM 19 force field as in (Moal & Bates, 2010) and shift it so that f (x) = 0 at the origin of the search space. f (x) is differentiable when we parameterize the search space as R12 (Smith & Sternberg, 2002). Here, only 100 interface atoms are considered. The details of this problem can be found in Appendix A.8.\n\nPlanner Mechanic Arm We further evaluate the performance of the proposed scheme on the planner mechanic arm problem, which has been widely used to evaluate the performance of the black-box optimization algorithms (Cully et al., 2015; Vassiliades et al., 2018; Vassiliades & Mouret, 2018; Mouret & Maguire, 2020). The optimization goal of this problem is to minimize the distance from the top of the mechanic arm to the target position by optimizing a set of lengths angles. The detailed problem can be found in Appendix A.5. r represents the distance from the target point to the origin of the mechanic arm, as shown in Fig. 4 (Appendix).\n\nTable 1: The compared results on six functions.\n\nd\n\n10\n\n100\n\nf\n\nF4 F5 F6 F7 F8 F9\n\nF4 F5 F6 F7 F8 F9\n\nDE\n\nES\n\nCMA-ES\n\nL2O-swarm\n\nDragonfly\n\nBOptformer\n\n0.13(0.06) 4.99(1.24) 210.2(49.7) 17.83(3.59) 0.21(0.07) 1.90(0.32)\n\n8.2e3(3.8e2) 28.2(0.61) 2.4e8(2.3e7) 9410(548) 3.04(0.14) 18.9(0.14)\n\n0.22(0.30) 0.55(0.37) 60.02(48.28) 51.53(8.05) 0.26(0.18) 20.56(0.03)\n\n8.9e4(9.5e3) 80.2(2.10) 2.5e10(4.5e9) 8.9e4(1.1e4) 23.0(3.00) 21.4(0.02)\n\n4.2e-4(3.5e-4) 0.03(0.01) 61.90(96.25) 45.74(17.02) 7.6e-3(0.01) 0.04(0.02)\n\n7.8e3(1.2e3) 78.3(9.18) 3.3e8(8.7e7) 8050(775) 3(0.22) 21.4(0.04)\n\n16.92(2.10) 2.97(0.01) 26.83(21.48) 4.88(3.55) 1.02(1.4e-3) 9.06(0.67)\n\n0.32(0.03) 0.28(1.3e-3) 692(108) 85.7(18.6) 0.16(2.4e-4) 2.49(2.5e-3)\n\n1.3e3(1.3e3) 48.4(9.58) 3.8e8(1.4e8) 81.1(24.0) 35.4(22.6) 16.2(3.64)\n\n11200(3750) 50(0) 99(0) 144(13.1) 125(11.3) 10.5(0.32)\n\n1.2e-4(5e-5) 8e-3(2e-3) 8.93(0.03) 0.01(0.03) le-5(2e-5) 0.01(3.4e-3)\n\n0.11(0.09) 0.14(0.15) 129(346) 24.1(13) 0.02(0.03) 0.15(0.05)\n\n4.1.2 BASELINES\n\nBOptformer is compared with standard EA baselines, such as DE(DE/rand/1/bin) (Das & Suganthan, 2010), ES((μ,λ)-ES), and CMA-ES, where DE and ES are implemented based on geatpy (Jazzbin, 2020), and CMA-ES is implemented by pymoo (Blank & Deb, 2020). L2O-swarm (Cao et al., 2019) is a representative L2O method for black-box optimization. Moreover, Dragonfly (Kandasamy et al., 2020), a representative algorithm for Bayesian optimization, is employed as a reference. We design three BOptformer models, including 3 OBs with WS (weight sharing), 5 OBs without WS, and 30 OBs with WS. The parameters of these methods are shown in Appendix A.4.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 RESULTS\n\nSynthetic Functions The results on six functions are provided in Table 1. BOptformer outperforms three EA baselines, Dragonfly, and L2O-swarm in all cases, but loses once to Dragonfly in F6 with d = 100. These cases also show the excellent generalization ability of BOptformer on more tasks unseen during the training stage. We think the transferability of BOptformer is proportional to the fitness landscape similarity between the training set and the problem. Although new problem attributes are not available in the training set, BOptformer can still perform better. However, this conclusion only holds when the similarity between the problem and training dataset is high. We plot the convergence curves of BOptformer (10 OBs with WS), ES, DE, and CMA-ES on F7 (see Appendix A.7, Figure 5). BOptformer converges quickly and can obtain better solutions. BOptformer can only iterate ten times to get the best solution relative to EA baselines. ES and DE converge around 100 generations, and CMA-ES shows a slow convergence rate.\n\nMethods\n\nTable 2: The results on the problem of protein docking.\n\nProtein Docking We also test the performance of BOptformer on the problem of protein docking. The experimental results is shown in 2. The performance of BOptformer exceeds that of L2O-swarm. During training, L2O-swarm does not converge. At the same time, we find that better solutions exist in the initial population than those found by L2O-swarm. However, during testing, L2O-swarm lost these good solutions.\n\nL2O-swarm 30OBs with WS 5OBs without WS\n\n1689(23.64) -5.97e3(125) -5.98e3(131)\n\n2090(25.08) -6.03e3(145) -6.01e3(146)\n\n2765(24.80) -6.03e3(127) -6.02e3(125)\n\n1ATN 7\n\n7CEI 1\n\n2JEL 1\n\nPlanner Mechanic Arm The detailed experimental results are given in Tables 3. BOptformer selects 5 OBs without WS as the example, which evolves only five generations. Untrained represents the untrained BOptformer. DE, ES, and CMA-ES are tested when the maximum generations is set to 100. EA baselines have 100/5 times as many function evaluations as BOptformer. However, even in this unfair situation, BOptformer achieves the best results. We have observed that BOptformer can achieve better results with deeper architectures. However, it is currently difficult for us to train deep BOptformer. Moreover, as far as we know, the use of ES to optimize deep models has been studied a lot (Vicol et al., 2021), which will be an essential research prospect in the future.\n\nTable 3: The results of planar mechanical arm. Simple Case (SC): searching for different angles with the fixed lengths. Complex Case (CC): searching for different angles and lengths.\n\nr 100 300 1000 100 300 1000\n\nDE 1.20(0.64) 1.38(0.71) 93.8(137) 0.81(0.47) 6.15(12.2) 232(233)\n\nES 10.6(5.58) 44.9(43.3) 183(239) 8.95(6.42) 47.8(56.0) 251(258)\n\nCMA-ES 1.36(0.35) 1.38(0.41) 43.7(110) 0.76(0.20) 0.87(0.37) 88.4(158)\n\nL2O-Swarm BOptformer 0.30(0.18) 0.48(0.37) 26.6(57.4) 0.06(0.05) 0.50(0.79) 25.0(55.8)\n\n40.4(3.89) 69.5(3.77) 176(7.20) 31.9(1.78) 89.1(1.96) 262(2.99)\n\nUntrained 243(238) 1210(820) 5070(2770) 243(238) 1210(820) 5070(2770)\n\nSC\n\nCC\n\n4.3 PARAMETER ANALYSIS\n\nf\n\nUntrained\n\n5 OBs without WS\n\nTable 4: The performance of different BOptformer structures.\n\nWe consider the performance of different BOptformer architectures. Here, d = 10. The experimental results are shown in Table 4. We find that they were sorted from good to worst by their performance, and the result is 30 OBs with WS>5 OBs without WS>3 OBs with WS. Deep architectures have better representation capabilities and also lead to better performance. However, it is challenging to train non-weight sharing BOptformers with more layers due to the difficulty of training deep architectures. Untrained represents that the parameters of 5 OBs without WS are randomly initialized. The results show that 5 OBs without WS outperforms Untrained, which demonstrates the effectiveness of the designed training process.\n\n1.2e-4(5e-5) 0.008(0.002) 8.93(0.03) 0.01(0.03) 1e-5(2e-5) 0.01(0.003)\n\n0.08(0.03) 0.15(0.03) 15.43(2.34) 4.43(1.82) 0.06(0.03) 0.29(0.07)\n\n8.16(3.44) 1.47(0.40) 1891(1396) 35.72(8.52) 0.82(0.10) 3.28(1.00)\n\n0.28(0.09) 0.37(0.05) 45.8(16.9) 1.08(0.72) 0.69(0.09) 0.85(0.20)\n\n3 OBs with WS\n\n30 OBs with WS\n\nF4 F5 F6 F7 F8 F9\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: The results of ablation study. d = 10.\n\nf\n\nF4 F5 F6 F7 F8 F9\n\nNot SAC\n\nNot FM\n\nNot RC\n\nNot RSSM\n\nBOptformer\n\n52.7(15.0) 5.08(0.98) 1.25e5(9.26e4) 47.2(5.11) 1.18(0.04) 4.49(1.09)\n\n23.3(6.68) 2.94(0.41) 1.10e4(1.44e4) 19.7(4.27) 1.19(0.07) 3.42(0.78)\n\n50.5(6.81) 3.75(0.09) 5.22e4(1.28e4) 63.4(8.60) 0.87(0.07) 8.36(0.43)\n\n271(346) 3.51(1.49) 6.99e6(1.21e8) 40.6(8.10) 3.28(1.14) 3.56(0.72)\n\n3.03(5.82) 1.02(0.20) 4.03e3(6.94e4) 44.8(8.34) 0.60(0.17) 3.03(0.70)\n\nWe also find an interesting phenomenon: 5 OBs without WS outperforms 3 OBs with WS in all cases. Our untrained deep architecture, 5 OBs without WS, can achieve good results on simple cases, which shows that BOptformer retains the advantages of Transformer architecture and has strong generalization ability. We use the untrained 5 OBs with WS to test on the complex plannar mechanic arm problem and find that it performs poorly.\n\nWe train BOptformer on the F1-F3 function set with different learning rates (lr) and then test them on the F4-F9 function set. The experimental results are shown in Table 9 (Appendix A.6). For 5 OBs without WS, setting lr = 0.01 achieves the relatively best performance. Using lr = 0.0001 would be a good choice for 30 OBs with WS and 3 OBs with WS.\n\n4.4 ABLATION STUDY\n\nThis section considers the performance impact of different parts in BOptformer. We take BOptformer with 3 OBs and weight sharing as an example, which is trained on F1-F3 and tested on F4-F9. We remove SAC, FM, RSSM, and RC in BOptformer, respectively, and denote them as Not SAC, Not FM, Not RSSM, and Not RC. The experimental results are shown in Table 5. When their results were sorted from good to worst, the rank is BOptformer > Not FM > Not RC ≈ Not SAC ≈ Not RSSM. The role of FM is slightly weaker than that of the other three modules. Taken as a whole, the parts of SAC, RSSM, and RC are of equal importance. The absence of these core components can seriously affect the performance of BOptformer. At the same time, it also shows the effectiveness of the proposed four modules. The removal of any one of the modules in the crossover, mutation, and selection of EAs will degrade the performance of EAs. This shows that BOptformer implements a learnable EA framework that does not require human-designed parameters.\n\nFigure 2: Crossover Strategy learned by BOptformer.\n\n4.5 VISUALIZATION ANALYSIS\n\nThe tested model is 5 OBs with WS trained on F1-F3 with d = 100. The population size is 100.\n\nVisual Analysis of SAC The crossover strategies learned by the five SAC are shown in Fig. 2. For the presentation, we select individuals with fitness rankings 1st, 50th, and 100th. The horizontal axis\n\n8\n\n0204060801000.00990.01000.01010.01020.01030.01040.01050.0106Crossover Strategy Learned by OB 1The 1st individual's strategyThe 50th individual's strategyThe 100th individual's strategy0204060801000.01000.01020.01040.0106Crossover Strategy Learned by OB 2The 1st individual's strategyThe 50th individual's strategyThe 100th individual's strategy0204060801000.009950.010000.010050.010100.010150.01020Crossover Strategy Learned by OB 3The 1st individual's strategyThe 50th individual's strategyThe 100th individual's strategy0204060801000.01000.01010.01020.01030.01040.01050.01060.0107Crossover Strategy Learned by OB 4The 1st individual's strategyThe 50th individual's strategyThe 100th individual's strategy0204060801000.0100.0120.0140.0160.0180.0200.022Crossover Strategy Learned by OB 5The 1st individual's strategyThe 50th individual's strategyThe 100th individual's strategyUnder review as a conference paper at ICLR 2023\n\nrepresents the fitness ranking of individuals, and the vertical axis represents the attention (weight when performing crossover) on these individuals. OB1 tends to crossover with lower-ranked individuals, showing a preference for exploration. From OB1 to OB5, the bias of SAC gradually changes from exploration to exploitation.\n\nVisual Analysis of FM We test 5 OBs with WS on F4 with d = 2. The mutation strategies learned by the five OBs are shown in Fig. 3. Input and output represent the input and output populations of the FM module, respectively. OB1 tends to explore a broad solution space, and the next 4 OBs gradually shift from searching the vast space to searching the space near the input population. The strategies learned by FM and SAC modules show a common feature: the preference for generating solutions gradually shifts from exploration to exploitation as the population converges.\n\nFigure 3: Mutation strategy learned by BOptformer.\n\n5 CONCLUSIONS\n\nWe successfully designed the Transformer-based L2O framework for black-box optimization, which does not need hand-designed operators. The better performance than that of EA baselines, Bayesian optimization, and the L2O method demonstrates the effectiveness of BOptformer. Moreover, BOptformer can be well adapted to unseen black-box optimization. Meanwhile, we experimentally demonstrate that the proposed three modules have positive effects. BOptformer still has room for improvement.\n\n1) Our scheme is not limited to black-box optimization. Similar to the LSTM architecture, our scheme can directly optimize differentiable functions. However, the architecture of BOptformer does not directly involve the gradient information of the optimization target, which makes BOptformer inferior to existing L2O schemes. In future work, we will design a new module that embeds the gradient information of the optimization target;\n\n2) In the loss function, we did not effectively consider the diversity of the population, and the population can be regularized in the future;\n\n3) The training set seriously affects the performance of BOptformer. If the similarity between the training set and the optimization objective is low, it will cause the performance of BOptformer to degrade drastically. Building the dataset as relevant to the target as possible is essential.\n\n9\n\nx10050050100y10050050100fitness05000100001500020000OB 1inputoutputx10050050100y10050050100fitness05000100001500020000OB 2inputoutputx10050050100y10050050100fitness05000100001500020000OB 3inputoutputx10050050100y10050050100fitness05000100001500020000OB 4inputoutputx10050050100y10050050100fitness05000100001500020000OB 5inputoutputUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981–3989, 2016.\n\nAnonymous. DECN: Evolution inspired deep convolution network for black-box optimization. In Submitted to The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Ur_qORZ6-9R. under review.\n\nA. Auger and N. Hansen. A restart cma evolution strategy with increasing population size. In 2005\n\nIEEE Congress on Evolutionary Computation, volume 2, pp. 1769–1776, 2005.\n\nMaximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: a framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020.\n\nIrwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. Neural optimizer search with reinforcement learning. In International Conference on Machine Learning, pp. 459–468. PMLR, 2017.\n\nJ. Blank and K. Deb. pymoo: Multi-objective optimization in python. IEEE Access, 8:89497–89509,\n\n2020.\n\nYue Cao and Yang Shen. Bayesian active learning for optimization and uncertainty quantification in\n\nprotein docking. Journal of Chemical Theory and Computation, 16(8):5334–5347, 2020.\n\nYue Cao, Tianlong Chen, Zhangyang Wang, and Yang Shen. Learning to optimize in swarms.\n\nAdvances in Neural Information Processing Systems, 32:15044–15054, 2019.\n\nTianlong Chen, Weiyi Zhang, Zhou Jingyang, Shiyu Chang, Sijia Liu, Lisa Amini, and Zhangyang Wang. Training stronger baselines for learning to optimize. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 7332–7343. Curran Associates, Inc., 2020.\n\nTianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and Wotao Yin. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23:1–59, 2022.\n\nYutian Chen, Matthew W Hoffman, Sergio G ́omez Colmenarejo, Misha Denil, Timothy P Lillicrap, Matt Botvinick, and Nando Freitas. Learning to learn without gradient descent by gradient descent. In International Conference on Machine Learning, pp. 748–756. PMLR, 2017.\n\nAntoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like\n\nanimals. Nature, 521:503–507, 2015.\n\nSwagatam Das and Ponnuthurai Nagaratnam Suganthan. Differential evolution: A survey of the\n\nstate-of-the-art. IEEE transactions on Evolutionary Computation, 15(1):4–31, 2010.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The\n\nJournal of Machine Learning Research, 20(1):1997–2017, 2019.\n\nSebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, and\n\nSatinder Singh. Bootstrapped meta-learning. arXiv preprint arXiv:2109.04504, 2021.\n\nDaniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and David Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1487–1495, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nNikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution\n\nstrategies. Evolutionary Computation, 9(2):159–195, 2001.\n\nFrank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated machine learning: methods, systems,\n\nchallenges. Springer Nature, 2019.\n\nHowook Hwang, Thom Vreven, Jo ̈el Janin, and Zhiping Weng. Protein–protein docking benchmark\n\nversion 4.0. Proteins: Structure, Function, and Bioinformatics, 78(15):3111–3114, 2010.\n\net.al. Jazzbin. geatpy: The genetic and evolutionary algorithm toolbox with high performance in\n\npython, 2020.\n\nKirthevasan Kandasamy, Jeff Schneider, and Barnab ́as P ́oczos. High dimensional bayesian optimisation and bandits via additive models. In International conference on machine learning, pp. 295–304. PMLR, 2015.\n\nKirthevasan Kandasamy, Gautam Dasarathy, Junier B Oliva, Jeff Schneider, and Barnab ́as P ́oczos. Gaussian process bandit optimisation with multi-fidelity evaluations. Advances in neural information processing systems, 29, 2016.\n\nKirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabas Poczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. Journal of Machine Learning Research, 21(81):1–27, 2020.\n\nPascal Kerschke, Holger H. Hoos, Frank Neumann, and Heike Trautmann. Automated algorithm selection: Survey and perspectives. Evolutionary Computation, 27(1):3–45, 2019. doi: 10.1162/ evco a 00242.\n\nShauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nCheng Li, Sunil Gupta, Santu Rana, Vu Nguyen, Svetha Venkatesh, and Alistair Shilton. High\n\ndimensional bayesian optimization using dropout. In IJCAI’17. AAAI Press, 2017.\n\nKe Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.\n\nLuke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-Dickstein. Understanding and correcting pathologies in the training of learned optimizers. In International Conference on Machine Learning, pp. 4556–4565. PMLR, 2019.\n\nMelanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.\n\nIain H Moal and Paul A Bates. Swarmdock and the use of normal modes in protein-protein docking.\n\nInternational Journal of Molecular Sciences, 11(10):3623–3648, 2010.\n\nJean-Baptiste Mouret and Glenn Maguire. Quality diversity for multi-task optimization. Proceedings\n\nof the 2020 Genetic and Evolutionary Computation Conference, 2020.\n\nMojmir Mutny and Andreas Krause. Efficient high dimensional bayesian optimization with additivity and quadrature fourier features. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nBrian G Pierce, Kevin Wiehe, Howook Hwang, Bong-Hyun Kim, Thom Vreven, and Zhiping Weng. Zdock server: interactive docking prediction of protein–protein complexes and symmetric multimers. Bioinformatics, 30(12):1771–1773, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a\n\nscalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.\n\nGraham R Smith and Michael JE Sternberg. Prediction of protein–protein interactions by docking\n\nmethods. Current opinion in structural biology, 12(1):28–35, 2002.\n\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.\n\nHaoran Sun, Xiangyi Chen, Qingjiang Shi, Mingyi Hong, Xiao Fu, and Nicholas D. Sidiropoulos. Learning to optimize: Training deep neural networks for interference management. IEEE Transactions on Signal Processing, 66(20):5438–5453, 2018. doi: 10.1109/TSP.2018.2866382.\n\nYe Tian, Shichen Peng, Xingyi Zhang, Tobias Rodemann, Kay Chen Tan, and Yaochu Jin. A recommender system for metaheuristic algorithms for continuous optimization based on deep recurrent neural networks. IEEE Transactions on Artificial Intelligence, 1(1):5–18, 2020. doi: 10.1109/TAI.2020.3022339.\n\nVassilis Vassiliades and Jean-Baptiste Mouret. Discovering the elite hypervolume by leveraging interspecies correlation. Proceedings of the Genetic and Evolutionary Computation Conference, 2018.\n\nVassilis Vassiliades, Konstantinos Chatzilygeroudis, and Jean-Baptiste Mouret. Using centroidal voronoi tessellations to scale up the multidimensional archive of phenotypic elites algorithm. IEEE Transactions on Evolutionary Computation, 22:623–630, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nPaul Vicol, Luke Metz, and Jascha Sohl-Dickstein. Unbiased gradient estimation in unrolled computation graphs with persistent evolution strategies. In International Conference on Machine Learning, pp. 10553–10563. PMLR, 2021.\n\nChao Wang, Jing Liu, Kai Wu, and Zhaoyang Wu. Solving multi-task optimization problems with adaptive knowledge transfer via anomaly detection. IEEE Transactions on Evolutionary Computation, 2021.\n\nOlga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Nando Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and generalize. In International Conference on Machine Learning, pp. 3751–3760. PMLR, 2017.\n\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J ̈urgen Schmidhuber.\n\nNatural evolution strategies. Journal of Machine Learning Research, 15(1):949–980, 2014.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8(3):229–256, 1992.\n\nKai Wu and Jing Liu. Classification-based optimization with multi-fidelity evaluations. In 2019 IEEE\n\nCongress on Evolutionary Computation (CEC), pp. 1126–1131. IEEE, 2019.\n\nJiangning Zhang, Chao Xu, Jian Li, Wenzhou Chen, Yabiao Wang, Ying Tai, Shuo Chen, Chengjie Wang, Feiyue Huang, and Yong Liu. Analogous to evolutionary algorithm: Designing a unified sequence model. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 26674–26688. Curran Associates, Inc., 2021.\n\nJiangning Zhang, Xiangtai Li, Yabiao Wang, Chengjie Wang, Yibo Yang, Yong Liu, and Dacheng Tao. Eatformer: Improving vision transformer inspired by evolutionary algorithm. arXiv preprint arXiv:2206.09325, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nQingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposition.\n\nIEEE Transactions on Evolutionary Computation, 11(6):712–731, 2007.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 11106–11115, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 VISION TRANSFORMER\n\nWe mainly introduce the core part of Vision Transformer, such as the multi-head self-attention layer (MSA), feed-forward network (FFN), layer normalization (LN), and residual connection (RC).\n\nMSA MSA fuses several SA operations to handle the queries (Q), keys (K), and values (V ) that jointly attend to information from different representation subspaces. MSA is formulated as follows: M ultiHead(Q, K, V ) = Concat(H1, H2, · · · Hh)W O. where Concat means concatenation operation. The head feature Hi can be formulated as:\n\nHi = SA(QW Q i , KW K (cid:16) QW Q\n\n= Sof tmax\n\ni\n\n, V W V\n\ni ) i )T /sqrt(dk)\n\n(cid:17)\n\ni (KW K\n\nV W V\n\ni = AV W V\n\ni\n\ni ∈ Rdm×dq , W K\n\ni ∈ Rdm×dk , and W V\n\nwhere W Q i ∈ Rdm×dv are parameter matrices for queries, keys, and values, respectively; W O ∈ Rhdv×dm maps each head feature Hi to the output. Moreover, dm is the input dimension, while dq, dk, and dv are hidden dimensions of the corresponding projection subspace; h is the head number. A ∈ Rl×l is the attention matrix of hth head, l is the sequence length.\n\nFFN FFN employs two cascaded linear transformations with a ReLU activation to handle X, which is shown as: F F N (X) = max(0, XW1 + b1)W2 + b2, where W1 and W2 are weights of two linear layers, and b1 and b2 are corresponding biases.\n\nLN LN is applied before each layer of MSA and FFN, and the output of LN is calculated by X + [M SA|F F N ](LN (X)).\n\nA.2 PRELIMINARY EAS\n\nThe crossover, mutation, and selection operators form the basic framework of EAs. EA starts with a randomly generated initial population. Then, genetic operations such as crossover and mutation will be carried out. After the fitness evaluation of all individuals in the population, a selection operation is performed to identify fitter individuals to undergo reproduction to generate offspring. Such an evolutionary process will be repeated until specific predefined stopping criteria are satisfied.\n\nCrossover The crossover operator generates a new individual ˆXi by Equation (11), and cr is the probability of the crossover operator.\n\nˆX c\n\ni,k =\n\n(cid:26)Xj,k Xi,k\n\nrand(0, 1) < cr otherwise\n\n(11)\n\nwhere k ∈ [1, · · · , d]. This operator is commonly conducted on n individuals. After an expression expansion, we re-formulate Equation (11) as (cid:80)n i is the diagonal matrix. If W c\n\ni is full of zeros, the ith individual has no contribution.\n\ni (Zhang et al., 2021). W c\n\ni=1 XiW c\n\nMutation The mutation operator brings random changes into the population. Specifically, an individual Xi in the population goes through the mutation operator to form the new individual ˆXi, formulated as follows:\n\nˆX m\n\ni,k =\n\n(cid:26)rand(lk, uk) ˆX c\n\ni,k\n\nrand(0, 1) < mr otherwise\n\n(12)\n\nwhere mr is the probability of mutation operator and k ∈ [1, · · · , d]. Similarly, Equation (12) can be re-formulated as XiW m is the diagonal matrix.\n\n, where W m\n\ni\n\ni\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nSelection We introduce the binary tournament mating selection operator in Equation (13). The selection operator survives individuals of higher quality for the next generation until the number of individuals is chosen.\n\npi =\n\n(cid:26)1 0\n\nf (Xi) < f (Xk) f (Xi) > f (Xk)\n\n, (Xi, Xk) ∈ X,\n\n(13)\n\nwhere pi reflects the probability that Xi is selected for the next generation, and (Xi, Xk) in Equation (13) are randomly selected from the population X ∪ ˆX m.\n\nA.3 SYNTHETIC FUNCTIONS\n\nTable 6: Training functions.\n\nID\n\nFunctions\n\nRange\n\n(cid:80)\n\nF1 F2 F3 (cid:80)\n\ni |wisin(xi − bi)| i |xi − bi|\n\n(cid:80) i |(xi − bi) + (xi+1 − bi+1)| + (cid:80)\n\ni |xi − bi|\n\nx ∈ [−10, 10], b ∈ [−10, 10] x ∈ [−10, 10], b ∈ [−10, 10] x ∈ [−10, 10], b ∈ [−10, 10]\n\nTable 7: Testing Functions.\n\nID\n\nF4(Sphere) F5\n\nF6(Rosenbrock)\n\nF7(Rastrigin)\n\nF8(Griewank)\n\nF9(Ackley)\n\nFunctions (cid:80)\n\ni z2\n\ni , zi = xi − bi\n\nmax{|zi|, 1 ≤ i ≤ D}, zi = xi − bi D−1 (cid:80)\n\n(100(z2\n\ni − zi+1)2 + (zi − 1)2), zi = xi − bi\n\ni=1 D\n(cid:80)\n\n(z2\n\ni − 10 cos(2πzi) + 10), zi = xi − bi\n\nRange\n\nx ∈ [−100, 100], b ∈ [−50, 50] x ∈ [−100, 100], b ∈ [−50, 50]\n\nx ∈ [−100, 100], b ∈ [−50, 50]\n\nx ∈ [−5, 5], b ∈ [−2.5, 2.5]\n\ni=1 D\n(cid:80)\n\ni=1\n\nz2 i\n\n4000 − (cid:81)D\n\n−20 exp(−0.2\n\ni=1 cos( zi√ (cid:113) 1\n\n(cid:80)D\n\ni\n\nD\n\ni=1 z2 i )\n\n) + 1, zi = xi − bi\n\nx ∈ [−600, 600], b ∈ [−300, 300]\n\n−\n\nx ∈ [−32, 32], b ∈ [−16, 16]\n\n(cid:80)D\n\ni=1 cos(2πzi)) + 20 + exp(1), zi =\n\nexp( 1 D\nxi − bi\n\nA.4 PARAMETERS\n\nBOptformer. For example, 30 OBs with WS contains 30 OBs, and each OB consists of 1 SAC, 1 FM, and 1 RSSM. In 30 OBs with WS, these 30 OBs share parameters. 5 OBs without WS has 5 OBs, and no parameters are shared among them. During the training process, BOptformer is iterated for 1000 epochs. The initial learning rate (lr) was set to 0.01 and lr = lr × 0.9 each 100 cycles. The 2-norm of the gradient is clipped so that it is not larger than 10. The bias of the function is regenerated each epoch, and a new batch of random initial populations is generated.\n\nBaselines. The number of generations of the reference algorithms is set to 100. The population size of ES, DE, and CMA-ES is set to 100. For all cases, we choose the optimal hyperparameters. To ensure validity, all experimental results are averaged over 10 runs. All experiments were performed on a Ubuntu20.04 PC with Intel(R) Core I7 (TM) I3-8100 CPU at 3.60GHz and NVIDIA GeForce GTX 1060.\n\nA.5 PLANNER MECHANIC ARM PROBLEM\n\nThe optimization goal of this problem is to search for a set of lengths L = (L1, L2, · · · , Ln) and a set of angles α = (α1, α2, · · · , αn) so that the distance f (L, α, p) from the top of the mechanic arm to the target position p is the smallest, where n represents the number of segments of the mechanic\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\narm, and Li ∈ (li, ui) and αi ∈ (−Π, Π) represent the length and angle of the ith mechanic arm, respectively. Typically, d is calculated as follows:\n\nf (L, α, p) =\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:32) n\n\n(cid:88)\n\ni=1\n\n(cid:33)2\n\ncos(αi)Li − px\n\n+\n\n(cid:32) n\n\n(cid:88)\n\ni=1\n\n(cid:33)2\n\nsin(αi)Li − py\n\n(14)\n\nwhere px and py represent the x-coordinate and y-coordinate of the target point, respectively.\n\nHere, n = 100, li = 0 and ui = 10. We design two groups of experiments. 1) Simple case. We fixed the length of each mechanic arm as li = 10 and only searched for the optimal α. 2) Complex case. We need to search for L and α simultaneously. We randomly selected 600 target points within the range of r ≤ 1000 to form a set S, where r represents the distance from the target point to the origin of the mechanic arm, as shown in Fig. 4. During the training process of BOptformer, a sample point set s is re-extracted from S for training every T training cycle. In the testing process, we extracted 128 target points (Stest) in the range of r ≤ 100, r ≤ 300, and r ≤ 1000, respectively, for testing. The purpose of testing in three different regions is to explore the generalization performance of BOptformer further.\n\nWe evaluate the generalization ability of the algorithm by\n\n(cid:16)(cid:80)Stest\n\ns\n\nf (L, α, s)\n\n/|Stest|.\n\n(cid:17)\n\nFigure 4: Planar Mechanical Arm.\n\nDE, ES, and CMA-ES are tested when the maximum generations M axgen is set to 10, 50, and 100, respectively. We find that BOptformer outperforms all baselines.\n\nA.6 EFFECT OF LEARNING RATE ON BOPTFORMER\n\nWe train BOptformer on the F1-F3 function set with different learning rates, and then test it on the F4-F9 function set. The experimental results are shown in Table 9. 5 OBs without WS and 30 OBs with WS perform poorly when the learning rate is 0.1, which may be because the learning rate is too large, which affects the convergence of BOptformer during the training process. For 5 OBs without WS, setting the learning rate to 0.01 achieves relatively best performance. Using a learning rate of 0.0001 would be a good choice for 30 OBs with WS and 3 OBs with WS. However, our experiments are coarse-grained. The learning rate has a greater impact on BOptformer. Then using Auto-ML to search for the optimal hyperparameter combination of the model is expected to achieve better performance.\n\nA.7 CONVERGENCE OF BOPTFORMER\n\nWe plot the convergence curves of 30 OBs with WS, ES, DE, and CMA-ES on F7. BOptformer converges quickly and can obtain better solutions. BOptformer can only iterate ten times to get the best solution relative to EA baselines. ES and DE converged around 100 generations, and CMA-ES showed a slow convergence rate.\n\n16\n\ndr1L2L3L4L1234Under review as a conference paper at ICLR 2023\n\nTable 8: The results of planar mechanical arm on searching for different angles with the fixed lengths.\n\nMaxgen=10\n\nr\n\n100 300 1000\n\nr\n\n100 300 1000\n\nr\n\n100 300 1000\n\nDE\n\nES\n\nCMA-ES\n\nL2O-Swarm BOptformer\n\n2.96(1.63) 11.3(14.7) 227(226)\n\n11.2(4.70) 45.3(43.3) 257(246)\n\n236(46.8) 243(125) 397(321)\n\n40.4(3.89) 69.5(3.77) 176(7.20)\n\n0.30(0.18) 0.48(0.37) 26.6(57.4)\n\nMaxgen=50\n\nDE\n\nES\n\nCMA-ES\n\nL2O-Swarm BOptformer\n\n1.28(0.60) 1.54(0.89) 110(152)\n\n10.7(5.91) 42.0(41.0) 193(235)\n\n2.42(0.65) 4.06(6.54) 103(182)\n\n40.4(3.89) 69.5(3.77) 176(7.20)\n\n0.30(0.18) 0.48(0.37) 26.6(57.4)\n\nMaxgen=100\n\nDE\n\nES\n\nCMA-ES\n\nL2O-Swarm BOptformer\n\n1.20(0.64) 1.38(0.71) 93.8(137)\n\n10.6(5.58) 44.9(43.3) 183(239)\n\n1.36(0.35) 1.38(0.41) 43.7(110)\n\n40.4(3.89) 69.5(3.77) 176(7.20)\n\n0.30(0.18) 0.48(0.37) 26.6(57.4)\n\nTable 9: Ablation study on learning rate. 5 OBs without WS\n\nlr\n\n0.1 0.01 0.001 0.0001\n\nF4\n\n0.93(7.42) 0.01(0.003) 0.88(3.41) 0.06(0.03)\n\nF5\n\n0.31(0.49) 0.05(0.02) 0.36(0.12) 0.13(0.03)\n\nF6\n\nF7\n\nF8\n\nF9\n\n2.04e7(2.03e8) 9.57(0.22) 226(1750) 13.6(2.11)\n\n30 OBs with WS\n\n15.3(6.4) 1.62(0.60) 6.18(2.66) 0.83(0.50)\n\n0.36(0.16) 0.03(0.01) 0.56(0.16) 0.17(0.04)\n\n0.61(0.18) 0.06(0.03) 1.36(0.36) 0.28(0.10)\n\n0.1 0.01 0.001 0.0001\n\n1.64(1.19) 0.05(0.30) 1.01e-3(0.001) 1.50e-3(0.001)\n\n0.85(1.59) 0.09(0.03) 0.02(0.01) 0.016(0.004)\n\n493(3110) 39.2(240) 9.03(0.18) 9.01(0.13)\n\n28.4(4.35) 1.05(1.40) 0.03(0.02) 0.02(0.02)\n\n0.47(0.11) 0.01(0.06) 0.003(0.001) 0.006(0.002)\n\n2.82(0.5) 0.28(0.07) 0.03(0.01) 0.02(0.0.01)\n\n3 OBs with weights sharing\n\n0.1 0.01 0.001 0.0001\n\n3.04(5.55) 29.9(47.7) 1.82(1.20) 0.39(0.21)\n\n0.98(0.4) 2.68(1.30) 0.76(0.32) 0.33(0.07)\n\n1150(7930) 6.24e4(4.87e5) 654(4780) 46.8(79.4)\n\n43.3(8.87) 40.4(8.52) 7.00(7.11) 2.22(2.41)\n\n0.64(0.12) 1.05(0.06) 0.79(0.13) 0.66(0.09)\n\n2.43(0.98) 4.45(0.85) 1.91(0.53) 0.59(0.19)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Convergence curves of Boptformer and EA baselines. The figure shows the convergence curve of these algorithms on F7 in Table 7.\n\nA.8 THE DETAILS OF PROTEIN DOCKING\n\nProtein Docking We also handle the problem of Ab initio protein docking (Cao & Shen, 2020), which optimizes a noisy and costly function in a high-dimensional conformational space. Mathematically, this problem is formulated as optimizing the Gibbs binding free energy f (x) for conformation x. We calculate the energy function in a CHARMM 19 force field as in (Moal & Bates, 2010) and shift it so that f (x) = 0 at the origin of the search space. f (x) is differentiable when we parameterize the search space as R12 (Smith & Sternberg, 2002). Here, only 100 interface atoms are considered.\n\nTraining dataset. 25 protein-protein complexes (see Appendix A.8) from the protein docking benchmark set 4.0 (Hwang et al., 2010), each of which has 5 starting points (top-5 models from ZDOCK (Pierce et al., 2014)).\n\nTesting dataset. Three complexes (with one starting model each) of different levels of docking difficulty are selected, including 1ATN 7, 2JEL 1, and 7CEI 1.\n\n25 Protein-protein Complexes The training dataset contains 25 protein-protein complexes from the protein docking benchmark set 4.0 (Hwang et al., 2010). The detailed information is shown as follows: 1ATN, 1AVX, 1AY7, 1BJ1, 1BVN, 1CGI, 1DFJ, 1EAW, 1EWY, 1EZU, 1GRN, 1IBR, 1IJK, 1IQD, 1JPS, 1KXQ, 1M10, 1MAH, 1N8O, 1PPE, 1R0R, 1XQS, 2B42, 2C0L, and 2HRK.\n\n18\n\n025507510012515017520002004006008001000120014001600BOptformer(ws30)ESDECMA-ES",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors proposed an interesting approach based on what I can tell to be a novel connection between vision transformers and evolutionary algorithms. The paper then modifies the transformer architecture to implement evolutionary-specific operations like crossover, mutation and selection. The authors then demonstrate their method on six black-box functions and show superior performance to baselines.\n\n# Strength And Weaknesses\n\nStrengths: \n- Interesting approach with potential impact on black-box optimisation \n- Favourable results in application domains considered \n\nWeaknesses: \n\n- Paper writing: I have found the paper hard to follow and not clearly written. Of course, I feel with the authors who had to introduce many related concepts before proposing their method. Maybe the authors could consider moving their contributions before page 4 to make it easier to grasp the novelty of the paper. \n\n- There does not seem to be regard to sample efficiency in the proposed approach. Are the authors not worried about expensive of evaluating black-box functions? I ask since the domains considered in the experiments are relatively toy and running the experiments in a real-world problem beyond Ab initio docking. Looking at the literature on high-dimensional Bayesian optimisation (which needs to be cited) can help in designing such experiments.  \n\n- I am also a bit confused about the experimental results, which seem to somewhat contradict the notion of black-box optimisation. To elaborate, it seems that the training set's similarity to the black box is a critical factor in the approach's success. If that is the case, how can we define this notion? If the target task (so to say) is a black box then what does it mean for us to be able to pick similar training data? What does similarity mean in this context? \n\n- Why haven't the authors compared to state-of-the-art bayesian optimisation solutions in their experiments? As far as I can tell, the dimensions vary from 10 to 100 d which SOTA BO should be able to handle. \n\n- What motivated the choices of the training and testing functions? After reading the paper, they seemed to be arbitrary. Maybe the authors can help me understand?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease see above.\n\n# Summary Of The Review\n\nIn general, I think this is an interesting approach. In its current state, however, I can not recommend acceptance. I am of course willing to change my score if the authors convince me in the rebuttal phase.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSEQUENTIAL LATENT VARIABLE MODELS FOR FEW-SHOT HIGH-DIMENSIONAL TIME-SERIES FORECASTING\n\nXiajun Jiang∗, Ryan Missel∗, Zhiyuan Li & Linwei Wang Golisano College of Computing and Information Sciences Rochester Institute of Technology Rochester, NY 14623, USA {xj7056,rxm7244,zl7904,Linwei.Wang}@rit.edu\n\nABSTRACT\n\nModern applications increasingly require learning and forecasting latent dynamics from high-dimensional time-series. Compared to univariate time-series forecasting, this adds a new challenge of reasoning about the latent dynamics of an unobserved abstract state. Sequential latent variable models (SLVMs) present an attractive solution, although existing works either struggle with long-term forecasting or have difficulty learning across diverse dynamics. In this paper, we first present a conceptual framework of SLVMs to unify existing works, contrast their fundamental limitations, and identify an intuitive solution to long-term forecasting for diverse dynamics via meta-learning. We then present a few-shot forecasting framework for high-dimensional time-series: instead of learning a single dynamic function, we leverage data of diverse dynamics and learn to adapt latent dynamic functions to few-shot support series. This is realized via Bayesian meta-learning underpinned by: 1) a latent dynamic function conditioned on knowledge derived from few-shot support series, and 2) a meta-model that learns to extract such dynamic-specific knowledge via feed-forward embedding of support set. We compared the presented framework with a comprehensive set of baseline models 1) trained globally on the large meta-training set with diverse dynamics, 2) trained individually on single dynamics with and without fine-tuning to k-shot support series, and 3) extended to few-shot meta-formulations. We demonstrated that the presented framework is agnostic to the latent dynamic function of choice and, at meta-test time, is able to forecast for new dynamics given variable-shot of support series.1\n\n1\n\nINTRODUCTION\n\nIn many applications, an ultimate goal is to forecast the future states or trajectories of a dynamic system from its high-dimensional observations such as series of images. Compared to the relatively well-studied univariate time-series forecasting (Makridakis et al., 2018; Oreshkina et al., 2020; Salinas et al., 2020), high-dimensional time-series forecasting raises new challenges: it requires the extraction of the dynamics of an abstract latent state that is not directly observed (Botev et al., 2021).\n\nSequential latent variable models (SLVMs) provide an attractive solution that, unlike autoregressive models, abstracts a latent dynamic function zi = f (z<i; θz) with state zi and parameter θz, along with zi’s emission to observations xi = g(zi) (Chung et al., 2015). This pair of learned models can support long-term forecasting given only initial frames of observations, as well as controlled generation of new dynamics. Critical bottlenecks however remain in reaching these goals.\n\nThe earlier formulation of SLVMs relies on a natural extension of the static LVMs: as illustrated in Fig. 1A, the latent state zi is modeled as the latent variable for the generation of xi, and a sequential encoder is used to facilitate the inference of zi from current and past observations x≤i (Chung et al., 2015; Krishnan et al., 2017). Recent works have argued to instead model and infer the parameter\n\n∗Both authors contributed equally to this work 1Source code available at https://github.com/john-x-jiang/meta_ssm.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Sequential latent-variable models for forecasting high-dimensional sequences.\n\nof the latent dynamic function, often modeled as time-varying linear coefficients θz,i (Karl et al., 2017; Fraccaro et al., 2017; Rangapuram et al., 2018; Klushyn et al., 2021). This results in an LVM formulation as illustrated in Fig. 1B1, where the latent variable θz,i is inferred at each i from observations x≤i. While strong at time-series reconstructions and classifications, a fundamental limitation makes these SLVMs less suited for long-term forecasting: the latent dynamic function has a limited ability to forecast without near-term observations to support the inference of zi or θz,i.\n\nThis limitation in the mainstream SLVMs raises a natural question: are we able to relax the assumption of linear dynamic function and directly infer its θz? Works adopting this idea have emerged: as illustrated in Fig. 1B2, by modeling a single θz – either deterministic (Rubanova et al., 2019; Botev et al., 2021) or stochastic (Yildiz et al., 2020) – f (z<i; θz) can be asked to predict a time sequence using only an inferred initial state. This formulation has shown strong long-term forecasting, although with its own fundamental limitation: it learns a single dynamic function global to all training sequences. This would not only require all training data to share identical latent dynamics, but also has difficulty to forecast test sequences with dynamics different from or unknown to the training.\n\nIn this paper, we answer this important open question of long-term forecasting for diverse dynamics. We first present a conceptual framework of SLVMs to unify existing works, and identify an intuitive solution to the underlying critical gap via meta-learning: instead of learning a single dynamic function, we can learn to pull knowledge across datasets of different dynamics and learn to adapt a dynamic function to few-shot high-dimensional time-series. We then present a Bayesian meta-learning framework as illustrated in Fig. 1C: instead of being a single fixed function as in Fig. 1B2, we let the latent dynamic function be conditioned on knowledge derived from few-shot support time-series via a feed-forward set-embedding meta-model; given k-shot time-series of a specific dynamics, the model is asked to forecast for query time-series using only the initial frames, meta-learned across dynamics. We develop this framework to be agnostic to the latent dynamic functions of choice, and with the flexibility to forecast with a variable size of k.\n\nWe evaluated the presented framework in benchmark image sequences with mixed physics including bouncing balls (Fraccaro et al., 2017), pendulum (Botev et al., 2021), and mass-spring (Botev et al., 2021). We further applied it to forecasting complex physics of turbulence flow (Wang et al., 2021) and electrical dynamics over 3D geometrical meshes of the heart. We compared the presented work with SLVMs representative of each of the formulations in Fig. 1A-B, along with a recent autoregressive model designed to forecast for diverse dynamics (Don`a et al., 2020). Each baseline model was trained on 1) the large meta-training set with diverse dynamics, and 2) each dynamics individually, both with and without fine-tuning to k-shot support data. Representative SLVMs were further tested in their feed-forward or optimization-based meta-extensions. Results demonstrated clear margins of improvements by the presented work in forecasting diverse dynamics, with added ability to recognize clusters of distinct dynamics and allow controlled time-series generation given only initial conditions.\n\n2 RELATED WORKS & BACKGROUND\n\nSequential LVMs: Among the first SLVMs is the variational recurrent neural networks (VRNN) (Chung et al., 2015), followed by a series of deep state-space models (SSMs) (Krishnan et al., 2017;\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nMaddison et al., 2017; Li et al., 2019) focused on modeling the dependence of the posterior and transitional density of the latent state zk on past latent states z<k and observations x<k (Fig. 1A) – resembling the deep extensions of the classic Kalman filter (Krishnan et al., 2017) and particle filter (Maddison et al., 2017). An alternative line of deep SSMs aims to infer the parameters of the latent dynamic function instead (Karl et al., 2017; Fraccaro et al., 2017; Rangapuram et al., 2018; Klushyn et al., 2021). Existing approaches along this line assumed linear latent dynamics, where the linear transition matrix at each time frame k is modeled as a linear combination of a set of global matrices. The linear coefficients are modeled to be time-varying and inferred from observations x≤k as illustrated in Fig. 1B1. In both formulations, the latent dynamic function’s reliance on inferred time-varying variables reduces its ability to forecast without near-term observations.\n\nIn parallel, a set of models (Fig. 1B2) have been presented that aims to learn a latent dynamic function that forecasts a sequence using only an inferred initial state, in stochastic (Rubanova et al., 2019; Yildiz et al., 2020) or deterministic forms (Botev et al., 2021). The resulting latent dynamic function is strong at forecasting, albeit only a single function is learned at a time. We build on and advance this formulation of learning to learn a dynamic-specific function from few-shot observations.\n\nModeling switching dynamics in SLVMs, often based on the formulation in Fig. 1A, shares the presented idea of using context variables to control the latent dynamics (Becker-Ehmck et al., 2019; Linderman et al., 2017). They however are concerned with the switching of dynamics within a time-series, whereas we are interested in learning to learn dynamics from k-shot support series.\n\nSequential neural processes (SNPs), based on SLVM formulation in Fig. 1A (Singh et al., 2019; Qin et al., 2019), are underlined by Bayesian meta-learning similar to the presented work. They are originally designed for supervised learning of a regression function over time instead of forecasting. In this work, we will extend SNP to realize a meta-version of the SLVM formulation in Fig. 1A, as a counterpart to be compared with the presented meta-SLVM in Fig. 1C.\n\nAutoregressive dynamics: Autoregressive models are also popular for modeling and forecasting dynamics, especially for approximating physics-based simulations (Wang et al., 2020; Pfaff et al., 2020). Some recent works have focused on generalizing across dynamics by, for instance, disentangling spatial and temporal modeling (Don`a et al., 2020) or learning dynamic-specific functions in addition to a global dynamic function (Yin et al., 2021). A recent autoregressive model considered ”meta-learning” dynamics by using task-embedding to condition the forecaster (Wang et al., 2021), although this task encoder is trained separately from the forecasting model via weak supervision and it infers the task from the observed frames of a forecasting series. Moreover, autoregressive models cannot support controlled generation of time-series as we will demonstrate in Section 5.\n\nGeneral few-shot learning: Few-shot learning has seen substantial progress with static data, including weight initialization (Finn et al., 2017; Yoon et al., 2018), model optimizers (Ravi & Larochelle, 2016), and feed-forward models to condition (Garnelo et al., 2018) or parameterize the primary networks (Bertinetto et al., 2016; Sung et al., 2018). Among these, feed-forward meta-models replace test-time optimization with simple feed-forward passes using support data. It also has an interesting high-level relation to Exemplar VAE (Norouzi et al., 2020) where the few-shot support samples can be viewed as the exemplar. It thus constitutes the basis of the presented few-shot forecasting methods.\n\nFew-shot time-series forecasting: Meta-learning is well studied in univariate time-series forecasting (Montero-Manso et al., 2020) including recent deep-learning advances (Oreshkin et al., 2021). Fewshot forecasting for high-dimensional time-series, however, has not been attempted to our knowledge.\n\n3 UNIFYING CONCEPTUAL FRAMEWORK FOR LEARNING LATENT DYNAMICS\n\nWe first describe an LVM framework that unifies existing works under two choices of probabilistic graphical models (PGMs). It includes a dynamic function of latent zk and its emission to data xk: zk = f (z<k; θz), xk = g(zk), where θz represents the parameter of the latent dynamic function.\n\nSystem states as latent variables: One natural choice of the latent variable is the latent state zk underlying the observations xk. This gives rise to the PGM as illustrated in Fig. 1A, where the marginal likelihood of an observed sequence x0:T can be expressed as:\n\np(x0:T ) =\n\n(cid:90)\n\nz0:T\n\np(x0|z0)p(z0)\n\n(cid:89)T\n\ni=1\n\np(xi|zi)p(zi|z<i, x<i)dz0:T ,\n\n(1)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nwhere p(xi|zi) describes emission and p(zi|z<i, x<i) describes latent dynamics. To facilitate inference, a variational approximation of the posterior density q(z0:T |x0:T ) is often modeled as q(z0:T |x0:T ) = (cid:81)T\n\ni=1 q(zi|z<i, x≤i). The evidence lower bound (ELBO) of Equation (1) is: (cid:88)T\n\nEq(zi|z<i,x≤i) [log p(xi|zi)] − KL(q(zi|z<i, x≤i)||p(zi|z<i, x<i)).\n\nlog p(x0:T ) ≥\n\n(2)\n\ni=0\n\nExisting works adopting this PGM (Chung et al., 2015; Krishnan et al., 2017; Li et al., 2019) differ primarily in how p(zi|z<i, x<i) and q(zi|z<i, x≤i) are modeled. The first term above encourages reconstruction using the inferred q(zi|z<i, x≤i) at each time frame i; this weakens the latent dynamic function underlying p(zi|z<i, x<i) that is constrained only by the KL-divergence term. This leads to limited ability to forecast without near-term x≤i to support the inference of q(zi|z<i, x≤i).\n\nSystem parameters as latent variables: An alternative choice of the latent variable is the parameters themselves of the LVM equation, especially θz of the latent dynamic function. This gives rise to the PGM in Fig. 1B, where the marginal likelihood of x0:T can now be expressed as:\n\np(x0:T ) =\n\np(x0|z0)p(z0)dz0\n\np(xi|zi)|zi=f (z<i;θz)p(θz)dθz,\n\n(3)\n\n(cid:90)\n\nz0\n\n(cid:90)\n\n(cid:89)T\n\nθz\n\ni=1\n\nwhere the observations are explained by an initial latent state z0 and parameter θz of the latent dynamic function. With a variational approximation of the posterior density q(θz, z0) and an assumption of their prior densities p(z0) and p(θz), the ELBO of Equation (3) becomes:\n\n(4)\n\nlog p(x0:T ) ≥ Eq(θz,z0) [log p(x0:T |z0, θz)] − KL(q(z0)||p(z0)) − KL(q(θz)||p(θz)), This covers different lines of existing works depending on how q(θz) and p(θz) are modeled. In a series of works (Karl et al., 2017; Fraccaro et al., 2017; Rangapuram et al., 2018; Klushyn et al., 2021), θz is modeled as time-varying system parameters θz,0:T . This involves intricate temporal modeling of q(θz,i|x≤i) and p(θz,i|z≤i) over time as illustrated in Fig. 1B1. Because the latent dynamic function relies on time-varying θz,0:T , its forecasting again relies on near-term observations to support the inference of θz,i. Alternatively, q(θz) can be simply assumed to be global across observations and the dynamic function becomes a Bayesian neural network as presented by (Yildiz et al., 2020). As a more special case, θz can be deterministic which leads to the latent ODE model presented by (Rubanova et al., 2019). If we further assume z0 to be deterministic, we arrive at the set of deterministic encoding-decoding network with latent dynamic functions examined by (Botev et al., 2021). This set of formulations, as summarized in Fig. 1B2, shares the advantage of strong long-term forecasting, albeit a fundamental limitation in learning a single dynamic function at a time.\n\nIn Section 5, we will include representative models from each PGM to provide empirical evidence for the identified limitations. With this basis, we derive an intuitive solution to the identified critical gaps by extending the PGM in Fig. 1B2 to the presented PGM in Fig. 1C: instead of learning a single dynamic function, we will learn to adapt a latent dynamic function to few-shot support time-series.\n\n4 FEW-SHOT FORECASTING VIA BAYESIAN META-LEARNING\n\n0:T , xs,2\n\nj=1. For each Dj, we consider disjoint few-shot support series Ds\n\nConsider a dataset D of high-dimensional time-series with M similar but distinct underlying dynamics: D = {Dj}M j = j = {xq,1 0:T , ..., xs,k {xs,1 Instead of maximizing the marginal likelihood of x0:T for all x0:T ∈ D as in Equation (3), we formulate a meta-objective to learn to maximize the marginal likelihood of xq 0:T ∈ Dq when conditioned on support series Ds j , for all dynamics j ∈ {1, 2, ..., M }: (cid:90)\n\n0:T } and query series Dq\n\n0:T for all query series xq\n\n0:T } where k ≪ l.\n\n0:T , ..., xq,l\n\n0:T , xq,2\n\nj\n\np(xq\n\n0:T |Ds\n\nj ) =\n\np(xq\n\n0:T |c)p(c|Ds\n\nj )dc, xq\n\n0:T ∈ Dq\n\nj\n\n(5)\n\nc\n\nwhere p(xq derived from support series of a specific dynamics. p(c|Ds extract such dynamic-specific knowledge from few-shot support set Ds j .\n\n0:T |c), though similar to Equation (3), is now conditioned on (thus adapted to) knowledge j ) is the meta-model describing how to\n\nSet-conditioned latent dynamic functions: We model p(xq\n\n0:T |c) based on Equation (3) as:\n\npθx (xi|zi)|zi=f (zi−1,c;θz),\n\n(6)\n\np(xq\n\n0:T |c) =\n\n(cid:90)\n\nz0\n\npθx (x0|z0)p(z0)dz0\n\n(cid:89)T\n\ni=1\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nwhere the latent dynamic function is parameterized by θz but conditioned on embedding c from the support set. To focus on c, we assume θz to be deterministic and global as in (Rubanova et al., 2019; Botev et al., 2021). As an example, we can describe zi = ̃zi−1 + ∆zi where ∆zi conditions gated recurrent units (GRUs) (Chung et al., 2014) on c, as detailed in Appendix B. This conditioning can be generalized to other functional forms of f (·), which we will demonstrate in experiments.\n\n0:T ∈ Ds\n\nMeta-model for amortized variational inference: We model pζ(c|Ds j ) with a meta-model parameterized by ζ in the form of feed-forward embedding of support set Ds j . Specifically, each support sequence xs 0:T ) with blocks of interlaced spatial convolution and temporal compression layers. To extract knowledge shared by the set, the embedding from all sequences in Ds 0:T ), where k is the size of the support set. The value of k can be fixed or variable in our framework. This set embedding parameterizes pζ(c|Ds\n\nj is first encoded through a neural function hφ(xs\n\nj is aggregated by an averaging function: 1\n\nj ) ∼ N (μc, σ2\n\nhφ(xs\n\n0:T ∈Ds\n\n(cid:80)\n\nxs\n\nk\n\nj\n\nTo enable inference, we approximate the posterior density p(c|Ds the same meta set-embedding model by augmenting Ds all dynamics D = {Dj}M\n\nj=1 can then be derived as:\n\nj with xq\n\nc ) via separate linear layers. j , xq\n\n0:T ) as qζ(c|Ds\n\n0:T ), sharing 0:T . The ELBO of Equation (5) across\n\nj ∪ xq\n\n(cid:88)M\n\n(cid:88)\n\nj=1\n\nxq\n\n0:T ∈Dq\n\nj\n\nlog p(xq\n\n0:T |Ds j )\n\n≥ (cid:80)M\n\nj=1\n\n(cid:80)\n\nxq\n\n0:T ∈Dq\n\nj\n\nEqφ(zq\n\n0),qζ (c|Ds\n\nj ∪xq\n\n0:T )[log pθx(xq\n\n0:T |zq\n\n0, c)] (7)\n\n−KL(qφ(zq\n\n0|xq\n\n0:lz0\n\n)||p(z0)) − KL (cid:0)qζ(c|Ds\n\nj ∪ xq\n\n0:T )||pζ(c|Ds\n\nj )(cid:1) ,\n\n0|xq\n\nwhere qφ(zq ) is parameterized by an encoder with lz0 = 2 in all experiments. p(z0) is assumed to be N (0, I). The likelihood term is estimated with reparameterization trick (Kingma & Welling, 2013), and the KL-divergence terms are calculated analytically.\n\n) ∼ N (μz0, σ2\n\n0:lz0\n\nz0\n\nThe optimization of Equation (7) is realized via episodic training where, in each training episode, data in each dynamic set Dj is divided into disjoint support set Ds j . For each query series across all dynamics, starting with an initial latent state z0 (inferred from lz0 frames) and k-shot support embedding c, the latent dynamic function is asked to propagate forward to forecast the entire sequence of z0:T and their corresponding high-dimensional observations x0:T .\n\nj and query set Dq\n\n5 EXPERIMENTS ON BENCHMARK IMAGE SEQUENCES\n\nData: We first considered benchmark images generated with controllable physics, including bouncing ball Fraccaro et al. (2017), Hamiltonian pendulum (Botev et al., 2021), and Hamiltonian mass-spring systems (Botev et al., 2021). Details of data generation are available in Appendix G. To intentionally create data with diverse dynamics, we included 1) a bouncing ball dataset with 16 different directions of gravity, each with 3000 samples simulated using a combination of different initial positions and velocities (gravity-16); and 2) a mixed-physics dataset consisting of bouncing balls under 4 gravity directions, and pendulums and mass springs each with four different values of friction coefficients of 0, 0.05, 0.1, 0.15 (mixed-physics). Each physics with a unique parameter includes 3000 samples.\n\nModels: We considered baseline models representative of each formulation outlined in Fig. 1. This includes VRNN Chung et al. (2015) and DKF Krishnan et al. (2017) representing Fig. 1A1, DVBF Karl et al. (2017) and KVAE Fraccaro et al. (2017) representing Fig. 1B1, and three models representing Fig. 1B2 with latent dynamic functions as residual GRUs (GRU-res), neural ordinary differential equation (NODE), and residual Recurrent Generative Networks (RGN-res) (Botev et al., 2021). We also considered a recent autoregressive model designed to tackle forecasting diverse dynamics (Don`a et al., 2020). All baseline models were 1) trained using the entire meta-training data consisting of mixed dynamics, 2) trained in 1) and further fine-tuned to the meta-test k-shot support set (k = 15) (except for (Don`a et al., 2020) as we were uncertain about a proper approach of fine-tuning due to its specialized architecture), and 3) trained individually for each single dynamics, with and without fine-tuning to the meta-test k-shot support set (k = 15).\n\nFor each of the global latent dynamic models (GRU-res, NODE, and RGN-res), we extended it into our few-shot framework. While few-shot learning with the rest of the SLVMs is not yet reported in literature, we further selected DKF as a representative of the SLVM in Fig. 1A and extended it into a feed-forward meta-formulation via a variant of the SNP (meta-DKF). We also attempted\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of the presented meta-models with all baselines trained on the meta-training set for gravity-16 data. The improvement of meta-GRU-res (best-performing) over its closest baseline is statistically significant in all metrics (p < 0.01, paired t-test).\n\nPGM type\n\nModel\n\nMSE↓\n\nVPT-MSE↑ Dist↓\n\nVPT-Dist↑\n\nFig. 1C\n\nFig. 1B2\n\nFig. 1B1\n\nFig. 1A\n\nmeta-GRU-res meta-NODE meta-RGN-res\n\nGRU-res GRU-res finetune NODE NODE finetune RGN-res RGN-res finetune\n\nDVBF DVBF finetune KVAE\n\nmeta-DKF DKF DKF finetune VRNN VRNN finetune\n\n1.44(0.34)e-2 1.60(0.26)e-2 1.59(0.24)e-2\n\n1.63(0.21)e-2 1.65(0.24)e-2 1.69(0.18)e-2 1.70(0.19)e-2 1.70(0.17)e-2 1.72(0.19)e-2\n\n2.32(14.4)e-2 2.33(13.4)e-2 3.37(1.36)e-2\n\n3.80(0.59)e-2 3.84(0.59)e-2 3.85(0.58)e-2 1.78(10.9)e-2 2.15(12.2)e-2\n\n0.68(0.26) 0.58(0.22) 0.56(0.21)\n\n0.50(0.17) 0.50(0.18) 0.48(0.16) 0.48(0.17) 0.47(0.16) 0.47(0.17)\n\n0.02(0.10) 0.02(0.10) 0.24(0.19)\n\n0.10(0.11) 0.10(0.11) 0.10(0.11) 0.24(0.11) 0.21(0.16)\n\n2.88(1.45) 6.10(2.63) 6.97(3.08)\n\n10.4(3.30) 9.35(3.33) 10.9(3.32) 10.4(3.23) 11.2(3.39) 10.0(3.36)\n\n45.3(0.00) 45.3(0.00) 4.81(3.61)\n\n7.35(3.26) 7.39(3.21) 7.51(3.26) 23.1(21.6) 8.31(11.6)\n\n0.97(0.07) 0.80(0.12) 0.76(0.13)\n\n0.61(0.09) 0.66(0.12) 0.59(0.08) 0.61(0.09) 0.58(0.09) 0.62(0.11)\n\n0.00(0.00) 0.00(0.00) 0.57(0.29)\n\n0.70(0.25) 0.69(0.25) 0.69(0.25) 0.51(0.07) 0.75(0.19)\n\nAutoregressive Don`a et al\n\n3.52(0.26)e-2\n\n0.001(0.01)\n\n13.7(3.05)\n\n0.06(0.15)\n\noptimization-based meta-learning of MAML (Finn et al., 2017) to the DKF and GRU-res models, although challenges of stability and convergence as noted in literature (Mehta et al., 2021; Antoniou et al., 2018) were encountered, suggesting that MAML extensions to SLVMs may not be trivial due to issues such as vanishing gradient issues over the complex computation graph.\n\nAll GRE-res, NODE, and RGN-res based models were trained to forecast for a sequence of 20 frames using only the first 3 frames. We investigated k-shot forecasting when k is fixed at different values of k = 1, 5, 10, 15, or allowed to be variable at both meta-training and -test with 15 as the upper limit. For VRNN, DKF, DVBF, and KVAE, we used their public implementations for training and evaluation. Similar network components with the meta-models were scaled to have comparable parameter scales. Because of their reliance on observed time frames to support prediction, 8 observed frames were exposed to the encoder to reconstruct the 8 frames and forecast the additional 12 frames.\n\nMetrics: We considered four quantitative metrics on meta-test series. We included the commonly used mean squared error (MSE) of forecasted images, and the recently-proposed metric of Valid Prediction Time (VPT) that measures how long the predicted object’s trajectory remains close to the ground truth trajectory based on the MSE (VPT-MSE) (Botev et al., 2021). Because pixel-level MSE does not necessary well capture the quality of the predicted dynamics due to the small object size on the image, we further introduced two new metrics: distance (Dist) between the ground-truth and predicted location of the moving object; and VPT determined based on this distance error (VPT-Dist).\n\nComparison with baseline models trained on full dynamics: For gravity-16 data, we used 10 gravity in meta-training, 2 in meta-validation, and 4 in meta-testing. Table 1 summarizes the quantitative test performance of the three k-shot meta-models obtained with k = 15, in comparison to each of the baseline models trained from the full meta-training set. We include complete results across all models in Appendix D with Table 4. Visual examples for these quantitative results are in Appendix D with Fig. 7 (shaded blue): all the baseline models, including their fine-tuned and meta-versions, struggled with limited forecasting ability, especially evidenced by the error in predicting the movement of the ball over time (Dist and VPT-Dist). For DKF/VRNN/KVAE and meta-DKF, there were strong reconstruction and near-term forecasting from partially observed frames (marked by red vertical lines), but incorrect forecasting further away from the observed frames. GRU-res/NODE/RGN-res and their fine-tuned versions exhibited difficulty to describe mixed gravity.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: A: Comparison with baselines trained on mixed-physics. B: Forecasting examples.\n\nTable 2: Comparison with baselines trained on single dynamics in meta-training data on gravity-16.\n\nModel\n\nDynamics MSE↓\n\nVPT-MSE↑ Dist↓\n\nVPT-Dist↑\n\nmeta-GRU-res\n\nGRU-res\n\nGRU-res finetune\n\nmeta-DKF\n\nDKF\n\nDKF finetune\n\nKVAE\n\nDon`a et al\n\nknown unknown\n\nknown unknown unknown\n\nknown unknown\n\nknown unknown unknown\n\nknown unknown\n\nknown unknown\n\n1.43(0.34)e-2 1.45(0.33)e-2\n\n1.80(0.29)e-2 1.99(0.277)e-2 2.03(0.27)e-2\n\n3.81(0.59)e-2 3.80(0.59)e-2\n\n3.74(0.55)e-2 3.79(0.52)e-2 3.82(0.52)e-2\n\n3.42(1.30)e-2 3.46(1.36)e-2\n\n3.58(0.33)e-2 3.56(0.34)e-2\n\n0.68(0.26) 0.67(0.25)\n\n0.46(0.23) 0.37(0.18) 0.35(0.17)\n\n0.10(0.11) 0.10(0.11)\n\n0.10(0.11) 0.09(0.10) 0.09(0.10)\n\n0.39(0.34) 0.22(0.19)\n\n0.00(0.01) 0.00(0.01)\n\n2.86(1.44) 2.96(1.49)\n\n0.97(0.06) 0.97(0.07)\n\n6.86(3.95) 8.07(3.56) 8.51(3.42)\n\n7.37(3.27) 7.30(3.21)\n\n8.37(3.79) 8.72(3.75) 8.77(3.77)\n\n5.05(3.57) 5.17(3.91)\n\n13.7(3.36) 14.1(3.84)\n\n0.77(0.19) 0.69(0.17) 0.66(0.18)\n\n0.70(0.25) 0.70(0.25)\n\n0.63(0.28) 0.60(0.27) 0.59(0.27)\n\n0.5(0.34) 0.53(0.28)\n\n0.07(0.18) 0.08(0.19)\n\nFor mixed-physics data, for each of the three physics, we included three dynamic settings in metatraining and left out one in meta-testing. Fig. 2A summarized the test results of the presented meta-GRU-res (with variable k) with representative baseline models. Visual examples are shown in Fig. 2B. As shown, meta-DKF, DKF, and DVBF again demonstrated limited ability for long-term forecasting across all physics. KVAE, VRNN, and the finetuned global latent GRU-res were more successful with the mass spring and pendulum systems with relatively simpler dynamics, yet they struggled with the gravity system. The presented meta-GRU-res model consistently outperformed all the baselines across all dynamics, with a larger gain in more complex dynamics.\n\nComparison with baseline models trained on single dynamics: Table 2 summarizes the performance of representative baseline models when trained on a single gravity on gravity-16 data in comparison to meta-GRU. As shown, in both test dynamics known and unknown to the training, the meta-models outperformed the single-dynamic baselines, suggesting the added benefits of learning across dynamics. This margin of improvements remained even when the single-dynamics baselines were fine-tuned to the k-shot support series of unknown test dynamics. Visual examples of these baselines are also shown in Appendix D with Fig. 7 (orange shade).\n\nAblation study: Table 3 summarized the effect of k on k-shot forecasting using the meta-GRU-res model. As expected, model performance improved as the size of k increased. Even with k = 5, however, the performance was significantly better than all the base models summarized in Table 1.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Performance metrics of meta-GRU-res models with fixed vs. variable k values\n\nK Mode\n\nMSE↓\n\nVPT-MSE↑ Dist↓\n\nVPT-Dist↑\n\n1\n\n5\n\n10\n\n15\n\nFixed\n\nFixed\n\nFixed\n\n1.80(0.21)e-2\n\n0.44(0.16)\n\n10.6(3.40)\n\n0.60(0.10)\n\n1.53(0.36)e-2\n\n0.61(0.25)\n\n3.49(1.89)\n\n0.94(0.10)\n\n1.46(0.34)e-2\n\n0.65(0.26)\n\n3.08(1.58)\n\n0.96(0.08)\n\nFixed Variable\n\n1.44(0.34)e-2 1.50(0.34)e-2\n\n0.68(0.26) 0.64(0.25)\n\n2.88(1.45) 3.44(1.80)\n\n0.97(0.07) 0.94(0.10)\n\nFigure 3: A: t-SNE plot of support-set embedding c from stochastic (left) and deterministic (right) meta-models. B: Generated forecasting by sampling the distribution of c given the same z0.\n\nAllowing k to be variable had no noticeable effect on model performance. This flexibility highlights the practicality of the presented framework to forecast with any given size of support series.\n\nLatent embedding and generation of diverse dynamics: Fig. 3A shows the distribution of the latent embedding c obtained from randomly-selected support set, in comparison to a deterministic version of the presented meta-model on mixed-physics data. As shown, the presented framework was able to recognize and separate the three dynamics using the k-shot support set: given an initial z0, it was then able to generate different time-series within the same dynamics as well as across dynamics by sampling the distribution of c ( Fig. 3B). This was not possible with its deterministic counterpart.\n\n6 EXPERIMENTS ON COMPLEX PHYSICS SIMULATIONS\n\nWe then considered learning and forecasting two more complex physics-based dynamics: turbulent flow dynamics and cardiac electrical dynamics.\n\nTurbulent flow dynamics: We customized the meta-GRU model to a dataset of turbulent flow dynamics, simulated with 25 varying buoyant forces acting on the fluid. Each dynamic contains 64 × 64 velocity fields of turbulent flows. We use 20 dynamics in meta-training and meta-validation with 80-20 split, and the rest 5 in meta-testing. We followed the experimental setup in (Wang et al., 2021) with an observed window (20 frames in theirs vs. 5 in ours) and a prediction roll-out of 20 frames. Despite using a smaller number of observed frames, the presented meta-GRU model obtained a rooted MSE (RMSE) of 0.26 ± 0.05 on seen dynamics and 0.49 ± 0.05 on unseen dynamics, in comparison to respective RMSEs of 0.42 ± 0.01 and 0.51 ± 0.02 reported in (Wang et al., 2021), all reported on the 20 roll-out frames. We included trajectory visualizations in Appendix E.\n\nCardiac electrical dynamics: The propagation of electrical waves in the heart is governed by reaction-diffusion partial differential equations (PDEs) (Aliev & Panfilov, 1996). While direct PDE-based simulation holds clinical potential (e.g., for virtually testing treatment response), its patient-specific parameters are difficult to estimate and its computational cost is high. Although neural approximations provide a promising computationally-efficient alternative (Fresca et al., 2021), how to personalize such a neural model remains an open challenge where existing models are typically trained for a PDE with given parameter configurations. Here, we apply the presented framework\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Visual examples (A) and performance metrics of meta-GRU versus other baselines trained on the meta-training set for forecasting electrical dynamics on the heart.\n\nfor few-shot learning of a personalized neural model that can be used to efficiently forecast how a patient-specific heart may respond to electrical simulations at different locations.\n\nWe simulate electrical propagation originating from various locations in a 3D heart mesh, with 15 settings of PDE parameters representing 15 dynamics with different locations of injury to the heart muscle. We use 9 dynamics in meta-training, 3 in meta-validation, and 3 in meta-testing, with disjoint time-series with different initial conditions (meta-training: 450; meta-test: 2,020). Each time series describes 3D+T propagation of electrical wave with blocks at locations of muscle injury specific to each dynamics (see an example if Fig. 4A column 1). The quality of the forecast series is measured by its MSE and spatial correlation coefficient (CC) with the actual time-series.\n\nWe adopted a graph-CNN encoder/decoder and an ODE-GRU latent dynamic function zk = f (z<k; θz) similar to that described in (Jiang et al., 2021). We trained it with a global θz (global GRU), an individual θz for each PDE parameter (single-dynamics GRU), a conditioned f (zi−1, c; θz) with c encoded from individual training series (instance-specific GRU), and the presented framework (meta GRU) with k varying between 1 and 5. We further added a strong personalized virtual heart (PVH) baseline using the original PDE simulation, with the PDE parameter optimized by a SOTA approach from k-shot support series as described in (Dhamala et al., 2018).\n\nAs shown in Fig. 4A and additional examples in Appendix F, only meta-GRU was able to accurately forecast the propagation block while the other baseline models missed the correct locations of muscle injury specific to a subject (black circles in column 1): note that the single-dynamics GRU performed well on the training dynamics Fig. 4A, but fails on unknown dynamics (Appendix F). Unable to identify injury to patient-specific heart, the forecasting model will be of little value in clinical tasks such as personalized prediction and treatment planning. This gain in forecasting performance by meta-GRU is quantitatively summarized in Fig. 4B across all meta-test time-series. Note that metaGRU exhibited a notable margin of improvement even versus the PVH: PVH takes on average 5 minutes to forecast each series, versus 0.24 seconds by the meta-GRU; moreover, to optimize PDE parameters of the PVH on average required 100 calls to the PDEs (i.e., ∼ 10 hours), versus 0.032 seconds for meta-GRU to adapt to patient-specific dynamics. This substantial gain in efficiency without loss of accuracy holds significant value for clinical applications.\n\n7 CONCLUSIONS AND DISCUSSION\n\nIn this paper, we present a sequential LVM framework to unify existing approaches to learning latent dynamics, identify their limitations associated with the underlying choices of PGMs, and provide empirical evidence for the identified limitations. We further identify meta-learning as an intuitive solution to the identified open gaps, present a framework for few-shot high-dimensional time-series forecasting, and demonstrate that its performance gain is agnostic to the underlying choice of latent dynamic functions. Limitations: An avenue of future work is to expand the latent dynamic functions in this framework, especially those integrating strong inductive biases based on physics such as Hamiltonian mechanics (Botev et al., 2021).\n\nACKNOWLEDGMENTS\n\nThis study was supported by NIH National Heart, Lung, And Blood Institute (NHLBI) grant R01HL145590, NIH National Institute of Nursing Research (NINR) grant R01NR018301, and NSF OAC-2212548.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nRubin R Aliev and Alexander V Panfilov. A simple two-variable model of cardiac excitation. 7(3):\n\n293–301, 1996.\n\nAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. arXiv preprint\n\narXiv:1810.09502, 2018.\n\nPhilip Becker-Ehmck, Jan Peters, and Patrick Van Der Smagt. Switching linear dynamics for variational bayes filtering. In International Conference on Machine Learning, pp. 553–562. PMLR, 2019.\n\nLuca Bertinetto, J. F Henriques, J Valmadre, P. H. S. Torr, and Vedaldi A. Learning feed-forward\n\none-shot learners. In Neural Information Processing Systems, 2016.\n\nAleksandar Botev, Andrew Jaegle, Peter Wirnsberger, Daniel Hennes, and Irina Higgins. Which priors matter? benchmarking models for learning latent dynamics. In Advances in Neural Information Processing Systems, 2021.\n\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in Neural Information Processing Systems, 2015.\n\nJwala Dhamala, Sandesh Ghimire, John L Sapp, B Milan Hor ́aˇcek, and Linwei Wang. Highdimensional bayesian optimization of personalized cardiac model parameters via an embedded generative model. In International Conference on Medical Image Computing and ComputerAssisted Intervention, pp. 499–507. Springer, 2018.\n\nJ ́er ́emie Don`a, Jean-Yves Franceschi, Sylvain Lamprier, and Patrick Gallinari. Pde-driven spatiotem-\n\nporal disentanglement. arXiv preprint arXiv:2008.01352, 2020.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.\n\nMarco Fraccaro, Simon Kamronn, Ulrich Paquetz, and Ole Winthery. A disentangled recognition and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information Processing Systems, 2017.\n\nStefania Fresca, Andrea Manzoni, Luca Ded`e, and Alfio Quarteroni. Pod-enhanced deep learningbased reduced order models for the real-time simulation of cardiac electrophysiology in the left atrium. Frontiers in physiology, pp. 1431, 2021.\n\nM. Garnelo, D. Rosenbaum, C. J. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. J. Rezende, , and S. M. A. Eslami. Conditional neural processes. In International conference on machine learning, 2018.\n\nLaurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard, Thomas Hueber, and Xavier AlamedaPineda. Dynamical variational autoencoders: A comprehensive review. Foundations and Trends® in Machine Learning, 15(1-2):1–175, 2021. ISSN 1935-8237. doi: 10.1561/2200000089.\n\nXiajun Jiang, Ryan Missel, Maryam Toloubidokhti, Zhiyuan Li, Omar Gharbia, John L Sapp, and Linwei Wang. Label-free physics-informed image sequence reconstruction with disentangled spatial-temporal modeling. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 361–371. Springer, 2021.\n\nMaximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. In International Conference on Learning Representations, 2017.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAlexej Klushyn, Richard Kurle, Maximilian Soelch, Botond Cseke, and Patrick van der Smagt. Latent matters: Learning deep-state-space models. In Advances in Neural Information Processing Systems, 2021.\n\nRahul G. Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state\n\nspace models. In Association for the Advancement of Artificial Intelligence, 2017.\n\nL Li, J Yan, X Yang, and Y Jin. Learning interpretable deep state space model for probabilistic time\n\nseries forecasting. In International Joint Conference on Artificial Intelligence, 2019.\n\nScott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial Intelligence and Statistics, pp. 914–922. PMLR, 2017.\n\nChris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, and Yee Teh. Filtering variational objectives. Advances in Neural Information Processing Systems, 30, 2017.\n\nS Makridakis, E Spiliotis, and V Assimakopoulos. S. Statistical and machine learning forecasting\n\nmethods: Concerns and ways forward. PLoS ONE, 13, 2018.\n\nBhairav Mehta, Ankur Handa, Dieter Fox, and Fabio Ramos. A user’s guide to calibrating robotic\n\nsimulators. In Conference on Robot Learning, pp. 1326–1340. PMLR, 2021.\n\nP. Montero-Manso, G. Athanasopoulos, R. J. Hyndman, and T. S Talagala. FFORMA: Feature-based\n\nforecast model averaging. International Journal of Forecasting, 36, 2020.\n\nSajad Norouzi, David J Fleet, and Mohammad Norouzi. Exemplar vae: Linking generative models, nearest neighbor retrieval, and data augmentation. Advances in Neural Information Processing Systems, 33:8753–8764, 2020.\n\nBoris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. Meta-learning framework with applications to zero-shot time-series forecasting. In AAAI Conference on Artificial Intelligence, 2021.\n\nB. N. Oreshkina, D. Carpov, N. Chapados, and Y. Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020.\n\nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning mesh-\n\nbased simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020.\n\nShenghao Qin, Jiacheng Zhu, Jimmy Qin, Wenshuo Wang, and Ding Zhao. Recurrent attentive neural\n\nprocess for sequential data. In arXiv, 2019.\n\nSyama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. In Neural Information Processing Systems, 2018.\n\nS. Ravi and H. Larochelle. Optimization as a model for few- shot learning.\n\nIn International\n\nConference on Learning Representations (ICLR), 2016.\n\nYulia Rubanova, Ricky T. Q. Chen, and David Duvenaud. Latent odes for irregularly-sampled time\n\nseries. In Neural Information Processing Systems, 2019.\n\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36: 1181–1191, 2020.\n\nGautam Singh, Jaesik Yoon, and Youngsung Son. Sequential neural processes. In Neural Information\n\nProcessing Systems, 2019.\n\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nRui Wang, Robin Walters, and Rose Yu. Incorporating symmetry into deep dynamics models for\n\nimproved generalization. arXiv preprint arXiv:2002.03061, 2020.\n\nRui Wang, Robin Walters, and Rose Yu. Meta-learning dynamics forecasting using task inference.\n\narXiv preprint arXiv:2102.10271, 2021.\n\nC. Yildiz, M. Heinonen, and H. Lahdesmaki. ODE2VAE: Deep generative second order odes with\n\nbayesian neural networks. In Neural Information Processing Systems, 2020.\n\nYuan Yin, Ibrahim Ayed, Emmanuel de B ́ezenac, Nicolas Baskiotis, and Patrick Gallinari. Leads: Learning dynamical systems that generalize across environments. Advances in Neural Information Processing Systems, 34:7561–7573, 2021.\n\nJaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic meta-learning. Advances in neural information processing systems, 31, 2018.\n\nA DERIVATION OF EQUATION 7\n\n(cid:88)M\n\n(cid:88)\n\nj=1\n\nxq\n\n0:T ∈Dq\n\nj\n\nlog p(xq\n\n0:T |Ds j )\n\n=\n\n=\n\n(cid:88)M\n\n(cid:88)\n\nj=1\n\nxq\n\n0:T ∈Dq\n\nj\n\nlog\n\n(cid:90)\n\nc\n\np(xq\n\n0:T |c)pζ(c|Ds\n\nj )dc\n\n(cid:88)M\n\n(cid:88)\n\nj=1\n\nxq\n\n0:T ∈Dq\n\nj\n\n(cid:90)\n\n(cid:90)\n\nlog\n\nc\n\nz0\n\n(cid:20)\n\npθx(x0|z0, c)\n\n(cid:89)T\n\ni=1\n\n(cid:21)\n\npθx (xi|zi, c)\n\n·\n\np(z0) qφ(z0|xq\n\n0:lz0\n\n)\n\n≥\n\n(cid:88)M\n\n(cid:88)\n\nj=1\n\nxq\n\n0:T ∈Dq\n\nj\n\n− log\n\nqζ(c|Ds\n\nj ∪ xq pζ(c|Ds j ) (cid:88)\n\n(cid:88)M\n\n=\n\nj=1\n\n0:T ∈Dq xq 0|xq − KL(qφ(zq\n\nj\n\n0:lz0\n\nqζ(c|Ds\n\npζ(c|Ds j ) j ∪ xq (cid:20) (cid:90) log pθx (xq\n\n0:T )\n\n(cid:90)\n\nc\n\nz0\n\n· qφ(z0|xq\n\n0:lz0\n\n)qζ(c|Ds\n\nj ∪ xq\n\n0:T )dz0dc\n\n0:T |zq\n\n0, c) − log\n\nqφ(z0|xq\n\n0:lz0\n\n)\n\np(z0)\n\n0:T )\n\n(cid:21)\n\n· qφ(z0|xq\n\n0:lz0\n\n)qζ(c|Ds\n\nj ∪ xq\n\n0:T )dz0dc\n\nEqφ(zq\n\n0),qζ (c|Ds\n\nj ∪xq\n\n0:T )[log pθx (xq\n\n0:T |zq\n\n0, c)]\n\n)||p(z0)) − KL (cid:0)qζ(c|Ds\n\nj ∪ xq\n\n0:T )||pζ(c|Ds\n\nj )(cid:1)\n\nB GATED RECURRENT UNIT (GRU) SET-CONDITIONING\n\nWe condition the GRU cell of the transition function through Equation 8,\n\nz(1) i−1 = ELU(α1zi−1 + β1c + γ1), i−1 = ELU(α2zi−1 + β2c + γ2), hi−1 = ELU(W2z(2) z(2) ̃zi−1 = α3zi−1 + β3c + γ3, ∆zi = (1 − gi−1) ⊙ (W3 ̃zi−1 + b3) + gi−1 ⊙ hi−1,\n\ngi−1 = σ(W1z(1)\n\ni−1 + b1)\n\ni−1 + b2)\n\n(8)\n\nwhere θz = {Wi, bi, αi, βi, γi}3\n\ni=1 are learnable parameters of the dynamic function.\n\nC EXTRA CONSTRAINT TO META-MODEL\n\nWe considered adding an additional regularization to the set embedding pζ(c|Ds constrained to a reasonable range. Since the true posterior density pζ(c|Ds\n\nj ) so that it is j ) is unknown, we assume\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Comparison with models with and without constraint.\n\nFigure 6: Examples of models with and without constraint.\n\nthat it is bounded by a standard Gaussian distribution N (0, I). Therefore, the objective function of the model becomes:\n\narg min\n\nEqφ(zq\n\n0),qζ (c|Ds\n\nj ∪xq\n\n0:T )[log pθx (xq\n\n0:T |zq\n\n0, c)]\n\n(cid:88)M\n\n(cid:88)\n\nj=1\n\nj\n\nxq\n\n0:T ∈Dq − KL(qφ(zq 0|xq − KL (cid:0)pζ(c|Ds\n\n0:lz0\n\nj )||N (0, I)(cid:1)\n\n)||p(z0)) − KL (cid:0)qζ(c|Ds\n\nj ∪ xq\n\n0:T )||pζ(c|Ds\n\nj )(cid:1)\n\nWe applied the extra constraint to the proposed meta-GRU-res model for both fixed and variable k at different value of k = 1, 5, 9, 15 and evaluated on gravity-16 dataset. Fig. 5 summaries the quantitative test performance of the two models trained with and without the constraint on the set embedding. The constraint generally had no noticeable effect on model performance. Visual examples at k = 1 are also shown in Fig. 6. It shows that when k is small, the model with the constraint had a slightly better performance.\n\nD ADDITIONAL GRAVITY-16 RESULTS\n\nHere we provide the complete results of all baseline models trained on gravity-16 in Table 4 when split between the known dynamics during training and the unknown ones during testing . Similarly, we provide the full visualization of all baselines within Fig. 7. We note that the VRNN fails to converge in any of the single dynamics cases, in contrast to its full dynamics training, which is likely due to the lower data availability in these settings. The other baselines DKF and KVAE see similar decreases in performance compared to their full meta-training set performance.\n\nE TURBULENT FLOW VISUALIZATIONS\n\nHere we provide visualizations of predicted trajectories for both known and unknown buoyancy factors on the turbulent flow dataset within Fig. 8 and Fig. 9.\n\nF ADDITIONAL CARDIAC FIGURE\n\nHere we provide an additional result on the cardiac experiments in Figure 10, this time highlighting the performance of the model on unknown dynamic cardiac dynamics when compared against the baselines. We use a varying support set k size between 1 to 5 samples. The proposed model manages\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Forecasting on gravity-16 under (blue) full dynamics and (orange) single dynamics training.\n\nFigure 8: Example of forecasting known turbulent flow dynamics.\n\nFigure 9: Example of forecasting unknown turbulent flow dynamics.\n\nFigure 10: Example of forecasting unknown electrical dynamics on the heart.\n\nto effectively model around the scar tissue present in the ground truth. The baselines, besides the expensive personalized virtual heart (PVH), are unable to account for the specific dynamics of this subject and propagate over it.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Comparison of the presented meta and base models on known and unknown dynamics.\n\nModel\n\nDynamics MSE↓\n\nVPT-MSE↑ Dist↓\n\nVPT-Dist↑\n\nmeta-GRU-res\n\nmeta-NODE\n\nmeta-RGN-res\n\nGRU-res\n\nGRU-res finetune\n\nNODE\n\nNODE finetune\n\nRGN-res\n\nRGN-res finetune\n\nmeta-DKF\n\nDKF\n\nDKF finetune\n\nVRNN\n\nVRNN finetune\n\nDVBF\n\nDVBF finetune\n\nKVAE\n\nDon`a et al\n\nknown unknown\n\nknown unknown\n\nknown unknown\n\nknown unknown unknown\n\nknown unknown unknown\n\nknown unknown unknown\n\nknown unknown\n\nknown unknown unknown\n\nknown unknown unknown\n\nknown unknown unknown\n\nknown unknown\n\nknown unknown\n\n1.43(0.34)e-2 1.45(0.33)e-2\n\n1.60(0.26)e-2 1.62(0.26)e-2\n\n1.59(0.25)e-2 1.60(0.23)e-2\n\n1.80(0.29)e-2 1.99(0.277)e-2 2.03(0.27)e-2\n\n1.97(0.21)e-2 2.06(0.22)e-2 2.07(0.21)e-2\n\n1.93(0.22)e-2 2.03(0.22)e-2 2.05(0.22)e-2\n\n3.81(0.59)e-2 3.80(0.59)e-2\n\n3.89(0.32)e-2 3.88(0.32)e-2 3.89(0.32)e-2\n\n2.32(14.6)e-2 2.32(14.6)e-2 2.34(14.5)e-2\n\n2.32(14.3)e-2 2.43(14.1)e-2 2.35(14.1)e-2\n\n3.42(1.30)e-2 3.46(1.36)e-2\n\n3.58(0.33)e-2 3.56(0.34)e-2\n\n0.68(0.26) 0.67(0.25)\n\n0.58(0.23) 0.57(0.22)\n\n0.57(0.21) 0.56(0.20)\n\n0.46(0.23) 0.37(0.18) 0.35(0.17)\n\n0.30(0.20) 0.27(0.17) 0.27(0.17)\n\n0.34(0.20) 0.30(0.16) 0.29(0.16)\n\n0.10(0.11) 0.10(0.11)\n\n0.08(0.06) 0.08(0.06) 0.08(0.06)\n\n0.02(0.11) 0.01(0.08) 0.04(0.15)\n\n0.01(0.02) 0.01(0.07) 0.01(0.08)\n\n0.39(0.34) 0.22(0.19)\n\n0.00(0.01) 0.00(0.01)\n\n2.86(1.44) 2.96(1.49)\n\n0.97(0.06) 0.97(0.07)\n\n6.05(2.65) 6.23(2.54)\n\n6.89(3.08) 7.23(3.09)\n\n6.86(3.95) 8.07(3.56) 8.51(3.42)\n\n10.6(4.13) 11.1(3.71) 11.2(3.72)\n\n10.1(4.11) 10.8(3.72) 10.7(3.62)\n\n7.37(3.27) 7.30(3.21)\n\n10.7(3.17) 10.7(3.23) 10.8(3.24)\n\n45.3(0.00) 45.3(0.00) 45.3(0.00)\n\n45.2(0.00) 45.3(0.00) 45.3(0.00)\n\n5.05(3.57) 5.17(3.91)\n\n13.7(3.36) 14.1(3.84)\n\n0.80(0.12) 0.80(0.12)\n\n0.76(0.13) 0.75(0.13)\n\n0.77(0.19) 0.69(0.17) 0.66(0.18)\n\n0.60(0.17) 0.57(0.16) 0.55(0.16)\n\n0.61(0.18) 0.57(0.16) 0.56(0.16)\n\n0.70(0.25) 0.70(0.25)\n\n0.46(0.20) 0.45(0.20) 0.45(0.20)\n\n0.00(0.00) 0.00(0.00) 0.00(0.00)\n\n0.00(0.00) 0.00(0.00) 0.00(0.00)\n\n0.5(0.34) 0.53(0.28)\n\n0.07(0.18) 0.08(0.19)\n\nG DATA DETAILS\n\nIn this section, we give the specific data sizes and splits used for training throughout the experiments, as well as generation procedures and considerations for each.\n\nFor the bouncing balls, we leveraged the PyMunk Physics Engine (www.pymunk.org) to perform simulations under various gravity following (Fraccaro et al., 2017). For pendulum and mass spring systems, we leveraged the Hamiltonian Dynamics Suite presented in (Botev et al., 2021). The suite’s default physical parameters were used and friction coefficients were introduced to build non-energy conserving systems. For data consistency, we extracted the red color channel of pendulum and mass spring systems to generate gray-scale images.\n\nFor gravity-16 experiments, we generated bouncing balls dynamic trajectories on 32 × 32 images. All dynamics are under 16 different gravitational constants, where g = 3 + ε, ε ∼ U (0, 1) and the direction of each gravity is evenly distributed over the 2D space. We took the trajectories samples at every ∆t = 0.2 intervals. The initial position of the ball is set to a 16 × 16 region centered in the image, and the initial velocity is randomly sampled from [0, 10] on both x and y directions. Each gravity setting has 3,000 samples in total. For gravity-16 data, we used 10 gravity in meta-training, 2 in meta-validation, and 4 in meta-testing. We also left out samples from both meta-training and\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nmeta-validation sets (∼1500 for each gravity) to evaluate the performance of the model on the known dynamics.\n\nFor mixed-physics experiments, we generated a mixed-physics dataset consisting of bouncing balls under 4 gravity directions, and pendulums and mass springs each with four different values of friction coefficients of 0, 0.05, 0.1, 0.15. The bouncing ball dataset is similar to gravity-16 experiment except for the 4 gravity directions. Both pendulums and mass springs took the trajectories samples at every ∆t = 0.2 intervals. In pendulums, the mass of the particle m = 0.5, the gravitational constant g = 3 and the pivot length l = 1. The friction coefficients are chosen in 0, 0.05, 0.1, 0.15. In mass springs, the mass of the particle m = 0.5, the spring force coefficient k = 2. The friction coefficients are chosen in 0, 0.05, 0.1, 0.15. For each of the three physics, we included three dynamic settings in meta-training and left out one in meta-testing.\n\nFor the turbulent flow dataset, we generated the tasks as given by the instructions and scripts found within the official code repository from (Wang et al., 2021), https://github.com/ Rose-STL-Lab/Dynamic-Adaptation-Network. We split the seen and unseen buoyancy factors according to the same task split used within their work and directly compare RMSE values based on their implementation and magnitude coefficients.\n\nFor cardiac electrical dynamics, we generated 3D electrical signal propagation in the heart simulated by the Aliev-Panfilov model Aliev & Panfilov (1996) on 3 heart meshes and a total of 12 different tissue parameters (4 on each heart) representing different injury to the heart muscle. This was treated as 12 tasks in meta-learning. All 12 tasks appeared in meta-training and -testing, with disjoint time sequences resulting from different external stimulations (meta-training: 300; meta-test: 2,020).\n\ncan\n\nball\n\ndata\n\nAll folders/1Tm3DNrugcSbWXSNyeGL3jQKR8y3iXx0m?usp=sharing. heart data can be found here: 12S579V0KWMgbHGXDQZt0rQyfzF1AyNCu?usp=sharing.\n\nhttps://drive.google.com/drive/ The https://drive.google.com/drive/folders/\n\nfound\n\nhere:\n\nbe\n\nH IMPLEMENTATION DETAILS\n\nIn this section, we give the specific hyper-parameters on each experiment over all models, as well as resources and considerations for each. All experiments were run on NVIDIA Tesla T4s with 16 GB memory.\n\nH.0.1 ARCHITECTURE FOR META MODELS\n\nhttps://github.com/ The implementation of our proposed meta models is here: john-x-jiang/meta_ssm. Specifically in the configuration file, the type of transition function can be changed by trans_mode and trans_args in the model section. The size of the support set is controlled by k_shot in the data section, and the variable/fixed K is set up by changeable in both training and evaluation section. During meta-testing, the paired support and query sets should be put under eval_tags and pred_tags in the data section. Detailed hyperparameter settings are shown below.\n\nMeta Model Architecture on Mixed-Physics and Gravity-16\n\n• Domain Input: 20 observation timesteps of 32 × 32 dimensions\n\n• Initialization Input: 3 observation timesteps of 32 × 32 dimensions • Optimizer: Adam, 5 × 10−4 learning rate\n\n• Transition: Gated transition function (GRU-res) / Recurrent Generative Network (RGN-res)\n\n/ Neural Ordinary Differential Equation (NODE)\n\n• Batch size: 50\n\n• Number of epochs: 200\n\n• Training time: 1.5 - 5 hours\n\n• Latent Units: 8\n\n16\n\nPublished as a conference paper at ICLR 2023\n\n• Transition Units: 100\n\n• Domain Encoder Filters: [8, 16, 8]\n\n• Domain Time Units: [10, 5, 1]\n\n• Initial Encoder Filters: [8, 16, 8]\n\n• Emission Filters: [32, 16, 8, 1] • KL term initialization: λ1 = 10−2 • KL term set-embedding: λ2 = 10−2\n\nMeta Model Architecture on Turbulent Flow\n\n• Domain Input: 20 observation timesteps of 64 × 64 dimensions\n\n• Initialization Input: 5 observation timesteps of 64 × 64 dimensions • Optimizer: Adam, 5 × 10−4 learning rate\n\n• Transition: Gated transition function (GRU-res) / Recurrent Generative Network (RGN-res)\n\n/ Neural Ordinary Differential Equation (NODE)\n\n• Batch size: 20\n\n• Number of epochs: 500\n\n• Training time: 6 hours\n\n• Latent Units: 64\n\n• Transition Units: 100\n\n• Domain Encoder Filters: [32, 64, 128, 32]\n\n• Domain Time Units: [10, 5, 2, 1]\n\n• Initial Encoder Filters: [64, 128, 256, 64]\n\n• Emission Filters: [256, 128, 64, 1] • KL term initialization: λ1 = 10−1 • KL term set-embedding: λ2 = 10−1\n\nH.0.2 ARCHITECTURE FOR BASELINE MODELS\n\nThe implementation of baseline models is here: https://github.com/john-x-jiang/ meta_ssm. The setting of transition function can be changed by trans_mode and trans_args in the model section. The testing sets should be put under pred_tags in the data section. Detailed hyperparameter settings are shown below.\n\nBaseline Model Architecture\n\n• Initialization Input: 3 observation timesteps of 32 × 32 dimensions • Optimizer: Adam, 5 × 10−4 learning rate\n\n• Transition: Gated transition function(GRU-res) / Recurrent Generative Network (RGN-res) /\n\nNeural Ordinary Differential Equation (NODE)\n\n• Batch size: 50\n\n• Number of epochs: 200\n\n• Training time: 1.3 hours\n\n• Latent Units: 8\n\n• Transition Units: 100\n\n• Initial Encoder Filters: [8, 16, 8]\n\n• Emission Filters: [32, 16, 8, 1] • KL term initialization: λ1 = 10−2\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nH.0.3 ARCHITECTURE FOR DKF\n\nThe DKF model (Krishnan et al., 2017) we used is based on this implementation: https:// github.com/yjlolo/pytorch-deep-markov-model. Our implementation is provided in our code. We modified the script to perform reconstruction of the observed part of input sequences and prediction of the unobserved parts. Specifically, the output of the correction function was used in the decoder in the reconstruction phase, while the transition module was used for prediction. Detailed hyperparameter settings are shown below.\n\nDKF Architecture\n\n• Input: 8 observation and 12 prediction timesteps of 32 × 32 dimensions • Optimizer: Adam, 5 × 10−4 learning rate\n\n• Transition: Gated transition function\n\n• Batch size: 50\n\n• Number of epochs: 200\n\n• Training time: 1.2 hours\n\n• Encoder Units: [2048, 2048, 100]\n\n• RNN Units: 100\n\n• Correction Units: 100\n\n• Transition Units: 100\n\n• Emission Filters: [32, 16, 8, 1]\n\nH.0.4 ARCHITECTURE FOR DVBF\n\nThe PyTorch implementation of DVBF (Karl et al., 2017) we used can be found here: https: //github.com/gregorsemmler/pytorch-dvbf. Our implementation is provided in our code. We modified this script to perform initial state generation, as highlighted in DVBF’s experiments (Karl et al., 2017). We found lacking convergence in more complex dynamics and lower dataavailability scenarios, specifically bouncing ball settings. We detail the hyperparameter values chosen per experiment with respect to the hyperparameters found in this repository.\n\nDVBF Architecture on Gravity-16\n\n• Input: 8 observation and 12 prediction timesteps of 32 × 32 dimensions • Optimizer: Adam, 1 × 10−3 learning rate annealed to 1 × 10−4 over 100 epochs\n\n• Delayed KL Weight: between epochs=[5,25], linearly annealed from 0.01 to 1\n\n• Batch-size: 25\n\n• Number of Epochs: 100\n\n• Training time: 15 hours\n\n• Number of matrices αt: 8\n\n• Code vector dim wt: 8\n\n• Feature dim zt: 8\n\n• State dim st: 1024\n\n• RNN Hidden Size: 100\n\n• Inference Size: 32\n\n• Transition Size: 64\n\n• Encoder CNN Filters: [8, 32, 8]\n\n• Decoder CNN Filters: [8, 32, 8]\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nDVBF Architecture on Mixed-Physics\n\n• Input: 8 observation and 12 prediction timesteps of 32 × 32 dimensions • Optimizer: Adam, 1 × 10−3 learning rate annealed to 1 × 10−4 over 100 epochs\n\n• Delayed KL Weight: between epochs=[5,25], linearly annealed from 0.01 to 1\n\n• Batch-size: 25\n\n• Number of Epochs: 100\n\n• Training time: 15 hours\n\n• Number of matrices αt: 8 • Code vector dim wt: 8 • Feature dim zt: 8 • State dim st: 1024 • RNN Hidden Size: 100\n\n• Inference Size: 32\n\n• Transition Size: 64\n\n• Encoder CNN Filters: [8, 32, 8]\n\n• Decoder CNN Filters: [8, 32, 8]\n\nH.0.5 ARCHITECTURE FOR KVAE\n\nThe public implementation of KVAE (Fraccaro et al., 2017) we used can be found here: https: //github.com/simonkamronn/kvae. No significant modifications were made to the repo - primarily just output formatting. When attempting to scale the hyperparameter K (number of mixture components for the LG-SSM matrices), we ran into numerical loss instability and lacking convergence in reconstruction. We detail the hyperparameter values chosen per experiment with respect to the hyperparameters found in this repository.\n\nKVAE Architecture on Gravity-16\n\n• Input: 8 observation and 12 prediction timesteps of 32 × 32 dimensions • Optimizer: Adam, 7 × 10−3 learning rate exponentially decayed by 0.85 over 20 epochs\n\n• Batch-size: 32\n\n• Number of Epochs: 80\n\n• Training time: 2 hours\n\n• K: 3\n\n• Latent variable dim at: 4 • Latent state dim zt: 8 • Control dim ut (unused): 1 • RNN Hidden Size: 50\n\n• RNN Layers: 2\n\n• Encoder CNN Filters: [3, 32, 32, 32]\n\n• Decoder CNN Filters: [3, 32, 32, 32]\n\nKVAE Architecture on Mixed-Physics\n\n• Input: 8 observation and 12 prediction timesteps of 32 × 32 dimensions • Optimizer: Adam, 7 × 10−3 learning rate exponentially decayed by 0.85 over 20 epochs\n\n• Batch-size: 32\n\n• Number of Epochs: 80\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n• Training time: 2 hours\n\n• K: 3\n\n• Latent variable dim at: 4 • Latent state dim zt: 8 • Control dim ut (unused): 1 • RNN Hidden Size: 50\n\n• RNN Layers: 2\n\n• Encoder CNN Filters: [3, 32, 32, 32]\n\n• Decoder CNN Filters: [3, 32, 32, 32]\n\nH.0.6 ARCHITECTURE FOR VRNN\n\nThe DVAE code implementation Girin et al. (2021) of VRNN (Chung et al., 2015) we used can be found here: https://github.com/XiaoyuBIE1994/DVAE/blob/master/dvae/ model/vrnn.py. The model was modified to take its own reconstructions as inputs to the feature extractor at every timestep after the true sequence observation period. We detail the hyperparameter values chosen per experiment with respect to the hyperparameters found in this repository.\n\nVRNN Architecture on Gravity-16\n\n• Input: 8 observation and 12 prediction timesteps of 32 × 32 dimensions • Optimizer: Adam, 1 × 10−3 learning rate\n\n• Batch-size: 50\n\n• Number of Epochs: 80\n\n• Training time: 8 hours\n\n• Latent state dim zt: 32 • Dropout probability: 0.2\n\n• Dense X Size: 128\n\n• Dense Z Size: 128\n\n• Dense H(X)-Z Size: 128\n\n• Dense H(Z)-X Size: 128\n\n• Dense H(Z) Size: 128\n\n• RNN Layers: 2\n\n• RNN Dim: 64\n\n• Beta coefficient: 1.0\n\n• Activation: LeakyReLU(1.0)\n\nVRNN Architecture on Mixed-Physics\n\n• Input: 8 observation and 12 prediction timesteps of 32 × 32 dimensions • Optimizer: Adam, 1 × 10−3 learning rate\n\n• Batch-size: 50\n\n• Number of Epochs: 80\n\n• Training time: 8 hours\n\n• Latent state dim zt: 32 • Dropout probability: 0.2\n\n• Dense X Size: 128\n\n• Dense Z Size: 128\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n• Dense H(X)-Z Size: 128\n\n• Dense H(Z)-X Size: 128\n\n• Dense H(Z) Size: 128\n\n• RNN Layers: 2\n\n• RNN Dim: 64\n\n• Beta coefficient: 1.0\n\n• Activation: LeakyReLU(1.0)\n\n21",
    "reference": "# Summary Of The Paper\n\nSequential Latent Variable Models for Few-Shot High-Dimensional Time-Series Forecasting introduces a sequential latent variable model for meta-learning various high-dimensional time-series. This model allows for learning diverse dynamics, and adapting in a few-shot setting during evaluation, evaluating on a variety of benchmark datasets against known models from prior literature.\n\n# Strength And Weaknesses\n\nStrengths:\n\nStrong presentation of the method, background, and related work. Logical extension to meta/few-shot setting, with direct references to how and why the architecture is logical. Results compare favorably with benchmarks, benchmarks are pretty thoroughly set up and tested on the chosen datasets. \n\nWeaknesses:\n\nFigure 4 color scheme is pretty difficult to discern, specifically two slightly different shades of green. Generally Figures 2, 3, and 4 (and to some extent 5) have many graphical results which should probably be condensed to a table, with a small example of frames from 1 or 2 chosen methods. More extensive videos/gifs can be linked in a project page, or in drive links like those provided in the appendix. Generally it would be nice to see improvements to the experimental results presentation in this section.\n\nThe comparisons are thorough but FiVO seems to be missing - I do not think a full comparison is totally necessary to show the value here, but some discussion or citation is probably in order given the close relation of FiVO to the related benchmark models tested https://arxiv.org/abs/1705.09279 .\n\nAddition of another experiment beyond the existing would broaden the pool of potential readers. Particularly there are a variety of benchmarks in the related work (midi music, speech, handwriting) that could make compelling comparison points. Despite the discussion of multi-dimensional data, it is worth considering or testing low-dimensional forecasting, as the formulation of few shot dynamics has direct relevance in that subfield, and there are many benchmarks in GluonTS which DeepAR used. Multi-speaker TTS (or audio modeling ala RAVE https://arxiv.org/abs/2111.05011 ) is another compelling area, but may be out of scope due to the limits of the reviewer/author feedback window. Online handwriting is at least multi-dimensional, and has many prior applications related to stylistic generation (see VRNN or https://arxiv.org/abs/2110.02891 ). The experiments in the paper show the value of the method, but the scope of experiments seems narrow compared to some of the prior work, making it difficult to asses the full impact in the design choices of this method.\n\nThe cardiac electrical dynamics seems to be an interesting task, but hard to understand or compare due to limited space. Consider redistributing importance (in terms of writing space allocated) between these experiments. However, a problem may arise given the comparison method - what do *failures* look like for this model in the cardiac electrical case? Are they disastrous, or tolerable for the setting? Given the strong baseline approach (PVH), the errors from this method are probably qualitatively different - discussion of what types of failures could occur seem important for practical applications.\n\nIt would be nice to mention use of exemplar based methods in static VAE settings, such as Exemplar VAE https://proceedings.neurips.cc/paper/2020/hash/63c17d596f401acb520efe4a2a7a01ee-Abstract.html , probably as related work.\n\nThe primary points I would like to see addressed are:\n1) modified presentation of results in figures 2/3/4/5, with a focus on moving numerical results into tables instead of bar charts for easier comparison - as it stands these results are difficult to parse.\n2) Further work on the experiments - either introducing an additional task with more common ground with the related work, or a deeper dive into the results on cardiac electrical dynamics, what the results mean, and what the potential impact of those results could be on the relevant application field.\nAddressing these points (either, or both - in whole or in part) would improve the quality of the presentation, and justify a higher score.\n\nAs mentioned in the paper, comparison to Lagrangian and Hamiltonian networks could be of interest if the focus remains on physics-based experimental benchmarks.\n\nSome minor typos to fix:\n\"abstracts a a latent\", paragraph 2\n\"Deepar\" -> \"DeepAR\"\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis work seems original, though there are similar efforts in related areas, this method both unifies and justifies the approach with background/prior work. The addition of few shot setups in dynamics learning is of broad interest, and well supported by the proposed model.\n\n\"We then present the first framework of few-shot forecasting for high-dimensional time-series\" in the abstract is a bit of an overclaim (given extensive prior work on few, one, and zero shot learning for video and audio generation, as well as the broader relationship of prompting and transformers to few shot settings), and in this case not necessary for the core method to be novel and of interest.\n\nThe appendix is extensive, and related links are helpful. Code is included, and the effort in terms of reproducibility of the approach is top notch. In addition the authors detail what packages were used for the benchmark comparisons, which is critical to any followup comparisons.\n\n# Summary Of The Review\n\nThis paper introduces an interesting method, and both the background writing and method discussion make clear the relevant materials, and why this method is interesting. However the scope of the experiments, along with the presentation of the experiments, currently limit the potential interest of this work.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nRECOVERING TOP-TWO ANSWERS AND CONFUSION PROBABILITY IN MULTI-CHOICE CROWDSOURCING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nCrowdsourcing has emerged as an effective platform to label a large volume of data in a cost- and time-efficient manner. Most previous works have focused on designing an efficient algorithm to recover only the ground-truth labels of the data. In this paper, we consider multi-choice crowdsourced labeling with the goal of recovering not only the ground truth but also the most confusing answer and the confusion probability. The most confusing answer provides useful information about the task by revealing the most plausible answer other than the ground truth and how plausible it is. To theoretically analyze such scenarios, we propose a model where there are top-two plausible answers for each task, distinguished from the rest of choices. Task difficulty is quantified by the confusion probability between the top two, and worker reliability is quantified by the probability of giving an answer among the top two. Under this model, we propose a two-stage inference algorithm to infer the top-two answers as well as the confusion probability. We show that our algorithm achieves the minimax optimal convergence rate. We conduct both synthetic and real-data experiments and demonstrate that our algorithm outperforms other recent algorithms. We also show the applicability of our algorithms in inferring the difficulty of tasks and training neural networks with the soft labels composed of the top-two most plausible classes.\n\n1\n\nINTRODUCTION\n\nCrowdsourcing has been widely adopted to solve a large number of tasks in a time- and cost-efficient manner with the aid of human workers. In this paper, we consider ‘multiple-choice’ tasks where a worker is asked to provide a single answer among multiple choices. Some examples are as follows: 1) Using crowdsourcing platforms such as MTurk, we solve object counting or classification tasks on a large collection of images. Answers can be noisy either due to the difficulty of the scene or due to unreliable workers who provide random guesses. 2) Scores are collected from reviewers for papers submitted at a conference. For certain papers, scores can vary widely among reviewers, either due to the paper’s inherent nature (clear pros and cons) or due to the reviewer’s subjective interpretation of the scoring scale (Stelmakh et al., 2019; Liu et al., 2022).\n\nIn the above scenarios, responses provided by human workers may not be consistent among themselves not only due to the existence of unreliable workers but also due to the inherent difficulty of the tasks. In particular, for multiple-choice tasks, there could exist plausible answers other than the ground truth, which we call confusing answers.1 For tasks with confusing answers, even reliable workers may provide wrong answers due to confusion. Thus, we need to decompose the two different causes of wrong answers: low reliability of workers and confusion due to task difficulty.\n\nMost previous models for multi-choice crowdsourcing, however, fall short of modeling the confusion. For example, in the single-coin Dawid-Skene model (Dawid & Skene, 1979), which is the most widely studied crowdsourcing model in the literature, it is assumed that a worker is associated with a single skill parameter fixed across all tasks, which models the probability of giving a correct answer for every task. Under this model, any algorithm that infers the worker skill would count a confused labeling as the worker’s error and lower its accuracy estimate for the worker, which results in a wrong estimate for their true skill level.\n\n1This phenomenon is evident on public datasets: for ‘Web’ dataset (Zhou et al., 2012), which has five labels, the most dominating top-two answers take 80% of the overall answers and the ratio between the top two is 2.4:1.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo model the effect of confusion in multi-choice crowdsourcing problems, we propose a new model under which each task can have a confusing answer other than the ground truth, with a varying confusion probability across tasks. The task difficulty is quantified by the confusion probability, and the worker skill is modeled by the probability of giving an answer among the top two, to distinguish reliable workers from pure spammers who just provide random guesses among possible choices. We justify the proposed top-two model with public datasets. Under this new model, we aim to recover both the ground truth and the most confusing answer with the confusion probability, indicating how plausible the recovered ground truth is compared to the most confusing answer.\n\nWe provide an efficient two-stage inference algorithm to recover the top-two plausible answers and the confusion probability. The first stage of our algorithm uses the spectral method to get an initial estimate for top-two answers as well as the confusion probability, and the second stage uses this initial estimate to estimate the worker reliabilities and to refine the estimates for the top-two answers. Our algorithm achieves the minimax optimal convergence rate. We then perform experiments where we compare our method to recent crowdsourcing algorithms on both synthetic and real datasets, and show that our method outperforms other methods in recovering top-two answers. This result demonstrates that our model better explains the real-world datasets including errors from confusion. Our key contributions can be summarized as follows.\n\n• Top-two model: We propose a new model for multi-choice crowdsourcing tasks where each task has top-two answers and the difficulty of the task is quantified by the confusion probability between the top-two. We justify the proposed model by analyzing six public datasets, and showing that the top-two structure explains well the real datasets.\n\n• Inference algorithm and its applicaitons: We propose a two-stage algorithm that recovers the top-two answers and the confusion probability of each task at the minimax optimal convergence rate. We demonstrate the potential applications of our algorithm not only in crowdsourced labeling but also in quantifying task difficulty and training neural networks for classification with soft labels including the top-two information and the task difficulty.\n\nRelated works In crowdsourcing (Welinder et al., 2010; Liu & Wang, 2012; Demartini et al., 2012; Aydin et al., 2014; Demartini et al., 2012), one of the most widely studied models is the Dawid-Skene (D&S) model (Dawid & Skene, 1979). In this model, each worker is associated with a single confusion matrix fixed across all tasks, which models the probability of giving a label b ∈ [K] for the true label a ∈ [K] for K-ary classification task. In the single-coin D&S model, the model is further simplified such that each worker possesses a fixed skill level regardless of the true label or the task. Under the D&S model, various methods were proposed to estimate the confusion matrix or skill of each worker by spectral method (Zhang et al., 2014; Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013), belief propagation or iterative algorithms (Karger et al., 2014; 2011; Li & Yu, 2014; Liu et al., 2012; Ok et al., 2016), or rank-1 matrix completion (Ma et al., 2018; Ma & Olshevsky, 2020; Ibrahim et al., 2019). The estimated skill can be used to infer the ground-truth answer by approximating the maximum likelihood (ML)-type estimators (Gao & Zhou, 2013; Gao et al., 2016; Zhang et al., 2014; Karger et al., 2013; Li & Yu, 2014; Raykar et al., 2010; Smyth et al., 1994; Ipeirotis et al., 2010; Berend & Kontorovich, 2014). In contrast to the D&S models, our model allows the worker to have different probability of error caused by confusion. Thus, our algorithm needs to estimate not only the worker skill but also the task difficulty. Since the number of tasks is often much larger than the number of workers in practice, estimating the task difficulties is much more challenging than estimating worker skills. We provide a statistically-efficient algorithm to estimate the task difficulties and use this estimate to infer the top-two answers.\n\nWe also remark that there are some recent attempts to model task difficulties (Khetan & Oh, 2016; Shah et al., 2020; Krivosheev et al., 2020; Shah & Lee, 2018; Bachrach et al., 2012; Li et al., 2019; Tian & Zhu, 2015). However, these works are either restricted to binary tasks (Khetan & Oh, 2016; Shah et al., 2020; Shah & Lee, 2018) or focus on grouping confusable classes (Krivosheev et al., 2020; Li et al., 2019; Tian & Zhu, 2015). Our result, on the other hand, applies to any set of multichoice tasks, where the choices of each task are not necessarily restricted to a fixed set of classes.\n\nNotation. For a vector x, xi represents the i-th component of x. For a matrix M , Mij refers to the (i, j)th entry of M . For any vector x, its (cid:96)2 and (cid:96)∞-norm are denoted by (cid:107)x(cid:107)2 and (cid:107)x(cid:107)∞, respectively. We follow the standard definitions of asymptotic notations, Θ(·), O(·), o(·), and Ω(·).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 MODEL AND PROBLEM SETUP\n\nWe consider a crowdsourcing model to infer the top-two most plausible answers among K choices for each task. There are n workers and m tasks. For each task j ∈ [m] := {1, . . . , m}, we denote the correct answer by gj ∈ [K] and the next plausible, or the most confusing answer by hj ∈ [K]. We call the pair (gj, hj) the top-two answers for task j ∈ [m]. Let p ∈ [0, 1]n and q ∈ (1/2, 1]m be parameters modeling the reliability of workers and difficulty of tasks, respectively. For every pair of (i, j), the j-th task is assigned to the i-th worker independently with probability s. We use a matrix A ∈ Rn×m to represent the responses of workers, where Aij = 0 if the j-th task is not assigned to the i-th worker, and if it is assigned, and Aij is equal to the received label. The distribution of Aij is specified by the worker reliability pi and task difficulty qj as follows:\n\nAij =\n\n \n\n\n\ngj, hj, each b ∈ [K]\\{gj, hj}, with prob. s (cid:0) 1−pi K\nwith prob. 1 − s. 0,\n\nwith prob. s (cid:0)piqj + 1−pi with prob. s (cid:0)pi(1 − qj) + 1−pi\n\n(cid:1) ,\n\nK\n\n(cid:1) ,\n\nK\n\n(cid:1) ,\n\n(1)\n\nHere pi stands for the reliability of the i-th worker, in giving the answer from the most plausible top two (gj, hj). If pi = 0, the worker is considered a spammer who provides random answers among K choices, and a larger value of pi indicates a higher worker reliability. The parameter qj represents the inherent difficulty of the task j in distinguishing between the top two answers: for an easy task, qj is closer to 1, and for a hard task, qj is closer to 1/2. We call qj the confusion probability. Our goal is to recover top-two answers (gj, hj) for all j ∈ [m] with high probability with the minimum possible sampling probability s. We assume that the model parameters (p, q) are unknown.\n\nWe propose the top-two model to reflect common attributes of public crowdsourcing datasets, summarized in Appendix §A. The most important observation is that the top-two answers dominate the overall answers, and only the second-dominating answer has an incidence rate comparable to that of the ground truth. In other words, the standard deviation in the incidence rate of the second dominating answer has an overlap with that of the ground truth, but not the third-, or fourth-dominating answers. This indicates that assuming a unique ‘confusing answer’ is sufficient to model the confusion stemming from task difficulty. More details are available in Appendix §A.\n\nij = −1 if 1 ≤ Aij ≤ k; A(k)\n\nBinary conversion. The K-ary task can be decomposed into (K − 1)-binary tasks (Karger et al., 2013): define A(k) for 1 ≤ k < K such that the (i, j)-th entry A(k) indicates whether the answer Aij is larger than k, i.e., A(k) ij = 0 if Aij = 0. We show that E[A(k)] is rank-1 and the singular value decomposition (SVD) of E[A(k)] can reveal the top-two answers {(gj, hj)}m Proposition 1. For every 1 ≤ k < K, the binary-mapped matrix A(k) ∈ {−1, 0, 1}n×m satisfies E[A(k)] − s(K−2k) Case I: gj > hj\n\n1n×m = 2sp(r(k))(cid:62), where r(k) = [r(k)\n\nij = 1 if k < Aij ≤ K; and A(k)\n\nj=1 and the confusion probability vector q.\n\nm ](cid:62) is defined as\n\n· · · r(k)\n\nK\n\nij\n\n1\n\nr(k)\n\nj\n\n:=\n\n \n\n\n\nk K\nk\n\nwhere k < hj;\n\nK − (1 − qj) where hj ≤ k < gj; K − 1\n\nwhere gj ≤ k,\n\nk\n\nCase II: gj < hj \n\n\nk K\nk\n\nr(k)\n\nj\n\n:=\n\nK − qj K − 1\n\nk\n\n\n\nwhere k < gj; where gj ≤ k < hj; where hj ≤ k.\n\nBy defining ∆r(k)\n\nj\n\n:= r(k)\n\nj − r(k−1)\n\nj\n\n∆r(k)\n\nj =\n\nfor k ∈ [K] with r(0) \n\n\n1\n\nj\n\nwhere k = gj, K − qj K − (1 − qj) where k = hj,\n\n1\n\n\n\n1 K\n\notherwise.\n\n:= 0 and r(K)\n\nj\n\n:= 0 for all j, we have\n\n(2)\n\nj\n\nhas its minimum at k = gj and the second smallest value at k = hj for qj ∈ (1/2, 1].\n\nNote that ∆r(k) If one can specify gj, the task difficulty qj can also be revealed from 1 . In the next section, we use this structure of r(k) for k ∈ [K] to infer the top two answers and the confusion probability.2\n\nK −∆r(gj )\n\nj\n\n2We assume that η\n\n√\n\n√\n\n(cid:107)r(k)(cid:107)2 = Θ(\n\n√\n\nn ≤ (cid:107)p(cid:107)2 ≤\n\nn for some η > 0, i.e., there are only o(n) spammers (pi = 0), and m) for every k ∈ [K], which can be easily satisfied except exceptional cases from equation 2.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Spectral Method for Initial Estimation (TopTwo1 Algorithm)\n\n1: Input: data matrix A1 ∈ {0, 1, . . . , K}n×m and parameter η > 0 where η n. 2: Randomly split (with equal probabilities) and convert A1 into binary matrices X (k) ∈\n\nn ≤ (cid:107)p(cid:107)2 ≤\n\n{−1, 0, 1}n×m and Y (k) ∈ {−1, 0, 1}n×m for 1 ≤ k < K as described in Sec. 3.1.\n\n3: Let u(k) be the leading normalized left singular vector of X (k). Trim the abnormally large n and denote the resulting vector as ̃u(k). s(cid:48) (Y (k))(cid:62) ̃u(k). Assume v(0) := 0 and v(K) :=\n\ncomponents of u(k) by letting it be zero if u(k) 4: Calculate the estimate of (cid:107)p(cid:107)r(k) by v(k) := 1\n\ni > 2\n\n√\n\nη\n\n√\n\n√\n\n5: For k ∈ [K], calculate ∆v(k)\n\n:= v(k)\n\nj − v(k−1)\n\nj\n\n. Estimate the top-two answers for j ∈ [m] by\n\nj\n\n0.\n\nˆgj := arg min\n\nk∈[K]\n\n∆v(k)\n\nj\n\n;\n\nˆhj := arg min\n\nk(cid:54)=ˆgj ,k∈[K]\n\n∆v(k)\n\nj\n\n.\n\n6: Estimate (cid:107)p(cid:107)2 by defining lj := K 7: Estimate qj for j ∈ [m] by defining\n\nK−2\n\n(cid:80)\n\nk(cid:54)=ˆgj ,k(cid:54)=ˆhj\n\n∆v(k)\n\nj\n\nand l := 1\n\nm\n\n(cid:80)m\n\nj=1 lj.\n\nˆqj := 1/K − ∆v(ˆgj )\n\nj\n\n/l.\n\n(3)\n\n(4)\n\n8: Output: estimated top-two answers {(ˆgj, ˆhj)}m\n\nj=1 and confusion probability vector ˆq.\n\n3 PROPOSED ALGORITHM\n\nOur algorithm consists of two stages. In Stage 1, we compute an initial estimate on top-two answers and the confusion probability q. In Stage 2, we estimate the worker reliability vector p by using the result of the first stage, and use the estimated p and q to refine our estimates for the top two answers. Assume that we randomly split the original response matrix A into A1 and A2 with probability s1 and 1 − s1, respectively, and use only A1 for stage 1 and (A1, A2) for stage 2.\n\n3.1 STAGE 1: INITIAL ESTIMATES USING SVD\n\nThe first stage begins with randomly splitting A1 again into two independent matrices B and C with equal probabilities. We then convert B and C into (K − 1)-binary matrices B(k) and C(k) as explained in Sec. 2. Define X (k) and Y (k) as X (k) := B(k) − s(cid:48)(K−2k) 1n×m and Y (k) := C(k) − s(cid:48)(K−2k) 1n×m for s(cid:48) = s · s1/2. We have E[X (k)] = E[Y (k)] = s(cid:48)p(r(k))(cid:62) from Prop. 1.\n\nK\n\nK\n\nWe use X (k) and Y (k) to estimate p∗ := p/(cid:107)p(cid:107)2 and (cid:107)p(cid:107)2r(k), respectively. The estimators are denoted by u(k) and v(k), respectively. We define u(k) as the left singular vector of X (k) with the largest singular value. Sign ambiguity of the singular vector is resolved by defining u(k) as the one between {u(k), −u(k)} in which at least half of the entries are positive. After trimming abnormally large components of u(k) and defining the trimmed vector as ̃u(k), we calculate v(k) := 1\ns(cid:48) (Y (k))(cid:62) ̃u(k), which is an estimate for (cid:107)p(cid:107)2r(k). By using v(k) for 1 ≤ k < K, we get estimates for top-two answers (ˆgj, ˆhj) based on the observation in equation 2. Lastly, we estimate (cid:107)p(cid:107)2 and use v(k)/(cid:107)p(cid:107)2 ≈ r(k) to estimate the confusion probability vector q. See Algorithm 1 for details.\n\n3.2 STAGE 2: PLUG-IN MAXIMUM LIKELIHOOD ESTIMATOR (MLE)\n\nThe second stage uses the result of Stage 1 to estimate the worker reliability vector p. We first propose an estimate for the worker reliability vector p by using the estimated top-two answers {(gj, hj)}m j=1 from Algorithm 1. We randomly split the original response matrix A into A1 and A2 with probability s1 and 1 − s1, respectively, and use A1 only for Algorithm 1 and A2 only for calculating the estimator ˆp. Our estimate for the worker reliability pi is defined as \n\n\n\n\n\n\n\nˆpi =\n\nK (K − 2)\n\n\n\n1 s(1 − s1)\n\n1 m\n\n\n\n1(A2\n\nij = ˆgj or ˆhj)\n\n −\n\n .\n\n(5)\n\n2 K\n\nm (cid:88)\n\nj=1\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Plug-in MLE (TopTwo2 Algorithm)\n\n1: Input: data matrix A ∈ {0, 1, . . . , K}n×m and the sample splitting rate s1 > 0. 2: Randomly split A into A1 and A2 by defining A1 := A ◦ S and A2 = A ◦ (1n×m − S) where S is an n × m matrix whose entries are i.i.d. with Bern(s1) and ◦ is an entrywise product.\n\n3: Apply Algorithm 1 to A1 to yield estimates for top-two answers {(ˆgj, ˆhj)}m\n\nj=1 and confusion\n\nprobability vector ˆq. 4: By using {(ˆgj, ˆhj)}m 5: By using the whole A and ( ˆp, ˆq), find the plug-in MLE estimates (ˆgMLE\n\nj=1 and A2, calculate the estimate ˆp as in equation 5.\n\n, ˆhMLE\n\nj\n\n) by\n\nj\n\narg max a,b∈[K]2,a(cid:54)=b\n\nn (cid:88)\n\ni=1\n\nlog\n\n(cid:18) K ˆpi ˆqj 1 − ˆpi\n\n(cid:19)\n\n+ 1\n\n1(Aij = a) + log\n\n(cid:18) K ˆpi(1 − ˆqj) 1 − ˆpi\n\n(cid:19)\n\n+ 1\n\n1(Aij = b).\n\n(6)\n\n6: Output: estimated top-two answers {(ˆgMLE\n\nj\n\n, ˆhMLE\n\nj\n\n)}m\n\nj=1.\n\nthe estimated ( ˆp, ˆq)\n\nOur plug-in MLE uses MLE, which finds (ˆgj, ˆhj) ∈ [K]2\\{(1, 1), (1, 2), . . . , (K, K)} such that (ˆgj, ˆhj) arg max(a,b)∈[K]2,a(cid:54)=b\n\nthe oracle := i=1 log P(Aij|p, qj, (a, b)) as in equation 6. Details can be found in Alg.2. The time complexity of Alg. 2 is O(m2 log m + nmK 2), since the SVD in Alg. 1 can be computed via power iterations within O(m2 log m) steps (Boutsidis et al., 2015), and the step for finding the pair of answers maximizing equation 6 requires O(nmK 2) steps.\n\nin the place of\n\n(p, q) at\n\n(cid:80)n\n\n4 PERFORMANCE ANALYSIS\n\nTo state our main theoretical results, we first need to introduce some notation and assumptions. Let μ(i,j) (a,b),k denote the probability that a worker i ∈ [n] gives label k ∈ [K] for the assigned task j ∈ [m] of which the top-two answers are (gj, hj) = (a, b). Note that μ(i,j) (a,b),k can be written in terms of (pi, qj) from the distribution in equation 1 for every a, b, k ∈ [K]3. Let μ(i,j) (a,b) = [μ(i,j) (a,b),K](cid:62). We introduce a quantity that measures the average ability of workers in distinguishing the ground-truth pair of top-two answers (gj, hj) from any other pair (a, b) ∈ [K]2/{(gj, hj)} for the task j ∈ [m]. We define\n\n(a,b),1 μ(i,j)\n\n· · · μ(i,j)\n\n(a,b),2\n\n(j)\n\nD\n\n:=\n\nmin (gj ,hj )(cid:54)=(a,b)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:16)\n\nDKL\n\nμ(i,j)\n\n(gj ,hj ), μ(i,j)\n\n(a,b)\n\n(cid:17)\n\n; D := min j∈[m]\n\nD\n\n(j)\n\n,\n\n(7)\n\n(j)\n\nwhere DKL(P, Q) := (cid:80) i P (i) log(P (i)/Q(i)) is the KL-divergence between P and Q. Note that D\nis strictly positive if there exist at least one worker i with pi > 0 and qj ∈ (1/2, 1) for the distribution in equation 1, so that (gj, hj) can be distinguished from any other (a, b) ∈ [K]2/{(gj, hj)} statistically. We define D as the minimum of D over j ∈ [m], indicating the average ability of workers in distinguishing (gj, hj) from any other (a, b) for the most difficult task in the set of tasks.\n\n(j)\n\nWe split the performance analysis of our algorithm into two parts. First, Theorem 1 states the performance guarantees for Alg. 1. Theorem 1 (Performance Guarantees for Algorithm 1). For any (cid:15), δ1 > 0, if the sampling probability s · s1 = Ω , Algorithm 1 guarantees the recovery of the ordered top-two answers (gj, hj) with probability at least 1 − (cid:15) for any j ∈ [m] with qj ∈ (1/2, 1), i.e., (cid:17)\n\n(cid:16) 1 1 (cid:107)p(cid:107)2 δ2\n\nlog K (cid:15)\n\n(cid:17)\n\n(cid:16)\n\n2\n\n≥ 1 − (cid:15)\n\nfor all j ∈ [m] with qj ∈ (1/2, 1),\n\n(8)\n\nP\n\n(ˆgj, ˆhj) = (gj, hj)\n\nand the recovery of the confusion probability qj with\n\nfor every sufficiently large number m of tasks and the number of workers n = O(m/ log m).\n\nP (|ˆqj − qj| < δ1) ≥ 1 − (cid:15)\n\nfor all j ∈ [m],\n\n(9)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nBy using Theorem 1, we can also find the sufficient conditions to guarantee the recovery of paired top-two answers for all tasks and q with an arbitrarily small (cid:96)∞-norm error.\n\nCorollary 1. For any (cid:15), δ1 > 0, if the sampling probability s·s1 = Ω guarantees the recovery of {(gj, hj)}m\n\nj=1 and q with probability at least 1 − (cid:15) as m → ∞ such that\n\n(cid:16) 1 1 (cid:107)p(cid:107)2 δ2\n\n2\n\n(cid:17)\n\nlog mK\n\n(cid:15)\n\n, Algorithm 1\n\n(cid:16)\n\nP\n\n(ˆgj, ˆhj) = (gj, hj), ∀j ∈ [m]\n\n≥ 1 − (cid:15)\n\nand\n\nP ((cid:107)q − ˆq(cid:107)∞ < δ1) ≥ 1 − (cid:15).\n\n(10)\n\n(cid:17)\n\nProofs of Theorem 1 and Corollary 1 are available in Appendix §G.\n\nWe next analyze the performance of Alg. 2, which uses Alg. 1 as the first stage. Before providing the main theorem for Alg. 2, we state a lemma charactering a sufficient condition for estimating the worker reliability vector p from equation 5 with an arbitrarily small (cid:96)∞-norm error. Lemma 1. Conditioned on (ˆgj, ˆhj) = (gj, hj) for all j ∈ [m], if s(1 − s1) = Ω , the estimator ˆpi defined in equation 5 of Alg. 2 guarantees P ((cid:107)p − ˆp(cid:107)∞ < δ2) ≥ 1 − (cid:15) for any (cid:15) > 0.\n\n2 m log n\n\n(cid:16) 1 δ2\n\n(cid:17)\n\n(cid:15)\n\nCombining Corollary 1 and Lemma 1, we can have the estimators ( ˆp, ˆq) for the worker reliability vector p and the confusion probability vector q with (cid:96)∞-norm error bounded by any arbitrarily small δ > 0 with probaiblity at least 1 − 2(cid:15) if\n\ns = s · s1 + s(1 − s1) = Ω\n\n(cid:18) log(mK/(cid:15)) δ2(cid:107)p(cid:107)2 2\n\n+\n\nlog(n/(cid:15)) δ2m\n\n√\n\n(cid:19)\n\n= Ω\n\n(cid:19)\n\n(cid:18) log(mK/(cid:15)) δ2(cid:107)p(cid:107)2 2\n\n(11)\n\nn) and n = O(m/ log m). In this where the last equality is from the assumption that (cid:107)p(cid:107)2 = Θ( regime, the sample complexity for estimating the task difficulty q is larger than that for estimating worker reliability p. To make sure that the sampling probability s < 1, we need n = Ω(log m).\n\nOur second theorem, Theorem 2, characterizes the sufficient condition on the sampling probability s to guarantee the recovery of the pair of top-two answers for all tasks by equation 6 of Alg. 2, when a sufficiently accurate estimation of (p, q) is given. Theorem 2. Assume that there is a positive scalar ρ such that μ(i,j) [n] × [m] × [K]3. For any (cid:15) > 0, if ( ˆp, ˆq) are given with\n\n(gj ,hj ),c ≥ ρ for all (i, j, gj, hj, c) ∈\n\nmax{(cid:107)p − ˆp(cid:107)∞, (cid:107)q − ˆq(cid:107)∞} ≤ δ := min\n\n(cid:26) ρ 4\n\n,\n\nρD\n\n4(6 + D)\n\n(cid:27)\n\n,\n\n(12)\n\nand the sampling probability s = Ω mates of {(gj, hj)}m\n\nj=1 from equation 6 of Algorithm 2 guarantees\n\n(cid:16) log(1/ρ) log(mK2/(cid:15))+D log(m/(cid:15)) nD\n\n(cid:17)\n\n, then for any (cid:15) > 0 the esti-\n\n(cid:16)\n\nP\n\n(ˆgj, ˆhj) = (gj, hj), ∀j ∈ [m]\n\n(cid:17)\n\n≥ 1 − (cid:15).\n\n(13)\n\nProofs of Lemma 1 and Theorem 2 are available in Appendix §H. The assumption in Theorem 2 that μ(i,j) (gj ,hj ),c ≥ ρ for some ρ > 0 holds if pi < 1 for all i ∈ [n], i.e., there is no perfectly reliable worker. This assumption can be easily satisfied by adding an arbitrary small random noise to the worker answers as well. By combining the statements in Corollary 1, Lemma 1, and Theorem 2 with δ1 = δ2 = δ for δ defined in equation 12, we get the overall performance guarantee for Alg. 2. Corollary 2 (Performance Guarantees for Alg. 2). Alg. 2 guarantees the recovery of top-two answers for all tasks with P\n\n≥ 1 − (cid:15) for any (cid:15) > 0 if s satisfies\n\n(ˆgj, ˆhj) = (gj, hj), ∀j ∈ [m]\n\n(cid:16)\n\n(cid:17)\n\ns = Ω\n\n(cid:18) log(mK/(cid:15)) δ2(cid:107)p(cid:107)2 2\n\n+\n\nlog(1/ρ) log(mK 2/(cid:15)) + D log(m/(cid:15))\n\nnD\n\n(cid:19)\n\n= Ω\n\n(cid:18) log(m/(cid:15)) δ2(cid:107)p(cid:107)2 2\n\n+\n\nlog(m/(cid:15))\n\nnD\n\n(cid:19)\n\n.\n\n(14)\n\nIn equation 14, the first term is for guaranteeing accurate estimates of p and q with (cid:96)∞-norm error bounded by δ and the second term is for guaranteeing the recovery of the top-two answers from MLE with high probability. Since (cid:107)p(cid:107)2 2 = Θ(n), the two terms effectively have the same order but with different constant scaling, depending on model-specific parameters (p, q).\n\nLastly, we show the optimality of convergence rates of Alg. 1 and Alg. 2 with respect to two types of minimax errors, respectively. The proof of Theorem 3 is available in Appendix §I.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 3. (a) Let Fp be a set of p ∈ [0, 1]n such that the collective quality of workers, measured by (cid:107)p(cid:107)2, is parameterized by p as F ̄p := {p : 1 n (cid:107)p(cid:107)2 2 = p}. Assume that p ≤ 2/3. If the average number ns of samples (queries) per task is less than (1/2p) log(1/(cid:15)), then\n\nmin ˆg\n\nmax p∈Fp, g∈[K]m\n\n1 m\n\n(cid:88)\n\nj∈[m]\n\nP(ˆgj (cid:54)= gj) ≥ (cid:15).\n\n(15)\n\n(b) There is a universal constant c > 0 such that for any p ∈ [0, 1]n, q ∈ (1/2, 1]m, if the sampling probability s < Ω (cid:0)1/(nD)(cid:1), then\n\nmin (ˆg,ˆh)\n\nmax (g,h)∈[K]m×[K]m gj (cid:54)=hj ,∀j[m]\n\n1 m\n\n(cid:88)\n\nj∈[m]\n\nP((ˆgj, ˆhj) (cid:54)= (gj, hj)) ≥ c.\n\n(16)\n\nFrom part (a) of Theorem 3, it is necessary to have s > Ω (cid:0)(1/(cid:107)p(cid:107)2 2) log(1/(cid:15))(cid:1) to make the minimax error in equation 15 less than (cid:15). Since Theorem 1 shows that Alg. 1 recovers (ˆgj, ˆhj) with probability at least 1 − (cid:15) if s > Ω (cid:0)(1/(cid:107)p(cid:107)2 2) log(1/(cid:15))(cid:1) when s1 = 1, we can conclude that Alg. 1 achieves the minimax optimal rate for a fixed collective intelligence of workers, measured by (cid:107)p(cid:107)2. From part (b) of Theorem 3, for any (p, q), unless we have s > Ω(1/(nD)) there always exists a constant fraction of tasks for which the recovered top-two answers are incorrect. This bound matches with our sufficient condition on s from Alg. 2 in equation 14 upto logarithmic factors, as long as δ2(cid:107)p(cid:107)2 (cid:38) nD, showing the minimax optimality of our Alg. 2 for any (p, q). More discussions on the theoretical results are available at Appendix §E.\n\n5 EXPERIMENTS\n\nWe evaluate the proposed algorithm under diverse scenarios of synthetic datasets in Sec. 5.1, and for two applications–in identifying difficult tasks in real datasets in Sec. 5.2 and in training neural network models with soft labels defined from the top-two plausible labels in Sec. 5.3.\n\n5.1 EXPERIMENTS ON SYNTHETIC DATASET\n\nWe compare the empirical performance of Alg. 1 and Alg. 2 (referred as TopTwo1 and TopTwo2) with other baselines: majority voting(MV), OTP-D&S and MV-D&S (Zhang et al., 2014), PGD (Ma et al., 2018), M-MSR (Ma & Olshevsky, 2020) and oracle-MLE, whose details can be found in Appx. §C. We choose these baselines since they have the strongest established guarantees in the literature and they are all MLE-based approaches, from which the top-two answers can be inferred. Obviously, oracle-MLE, which uses the ground-truth model parameters, provides the best possible performance. We devise four scenarios described in Table 1 to verify the robustness of our model for various (p, q) ranges, at (n, m) = (50, 500) with s ∈ (0, 0.2]. The number of choices for each task P((ˆgj, ˆhj) (cid:54)= (gj, hj)) averaged is fixed as 5. Fig. 1 reports the empirical error probability 1 over 30 runs, with 95% confidence intervals (shaded region). Four columns correspond to the four scenarios, resp. The prediction errors for gj and hj are plotted in Fig. 6 of Appx. §D.1.\n\n(cid:80)m\n\nj=1\n\nm\n\nWe can observe that for all the considered scenarios TopTwo2 achieves the best performance, near the oracle MLE, in recovering (gj, hj). Depending on the scenarios, the reason TopTwo2 outperforms can be explained differently. For the Easy scenario, since qj is close to 1, it is easy to distinguish gj from hj but hard to distinguish hj from other labels. Our algorithm achieves the best\n\nTable 1: Parameters for synthetic data experiments under diverse scenarios.\n\nEasy\n\nHard\n\nFew-smart\n\nHigh-variance\n\nWorker\n\npi ∈ [0, 1]\n\npi ∈ [0, 1]\n\n90% pi ∈ [0, 0.1] 10% pi ∈ [0.9, 1]\n\npi ∈ [0, 1]\n\nTask\n\nqj ∈ [0.9, 1]\n\nqj ∈ (0.5, 0.6]\n\nqj ∈ (0.5, 1]\n\n50% qj ∈ (0.5, 0.6] 50% qj ∈ [0.9, 1.0]\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Prediction error for (g, h) for four scenarios as the avg. number of queries per task changes. Our TopTwo2 alg. achieves the best performance, near the oracle MLE for all the scenarios.\n\nperformance in estimating hj by a large margin (Fig. 6). For the Hard scenario, it is hard to distinguish gj and hj, but our algorithm, which uses an accurate ˆqj, can better distinguish gj and hj. For Few-smart, our algorithm achieves the biggest gain compared to other methods, since our algorithm can effectively distinguish few smart workers from spammers. High-variance shows the effect of having diverse qj in a dataset. We remark that our algorithm achieves the best performance, near that of the oracle-MLE, for all the scenarios, while the next performer keeps changing depending on scenarios. For example, the OPT D&S is the second best performer in the Easy scenario, while it is the worst performer in the Few-smart scenario. We also show the robustness of our algorithm against changes in model parameters in Appendix §D.\n\n5.2 EXPERIMENTS ON REAL-WORLD DATASET: INFERRING TASK DIFFICULTIES\n\nWe next provide experimental results using real-world data collected from MTurk and show that our algorithm can be used for inferring task difficulties. We devised a color comparison task where we asked the crowd to choose a color, among six given choices, that looks the most similar to a reference color of each task. See Fig. 4 in Appx. §A.1 for example tasks. After randomly generating a reference color and the six choices, we identified the ground truth and the most confusing answer for each task by measuring the distance between colors using the CIEDE2000 color difference formula (Sharma et al., 2005). If the distance from the reference color to the ground truth is much shorter than that to the most confusing answer, then the task was considered easy. We designed 1000 tasks and distributed it to 200 workers, collecting 19.5 responses on each task. After collecting the data, we subsampled it to simulate how the prediction error decreases as the number of responses per task increases. Fig. 2a shows the performances in detecting (gj, hj), gj and hj, averaged over 10 times of random sampling, with 95% confidence interval (shaded region). TopTwo2 algorithm achieved the best performance in detecting (gj, hj), gj and hj in all ranges. We further examined the correlation between the task difficulty - quantified by the distance gap between the ground truth and the most\n\n(a) The average prediction error on color comparison tasks\n\n(b) Histogram of dist. gap\n\nFigure 2: (a) Prediction error for (gj, hj), gj and hj (from left to right) for color comparison tasks using real data collected from MTurk. Our TopTwo2 algorithm achieves the best performance. (b) Histogram of color distance gap for the task groups with the highest qj (easiest tasks) and lowest qj (most difficult tasks). The difficult task group (blue) tends to have a smaller color distance gap.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nconfusing answer from the reference color - and the estimated confusion probability qj across tasks. We selected top 50 most difficult/easiest tasks according to the estimated confusion probability qj and plotted the histograms of the distance gap for the two groups in Fig 2b. We can see that the difficult group (blue, having lowest qj) tends to have a smaller distance gap than those of the easy task group (red). This result shows that our algorithm can identify difficult tasks in real datasets.\n\n5.3 TRAINING NEURAL NETWORKS WITH SOFT LABELS HAVING TOP-TWO INFORMATION\n\nAn appealing example where we can use the knowledge of the second best answer is in training deep neural networks for classification tasks. Traditionally, a hard label (one ground-truth label per image) has been used to train a classifier. In recent works, it has been shown that using a soft label (full label distribution that reflect human perceptual uncertainty) is sometimes beneficial in obtaining a model with better generalization capability (Peterson et al., 2019). However, obtaining an accurate full label distribution requires much higher sample complexity than recovering only the groundtruth. For example, Peterson et al. (2019) provided a CIFAR10H dataset with full human label distributions for 10000 instances of CIFAR10 test examples by collecting on average 50 judgements per image, which is about 5-10 times larger than those of usual datasets (Table 4 in Appendix A.1).\n\nOur top-two model, on the other hand, can effectively reduce the required sample complexity, while still guaranteeing the advantages in training models with soft labels. To demonstrate this idea, we trained two deep neural networks, VGG-19 and ResNet18, with the soft-label vectors having the top-two structure (top2) for CIFAR10H dataset3. We then compared the training/test results with those of the hard label (hard) and full label distribution (full). Experimental details are in Appendix §B. Compared to the original training with hard labels, training with top-two soft labels achieved 1.56% and 4.09% higher test accuracy in VGG-19 and ResNet18, respectively (averaged in three runs, 150 epochs) as shown in Table 2, which is even higher than that of the full label distribution in VGG-19. This result shows that training with the top-two soft labels results in better generalization (test accuracy) than training with hard labels, since the top-two soft label includes simple yet helpful side information, the most confusable class and the confusion probability.\n\nTable 2: Comparison of performances for CIFAR10H dataset with hard/soft label training\n\nNetwork\n\nTrain accuracy\n\nTraining loss\n\nTest accuracy\n\nTest loss\n\nVGG-19 (hard) VGG-19 (top2) VGG-19 (full)\n\n97.46±0.59% 0.081±0.012 0.231±0.014 97.00±0.51% 0.282±0.010 96.69±0.48%\n\n1.057±0.118 77.64±1.54% 79.20±1.04% 0.754±0.050 0.740±0.030 78.66±0.97%\n\nResNet18 (hard) ResNet18 (top2) ResNet18 (full)\n\n98.47±0.320% 0.046±0.009 98.67±0.491% 0.168±0.024 99.19±0.125% 0.189±0.023\n\n76.49%±1.80% 1.275±0.157 80.58%±2.36% 0.640±0.093 80.93%±2.66% 0.611±0.102\n\n6 DISCUSSION\n\nWe proposed a new model for multiple-choice crowdsourcing, with top-two confusable answers and varying confusion probability over tasks. We provided an algorithm to infer the top-two answers and the confusion probability. This work can benefit several query-based data acquisition systems such as MTurk or review systems by providing additional information about the task such as the most plausible answer other than the ground truth and how plausible it is, which can be used to quantify the accuracy of the ground truth or to classify the tasks based on difficulty. The topic of confusion is getting increasing attention in the machine learning community for designing reliable classifiers (Jin et al., 2017; Luque et al., 2019; Chang et al., 2017). We also demonstrated possible applications of our algorithm in designing soft labels for better generalization of neural networks.\n\n3As in (Peterson et al., 2019), we used the original 10000 test examples of CIFAR10 for training and 50000 training examples for testing. Thus, the final accuracy is lower than usual. Since CIFAR10H is collected from selected ‘reliable’ workers who answered a set of test examples with an accuracy higher than 75%, we directly used the top-two dominating answers and the fraction between the two in designing the soft label vector (top2).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nBahadir Ismail Aydin, Yavuz Selim Yilmaz, Yaliang Li, Qi Li, Jing Gao, and Murat Demirbas. In Proceedings of the Twenty-Eighth\n\nCrowdsourcing for multiple-choice question answering. AAAI Conference on Artificial Intelligence, pp. 2946–2953, 2014.\n\nYoram Bachrach, Thore Graepel, Tom Minka, and John Guiver. How to grade a test without knowing the answers—a bayesian graphical model for adaptive crowdsourcing and aptitude testing. arXiv preprint arXiv:1206.6386, 2012.\n\nAfonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of random\n\nmatrices with independent entries. The Annals of Probability, 44(4):2479–2506, 2016.\n\nDaniel Berend and Aryeh Kontorovich. Consistency of weighted majority votes. Advances in Neural\n\nInformation Processing Systems, 27, 2014.\n\nChristos Boutsidis, Prabhanjan Kambadur, and Alex Gittens. Spectral clustering via the power method-provably. In International conference on machine learning, pp. 40–48. PMLR, 2015.\n\nJoseph Chee Chang, Saleema Amershi, and Ece Kamar. Revolt: Collaborative crowdsourcing for labeling machine learning datasets. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pp. 2334–2346, 2017.\n\nNilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. Aggregating crowdsourced In Proceedings of the 22nd international conference on World Wide Web, pp.\n\nbinary ratings. 285–294, 2013.\n\nAlexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics), 28 (1):20–28, 1979.\n\nGianluca Demartini, Djellel Eddine Difallah, and Philippe Cudr ́e-Mauroux. Zencrowd: leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking. In Proceedings of the 21st international conference on World Wide Web, pp. 469–478, 2012.\n\nChao Gao and Dengyong Zhou. Minimax optimal convergence rates for estimating ground truth\n\nfrom crowdsourced labels. arXiv preprint arXiv:1310.5764, 2013.\n\nChao Gao, Yu Lu, and Dengyong Zhou. Exact exponent in optimal rates for crowdsourcing. In\n\nInternational Conference on Machine Learning, pp. 603–611. PMLR, 2016.\n\nArpita Ghosh, Satyen Kale, and Preston McAfee. Who moderates the moderators? crowdsourcing abuse detection in user-generated content. In Proceedings of the 12th ACM conference on Electronic commerce, pp. 167–176, 2011.\n\nShahana Ibrahim, Xiao Fu, Nikolaos Kargas, and Kejun Huang. Crowdsourcing via pairwise cooccurrences: Identifiability and algorithms. Advances in neural information processing systems, 32, 2019.\n\nPanagiotis G Ipeirotis, Foster Provost, and Jing Wang. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD workshop on human computation, pp. 64–67, 2010.\n\nRuochun Jin, Yong Dou, Yueqing Wang, and Xin Niu. Confusion graph: detecting confusion communities in large scale image classification. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 1980–1986, 2017.\n\nDavid Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing sys-\n\ntems. Advances in neural information processing systems, 24, 2011.\n\nDavid R Karger, Sewoong Oh, and Devavrat Shah. Efficient crowdsourcing for multi-class labelIn Proceedings of the ACM SIGMETRICS/international conference on Measurement and\n\ning. modeling of computer systems, pp. 81–92, 2013.\n\nDavid R Karger, Sewoong Oh, and Devavrat Shah. Budget-optimal task allocation for reliable\n\ncrowdsourcing systems. Operations Research, 62(1):1–24, 2014.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAshish Khetan and Sewoong Oh. Achieving budget-optimality with adaptive schemes in crowd-\n\nsourcing. Advances in Neural Information Processing Systems, 29:4844–4852, 2016.\n\nEvgeny Krivosheev, Siarhei Bykau, Fabio Casati, and Sunil Prabhakar. Detecting and preventing confused labels in crowdsourced data. Proceedings of the VLDB Endowment, 13(12):2522–2535, 2020.\n\nHongwei Li and Bin Yu. Error rate bounds and iterative weighted majority voting for crowdsourcing.\n\narXiv preprint arXiv:1411.4086, 2014.\n\nYuan Li, Benjamin Rubinstein, and Trevor Cohn. Exploiting worker correlation for label aggregation In International conference on machine learning, pp. 3886–3895. PMLR,\n\nin crowdsourcing. 2019.\n\nChao Liu and Yi-Min Wang. Truelabel + confusions: A spectrum of probabilistic models in analyzing multiple ratings. In Proceedings of the 29th International Conference on Machine Learning, ICML, 2012.\n\nQiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. Advances in\n\nneural information processing systems, 25, 2012.\n\nYusha Liu, Yichong Xu, Nihar B Shah, and Aarti Singh. Integrating rankings into quantized scores\n\nin peer review. arXiv preprint arXiv:2204.03505, 2022.\n\nAmalia Luque, Alejandro Carrasco, Alejandro Mart ́ın, and Ana de Las Heras. The impact of class imbalance in classification performance metrics based on the binary confusion matrix. Pattern Recognition, 91:216–231, 2019.\n\nQianqian Ma and Alex Olshevsky. Adversarial crowdsourcing through robust rank-one matrix com-\n\npletion. Advances in Neural Information Processing Systems, 33:21841–21852, 2020.\n\nYao Ma, Alexander Olshevsky, Csaba Szepesvari, and Venkatesh Saligrama. Gradient descent for sparse rank-one matrix completion for crowd-sourced aggregation of sparsely interacting workers. In International Conference on Machine Learning, pp. 3335–3344. PMLR, 2018.\n\nJungseul Ok, Sewoong Oh, Jinwoo Shin, and Yung Yi. Optimality of belief propagation for crowdsourced classification. In International Conference on Machine Learning, pp. 535–544. PMLR, 2016.\n\nJoshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human In Proceedings of the IEEE/CVF International\n\nuncertainty makes classification more robust. Conference on Computer Vision, pp. 9617–9626, 2019.\n\nVikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. Journal of machine learning research, 11(4), 2010.\n\nDevavrat Shah and Christina Lee. Reducing crowdsourcing to graphon estimation, statistically. In\n\nInternational Conference on Artificial Intelligence and Statistics, pp. 1741–1750, 2018.\n\nNihar B Shah, Sivaraman Balakrishnan, and Martin J Wainwright. A permutation-based model for crowd labeling: Optimal estimation and robustness. IEEE Transactions on Information Theory, 67(6):4162–4184, 2020.\n\nGaurav Sharma, Wencheng Wu, and Edul N Dalal. The ciede2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations. Color Research & Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation, Colour Society of Australia, Centre Franc ̧ais de la Couleur, 30(1):21–30, 2005.\n\nPadhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. Inferring ground truth from subjective labelling of venus images. Advances in neural information processing systems, 7, 1994.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nIvan Stelmakh, Nihar B Shah, and Aarti Singh. Peerreview4all: Fair and accurate reviewer assign-\n\nment in peer review. In Algorithmic Learning Theory, pp. 828–856. PMLR, 2019.\n\nTian Tian and Jun Zhu. Max-margin majority voting for learning from crowds. Advances in neural\n\ninformation processing systems, 28, 2015.\n\nPeter Welinder, Steve Branson, Pietro Perona, and Serge Belongie. The multidimensional wisdom\n\nof crowds. Advances in neural information processing systems, 23, 2010.\n\nYuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. Spectral methods meet em: A provably optimal algorithm for crowdsourcing. Advances in neural information processing systems, 27, 2014.\n\nDengyong Zhou, Sumit Basu, Yi Mao, and John Platt. Learning from the wisdom of crowds by\n\nminimax entropy. Advances in neural information processing systems, 25, 2012.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA VERIFICATION FOR THE PROPOSED TOP-TWO MODEL\n\nWe proposed the top-two model to reflect the key attributes of seven datasets including Adult2, Dog, Web, Flag, Food, Plot, and Color, of which the details are summarized in Appendix A.1.\n\nTable 3 shows empirical distributions of the mean incidence of responses for the top-three dominating answers, sorted by the dominance proportions, for the six public datasets and the Color dataset that we collected, with the standard deviation over the tasks in the dataset. In Fig. 3, we also plot empirical distributions of the mean incidence of responses sorted by the dominant proportion with error bars indicating the standard deviation. The i-th data point represents the average incidence of the i-th highest response in each task. For example, in Adult2 dataset, the most dominating answer takes 0.8 portion of the total answers, and the next dominating answer takes 0.14 portion of the total answers on average.\n\nTable 3: Proportions of top-three dominating answers in public datasets\n\nDataset Ground truth\n\n2nd dominating answer\n\n3rd dominating answer\n\nAdult2 Dog Web Flag Food Plot Color\n\n0.80±0.19 0.76±0.15 0.59±0.20 0.90±0.16 0.80±0.18 0.62±0.21 0.43±0.1\n\n0.14±0.13 0.22±0.14 0.25±0.12 0.09±0.13 0.17±0.15 0.30±0.16 0.23±0.06\n\n0.04±0.07 0.01±0.04 0.12±0.09 0.01±0.03 0.02±0.05 0.06±0.07 0.15±0.05\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n(f)\n\n(g)\n\nFigure 3: Empirical distribution of the mean incidence of responses sorted by the dominant proportion, averaged over all tasks in each dataset. The i-th data point represents the average incidence of the i-th highest response in each task. The error bars indicate the standard deviation of the mean incidence of the i-th dominating answer over the tasks in the dataset.\n\nFrom the table and figure, we can observe that for all the considered public datasets the top-two answers dominate the overall answers, i.e., about 65-90% of the total answers belong to the top two. Furthermore, the average ratio from the most dominating answer to the second one is 4:1, while that between the second and the third is 7.5:1. There often exist overlaps in the error bars between\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n(a) gj = 6 and hj = 5\n\n(b) gj = 4 and hj = 3\n\n(c) gj = 5 and hj = 3\n\n(d) gj = 6 and hj = 2\n\nFigure 4: Example tasks for ‘Color’ dataset where the ground truth g and the most confusing answer h are determined by the color distance from the reference color (top).\n\nthe ground truth and the second dominating answer, e.g., for Web, Plot, and Color datasets, but no such overlap is found between the ground truth and the third dominating answer. What we can call a ‘confusing answer’ is an answer that has an incidence rate comparable to that of the ground truth. In all the considered datasets, only the second dominating answer shows such a tendency, and thus, we can conclude that the third dominating answer cannot be called a ‘confusing answer’, and the top-two model in equation 1 well describes the errors in answers caused by confusion.\n\nMoreover, from the public datasets, we also observe that the task difficulty can be quantified by the confusion probability between the top-two answers. As an example, for the Web dataset, when we select the easiest 500 tasks and hardest 500 tasks by ordering tasks with the ratio of correct answers, the ratio between the ground-truth to the 2nd best answer was 10.7:1 for the easiest group, while it was 1.5:1 for the hardest group. This observation shows that the ratio between the top-two answers indeed captures task difficulty as does our model parameter for task difficulty qj in equation 1.\n\nA.1 DATASETS\n\nWe collect six publicly available multi-class datasets: Adult2, Dog, Web, Flag, Food and Plot. Since these datasets do not provide information about the most confusing answer or the task difficulty, we additionally create a new dataset called ‘Color’, for which we can identify the most confusing answer and also quantify the task difficulty for all the included tasks.\n\n• Color is a dataset where the task is to find the most similar color to the reference color among six different choices. For each task, we randomly create a reference color and then choose six choices of colors. The distance from the reference color to the ground truth color is in between 4.5 and 5.5, the distance to the most confusing answer is in between 5.5 and 6.5, and the distance to the rest of the choices is between 11 and 12, where the distance between the pairs of colors is measured by CIEDE2000 (Sharma et al., 2005) color difference formulation. The tasks are ordered in terms of their difficulty levels by measuring the gap between: the distance from the reference color to the ground truth; and that to the most confusing answer. If the distance from the reference color to the ground truth is much shorter than that to the most confusing answer, then the task is considered easy. Using MTurk, we collected 19600 labels from 196 workers for 1000 tasks. Each Human Intelligence Task (HIT) is composed of randomly selected 100 tasks, and we pay $1 to each worker who completed a HIT. Fig. 4 shows an example task for the Color dataset.\n\n• Adult2 (Ipeirotis et al., 2010) is a 4-class dataset where the task is to classify the web pages into four categories (G, PG, R, X) depending on the adult level of the websites. This dataset contains 3317 labels for 333 websites which are offered by 269 workers.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Images with lowest q (considered to be hard)\n\n(b) Images with highest q (considered to be easy)\n\nFigure 5: Training images with (a) lowest and (b) highest confusion probabilities.\n\n• Dog (Zhang et al., 2014) is a 4-class dataset where the task is to discriminate a breed (out of Norfolk Terrire, Norwich Terrier, Irish Wolfhound, and Scottich Deerhound) for a given dog. This dataset contains 7354 labels collected from 52 workers for 807 tasks.\n\n• Web (Zhou et al., 2012) is a 5-class dataset where the task is to determine the relevance of query-URL pairs with a 5-level rating (from 1 to 5). The dataset contains 15567 labels for the 2665 query-URL pairs offered by 177 workers.\n\n• Flag (Krivosheev et al., 2020) is a dataset for multiple-choice tasks where each task is to identify the country for a given flag from 10 given choices. A total of 1600 votes are collected from 220 workers for the 100 tasks.\n\n• Food (Krivosheev et al., 2020) is a dataset for multiple-choice tasks where each task asks to identify a picture of a given food or dish from 5 given choices. This dataset contains 1220 labels for 76 tasks collected from 177 workers.\n\n• Plot (Krivosheev et al., 2020) is a dataset for multiple-choice tasks where the task is to identify a movie from a description of its plot from 10 given choices. Only workers who correctly solved the first 10 test questions can answer the rest of the tasks. A total of 1937 labels are collected from 122 workers for 100 tasks.\n\nTable 4 shows a summarized information for the introduced datasets.\n\nTable 4: Dataset information\n\nDataset\n\n# workers\n\n# tasks\n\n# labels or choices\n\nsparsity\n\ndtask\n\ndworker\n\nAdult2 Dog Web Flag Food Plot Color\n\n269 109 176 220 177 122 196\n\n333 807 2653 100 54 56 1000\n\n4 4\n5 10 5\n10 6\n\n0.037 0.092 0.033 0.073 0.125 0.293 0.1\n\n10.0 10.0 5.9 16.0 22.1 35.7 19.5\n\n12.4 74.0 88.3 7.3 6.7 16.4 99.4\n\nB EXPERIMENTAL DETAILS FOR NEURAL NETWORK TRAINING IN SEC. 5.3\n\nWe show the details of the experiments in Sec. 5.3.\n\nB.1 DATASETS\n\nThe CIFAR10H dataset (Peterson et al., 2019) consists of 511,400 human classifications by 2,571 participants which were collected via Amazon Mechanical Turk. Each participant classified 200 images, 20 from each category. Every 20 tasks, a trivial question is presented to prevent random guessing, and participants who scored below 75% were excluded from the dataset. We present the images with the lowest/highest q from the training samples in Fig 5. The image with a lower q means that the first answer and the second answer are hard to distinguish.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nB.2 MODEL\n\nWe trained two simple CNN architectures, VGG-19 and ResNet-18, to show the usefulness of the second answer and the confusion probability. For each model, our loss function is defined as the cross-entropy between the softmax output and the two-hot vector (in which the values are q and 1 − q for g and h, respectively). We compare the results of our top-two label training with those of full-distribution training and hard label (one-hot vector) training.\n\nB.3 TRAINING\n\nWe train each model using 10-fold cross validation (using 90% of images for training and 10% images for validation) and average the results across 5 runs. We run a grid search over learning rates, with the base learning rate chosen from {0.1, 0.01, 0.001}. We find 0.1 to be optimal in all cases. We trained each model for a maximum of 150 epochs using SGD optimizer with a momentum of 0.9 and a weight decay of 0.0001. Our neural networks are trained using NVIDIA GeForce 3090 GPUs.\n\nC BASELINE METHODS\n\nIn this section, we explain the baseline methods with which we compare the performance of our algorithms. To analyze the performance in recovering the top-two answers, we considered the MLbased algorithms, including the Spectral-EM algorithm (MV-D&S and OPT-D&S) (Zhang et al., 2014), Projected Gradient Descent (PGD) (Ma et al., 2018) and M-MSR (Ma & Olshevsky, 2020), which provide a “score” for each label so that we can recover the top-two answers.\n\n• Spectral-EM algorithm (MV-D&S and OPT-D&S) (Zhang et al., 2014) is a two-stage algorithm for multi-class crowd labeling problems. These algorithms are built for the D&S model where each worker has his/her own confusion matrix. In the first stage of the algorithm, the confusion matrix of each worker is estimated via spectral method (OPT-D&S) or majority voting (MV-D&S), respectively, and in the second stage, the estimates for the confusion matrices are refined by optimizing the objective function of the D&S estimator via the Expectation Maximization (EM) algorithm.\n\n• Projected Gradient Descent (PGD) (Ma et al., 2018) is an approach to estimate the skills of each worker in the single-coin D&S model. The authors formulate the skill estimation problem as a rank-one correlation-matrix completion problem. They propose a projected gradient descent method to solve the correlation-matrix completion problem.\n\n• M-MSR (Ma & Olshevsky, 2020) algorithm is an approach to estimate the reliability of each worker in the multi-class D&S model. M-MSR algorithm utilizes that the rank of the response matrix is one. To estimate the reliability of the workers, they use update rules to find the left singular vector and right singular vector of the response matrix. In this process, the extreme values are filtered out to guarantee the stable convergence of the algorithm.\n\nD SYNTHETIC EXPERIMENTS\n\nD.1 ADDITIONAL PLOTS FOR SYNTHETIC DATA EXPERIMENTS IN SEC. 5.1\n\nj=1\n\n(cid:80)m\n\n(cid:80)m\n\nP(ˆgj (cid:54)= gj), 1\n\nIn Section 5.1, we devised four scenarios described in Table 1 to verify the robustness of our model for various (p, q) ranges, with (n, m, s) = (50, 500, 0.2). The performance of algorithms is measured by the empirical average error probabilities in recovering gj, hj and (gj, hj), i.e., P((ˆgj, ˆhj) (cid:54)= (gj, hj)) and plotted in 1\nm Fig. 6. We can observe that for all the considered scenarios TopTwo2 achieves the best performance, near the oracle MLE, in recovering (gj, hj). Depending on scenarios though, the reason TopTwo2 outperforms can be explained differently. For Easy scenario, since qj is close to 1, it becomes easy to distinguish gj from hj but hard to distinguish hj from other labels. Our algorithm achieves the best performance in estimating hj by a large margin. For Hard scenario, it becomes hard to distinguish gj and hj, but our algorithm, which uses an accurate ˆqj, can better distinguish\n\nP(ˆhj (cid:54)= hj) and 1\n\n(cid:80)m\n\nj=1\n\nj=1\n\nm\n\nm\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Prediction error for (gj, hj) (top row), gj (middle) and hj (bottom) for four scenarios. Our algorithm (TopTwo2) achieves the best performance, near the oracle MLE for all the scenarios.\n\ngj and hj. High-variance show the effect of having diverse qj in a dataset. For Few-smart, our algorithm achieves the biggest gain compared to other methods, since our algorithm can effectively distinguish few smart workers from spammers. We remark that even though the performance gap between TopTwo2 and the next best performer is not significant for some cases, our algorithm always achieves the best performance, near that of the oracle-MLE, for all the scenarios, while the next performer keeps changing depending on scenarios. For example, the OPT D&S is the second best performer in the ‘Easy’ scenario, while it is the worst performer in the ‘Few smart’ scenario.\n\nD.2 ROBUSTNESS OF OUR METHODS\n\nIn this section, we present a set of four additional synthetic experiments to demonstrate the robustness of our methods, Alg. 1 and Alg. 2 (referred to as TopTwo1 and TopTwo2). In each experiment, we change a parameter of our synthetic error model and compare the prediction error of our algorithms to the baselines: majority voting(MV), OTP-D&S and MV-D&S Zhang et al. (2014), PGD Ma et al. (2018) and Oracle-MLE. We measure the performance of each algorithm by the empirical average error probabilties in recovering the ground truth gj, the most confusing P(ˆhj (cid:54)= hj) and answer hj and the pair of top two (gj, hj), i.e., 1 P((ˆgj, ˆhj) (cid:54)= (gj, hj)). Obviously, Oracle-MLE provides a lower bound for the perfor-\n\nP(ˆgj (cid:54)= gj), 1\n\n(cid:80)m\n\n(cid:80)m\n\n(cid:80)m\n\nj=1\n\nj=1\n\nm\n\nm\n\n1 m\nmance.\n\nj=1\n\nChanging the dimension of observed matrix: We first check the robustness of our methods against the change of dimensions of the observation matrix A ∈ {0, 1 . . . , K}n×m with n ≤ m. We vary the number of workers (n) or the number of tasks (m) while fixing the other dimension. The default values of n and m are 50 and 500, respectively, and the sampling probability s is fixed as 0.1 throughout the experiments. The worker reliability pi and the task difficulty qj is sampled uniformly at random from [0, 1] and (1/2, 1], respectively, for all i ∈ [n] and j ∈ [m].\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Effect of the number of workers on the performance\n\n(b) Effect of the number of tasks on the performance\n\n(c) Effect of the variance of worker reliability on the performance\n\n(d) Effect of the variance of task difficulty on the performance\n\n(e) Effect of the portion of spammers on the performance\n\nFigure 7: Prediction error for (gj, hj) (first column), gj (second column), and hj (third column) for five different setups. The solid lines represent the mean prediction errors of each algorithm averaged over 10 runs, and the shaded regions represent the standard deviations.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nIn Fig. 7a and 7b, we report the results when we change n for a fixed m and s, or when we change m for a fixed n and s, respectively. From Fig. 7a, we can see that as the number of workers increases, the performance of every algorithm improves since the number of samples per task scales as ns for a fixed s. Our algorithm achieves the performance close to the Oracle-MLE for all the considered range, which implies that the worker reliabilities {pi} are well estimated with our methods. From Fig. 7b, we can see that our algorithm achieves a robust performance against the change in the number of tasks, although the performance gets closer to that of Oracle-MLE as the number of tasks increases. Since our method uses SVD in the first stage, the larger dimension is beneficial for the concentration of the random perturbation matrix with respect to the expectation of the observation matrix. This phenomenon is observed for other baseline methods as well, which are based on the spectral method, OPT D&S, for example.\n\nChanging the variance of worker reliability: In this experiment, we change the range of pi, the parameter for worker skill/reliability, for i ∈ [n], with a fixed mean in order to observe the impact of the variance of the worker reliability on the prediction error. We randomly sample pi from the window [0.5 − x, 0.5 + x] with x varying from 0.05 to 0.25. The mean of the worker reliability is fixed as 0.5.\n\nAs shown in Fig. 7c, when the variance of the worker reliability increases, the baseline methods estimating worker reliabilities perform better than the majority voting. Our TopTwo2 algorithm achieves the best performance close to Oracle-MLE, as the standard deviation increases, i.e., as the workers become more heterogeneous.\n\nChanging the variance of task difficulty: We also design an experiment to observe the impact of the variance of qj, j ∈ [m], the parameter for task difficulty, on the prediction error. We randomly sample qj from the window [0.75 − x, 0.75 + x] with x varying from 0.05 to 0.25. The mean of the worker reliability is fixed as 0.75. If the variance of the task difficulty is small, it could be sufficient to only estimate the worker reliability since all the tasks have almost the similar task difficulties.\n\nAs shown in Fig. 7d, when the variance of the task difficulty increases, our TopTwo2 algorithm performs better than the other baselines. This is the evidence for the validity of our method in estimating the task difficulty.\n\nChanging the portion of spammers: Spammers who provide random answers always exist in crowdsourcing systems. To improve the inference performance, it is important to distinguish spammers from reliable workers. In our experimental setup, we define a spammer as a worker whose reliability parameter pi is in the range [0, 0.1]. We change the portion of spammers among the workers from 0.1 to 0.9 and compare the prediction error of our methods to those of other baseline methods.\n\nIn Fig. 7e, we can see that our algorithm achieves the best performance among all the considered baselines except Oracle-MLE, which can exactly distinguish spammers from reliable workers. This result demonstrates the superiority of our methods in detecting spammers compared to other methods.\n\nD.3 ESTIMATING THE WORKER RELIABILITY VECTOR AND THE TASK DIFFICULTY VECTOR\n\nIn this section, we examine the accuracy of our estimates for the worker reliability vector p and the task difficulty vector q. The worker reliability is estimated by ˆp defined in equation 5 of Algorithm 2 and the task difficulty is estimated by ˆq defined in equation 4 of Algorithm 1. To analyze the accuracy of these estimators, we compute the mean squared error (MSE), 1 m (cid:107) ˆq−q(cid:107)2 2, respectively.\n\nn (cid:107) ˆp−p(cid:107)2\n\n2 and 1\n\nTo analyze the estimation accuracy for the worker reliability, we first sample pi uniformly at random from [0, 1] for all i ∈ [n] and fix the worker reliability vector p. Then, we randomly sample the task difficulty vector q ∈ (1/2, 1]m fifty times and then sample the observation matrices from the distribution equation 1 for each (p, q) pair with a fixed p. For each observation matrix, we subsample the data with varying probabilities and apply Algorithm 2 to get the estimate ˆp, which is then used to calculate the MSE of p. We report the MSE averaged over these fifty cases. Similarly, to analyze the estimation accuracy for the task difficulty, we randomly sample and fix a task difficulty vector q ∈ (1/2, 1]m and generate fifty different observation matrices while varying the worker reliability vector p. We again report the MSE averaged over these fifty cases. The number of workers\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nand that of tasks is set to be (50, 500) for the worker reliability estimation, and to be (100, 1000) for the task difficulty estimation.\n\nIn Fig. 8a and 8b, we plot the MSE for p and q, respectively, as the average number of queries per task increases. We can see that both for p and q, the MSEs converge to near zero as the average number of queries per task increases. However, estimating the task difficulty requires more number of samples as our theory equation 11 suggests.\n\n(a) Mean squared error 1\n\nn (cid:107) ˆp − p(cid:107)2\n\n2\n\n(b) Mean squared error 1\n\nm (cid:107) ˆq − q(cid:107)2\n\n2\n\nFigure 8: Mean squared errors in estimating the worker reliability vector p (left) and the task difficulty vector q (right), respectively.\n\nE DISCUSSION OF THEORETICAL RESULTS\n\nIn this section, we present a discussion of the main theoretical results.\n\n• Theorem 1 asserts that the sampling probability of Ω\n\n(cid:16) 1 1 (cid:107)p(cid:107)2 δ2\n\n2\n\n(cid:17)\n\nlog K (cid:15)\n\nis sufficient to re-\n\ncover the top-two answers (gj, hj) for any task j ∈ [m] and to estimate the confusion probability qj with accuracy of |ˆqj − qj| < δ1 by Algorithm 1 with probability at least 1 − (cid:15). Combined with Theorem 3 part (a), we can see that this sample complexity is the minimax optimal rate for a fixed collective quality of workers, measured by (cid:107)p(cid:107)2 2.\n\n• It is also worth comparing our algorithm with the simple majority voting (MV) scheme. The MV scheme infers the top-two answers by counting the majority of the received answers. Simple analysis shows that the MV scheme requires the sampling probabil- (cid:1) to recover (gj, hj) with probability 1 − (cid:15). ity s such that ns = Θ (cid:0)( 1 samples per task. Since\n\ni pi)−2 log 1\n\n(cid:80)\n\n(cid:17)\n\nn\n\n(cid:15)\n\nRemind that Algorithm 1 requires ns = Ω i ≥ (cid:0) 1\n\n(cid:80)\n\n(cid:80)\n\n(cid:1)2\n\n1\n\nn\n\nn\n\ni pi\n\ni p2\n\nn (cid:107)p(cid:107)2 = 1 by Cauchy-Schwarz inequality, Algorithm 1 achieves strictly better trade-offs unless pi is same for all workers i ∈ [n]. As an example, for a spammer-hammer model where α ∈ (0, 1) fraction of workers are hammers with pi = 1 (cid:1) samples per and the rest are spammers with pi = 0, Algorithm 1 requires ns = Θ (cid:0) 1 task, while MV requires ns = Θ (cid:0) 1 (cid:1) samples per task to recover top-two answers with probability 1 − (cid:15).\n\nα2 log 1\n\nα log 1\n\n(cid:15)\n\n(cid:15)\n\n(cid:16) n 1 (cid:107)p(cid:107)2 δ2\n\n2\n\nlog K (cid:15)\n\n• Theorem 2 shows that when we have an entrywise bound on the estimated worker reliability vector p and the task difficulty vector q, the plug-in MLE estimator, used in Algorithm 2, guarantees the recovery of top-two answers if the sampling probability s = Ω( log(m/(cid:15)) n ̄D ) where ̄D, which depend on (p, q), indicates the average reliability of workers in distinguishing the top-two answers from any other pairs for the most difficult task. Combined with Theorem 3 part (b), we can see that this sample complexity is the minimax optimal rate for any (p, q), ignoring the logarithmic terms.\n\n• Combining the conditions for the accurate estimation of model parameters in equation 11 and the convergence of the plug-in MLE (Theorem 2), Corollary 2 shows the condition on the sample complexity to guarantee the performance of Algorithm 2.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nF PROOF OF PROPOSITION 1\n\nFor each task j and label k, define four indicator functions:\n\nΠa(j, k) :=1(gj > k, hj > k), Πb(j, k) :=1(gj ≤ k, hj > k), Πc(j, k) :=1(gj > k, hj ≤ k), Πd(j, k) :=1(gj ≤ k, hj ≤ k),\n\n(17)\n\nwhich satisfy Πa(j, k) + Πb(j, k) + Πc(j, k) + Πd(j, k) = 1. For notational simplicity, we will often drop (j, k) fron Π∗. The pmf of A(k) is given by\n\nA(k)\n\nij =\n\n \n\n\n\n−1 with probability s(1 − ρ(k) with probability sρ(k) 1\nij , with probability 1 − s, 0\n\nij ),\n\n(18)\n\n, and its expectation is\n\nwhere ρ(k) E[A(k) written as ρ(k)\n\nij ] = s(2ρ(k) ij = pi\n\nij = Πa(j, k)pi + Πb(j, k)pi(1 − qj) + Πc(j, k)piqj + (K−k)(1−pi)\n\nK\n\nij − 1). Note that by using Πa = 1 − Πb − Πc − Πd, the probability ρ(k)\n\nij can be\n\n(cid:0)qj(Πc − Πb) − (Πc + Πd) + k\n\n(cid:1) + K−k\n\nK\n\nK . Thus, by defining\n\nr(k)\n\nj\n\n:= qj(Πc − Πb) − (Πc + Πd) +\n\nk K\n\n,\n\nthe expectation of A(k)\n\nij can be written as\n\nE[A(k)\n\nij ] = s(2ρ(k)\n\nij − 1) = s\n\n(cid:18)\n\n2pir(k)\n\nj +\n\nK − 2k K\n\n(cid:19)\n\n,\n\nand\n\nNote that\n\nE[A(k)] −\n\ns(K − 2k) K\n\n1n×m = 2sp(r(k))(cid:62).\n\nCase I: gj > hj Πa(j, k) = 1 where k < hj, Πc(j, k) = 1 where hj ≤ k < gj, Πd(j, k) = 1 where gj ≤ k;\n\nCase II: gj < hj Πa(j, k) = 1 where k < gj, Πb(j, k) = 1 where gj ≤ k < hj, Πd(j, k) = 1 where hj ≤ k.\n\nThus, r(k)\n\nj\n\nin equation 19 is equal to\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\nCase I: gj > hj \n\n\nk K\nk\n\nr(k) j =\n\nwhere k < hj;\n\nK − (1 − qj) where hj ≤ k < gj; K − 1\n\nwhere gj ≤ k,\n\nk\n\n\n\nCase II: gj < hj \n\n\nk K\nk\n\nr(k) j =\n\nwhere k < gj;\n\nK − qj where gj ≤ k < hj; K − 1\n\nwhere hj ≤ k.\n\nk\n\n\n\nG PERFORMANCE ANALYSIS OF ALGORITHM 1\n\nG.1 PROOFS OF THEOREM 1 AND COROLLARY 1\n\nIn Algorithm 1, we use the data matrix A1, which is obtained by randomly splitting the original data matrix A into A1 and A2 with probability s1 and (1 − s1), respectively. Then, the first stage of Algorithm 1 begins with randomly splitting A1 again into two independent matrices B and C with equal probabilities, and then converting B and C into (K − 1)-binary matrices B(k) and C(k) as explained in Sec. 2. We define X (k) and Y (k) as X (k) := B(k) − s(cid:48)(K−2k) 1n×m and Y (k) := C(k) − s(cid:48)(K−2k) 1n×m where s(cid:48) = s · s1/2. We have E[X (k)] = E[Y (k)] = s(cid:48)p(r(k))(cid:62)\n\nK\n\nK\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nfrom Prop. 1. For notational simplicity, we will ignore this random splitting for a moment and just pretend that X (k) and Y (k) are sampled independently with s(cid:48) = s throughout this section.\n\n(cid:107)p(cid:107)2\n\nWe first outline the proof. Based on the observation that E[X (k)] = sp(r(k))(cid:62), if E[X (k)] is available we can recover p∗ = p by SVD, and by using p∗ it is possible to recover (cid:107)p(cid:107)2r(k), which then reveals {(gj, hj)}m j=1 as well as q from the relation in equation 2. To estimate p∗ from X (k), we first bound the spectral norm of the perturbation, (cid:107)X (k) − E[X (k)](cid:107)2. We then use this bound and Wedin SinΘ theorem to bound sin θ(u(k), p∗) where u(k) is the left singular vector of X (k) with the largest singular value. We trim the abnormally large components of u(k) and denote the resulting vector by ̃u(k). After trimming, it is still possible to show that sin θ( ̃u(k), p∗) can be bounded in the same order as that of sin θ(u(k), p∗). Finally, we provide an entrywise bound between v(k) = 2 s (Y (k))(cid:62) ̃u(k) and (cid:107)p(cid:107)2r(k) in Lemma 5, which is the main lemma to prove Theorem 1. We state our main technical lemmas first and then prove Theorem 1.\n\nLet us define the perturbation matrix\n\nE := X (k) − E[X (k)] = B(k) −\n\ns(K − 2k) K\n\n1n×m − sp(r(k))(cid:62) = B(k) − E[B(k)]\n\n(23)\n\nwhere\n\nB(k)\n\nij =\n\n \n\n\n\n−1 w.p. s(1 − ρ(k) w.p. sρ(k) 1\nij , w.p. 1 − s, 0\n\nij ),\n\nij = Πa(j, k)pi + Πb(j, k)pi(1 − qj) + Πc(j, k)piqj + (K−k)(1−pi)\n\nK\n\nand ρ(k) defined in equation 17.\n\n(24)\n\nfor (Πa, Πb, Πc, Πd)\n\nFor the perturbation matrix E in equation 23, we have\n\nE[Ei,j] = 0,\n\nand\n\n|Ei,j| ≤ 2,\n\n1 ≤ i ≤ n, 1 ≤ j ≤ m,\n\nand also\n\nvar(Eij) = var(B(k)\n\nij ) = E[(B(k)\n\nij )2] − (E[B(k)\n\nij ])2\n\n= s − (s(ρ(k)\n\nij − 1/2))2 ≤ s.\n\n(25)\n\n(26)\n\nNote that {Eij} are independent across all i, j. Define \n\n\nν := max\n\n(cid:88)\n\nmax i\n\n\n\nj\n\nE[E2\n\ni,j], max\n\nj\n\nE[E2\n\ni,j]\n\n \n\n\n\n≤ max{m, n}s.\n\n(27)\n\n(cid:88)\n\ni\n\nBy applying the spectral norm bound to random matrices with independent entires, appeared in Bandeira & Van Handel (2016) and summarized in Theorem 4, we can bound the spectral norm of E as below. Lemma 2 (Spectral norm bound of E). With probability 1 − (n + m)−8, we have\n\n(cid:107)E(cid:107) ≤ 4(cid:112)s max (m, n) + ̃c(cid:112)log(n + m)\n\n(28)\n\nfor some constant ̃c > 0 when m ≥ n. For some sufficiently large m, assuming n = o(m) and s = Ω(log(n + m)/m), the spectral norm of E can be further bounded by\n\n√\n\n(cid:107)E(cid:107) ≤ 5\n\nsm.\n\n(29)\n\nUsing the bounded spectral norm of E in equation 29 and applying the Wedin SinΘ theorem, summarized in Theorem 5, we can bound the angle between u(k) and p∗.\n\nLemma 3. For some sufficiently large m, assuming n = o(m) and s = Ω(log(n + m)/m), we have\n\n√\n\nsin θ(u(k), p∗) ≤ Θ(1/\n\nsn)\n\n(30)\n\nwith probability at least 1 − (n + m)−8.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nProof. By applying the Wedin SinΘ Theorem (Theorem 5), we have\n\nsin θ(u(k), p∗) ≤\n\n.\n\n(31)\n\n√\n\n2(cid:107)E(cid:107) s(cid:107)p(cid:107)2 · (cid:107)r(k)(cid:107)2 − (cid:107)E(cid:107)\n\n√\n\nWe have (cid:107)p(cid:107)2 = Θ( m) by assumptions on model parameters. By Lemma 2, for some sufficiently large m, assuming n = o(m) and s = Ω(log(n + m)/m), we have (cid:107)E(cid:107) ≤ 5\n\nsm with probability at least 1 − (n + m)−8. Combining these bounds, we get\n\nn) and (cid:107)r(k)(cid:107)2 = Θ(\n\n√\n\n√\n\n√\n\nsin θ(u(k), p∗) ≤\n\n√\n\nΘ(s\n\nsm) Θ( mn) − Θ(\n\n√\n\nsm)\n\n=\n\n1 √\n\nΘ (\n\n.\n\nsn)\n\n(32)\n\nWe trim the abnormally large components of u(k) by letting it zero if u(k) n) and denote the resulting vector as ̃u(k). This process is required to control the maximum entry size of ̃u(k), which is used later in the proof. For the next lemma, we show that after the trimming process, the norm of ̃u(k) is still close to 1 and the angle between ̃u(k) and p∗ has the same order as that of sin θ(u(k), p∗). Lemma 4. Given (cid:107)p∗(cid:107)2 ≥ η\n\ni > 2/(η\n\nn, we have\n\n√\n\n√\n\n(cid:107) ̃u(k)(cid:107)2 ≥\n\n1 − 50 sin2 θ(u(k), p∗),\n\nsin θ( ̃u(k), p∗) ≤ 6\n\n2 sin θ(u(k), p∗).\n\n(cid:113)\n\n√\n\n(33)\n\n(34)\n\nThe proof of Lemma 4 is provided in Section G.2.\n\nFinally, we provide our main lemma giving the entrywise bound on the difference between v(k) = 1\ns (Y (k))(cid:62) ̃u(k) and (cid:107)p(cid:107)2r(k). Lemma 5 (Entrywise Bound). For any δ1, (cid:15) > 0, and any task j ∈ [m] and label index k ∈ [K], if the sampling probability s ≥ Θ\n\n, then we can guarantee\n\n(cid:17)\n\n(cid:16) 1 1 (cid:107)p(cid:107)2 δ2\n\n2\n\nlog 1 (cid:15)\n\nP\n\n(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 s\n\n(cid:68)\n\n∗j , ̃u(k)(cid:69) Y (k)\n\n− (cid:107)p(cid:107)2r(k)\n\nj\n\n(cid:19)\n\n< δ1(cid:107)p(cid:107)2\n\n> 1 − (cid:15)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(35)\n\nas m → ∞ when n = O(m/ log m).\n\nProof. For notional simplicity, denote θ( ̃u(k), p∗) by θ. To prove equation 35, we show bounds on two probabilities,\n\nP\n\n(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 s\n\n(cid:68)\n\nP\n\n∗j , ̃u(k)(cid:69) Y (k) (cid:18)(cid:12) (cid:12)\n\n− (cid:107) ̃u(k)(cid:107)2(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ\n\n(cid:12)(cid:107) ̃u(k)(cid:107)2(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ − (cid:107)p(cid:107)2r(k)\n\nj\n\n>\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) >\n\n(cid:19)\n\n(cid:19)\n\nδ1(cid:107)p(cid:107)2 2\n\nδ1(cid:107)p(cid:107)2 2\n\n< (cid:15)/2,\n\n< (cid:15)/2.\n\n(36)\n\n(37)\n\nThen, the triangle inequality implies equation 35.\n\nWe first prove equation 36. Remind that we do the random splitting of the input matrix A and define the two independent binary-converted matrices as X (k) and Y (k), for 1 ≤ k < K, which are used to estimate ̃u(k) and v(k), respectively. Thus, ̃u(k) is independent from Y (k) and this independence is used when we bound the first and second moments of v(k) ∗j , ̃u(k)(cid:105). For any 1 ≤ j ≤ m, the first and second moments of v(k) ∗j , ̃u(k)(cid:105) satisfy\n\ns (cid:104)Y (k)\n\nj = 1\n\nj = 1\n\ns (cid:104)Y (k)\n\nE\n\n(cid:20) 1 s\n\n(cid:68)\n\n∗j , ̃u(k)(cid:69)(cid:21)\n\nY (k)\n\n= (cid:104)p, ̃u(k)(cid:105)r(k)\n\nj = (cid:107)p(cid:107)2(cid:107) ̃u(k)(cid:107)2(cos θ)r(k)\n\nj = Θ(\n\n√\n\nn)\n\n(38)\n\nif r(k)\n\nj\n\n(cid:54)= 0 by Lemma 3 and 4, and\n\nvar\n\n(cid:18) 1 s\n\n(cid:68)\n\n∗j , ̃u(k)(cid:69)(cid:19)\n\nY (k)\n\n( ̃u(k)\n\ni\n\n)2E[(Y (k)\n\nij )2] = Θ\n\n(cid:19)\n\n(cid:18) 1 s\n\n(39)\n\n≤\n\n1 s2\n\nn (cid:88)\n\ni=1\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nsince E[(Y (k) max1≤i≤m |Y (k) can show that\n\nij )2] = Θ(s) and (cid:80)n i=1( ̃u(k) (cid:17) since ̃u(k)\n\n| ≤ Θ\n\n(cid:16) 1√\n\nij ̃u(k)\n\ni\n\ni\n\nn\n\n)2 = Θ(1) by Lemma 3 and 4. Furthermore, we have i ≤ 2\n\nn . By applying the Bernstein’s inequality, we\n\n√\n\nη\n\nP\n\n(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 s\n\n(cid:68)\n\n∗j , ̃u(k)(cid:69) Y (k)\n\n− (cid:107) ̃u(k)(cid:107)2(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ\n\n(cid:19)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n>\n\nδ1(cid:107)p(cid:107)2 2\n\n(cid:32)\n\nΘ(δ2\n\n1(cid:107)p(cid:107)2 2) (cid:1) + Θ (δ1(cid:107)p(cid:107)2/\n\n√\n\n(cid:33)\n\nn)\n\n−\n\n≤ 2 exp\n\nΘ (cid:0) 1 ≤ exp (cid:0)−Θ(sδ2\n\ns\n\n1(cid:107)p(cid:107)2\n\n2)(cid:1)\n\n(40)\n\nn). To make this probability less\n\n√\n\nwhere the second inequality is due to the assumption (cid:107)p(cid:107)2 = Θ( than (cid:15)\n\n2 , it is sufficient to have s ≥ Ω\n\nlog 1 (cid:15)\n\n(cid:17)\n\n.\n\n(cid:16) 1 1 (cid:107)p(cid:107)2 δ2 (cid:12) (cid:12)\n\n2\n\nWe next prove equation 37 by bounding\n\nequality, we have\n\n(cid:12)(cid:107) ̃u(k)(cid:107)2(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ − (cid:107)p(cid:107)2r(k)\n\nj\n\n(cid:12) (cid:12) (cid:12). By the triangle in-\n\n(cid:12) (cid:12)\n\n(cid:12)(cid:107) ̃u(k)(cid:107)2(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ − (cid:107)p(cid:107)2r(k)\n\nj\n\n(cid:12) (cid:12) (cid:12) ≤\n\n(cid:12) (cid:12)\n\n(cid:12)(cid:107) ̃u(k)(cid:107)2(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ − (cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ\n\n(cid:12) (cid:12) (cid:12)\n\n+\n\n(cid:12) (cid:12)\n\n(cid:12)(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ − (cid:107)p(cid:107)2r(k)\n\nj\n\n(cid:12) (cid:12) (cid:12) .\n\nNote that\n\n1 (cid:107)p(cid:107)2\n\n·\n\n(cid:12) (cid:12)\n\n(cid:12)(cid:107) ̃u(k)(cid:107)2(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ − (cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ\n\n(cid:12) (cid:12)\n\n(cid:12) = r(k)\n\nj\n\ncos θ\n\n(cid:12) (cid:12) (cid:12)(cid:107) ̃u(k)(cid:107)2 − 1 (cid:12) (cid:12) (cid:12)\n\nwith probability 1 − (n + m)−8 by Lemma 3 and 4, and also note that\n\n≤ Θ(sin2 θ(u(k), p∗)) =\n\n1 Θ (ns)\n\n,\n\n1 (cid:107)p(cid:107)2\n\n·\n\n(cid:12) (cid:12)\n\n(cid:12)(cid:107)p(cid:107)2r(k)\n\nj\n\ncos θ − (cid:107)p(cid:107)2r(k)\n\nj\n\n(cid:12) (cid:12)\n\n(cid:12) = r(k)\n\nj\n\n(1 − cos θ)\n\n≤ Θ(sin2 θ(u(k), p∗)) =\n\n1 Θ (ns)\n\n,\n\n(41)\n\n(42)\n\n(43)\n\nwith probability 1 − (n + m)−8 by Lemma 3 and 4. To make these errors of order 1/Θ (ns) less than δ1\n\n2 , it is sufficient to have s ≥ Ω\n\n(cid:16) 1 δ1n\n\n(cid:17)\n\n.\n\nBy combining the above results, it can be guaranteed that\n\nwith probability at least 1 − (cid:15), if the sampling probability\n\n(cid:12) (cid:12) (cid:12)\n\n1 2s\n\n(cid:68)\n\n∗j , ̃u(k)(cid:69) Y (k)\n\n− (cid:107)p(cid:107)2r(k)\n\nj\n\n(cid:12) (cid:12) (cid:12) < δ(cid:107)p(cid:107)2\n\n(cid:26)\n\ns ≥ max\n\nΩ\n\n(cid:18) 1 1(cid:107)p(cid:107)2 δ2\n\n2\n\nlog\n\n(cid:19)(cid:27)\n\n, Ω\n\n(cid:18) 1 δ1n\n\n= Ω\n\n(cid:18) 1 1(cid:107)p(cid:107)2 δ2\n\n2\n\nlog\n\n(cid:19)\n\n1 (cid:15)\n\n(44)\n\n(cid:19)\n\n1 (cid:15) √\n\nwhere the last equality is due to (cid:107)p(cid:107)2 = Θ( 3 is immediately satisfied by equation 44 when n = O(m/ log m).\n\nn). The condition s = Ω(log(n + m)/m) in Lemma\n\nProof of Theorem 1. By using Lemma 5, we next prove Theorem 1. By applying the union bound\n\nover k ∈ [K], if s ≥ Θ\n\n(cid:16) 1 1 (cid:107)p(cid:107)2 δ2\n\n2\n\n(cid:17)\n\nlog K (cid:15)\n\n(cid:107)p(cid:107)2(r(k)\n\nj − δ1) ≤ v(k)\n\nj =\n\nthen we have\n\n(cid:68)\n\n∗j , ̃u(k)(cid:69) Y (k)\n\n1 s\n\n≤ (cid:107)p(cid:107)2(r(k)\n\nj + δ1), ∀k ∈ [K]\n\n(45)\n\nfor any δ1 > 0 and j ∈ [m] with probability at least 1 − (cid:15). Under the condition equation 45, for any qj ∈ (1/2, 1) and δ < min\n\n, we can guarantee that\n\n(cid:110) 2qj −1\n\n, 1−qj\n\n(cid:111)\n\n2\n\n2\n\n1 K\n\n− qj + δ <\n\n1 K\n\n− (1 − qj) − δ and\n\n1 K\n\n− (1 − qj) + δ <\n\n1 K\n\n− δ,\n\n(46)\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nwhich implies (ˆgj, ˆhj) = (gj, hj) for (ˆgj, ˆhj) defined in equation 3. This proves equation 8 of Theorem 1.\n\nWe next prove equation 9, the accuracy guarantee in estimating the task difficulty vector q. After estimating (cid:107)p(cid:107)2r(k) by v(k) = 1 s (Y (k))(cid:62) ̃u(k), we estimate (cid:107)p(cid:107)2 by calculating l where lj := j=1 lj. Assume that |(cid:107)p(cid:107)2 − l| ≤ (cid:107)p(cid:107)2δ(cid:48). We will specify\n\nand l := 1\n\n∆v(k)\n\n(cid:80)m\n\n(cid:80)\n\nK K−2\n\nk(cid:54)=ˆgj ,k(cid:54)=ˆhj\n\nm\n\nj\n\nthe required order of δ(cid:48) later. Remind that the estimate for qj is defined as ˆqj := 1 the condition that ˆgj = gj and |vj − (cid:107)p(cid:107)2r(k) conditions of Lemma 5, we have\n\nj\n\n| ≤ (cid:107)p(cid:107)2δ1, both of which are satisfied under the\n\n∆v\n\n(ˆgj ) j\nl\n\n. Under\n\nK −\n\nBy the Taylor expansion for\n\n(cid:0) 1\n\n(cid:1)\n\nK − qj − 2δ1 1 + δ(cid:48) 1\n\n∆v(ˆgj ) j\nl 1−x = 1 + x + Θ(x2) as x → 0, we have\n\nK − qj + 2δ1 1 − δ(cid:48)\n\n≤\n\n≤\n\n(cid:0) 1\n\n(cid:1)\n\n.\n\n|ˆqj − qj| ≤ 2δ1 + δ(cid:48)\n\n− qj + 2δ1\n\n+ Θ(δ(cid:48)2) = Θ(δ1 + δ(cid:48)).\n\n(cid:18) 1 K\n\n(cid:19)\n\n(47)\n\n(48)\n\nThus, both the order of δ(cid:48), which is the estimation error of (cid:107)p(cid:107)2, and that of δ, which is the estimation error of (cid:107)p(cid:107)2r(k) , govern the estimation accuracy of qj. We next show that we can have δ(cid:48) = Θ(δ1). By Lemma 5, we have |vj − (cid:107)p(cid:107)2r(k)\n\n| ≤ (cid:107)p(cid:107)2δ1, which implies\n\nj\n\nj\n\nj − 2δ1) ≤ ∆v(k) Under the condition (ˆgj, ˆhj) = (gj, hj), since ∆r(k)\n\n(cid:107)p(cid:107)2(∆r(k)\n\n(cid:107)p(cid:107)2 − (cid:107)p(cid:107)2\n\n2δ1K K − 2\n\n≤ lj =\n\nK K − 2\n\nj ≤ (cid:107)p(cid:107)2(∆r(k) j = 1 (cid:88)\n\nj + 2δ1). K for k (cid:54)= ˆgj, ˆhj, we have ∆v(k)\n\nj ≤ (cid:107)p(cid:107)2 + (cid:107)p(cid:107)2\n\n2δ1K K − 2\n\n(49)\n\n,\n\n(50)\n\nk(cid:54)=ˆgj ,k(cid:54)=ˆhj\n\nand thus δ(cid:48) = 2δ1K equation 9.\n\nK−2 = Θ(δ1). Thus, it is enough to have s = Ω\n\n(cid:16) 1 1 (cid:107)p(cid:107)2 δ2\n\n2\n\n(cid:17)\n\nlog K (cid:15)\n\nto guarantee\n\nProof of Corollary 1. By using Lemma 5 and taking the union bound over all tasks j ∈ [m] as well as k ∈ [K], we can prove Corollary 1 in a similar way as that of Theorem 1.\n\nG.2 PROOF OF LEMMA 4\n\nWe first prove equation 33,\n\n(cid:113)\n\n(cid:107) ̃u(k)(cid:107)2 ≥\n\n1 − 50 sin2 θ(u(k), p∗).\n\nLet I be the set of indices 1 ≤ i ≤ n such that u(k) i ∈ I since p∗\n\ni ≥ 2\n\n√\n\nη\n\nn . Then, we have u(k)\n\ni − p∗\n\ni ≥ 1\n\n√\n\nη\n\nn due to the assumption that (cid:107)p(cid:107)2\n\n2 ≥ η2n. Thus, we have\n\nn for all\n\n(51)\n\nBy using the triangle inequality, we can show that\n\n≤\n\n(cid:88)\n\n(u(k)\n\ni − p∗\n\ni )2 ≤ (cid:107)u(k) − p∗(cid:107)2 2.\n\ni∈I\n\nu(k)\n\ni −\n\n(cid:19)2\n\n+\n\n2 √\n\nη\n\nn\n\n(cid:115)\n\n4|I| η2n\n\np∗\n\ni −\n\n(cid:19)2\n\n+\n\n2 √\n\nη\n\nn\n\n(cid:115)\n\n(cid:88)\n\n(cid:16)\n\nu(k)\n\ni − p∗\n\ni\n\n(cid:115)\n\n(cid:17)2\n\n+\n\n4|I| η2n\n\n(52)\n\n√\n\ni = pi/(cid:107)p(cid:107)2 ≤ 1 η\n|I| η2n\n\n(cid:18)\n\n(cid:18)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\ni∈I\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\ni∈I\n\n(cid:115)\n\n(cid:88)\n\n(cid:16)\n\n(cid:17)2\n\nu(k)\n\ni\n\n≤\n\ni∈I\n\n≤\n\n≤\n\n(cid:115)\n\n4|I| η2n\n\n+\n\n(cid:115)\n\n(cid:88)\n\n(cid:16)\n\ni∈I\n\nu(k)\n\ni − p∗\n\ni\n\ni∈I\n\n(cid:17)2\n\n+\n\n(cid:115)\n\n4|I| η2n\n\n≤ 5(cid:107)u(k) − p∗(cid:107)2.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nTherefore, we get\n\n1 ≥ (cid:107) ̃u(k)(cid:107)2\n\n2 = 1 −\n\n(cid:88)\n\n(u(k)\n\ni\n\nBy the law of cosine, we have\n\ni∈I\n\n)2 ≥ 1 − 25(cid:107)u(k) − p∗(cid:107)2 2.\n\n(53)\n\n(cid:107)p∗ − u(k)(cid:107)2\n\n2 = sin2 θ(u(k), p∗) + (1 − cos θ(u(k), p∗))2 = 2 − 2 cos θ(u(k), p∗) (cid:19)\n\n(cid:18)\n\nsin2 θ(u(k), p∗)\n\n(cid:113)\n\n= 2\n\n1 −\n\n1 − sin2 θ(u(k), p∗)\n\n= 2\n\n(cid:113)\n\n1 +\n\n1 − sin2 θ(u(k), p∗)\n\n(54)\n\n≤ 2 sin2 θ(u(k), p∗). Combining equation 53 and equation 54 proves equation 33.\n\nWe next prove equation 34,\n\nFirst, note that (cid:107) ̃u(k) − u(k)(cid:107)2\n\n√\n\n2 sin θ(u(k), p∗).\n\nsin θ( ̃u(k), p∗) ≤ 6 (cid:17)2 (cid:16) 2 = (cid:80)\n\nu(k)\n\ni∈I\n\ni\n\n. We have\n\nsin θ( ̃u(k), p∗) ≤ (cid:107) ̃u(k) − p(cid:107)2 ≤ (cid:107) ̃u(k) − u(k)(cid:107)2 + (cid:107)u(k) − p∗(cid:107)2 ≤ 6(cid:107)u(k) − p∗(cid:107)2 where the last inequality is from equation 52. Combined with equation 54, we get equation 34.\n\n(55)\n\nH PERFORMANCE ANALYSIS OF ALGORITHM 2\n\nH.1 PROOF OF LEMMA 1\n\nIn this lemma, we show that conditioned on (ˆgj, ˆhj) = (gj, hj) for all j ∈ [m], if s(1 − s1) = Ω\n\n, the estimator ˆpi defined in equation 5,\n\n(cid:16) 1\n\n(cid:17)\n\nδ2m log n\n\n(cid:15)\n\nˆpi =\n\nK (K − 2)\n\n\n\n\n\n1 s(1 − s1)\n\n\n\n\n\n1 m\n\nm (cid:88)\n\nj=1\n\n\n\n1(A2\n\nij = ˆgj or ˆhj)\n\n −\n\n\n\n ,\n\n2 K\n\nguarantees P ((cid:107)p − ˆp(cid:107)∞ < δ2) ≥ 1 − (cid:15) for any (cid:15) > 0.\n\nGiven (ˆgj, ˆhj) = (gj, hj) for all j ∈ [m], since A2 is independent of (ˆgj, ˆhj), we have 2\nK\n\nij = ˆgj or ˆhj) = s(1 − s1)\n\n(cid:105) ij = ˆgj or ˆhj)\n\n(cid:18) K − 2 K\n\n(cid:104) 1(A2\n\n= P(A2\n\npi +\n\nE\n\n(cid:16)\n\nvar\n\n1(A2\n\nij = ˆgj or ˆhj)\n\n(cid:17)\n\n≤ s(1 − s1).\n\n(cid:19)\n\n,\n\n(56)\n\nBy applying the Bernstein’s inequality, we can show that\n\nP\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\nm (cid:88)\n\n(cid:18)\n\nj=1\n\n\n\n≤ exp\n\n −\n\n1(A2\n\nij = ˆgj or ˆhj) − s(1 − s1)\n\n(cid:18) K − 2 K\n\npi +\n\n2 K\n\n(cid:19)(cid:19)\n\n>\n\n(K − 2)ms(1 − s1)δ2 K\n\n\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:16) (K−2)ms(1−s1)δ2\n\n(cid:17)2\n\n\n\n1 2\n\nK\n\nms(1 − s1) + 1\n\n3\n\n(K−2)ms(1−s1)δ2 K\n\n\n\n ≤ exp (cid:0)−Θ (cid:0)ms(1 − s1)δ2\n\n2\n\nThus, if the sampling probability satisfies\n\ns(1 − s1) = Ω\n\nthen we can guarantee that P(|ˆpi − pi| < δ2) ≥ 1 − (cid:15). By taking the union bound over i ∈ [n], if the sampling probability satisfies\n\nmδ2 2\nthen we can guarantee that P ((cid:107) ˆp − p(cid:107)∞ < δ2) ≥ 1 − (cid:15).\n\ns(1 − s1) = Ω\n\nlog\n\n,\n\n(59)\n\n(cid:18) 1\n\nmδ2 2\n\nlog\n\n(cid:19)\n\n,\n\n1 (cid:15)\n\n(cid:18) 1\n\n(cid:19)\n\nn (cid:15)\n\n26\n\n(cid:1)(cid:1) .\n\n(57)\n\n(58)\n\nUnder review as a conference paper at ICLR 2023\n\nH.2 PROOF OF THEOREM 2\n\nTo prove this theorem, we use similar proof techniques from Zhang et al. (2014). Since the work in Zhang et al. (2014) focuses on the recovery of only the ground-truth label for each task, we generalize the techniques to recover not only the ground-truth label but also the most confusing answer. We first introduce some notations. Let μ(i,j) (a,b),k denote the probability that a worker i ∈ [n] gives label k ∈ [K] for the assigned task j ∈ [m] of which the top-two answers are (gj, hj) = (a, b). Let μ(i,j) (a,b),K](cid:62). We introduce a quantity that measures the average ability of workers in distinguishing the ground-truth pair of top-two answers (gj, hj) from any other pair (a, b) ∈ [K]2/{(gj, hj)} for the task j ∈ [m]. We define\n\n(a,b) = [μ(i,j)\n\n(a,b),1 μ(i,j)\n\n· · · μ(i,j)\n\n(a,b),2\n\n(j)\n\nD\n\n:=\n\nmin (gj ,hj )(cid:54)=(a,b)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:16)\n\nDKL\n\nμ(i,j)\n\n(gj ,hj ), μ(i,j)\n\n(a,b)\n\n(cid:17)\n\n; D := min j∈[m]\n\nD\n\n(j)\n\n,\n\n(60)\n\n(j)\n\nwhere DKL(P, Q) := (cid:80) i P (i) log(P (i)/Q(i)) is the KL-divergence between P and Q. Note that is strictly positive if qj ∈ (1/2, 1) and there exists at least one worker i with pi > 0 for the D\ndistribution equation 1, so that (gj, hj) can be distinguished from any other (a, b) ∈ [K]2/{(gj, hj)} statistically. We define D as the minimum of D over j ∈ [m], indicating the average ability of workers in distinguishing (gj, hj) from any other (a, b) for the most difficult task in the set.\n\n(j)\n\nLet us define an event that will be shown holding with high probability,\n\nE :\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\nDefine\n\n1(Aij = k) log\n\n\n\n\n\nμ(i,j)\n\n(gj ,hj ),k\n\nμ(i,j)\n\n(a,b),k\n\n\n\n ≥ nsD/2 for all j ∈ [m] and (a, b) ∈ [K]×[K]\\(gj, hj).\n\nli :=\n\nK (cid:88)\n\nk=1\n\n1(Aij = k) log\n\n(cid:16)\n\n(gj ,hj ),k/μ(i,j) μ(i,j)\n\n(a,b),k\n\n(cid:17)\n\n.\n\n(61)\n\n(62)\n\nWe can see that l1, . . . , ln are mutually independent on any value of (gj, hj), and each li belongs to the interval [0, log(1/ρ)] where μ(i,j) (gj ,hj ),c ≥ ρ for all (i, j, gj, hj, c) ∈ [n] × [m] × [K]3. We can easily show that\n\n(cid:34) n\n\n(cid:88)\n\nE\n\n(cid:35)\n\n(cid:12) (cid:12) (cid:12) (gj, hj) (cid:12) (cid:12)\n\nli\n\n=\n\nn (cid:88)\n\ni=1\n\nsDKL\n\n(cid:16)\n\nμ(i,j)\n\n(gj ,hj ), μ(i,j)\n\n(a,b)\n\nWe define\n\ni=1\n\n(cid:17)\n\n.\n\n(63)\n\n(64)\n\nD :=\n\n(cid:16)\n\nDKL\n\nn (cid:88)\n\ni=1\n\nμ(i,j)\n\n(gj ,hj ), μ(i,j)\n\n(a,b)\n\n(cid:17)\n\n.\n\nThe following lemma shows that the second moment of li is bounded above by the KL-divergence between the label distribution under (gj, hj) pair and the label distribution under (a, b) pair. Lemma 6. Conditioning on any value of (gj, hj), we have (cid:16)\n\n(cid:17)\n\nE (cid:2)l2\n\ni |(gj, hj)(cid:3) ≤\n\n2 log(1/ρ) 1 − ρ\n\nsDKL\n\nμ(i,j)\n\n(gj ,hj ), μ(i,j)\n\n(a,b)\n\n.\n\n(65)\n\nThe proof of this lemma can be obtained by following the proof of the similar result, Lemma 4 of Zhang et al. (2014).\n\nAccording to Lemma 6, the aggregated second moment of li is bounded by (cid:35)\n\n(cid:34) n\n\nE\n\n(cid:12) (cid:12) (cid:12) (gj, hj) (cid:12) (cid:12)\n\nl2\n\ni\n\n(cid:88)\n\ni=1\n\n(cid:16)\n\nsDKL\n\nμ(i,j)\n\n(gj ,hj ), μ(i,j)\n\n(a,b)\n\n(cid:17)\n\n(66)\n\nn (cid:88)\n\ni=1\n\nsD.\n\n≤\n\n=\n\n2 log(1/ρ) 1 − ρ\n\n2 log(1/ρ) 1 − ρ\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nThus, applying the Bernstein’s inequality, we have\n\n(cid:34) n\n\n(cid:88)\n\nP\n\ni=1\n\n(cid:12) (cid:12) (cid:12) li ≥ sD/2 (cid:12) (cid:12)\n\n(cid:35)\n\n(cid:32)\n\n(gj, hj)\n\n≥ 1 − exp\n\n−\n\n1\n\n2 (sD/2)2\n\n2 log(1/ρ) 1−ρ\n\nsD + 1\n\n3 (2 log(1/ρ))(sD/2)\n\n(cid:33)\n\n.\n\n(67)\n\nSince ρ ≤ 1/2 and D ≥ nD j ∈ [m], we have\n\n(j)\n\n≥ nD, combining the above inequality with union bound over\n\nP [E] ≥ 1 − mK 2 exp\n\n(cid:18)\n\n−\n\nnsD 33 log(1/ρ)\n\n(cid:19)\n\n.\n\n(68)\n\nThe maximum likelihood estimator finds a pair of (a, b) ∈ [K]2, a (cid:54)= b, maximizing\n\n(ˆgj, ˆhj) = arg max\n\n(a,b)∈[K]2,a(cid:54)=b\n\n= arg max\n\n(a,b)∈[K]2,a(cid:54)=b\n\nn (cid:89)\n\ni=1 n\n(cid:88)\n\ni=1\n\nP(Aij|p, qj, (a, b))\n\nlog P(Aij|p, qj, (a, b))\n\n= arg max\n\n(a,b)∈[K]2,a(cid:54)=b\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n1(Aij = k) log μ(i,j)\n\n(a,b),k.\n\n(69)\n\nThe plug-in MLE in equation 6, on the other hand, finds a pair of (a, b) ∈ [K]2, a (cid:54)= b, maximizing\n\n(ˆgj, ˆhj) = arg max\n\n(a,b)∈[K]2,a(cid:54)=b\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n1(Aij = k) log ˆμ(i,j)\n\n(a,b),k\n\n(70)\n\nwhere ˆμ(i,j) (a,b),k is the estimated probability that a worker i ∈ [n] gives label k ∈ [K] for the assigned task j ∈ [m] of which the top two answers are (gj, hj) = (a, b) assuming pi = ˆpi from equation 5 and qj = ˆqj from equation 4 in the distribution equation 1. Thus, for the plug-in MLE to correctly find the ground-truth top two answers (gj, hj), we need to satisfy the following event:\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n1(Aij = k) log\n\n(cid:16)\n\n(gj ,hj ),k/ˆμ(i,j) ˆμ(i,j)\n\n(a,b),k\n\n(cid:17)\n\n≥ 0 for all (a, b) ∈ [K] × [K]\\(gj, hj).\n\n(71)\n\nFor any arbitrary (a, b) (cid:54)= (gj, hj), consider the quantity\n\nQ(a,b) :=\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n1(Aij = k) log\n\n(cid:16)\n\n(gj ,hj ),k/ˆμ(i,j) ˆμ(i,j)\n\n(a,b),k\n\n(cid:17)\n\n,\n\n(72)\n\nwhich can be written as\n\nQ(a,b) =\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n1(Aij = k) log\n\nμ(i,j)\n\n(gj ,hj ),k\n\nμ(i,j)\n\n(a,b),k\n\nn (cid:88)\n\nK (cid:88)\n\n+\n\ni=1\n\nk=1\n\n\n\n\n\n1(Aij = k)\n\nlog\n\n\n\nˆμ(i,j)\n\n(gj ,hj ),k\n\nμ(i,j)\n\n(gj ,hj ),k\n\n\n\n\n\n − log\n\n\n\nˆμ(i,j)\n\n(a,b),k\n\n(a,b),k\n\nμ(i,j) (73)\n\n\n\n\n\n\n\n .\n\nAssuming that there exist ρ > δ3 such that (a,b),k − μ(i,j) (a,b),k ≥ ρ and |ˆμ(i,j) μ(i,j)\n\n(a,b),k| ≤ δ3 for all i ∈ [n], j ∈ [m], (a, b) ∈ [K]2,\n\nwe have\n\n\n\n\n\nmax i∈[n],k∈[K]\n\nlog\n\n\n\nˆμ(i,j)\n\n(gj ,hj ),k\n\nμ(i,j)\n\n(gj ,hj ),k\n\n\n\n\n\n − log\n\n\n\nˆμ(i,j)\n\n(a,b),k\n\nμ(i,j)\n\n(a,b),k\n\n\n\n\n\n\n\n ≤ 2 log\n\n(cid:18) ρ\n\nρ − δ3\n\n(cid:19)\n\n.\n\nBy the Bernstein’s inequality, we also have\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n(cid:12) (cid:12) 1(Aij = k) − ns (cid:12) (cid:12) (cid:12)\n\n(cid:35)\n\n(cid:18)\n\n> ns/2\n\n≤ exp\n\n−\n\n1\n\n2 (ns/2)2\n\n(cid:19)\n\nns + 1\n\n3 (ns/2)\n\n(cid:18)\n\n= exp\n\n−\n\n(cid:19)\n\n.\n\n3ns 28\n\n(74)\n\n(75)\n\n(76)\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nBy taking the union bound over j ∈ [m], we have\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n(cid:12) (cid:12) 1(Aij = k) − ns (cid:12) (cid:12) (cid:12)\n\n> ns/2 for any j ∈ [m]\n\n≤ m exp\n\n−\n\n(cid:35)\n\n(cid:18)\n\n(cid:19)\n\n.\n\n3ns 28\n\n(77)\n\nUnder the intersection of the event\n\nevent E, we can guarantee\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:80)n\n\ni=1\n\n(cid:80)K\n\nk=1\n\n(cid:12) 1(Aij = k) − ns (cid:12) (cid:12) ≤ ns/2 for all j ∈ [m] and the\n\nQ(a,b) =\n\nn (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n1(Aij = k) log\n\nμ(i,j)\n\n(gj ,hj ),k\n\nμ(i,j)\n\n(a,b),k\n\nn (cid:88)\n\nK (cid:88)\n\n+\n\ni=1\n\nk=1\n\n\n\n\n\n1(Aij = k)\n\nlog\n\n\n\nˆμ(i,j)\n\n(gj ,hj ),k\n\nμ(i,j)\n\n(gj ,hj ),k\n\n\n\n\n\n − log\n\n\n\n\n\n\n\n\n\n\n\nˆμ(i,j)\n\n(a,b),k\n\nμ(i,j)\n\n(a,b),k\n\n≥\n\nnsD 2\n\n− 3ns log\n\n(cid:18) ρ\n\n(cid:19)\n\nρ − δ3\n\n≥ ns\n\n(cid:18) D 2\n\n−\n\n3δ3 ρ − δ3\n\n(cid:19)\n\n> 0\n\nfor every j ∈ [m] where the last inequality holds if\n\nδ3 < ρ\n\nD\n\n6 + D\n\n.\n\n(78)\n\n(79)\n\nIn summary, under that the event event E hold, if we have δ3 such that\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:80)n\n\ni=1\n\n(cid:80)K\n\nk=1\n\n(cid:12) 1(Aij = k) − ns (cid:12) (cid:12) ≤ ns/2 for all j ∈ [m] and the\n\n|ˆμ(i,j)\n\n(a,b),k − μ(i,j)\n\n(a,b),k| ≤ δ3 for all i ∈ [n], j ∈ [m], (a, b) ∈ [K]2\n\nand\n\nδ3 < ρ and\n\nδ3 < ρ\n\nD\n\n6 + D\n\n,\n\n(80)\n\n(81)\n\nthen we can guarantee that the plug-in MLE in equation 70 successfully recovers the pair of top two (gj, hj) for all the tasks j ∈ [m]. To make the right-hand side of equation 68 and equation 77 less than (cid:15)/2, it is sufficient to have\n\ns = Ω\n\n(cid:18) log(1/ρ) log(mK 2/(cid:15)) + D log(m/(cid:15)) nD\n\n(cid:19)\n\n.\n\nLastly, when we have\n\nwe can guarantee that\n\nmax{(cid:107)p − ˆp(cid:107)∞, (cid:107)q − ˆq(cid:107)∞} ≤ δ,\n\n(a,b),k − μ(i,j) Thus, it is sufficient to guarantee equation 83 with\n\n|ˆμ(i,j)\n\n(a,b),k| ≤ 4δ := δ3.\n\nδ < min\n\n(cid:26) ρ 4\n\n,\n\nρD\n\n4(6 + D)\n\n(cid:27)\n\n.\n\nI PROOF OF THEOREM 3\n\nI.1 PROOF OF PART (A)\n\n(82)\n\n(83)\n\n(84)\n\n(85)\n\nTo prove this minimax bound, we use the similar arguments from Karger et al. (2014). In particular, we consider a spammer-hammer model such that\n\npi =\n\n(cid:26)0, for 1 ≤ i ≤ (cid:98)(1 − p)n(cid:99)\n\n1, otherwise.\n\n(86)\n\nAssume that total lj workers randomly sampled from [n] provide answers for the task j. Under the spammer-hammer model, the oracle estimator makes a mistake on task j with probability (K−1)/K if it is only assigned to spammers. When lj is the number of assignments, we have\n\nP(ˆgj (cid:54)= gj) =\n\nK − 1 K\n\n(1 − p)lj .\n\n(87)\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nBy convexity and using Jensen’s inequality, the average probability of error is lower bounded by\n\n1 m\n\n(cid:88)\n\nj∈[m]\n\nP(ˆgj (cid:54)= gj) ≥\n\nK − 1 K\n\n(1 − p)l\n\nwhere 1\n\nm\n\n(cid:80)\n\ni∈[m] li ≤ l. By assuming p ≤ 2/3, we have (1 − p) ≥ e−(p+p2). Thus,\n\nmin ˆg\n\nmax p∈Fp, g∈[K]m\n\n1 m\n\n(cid:88)\n\nj∈[m]\n\nP(ˆgj (cid:54)= gj) ≥\n\nK − 1 K\n\ne−(p+p2)l ≥\n\nK − 1 K\n\ne−2pl.\n\n(88)\n\n(89)\n\n(cid:1), then no algorithm can make The inequality in equation 89 implies that if l is less than 1 the minimax error in equation 89 less than (cid:15). Since the average number of queries per task in our\n\n2p log (cid:0) K−1\n\nK(cid:15)\n\nmodel is ns, it implies that it is necessary to have s = Ω\n\nI.2 PROOF OF PART (B)\n\n(cid:16) 1\n\n(cid:107)p(cid:107)2 2\n\n(cid:17)\n\n.\n\nlog 1 (cid:15)\n\nTo prove the second part of the theorem, we use proof techniques from Zhang et al. (2014), but generalizes the results for pair of top two answers. We assume that jc ∈ [m], (gc, hc) ∈ [K]2 and (ac, bc) ∈ [K]2 are the task index and the pairs of labels such that\n\nD =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:16)\n\nDKL\n\nμ(i,jc)\n\n(gc,hc), μ(i,jc)\n\n(ac,bc)\n\n(cid:17)\n\nfor D defined in equation 60.\n\nLet Q be a uniform distribution over the set {(gc, hc), (ac, bc)}m. For any (ˆg, ˆh), we have\n\nmax (v,u)∈[K]m×[K]m vj (cid:54)=uj ,∀j[m]\n\nE\n\n\n\n\n\nm (cid:88)\n\nj=1\n\n1((ˆgj, ˆhj) (cid:54)= (gj, hj))\n\n(cid:12) (cid:12) (cid:12)(g, h) = (v, u)\n\n\n\n\n\nm (cid:88)\n\n≥\n\n(cid:88)\n\nj=1\n\n(v,u)∈{(gc,hc),(ac,bc)}m\n\nQ((v, u))E\n\n(cid:104)\n\n1((ˆgj, ˆhj) (cid:54)= (gj, hj))\n\n(cid:12) (cid:12) (cid:12)(g, h) = (v, u)\n\n(cid:105)\n\n(90)\n\n(91)\n\nLet A := {Aij : i ∈ [n], j ∈ [m]} be the set of observations. Define two probability measures P0 and P1, such that P0 is the measure of A conditioned on (gj, hj) = (gc, hc), while P1 is that on (gj, hj) = (ac, bc). Then, we can have\n\n(cid:88)\n\n(v,u)∈{(gc,hc),(ac,bc)}m\n\nQ((v, u))E\n\n(cid:104) 1((ˆgj, ˆhj) (cid:54)= (gj, hj))\n\n(cid:12) (cid:105) (cid:12) (cid:12)(g, h) = (v, u)\n\n= Q((gj, hj) = (gc, hc))P0((ˆgj, ˆhj) (cid:54)= (gc, hc)) + Q((gj, hj) = (ac, bc))P1((ˆgj, ˆhj) (cid:54)= (ac, bc))\n\n≥\n\n≥\n\n1 2\n1 2\n\n−\n\n−\n\n1 2\n1 4\n\n(cid:107)P0 − P1(cid:107)TV\n\n(cid:112)DKL(P0, P1).\n\n(92)\n\nwhere the second to the last inequality is by Le Cam’s method and the last inequality is by Pinsker’s inequality.4\n\nConditioned on (gj, hj), the set of random variables Aj := {Aij : i ∈ [n]} are independent of A\\Aj for both P0 and P1, and thus\n\nDKL(P0, P1) = DKL(P0(Aj), P1(Aj)) + DKL(P0(A\\Aj), P1(A\\Aj)) = DKL(P0(Aj), P1(Aj))\n\n(93)\n\n4The total variation distance between probability distributions P and Q defined on a set X is defined as the maximum difference between probabilities they assign on subsets of X : (cid:107)P − Q(cid:107)TV := supA⊂X |P (A) − Q(A)|.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nwhere P(X) denote the distribution of X with respect to the probability measure P. Given (gj, hj), since A1j, . . . , Anj are independent, we can show that\n\nDKL(P0(Aj), P1(Aj)) =\n\n=\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\nDKL(P0(Aij), P1(Aij))\n\n(cid:18)\n\n(1 − s) log\n\n1 − s 1 − s\n\n(cid:16)\n\n+ sDKL\n\n≥ snD.\n\nCombining equation 91– equation 94, we have\n\nμ(i,j)\n\n(gc,hc), μ(i,j)\n\n(ac,bc)\n\n\n\n\n\n1 m\n\nE\n\nm (cid:88)\n\nj=1\n\nmax (v,u)∈[K]m×[K]m vj (cid:54)=uj ,∀j[m] 1\n1 4\n2\n\n(cid:112)\n\n−\n\n≥\n\nsnD.\n\n(cid:12) 1((ˆgj, ˆhj) (cid:54)= (gj, hj)) (cid:12) (cid:12)(g, h) = (v, u)\n\n\n\n\n\n(cid:17)(cid:19)\n\n(94)\n\n(95)\n\nThus, if s ≤ 1\n\n4nD\n\n, then the above inequality is lower bounded by 3/8. This completes the proof.\n\nJ USEFUL INEQUALITIES\n\nIn this section, we summarize the useful inequalities used in the proof of the main results.\n\nThe following inequality, which appeared in Bandeira & Van Handel (2016) provides a nonasymptotic spectral norm bound for random matrices with independent random entries.\n\nTheorem 4 (Spectral norm bound of a random matrice with independent entries). Consider a random matrix X ∈ Rn×m, whose entries are independently generated and obey\n\nE[Xi,j] = 0,\n\nand |Xi,j| ≤ B,\n\n1 ≤ i ≤ n, 1 ≤ j ≤ m.\n\nDefine\n\nν := max\n\n \n\n\n\nmax i\n\n(cid:88)\n\nj\n\nE[X 2\n\ni,j], max\n\nj\n\nE[X 2\n\ni,j]\n\n(cid:88)\n\ni\n\n \n\n\n\n.\n\nThen there exists some universal constant c > 0 such that for any t > 0,\n\nP (cid:8)(cid:107)X(cid:107) ≥ 4\n\n√\n\nν + t(cid:9) ≤ (n + m) exp\n\n(cid:18)\n\n−\n\nt2 cB2\n\n(cid:19)\n\n.\n\n(96)\n\n(97)\n\n(98)\n\n√\n\n9c and t = B(cid:112)9c log(n + m).\n\nWe also present a useful corollary of Theorem 4, which can be shown from equation 98 by setting ̃c = Corollary 3 (Corollary of Theorem 4). If E[X 2 Theorem 4, then we have\n\ni,j] ≤ σ2 for all i, j and satisfying conditions in\n\n(cid:107)X(cid:107) ≤ 4σ(cid:112)max(m, n) + ̃cB(cid:112)log(n + m)\n\n(99)\n\nwith probability 1 − (n + m)−8 for some constant ̃c > 0.\n\nWe next summarize the eigenspace perturbation theory for asymmetric matrices with singular value composition (SVD). Suppose X := [X0, X1] and Z := [Z0, Z1] are orthonormal matrices. When we define the distance between two subspaces X0 and Z0 by\n\nthen we have\n\ndist(X0, Z0) := (cid:107)X0X (cid:62)\n\n0 − Z0Z(cid:62)\n\n0 (cid:107),\n\ndist(X0, Z0) = (cid:107)X (cid:62)\n\n0 Z1(cid:107) = (cid:107)Z(cid:62)\n\n0 X1(cid:107).\n\n(100)\n\n(101)\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nGiven (cid:107)X (cid:62) 0 Z0 := U cos ΘV (cid:62) where cos Θ = diag(cos θ1, . . . , cos θr). We call {θ1, . . . , θr} principal angles between X0 and Z0. Then, we have\n\n0 Z0(cid:107) ≤ 1, we write SVD of X (cid:62)\n\n0 Z0 ∈ Rr×r as X (cid:62)\n\n(cid:107)X (cid:62)\n\n0 Z1(cid:107) = (cid:107) sin Θ(cid:107) = max{| sin θ1|, · · · , | sin θr|}.\n\n(102)\n\nLet M ∗ and M = M ∗ + E be two matrices in Rn×m with n ≤ m, whose SVD are represented by M ∗ = (cid:80)n n). Let us define\n\n(cid:62), where σ1 ≥ · · · ≥ σn (resp. σ∗\n\n(cid:62) and M = (cid:80)n\n\n1 ≥ · · · ≥ σ∗\n\ni=1 σiuivi\n\ni=1 σ∗\n\ni u∗\n\ni v∗\n\ni\n\nU0 := [u1, · · · , ur] ∈ Rn×r, V0 := [v1, · · · , vr] ∈ Rm×r.\n\n(103)\n\nThe matrices U ∗ 0 are defined analogously. Theorem 5 (Wedin sin Θ Theorem). If (cid:107)E(cid:107) < σ∗\n\n0 and V ∗\n\nr − σ∗\n\nr+1, then one has\n\nmax{(cid:107)dist(U0, U ∗\n\n0 )(cid:107), (cid:107)dist(V0, V ∗\n\n0 )(cid:107)} ≤\n\n√\n\n2(cid:107)E(cid:107) r+1 − (cid:107)E(cid:107)\n\nσ∗\n\nr − σ∗\n\n,\n\n(104)\n\nwhere U ∗ 0 (V ∗ M ∗ and M , respecively.\n\n0 ) and U0 (V0) are subspaces spanned by the largest r left (right) singular vectors of\n\nLastly, we also write down two useful concentration inequalities.\n\nTheorem 6 (Hoeffding). Let X1, X2, . . . , Xn be independent random variables such that Xi ∈ [ai, bi] for 1 ≤ i ≤ n. Then, we have\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\ni=1\n\n(cid:12) (cid:12) (Xi − E[Xi]) (cid:12) (cid:12) (cid:12)\n\n(cid:35)\n\n(cid:18)\n\n> t\n\n≤ 2 exp\n\n−\n\n2t2\n\n(cid:80)n\n\ni=1(bi − ai)2\n\n(cid:19)\n\n.\n\n(105)\n\nTheorem 7 (Bernstein). Let X1, X2, . . . , Xn be independent random variables such that Xi ∈ [ai, bi] for 1 ≤ i ≤ n. Let C := max1≤i≤n(bi − ai) and σ2 = (cid:80)n\n\ni=1 var(Xi). Then we have\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\ni=1\n\n(cid:12) (cid:12) (Xi − E[Xi]) (cid:12) (cid:12) (cid:12)\n\n(cid:35)\n\n(cid:18)\n\n> t\n\n≤ 2 exp\n\n−\n\nt2/2 σ2 + C · t/3\n\n(cid:19)\n\n.\n\n(106)\n\n32",
    "reference": "# Summary Of The Paper\n\nIn crowd-computing tasks there are often two causes for wrong answers: 1) low reliability of worker (i.e. possibly a spammer or someone making a random guess), 2) confusion due to task (or question) difficulty (i.e. the question is simply harder than others or the potential answers could be confusing). The David-Skene model only captures the worker reliability portion of this. The authors have built a model for multi-choice crowdsourcing, that infers the top-two answers and the confusion probability. This would benefit downstream uses of these crowdsource tasks by providing the most plausible answer other than the ground truth and how plausible that second answer is. The authors show how using this top-two information can add neural network training versus using a single hard label.\n\n# Strength And Weaknesses\n\n- The authors evaluate on both synthetic and real-world datasets. They also demonstrate how their model can improve neural network performance.\n- As I was reading through the paper I wondered why only top-two, the authors provided good explanation in the appendix using analysis of public crowdsourcing datasets.\n\nWeakness\n- 5.3, It isn't clear to me that top-two is much better than full distribution.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written. I have seen other approaches to using the confusion of raters versus hard labels.\n\n# Summary Of The Review\n\nOverall it was a well written paper. The evaluations on the synthetic and real-world datasets were well done. I am unsure of the results of the neural network training.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMANISKILL2: GENERALIZABLE MANIPULATION SKILLS\n\nA UNIFIED BENCHMARK FOR\n\nJiayuan Gu1†, Fanbo Xiang1†, Xuanlin Li1∗, Zhan Ling1∗, Xiqiang Liu1∗, Tongzhou Mu1∗ Yihe Tang1∗, Stone Tao1∗, Xinyue Wei1∗, Yunchao Yao1∗, Xiaodi Yuan2, Pengwei Xie2 Zhiao Huang1, Rui Chen2, Hao Su1 1University of California San Diego, 2Tsinghua University\n\nABSTRACT\n\nGeneralizable manipulation skills, which can be composed to tackle longhorizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dualarm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 It implements a render GPU and 16 processes on a regular workstation. server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.\n\nFigure 1: ManiSkill2 provides a unified, fast, and accessible system that encompasses well-curated manipulation tasks (e.g., stationary/mobile-base, single/dual-arm, rigid/soft-body).\n\n1† and * indicate equal contribution (in alphabetical order). See Appendix H for contributions. 2Project website: https://maniskill2.github.io/ 3Codes: https://github.com/haosulab/ManiSkill2 4Challenge website: https://sapien.ucsd.edu/challenges/maniskill/2022/\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n1\n\nINTRODUCTION\n\nMastering human-like manipulation skills is a fundamental but challenging problem in Embodied AI, which is at the nexus of vision, learning, and robotics. Remarkably, once humans have learnt to manipulate a category of objects, they are able to manipulate unseen objects (e.g., with different appearances and geometries) of the same category in unseen configurations (e.g., initial poses). We refer such abilities to interact with a great variety of even unseen objects in different configurations as generalizable manipulation skills. Generalizable manipulation skills are one of the cornerstones of Embodied AI, which can be composed to tackle long-horizon and complex daily chores (Ahn et al., 2022; Gu et al., 2022). To foster further interdisciplinary and reproducible research on generalizable manipulation skills, it is crucial to build a versatile and public benchmark that focuses on object-level topological and geometric variations as well as practical manipulation challenges.\n\nHowever, most prior benchmarks are insufficient to support and evaluate progress in learning In this work, we present ManiSkill2, the next generation of generalizable manipulation skills. SAPIEN ManiSkill Benchmark (Mu et al., 2021), which extends upon fully simulated dynamics, a large variety of articulated objects, and large-scale demonstrations from the previous version. Moreover, we introduce significant improvements and novel functionalities, as shown below. 1) A Unified Benchmark for Generic and Generalizable Manipulation Skills: There does not exist a standard benchmark to measure different algorithms for generic and generalizable manipulation skills. It is largely due to well-known challenges to build realistically simulated environments with diverse assets. Many benchmarks bypass critical challenges as a trade-off, by either adopting abstract grasp (Ehsani et al., 2021; Szot et al., 2021; Srivastava et al., 2022) or including few object-level variations (Zhu et al., 2020; Yu et al., 2020; James et al., 2020). Thus, researchers usually have to make extra efforts to customize environments due to limited functionalities, which in turn makes reproducible comparison difficult. For example, Shridhar et al. (2022) modified 18 tasks in James et al. (2020) to enable few variations in initial states. Besides, some benchmarks are biased towards a single type of manipulation, e.g., 4-DoF manipulation in Yu et al. (2020). To address such pain points, ManiSkill2 includes a total of 20 verified and challenging manipulation task families of multiple types (stationary/mobile-base, single/dual-arm, rigid/softbody), with over 2000 objects and 4M demonstration frames, to support generic and generalizable manipulation skills. All the tasks are implemented in a unified OpenAI Gym (Brockman et al., 2016) interface with fully-simulated dynamic interaction, supporting multiple observation modes (point cloud, RGBD, privileged state) and multiple controllers. A unified protocol is defined to evaluate a wide range of algorithms (e.g., sense-plan-act, reinforcement and imtation learning) on both seen and unseen assets as well as configurations. In particular, we implement a cloud-based evaluation system to publicly and fairly compare different approaches. 2) Real-time Soft-body Environments: When operating in the real world, robots face not only rigid bodies, but many types of soft bodies, such as cloth, water, and soil. Many simulators have supported robotic manipulation with soft body simulation. For example, MuJoCo (Todorov et al., 2012) and Bullet Coumans & Bai (2016–2021) use the finite element method (FEM) to enable the simulation of rope, cloth, and elastic objects. However, FEM-based methods cannot handle large deformation and topological changes, such as scooping flour or cutting dough. Other environments, like SoftGym (Lin et al., 2020) and ThreeDWorld (Gan et al., 2020), are based on Nvidia Flex, which can simulate large deformations, but cannot realistically simulate elasto-plastic material, e.g., clay. PlasticineLab (Huang et al., 2021) deploys the continuum-mechanics-based material point method (MPM), but it lacks the ability to couple with rigid robots, and its simulation and rendering performance have much room for improvement. We have implemented a custom GPU MPM simulator from scratch using Nvidia’s Warp (Macklin, 2022) JIT framework and native CUDA for high efficiency and customizability. We further extend Warp’s functionality to support more efficient host-device communication. Moreover, we have supported a 2-way dynamics coupling interface that enables any rigid-body simulation framework to interact with the soft bodies, allowing robots and assets in ManiSkill2 to interact with soft-body simulation seamlessly. To our knowledge, ManiSkill2 is the first embodied AI environment to support 2-way coupled rigid-MPM simulation, and also the first to support real-time simulation and rendering of MPM material. 3) Multi-controller Support and Conversion of Demonstration Action Spaces: Controllers transform policies’ action outputs into motor commands that actuate the robot, which define the action space of a task. Mart ́ın-Mart ́ın et al. (2019); Zhu et al. (2020) show that the choice of action\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nspace has considerable effects on exploration, robustness and sim2real transferability of RL policies. For example, task-space controllers are widely used for typical pick-and-place tasks, but might be suboptimal compared to joint-space controllers when collision avoidance (Szot et al., 2021) is required. ManiSkill2 supports a wide variety of controllers, e.g., joint-space controllers for motion planning and task-space controllers for teleoperation. A flexible system is also implemented to combine different controllers for different robot components. For instance, it is easy to specify a velocity controller for the base, a task-space position controller for the arm, and a joint-space position controller for the gripper. It differs from Zhu et al. (2020), which only supports setting a holistic controller for all the components. Most importantly, ManiSkill2 embraces a unique functionality to convert the action space of demonstrations to a desired one. It enables us to exploit large-scale demonstrations generated by any approach regardless of controllers. 4) Fast Visual RL Experiment Support: Visual RL training demands millions of samples from interaction, which makes performance optimization an important aspect in environment design. Isaac Gym (Makoviychuk et al., 2021) implements a fully GPU-based vectorized simulator, but it lacks an efficient renderer. It also suffers from reduced usability (e.g., difficult to add diverse assets) and functionality (e.g., object contacts are inaccessible). EnvPool (Weng et al., 2022) batches environments by a thread pool to minimize synchronization and improve CPU utilization. Yet its environments need to be implemented in C++, which hinders fast prototyping (e.g., customizing observations and rewards). As a good trade-off between efficiency and customizability, our environments are fully scripted in Python and vectorized by multiple processes. We implement an asynchronous RPC-based render server-client system to optimize throughput and reduce GPU memory usage. We manage to collect samples with an RGBD-input PPO policy at about 2000 FPS 1 with 1 GPU and 16 CPU processors on a regular workstation.\n\n2 BUILDING ENVIRONMENTS FOR GENERALIZABLE MANIPULATION SKILLS\n\nBuilding high-quality environments demands cross-disciplinary knowledge and expertise, including physical simulation, rendering, robotics, machine learning, software engineering, etc. Our workflow highlights a verification-driven iterative development process, which is illustrated in Appendix A.1. Different approaches, including task and motion planning (TAMP), model predictive control (MPC) and reinforcement learning (RL), can be used to generate demonstrations according to characteristics and difficulty of tasks, which verify environments as a byproduct.\n\n2.1 HETEROGENEOUS TASK FAMILIES\n\nManiSkill2 embraces a heterogeneous collection of 20 task families. A task family represents a family of task variants that share the same objective but are associated with different assets and initial states. For simplicity, we interchangeably use task short for task family. Distinct types of manipulation tasks are covered: rigid/soft-body, stationary/mobile-base, single/dual-arm. In this section, we briefly describe 4 groups of tasks. More details can be found in Appendix C.\n\nSoft-body Manipulation: ManiSkill2 implements 6 soft-body manipulation tasks that require agents to move or deform soft bodies into specified goal states through interaction. 1) Fill: filling clay from a bucket into the target beaker; 2) Hang: hanging a noodle on the target rod; 3) Excavate: scooping up a specific amount of clay and lifting it to a target height; 4) Pour: pouring water from a bottle into the target beaker. The final liquid level should match the red line on the beaker. 5) Pinch: deforming plasticine from an initial shape into a target shape; target shapes are generated by randomly pinching initial shapes, and are given as RGBD images or point clouds from 4 views. 6) Write: writing a target character on clay. Target characters are given as 2D depth maps. A key challenge of these tasks is to reason how actions influence soft bodies, e.g., estimating displacement quantity or deformations, which will be illustrated in Sec 5.2. Precise Peg-in-hole Assembly: Peg-in-hole assembly is a representative robotic task involving rich contact. We include a curriculum of peg-in-hole assembly tasks that require an agent to place an object into its corresponding slot. Compared to other existing assembly tasks, ours come with two\n\n1The FPS is reported for rigid-body environments.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nnoticeable improvements. First, our tasks target high precision (small clearance) at the level of millimeters, as most day-to-day assembly tasks demand. Second, our tasks emphasize the contactrich insertion process, instead of solely measuring position or rotation errors. 1) PegInsertionSide: a single peg-in-hole assembly task inspired by MetaWorld (Yu et al., 2020). It involves an agent picking up a cuboid-shaped peg and inserting it into a hole with a clearance of 3mm on the box. Our task is successful only if half of the peg is inserted, while the counterparts in prior works only require the peg head to approach the surface of the hole. 2) PlugCharger: a dual peg-in-hole assembly task inspired by RLBench (James et al., 2020), which involves an agent picking up and plugging a charger into a vertical receptacle. Our assets (the charger and holes on the receptacle) are modeled with realistic sizes, allowing a clearance of 0.5mm, while the counterparts in prior works only examine the proximity of the charger to a predefined position without modeling holes at all. 3) AssemblingKits: inspired by Transporter Networks (Zeng et al., 2020), this task involves an agent picking up and inserting a shape into its corresponding slot on a board with 5 slots in total. We devise a programmatic way to carve slots on boards given any specified clearance (e.g., 0.8mm), such that we can generate any number of physically-realistic boards with slots. Note that slots in environments of prior works are visual marks, and there are in fact no holes on boards. We generate demonstrations for the above tasks through TAMP. This demonstrates that it is feasible to build precise peg-in-hole assembly tasks solvable in simulation environments, without abstractions from prior works. Stationary 6-DoF Pick-and-place: 6-DoF pick-and-place is a widely studied topic in robotics. At the core is grasp pose Mahler et al. (2016); Qin et al. (2020); Sundermeyer et al. (2021). In ManiSkill2, we provide a curriculum of pick-and-place tasks, which all require an agent to pick up an object and move it to a goal specified as a 3D position. The diverse topology and geometric variations among objects call for generalizable grasp pose predictions. 1) PickCube: picking up a cube and placing it at a specified goal position; 2) StackCube: picking up a cube and placing it on top of another cube. Unlike PickCube, the goal placing position is not explicitly given; instead, it needs to be inferred from observations. 3) PickSingleYCB: picking and placing an object from YCB (Calli et al., 2015); 4) PickSingleEGAD: picking and placing an object from EGAD (Morrison et al., 2020); 5) PickClutterYCB: The task is similar to PickSingleYCB, but multiple objects are present in a single scene. The target object is specified by a visible 3D point on its surface. Our pick-and-place tasks are deliberately designed to be challenging. For example, the goal position is randomly selected within a large workspace (30×50×50 cm3). It poses challenges to sense-planact pipelines that do not take kinematic constraints into consideration when scoring grasp poses, as certain high-quality grasp poses might not be achievable by the robot. Mobile/Stationary Manipulation of Articulated Objects: We inherit four mobile manipulation tasks from ManiSkill1 (Mu et al., 2021), which are PushChair, MoveBucket, OpenCabinetDoor, and OpenCabinetDrawer. We also add a stationary manipulation task, TurnFaucet, which uses a stationary arm to turn on faucets of various geometries and topology (details in Appendix C.3).\n\nBesides the above tasks, we have one last task, AvoidObstacles, which tests the navigation ability of a stationary arm to avoid a dense collection of obstacles in space while actively sensing the scene.\n\n2.2 MULTI-CONTROLLER SUPPORT AND CONVERSION OF DEMONSTRATION ACTION SPACES\n\nThe selection of controllers determines the action space. ManiSkill2 supports multiple controllers, e.g., joint position, delta joint position, delta end-effector pose, etc. Unless otherwise specified, controllers in ManiSkill2 translate input actions, which are desired configurations (e.g., joint positions or end-effector pose), to joint torques that drive corresponding joint motors to achieve desired actions. For instance, input actions to joint position, delta joint position, and delta endeffector pose controllers are, respectively, desired absolute joint positions, desired joint positions relative to current joint positions, and desired SE(3) transformations relative to the current endeffector pose. See Appendix B for a full description of all supported controllers.\n\nDemonstrations are generated with one controller, with an associated action space. However, researchers may select an action space that conforms to a task but is different from the original one. Thus, to exploit large-scale demonstrations, it is crucial to convert the original action space\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nto many different target action spaces while reproducing the kinematic and dynamic processes in demonstrations. Let us consider a pair of environments: a source environment with a joint position controller used to generate demonstrations through TAMP, and a target environment with a delta end-effector pose controller for Imitation / Reinforcement Learning applications. The objective is to convert the source action asrc(t) at each timestep t to the target action atgt(t). By definition, the target action (delta end-effector pose) is atgt(t) = ̄Ttgt(t) · T −1 tgt (t), where ̄Ttgt(t) and Ttgt(t) are respectively desired and current end-effector poses in the target environment. To achieve the same dynamic process in the source environment, we need to match ̄Ttgt(t) with ̄Tsrc(t), where ̄Tsrc(t) is the desired end-effector pose in the source environment. ̄Tsrc(t) can be computed from the desired joint positions (asrc(t) in this example) through forward kinematics (F K(·)). Thus, we have tgt (t) = ̄Tsrc(t) · T −1 Note that our method is closed-loop, as we instantiate a target environment to acquire T −1 comparison, an open-loop method would use T −1\n\ntgt (t). For src (t) and suffers from accumulated execution errors.\n\ntgt (t) = F K( ̄asrc(t)) · T −1\n\natgt(t) = ̄Ttgt(t) · T −1\n\ntgt (t)\n\n3 REAL-TIME SOFT BODY SIMULATION AND RENDERING\n\nIn this section, we describe our new physical simulator for soft bodies and their dynamic interactions with the existing rigid-body simulation. Our key contributions include: 1) a highly efficient GPU MPM simulator; 2) an effective 2-way dynamic coupling method to support interactions between our soft-body simulator and any rigid-body simulator, in our case, SAPIEN. These features enable us to create the first real-time MPM-based soft-body manipulation environment with 2-way coupling.\n\nsimilar\n\nis MLS-MPM (Hu et al., 2018)\n\nRigid-soft Coupling: Our MPM solver to PlasticineLab (Huang et al., 2021), but with a different contact modeling approach, which enables 2-way coupling with external rigid-body simulators. The coupling works by transferring rigid-body poses to the soft-body simulator and transferring soft-body forces to the rigid-body simulator. At the beginning of the simulation, all collision shapes in the external rigid-body simulator are copied to the soft-body simulator. Primitive shapes (box, capsule, etc.) are represented by analytical signed distance functions (SDFs); meshes are converted to SDF volumes, stored as 3D CUDA textures. After each rigid-body simulation step, we copy the poses and velocities of the rigid bodies to the soft-body simulator. During soft-body simulation steps, we evaluate the SDF functions at MPM particle positions and apply penalty forces to the particles, accumulating forces and torques for rigid bodies. In contrast, PlasticineLab evaluates SDF functions on MPM grid nodes, and applies forces to MPM grids. Our simulator supports both methods, but we find applying particle forces produces fewer artifacts such as penetration. After soft-body steps, we read the accumulated forces and torques from the soft-body simulator and apply them to the external rigid-body simulator. This procedure is summarized in Appendix D.1. Despite being a simple 2-way coupling method, it is very flexible, allowing coupling with any rigid-body simulator, so we can introduce anything from a rigid-body simulator into the soft-body environment, including robots and controllers.\n\nPerformance Optimization: The performance of our soft-body simulator is optimized in 4 aspects. First, the simulator is implemented in Warp, Nvidia’s JIT framework to translate Python code to native C++ and CUDA. Therefore, our simulator enjoys performance comparable to C and CUDA. Second, we have optimized the data transfer (e.g., poses and forces in the 2-way coupling) between CPU and GPU by further extending and optimizing the Warp framework; such data transfers are performance bottlenecks in other JIT frameworks such as Taichi (Hu et al., 2019), on which PlasticineLab is based. Third, our environments have much shorter compilation time and startup time compared to PlasticineLab thanks to the proper use of Warp compiler and caching mechanisms. Finally, since the simulator is designed for visual learning environments, we also implement a fast surface rendering algorithm for MPM particles, detailed in Appendix D.2. Detailed simulation parameters and performance are provided in Appendix D.3.\n\n4 PARALLELIZING PHYSICAL SIMULATION AND RENDERING\n\nManiSkill2 aims to be a general and user-friendly framework, with low barriers for customization and minimal limitations. Therefore, we choose Python as our scripting language to model environments, and the open-source, highly flexible SAPIEN (Xiang et al., 2020) as our physical\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Two pipelines for visual RL sample collection. (a) Sequential pipeline. (b) Our pipeline with asynchronous rendering and render server improves CPU utilization, reduces data transfer, and saves memory.\n\nsimulator. The Python language determines that the only way to effectively parallelize execution is through multi-process, which is integrated in common visual RL training libraries (Raffin et al., 2021; Weng et al., 2021). Under the multi-process environment paradigm, we make engineering efforts to enable our environments to surpass previous frameworks by increased throughput and reduced GPU memory usage.\n\nWhat are the Goals in Visual-Learning Environment Optimization? The main goal of performance optimization in a visual-learning environment is to maximize total throughput: the number of samples collected from all (parallel) environments, measured in steps (frames) per second. Another goal is to reduce GPU memory usage. Typical Python-based multi-process environments are wasteful in GPU memory usage, since each process has to maintain a full copy of GPU resources, e.g., meshes and textures. Given a fixed GPU memory budget, less GPU memory spent on rendering resources means more memory can be allocated for neural networks.\n\nAsynchronous Rendering and Server-based Renderer: Fig. 2(a) illustrates a typical pipeline (sequential simulation and rendering) to collect samples from multi-process environments. It includes the following stages: (1) do physical simulation on worker processes; (2) take pictures (update renderer GPU states and submit draw calls); (3) wait for GPU render to finish; (4) copy image observations to CPU; (5) compute rewards and get other observations (e.g., robot proprioceptive info); (6) copy images to the main python process and synchronize; (7) copy these images to GPU for policy learning; (8) forward the policy network on GPU; (9) copy output actions from the GPU to the simulation worker processes.\n\nWe observe that the CPU is idling during GPU rendering (stage 3), while reward computation (stage 5) often does not rely on rendering results. Thus, we can increase CPU utilization by starting reward computation immediately after stage 2. We refer to this technique as asynchronous rendering.\n\nAnother observation is that images are copied from GPU to CPU on each process, passed to the main python process, and uploaded to the GPU again. It would be ideal if we can keep the data on GPU at all times. Our solution is to use a render server, which starts a thread pool on the main Python process and executes rendering requests from simulation worker processes, summarized in figure 2(b). The render server eliminates GPU-CPU-GPU image copies, thereby reducing data transfer time. It allows GPU resources to share across any number of environments, thereby significantly reducing memory usage. It enables communication over network, thereby having the potential to simulate and render on multiple machines. It requires minimal API changes – the only change to the original code base is to receive images from the render server. It also has additional benefits in software development with Nvidia GPU, which we detail in Appendix E.2.\n\nComparison: We compare the throughput of ManiSkill2 with 3 other framework that support visual RL: Habitat 2.0 (Szot et al., 2021), RoboSuite 1.3 (Zhu et al., 2020), and Isaac Gym (Makoviychuk et al., 2021). We build a PickCube environment in all simulators. We use similar physical simulation parameters (500Hz simulation frequency and 20Hz control frequency2), and we use the GPU\n\n2A fair comparison of different frameworks is still challenging as their simulation and rendering differ in\n\nfidelity. We try our best to match the simulation parameters among the frameworks.\n\n6\n\nCopy State to Main Take Picture Receives (Thread Pool) Copy State to Main Copy State to Main Render Render Render Compute RewardGet Other Obs Compute RewardGet Other Obs Compute RewardGet Other Obs PhysicalSimulationTakePictures PhysicalSimulationPhysicalSimulationTakePictures TakePictures PolicyNetwork Copy State H2D Copy Action D2H Copy Actionto WorkersCopy Image to Main Worker 1 Worker 2 Worker N Main Compute RewardGet Other Obs PhysicalSimulationTakePictures PhysicalSimulationPhysicalSimulationTakePictures TakePictures PolicyNetwork Copy Image H2D Copy Action D2H Copy Actionto WorkersRender Render Copy ImageD2H Copy Image to Main Compute RewardGet Other Obs Copy ImageD2H Render Copy Image to Main Compute RewardGet Other Obs Copy ImageD2H EnvironmentStep EnvironmentStep CPU Work CommunicationGPU Work (a)(b)Published as a conference paper at ICLR 2023\n\nTotal FPS (rand. action) Total FPS (nature CNN) Optimal #Envs\n\nManiSkill2 ManiSkill2\n\nServer\n\n2487±24 2532±63 64\n\nSync\n\n942±19 931±4 32\n\n(a)\n\nHabitat 2.0\n\nRoboSuite 1.3\n\nIsaac Gym\n\n#Envs\n\nManiSkill2 Server\n\n1275±10 1224±13 64\n\n924±3 894±15 32\n\n865±35 835±5 512\n\n4 8\n64\n\n4.9G 5.1G 5.8G\n\n(b)\n\nHabitat 2.0\n\n6.4G 12.9G (OOM)\n\nTable 1: (a) Comparison of sample collection speed (FPS) on PickCube across different frameworks. (b) GPU memory usage on multi-process environments with 74 YCB objects each.\n\npipeline for rendering. Images have resolution 128x128. All frameworks are given a computation budget of 16 CPU cores (logical processors) of an Intel i9-9960X CPU with 128G memory and 1 RTX Titan GPU with 24G memory. We test with random actions and also with a randomly initialized nature CNN (Mnih et al., 2015) as policy network. We test all frameworks on 16, 32, 64, 128, 256, 512 parallel environments and report the highest performance. Results are shown in Table 1(a) (more details in Appendix E.3). An interesting observation is that our environment performs the best when the number of parallel environments exceeds the number of CPU cores. We conjecture that this is because execution on CPU and GPU are naturally efficiently interleaved through OS or driver-level schedulers when the requested computation far exceeds the available resources.\n\nAdditionally, we demonstrate the advantage of memory sharing thanks to our render server. We extend the PickClutterYCB environment to include 74 YCB objects per scene, and create the same setting in Habitat 2.0. As shown in Table 1(b), even though we enable GPU texture compression for Habitat and use regular textures for ManiSkill2, the memory usage of Habitat grows linearly as the number of environments increases, while ManiSkill2 requires very little memory to create additional environments since all meshes and textures are shared across environments.\n\n5 APPLICATIONS\n\nIn this section, we show how ManiSkill2 supports widespread applications, including sense-planact frameworks and imitation / reinforcement learning. In this section, for tasks that have asset variations, we report results on training objects. Results on held-out objects are presented in Appendix F. Besides, we demonstrate that policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\n5.1 SENSE-PLAN-ACT\n\nSense-plan-act (SPA) is a classical framework in robotics. Typically, a SPA method first leverages perception algorithms to build a world model from observations, then plans a sequence of actions through motion planning, and finally execute the plan. However, SPA methods are usually openloop and limited by motion planning, since perception and planning are independently optimized. In this section, we experiment with two perception algorithms, Contact-GraspNet (Sundermeyer et al., 2021) and Transporter Networks (Zeng et al., 2020).\n\nContact-GraspNet for PickSingleYCB: The SPA solution for PickSingleYCB is implemented as follows. First, we use Contact-GraspNet (CGN) to predict potential grasp poses along with confidence scores given the observed partial point cloud. We use the released CGN model pretrained on ACRONYM (Eppner et al., 2021). Next, we start with the predicted grasp pose with the highest score, and try to generate a plan to achieve it by motion planning. If no plan is found, the grasp pose with the next highest score is attempted until a valid plan is found. Then, the agent executes the plan to reach the desired grasp pose and closes its grippers to grasp the object. Finally, another plan is generated and executed to move the gripper to the goal position.\n\nFor each of the 74 objects, we conduct 5 trials (different initial states). The task succeeds if the object’s center of mass is within 2.5cm of the goal. The success rate is 43.24%. There are two main failure modes: 1) predicted poses are of low confidence (27.03% of trials), especially for objects (e.g., spoon and fork) that are small and thin; 2) predicted poses are of low grasp quality or unreachable (29.73% of trials), but with high confidence. See Appendix F.1 for examples.\n\nTransporter Network for AssemblingKits: We benchmark Transporter Networks (TPN) on our AssemblingKits. The original success metric (pose accuracy) used in TPN is whether the peg is\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nObs. Mode\n\nPickCube\n\nStackCube\n\nFill\n\nHang\n\nExcavate\n\nPour\n\nPinch\n\nWrite\n\nPoint Cloud RGBD\n\n0.22 ± 0.06 0.04 ± 0.02 0.45 ± 0.04 0.35 ± 0.15 0.08 ± 0.04 0.02 ± 0.02 0.00 ± 0.00 0.00 ± 0.00 0.01 ± 0.02 0.00 ± 0.00 0.62 ± 0.07 0.20 ± 0.12 0.21 ± 0.04 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nTable 2: Mean and standard deviation of the success rate of behavior cloning on rigid-body and soft-body tasks. For rigid-body assembly tasks not shown in the table, success rate is 0.\n\nObs. Mode\n\nPickCube\n\nStackCube\n\nPickSingleYCB PegInsSide\n\nPlugCharger AssemblingKits TurnFaucet AvoidObstacles\n\nPoint Cloud 0.94 ± 0.03 0.95 ± 0.02 0.51 ± 0.05 0.01 ± 0.01 0.01 ± 0.02 0.00 ± 0.00 0.04 ± 0.03 0.00 ± 0.00 0.91 ± 0.05 0.87 ± 0.04 0.18 ± 0.07 0.01 ± 0.01 0.01 ± 0.01 0.00 ± 0.00 0.03 ± 0.03 0.00 ± 0.00 RGBD\n\nTable 3: Mean and standard deviation of success rates of DAPG+PPO on rigid-body tasks. Training budget is 25M time steps.\n\nplaced within 1cm and 15 degrees of the goal pose. Note that our version requires pieces to actually fit into holes, and thus our success metric is much stricter. We train TPN from scratch with image data sampled from training configurations using two cameras, a base camera and a hand camera. To address our high-precision success criterion, we increase the number of bins for rotation prediction from 36 in the original work to 144. During evaluation, we employ motion planning to move the gripper to the predicted pick position, grasp the peg, then generate another plan to move the peg to the predicted goal pose and drop it into the hole. The success rate over 100 trials is 18% following our success metric, and 99% following the pose accuracy metric of (Zeng et al., 2020). See Appendix F.2 for more details.\n\n5.2\n\nIMITATION & REINFORCEMENT LEARNING WITH DEMONSTRATIONS\n\nFor the following experiments, unless specified otherwise, we use the delta end-effector pose controller for rigid-body environments and the delta joint position controller for soft-body environments, and we translate demonstrations accordingly using the approach in Sec.2.2. Visual observations are captured from a base camera and a hand camera. For RGBD-based agents, we use IMPALA (Espeholt et al., 2018) as the visual backbone. For point cloud-based agents, we use PointNet (Qi et al., 2017) as the visual backbone. In addition, we transform input point clouds into the end-effector frame, and for environments where goal positions are given (PickCube and PickSingleYCB), we randomly sample 50 green points around the goal position to serve as visual cues and concatenate them to the input point cloud. We run 5 trials for each experiment and report the mean and standard deviation of success rates. Further details are presented in Appendix F.3.\n\nImitation Learning: We benchmark imitation learning (IL) with behavior cloning (BC) on our rigid and soft-body tasks. All models are trained for 50k gradient steps with batch size 256 and evaluated on test configurations.\n\nResults are shown in Table 2. For rigid-body tasks, we observe low or zero success rates. This suggests that BC is insufficient to tackle many crucial challenges from our benchmark, such as precise control in assembly and large asset variations. For soft-body tasks, we observe that it is difficult for BC agents to precisely estimate action influence on soft body properties (e.g. displacement quantity or deformation). Specifically, agents perform poorly on Excavate and Pour, as Excavate is successful only if a specified amount of clay is scooped up and Pour is successful when the final liquid level accurately matches a target line. On the other hand, for Fill and Hang, which do not have such precision requirements (for Fill, the task is successful as long as the amount of particles in the beaker is larger than a threshold), the success rates are much higher. In addition, we observe that BC agents cannot well utilize target shapes to guide precise soft body deformation, as they are never successful on Pinch or Write. See Appendix F.5 for further analysis.\n\nRL with Demonstrations: We benchmark demonstration-based online reinforcement learning by augmenting Proximal Policy Gradient (PPO) (Schulman et al., 2017) objective with the demonstration objective from Demonstration-Augmented Policy Gradient (DAPG) (Rajeswaran et al., 2017). Our implementation is similar to Jia et al. (2022), and further details are presented in Appendix F.3. We train all agents from scratch for 25 million time steps. Due to limited computation resources, we only report results on rigid-body environments.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Left: Simulation and real world setup for PickCube. Right: Motion planning execution results for Pinch in simulation and in the real world: (a) initial state; (b) letter “M”; (c) letter “S”.\n\nResults are shown in Table 3. We observe that for pick-and-place tasks, point cloud-based agents perform significantly better than RGBD-based agents. Notably, on PickSingleYCB, the success rate is even higher than Contact-GraspNet with motion planning. This demonstrates the potential of obtaining a single agent capable of performing general pick-and-place across diverse object geometries through online training. We also further examine factors that influence point cloud-based manipulation learning in Appendix F.6. However, for all other tasks, notably the assembly tasks that require high precision, the success rates are near zero. In Appendix F.7, we show that if we increase the clearance of the assembly tasks and decrease their difficulty, agents can achieve much higher performance. This suggests that existing RL algorithms might have been insufficient yet to perform highly precise controls, and our benchmark poses meaningful challenges for the community.\n\nIn addition, we examine the influence of controllers for agent learning, and we perform ablation experiments on PickSingleYCB using point cloud-based agents. When we replace the delta endeffector pose controller in the above experiments with the delta joint position controller, the success rate falls to 0.22±0.18. The profound impact of controllers demonstrates the necessity and significance of our multi-controller conversion system.\n\n5.3 SIM2REAL\n\nManiSkill2 features fully-simulated dynamic interaction for rigid-body and soft-body. Thus, policies trained in ManiSkill2 have the potential to be directly deployed in the real world.\n\nPickCube: We train a visual RL policy on PickCube and evaluate its transferability to the real world. The setup is shown in Fig. 3-L, which consists of an Intel RealSense D415 depth sensor, a 7DoF ROKAE xMate3Pro robot arm, a Robotiq 2F-140 2-finger gripper, and the target cube. We first acquire the intrinsic parameters for the real depth sensor and its pose relative to the robot base. We then build a simulation environment aligned with the real environment. We train a point cloud-based policy for 10M time steps, where the policy input consists of robot proprioceptive information (joint position and end-effector pose) and uncolored point cloud backprojected from the depth map. The success rate in simulation is 91.0%. We finally directly evaluate this policy in the real world 50 times with different initial states, where we obtain 60.0% success rate. We conjecture that the performance drop is caused by the domain gap in depth maps, as we only used Gaussian noise and random pixel dropout as data augmentation during simulation policy training.\n\nPinch: We further evaluate the fidelity of our soft-body simulation by executing the same action sequence generated by motion planning in simulation and in the real world and comparing the final plasticine shapes. Results are shown in Fig. 3-R, which demonstrates that our 2-way coupled rigidMPM simulation is able to reasonably simulate plasticine’s deformation caused by multiple grasps.\n\n6 CONCLUSION\n\nTo summarize, ManiSkill2 is a unified and accessible benchmark for generic and generalizable manipulation skills, providing 20 manipulation task families, over 2000 objects, and over 4M demonstration frames. It features highly efficient rigid-body simulation and rendering system for sample collection to train RL agents, real-time MPM-based soft-body environments, and versatile multi-controller conversion support. We have demonstrated its applications in benchmarking senseplan-act and imitation / reinforcement learning algorithms, and we show that learned policies on ManiSkill2 have the potential to transfer to the real world.\n\n9\n\nSimulationRealDepth SensorRobot ArmGripperCube(a)(b)(c)Published as a conference paper at ICLR 2023\n\nREPRODUCIBLITY STATEMENT\n\nOur Benchmark, algorithms, and applications are fully open source. Specifically, we open source the ManiSkill benchmark as well as all simulators and renderers used to build it. We open source all code for demonstration generation and controller / action space conversion. We open source the entire learning framework used to train Imitation / Reinforcement Learning and Sense-Plan-Act algorithms. We release all training assets used in ManiSkill2. Since we will hold a public challenge based on ManiSkill2, code related to asset generation and cloud-based evaluation service cannot be released for fairness. Nonetheless, all results in this work are reproducible, and we welcome researchers to participate in our challenge.\n\nREFERENCES\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.\n\nKent Beck, Mike Beedle, Arie Van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning, Jim Highsmith, Andrew Hunt, Ron Jeffries, et al. Manifesto for agile software development, 2001.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\nBerk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR), pp. 510–517. IEEE, 2015.\n\nHilko Cords and Oliver G. Staadt. Instant liquids: Fast screen-space surface generation for particlebased liquids. In Poster proceedings of ACM Siggraph/Eurographics Symposium on Computer Animation, 2008.\n\nErwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,\n\nrobotics and machine learning. http://pybullet.org, 2016–2021.\n\nKiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: A framework for visual object manipulation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4495– 4504, 2021.\n\nClemens Eppner, Arsalan Mousavian, and Dieter Fox. Acronym: A large-scale grasp dataset based on simulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 6222–6227. IEEE, 2021.\n\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Impala: Scalable distributed deep-rl In International conference on machine\n\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. with importance weighted actor-learner architectures. learning, pp. 1407–1416. PMLR, 2018.\n\nChuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threedworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\n\nChuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, et al. The threedworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint arXiv:2103.14025, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJiayuan Gu, Devendra Singh Chaplot, Hao Su, and Jitendra Malik. Multi-skill mobile manipulation\n\nfor object rearrangement. arXiv preprint arXiv:2209.02778, 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nYuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. A moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics (TOG), 37(4):150, 2018.\n\nYuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fr ́edo Durand. Taichi: ACM\n\na language for high-performance computation on spatially sparse data structures. Transactions on Graphics (TOG), 38(6):1–16, 2019.\n\nZhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B Tenenbaum, and Chuang Gan. Plasticinelab: A soft-body manipulation benchmark with differentiable physics. arXiv preprint arXiv:2104.03311, 2021.\n\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019– 3026, 2020.\n\nZhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, and Hao Su.\n\nImproving policy In International Conference on Machine\n\noptimization with generalist-specialist Learning, pp. 10104–10119. PMLR, 2022.\n\nlearning.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nChengshu Li, Cem Gokmen, Gabrael Levine, Roberto Mart ́ın-Mart ́ın, Sanjana Srivastava, Chen Wang, Josiah Wong, Ruohan Zhang, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In 6th Annual Conference on Robot Learning, 2022.\n\nXingyu Lin, Yufei Wang, Jake Olkin, and David Held. Softgym: Benchmarking deep reinforcement\n\nlearning for deformable object manipulation. In Conference on Robot Learning, 2020.\n\nMinghua Liu, Xuanlin Li, Zhan Ling, Yangyan Li, and Hao Su. Frame mining: a free lunch for learning robotic manipulation from 3d point clouds. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=d-JYso87y6s.\n\nMiles Macklin. Warp: A high-performance python framework for gpu simulation and graphics. https://github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC).\n\nJeffrey Mahler, Florian T Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kr ̈oger, James Kuffner, and Ken Goldberg. Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards. In IEEE International Conference on Robotics and Automation (ICRA), pp. 1957–1964. IEEE, 2016.\n\nViktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021.\n\nRoberto Mart ́ın-Mart ́ın, Michelle A Lee, Rachel Gardner, Silvio Savarese, Jeannette Bohg, and Animesh Garg. Variable impedance control in end-effector space: An action space for In 2019 IEEE/RSJ International Conference on reinforcement learning in contact-rich tasks. Intelligent Robots and Systems (IROS), pp. 1010–1017. IEEE, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Humanlevel control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nDouglas Morrison, Peter Corke, and J ̈urgen Leitner. Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5 (3):4368–4375, 2020.\n\nTongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Cathera Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652–660, 2017.\n\nYuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, and Hao Su. S4g: Amodal single-view single-shot se (3) grasp detection in cluttered scenes. In Conference on robot learning, pp. 53–65. PMLR, 2020.\n\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Journal of Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Machine Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/ 20-1364.html.\n\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for\n\nrobotic manipulation. arXiv preprint arXiv:2209.05451, 2022.\n\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart ́ın-Mart ́ın, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pp. 477–490. PMLR, 2022.\n\nMartin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13438–13444. IEEE, 2021.\n\nAndrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems, 34:251–266, 2021.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\nXinyue Wei, Minghua Liu, Zhan Ling, and Hao Su. Approximate convex decomposition for 3d meshes with collision-aware concavity and tree search. ACM Transactions on Graphics (TOG), 41(4):1–18, 2022.\n\nJiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. arXiv preprint arXiv:2107.14171, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, et al. Envpool: A highly parallel reinforcement learning environment execution engine. arXiv preprint arXiv:2206.10558, 2022.\n\nFanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11097–11107, 2020.\n\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094–1100. PMLR, 2020.\n\nAndy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.\n\nYuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart ́ın-Mart ́ın.\n\nrobosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA SYSTEM DESIGN FOR DEVELOPMENT AND EVALUATION\n\nA.1 VERIFICATION-DRIVEN ITERATIVE DEVELOPMENT\n\nFigure 4: The workflow to build environments for generalizable manipulation skills.\n\nOur workflow, following agile software development (Beck et al., 2001), highlights a verificationdriven iterative development process. Different approaches can be used to generate demonstrations according to characteristics and difficulties of tasks, which verifies environments as a byproduct. The workflow is also designed to be scalable and affordable to continuous integration of assets and tasks. Fig 4 illustrates the workflow.\n\ntask creation, reward design, and observation configuration. Our workflow consists of 3 stages: including creating assets, (e.g., convex The first stage focuses on building task essentials, decomposition (Wei et al., 2022), texture baking), configuring robots, generating initial states, and defining success metrics. The second stage aims at prototyping shaped reward functions. The reward function is a requisite for methods like Model-Predictive Control (MPC) and RL. The third stage addresses observation spaces. For instance, camera parameters and placements need to be tailored for tasks so that observations contain adequate information. To collect demonstrations and verify environments, we employ one of or a mixture of 3 complementary approaches: task and motion planning (TAMP), MPC, and RL.\n\nDifferent methods have their own advantages and disadvantages. TAMP is free of crafting reward, and is suitable for many stationary manipulation tasks like pick-and-place, but shows difficulty when tackling underactuated systems (e.g. pushing chairs and moving buckets in Mu et al. (2021)). MPC is able to search solutions to difficult tasks given well-designed shaped rewards without training or observations. However, it is non-trivial to design a universal shaped reward for a variety of objects. RL requires additional training and hyperparameter tuning, but is more scalable than MPC during inference.\n\nA.2 CLOUD BASED EVALUATION SYSTEM\n\nTo allow the community to evaluate and benchmark together we build and provide a simple cloud based evaluation system. Users can register accounts and enter the benchmark and begin making submissions that solve our various tasks.\n\nA key feature of the evaluation system is flexibility. This is increasingly important as the number of unique approaches from heuristics, motion planners, and end-to-end RL solutions grow. Evaluation systems need the flexibility to allow all kinds of approaches to be benchmarked. To this end, we do the following.\n\nUser submissions are in the form of docker images, allowing users to install any code and save any models necessary to solve various tasks. This makes programming on the user’s side very flexible. The user only needs to provide a function that accepts observations and returns actions.\n\n14\n\nProcessTask CreationAsset CreationRobot ConfigurationInitial States GenerationSuccess MetricReward DesignReward DesignDevelopment TimelineTask and Motion PlanningModel Predictive ControlDemonstrationsVerification MethodsObservationReinforcement LearningImitation LearningPublished as a conference paper at ICLR 2023\n\nWe further give users the flexibility to configure the evaluation environment to suit their needs before running the evaluation. For example, users can define a configuration function in their solution that sets the observation mode (e.g. RGB-D or Point Cloud), as well as controllers (e.g. delta endeffector pose or joint position).\n\nTo benchmark a submitted docker image, we simply pull it to our server and run the evaluation code that loads the user’s solution. The results of the evaluation are then submitted to a database and displayed on a pubic benchmark.\n\nB DETAILS OF OUR COMPREHENSIVE CONTROLLER SUITE\n\nControllers are interfaces between policies and robots. Policies output actions to controllers, and controllers convert actions to control signals to drive the robot joints. In ManiSkill2, the default robot being controlled is the Franka Emika, also known as Panda. The degree of freedom (DoF) of a single Panda arm is 7.\n\nB.1 TERMINOLOGY\n\n1. Fixed Joint: A joint that cannot be controlled. The degree of freedom (DoF) is 0.\n\n2. qpos (q): Controllable joint positions.\n\n3. qvel ( ̇q): Controllable joint velocities.\n\n4. Target Joint Positions ( ̄q): Target positions of motors that drive each joint.\n\n5. Target Joint Velocities ( ̇ ̄q): Target velocities of motors that drive each joint.\n\n6. End-effector Position (p): The position of an end-effector.\n\n7. End-effector Rotation (R): The rotation of an end-effector.\n\n8. End-effector Target Position ( ̄p): The target position of an end-effector. 9. End-effector Target Rotation ( ̄R): The target rotation of an end-effector. 10. PD Controller: Control loop based on τ (t) = Kp( ̄q(t) − q(t)) + Kd( ̄ ̇q(t) − ̇q(t)). Kp (stiffness) and Kd (damping) are hyperparameters. τ (t) denotes the torque (generalized force) of the motors.\n\n11. Augmented PD Controller: Augmented PD controller: Passive forces (like gravity) are\n\ncompensated for the PD controller.\n\n12. Action (a): Input to the controllers, and also output of the policy.\n\n13. Tool Center Point (TCP): TCP is a user defined coordinate frame, often relatively fixed to the robot end effector. For example, in our case, if the robot uses a two-finger gripper, TCP is defined at the center point between the gripper’s two fingers.\n\nB.2 TARGET VS. NON-TARGET CONTROLLERS\n\nIn our controller implementation, we have the notion of “target” and “non-target” controllers. For example, when we say delta end-effector pose controller, the new desired pose is specified relative to the current end-effector pose. In contrast, if we say target delta end-effector pose controller, the new desired pose is specified relative to the previous desired pose. Please read the next section for their formal definitions.\n\nB.3 NORMALIZED ACTION SPACE\n\nWe design the action space of controllers to conform to the preferences of users. As RL users generally prefer normalized action space, most controllers listed below will have a normalized action space [−1, 1], with a few exceptions listed individually.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB.4 DETAILS OF CONTROLLERS\n\nB.4.1 ARM CONTROLLERS\n\n1. arm pd joint pos (7-dim, unnormalized): at = ̄qt. The target joint velocities ̇qt are always 0. As this controller is suitable for motion planning, the action space of this controller is not normalized.\n\n2. arm pd joint delta pos (7-dim): at = ̄qt − qt−1.\n\n3. arm pd joint target delta pos (7-dim): at = ̄qt − ̄qt−1.\n\n4. arm pd ee delta pos (3-dim): at = ̄pt − pt−1, where pt is the position of the end-effector at timestep t. The controller then internally computes the target joint positions of the robot: qt = IK( ̄pt, ̄Rt), where IK(·, ·) computes the joint positions from the end-effector’s position and rotation before sending the joint positions to the PD controller. Note that this controller only controls the position, but not the rotation, of the end-effector.\n\n5. arm pd ee target delta pos (3-dim): at = ̄pt − ̄pt−1\n\n6. arm pd ee delta pose (6-dim):\n\nto the previous arm pd ee delta pos controller with the addition of end-effector’s rotation being passed in as input. Thus ̄Tt = Ta · Tt−1, where ̄T is the target transformation of the end-effector, and Ta is the delta pose induced by the 3D position and 3D rotation (represented as compact axis-angle) of the action.\n\nis very similar\n\nThis controller\n\n7. arm pd ee target delta pose (6-dim): ̄Tt = Ta · ̄Tt−1. 8. arm pd joint vel (7-dim): at = ̄ ̇qt. The stiffness value Kp of this controller is always set\n\nto 0.\n\n9. arm pd joint pos vel (14-dim): An extension of arm pd joint pos that supports target\n\nvelocities input.\n\n10. arm pd joint delta pos vel (14-dim): The delta variant of the arm pd joint pos vel\n\ncontroller.\n\nB.4.2 GRIPPER CONTROLLER (1-DIM)\n\nWe use joint position control for the gripper, and we force the two gripper fingers to have the same target position.\n\nB.5 EFFECTIVENESS OF CONVERSION OF DEMONSTRATION ACTION SPACES\n\nIn this section, we exemplify the success rate of our demonstration conversion method by converting from the arm pd joint pos controller to the arm pd ee delta pose controller. A demonstration is converted successfully if, following the same trajectory initialization and the converted actions, the task is successful at the last time step. We experiment on 4 representative tasks: PickSingleYCB, AssemblingKits, TurnFaucet, Write. For each task, we select 100 demonstrations randomly. The success rates for PickSingleYCB, AssemblingKits, Write, TurnFaucet are 99%, 98%, 100%, and 80%, respectively. Note that our policy to generate demonstrations for TurnFaucet involves rich and inconsistent contact between the end-effector and the faucet handle (i.e., our policy uses the gripper to push the handle, rather than grasping and rotating it, in which case force closure can lead to more consistent contact). Such polices can be sensitive to accumulated errors during execution, which can result in task failure, although the actions converted by our demonstration conversion method attempt to reproduce motion faithfully.\n\nC DETAILS OF OBSERVATIONS, TASK FAMILIES, DEMONSTRATIONS AND\n\nEVALUATION PROTOCOLS\n\nUnless otherwise noted, all demonstrations are generated through TAMP. For evaluation, we employ a two-stage setup. Final result is the average result from the two stages.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC.1 SUPPORTED OBSERVATION MODES\n\nWe support most observation modes found in OpenAI gym (Brockman et al., 2016). The details of each observation mode are listed below.\n\n1. state dict: Returns a dictionary of states that contains robot proprioceptive information, ground truth object information (such as object poses), and task-specific goal information (if given). Visual observations (images and point clouds) are not included.\n\n2. state: Returns the flattened version of a state dict.\n\n3. rgbd: Returns rendered RGBD images from all cameras, along with robot proprioceptive\n\ninformation and task-specific goal information (if given).\n\n4. rgbd robot seg: On top of rgbd, returns ground truth segmentation masks for the robot\n\njoints.\n\n5. pointcloud: Returns a fused point cloud from all cameras, along with robot proprioceptive\n\ninformation and task-specific goal information (if given).\n\n6. pointcloud robot seg: On top of pointcloud, returns ground truth segmentation masks for\n\nthe robot joints.\n\nHere, the robot proprioceptive information includes joint positions, joint velocities, the pose of the robot base, along with the pose of the gripper’s tool center point if the robot uses a two-finger gripper.\n\nNote that different from the previous version of ManiSkill, ManiSkill2 does not include groundtruth segmentation in the default observation modes (rgbd or pointcloud). All visual-based experiments in this paper do not leverage such privileged information. For example, to specify which link of a faucet should be manipulated in TurnFaucet, we use its initial position instead of a ground-truth segmentation mask. Besides, we also support observations modes (rgbd+robot seg, pointcloud+robot seg) to provide the segmentation masks of robot links, which facilitates robotic applications and can be obtained in the real world using the robot proprioceptive information.\n\nC.2 PICK-AND-PLACE\n\nPickCube\n\n• Objective: Pick up a cube and move it to a goal position.\n\n• Success Metric: The cube is within 2.5 cm of the goal position, and the robot is static.\n\n• Demonstration Format: 1,000 successful trajectories.\n\n• Evaluation Protocol: 100 episodes with different initial joint positions of the robot and initial cube\n\npose for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: 3D goal position of the cube.\n\nStackCube\n\n• Objective: Pick up a red cube and place it onto a green one.\n\n• Success Metric: The red cube is placed on top of the green one stably and it is not grasped.\n\n• Demonstration Format: 1,000 successful trajectories.\n\n• Evaluation Protocol: 100 episodes with different initial joint positions of the robot and initial\n\nposes of both cubes for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: None.\n\nPickSingleYCB\n\n• Objective: Pick up a YCB object and move it to a goal position.\n\n• Success Metric: The object is within 2.5 cm of the goal position, and the robot is static.\n\n• Demonstration Format: 100 successful trajectories for each of the 74 YCB objects.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n• Evaluation Protocol: In addition to the training objects, we also use another confidential set of 40 objects from other sources as the test object set. For the two evaluation stages in total, for each object in the training set, we test 5 episodes with different seeds. For each object in the test set, we test 10 episodes with different seeds. Half of the objects are evaluated in each stage.\n\n• Task-specific Extra Observations: 3D goal position of the object.\n\nPickSingleEGAD\n\n• Objective: Pick up an EGAD object and move it to a goal position. The color for the EGAD\n\nobject is randomized.\n\n• Success Metric: The object is within 2.5 cm of the goal position, and the robot is static.\n\n• Demonstration Format: 5 trajectories for each of the 1,600 training objects sampled from EGAD. For certain objects where it’s difficult to apply TAMP, we might provide less than 5 trajectories.\n\n• Evaluation Protocol: For this task, we have held out a portion of the EGAD dataset. This held-out test dataset consists of 150 objects. During evaluation, in each stage, we evaluate 1 trajectory for each of the 150 objects sampled from the training dataset and 2 trajectories for each of the 75 objects sampled from the held-out test dataset.\n\n• Task-specific Extra Observations: 3D goal position of the object.\n\nPickClutterYCB\n\n• Objective: Pick up an object from a clutter of 4-8 YCB objects.\n\n• Success Metric: The object is within 2.5 cm of the goal position, and the robot is static.\n\n• Demonstration Format: A total of 4986 trajectories from the training object set.\n\n• Evaluation Protocol: In addition to the training objects, we also use another confidential set of 40 objects from other sources as the test object set. For each evaluation stage, we test 100 episode configurations on the training object set and on the test object set.\n\n• Task-specific Extra Observations: 3D position of the object to pick up, and 3D position of the\n\ngoal.\n\nC.3 ASSEMBLY\n\nAssemblingKits\n\n• Objective: Insert an object into the corresponding slot on a plate.\n\n• Success Metric: An object must fully fit into its slot, which must simultaneously satisfy 3 criteria: (1) height of the object center is within 3mm of the height of the plate; (2) rotation error is within 4 degrees; (3) position error is within 2cm.\n\n• Demonstration Format: We provide 1,720 trajectories in total. These trajectories are generated\n\nfrom over 300 kit configurations and 20 training shapes.\n\n• Evaluation Protocol: This task has a held-out test dataset for evaluation. The test dataset features 20 shapes that are similar to the shapes in the training set. We provide samples for test assets in Fig. 5. In each evaluation stage, we evaluate on 100 sampled training episode configurations and 100 sampled test dataset configurations.\n\n• Task-specific Extra Observations: 3D initial and goal position of the object to be placed.\n\nPegInsertionSide\n\n• Objective: Insert a peg into the horizontal hole in a box.\n\n• Success Metric: Half of the peg is inserted into the hole.\n\n• Demonstration Format: 1,000 successful trajectories.\n\n• Evaluation Protocol: 100 episodes with different initial joint positions of the robot, initial poses\n\nof the peg and box, the position and size of the hole for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: None.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: A sample plate with test assets.\n\nPlugCharger\n\n• Objective: Plug a charger into a wall receptacle.\n\n• Success Metric: The charger is fully inserted into the receptacle.\n\n• Demonstration Format: 1,000 successful trajectories.\n\n• Evaluation Protocol: 100 episodes with different initial joint positions of the robot, initial poses\n\nof the charger and wall for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: None.\n\nC.4 MISCELLANEOUS TASKS\n\nAvoidObstacles\n\n• Objective: Navigate the robot arm through a region of dense obstacles and move the end-effector\n\nto a goal pose. The shape and color of dense obstacles are randomized.\n\n• Success Metric: The end-effector pose is within 2.5 cm and 15 degrees of the goal pose.\n\n• Demonstration Format: 1976 trajectories for different layouts.\n\n• Evaluation Protocol: 100 episodes with different layouts of obstacles for each of stage 1 and stage\n\n2.\n\n• Task-specific Extra Observations: The goal pose of the end-effector.\n\nTurnFaucet\n\n• Objective: Turn on a faucet by rotating its handle.\n\n• Success Metric: The faucet handle has been turned past a target angular distance.\n\n• Demonstration Format: For most faucet models, we provide 100 trajectories per asset. For approximately 15 of the 60 training models were TAMP cannot find a solution, demonstrations are generated through MPC-CEM using our designed rewards.\n\n• Evaluation Protocol: This task has a held-out test object set. For the two evaluation stages in total, we evaluate 5 episodes for each of the 60 training objects and 17 episodes for each of the 18 test objects. Half of the objects are evaluated in each stage.\n\n• Task-specific Extra Observations: The remaining angular distance to rotate the handle, the target handle position (since there can be multiple handles in a single faucet), and the direction to rotate the handle specified as 3D joint axis.\n\nC.5 SOFT-BODY MANIPULATION\n\nFill\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n• Objective: Fill clay from a bucket into the target beaker.\n\n• Success Metric: The amount of clay inside the target beaker > 90%; soft body velocity < 0.05.\n\n• Demonstration Format: 200 successful trajectories generated through motion planning.\n\n• Evaluation Protocol: 100 episodes with different initial rotations of the bucket and initial positions\n\nof the beaker for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: Beaker position.\n\nHang\n\n• Objective: Hang a noodle on a target rod.\n\n• Success Metric: Part of the noodle is higher than the rod; two ends of the noodle are on different sides of the rod; the noodle is not touching the ground; the gripper is open; soft body velocity < 0.05.\n\n• Demonstration Format: 200 successful trajectories generated through motion planning.\n\n• Evaluation Protocol: 100 episodes with different initial positions of the gripper and rod poses for\n\neach of stage 1 and stage 2.\n\n• Task-specific Extra Observations: Rod position.\n\nExcavate\n\n• Objective: Lift a specific amount of clay to a target height.\n\n• Success Metric: The amount of lifted clay must be within a given range; the lifted clay is higher than a specific height; fewer than 20 clay particles are spilled on the ground; soft body velocity < 0.05.\n\n• Demonstration Format: 200 successful trajectories generated through motion planning.\n\n• Evaluation Protocol: 100 episodes with different bucket poses and initial heightmaps of clay for\n\neach of stage 1 and stage 2.\n\n• Task-specific Extra Observations: Target clay amount.\n\nPour\n\n• Objective: Pour liquid from a bottle into a beaker.\n\n• Success Metric: The liquid level in the beaker is within 4mm of the red line; the spilled water is fewer than 100 particles; the bottle returns to the upright position in the end; robot arm velocity < 0.05.\n\n• Demonstration Format: 200 successful trajectories generated through motion planning.\n\n• Evaluation Protocol: 100 episodes with different bottle positions, the level of water in the bottle,\n\nand beaker positions for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: Red line height.\n\nPinch\n\n• Objective: Deform plasticine into a target shape.\n\n• Success Metric: The Chamfer distance between the current plasticine and the target shape is less\n\nthan 0.3t, where t is the Chamfer distance between the initial shape and target shape.\n\n• Demonstration Format: 1556 successful trajectories generated through heuristic motion planning.\n\nDifferent trajectories correspond to different target shapes.\n\n• Evaluation Protocol: 50 episodes with different target shapes for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: RGBD / point cloud observations of the target plasticine from\n\n4 different views.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nWrite\n\n• Objective: Write a given character on clay. The target character is randomly sampled from an\n\nalphabet of over 50 characters.\n\n• Success Metric: The IoU (Intersection over Union) between the current pattern and the target\n\ncharacter is larger than 0.8.\n\n• Demonstration Format: 200 successful trajectories generated through heuristic motion planning.\n\n• Evaluation Protocol: 50 episodes with different target characters for each of stage 1 and stage 2.\n\n• Task-specific Extra Observations: The depth map of the target character.\n\nC.6 MOBILE MANIPULATION\n\nOpenCabinetDrawer\n\n• Objective: A single-arm mobile robot needs to open a designated target drawer on a cabinet. The\n\nfriction and damping parameters for the drawer joints are randomized.\n\n• Success Metric: The target drawer has been opened to at least 90% of the maximum range, and\n\nthe target drawer is static.\n\n• Demonstration Format: 300 trajectories for each target drawer in the training object set. The\n\ntraining object set consists of 25 cabinets. Each cabinet could contain multiple drawers.\n\n• Evaluation Protocol: This task has a held-out test object set (10 unseen cabinets). In the first stage, we evaluate 250 trajectories in total. Among these 250 trajectories, 125 levels are evenly distributed over 5 unseen objects in the test set, and the other 125 levels are evenly distributed over all objects in the training set. In the second stage, we evaluate another 250 trajectories. Similarly, 125 levels come from the training set and the other 125 levels from the 5 other unseen objects in the test set (different from the 5 test objects in stage 1).\n\n• Task-specific Extra Observations: Since one cabinet can contain several drawers, we specify the\n\ntarget drawer by its initial center of mass.\n\nOpenCabinetDoor\n\n• Objective: A single-arm mobile robot needs to open a designated target door on a cabinet. The\n\nfriction and damping parameters for the door joints are randomized.\n\n• Success Metric: The target door has been opened to at least 90% of the maximum range, and the\n\ntarget door is static.\n\n• Demonstration Format: 300 trajectories for each target door in the training object set. The training\n\nobject set consists of 42 cabinets. Each cabinet could contain multiple doors.\n\n• Evaluation Protocol: This task has a held-out test object set (10 unseen cabinets). The evaluation\n\nprotocol is similar to OpenCabinetDrawer.\n\n• Task-specific Extra Observations: Since one cabinet can contain several doors, we specify the\n\ntarget door by a segmentation mask.\n\nPushChair\n\n• Objective: A dual-arm mobile robot needs to push a swivel chair to a target location on the ground (indicated by a red hemisphere) and prevent it from falling over. The friction and damping parameters for the chair joints are randomized.\n\n• Success Metric: The chair is close enough (within 15 cm) to the target location, is static, and does\n\nnot fall over.\n\n• Demonstration Format: 300 trajectories for each chair in the training object set. The training\n\nobject set consists of 26 chairs.\n\n• Evaluation Protocol: This task has a held-out test object set (10 unseen chairs). The evaluation\n\nprotocol is similar to OpenCabinetDrawer.\n\n• Task-specific Extra Observations: None.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nMoveBucket\n\n• Objective: A dual-arm mobile robot needs to move a bucket with a ball inside and lift it onto a\n\nplatform.\n\n• Success Metric: The bucket is placed on or above the platform at the upright position, and the\n\nbucket is static, and the ball remains in the bucket.\n\n• Demonstration Format: 300 trajectories for each bucket in the training object set. The training\n\nobject set consists of 29 buckets.\n\n• Evaluation Protocol: This task has a held-out test object set (10 unseen buckets). The evaluation\n\nprotocol is similar to OpenCabinetDrawer.\n\n• Task-specific Extra Observations: None.\n\nD SOFT-BODY DETAILS\n\nD.1 SOFT-BODY SIMULATION AND 2-WAY COUPLING ALGORITHM\n\nThe simulation and 2-way coupling algorithm are summarized in algorithm 1.\n\nAlgorithm 1 Rigid MPM Simulation and Dynamic Coupling\n\ninitialize rigid scene initialize soft scene copy rigid body shapes and center of mass to soft scene initialize renderer for environment step do\n\nexecute ManiSkill2 controllers for rigid step per environment step do\n\nprocess ManiSkill2 substep step rigid scene copy rigid body poses to soft scene initialize force-torque buffers per rigid body for soft step per rigid step do compute penalty forces accumulate equivalent rigid-body forces and torques in force-torque buffers MPM particle to grid (mass, momentum, forces) MPM grid compute velocity MPM grid to particle (velocity) integrate MPM particles\n\nend for apply accumulated forces and torques on rigid bodies\n\nend for copy rigid body states to SAPIEN renderer copy MPM particles to SAPIEN renderer execute renderer\n\nend for\n\nD.2 SOFT-BODY RENDERING\n\nTo support visual learning, we extended SAPIEN’s renderer to support rendering particle-based soft body. To render particle-based material, one common approach is to convert the particles to triangle meshes using a meshing algorithm such as marching cubes; PlasticineLab implements a ray-tracing framework that renders spheres directly. Our approach is screen-space splatting (Cords & Staadt, 2008), similar to Nvidia Flex’s built-in renderer. We customize SAPIEN’s shader to render softbody particles as spheres, use a bilateral filter to smooth the depth buffer, then compute normal and lighting on the smoothed soft-body depth. These are implemented as extra screen-space render passes. The effect of the smoothing filter is shown in figure 6. Moreover, we customize Warp to support the transfer of particle positions from simulation to renderer with a single GPU-GPU copy;\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nthis further reduces rendering latency. A concern of the screen-space splatting is the inconsistency across different views due to the use of screen-space filters. However, in practice, by scaling the bilateral filter according to pixel distance from camera, the rendering results produced are visually consistent most of the time.\n\n(a)\n\n(b)\n\nFigure 6: Our particle renderer (a) without bilateral filter, and (b) with bilateral filter.\n\nD.3 SOFT BODY DETAILS\n\nWe summarize the key parameters In table 4. For all soft body environments with rendering, a single environment runs at around 17-18 FPS; 16 parallel environments on a single GPU gives overall 8084 FPS on a single RTX Titan GPU (4x real time). This performance gain is mainly due to the sequential CPU-rigid-body and GPU-soft-body simulation. It is also partially due to a single CPU processor not able to submit CUDA kernels fast enough to keep up with the GPU. Therefore, it can potentially be further optimized with vectorized simulation, which we leave as future work.\n\nGrid Length Particle Volume Density Young’s Modulus Poisson Ratio Yield Stress\n\n0.005 to 0.015 6.2e-8/1.2e-7 300 to 3000\n\n1e4/3e5\n\n0.3\n\n2e3/1e4\n\nTable 4: Ranges for key parameters used in our MPM simulation. All fields are in SI units.\n\nE PERFORMANCE OPTIMIZATION DETAILS\n\nE.1 RENDER SERVER IMPLEMENTATION\n\nOur render server is implemented with the gRPC framework, which exchanges Protocal Buffers with the HTTP 2 protocol over networks or unix sockets. The server side is managed by a thread pool, listening to client requests on multiple concurrent threads. For a “take picture” request from a worker process, our implementation puts the task of updating GPU matrices and launching draw calls onto another thread and returns immediately back to the worker process. This ensures minimum waiting time on the worker side.\n\nOn the rendering server, all rendering resources are managed by a central resource manager. Any resource loading request (e.g., models, images, textures) must go through the manager. The manager ensures only one copy of any resource is loaded onto the GPU, shared across potentially multiple scenes.\n\nE.2 ADDITIONAL BENEFITS OF RENDER SERVER\n\nIn general, rendering resource sharing across processes is not well supported. Rendering frameworks like OpenGL and Vulkan are designed to be efficient in a single-process multi-threaded environment. Resources and documentations on multi-process renderer are rarely provided. Therefore as a developer, it is far easier to understand renderers in a single process setting.\n\nIn addition, Nvidia’s GPU profiler, Nsight System, is currently unable to profile Vulkan in multiple processes on Linux in our experiments. Running the renderer in multiple processes makes it hard to understand GPU performance. Thus, running rendering in one main process is almost required for a developer to optimize for hardware utilization.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Comparison of sample collection speed (FPS) with random actions and with Nature CNN-sampled actions across different frameworks and different numbers of parallel environments. “ManiSkill2-Sync-Render” refers to ManiSkill2 with synchronous rendering and without render server. “ManiSkill2” refers to ManiSkill2 with asynchronous rendering but without render server. “ManiSkill2-Server” refers to ManiSkill2 with both asynchronous rendering and render server enabled. Some curves are not fully drawn beyond a certain number of parallel environments due to performance drop (“ManiSkill2-Server” & “Isaac Gym”), GPU out of memory (“RoboSuite” & “ManiSkill2-Sync-Render” & “ManiSkill2”), crash (“Habitat 2.0”).\n\nE.3 MORE DETAILS ON SAMPLE COLLECTION SPEED COMPARISON\n\nWe use the PickCube environment to compare the sample collection speed between different simulators. The resulting total number of vertices of collision meshes is 1009 for ManiSkill2 and Habitat-2.0 and 1210 for IsaacGym and Robosuite. The number of vertices of visual meshes is 69747 for all simulators. Each control step consists of 25 physical simulation steps. We use the GPU pipeline for rendering. All frameworks are given a budget of 16 CPU cores(logical processors) of an Intel i9-9960X CPU with 128G memory and 1 RTX Titan GPU with 24G memory.\n\nTo complement results in Table 1, we plot further details about the relationship between the number of parallel environments and the policy sample collection speed across different frameworks in Figure 7. We adopt an agent that outputs random actions, along with a CNN-based agent that uses a randomly-initialized nature CNN (Mnih et al., 2015) as its visual backbone. We observe that ManiSkill2 with asynchronous rendering enabled (and without the render server) is already able to outperform the speed of other frameworks. With render server enabled, ManiSkill2 further achieves 2000+ FPS with 16 parallel environments on a single GPU.\n\nF ADDITIONAL EXPERIMENT DETAILS, RESULTS, AND ANALYSIS\n\nF.1 CONTACT-GRASPNET FOR PickSingleYCB\n\nWe directly use the pretrained model provided by the original authors of Contact-GraspNet (Sundermeyer et al., 2021). We exemplify successful trajectories by the model in Fig. 8, as well as common failure modes in Fig. 9.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 8: Sampled frames demonstrating a correct and successful grasp of a can. Frame (a) shows the initial state; (b) shows the gripper approaching the predicted grasp from above; (c) shows the gripper grasping the can; (d) shows the robot moving the can towards the green goal position.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 9: Examples of unsuccessful grasps. (a) shows an erroneous rotation prediction in grasp pose; (b) shows a correct rotation prediction in grasp pose, but the gripper is not close enough to the object to grasp it; (c) shows a reasonable grasp pose, but the gripper will slip away from the bottle upon finger closure due to friction and bottle geometry; (d) shows a reasonable grasp pose that is not achievable through motion planning due to kinematic constraints of the robot arm.\n\nF.2 TRANSPORTER NETWORK FOR AssemblingKits\n\nGripper Type\n\nSuccess (ours) Success (Zeng et al.) Pos < 5mm Pos < 2.5mm Pos < 1mm Rot < 4° Rot < 1° Rot < 0.25°\n\nTwo-finger Gripper Suction Gripper\n\n0.18 0.15\n\n0.98 0.93\n\n0.98 0.89\n\n0.80 0.66\n\n0.30 0.18\n\n0.96 0.91\n\n0.48 0.48\n\n0.22 0.22\n\nGripper Type\n\nSuccess (ours) Success (Zeng et al.) Pos < 5mm Pos < 2.5mm Pos < 1mm Rot < 4° Rot < 1° Rot < 0.25°\n\nTwo-finger Gripper Suction Gripper\n\n0.18 0.14\n\n0.99 0.96\n\n0.99 0.92\n\n0.82 0.66\n\n0.31 0.17\n\n0.96 0.94\n\n0.48 0.52\n\n0.24 0.31\n\nTable 5: Success rate of Transporter Networks on our AssemblingKits task on training assets (up) and test assets (down). We also report the success rate based on the original paper, where a trial is successful if the target piece is placed within 1cm and 15 degrees of the goal position.\n\nWe adopt the official code from Zeng et al. (2020). To encourage precise rotation prediction, we increase the number of rotation prediction bins from 36 in the original work to 144. We further benchmark with two grippers: a suction gripper following the original work, and another two-finger gripper from Franka Panda. Our training dataset is generated from random initial configurations of training assets in AssemblingKits. More specifically, for each sampled initial configuration, we capture RGBD images from the hand-camera and the base-camera, unproject them to 3D point clouds, fuse the point clouds, and render the top-down orthographic image to feed to the Transporter Network model. The ground truth labels, namely the object pick position and the goal pose to place the object, are directly obtained from the environment state information.\n\nTable 5 shows the results. The success rate of Transporter Network following our success criterion (which requires pieces to fully fit in holes) is a lot lower than following the metric in the original paper. We observe that failure modes are mainly due to imprecise rotation/position prediction for placing the target piece.\n\nF.3 DETAILED SETUP FOR IMITATION LEARNING & RL FROM DEMONSTRATIONS\n\nIn ManiSkill2, our demonstrations are provided using the joint position controller. Before we train demonstration-based agents on rigid & soft-body tasks, we first use the approach in Sec.2.2 to translate the provided demonstrations into the delta end-effector pose controller for rigid-body environments, and into the delta joint position controller for soft-body environments. We then filter the translated trajectories, such that only successful trajectories are used for agent learning.\n\nFor RGBD-based agents, we use IMPALA (Espeholt et al., 2018) as the visual backbone, and we concatenate images captured from the base camera and the hand camera as visual input. Image resolution is 128x128. For point cloud-based agents, we use PointNet (Qi et al., 2017) as the visual backbone. We first obtain a single fused point cloud by transforming point clouds from different cameras into the robot-base frame and concatenating the points together. We then remove the ground using height clip and randomly downsample the point cloud to 1200 points. For rigidbody environments, we also transform the point cloud (along with robot proprioceptive information\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nObs. Mode\n\nPickSingleYCB AssemblingKits\n\nTurnFaucet\n\nAvoidObstacles\n\nPoint Cloud RGBD\n\n0.36 ± 0.06 0.08 ± 0.03\n\n0.00 ± 0.00 0.00 ± 0.00\n\n0.01 ± 0.01 0.01 ± 0.01\n\n0.00 ± 0.00 0.00 ± 0.00\n\nTable 6: Mean and standard deviation of success rates of DAPG+PPO on rigid-body tasks on heldout test objects. Training budget is 25M time steps.\n\nand goal position, if given) into the end-effector frame. In addition, for environments where goal positions are given (PickCube and PickSingleYCB), we randomly sample 50 green points around the goal position to serve as visual cues and concatenate them to the input point cloud.\n\nTo train demonstration-based agents, for rigid-body environments, we use 1000 demonstration trajectories, except environments that have multiple assets (PickSingleYCB: 7300 trajectories; TurnFaucet: 4500 trajectories; AssemblingKits: 1700 trajectories). For soft-body environments, we use 200 demonstration trajectories (except Pinch with 1550 trajectories).\n\nIn this work, our demonstration-based online RL algorithm is modified from Proximal Policy Gradient (PPO) (Schulman et al., 2017) and Demonstration-Augmented Policy Gradient (DAPG) (Rajeswaran et al., 2017). We adopt the training objective modified from Jia et al. (2022). Here, we use ρD and ρπ to denote the distribution of demonstration transitions and online environment rollout transitions, respectively. We can then write the overall policy loss as follows (value loss omitted here):\n\nLCLIP\n\nρ\n\n(θ) = −E(s,a)∼ρ\n\n(cid:20)\n\nmin\n\n(cid:18) πθ(a|s) πθold (a|s)\n\nˆA(s, a), clip(rt(θ), 1 − ε, 1 + ε) ˆA(s, a)\n\n(cid:19)(cid:21)\n\nL1 LDAP G+P P O(θ) = LCLIP\n\nρ(θ) = −E(s,a)∼ρ[πθ(a|s)] (θ) + ω · L1\n\nρπ\n\n(θ)\n\nρD\n\nWe set ω = 0.1 · 0.995N , where N is the epoch count for PPO. A PPO epoch is defined as online environment sampling steps followed by policy and value network updates.\n\nFor further details about networks and algorithm hyperparameters, see Appendix F.8.\n\nF.4 RESULTS FOR DAPG+PPO ON HELD-OUT OBJECT SETS\n\nIn Section 5.2, for tasks that have asset variations, we presented evaluation results on the training object set. In this section, we present results on the held-out object set. See Table 6 for details.\n\nF.5 FURTHER ANALYSIS OF IMITATION LEARNING ON SOFT-BODY TASKS\n\nWe observe that it is difficult for BC agents to accurately estimate the influence of an action on finegrained soft body properties, such as displacement quantity and deformation. For example, both Fill and Pour tasks require a robot agent to move soft body objects (clay or liquid) into a target container, but Fill has a much higher success rate. An underlying cause is that Fill allows the robot agent to put all of the clay into the beaker while Pour requires higher precision i.e. the final liquid level must match the target line. Therefore, agents need to precisely control the bottle tilt angle in order to precisely control the amount of liquid poured into the beaker. Similarly, for Excavate, agents must perform fine judgments on how deep they should dig in order to scoop up a specified amount of clay. On the other hand, Hang does not require an agent to perform high accuracy measurements, so it is easier for agents to succeed.\n\nIn addition, we notice that BC agents cannot well utilize target shapes to guide precise soft body deformation. Specifically, for Pinch and Write that require shape deformation, the BC models have very poor performance. As shown in Fig. 10, the robot learns the motion of pinching and has made some progress toward the goal, but it is not good enough. Similarly, the robot agent has learned to draw some patterns, but they are not close enough to the target character.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Behaviour cloning examples for Pinch and Write tasks. BC models have learned to make some progress towards the goals but not enough to meet the success conditions.\n\nOriginal\n\nDelta Joint Position Controller Robot Base Frame Point Cloud Remove Visual Goal Cues\n\n0.51 ± 0.05\n\n0.22 ± 0.18\n\n0.00 ± 0.00\n\n0.16 ± 0.07\n\nTable 7: Ablations on PickSingleYCB (training object set) for point cloud-based agents trained with DAPG+PPO. The “original” result refers to the result in Table 3, which uses the delta end-effector pose controller, transforms inputs point clouds into the robot’s end-effector frame, and (for pick-andplace tasks where goal position is given) appends 50 green points sampled around the goal position to the input point clouds to serve as visual cues.\n\nF.6 MORE RESULTS ON POINT CLOUD-BASED MANIPULATION LEARNING\n\nIn this section, we examine factors that influence point cloud-based manipulation learning to complement the results in Section 5.2. Results are shown in Table 7. In addition to our prior observation that choices of controllers have a significant effect on performance, we also observe that (1) selecting good coordinate frames to represent input point clouds could be crucial for agents’ success, which corroborates the findings in Liu et al. (2022); (2) adding proper visual cues could benefit point cloud-based agent learning.\n\nF.7 MORE ANALYSIS ON ASSEMBLY TASKS\n\nIn Section 5.2, we observed that agents trained with Imitation Learning or Reinforcement Learning perform poorly on many tasks that require high precision, such as the assembly tasks. We further analyze sources of difficulty from these tasks by training agents on easier versions of these tasks where the clearance threshold is significantly increased. Results are shown in Table 8. We observe that when we increase clearance threshold and decrease task difficulty, some agents are able to achieve a lot higher performance. However, even in this case, many agents still do not perform well. We conjecture that this is due to many other challenging factors, such as occlusions of the target slots when agents attempt to insert a peg or a charger.\n\nF.8 NETWORK ARCHITECTURES AND HYPERPARAMETERS FOR IL & RL\n\nIn this section, we present the specific network architectures and algorithm hyperparameters used for Section 5.2. Here we define “state vector” as the concatenation of proprioceptive robot state information and task-specific goal information (if given).\n\nFor IMPALA (Espeholt et al., 2018), the visual backbone is similar to ResNet10 (He et al., 2016), with hidden size in all layers equal to 64, normalizations removed, and the first convolution layer modified to have kernel size 4 and stride 4. Features from the last layer are projected to a 384-\n\n27\n\nBehaviour Cloning ResultsGoalsPublished as a conference paper at ICLR 2023\n\nObservation Mode PegInsertionSide(1x) PegInsertionSide(10x) PlugCharger(1x) PlugCharger(10x)\n\nPoint Cloud RGBD\n\n0.00 ± 0.00 0.00 ± 0.00\n\n0.01 ± 0.01 0.00 ± 0.00\n\n0.00 ± 0.00 0.00 ± 0.00\n\n0.01 ± 0.01 0.00 ± 0.00\n\nObservation Mode PegInsertionSide(1x) PegInsertionSide(10x) PlugCharger(1x) PlugCharger(10x)\n\nPoint Cloud RGBD\n\n0.01 ± 0.02 0.01 ± 0.01\n\n0.74 ± 0.10 0.05 ± 0.03\n\n0.01 ± 0.01 0.01 ± 0.01\n\n0.02 ± 0.02 0.29 ± 0.07\n\nTable 8: Analysis of IL & demonstration-based RL on assembling tasks. We control the difficulty of tasks by changing clearance configurations. Top: Behavior Cloning. Bottom: DAPG+PPO. 1x=default clearance (3mm for PegInsertionSide and 0.5mm for PlugCharger); 10x = 10 times default clearance.\n\ndimensional vector before being concatenated with the state vector. For PointNet (Qi et al., 2017), the hidden layer sizes are [64, 128, 512] before maxpooling over the number-of-points dimension. We do not use spatial normalization layers, and we add layer normalization to the output of each intermediate layer before maxpooling.\n\nFor both BC and DAPG+PPO, we use learning rate of 3e-4 for training and optimize with Adam (Kingma & Ba, 2015). For DAPG+PPO, the actor and critic networks share the same visual backbone. In addition, when training point cloud-based policies, we initialize the linear layer right before the policy and value head output to be zero. We find this helpful for stabilizing point cloudbased policy learning. Hyperparameters are shown in Table 9.\n\nHyperparameters\n\nValue\n\nDiscount (γ) λ in GAE PPO clip range Max KL Gradient norm clipping Entropy Number of samples per PPO step Number of samples per minibatch Number of critic warmup epochs Number of PPO update epochs Critic loss coefficient Demonstration loss coefficient Recompute advantage after each PPO update epoch Reward normalization Advantage normalization Only use critic loss to update visual backbone Reset environment upon success during policy rollout\n\n0.95 0.95 0.2 0.1 0.5 0.0 20000 300 4\n2 0.5 0.1 · 0.995N True True True True False\n\nTable 9: Hyperparameters for DAPG+PPO.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nG COMPARISON WITH OTHER BENCHMARKS FOR ROBOTIC MANIPULATION\n\nBenchmark\n\nManiSkill2 BEHAVIOR-1K3 Habitat 2.04 IsaacGym5 ManipulaThor6 MetaWorld Robosuite RLBench TDW7\n\nGrasp implementation #Demo trajectories Multi-controller support Visual RL/IL baselines #Object models #Scenes Ray-tracing support Domain randomization Rigid-body simulation Soft-body simulation Warp-MPM10\n\nPhysical >30k Yes Full >2144*8 -\nKuafu Partial9 SAPIEN\n\nAbstract -\nUnknown Limited 3324 306 Omniverse Unknown Omniverse Omniverse\n\nAbstract -\nYes Full YCB 105 -\nNo Bullet -\n\nPhysical -\nNo No -\n- -\nYes PhysX 5 -\n\nAbstract -\nNo Full 150 30 -\nNo Unity -\n\nPhysical\n\nPhysical Abstract Abstract\n\nProcedural ∼2000 Procedural Yes No 10 -\nNVISII Yes\n\nNo No 80 -\n- No\n\nYes No 28 -\n- Yes V-REP -\n\nMujoco Mujoco\n\n-\n\n-\n\n- No No 112 105 Unity No Unity -\n\nTable 10: Comparison with other existing benchmarks for robotic manipulation. The information of each benchmark is based on its major focus. For example, all the simulation backends of these benchmarks can support physically implemented grasp, but some of them focus on high-level actions and thus use abstract grasp for benchmarking algorithms. Multi-controller support measures whether a benchmark has implemented multiple controllers and provided interfaces to select one for each task.\n\nTable 10 compares ManiSkill2 with other existing benchmarks for robotic manipulation. ManiSkill2 features large-scale demonstrations for every task, a great variety of objects, multi-controller support and conversion of demonstration action spaces, and a focus on fully physically implemented grasp. We invested significant efforts to select, fix, and re-model objects and integrate them to our task families, generate demonstrations with fully physical grasping, and perform large-scale visual manipulation benchmarking. Through these processes, we carefully verify all of our tasks. The low success rates on many tasks demonstrate that our benchmark poses interesting and challenging problems for the community.\n\nWe are actively working on current limitations of ManiSkill2: photorealism, domain randomization, and scene-level variation. First, the simulation backend (SAPIEN) of ManiSkill2 supports a raytracing renderer (Kuafu). However, photorealism is usally achieved at the cost of speed. For example, BEHAVIOR-1K Li et al. (2022) can only achieve 60 FPS with Nvidia Omniverse and require high-end GPUs. In this work, we focus on physically realistic short-horizon and lowlevel visuomotor manipulation. We have experimentally supported ray-tracing rendering for use cases like generating data offline for supervised learning or fine-tuning a policy learned on nonphotorealistic data. Second, we have included domain randomization for physical parameters and visual appearance for some of tasks. We will provide tutorials for users to customize domain randomization according to their use cases. Third, we are introducing mobile manipulation tasks similar to those in ManipularTHOR ArmPointNav (Ehsani et al., 2021) and Habitat 2.0 (Szot et al., 2021) but with physically implemented base movement and grasp. Those tasks will involve scenelevel variations and demand advanced navigation abilities.\n\n3Since BEHAVIOR-1K (Li et al., 2022) is not public yet, some detailed information is unknown. 4For Habitat 2.0 (Szot et al., 2021), we refer to its Home Assistant Benchmark. 5For IsaacGym (Makoviychuk et al., 2021), we refer to its 4 manipulation tasks in the original paper: Franka\n\nCube Stacking, Shadow Hand, Allegro Hand, and TriFinger.\n\n6For ManipularTHOR (Ehsani et al., 2021), we refer to its ArmPointNav task. 7For TDW (Gan et al., 2020), we refer to its Transport Challenge (Gan et al., 2021) 8Assets like boards with slots for AssemblingKits, feasible layouts of obstacles for AvoidObstacles, and\n\ntarget characters for Write are not included. All of these assets require offline generation.\n\n9ManiSkill2 has included some domain randomization for physical parameters (e.g., joint damping and friction of PushChair, OpenCabinetDoor, and OpenCabinetDrawer) and visual appearance (e.g., colors in PickSingleEGAD and PandaAvoidObstacles).\n\n10ManiSkill2 implements a MPM-based soft-body simulator based on Warp (Macklin, 2022). Deformable\n\nobjects like cloth are not supported yet.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nH CONTRIBUTIONS\n\n• Designed and implemented ManiSkill2 infrastructure: Jiayuan Gu, Fanbo Xiang, Rui Chen\n\n• Designed rigid-body environments and collected demonstrations: Jiayuan Gu, Xuanlin Li,\n\nXiqiang Liu, Tongzhou Mu\n\n• Designed soft-body environments and collected demonstrations: Fanbo Xiang, Xinyue Wei\n\n• Designed and implemented server-based asynchronous rendering: Fanbo Xiang, Zhan\n\nLing, Jiayuan Gu\n\n• Designed and implemented the soft-body simulator: Fanbo Xiang, Xinyue Wei, Xiaodi\n\nYuan\n\n• Conducted sense-plan-act experiments: Stone Tao, Xiqiang Liu, Jiayuan Gu\n\n• Conducted RL/IL experiments on rigid-body environments: Yunchao Yao, Xuanlin Li,\n\nZhan Ling\n\n• Conducted IL experiments on soft-body environments: Yihe Tang, Xinyue Wei, Zhan Ling\n\n• Conducted real-world experiments: Rui Chen, Pengwei Xie\n\n• Implemented the cloud-based evaluation system: Stone Tao, Fanbo Xiang\n\n• Managed or advised on the project: Hao Su, Rui Chen, Zhiao Huang\n\n30",
    "reference": "# Summary Of The Paper\n\nThe paper presents a new benchmark environment for manipulation tasks. The environment supports both traditional rigid-body manipulation tasks and soft-body tasks, while unified interface allows a wide range of algorithms. Results show that the asynchronous rendering and render server approach provides an improved FPS for CNN-based policy learning.\n\n# Strength And Weaknesses\n\nStrength:\n1.\tThe proposed environment supports a wide range of observation and controller types and supports mainstream algorithms in sense-plan-act, RL and IL frameworks.\n2.\tThe tasks include those from previous works and some newly proposed ones, covering a wide aspect of challenges in manipulation. The support for soft-body tasks is of great importance.\n3.\tThe asynchronous rendering approach and render server implementation allow improved rendering performance and reduced memory usage.\n\nWeaknesses:\n1.\tAll the proposed manipulation tasks are based on robotic arms and lack support for other actuators (e.g. Dexterous hands).\n2.\tSince different simulators have different parameters that may drastically affect the overall performance, please include the details of parameters for each simulator (e.g. Did you use the GPU pipeline in IsaacGym or just used the CPU pipeline, the substeps for physics simulation, the number of the total vertex in the simulation). \n3.\tThe performance comparison is carried out on RGBD input settings. The performance of pure state input and point-cloud input is not reported.\n4.\tThe effectiveness and accuracy of demonstration conversion are neither discussed in detail nor measured quantitatively.\n5.\tPoint-cloud observations in some tasks contain ground truth segmentation, which cannot be easily obtained in the real world. This barrier for sim-to-real transfer could be avoided with a better observation design.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is of good quality. The illustrations and experimental results are clear to me.\n\n# Summary Of The Review\n\nThis work proposes several improvements to the current simulator for manipulation. The improvements in rigid-body soft-body interface and simulation performance, along with the variety of supported tasks, is of great value. This work's weakness includes omitted details from different sections and an issue in observation. Overall, I am leaning towards acceptance.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nRETHINKING THE VALUE OF PROMPT LEARNING FOR VISION-LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLarge-scale visual-language pre-training like CLIP has demonstrated great success in open-set visual concept learning that enables zero-shot transfer to downstream tasks through prompting. To automate prompt engineering, prompt learning is proposed to automatically learn the optimal task-relevant prompts. In this paper, we make some surprising observations that contradict common beliefs about prompts. We observe that even random prompts can achieve pretty good performance for zero-shot recognition. We also find that prompt learning gives comparable or worse performance than directly fine-tuning of the linear classifier. Moreover, prompt learning is no more than parameter-efficient learning, and is a trade-off between optimality and generalization. Our results highlight the need for the rethinking of existing prompt learning, more careful baseline evaluations in future research on prompt learning methods in vision-language models.\n\n1\n\nINTRODUCTION\n\nBuilding a state-of-the-art visual recognition system is one of the core tasks in the field of computer vision. Current state-of-the-art visual recognition systems are almost all based on Deep Neural Networks (DNNs), which can be roughly divided into two parts: a non-linear feature extractor and a linear classifier. For traditional visual recognition, where the class number are fixed and the labels are discretized, the standard practice is to assign each category with a weight vector, which is optimized to maximize the classification accuracy. Take the ResNet for ImageNet classification as an example, the weight vectors for 1000 classes form the weight matrix W ∈ R1000×4096 of the linear classifier (the last fully-connected layer of ResNet), where 4096 is the dimension of the features from the feature extractor. This learning paradigm can only learn closed-set visual concepts related to the pre-defined categories, and can not generalize to new classes beyond these closed-set categories.\n\nIn contrast to supervised learning with fixed labels of a closed-set categories, visual concept learning with the supervision of text has shown great potential. The main inspiration is that language is a high level abstraction of human understanding the world, thus it contains rich information and can naturally generalize well. One of the representative works is the CLIP (Contrastive LanguageImage Pretraining) (Radford et al., 2021), which learns joint representations of vision and language using contrastive learning on large-scale image and text data. Thanks to the rich information and the generality of natural language, the CLIP model can learn diverse and task-agnostic visual-textual representations, which can be generalized to many downstream tasks even under the zero-shot setting. This is done by using the names of all classes of a downstream task as the text for textual feature extraction, and conducting classification based on the alignment score of the visual features and the textual features for each class. However, using the class names as the text is deficient due to the lack of context. To this end, the authors of Radford et al. (2021) resort to the technique of prompt tuning (Liu et al., 2021a). Here the “prompt” is a cloze templates which specifies the context about the task at hand. They find that the template “a photo of a {CLASS}.” is a good prompt for image classification. By using elaborate prompt engineering and ensemble, much higher zero-shot performance can be achieved.\n\nPrompt engineering has shown greater transferability than the contextless baseline of using class names. The drawback is that the handcrafted prompt tuning requires prior knowledge about the downstream task. Moreover, as pointed out in Zhou et al. (2022b), the performance is very sensitive\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nto a slight change in the wording of the prompt template. Thus prompt tuning is a non-trivial task. To solve this problem, the authors of Zhou et al. (2022b) bring the concept of prompt learning from natural language processing (NLP) and propose Context Optimization (CoOp) to automate the prompt engineering in vision-language models. More recent works including (Ju et al., 2021; Yao et al., 2021; Zhou et al., 2022a) are continually developed. The core idea of these prompt learning approaches is to treat the embeddings of the words in a prompt as a set of learnable vectors, which are learned through back-propagation w.r.t. the downstream task loss.\n\nPrompts can encode context information expressed in natural language about the target tasks, thus they can generalize well and show promising results even in zero-shot. Prompt learning, which automatically optimize the prompts in the same word embedding space of natural language, is believed to have two advantages. First, it is believed that prompt learning converge faster and requires fewer training examples than fine-tuning. This is because only the context vectors are updated while the pre-trained parameters of both text encoder and image encoder are fixed. Moreover, during the gradients calculation, the pre-trained knowledge encoded in the text encoder can also be back-propagated through the network to the context vectors. Therefore, prompt learning is commonly believed to be superior to linear probe, partial fine-tuning or even full fine-tuning. Second, it is believed that the learned prompts have strong robustness and generalization ability, as the optimization is conducted in the NLP embedding space, thus the learned prompts are expected to provide high generalization ability in the same way as natural language.\n\nIn this paper, we test these two beliefs by evaluating the prompt tuning/learning performance of CLIP on various downstream tasks. We start from examining the influence of text encoder on the prompts through handcrafted prompts and random prompts and show that the text encoder can indeed provide some regularization on the prompts. To our surprise, we find that even random prompts can still achieve pretty good performance for zero-shot recognition. Then, we compare prompt learning and fine-tuning for closed-set recognition, and observe that prompt learning gives comparable or worse performance than directly fine-tuning the weights of the linear classifier. Last, we examine the generalization ability of the learned prompts, and reveal that prompt learning is no more than parameter-efficient learning, and is a trade-off between optimality and generalization.\n\n2 RELATED WORKS\n\nPrompt learning is originally proposed to transfer knowledge from pre-trained language models to downstream tasks, which has demonstrated great performance in NLP domain Devlin et al. (2018); Brown et al. (2020). A typical example of prompt learning is “fillin-the-blank” cloze templates Petroni et al. (2019), which transforms the down-stream task to a format familiar to the pre-trained model. Instead of manually designing prompt templates, later studies focus on automated prompt learning which can be categorized into discrete prompts and continuous prompts Liu et al. (2021a). Researchers discover the discrete prompts in a discrete space, e.g. natural language phrases, and most works generate discrete prompts by either gradient-based search Wallace et al. (2019), or prompt mining Jiang et al. (2020), or prompt generation Gao et al. (2020), etc. Instead of limiting the prompt to human-interpretable natural language domain, continuous prompts in the embedding space of the model are proposed. Several representative methods on continuous prompts learning include prefix tuning Li & Liang (2021), tuning initialized with discrete prompts Zhong et al. (2021), and hard-soft prompt hybrid tuning Liu et al. (2021b).\n\nMotivated by the well performance of prompt learning on NLP, recently researchers begin to apply it into the vision-language models. CLIP Radford et al. (2021) uses a manually designed prompt on the text encoder, which enables the zero-shot image classification of vision-language model. To avoid human efforts on prompt design, CoOp Zhou et al. (2022b) proposes a continuous prompts learning method and two implementations that can be applied on different recognition tasks. Yet CoOp Zhou et al. (2022b) seems over-fitting the base classes in the training, resulting in inferior performance on unseen classes even within the same dataset. To cure this problem, CoCoOp Zhou et al. (2022a) propose to generate an input-conditional vector for each image by a lightweight neural network, which boosts the classifier performance on new classes. Although CoOp and CoCoOp achieve promising improvements, they requires supervised data from the target datasets which may restrict the model scalability. In the contrary, Huang et al. Huang et al. (2022) propose the unsupervised prompt learning (UPL) method which improves transfer performance of CLIP-like VL models\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwithout labeled data. Different from above prompt learning methods which apply the prompts on the text encoder of VL model, VPT Jia et al. (2022) uses prompts learning on the image encoder. Specifically, they prepend a small amount (less than 1% of model parameters) of trainable parameters into the input sequence of transformer layers, and keep the model backbone frozen. Besides the prompt learning for the classification task, there are some studies about transferring knowledge from VL models to other downstream tasks, such video understanding Ju et al. (2021), object detection Du et al. (2022), and visual grounding Yao et al. (2021).\n\n3 PROMPT LEARNING BASED ON CLIP\n\nThe analysis throughout this paper is based on CLIP model (Radford et al., 2021), which consists of an image encoder f (·) and a text encoder g(·). The image encoder is usually a ResNet or a ViT, while the text encoder is a Transformer. Through contrastive learning, the two encoders are trained to transform input images and texts into the same feature space.\n\nThe aligned visual-textual feature space makes CLIP to be capable of zero-shot image recognition. Specifically, the input image x is feed into the image encoder to obtain the visual representation f . Similarly, for each category, the class name (e.g., “dog”) wrapped in the prompt template (e.g., “a photo of a {CLASS}.”) is feed into the text encoder to obtain the textual representation {wi}K\n\ni=1, where K is the class number. Then the prediction probability is as follows:\n\np(y = i|x) =\n\nexp(cos(f , wi)/T )\n\n(cid:80)K\n\nj=1 exp(cos(wj, wjx)/T )\n\n,\n\n(1)\n\nwhere T is the Softmax temperature and cos(·, ·) denotes cosine similarity.\n\nTo ease the prompt engineering process, the concept of prompt learning is proposed. Context Optimization (CoOp) (Zhou et al., 2022b) is one of the earliest works that introduce prompt learning to adapt pre-trained vision-language models to downstream tasks. The key idea of prompt learning is to automatically learn the prompt template instead of using a handcrafted template. Specifically, CoCop introduces a set of learnable prompt vectors with the following format,\n\nt = [v]1[v]2 · · · [v]M [v]CLASS, where {[v]m, m = 1, · · · , M } are the set of the learnable word embeddings of the prompt template, which are shared for all classes, M is the number of context tokens of the prompt, and [v]CLASS is the embedding of the class name. For each class i ∈ 1, · · · , K, we can obtain the prompt ti according to Eq. (2). Then the prediction probability is as follows:\n\n(2)\n\np(y = i|x) =\n\nexp(cos(f , g(ti))/T ) j=1 exp(cos(f , g(tj))/T )\n\n(cid:80)K\n\n,\n\n(3)\n\nWith the prediction probability of Eq. (3) and the classification loss of the downstream task, we can optimize the learnable prompt vectors {[v]m, m = 1, · · · , M } while frozening the pre-trained weights of the CLIP. Specifically, the gradients w.r.t. the learnable prompt vectors can be backpropagated all the way through the text encoder g(·), at which time, the pre-trained knowledge encoded in the text encoder of CLIP can be distilled to the learnable prompt vectors. In this way, the learned prompts can encode some useful information about the downstream task.\n\n4 TEXT ENCODER AS A REGULARIZATION\n\nIn section 3, we show that different prompts would result in different weights for classification, i.e., wi = g(ti), for i = 1, · · · , K. In this section, we examine how different prompts would affect the performance of classification across a wide range of datasets.\n\n4.1 HANDCRAFTED PROMPTS\n\nWe start from handcrafted prompts. Previous works have shown that a good handcrafted prompt could greatly improve zero-shot classification accuracy Radford et al. (2021). In this section, we evaluate the performance of zero-shot classification with various handcrafted prompt templates.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Various handcrafted prompts.\n\nPrompt Type ClassName Basic Revised Negative\n\nPrompt Template\n\n“{CLASS}” “a photo of a {CLASS}.” “this is a photo of a {CLASS}.” “this is not a photo of a {CLASS}.”\n\nTable 2: Results of zero-shot classification with various handcrafted prompts and random prompts across the 11 datasets.\n\nDataset ImageNet Caltech101 OxfordPets StanfordCars Flowers102 Food101 FGVCAircraft SUN397 DTD EuroSAT UCF101\n\nClassName Basic Revised Negative\n\nRandToken RandEmbed\n\n55.3 80.9 78.8 54.4 57.3 73.9 15.3 54.9 41.1 28.4 56.4\n\n58.2 85.9 83.7 55.6 60.9 75.3 15.7 58.5 40.0 24.2 58.3\n\n59.2 87.0 84.7 55.6 62.8 77.2 15.8 57.7 41.1 28.0 57.9\n\n58.0 84.6 81.0 49.0 62.7 75.5 15.0 58.5 42.3 36.7 57.7\n\n50.0±2.7 75.3±3.4 73.2±3.8 52.6±0.9 51.4±2.6 72.8±3.1 12.2±2.5 47.6±3.3 34.1±5.6 22.4±3.9 52.5±2.4\n\n47.0±2.8 74.7±5.4 72.3±3.4 46.5±2.5 46.7±7.6 69.0±2.8 10.2±2.1 45.0±2.4 28.6±3.4 24.2±4.5 47.9±2.2\n\nFirst, we want to evaluate the prompts used by current prompt-based methods. The simplest baseline is to directly use the class name as the input of the text encoder (ClassName). To improve performance, the authors of CLIP Radford et al. (2021) propose a basic prompt template for image recognition (Basic). The authors of Ju et al. (2021) further revise the prompts by adding a “this is” prefix (Revised). Second, we want to examine what will happen if we use the “negative prompt” for the classification task, i.e., by adding a “not” in the prompt template (Negative). For clearness, we summarize the handcrafted templates studied in this section in Table 1.\n\nFollowing (Zhou et al., 2022b), we conduct experiments on the 11 image classification datasets used in CLIP which are publicly available, i.e., ImageNet (Deng et al., 2009), Caltech101 (Fei-Fei, 2004), OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), FGVCAircraft (Maji et al., 2013), SUN397 (Xiao et al., 2010), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019) and UCF101 (Soomro et al., 2012). The results are summarized in Table 2.\n\nAs expected, the results in Table 2 show that the ClassName prompt template achieves the worst performance. On only one dataset (EuroSAT), the ClassName could outperforms the Basic and the Revised prompt. There is a large accuracy improvement using the Basic prompt template introduced in Radford et al. (2021). The accuracy can be further improved when using the Revised template (Ju et al., 2021), which achieves the best accuracy for 8 out of the 11 datasets.\n\nAn important finding we want to point out is that the relative performance for these three handcrafted prompts are somewhat consistent across various dataset. Specifically, the Basic template outperforms the ClassName prompt for 9 out of the 11 datasets, and the Revised template outperforms the Basic template for other 9 out of the 11 datasets. This finding indicates that the handcrafted prompts expressed in natural language can generalize well across various datasets. In other word, a prompt template works on one dataset has a high probability to also work on another dataset.\n\nAnother surprising observation is that the negative prompt can also achieve very good performance. On most of the datasets, the Negative prompt shows quit large improvement over the ClassName template. By comparing with the other handcrafted prompts, it even get the best accuracy on 3 out of the 11 datasets.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 RANDOM PROMPTS\n\nThen we evaluate what would happen if random prompts are used. Here we consider two kinds of random prompts, namely the random token template and the random embedding template:\n\n• Random Token Template prepends some random word IDs which are selected from the\n\n49,152 vocabulary of CLIP.\n\n• Random Embedding Template prepends some random embedding vectors after trans-\n\nforming token IDs into word embedding.\n\nSimilarly, we evaluate the performance across 11 image classification datasets. Note that the selection of the random seed would make a large difference in performance. So we run the experiments for 10 times with different random seeds and report the average performance. The results are shown in the last columns of Table 2. To our surprise, the results show that even random prompts can still achieve pretty good performance for zero-shot recognition. It seems that the text encoder can indeed provide some regularization at the encoding process of the input text with prompts.\n\n4.3 SUMMARY\n\nBy now, we have evaluated the performance of different handcrafted prompts and random prompts. In the following list, we summarize our findings, some of which are quit surprising.\n\n1. Handcrafted prompts expressed in natural language show great power and generalization ability, and the relative performance for different prompt templates are somewhat consistent across a wide range of datasets.\n\n2. The negative prompts, which provide wrong context information for the downstream tasks,\n\ncan also achieve very good performance.\n\n3. Even random prompts can still achieve pretty good performance for zero-shot recognition.\n\n5 PROMPT LEARNING V.S. CLASSIFIER FINE-TUNING\n\nIn this section, we evaluate the effect of prompt learning for closed-set image classification task, where the training set and testing set are from the same categories. For closed-set classification, it is widely believed that prompt learning converge faster and requires fewer training examples than fine-tuning. The reasons are two-fold. First, in prompt learning, only the prompt vectors are learned while the pre-trained CLIP model is fixed. Second, as shown in Eq. 3, to optimize the prompt vectors {[v]m, m = 1, · · · , M }, the gradients need to be back-propagated all the way through the text encoder g(·). This process allows the knowledge learned by the CLIP model to be distilled from the weights to the prompts (Zhou et al., 2022b).\n\nIn this paper, we challenge this common belief. By comparing Eq. 1 and Eq. 3, it is easy to see that wi = g(ti) for i = 1, · · · , K, which can be viewed as the weights of the last classifier. At inference time, we first need to feed each ti into the text encoder to obtain wi, then the generated weights {wi, i = 1, · · · , K} are used for classification. Our key question is that if we could directly optimize wi, why should we optimize the latent vector ti? To this end, we conduct comprehensive experiments on the 11 image classification datasets to see if prompt learning (optimize ti) is superior to classifier fine-tuning (optimize wi).\n\n5.1 TRAINING DETAILS\n\nFor prompt learning, we utilize the Context Optimization (CoOp) method proposed in Zhou et al. (2022b), which is one of the earliest works that introduce prompt learning to adapt pre-trained vision-language models to downstream tasks. We use the CLIP pre-trained model with ResNet-50 as the image encoder. The number of learnable prompt vectors M is set to 16 and are shared across call categories, which is the default setting for in the original paper of Zhou et al. (2022b). During the experiments, we find that the results for prompt learning on downstream tasks are very sensitive to the choice of hyper-parameters. For fair comparison, we use the grid search over the training epochs and learning-rate and report the best accuracy for all experiments. Specifically, the number\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Comparison between prompt learning (CLIP + Prompt Learning) and classifier fine-tuning (CLIP + Classifier FT) of few-shot learning on the 11 datasets. The results for zero-shot CLIP (stars) and linear probe (dashed lines) are also given.\n\nof training epoch is set to {50, 100, 200}. The starting learning-rate is set to {2e − 2, 2e − 3, 2e − 4} and {2e−1, 2e−2, 2e−3} for prompt learning and classifier fine-tuning, which are the best choices for both cases. Other hyper-parameters are the same as in Zhou et al. (2022b).\n\n5.2 RESULTS\n\nWe compare the performance of prompt learning and classifier fine-tuning across all 11 datasets. Following Zhou et al. (2022b), we report few-shot learning results with 1/2/4/8/16 shots for training while using the original test set for testing. All the results for prompt learning and classifier finetuning are the average over 3 runs with different random seeds. We also give the zero-shot results using CLIP model as well as the linear probe results based on CLIP for comparison. The main results are shown in Figure 1.\n\nFrom Figure 1, we can see that both the prompt learning and classifier fine-tuning can dramatically outperforms the zero-shot and the linear probe based on CLIP for most of experiments. Compared with prompt learning, the simple classifier fine-tuning cal obtain much higher accuracy, except for the OxfordPets dataset, on which the classifier fine-tuning is slightly inferior to prompt learning. The average results on 11 datasets displayed in the top-left corner of Figure 1 show that classifier\n\n6\n\n0124816Number of labeled training examples per class354045505560657075Score (%)Zero-shotCLIPAverage over 11 datasetsCLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class2030405060Score (%)Zero-shotCLIPImageNetCLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class7075808590Score (%)Zero-shotCLIPCaltech101CLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class30405060708090Score (%)Zero-shotCLIPOxfordPetsCLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class3040506070Score (%)Zero-shotCLIPStanfordCarsCLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class6065707580859095Score (%)Zero-shotCLIPFlowers102CLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class304050607080Score (%)Zero-shotCLIPFood101CLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class1520253035Score (%)Zero-shotCLIPFGVCAircraftCLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class3540455055606570Score (%)Zero-shotCLIPSUN397CLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class3035404550556065Score (%)Zero-shotCLIPDTDCLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class4050607080Score (%)Zero-shotCLIPEuroSATCLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP0124816Number of labeled training examples per class4045505560657075Score (%)Zero-shotCLIPUCF101CLIP + Prompt LearningCLIP + Classifier FTLinear probe CLIP11Under review as a conference paper at ICLR 2023\n\nTable 3: The optimization time (in minutes) of prompt learning and classifier fine-tuning with various backbones for 16 shots ImageNet classification.\n\nMethod Prompt Learning Classifier Finetuning\n\nResNet-50 ResNet-101 ViT-B/32 ViT-B/16\n\n139.5 13.8\n\n143.3 15.9\n\n136.3 13.7\n\n143.1 14.8\n\nTable 4: Comparison between prompt learning and classifier fine-tuning on robustness to distribution shift using different vision backbones. Bold value indicates the best result.\n\nMethod\n\nResNet-50 Zero-Shot CLIP Linear Probe CLIP Prompt Learning Classifier Fine-tuning\n\nResNet-101 Zero-Shot CLIP Linear Probe CLIP Prompt Learning Classifier Fine-tuning\n\nViT-B/32 Zero-Shot CLIP Linear Probe CLIP Prompt Learning Classifier Fine-tuning\n\nViT-B/16 Zero-Shot CLIP Linear Probe CLIP Prompt Learning Classifier Fine-tuning\n\nSource\n\nTarget\n\nImageNet\n\n-V2\n\n-Sketch\n\n-A\n\n-R\n\nAverage\n\n58.18 55.87 63.00 64.73\n\n61.62 59.75 66.53 67.50\n\n62.05 59.58 66.80 68.10\n\n66.73 65.85 71.97 72.97\n\n51.34 45.97 55.27 56.03\n\n54.81 50.05 58.73 58.60\n\n54.79 49.73 58.43 58.17\n\n60.83 56.26 64.40 64.47\n\n33.32 19.07 34.03 34.13\n\n38.71 26.80 40.00 40.37\n\n40.82 28.06 40.97 41.47\n\n46.15 34.77 47.97 47.97\n\n21.65 12.74 22.40 22.10\n\n28.05 19.44 29.00 28.90\n\n29.57 19.67 31.30 30.63\n\n47.77 35.68 49.97 48.50\n\n56.00 34.86 55.90 58.30\n\n64.38 47.19 64.03 64.20\n\n65.99 47.20 65.33 67.60\n\n73.96 58.43 75.03 75.70\n\n44.10 33.70 46.12 47.06\n\n49.51 40.65 51.66 51.91\n\n50.64 40.85 52.57 53.19\n\n59.09 50.20 61.86 61.92\n\nfine-tuning is not as vulnerable as we think in the few shot setting. On the contrary, for 1 shot to 16 shots, classifier fine-tuning consistently outperforms prompt learning by 1.5% to 2.3%.\n\nAnother difference between prompt learning and classifier fine-tuning is about the optimization time. Prompt learning need to propagate the gradients through the text encoder back to the learnable prompt vectors, which is quit time-consuming. By contrast, the classifier fine-tuning can directly optimize the weights of the classifier, thus it is much more efficient. Here we report the optimization time of prompt learning and classifier fine-tuning with various network backbones for 16 shots classification on ImageNet. The results are shown in Table 3. We can observe that classifier finetuning is about 10× more efficient in speed than prompt learning.\n\nThese results have confirmed our assumption about prompt learning for closed-set classification. Specifically, prompt learning can not achieve the goal of sample efficient training as commonly expected. The simple baseline of classifier fine-tuning is much time efficient and achieves much higher accuracy than prompt learning across various datasets.\n\n5.3 ROBUSTNESS TO DISTRIBUTION SHIFTS\n\nPrevious works have shown that prompt learning has high domain generalization ability compared with handcrafted prompts. Thus we need to compare the robustness of classifier fine-tuning and prompt learning with respect to distribution shifts across domains. To this end and following Zhou\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\net al. (2022b), we use the ImageNet as the source domain and use the ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNetR (Hendrycks et al., 2021a) as the target domain. These datasets have the compatible class names with ImageNet, thus the optimized prompts of prompt learning and the learned weights of classifier fine-tuning can be transfered from ImageNet to these datasets.\n\nWe summarize the results in Table 4. As we can see, despite exposure to the source dataset, both prompt learning and classifier fine-tuning outperform the zero-shot and linear probe CLIP for the target datasets, which demonstrates their strong robustness to distribution shift. Moreover, classifier fine-tuning surpasses prompt learning on most models and datasets, verifying the generalization advantage of classifier fine-tuning over prompt learning.\n\n6 OPTIMALITY-GENERALIZATION TRADE-OFF\n\nIn this section, we examine the generalization ability of prompt learning method. It is usually believed that the learned prompts have strong robustness and generalization ability, as the optimization is conducted in the NLP embedding space, thus the learned prompts are expected to provide high generalization ability in the same way as natural language. However, as pointed out in Zhou et al. (2022a), the prompt learning method used in Zhou et al. (2022b) fails to learn task-specific context that generalizes well to unseen classes. To solve this problem, conditional prompt learning (Conditional Context Optimization, CoCoOp) is proposed in Zhou et al. (2022a), in which the prompts are the outputs of a meta network with the visual features of each image as the inputs. In this paper, we want to ask the question, why would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?\n\nBefore answering this question, we would like to take one step back and to think where does the generalization ability of a machine learning model comes from. Here we summarize three sources of the generalization ability of machine learning models.\n\n1. Knowledge. Human knowledge is general. As a high level abstraction of human knowledge, natural language also has strong generalization power. The pre-trained language model and prompt engineering are some examples.\n\n2. Inductive bias. During model design, experts can add some biases into the model based on the prior knowledge about the tasks to deal with. An example is the translation invariance of convolutional networks.\n\n3. Diverse training data. The most simple and widely used method to improve generalization\n\nis to use large scale diverse data. The CLIP model is an example.\n\nDespite that the learnable prompt vectors are optimized in the same word embedding space as NLP, the learned vectors are not natural language. Thus the prompt learning method can not generalize well to new categories that are not seen during training Zhou et al. (2022a). Thus conditional prompt learning is proposed which has shown high generalization ability than prompt learning. The question is where does the generalization power come from? Does it come from the introduced inductive bias or from the diverse training data? The first case (inductive bias) is hard to verify. Thus we will examine if the generalization power of conditional prompt learning comes from the optimization process.\n\nOur assumption is that, due to the changed architecture, the improved prompt learning method may be actually trying to find a better optimality-generalization trade-off. If the prompt vectors are learned on the source dataset, then they will have poor generalization power on the target dataset. As a trade-off, the better the prompt learning fit the source dataset (optimality), the weaker generalization power it would have.\n\nTo verify our assumption, we conduct experiments using the original prompt learning method (CoOp). To find a better trade-off between optimality and generalization, we use a very simple method. Specifically, we train the CoOp model for various training epochs and compare the performance with conditional prompt learning method (we use the CoCoOp method in this paper). We use the same setting as in Zhou et al. (2022a) for all the experiments. The results are summarized in Figure 2.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2 shows that by controlling the optimality-generalization trade-off, the original prompt learning method (CoOp) can achieve higher generalization ability than conditional prompt learning method (CoCoOp) for most of the case. It seems that the learned prompts (either learned with or without conditional) are not related with natural languages. They are just some parameters which make parameter-efficient fine-tuning possible. All we need is to find a better optimalitygeneralization trade-off. These results highlight the need for the rethinking of existing prompt learning, and more careful baseline evaluations metrics are needed in future research on prompt learning methods in vision-language models.\n\nFigure 2: The trade-off between optimality and generalization.\n\n7 CONCLUSION\n\nThis paper rethinks the existing prompt learning, making some surprising observations that contradict common beliefs about the prompt. First, we find that random prompts without fine-grained design or learning can also perform well in zero-shot recognition. Second, directly fine-tuning the linear classifier exhibits better performance than prompt learning. Moreover, we reveal that prompt learning is just a special case of parameter-efficient learning, and is a trade-off between optimality and generalization. Our results on 11 datasets highlight the rethinking in this paper can further boost the deployment of pre-trained vision-language models in downstream tasks.\n\n9\n\n70727476788082Base Accuracy (%)62646668707274New Accuracy (%)Average over 11 datasetsCoOpCoCoOpCLIP73747576Base Accuracy (%)666768697071New Accuracy (%)ImageNetCoOpCoCoOpCLIP96.897.097.297.497.697.898.098.2Base Accuracy (%)88899091929394New Accuracy (%)Caltech101CoOpCoCoOpCLIP9192939495Base Accuracy (%)9495969798New Accuracy (%)OxfordPetsCoOpCoCoOpCLIP6466687072747678Base Accuracy (%)606264666870727476New Accuracy (%)StanfordCarsCoOpCoCoOpCLIP7580859095Base Accuracy (%)626466687072747678New Accuracy (%)Flowers102CoOpCoCoOpCLIP88.589.089.590.090.5Base Accuracy (%)8486889092New Accuracy (%)Food101CoOpCoCoOpCLIP28303234363840Base Accuracy (%)24262830323436New Accuracy (%)FGVCAircraftCoOpCoCoOpCLIP707274767880Base Accuracy (%)6466687072747678New Accuracy (%)SUN397CoOpCoCoOpCLIP556065707580Base Accuracy (%)42.545.047.550.052.555.057.560.0New Accuracy (%)DTDCoOpCoCoOpCLIP5560657075808590Base Accuracy (%)5055606570New Accuracy (%)EuroSATCoOpCoCoOpCLIP7072747678808284Base Accuracy (%)5560657075New Accuracy (%)UCF101CoOpCoCoOpCLIP1Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with random forests. In European conference on computer vision, pp. 446–461. Springer, 2014.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. DeIn Proceedings of the IEEE conference on computer vision and\n\nscribing textures in the wild. pattern recognition, pp. 3606–3613, 2014.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nYu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for open-vocabulary object detection with vision-language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14084–14093, 2022.\n\nLi Fei-Fei. Learning generative visual models from few training examples.\n\nIn Workshop on\n\nGenerative-Model Based Vision, IEEE Proc. CVPR, 2004, 2004.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\n\nlearners. arXiv preprint arXiv:2012.15723, 2020.\n\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217–2226, 2019.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical In Proceedings of the IEEE/CVF International analysis of out-of-distribution generalization. Conference on Computer Vision, pp. 8340–8349, 2021a.\n\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15262–15271, 2021b.\n\nTony Huang, Jack Chu, and Fangyun Wei. Unsupervised prompt learning for vision-language mod-\n\nels. arXiv preprint arXiv:2204.03649, 2022.\n\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n\nSer-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.\n\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.\n\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models\n\nfor efficient video understanding. arXiv preprint arXiv:2112.04478, 2021.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pp. 554–561, 2013.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\n\npreprint arXiv:2101.00190, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021a.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021b.\n\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\n\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722–729. IEEE, 2008.\n\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012\n\nIEEE conference on computer vision and pattern recognition, pp. 3498–3505. IEEE, 2012.\n\nFabio Petroni, Tim Rockt ̈aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763, 2021.\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers In International Conference on Machine Learning, pp. 5389–5400.\n\ngeneralize to imagenet? PMLR, 2019.\n\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\n\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. Do nlp models know\n\nnumbers? probing numeracy in embeddings. arXiv preprint arXiv:1909.07940, 2019.\n\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019.\n\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 3485–3492. IEEE, 2010.\n\nYuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021.\n\nZexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [mask]: Learning vs. learning\n\nto recall. arXiv preprint arXiv:2104.05240, 2021.\n\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16816–16825, 2022a.\n\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n\nlanguage models. International Journal of Computer Vision, 130(9):2337–2348, 2022b.\n\n11",
    "reference": "# Summary Of The Paper\n\nIn this work, prompt learning is reexamined, and several unexpected findings that defy accepted notions of the prompt are presented. First , random prompts without learning or fine-grained design may likewise function effectively in zero-shot recognition. Second, direct linear classifier fine-tuning performs more effectively than prompt learning. Furthermore, prompt learning is essentially a subset of parameter-efficient learning and represents a trade-off between generalization and optimality.  Findings across 11 datasets show that the approach presented in this research can significantly influnce the use of trained vision-language models in subsequent challenges.\n\n# Strength And Weaknesses\n\nStrength\n1. The experiments are sufficient and exhaustive.\n2. The results provide the valuable hints that what's the better way to deploy the pretrained vision-language model.\n3. The paper inspire people to think about more effective prompt design.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nWeakness,\n\n1. The results are sufficent and the conclusion are well established but the reasons behind the result need to be more explored. For example, why classifier fintuning is much better than learning prompt?\n\n2. The novelty is limited but as a rethinking paper, it is fine.\n\n3. Can other models except clip still support findings?\n\n# Summary Of The Review\n\nOverall, this paper provides some valuable hints about prompts but the novelty is limited. Therefore, I give my initial rating as borderline\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCOLORISTANET FOR PHOTOREALISTIC VIDEO STYLE TRANSFER\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nPhotorealistic style transfer aims to transfer the artistic style of an image onto an input image or video while keeping photorealism. In this paper, we think it’s the summary statistics matching scheme in existing algorithms that leads to unrealistic stylization. To avoid employing the popular Gram loss, we propose a self-supervised style transfer framework, which contains a style removal part and a style restoration part. The style removal network removes the original image styles, and the style restoration network recovers image styles in a supervised manner. Meanwhile, to address the problems in current feature transformation methods, we propose docouple instance normalization to decompose feature transformation into style whitening and restylization. It works quite well in ColoristaNet and can transfer image styles efficiently while keeping photorealism. To ensure temporal coherency, we also incorporate optical flow methods and ConvLSTM to embed contextual information. Experiments demonstrates that ColoristaNet can achieve better stylization effects when compared with state-of-the-art algorithms.\n\n1\n\nINTRODUCTION\n\nNowadays rapid development of video-capture devices has made videos become a mainstream information carrier (Hansen, 2004). People usually post videos accompanied with different color styles on social media (Kopf et al., 2012; Xu et al., 2014) to share daily life, express different emotions, and get more exposures (Yan et al., 2016; Zabaleta & Bertalm ́ıo, 2021). Thus, photorealistic video style transfer or automatic color stylization becomes popular in many mobile devices. Different from artistic style transfer (Gatys et al., 2016; Huang & Belongie, 2017), photorealistic video style transfer or automatic color stylization needs to replace color styles in original videos with one or multiple reference images and keep the outputs maintain ”photorealism”. The photorealism in style transfer refers to that stylization results should look like real photos taken from cameras without any spatial distortions or unrealistic artifacts. Moreover, algorithms need to run in realtime.\n\nSeveral popular algorithms have been proposed to conduct photorealistic style transfer for single image. DeepPhoto (Luan et al., 2017) incorporated semantic segmentation masks to guide style transfer and utilized a photorealism regularization term to reduce spatial distortions. PhotoWCT (Li et al., 2018) exploited whitening and coloring transforms (WCT (Li et al., 2017c)) to conduct arbitrary style transfer and used photorealistic smoothing to remove spatially inconsistent stylization. WCT2 (Yoo et al., 2019) proposed a wavelet corrected transfer based on WCT to preserve structural information while stylizing images at the same time. PhotoNAS (An et al., 2020) proposed a neural architecture search framework for photorealistic style transfer and achieved impressive results.\n\nAlthough these algorithms can conduct style transfers in many scenarios, their stylization results still contain unpleasant artifacts or look unreal, and some algorithms need additional supports. In Figure 1 (a), given a content image which contains a tree in autumn and a style reference, previous state-ofthe-art algorithm WCT2 (Yoo et al., 2019) will generate synthesized images with obvious structural artifacts. Besides, these algorithms conduct style transfer by matching the summary statistics of content features with style references completely, which will lead to unrealistic stylization as in Figure 1 (b). For photorealistic style transfer in videos, there are only very few existing algorithms that can only perform style transfer with constraints. MVStylizer (Li et al., 2020) need good stylization initilaization at the first frame and Xia’s method (Xia et al., 2021) incorporates additional semantic masks for each frame in videos. These problems limit these methods’ usage in many real applications.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of unsolved problems in photorealistic style transfer. From left to right: (a) Previous state-of-the-art algorithm WCT2 (Yoo et al., 2019) generates stylization results with obvious structural artifacts. (b) The stylization result produced by WCT2 (Yoo et al., 2019) looks painterly and slightly unreal. (c) Video stylization algorithms need additional inputs, such as good stylization initialization (Li et al., 2020) or semantic masks (Xia et al., 2021), to guide style transfer.\n\nIn this paper, we aim to solve the problems listed above in photorealistic video style transfer. Different from previous algorithms which match summary statistics of content images to that of style references through whitening and coloring transformation (Li et al., 2018), adaptive instance normalization (An et al., 2020) and the Gram loss (Luan et al., 2017), we propose a style removal and restoration framework in a self-supervised manner to conduct arbitrary style transfer while keeping photorealism. Our motivation is that during photorealistic style transfer, if we can remove the style of image content without destroying image structures, we can recover its original style by using the content image both as style reference and stylization target. According to our experiences, artifacts produced by PhotoWCT (Li et al., 2018), WCT2 (Yoo et al., 2019), and PhotoNAS (An et al., 2020) come from two parts: (1) the Gram loss; (2) whitening and coloring transformation (WCT (Li et al., 2017c)). In our method, we avoid using the Gram loss and train networks with the content loss only (Gatys et al., 2016). We improve the summary statistics matching scheme with decoupled instance normalization which can remove original image styles and add new styles for inputs without hurting image structures. Meanwhile, decoupled instance normalization does not match styles of reference images completely and avoid unrealistic stylization in Figure 1 (b). To keep temporal consistency in videos, we exploit optical flow estimation (Teed & Deng, 2020) and ConvLSTM (Shi et al., 2015a) to conduct consecutively style transfer. We summarize our contributions as follows:\n\n• In this paper, we propose a novel photorealistic video style transfer network called ColoristaNet, which can conduct color style transfer in videos without introducing painterly spatial distortions and inconsistent flickering artifacts. We put many videos in the supplementary material to compare with other state-of-the-art algorithms.\n\n• We propose decoupled instance normalization which works together with ConvLSTM (Shi et al., 2015a) to implement structure-preserving and temporally consistent feature transformation. The decoupled instance normalization decomposes style transfer into feature whitening and stylization, which can avoid unrealistic style transfer.\n\n• ColoristaNet can adapt color styles in videos consecutively with multiple different style references and runs faster than most of recent algorithms. Qualitative results and a user study show that our method outperforms other state-of-art algorithms in making a balance between good stylization results and photorealism. Besides, we also conduct extensive ablation studies whose results demonstrate the effectiveness of different modules and designs in ColoristaNet clearly.\n\n2\n\n(a.1)Content Image(a.2)StyleImage(a.3)WCT2(a.4)Ours(b.1)Content Image(b.2)StyleImage(b.3)WCT2(b.4)Ours(a)StructuralArtifacts(b)Unrealistic Stylization(c.1)Video Stylization ExampleContentStyleResult(c.2)Video Stylization FrameworkEncoderDecoderConstraints, e.g. good stylization initialization, semantic masks and etc.(c)Style Transferwith ConstraintsUnder review as a conference paper at ICLR 2023\n\n2 PRELIMINARIES AND MOTIVATIONS\n\nFigure 2: The training and test pipeline of ColoristaNet. During training, a ColoristaNet is firstly exploited to replace the color style of a content image with a style reference. Then, another ColoristaNet restores the style of the synthesized image by using the original content input as a style reference. During testing, the style restoration ColoristaNet can conduct style transfer efficiently.\n\nNeural style transfer algorithms (Gatys et al., 2016; Li et al., 2017b) have achieved great success in creating artistic images of high perceptual quality. Using neural representations to separate and recombine content and style of arbitrary images is widely investigated and adopted by researchers (Li et al., 2017a; Zhu et al., 2017b; Johnson et al., 2016; Ledig et al., 2017). In Gatys’ paper, the Gram matrix consists of the correlations between different filter responses and describe the overall image style, and features in deeper layers is thought capturing the high-level content in term of objects and their arrangement. Then style transfer problems can be solved by matching summary statistics of content inputs to that of style references. However, although such a framework works quite well for artistic style transfer, it is not suitable for photorealistic style transfer. Because matching the summary statistics of content images with arbitrary style references will generate unpleasant artifacts or distortions. Photorealistic style transfer algorithms, such as DeepPhoto (Luan et al., 2017), PhotoWCT (Li et al., 2018), WCT2 (Yoo et al., 2019), and PhotoNAS (An et al., 2020), focus on eliminating artifacts or distortions with additional smoothing term or other regularization terms. As shown in Figure 1, structural artifacts and unrealistic stylization is hard to be avoided.\n\nIn this paper, we hold an assumption that in previous methods, it’s the summary statistics matching scheme in learning objectives and feature transformation modules that lead to structural artifacts or unrealistic stylization. That means the Gram loss (Gatys et al., 2016), AdaIN (Huang & Belongie, 2017)] and WCT (Li et al., 2018) are problematic in photorealistic style transfer. To address these issues, we propose ColoristaNet with: (1) a self-supervised style transfer framework that avoids employing the Gram loss during training; and (2) a novel feature transformation module to substitute AdaIN or WCT to perform summary statistics matching. For the self-supervised style transfer framework, as shown in Figure 2, if we can remove the style of an image without hurting its structure, the style restoration problem becomes a fully supervised one. The reason why our idea works is because in the photorealistic setting, image structures are shared and unchanged during style transfer. The benefits of our self-supervised learning scheme come from two folds: (1) We avoid employing the Gram loss or other regularization loss functions that will result structural artifacts or blur effects; (2) Our learning targets are real photos which can ensure that the stylization results make a good balance between stylization and photorealism. We discuss more details in Appendix B.1.\n\nTo address the problems brought by AdaIN and WCT, ColoristaNet incorporates a novel feature transformation module called decoupled instance normalization (DecoupleIN) to match the styles of content images with that of reference images. DecoupledIN is inspired by AdaIN, and decompose the feature transformation into a style whitening step and a restylization step. This decomposition avoids forcing the feature statistics of content images to match that of style images directly, and show impressive results. In addition, as we conduct video style transfer, we need to keep temporal coherency in consecutive frames. So we employ optical flow methods to estimate pixel locations in a next frame and propagate style information through ConvLSTM (Shi et al., 2015a) as shown in Figure 3. We give more explanations about the design of ColoristaNet in Appendix B.2.\n\n3\n\nStyleImageStyle RemovalResult(a)Training Pipeline of ColoristaNetColoristaNet1Style RestorationResultColoristaNet2(b)Test Pipeline of ColoristaNetContentImageStyleImageContent ImageColoristaNet2StyleImageStylizationResultUnder review as a conference paper at ICLR 2023\n\n3 METHOD\n\nFigure 3: Illustration of the training pipeline of ColoristaNet. There are five content frames and five style references for a video clip. For each frame, it firstly passes through a style transfer network to conduct style removal and then go through another style transfer network for style restoration. In style restoration, features from different time steps are connected with a ConvLSTM unit. A flow estimation network (RAFT (Teed & Deng, 2020) with fixed parameters) predicts optical flow between two adjacent frames to warp the hidden states of ConvLSTM for movement compensation. Note that parameters of style removal and restoration networks at different time steps are shared.\n\n3.1 OVERVIEW OF THE PROPOSED METHOD\n\nIn this section, we introduce the training pipeline of ColoristaNet. For the test pipeline, we give more details in Appendix A.2. As discussed in the previous sections, we exploits a style removal ColoristaNet and a style restoration ColoristaNet to conduct end-to-end training (as shown in Figure 3). Given a video clip with five image frames, we send them together with randomly selected style references to a style transfer network to conduct style removal. Then these style removal results are used as content inputs and the original inputs are used as style images to perform style restoration with another style transfer network. Both style transfer networks are in a similar structure except that the style restoration incorporates RAFT (Teed & Deng, 2020) and ConvLSTM (Shi et al., 2015a) to keep temporal coherency. For the learning objective, they are trained with two content loss respectively, and their learning targets are original video frames. During the test, the second style transfer network is exploited to conduct style transfer without additional constraints.\n\n3.2 STYLE TRANSFER NETWORK\n\nFigure 3 shows the architecture of two style transfer networks roughly. A style transfer network consists of a VGG-19 encoder Simonyan & Zisserman (2014), four decoupled instance normalization modules, four ConvLSTM units Shi et al. (2015b), and a decoder to generate final output. Note that, in style removal, ConvLSTM units are removed, since it doesn’t need context information. Given an input image pair (ICt, ISt), feature maps at ”conv1 1”, ”conv2 1”, ”conv3 1” and ”conv4 1” of a VGG-19 network Φvgg with frozen parameters are extracted: These multiscale features pass through four decoupled instance normalization modules (shown in Figure 4) to map feature statistics of the content image to match that of its style reference. In style restoration, RAFT is exploited to estimate pixel locations in adjacent frames and ConvLSTM is responsible to incorporate contextual information.\n\n4\n\nDecoder1Decoder2ConvLSTMDecoder1Decoder2ConvLSTMDecoder1Decoder2ConvLSTMFlow Est.Decoder1Decoder2ConvLSTMFlow Est.Decoder1Decoder 2ConvLSTMStyle Removal Style RestorationwarppingContent Loss 2Content Loss1 Content Loss 2Content Loss 2Content Loss 2Content Loss 2Content Loss 1Content Loss 1Content Loss 1VggNetContent Loss 1VggVggVggNetVggNetTime Step 1DecoupledIN 1DecoupledIN 2VggNetVggNetVggNetDecoupledIN 1DecoupledIN 1DecoupledIN 1DecoupledIN 1DecoupledIN 2DecoupledIN 2DecoupledIN 2DecoupledIN 2warppingwarppingwarppingFlow Est.VggNetVggNetVggNetVggNetTime Step 2Time Step 3Time Step 4Time Step 5 Flow Est.Under review as a conference paper at ICLR 2023\n\nThen, a U-net Ronneberger et al. (2015) style decoder fuses information across different scales and generate stylization results. We give a very detailed structure configurations in Appendix A.2.\n\n3.3 DECOUPLED INSTANCE NORMALIZATION\n\nMatching feature statistics through feature transformation has been proven powerful in both artistic style transfer and photorealistic style transfer (AdaIN (Huang & Belongie, 2017) and WCT (Li et al., 2017c)). But directly applying AdaIN will hurt some subtle image details, and WCT often leads to unrealistic stylization. Here we propose decoupled instance normalization (DecoupledIN) that decomposes the feature transformation into feature whitening and stylization. Figure 4 shows the DecoupledIN module. Given a content input fCt,i and a style input fSt,i at i-th layer, we remove the style of fCt,i as Equation 1 firstly, and then send the whitened result (cid:101)fCt,i and style input fSt,i into a 3 × 3 convolutional layer with 2c filters to conduct AdaIN (c is the number of input feature channel). Finally, we reduce the feature channels of stylized features gt,i to be the same with inputs. The overall process of DecoupledIN can be described with the following equations:\n\nFigure 4: Conceptual illustration of decoupled instance normalization and visual comparison with AdaIN (Huang & Belongie, 2017).\n\nWhitening : f ′\n\nCt,i =\n\nfCt,i − μ (fCt,i) σ (fCt,i)\n\n,\n\nTransform : f ′′\n\nCt,i, f ′′\n\nSt,i = Conv (cid:0)f ′\n\nCt,i\n\nStylization : gt,i = σ (cid:0)f ′′\n\nSt,i\n\n(cid:1)\n\n\n\n(cid:1) , Conv (fSt,i) , (cid:1)\n\n\n\nCt,i − μ (cid:0)f ′′ f ′′ f ′′\n\n(cid:16)\n\nσ\n\nCt,i (cid:17)\n\nCt,i\n\n  + μ (cid:0)f ′′\n\nSt,i\n\n(1)\n\n(cid:1) ,\n\nwhere μ and σ calculate the mean and standard deviation for each feature channel respectively. Figure 4 indicates that style transfer through DecoupledIN will generate better stylization results without hurting image details when compared with using AdaIN. We attribute this to that directly changing the content feature statistics will make some neurons work out of their working range and remove detailed structures. When we separate feature whitening from stylization, we conduct feature transformation more smoothly and get better results. We disucss DecoupledIN and conduct more ablations in Appendix B.2 to investigate our assumptions. Experiments indicates that more whitening will lead to better stylization effects.\n\n3.4 LOSS\n\nUnlike previous works which employed multiple loss functions including content loss, temporal loss, Gram loss, and etc, we simply employ content loss to constrain the structure of the stylization results to be the same as content inputs (for style removal) and guide ColoristaNet to generate photorealistic videos like real world videos (for style restoration). Given a stylization result IG and an input image IC, we use the feature maps at conv4 1 layer of VGG-19 to calculate the content loss. The two ColoristaNets are trained end-to-end without sharing parameters. For a video clip with N frames, the learning objective becomes:\n\nL =\n\nN (cid:88)\n\ni=1\n\nLcontent\n\n(cid:0)IG1,i, ICi\n\n(cid:1) + λLcontent\n\n(cid:0)IG2,i, ICi\n\n(cid:1) ,\n\n(2)\n\nwhere IG1,i and IG2,i are generated images of style removal and restoration networks respectively. In experiments, we set λ = 1 and get impressive results.\n\n5\n\n(a) Decoupled InstanceNormalization(b) Visual Comparison of AdaIN andDecoupledINContent and StyleDecoupleINAdaINUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL DETAILS\n\nWe conduct extensive experiments to indicates the effectiveness of the proposed method. Due to the page length limit, we put implementation details into the appendix, including datasets, evaluation protocols and training and test settings (in Appendix A). We also put more results and discussions in the appendix part to compare with state-of-the-art methods and investigate the effectiveness of different designs and modules in ColoristaNet.\n\n4.2 QUALITATIVE COMPARISON\n\nFigure 5: Multiple color stylization on a set of consecutive frames with ColoristaNet. Stylization targets are visualized in the right up corner of first, fourth, and seventh columns. From left to right, color styles of input frames changed smoothly without any painterly distortions or flickering artifacts. More stylization videos are packed in the supplementary material.\n\nPhotorealism. In photorealistic style transfer, the most important principle is to change color styles of images without resulting distortions or artifacts. Meanwhile, photorealism means that stylization results should looks like taken from cameras. Figure6 compares ColoristaNet with state-of-the-art methods in terms of photorealism. When zooming in to check details of local image patches, no obvious unpleasant artifacts in results produced by ColoristaNet.\n\nCoherency. For video stylization, maintaining temporal coherency is vital in many real applications. In figure 5, from left to right are stylization results at different time steps. There are three different style images in large style variations for each video. A Gaussian smoothing function is exploited to smooth the stylization vectors among frames with different style references. Stylized video frames change smoothly without any flickers. Nor there is any structural inconsistency between stylized video frames. This proves that ColoristaNet is able to produce temporally coherent videos even when there are multiple style references. Please refer to our supplementary material for videos generated by ColoristaNet (with single style reference and multiple style reference).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Visual comparison with popular algorithms including WCT2 (Yoo et al., 2019), PhotoNAS (An et al., 2020) and our ColoristaNet. Images in the first column and their top right corners are content images and their style counterparts. Each row contains stylization results rendered by different styles. While ColoristaNet generates photorealistic results, other methods either damage image structure or produce over-stylization.\n\n4.3 QUANTITATIVE COMPARISON\n\nQuantitative Metrics. Following PhotoNAS (An et al., 2020), we evaluate the stylization results with SSIM, LPIPS (Zhang et al., 2018), content loss (Gatys et al., 2016) and Gram loss (Gatys et al., 2016). SSIM, LPIPS (Zhang et al., 2018), content loss (Gatys et al., 2016) are employed to measure the strcture similarity between two images, and gram loss is used to calculate the style distance of two images. We randomly select 100 video/style image pairs to evaluate the performance of state-of-art\n\n7\n\n(a) Content and Style(b) WCT2(c) PhotoNAS(d) OursUnder review as a conference paper at ICLR 2023\n\nalgorithms. We use official codes and models provided by PhotoWCT (Li et al., 2018), WCT2 (Yoo et al., 2019) and PhotoNAS (An et al., 2020) to generate stylization results. In Table 1, the proposed ColoristaNet achieves better scores in SSIM, LPIPS, the content loss, and a comparable gram loss. It means our ColoristaNet has a stronger ability in preserving structural details.\n\nTable 1: Quantitative comparison with state-of-art photorealistic style transfer algorithms.Higher SSIM scores mean test images are more similar to input contents with fine details. Lower LPIPS scores mean higher perceptual similarities between stylization results and content images.\n\nMethod\n\nPhotoWCT(full) WCT2\n\nPhotoNAS\n\nOurs\n\nSSIM↑ LPIPS↓ Content Loss↓ Gram Loss↓\n\n0.548 0.464 11.035 0.00025\n\n0.555 0.391 7.256 0.00032\n\n0.737 0.326 4.351 0.00028\n\n0.785 0.223 2.427 0.00026\n\nTable 2: User study results.\n\nTable 3: Computing-time comparison.\n\nMethod\n\nPhotoWCT(full) WCT2\n\nPhotoNAS Ours\n\nImage Size\n\nPhotoWCT(full) WCT2\n\nPhotoNAS\n\nOurs\n\nPhotorealism Stylization Coherency Overall quality\n\n2.20 2.03 2.00 2.23\n\n2.77 2.70 3.37 3.03\n\n2.67 2.17 3.27 2.77\n\n3.57 3.40 3.70 3.57\n\n600×360 854×480 1280×720 1920×1080\n\n0.408s 0.495s 0.811s 1.430s\n\n2.13s 2.15s 4.267s 4.362s\n\n0.124s 0.175s 0.383s 0.564s\n\n0.068s 0.111s 0.225s 0.482s\n\nUser Study. We recruited 30 testers who are not connected with this project to evaluate the quality of the stylization results. The testers are asked to take the quality of details in the image and the stylization effects into consideration during their evaluation. Images are rated on a scale of 1-5, where higher scores stand for better stylization results. In total, we collected 900 responses (30 videos × 30 users) for each kind of method. As shown in Table 2, our approach perform best for photorealism, stylization effects, temporal coherency and overall quality.\n\nInference Speed. To demonstrate the efficiency of our method, we compare the inference speed of the different models. We use GeForce RTX 3090 GPU to test all state-of-the-art methods. We randomly select 5 different videos, each of which contains 80 frames, and compute the average running time of each method. Furthermore, we also test the speed of these algorithms in different resolutions. As shown in Table 3, our ColoristaNet is much faster than other methods.\n\nFigure 7: Investigation of the effectiveness of RAFT and ConvLSTM in ColoristaNet.\n\n4.4 ABLATION STUDY\n\nWhether optical flow estimation and ConvLSTM are necessary? To check whether RAFT together with ConvLSTM are necessary for video style transfer, we remove them to test the performance of ColoristaNet. From Figure7, we can find that when we remove RAFT optical flow estimation,\n\n8\n\nTime step 1 Time Step 2 Time Step 3Time Step 1 Time Step 2 Time step 3(a)without RAFT(b)without ConvLSTMColoristaNetContent & Stylewithout RAFT or ConvLSTMUnder review as a conference paper at ICLR 2023\n\nColoristaNet will generate noticeable artifacts. When we remove all ConvLSTM units, some images details disappeared. It shows both RAFT and ConvLSTM are necessary.\n\nFigure 8: Visual comparison of the results produced by AdaIN and DecoupledIN.\n\nWhether DecoupledIN is important to get good results? To verify decoupledIN’s ability in preserving subtle image details, we replace all decoupledIN modules in ColoristaNet with AdaIN. As shown in Figure 8, AdaIN is powerful in generating synthesised images with good stylization effects and keeping photorealism. However, if we zoom in to see more details, we find that AdaIN will overwrite some image details and produce images that seem to be ”overexposed”.\n\nFigure 9: Results of ablation on the multi-scale feature fusion scheme of ColoristaNet. (d) is complete multi-scale feature fusion scheme, (c) removes the feature transformation at the ”conv4 1” stage, (b) further removes the feature transformation at the ”conv3 1” stage on the basis of (c).\n\nWhether multi-scale features are useful in the decoder? We remove feature maps produced by ”conv4 1” and ”conv3 1” stages and compare their results with ColoristaNet in Figure 9. From left to right, we list the stylization results produced by ColoristaNet with two, three, and four different feature scales. These results demonstrate that multi-scale features can generate better stylization results. More results in Appendix B.4 indicates more feature scales will lead to less artifacts.\n\n5 CONCLUSIONS, LIMITATIONS AND FUTURE WORKS\n\nIn this paper, we propose ColoristaNet, a photorealistic video style transfer network, along with a removal-and-restoration training pipeline. ColoristaNet learns color stylization in a self-supervised manner and generates stylization results looking as if taken from cameras. Two important components of ColoristaNet are decoupled instance normalization and ConvLSTM units that can implement arbitrary style transfer while preserving salient image structure and temporal stylization coherency. Experiments show that results of ColoristaNet have strong visual quality and high artistic value.\n\nLimitation. However, since ColoristaNet makes a balance between stylization results and photorealism, it doesn’t completely embed styles of reference images into input videos like that of other methods. According to our experiments, in many scenarios, enforcing stylization results to have the exactly same style of references will make images looks like paintings not real photos. Besides, we exploit RAFT to compute optical flow and many other modules with heavy computational cost to conduct style transfer. This makes ColoristaNet unable to run in realtime on a single NVIDIA GeForce RTX 3090 GPU, which deserves thorough analysis in the future. In the future, we will try to work on this problem to alleviate these limitations. Our techniques can be used in many social media platforms and we haven’t found obvious negative societal impact of ColoristaNet.\n\n9\n\n(a) Content and Style(b) AdaIN(c) DecoupledIN(f) DecoupledIN(d) Content and Style(e) AdaIN(a) Content & Style(b) ColoristaNet without \"conv3_1\" and \"conv4_1\" branches(c) ColoristaNet without the \"conv4_1\" branch(d) ColoristaNetUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJie An, Haoyi Xiong, Jun Huan, and Jiebo Luo. Ultrafast photorealistic style transfer via neural architecture search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 10443–10450, 2020.\n\nAlexander G. Anderson, Cory P. Berg, Daniel P. Mossing, and Bruno A. Olshausen. Deepmovie: Using optical flow and deep neural networks to stylize movies. arXiv: Computer Vision and Pattern Recognition, 2016.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\n\nDongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang Hua. Coherent online video style transfer. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1105–1114, 2017.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.\n\nXinghao Chen, Yiman Zhang, Yunhe Wang, Han Shu, Chunjing Xu, and Chang Xu. Optical flow distillation: Towards efficient and stable video style transfer. In European Conference on Computer Vision, pp. 614–630. Springer, 2020b.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAlexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pp. 2758–2766, 2015.\n\nWei Gao, Yijun Li, Yihang Yin, and Ming-Hsuan Yang. Fast video multi-style transfer. workshop on\n\napplications of computer vision, 2020.\n\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414–2423, 2016.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\n\nMark Boris Nicola Hansen. New philosophy for new media. MIT press, 2004.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pp. 1501–1510, 2017.\n\nEddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. arXiv: Computer Vision and Pattern Recognition, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\n\nsuper-resolution. In European conference on computer vision, pp. 694–711. Springer, 2016.\n\nStephan Kopf, Stefan Wilk, and Wolfgang Effelsberg. Bringing videos to social media. In 2012 IEEE\n\nInternational Conference on Multimedia and Expo, pp. 681–686. IEEE, 2012.\n\nHildegard Kuehne, Hueihan Jhuang, Est ́ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pp. 2556–2563. IEEE, 2011.\n\nChristian Ledig, Lucas Theis, Ferenc Husz ́ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4681–4690, 2017.\n\nAng Li, Chunpeng Wu, Yiran Chen, and Bin Ni. Mvstylizer: an efficient edge-assisted video photorealistic style transfer system for mobile phones. In Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pp. 31–40, 2020.\n\nShaohua Li, Xinxing Xu, Liqiang Nie, and Tat-Seng Chua. Laplacian-steered neural style transfer. In Proceedings of the 25th ACM international conference on Multimedia, pp. 1716–1724, 2017a.\n\nYanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Demystifying neural style transfer. arXiv\n\npreprint arXiv:1701.01036, 2017b.\n\nYijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. Advances in neural information processing systems, 30, 2017c.\n\nYijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and Jan Kautz. A closed-form solution to photorealistic image stylization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 453–468, 2018.\n\nFujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. In Proceedings\n\nof the IEEE conference on computer vision and pattern recognition, pp. 4990–4998, 2017.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nXingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai kin Wong, and Wang chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. arXiv: Computer Vision and Pattern Recognition, 2015a.\n\nXingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. Advances in neural information processing systems, 28, 2015b.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European\n\nconference on computer vision, pp. 402–419. Springer, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVideoNet. Videvonet. https://www.videvo.net/.\n\nXide Xia, Tianfan Xue, Wei-sheng Lai, Zheng Sun, Abby Chang, Brian Kulis, and Jiawen Chen. Real-time localized photorealistic video style transfer. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1089–1098, 2021.\n\nJie Xu, Mihaela Van Der Schaar, Jiangchuan Liu, and Haitao Li. Forecasting popularity of videos using social media. IEEE Journal of Selected Topics in Signal Processing, 9(2):330–343, 2014.\n\nNing Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: A large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018.\n\nZhicheng Yan, Hao Zhang, Baoyuan Wang, Sylvain Paris, and Yizhou Yu. Automatic photo adjustment using deep neural networks. ACM Transactions on Graphics (TOG), 35(2):1–15, 2016.\n\nJaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha. Photorealistic style transfer via wavelet transforms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9036–9045, 2019.\n\nYouTube. Youtube. https://www.youtube.com/.\n\nItziar Zabaleta and Marcelo Bertalm ́ıo. Photorealistic style transfer for video. Signal Processing:\n\nImage Communication, 95:116240, 2021.\n\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\n\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\n\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2223–2232, 2017a.\n\nJun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. Advances in neural information processing systems, 30, 2017b.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAppendix\n\nA IMPLEMENTATION\n\nA.1 SETTINGS\n\nDataset. ColoristaNet is trained by videos collected from HMDB (Kuehne et al., 2011) and Youtube VOS (Xu et al., 2018) as content inputs and images collected from Internet as style images. We have around 6000 videos and around 24000 images in our training set. During test, we download videos from Youtube (YouTube), Videvo (VideoNet) and other websites, and select style images that can generate pleasant stylization results.\n\nEvaluation. To check the color stylization ability of ColoristaNet, we conduct style transfer on various high-definition videos with different style images as shown in Figure 5. We compare with photorealistic image style transfer algorithms, such as WCT2 (Yoo et al., 2019), PhotoNAS (An et al., 2020). We directly conduct style transfer frame by frame using their official codes. We can’t compare with photorealistic video style transfer algorithms MVStylizer (Li et al., 2020) and Xia et al. (Xia et al., 2021) and other methods, since their source codes are not released. We conduct both quantitative comparison and a user study to evaluate different algorithms.\n\nTraining. All experiments are implemented with PyTorch (Paszke et al., 2019), and run on two NVIDIA GeForce RTX 3090 GPUs with 24 GB RAM. Parameters of VGG-19 (Simonyan & Zisserman, 2014) and RAFT (Teed & Deng, 2020) are initialized with pretrained weights and are fixed during training. We use the SGD optimizer with momentum 0.9 and basic learning rate 1e-5 to optimize parameters of ColoristaNet in 80 epochs. We schedule our learning rate a bit different from common practise. The learning rate at the first epoch is set to 0.01, and then is decreased to 1e-5 in the next five epochs. After that we apply cosine decay for the rest epochs. Images are cropped and resized into the resolution 128 × 128 during training.\n\nFigure 10: Inference with style transfer network. For a video clip, each frame is paired with a style reference to pass VGG-19 feature extractor, decoupled instance normalization, ConvLSTM and the decoder to generate the final output. There may be multiple style references at the same time. ColoristaNet can conduct arbitrary style on videos of any length according to users’ preference.\n\nA.2 CONFIGURATIONS OF STYLE TRANSFER NETWORK\n\n13\n\nDecoder 2ConvLSTMFlow Est.Decoder 2ConvLSTMFlow Est.Decoder 2ConvLSTMDecoder 2ConvLSTMFlow Est.Decoder 2ConvLSTMStyle TransferwarppingVggNetVggNetVggNetTime Step 1Time Step 2Time Step 3Time Step N-1Time Step NDecoupledIN 2VggNetVggNetDecoupledIN 2DecoupledIN 2DecoupledIN 2DecoupledIN 2warppingwarppingFrame Style ......Under review as a conference paper at ICLR 2023\n\nFigure 11: Detailed illustration of style transfer network. There are five shared blocks in a style transfer network: (b) Upsample block, (c) Downsample Block, (d) Convolution Block, (e) Concatenation Block, and (f) Output Block. Structures of style transfer networks in style removal and restoration are slightly different. In style removal network, there are no ConvLSTM (Shi et al., 2015b) units.\n\nFigure 11 gives detailed architectures of ColoristaNet. The style transfer network has a U-net (Ronneberger et al., 2015) style encoder-decoder structure. During training, networks are shared by different time steps. The number of convolution filters in each block is denoted with c. ”ConvBlock, 64” stands for the filter number in a convolution block is 64 and the kernel size of convolutional layers is 3 × 3. Other blocks have the similar notations. The style removal network and style restoration network have a similar architecture, except that there are no ConvLSTM (Shi et al., 2015b) units across style removal networks.\n\nB COLORISTANET: DETAILS, ADDITIONS AND ABLATIONS\n\nTo generate photorealistic stylization results which look like taken from cameras, ColoristaNet exploits a set of training strategies and micro designs to avoid structural distortions and painterly artifacts, including self-supervised learning, decoupled instance normalization, flow estimation network (RAFT) (Teed & Deng, 2020), ConvLSTM (Shi et al., 2015a), multi-scale feature learning and etc. Here, we give more details, additional experiments and ablations on the designs and choices of these different modules.\n\nB.1 SELF-SUPERVISED LEARNING IN COLORISTANET\n\nSelf-supervised learning obtains supervisory signals from unlabeled data itself and thus leverages underlying structure and common representation in data, which has achieved great success in natural language processing (Devlin et al., 2018; Brown et al., 2020) and computer vision (Chen et al., 2020a; Caron et al., 2021; He et al., 2022). In unpaired image-to-image translation, CycleGAN (Zhu et al., 2017a) introduces consecutive image-to-image translations in cycles to ensure content consistencies in images with the help of generative adversarial networks (Goodfellow et al., 2014). In this paper, we exploit the self-supervised learning strategy for photorealistic style transfer. Our motivation here is that if the style of an image can be replaced without hurting subtle structures arbitrarily, its style can be recovered by using itself as the style reference. Such an assumption holds in photorealistic style transfer because the underlying image structure remains unchanged during color style transfer. So in our training pipeline, we apply two ColoristaNets that are responsible for style removal and style restoration respectively. When styles of images are removed, the style transfer or restoration task becomes a fully supervised one. As shown in Figure 12, such a strategy has been proven to be the key of the success of ColorsitaNet.\n\n14\n\nVgg conv1_1Vgg conv2_1Vgg conv3_1Vgg conv4_1DecoupledINDecoupledINDecoupledINDecoupledINConvBlock, 64ConvBlock, 128ConvBlock, 256ConvBlock, 512ConvLSTMConvLSTMConvLSTMConvLSTM3x3 Conv, cGeLU3x3 Conv, cBatch NormGeLUInterpolation x23x3 Conv, cGeLU3x3 Conv stride 2, c/2Batch NormGeLU(b) UpsampleBlock(c) DownsampleBlock3x3 Conv, cGeLU3x3 Conv, cBatch NormGeLU(d) ConvolutionBlock3x3 Conv, cBatch NormGeLU(e) ConcatenationBlockDownsample, 64AddDownsample, 128AddDownsample, 256AddConvBlock, 512concatenationUpsample, 512ConcatBlock, 256Upsample, 256ConvBlock, 256ConvBlock, 256ConcatBlock, 128Upsample, 128ConvBlock, 128ConvBlock, 128ConcatBlock, 64ConvBlock, 64ConvBlock, 64Output3x3 Conv, cGeLU3x3 Conv, 32GeLU(f) OutputBlock3x3 Conv, 3(a) Detailed Architecture of Style Transfer NetworkUnder review as a conference paper at ICLR 2023\n\nFigure 12: Stylization results comparison of different style transfer networks: (a) Content and style images, (b) A single ColoristaNet simply trained with content loss (without the removal-restoration pipeline), (c) The style removal ColoristaNet in the proposed training pipeline, (d) The style removal ColoristaNet in the proposed training pipeline.\n\nStyle Removal. During style removal, style transfer network overwrites styles of images with given style references through decoupled instance normalization at different feature resolutions. Conducting style transfer without resulting any distortions or artifacts is the most important principle in style removal. We implement style removal with decoupled instance normalization and give detailed introduction and analysis in Appendix B.2. Meanwhile, to ensure image structures to be full preserved, we simply exploit the content loss (Gatys et al., 2016) to enforce the structure consistency. As is known that style loss (Gatys et al., 2016) can conduct mixed transfers of both texture and color, it will result in painterly distortions of image structures. Figure 12 visualizes stylization results of the style removal network. It can be found that image structures of content images are preserved and original image styles are partly removed. Its stylization effects are unsatisfactory and look just like paintings.\n\nStyle Restoration. Following the assumption described above, the style restoration network just takes the style removal result in as the content input and use the original image as the style reference to conduct style transfer. Thus, the stylization results can be expected to be the same with the original content image. In this way, the content image, the style reference and the stylization result can be defined clearly. We use the style transfer network in the same architecture with that in the style removal part to conduct supervised style transfer. Again, no style loss is exploited to enforce good stylization. Surprisingly, we can find that such a strategy can help to generate good stylization results without obvious distortions or artifacts. We attribute this to that the linear feature transformations in DecoupledIN and the self-supervised learning framework without style loss. Besides, we find that training the style removal and restoration networks jointly will make the training much more stable and converge faster. To validate the effectiveness of the two stage style transfer framework, we conduct an ablation study to check whether the self-supervised learning strategy is necessary. We train ColoristaNet with a randomly selected style reference to perform parameter learning without self-supervision. Figure 12 shows the stylization results produced by ColoristaNet without selfsupervision, the style removal and restoration ColoristaNets respectively. It can be found that without the self-supervised learning strategy, ColoristaNet can not transfer the style of a reference image to the target successfully.\n\n15\n\n(a) Content and Style(b) Without self-supervision(c) Style removal net(d) Style restoration netUnder review as a conference paper at ICLR 2023\n\nFigure 13: Visual comparison of the results produced by AdaIN and DecoupledIN.\n\nB.2 DECOUPLE INSTANCE NORMALIZATION\n\nTransferring styles of images arbitrarily through feature transformations has been widely accepted in style transfer (Huang & Belongie, 2017; Li et al., 2017c; 2018; Yoo et al., 2019). In (Huang & Belongie, 2017), Huang et al. proposed adaptive instance normalization (AdaIN) to aligns the mean and variance of the content features with those of the style features to achieve arbitrarily artistic style transfer without iterative optimization process. Li et al. (Li et al., 2017c) conducted universal style transfer through the whitening and coloring transforms (WCT) to match feature covariance of the content image to a given style image. PhotoWCT (Li et al., 2018) brought the idea of conducting style transfer through feature transformations from WCT (Li et al., 2017c) to perform photorealistic style transfer. However, the feature transformation in WCT is nonlinear, and thus lead to distortions in image structures obviously. PhotoWCT (Li et al., 2018) exploits a photorealistic smoothing term to ensure local consistency in pixel intensities. But in many cases, there are obvious artifacts produced by WCT that can not be removed by the smoothing term. Besides, the smoothing term also make images blurry and lost some subtle local contrast. Yoo et al. (Yoo et al., 2019) proposed a wavelet corrected transfer based on whitening and coloring transforms (WCT2) to avoid introducing additional masks and unfavorable blurry artifacts. Although these methods are powerful and can conduct arbitrary style transfer, they can’t achieve photorealistic stylization results. According to our analysis, the feature whitening and coloring transforms in WCT (Li et al., 2017c) can match the feature correlation of content images to that of style references exactly, but it will change the local feature contrast which maintains image structures. AdaIN has very nice properties that won’t damage local feature contrast but it can not match the feature statistics of style references compactly and thus lead to inferior stylization effects.\n\nIn this paper, we aim to achieve photorealistic stylization while still make synthesized images look like taken from cameras. As shown in Figure 13, AdaIN is powerful to generate synthesized images with good stylization effects and keeping photorealistic. However, if we zoom in to see more details, we find that AdaIN will overwrite some image details and produce images that seem to be ”overexposed”. We attribute this to that AdaIN aligns feature statistics of content images with their style references in a shared feature space, which may weaken the local structural details. To alleviate this issue and achieve better stylization results, the decoupled instance normalization aims to decompose the feature transformation into a style whitening step and a restylization step. Thus, we need to insert a convolutional layer after the style whitening operation. The motivation of designing the 3x3 convolutional layer with 2c filters is to expand the feature channels to recover the missing information during the whitening step. Then in the subsequent stylization step, we can conduct style transfer in a higher dimension to benefit from the rich information provided by more feature channels. At last, we fuse the transformation with a simple convolution operation. From Figure 13 and Figure 16, we can find that DecoupledIN can preserve more image details and achieve better stylization results.\n\nMultiple Style Whitening in DecoupledIN. The structure of DecoupledIN modules with more whitening operations are demonstrated in figure 14 and figure 15. From Figure 16, we can find\n\n16\n\n(a) Content and Style(b) AdaIN(c) DecoupledINUnder review as a conference paper at ICLR 2023\n\nFigure 14: Visualisation of the module structure of DecoupledIN with two style whitening.\n\nFigure 15: Visualisation of the module structure of DecoupledIN with three style whitening.\n\nFigure 16: Visual comparison of AdaIN and DecoupledIN with different style whitening strategies. From left to right: (a)content and style images, (b) AdaIN, (c) DecoupledIN with one style whitening, (d) DecoupledIN with two style whitening, and (e) DecoupledIN with three style whitening. The stylization effects is enhanced from left to right gradually.\n\n17\n\nNomalization3x3 Conv3x3 Conv3x3 Convmeanstd3x3 ConvNomalizationNomalization3x3 Conv3x3 Conv3x3 Convmeanstd3x3 ConvNomalizationx2(a) Content and Style(b) AdaIN(c) DecoupledIN(d) Whitening x 2(e) Whitening x 3Under review as a conference paper at ICLR 2023\n\nthat althrough DecoupledIN gets better stylization effects compared with AdaIN, the style of the original content input is remained and combined with the style reference to generate mixed stylization results. To remove the original image style more completely, we conduct multiple style whitening in DecoupledIN and visulaize the results in Figure 16. We conduct the style whitening in DecoupledIN by 1, 2 and 3 times respectively. When looking at these image carefully, we can find that more style whitening will lead to better stylization effects. Since the influence of different feature transformations is hard to be distinguished, please zoom in to check the color variations carefully.\n\nB.3 FLOW ESTIMATION AND CONVLSTM FOR TEMPORAL CONSISTENCY MAINTENANCE\n\nOptical flow estimation methods (Dosovitskiy et al., 2015; Ilg et al., 2016; Teed & Deng, 2020) and ConvLSTM (Shi et al., 2015a) have been exploited in many video style transfer literature (Chen et al., 2017; 2020b; Anderson et al., 2016; Gao et al., 2020) to ensure temporal coherency. In ColoristaNet, we exploit RAFT (Teed & Deng, 2020) to track pixels across different frames and use ConvLSTM to pass the contextual information to adjacent frames. With RAFT and ConvLSTM, we implement temporal consistent feature transformations and thus can conduct photorealistic video style transfer with high temporal coherency. While as it has been shown in the supplementary videos, simply transferring styles of each frame in videos with image-based photorealistic style transfer algorithms can still generate stylized videos in good quality. It’s because that consecutive frames in videos are temporally coherent and we use a Gaussian smoothing function to smooth the style vectors during switches between different style references. However, if we zoom in to check the details of other state-of-the-art methods, such as PhotoWCT (Li et al., 2018), WCT2 (Yoo et al., 2019) and PhotoNAS (An et al., 2020), we can find that there are many structural distortions and flickering artifacts. Since these previous methods can’t ensure the photorealism in most of their stylization results, the temporal inconsistency problem are not in the first place to be solved.\n\nWithout RAFT. To check whether RAFT can maintain temporal consistency during style transfer, we conduct ablation study by removing RAFT from ColoristaNet and still use ConvLSTM to fuse information across different frames. Figure 17 shows that without RAFT, ColoristaNet generates many stylization results with obvious artifacts. It’s because without the guidance of optical flow, the information propagation between adjacent frames becomes incorrect. In fact, in some cases, RAFT failed to generate accurate correspondences between image pixels and thus lead to bad stylization effects.\n\nWithout ConvLSTM. To check whether ConvLSTM can incorporate contextual information among adjacent frames, we remove the ConvLSTM units to test the performance of ColoristaNet. Since ConvLSTM aims to propagate information across different frames, if there is no ConvLSTM, RAFT should be removed too. That means we just conduct video style transfer with image-based algorithms as that of PhotoWCT (Li et al., 2018), WCT2 (Yoo et al., 2019) and PhotoNAS (An et al., 2020). Figure 18 shows two different kinds of scenarios that ConvLSTM is important for ColoristaNet. In the first case, if there is no ConvLSTM module, there will have some flickering artifacts. And in the second case, ColoristaNet with ConvLSTM can ensure style consistency when there are multiple styles to be transferred.\n\nB.4 EMPLOYMENT OF MULTI-SCALE FEATURES\n\nSince deep convolutional neural networks have multi-scale hierarchical structures and can encode images into features with different semantic levels, it’s popular to conduct style transfer with the benefits of multi-scale features (Li et al., 2018; Yoo et al., 2019). PhotoWCT (Li et al., 2018) passed the lost spatial information to the decoder in a hierarchical manner to facilitate reconstructing fine details for photorealistic image synthesis. WCT2 (Yoo et al., 2019) inherited the U-net style structures from PhotoWCT and achieved impressive results. PhotoNAS (An et al., 2020) searched the best architecture for photorealistic style transfer and found that the multi-level stylization strategy is important for style transfer in high quality. We adopt the multi-scale architecture as previous methods to conduct style transfer. We conduct ablations on the multi-scale feature fusion scheme of ColoristaNet. In the style transfer network, there are feature transformations at four different resolutions. Denote the feature transformations in ColoristaNet from lower resolutions to higher resolutions with ”conv1 1”, ”conv2 1”, ”conv3 1” and ”conv4 1” respectively. We remove the feature transformation at the ”conv4 1” stage at first, and remove other feature transformations\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 17: Investigation of the effectiveness of flow estimation network in ColoristaNet. We test ColoristaNet in three different scenarios to see that without flow estimation network (RAFT (Teed & Deng, 2020)), ColoristaNet will generate many artifacts.\n\n19\n\n Time Step 1 Time Step 2 Time Step 3 Time Step 4 Time Step 5 Content & Stylewithout flow estimationColoristaNetContent & Stylewithout flow estimationColoristaNetContent & Stylewithout flow estimationColoristaNetUnder review as a conference paper at ICLR 2023\n\nFigure 18: Investigation of the effectiveness of ConvLSTM in ColoristaNet.\n\n20\n\nTime Step 1 Time Step 2 Time Step 3 Time Step 4 Time Step 5 Content & Stylewithout LSTMColoristaNetContent & Stylewithout LSTMColoristaNetUnder review as a conference paper at ICLR 2023\n\nFigure 19: Results of ablation on the multi-scale feature fusion scheme of ColoristaNet. (d) is complete multi-scale feature fusion scheme, (c) removes the feature transformation at the ”conv4 1” stage, (b) further removes the feature transformation at the ”conv3 1” stage on the basis of (c).\n\nconsecutively to check the stylization results. As shown In Figure 19, from left to right, we list the stylization results produced by ColoristaNet with two, three, and four different feature scales. It can be found that with more different scales, we get better stylization results with less artifacts.\n\nC EXPERIMENTAL RESULTS WITH FURTHER ANALYSIS\n\nC.1 COMPARISON WITH STATE-OF-THE-ART METHODS\n\nPrevious state-of-the-art photorealistic style transfer algorithms, such as MVStylizer (Li et al., 2020) and Xia’s video style transfer (Xia et al., 2021), don’t make their codes publicly available, so we can just compare with image-based photorealistic style transfer algorithms. We compare with four state-of-the-art algorithms including PhotoWCT (Li et al., 2018), WCT2 (Yoo et al., 2019) and PhotoNAS (An et al., 2020). We simply conduct style transfer on image frames independently and put the stylization results together to generate stylized videos. To avoid drastic style change when there are multiple style references, a Gaussian smoothing function is exploited to smooth the stylization vectors among frames with different style references. The Gaussian kernel size is 20. That means for every 20 video frames, their corresponding stylization vectors are smoothed. Figure 20, Figure 21, Figure 27, Figure 28 and Figure 29 give the visual comparison with state-of-the-art algorithms. All previous state-of-the-art algorithms will produce observable blur, structural distortions and flickering artifacts. While ColoristaNet generates videos that look like taken from cameras without any blur, distortions and artifacts. Although ColoristaNet does not transfer the colors of style references to the targets completely, the stylization results are still very competitive.\n\n21\n\n(a) Content & Style(b) ColoristaNet without \"conv3_1\" and \"conv4_1\" branches(c) ColoristaNet without the \"conv4_1\" branch(d) ColoristaNetUnder review as a conference paper at ICLR 2023\n\nFigure 20: Visual comparison with popular algorithms including WCT2 (Yoo et al., 2019), PhotoWCT (Li et al., 2018), PhotoNAS (An et al., 2020) and our ColoristaNet.\n\n22\n\n(a) Content and Style(b) WCT2(c) PhotoNAS(d) OursUnder review as a conference paper at ICLR 2023\n\nFigure 21: Additional Visual comparison with popular algorithms including WCT2 (Yoo et al., 2019), PhotoWCT (Li et al., 2018), PhotoNAS (An et al., 2020) and our ColoristaNet.\n\nContradictions between Stylization and Photorealism. Separating styles of images from their contents is very difficult since there is no concrete definition of the content and style of an image. In Gatys’ paper (Gatys et al., 2016), they stated features in higher layer of deep convolutional neural networks capture the high-levl contents interms of objects and their arrangements and can be thought as the content representation. However, as stated in Lapstyle (Li et al., 2017a), the low-level features of the content image is substituted by that of style references and thus will generate unpleasing artifacts. Photorealistic style transfer is some kind of color transfer that need to get the color styles of references image while keeps image content unchanged. In fact, it’s hard to achieve a balance between stylization and photorealism. If one want to get a stylization result that matches the style of another image exactly, the color style and the local textures need to be modified. Modifications in local textures will lead to painterly artifacts easily as shown in Figure 26.\n\nC.2 COMPUTATIONAL COST ANALYSIS\n\nIn Table 4, we list the computational cost of different sections of ColoristaNet. There are three main components in ColoristaNet, including VGG-19 feature backbone (Simonyan & Zisserman, 2014), RAFT (Teed & Deng, 2020) and the decoder. There are 3.5M, 1M and 72.2M parameters in them respectively. The VGG-19 feature backbone has only 3.5M parameters because we only use feature maps at the first four convolutional stages. There are relative less parameters in these convolutional layers. RAFT has 1M parameters and the inference speed is very slow compared with other components. The decoder has 72.2M parameters because there four sets of ConvLSTM units, DecoupledIN modules and many other convolutional operations. It can be found that RAFT takes the most time in ColorsitaNet. But when there is no RAFT, ColoristaNet will generate obvious artifacts in many cases as shown in Figure 17. So how to replace RAFT with some other faster methods that can track pixels across different frames is an important problem for future study.\n\nC.3 STYLIZATION EFFECTS CONTROL\n\nColoristaNet achieves a balance between good stylization effects and photorealism. We try to improve and control the stylization effects by incorporating a style loss during training, adding a\n\n23\n\nContent & StyleWCT2PhotoWCTPhotoNASColoristaNetUnder review as a conference paper at ICLR 2023\n\nModules VGG-19 backbone RAFT the decoder\n\nParameters (M) 3.5 1.0 72.2\n\nInference Speed (ms) 5\n209 14\n\nTable 4: Comparison of efficiency between the VGG-19 feature backbone, RAFT and the decoder.\n\nstylization factor in DecoupledIN, and exploiting several DecoupledIN consecutively. We visualize the stylization results in Figure 22 and give some analysis in the following subsections.\n\nIncorporation of the Style Loss. We add a stylization loss in both style removal and restoration networks. We have try different weights for the style loss, but these training don’t converge.\n\nStylization Factor. Since DecoupledIN can conduct style transfer by matching feature statistics of content images to that of style references, We add a stylization factor λ ∈ [0, 1] in the matching process as follows\n\nWhitening : f ′\n\nCt,i =\n\nfCt,i − μ (fCt,i) σ (fCt,i)\n\n,\n\n(cid:1) , Conv (fSt,i) ,\n\nCt,i, f ′′\n\nSt,i = Conv (cid:0)f ′\n\nTransform : f ′′ Reweighting : Varnew, Meannew = λσ (cid:0)f ′′ Ct,i − μ (cid:0)f ′′ f ′′ f ′′\n\nStylization : gt,i = Varnew\n\nCt,i\n\n\n\n\n\n(cid:16)\n\nσ\n\nCt,i (cid:17)\n\nCt,i\n\n\n\n(cid:1)\n\n + Meannew,\n\n(cid:1) + (1 − λ) σ (cid:0)f ′′\n\n(cid:1) , λμ (cid:0)f ′′\n\nSt,i\n\n(cid:1) + (1 − λ) μ (cid:0)f ′′\n\nCt,i\n\n(cid:1) ,\n\nCt,i\n\nSt,i\n\n(3) where μ and σ calculate the mean and standard deviation for each feature channel respectively, and Varnew and Meannew are the reweighted mean and standard deviation vectors. During the training, we add the stylization factor in the style restoration network, and get the λ value randomly using the sampling strategy based on a uniform distribution. During the inference, we set λ from 0.2 to 1.0. Figure 22 visualizes the stylization results. It can be found that ColoristaNet can control the stylization effects explicitly.\n\nMultiple Decoupled Instance Normalization. In some cases, the style of the original content input is still contained in stylization results produced by ColoristaNet. Here we aim to get stronger stylization results by matching the style of reference images much more closely through adding more DecoupledIN modules consecutively. Figure 23 indicates that when we apply more DecoupledIN modules consecutively in ColoristaNet, the stylization effects will become stronger.\n\nD FAILURE CASES AND FURTHER DISCUSSIONS\n\nTo examine the robustness of ColoristaNet, we conduct experiments on a plenty of videos to find the failure cases of ColoristaNet. After huge amount of experiments, we find some failure cases and summarize them into three categories, including (1) temporal inconsistency in small regions; (2) under-stylization; (3) over-stylization (caused by extra whitening steps). As in figure 24, we can see that in the original frames, the background color in the red boxes remain unchanged, while in the stylized frames, the coloring is shifting between bright green and dark green. Such inconsistency is caused by properties of DecoupledIN. As the foreground object moves dramatically, the mean and standard deviation of content feature channels will change, which will influence stylizations of both the moving object and the remaining stationary pixels. As such inconsistency can be fixed by ConvLSTM combined with flow estimation in most cases, they become identifiable when flow estimation fails. Among several hundreds of test videos we have examined, we only spotted subtle temporal inconsistencies like Figure 24. As discussed in Appendix B.3, we attribute this to that the input content videos are temporally coherent so in most cases the temporal inconsistency is not obvious. Compared with state-of-the-art methods, our methods sometimes suffer under-stylization (in Figure 25). This is expected since we prioritize photorealism. In the future, we aim to enhance the stylization performance of our model while still maintaining a high level of photorealism. When we\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 22: Illustration of the gradual change in stylisation effect when different style control factors λ are taken. ColoristaNet can control the stylization effect through the stylization factor explicitly.\n\n25\n\n(a) Content and Style(b) λ = 0.2(c) λ = 0.4(d) λ = 0.6(e) λ = 0.8(f) λ = 1.0Under review as a conference paper at ICLR 2023\n\nFigure 23: Visualising the results of using AdaIN, decoupledIN, two consecutive decoupledIN and three consecutive decoupledIN.\n\nFigure 24: Illustration of failure cases produced by ColoristaNet: Temporal Inconsistency.\n\nFigure 25: Illustration of failure cases produced by ColoristaNet: Under-stylization.\n\n26\n\n(a) Content and Style(b) AdaIN(c) DecoupledIN(d) DecoupledIN x 2(e) DecoupledIN x 3(a) Frame 32(b) Frame 40(d) Frame 132(c) Frame 128(a) Content and Style(b) WCT2(c) PhotoNAS(d) OursUnder review as a conference paper at ICLR 2023\n\nFigure 26: Illustration of failure cases produced by ColoristaNet: Over-stylization.\n\napply multiple whitening steps in our DecoupledIN module, it is possible that too much information is erased, which makes the stylized video look misty (in Figure 26). This is a trade off for stronger stylization effects.\n\n27\n\n(a) Content and Style(b) Normalize once(c) Normalize twice(d) Normalize three timesUnder review as a conference paper at ICLR 2023\n\nFigure 27: Additional comparison between ColoristaNet and state-of-the-art methods.\n\n28\n\n(a) Content and Style(b) WCT2(c) PhotoNAS(d) OursUnder review as a conference paper at ICLR 2023\n\nFigure 28: Additional comparison between ColoristaNet and state-of-the-art methods.\n\n29\n\n(a) Content and Style(b) WCT2(c) PhotoNAS(d) OursUnder review as a conference paper at ICLR 2023\n\nFigure 29: Additional comparison between ColoristaNet and state-of-the-art methods.\n\n30\n\n(a) Content and Style(b) WCT2(c) PhotoNAS(d) Ours",
    "reference": "# Summary Of The Paper\n\nThis paper presents a novel method for photorealistic video style transfer that conducts color style transfer in videos without undesirable painterly spatial distortions and temporally inconsistent flickering artifacts. The proposed style removal-and-restoration framework, namely ColoristaNet, is capable of learning stylization in a self-supervised fashion. The authors propose to leverage docoupled instance normalization and ConvSLTM units to achieve arbitrary style transfer while keeping the photorealism and temporal conherency. Through a number of comparisons and ablation study, the authors demonstrate ColoristaNet's superior quality to previous state-of-the-art photorealistic style transfer algorithms.\n\n# Strength And Weaknesses\n\nStrengths:\n\nThe paper is well written and the exposition is clear.\n- Inspired by AdaIN that applied adaptive affine transformations, the authors propose decoupled instance normalization (DecoupledIN) that uses linear transforms in both feature whitening and stylization, capable of conducting photorealistic style transfer effectively.\n- To maintain temporal consistency and avoid flickering artifacts, a novel architecture employing optical flow estimation and contextual information with ConvLSTM units is introduced to capture temporal dependencies.\n- Sufficient amount of qualitative and quantitative comparisons with prior work and ablation studies are conducted to show the efficiency and quality improvement of the proposed photorealistic style transfer framework.\n\nWeaknesses:\n\n- It seems there is no explicit control, i.e., hyperparameters, to balance the content and style in the stylization output. It is known that more stylized results typically tend to present painterly distortions instead of maintaining the salient image structure. Still, it would be nice to present such extensions or simply show a figure to display results with different level of stylization.\n- Show some failure cases. For example, what happen when optical flow estimation is inaccurate? What happen when content video and style exemplar does not share any similarity in high-level contents?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written, clear and easy to follow. I would suggest the authors to release the reference implementation if accepted to faciliate reproducibility. \n\nSome limitations:\nAs the authors pointed out, current framework relies on FlowNet to compute optical flow which is computationally expensive. Along with some other modules, the inference speed prevents ColoristaNet to be a real-time style transfer approach. Would be interesting to explore how the runtime performance can be improved, possibly revisiting some of the modules.\n\nAnother limitation, as mentioned above, is that there seems to have no stylization strength control. This is understable since the method aims for photorealistic style transfer and needs to avoid painterly distortions. That said, it would be interesting to explore how to extend the existiing pipeline to support different stylization levels, while maintaining the photorealism and image structure of the scene content.\n\n# Summary Of The Review\n\nI am leaning towards acceptance. The paper is a technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "INVARIANT AGGREGATOR FOR DEFENDING AGAINST FEDERATED BACKDOOR ATTACKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFederated learning is gaining popularity as it enables training of high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Specifically, an adversary can perform backdoor attacks to control model predictions via poisoning the training dataset with a trigger. In this work, we propose a mitigation for backdoor attacks in a federated learning setup. Our solution forces the model optimization trajectory to focus on the invariant directions that are generally useful for utility and avoid selecting directions that favor few and possibly malicious clients. Concretely, we consider the sign consistency of the pseudo-gradient (the client update) as an estimation of the invariance. Following this, our approach performs dimension-wise filtering to remove pseudo-gradient elements with low sign consistency. Then, a robust mean estimator eliminates outliers among the remaining dimensions. Our theoretical analysis further shows the necessity of the defense combination and illustrates how our proposed solution defends the federated learning model. Empirical results on three datasets with different modalities and varying number of clients show that our approach mitigates backdoor attacks with a negligible cost on the model utility.\n\n1\n\nINTRODUCTION\n\nFederated learning enables multiple distrusting clients to jointly train a machine learning model without sharing their private data directly. However, a rising concern in this setting is the ability of potentially malicious clients to perpetrate backdoor attacks. To this end, it has been argued that conducting backdoor attacks in a federated learning setup is practical (Shejwalkar et al., 2022) and can be effective (Wang et al., 2020). For instance, the adversary can connect to a federated learning system as a legitimate user and conduct a backdoor attack that forces the model to mispredict. The impact of such attacks is quite severe in many mission-critical federated learning applications. For example, anomaly detection is a common federated learning task where multiple parties (e.g., banks or email users) collaboratively train a model that detects frauds or phishing emails. Backdoor attacks allow the adversary to successfully circumvent these detection methods.\n\nThe most common backdoor attack embeds triggers in the data samples and forces the model to make an adversary-specified prediction when the trigger is observed (Liu et al., 2018; Bagdasaryan et al., 2020). Thus, an adversary can conduct a backdoor attack by generating a trigger that statistically correlates with a particular label. Once the adversary injects these trigger-embedded backdoor data samples into the training data, the model can entangle the trigger-label correlation and predict as the adversary specifies. Meanwhile, the backdoor attack often does not degrade the predictive accuracy on the benign samples, making backdoor detection difficult in practice (Wang et al., 2020).\n\nIn federated learning, the server aggregates only the client-level updates (a.k.a. pseudo-gradient or gradient for short) without control over the training procedure or any data samples. Such limited visibility of the federated learning server on the client-side training makes defending against backdoor attacks challenging. Common defenses against backdoor attacks aim at identifying the backdoor data samples or poisoned model parameters and usually require access to at least a subset of the training data (Tran et al., 2018; Li et al., 2021a), which is prohibitive for a federated learning server. Other defense methods against untargeted poisoning attacks that degrade the model utility\n\n1\n\n(Shejwalkar et al., 2022) are applicable but lack robustness against backdoor attacks, as discussed in Section 6.2.\n\nOur approach. Our defense leverages the observation that learning from the poisonous data does not benefit the model on benign data and vice versa. Therefore, focusing on the invariant directions that are generally beneficial in the model optimization trajectory helps defending against the aforementioned backdoor attack (which often lead to non-invariant directions). To this end, we develop a defense by examining each dimension of the gradients on the server-side and checking whether the dimension-wise gradients point in the same direction across the clients. Here, a dimension-wise gradient can point to a positive or negative direction, or have a zero value. In the case of small learning rates and for a specific dimension, two gradients pointing in the same direction means that taking the direction of one gradient can benefit the other. As such, the invariance of a direction depends on how many dimension-wise gradients align with that direction. Following this intuition, we define the sign consistency of a dimension by the average gradient sign. The higher the sign consistency is, the more invariant direction the gradient dimension may have.\n\nDesigning such a method carefully selecting only the invariant gradient directions is non-trivial, especially given the non-i.i.d. gradient distributions across benign clients and the presence of malicious clients. Hence, our approach enforces two separate treatments for each gradient dimension. First, we employ an AND-mask (Parascandolo et al., 2021), a dimension-wise filter setting the gradient dimension with sign consistency below a given threshold to zero. However, this alone is not enough: the malicious clients can still use outliers to mislead the aggregation result in the remaining highly consistent dimensions. To address this issue, we propose using the trimmed-mean estimator (Xie et al., 2020b; Lugosi & Mendelson, 2021), as a means to remove the outliers. Our analysis suggests that the AND-mask complements the trimmed-mean estimator well, motivating their composition.\n\nWe support the proposed approach with a theoretical analysis under a conventional linear regime (Rosenfeld et al., 2021; Wang et al., 2022; Zhou et al., 2022; Manoj & Blum, 2021), showing that the composition of the AND-mask and the trimmed-mean estimator is necessary for defending against backdoor attacks. Our analysis starts with feature invariance and discusses the connection between feature invariance and gradient sign consistency. Then, we outline conditions under which triggerbased backdoor attacks can lead to non-invariant directions and decrease the sign consistency of a dimension. Further analysis results demonstrate the necessity for the combination of both the AND-mask and the trimmed-mean estimator. Simulation results in Appendix D.1 further verify our theoretical results.\n\nOur empirical evaluation employs the strong edge-case backdoor attack (Wang et al., 2020), as detailed in Section 6.1, to test our defense. Empirical results on tabular (phishing emails), visual (CIFAR-10) (Krizhevsky, 2009; McMahan et al., 2017), and text (Twitter) (Caldas et al., 2018) datasets demonstrate that our method is effective in defending against backdoor attacks without degrading utility as compared to prior works. On average, our approach decreases the model accuracy on backdoor samples by 61.6% and only loses 1.2% accuracy on benign samples compared to the standard FedAvg aggregator (McMahan et al., 2017).\n\nContributions. Our contributions are as follows:\n\n• We develop a combination of defenses using an AND-mask and the trimmed-mean estimator against the backdoor attack by focusing on the dimension-wise invariant directions in the model optimization trajectory.\n\n• We theoretically analyze our strategy and demonstrate that a combination of an AND-mask\n\nand the trimmed-mean estimator is necessary in some conditions.\n\n• We empirically evaluate our method on three datasets with varying modality, model architecture, and client numbers, as well as comparing the performance to existing defenses.\n\n2 RELATED WORK\n\nBackdoor Attack. Common backdoor attacks aim at misleading the model predictions using a trigger (Liu et al., 2018). The trigger can be digital (Bagdasaryan et al., 2020), physical (Wenger et al., 2021), semantic (Wang et al., 2020), or invisible (Li et al., 2021b). Recent works extended\n\n2\n\nbackdoor attacks to the federated learning setting and proposed effective improvements such as gradient scaling (Bagdasaryan et al., 2020) or generating edge-case backdoor samples (Wang et al., 2020). The state-of-the-art edge-case backdoor attack shows that using backdoor samples with low probability density on benign clients (i.e., unlikely samples w.r.t. the training distribution) are hard to defend in the federated learning setting.\n\nCentralized Defense. There is a line of work proposing centralized defenses against backdoor attacks where the main aim is either detecting the backdoor samples (Tran et al., 2018) or purifying the model parameters that are poisoned (Li et al., 2021a). However, applying such centralized defense to federated learning systems is in practice infeasible due to limited access to the client data in many implementations.\n\nFederated Defenses. Several recent works have attempted to defend against backdoor attacks in federated learning systems. Sun et al. (2019) shows that weak differential-private (weak-dp) federated averaging can mitigate the backdoor attack. However, the weak-dp defense is circumvented by the improved edge-case federated backdoor attack (Wang et al., 2020). Nguyen et al. (2021) suggest that the vector-wise cosine similarity can help detect malicious clients performing backdoor attacks. The vector-wise cosine similarity is insufficient when the backdoor attacks can succeed with few poisoned parameters, incurring little vector-wise difference (Wu & Wang, 2021). Other defenses against untargeted poisoning attacks (Blanchard et al., 2017; Xie et al., 2020b) lack robustness against the backdoor attack. Sign-SGD with majority vote (Bernstein et al., 2018; 2019) is similar to our approach, but it always takes the majority direction instead of focusing on the invariant directions. Section 6.2 discusses the limitation of previous defenses in more detail, along with the empirical evaluation. Unlike existing works, our defense encourages the model to pursue invariant directions in the optimization procedure.\n\nCertification. Unlike the above discussed defenses, certification (Xie et al., 2021) aims at extinguishing backdoor samples within a neighborhood of a benign sample. A direct comparison between certification and our defense is not meaningful due to the different evaluation metrics. Certification considers the certification rate of benign samples as the metric, while our defense aims at reducing the accuracy of the backdoor samples. However, it would be interesting to investigate whether the proposed defense can ease the certification of a model.\n\n3 PROBLEM SETUP\n\nNotation. We assume a synchronous federated learning system, where N clients collaboratively train an ML model f : X −→ Y with parameter w coordinated by a server. An input to the model are the data samples x ∈ X = Rd with d features indexed by k and a label y. There are N ′ < N 2\nadversarial clients aiming at corrupting the ML model during training (Shejwalkar et al., 2022). The ith, i ∈ [1, ..., N ], client has ni data samples, being benign for i ∈ [1, ..., N − N ′] or being adversarial for i ∈ [N − N ′ + 1, ..., N ]. The synchronous federated learning is conducted in T rounds. In each round t ∈ [1, ..., T ], the server broadcasts a model parameterized by wt−1 to all the participating clients. We omit the subscript t while focusing on a single round. Then, the ith client optimizes wt−1 on their local data samples indexed by j and report the locally optimized wt,i to the server. We define pseudo-gradient gt,i = wt−1 − wt,i being the difference between the locally optimized model and the broadcasted model from the previous round. Note, for simplicity, that we often use the term “gradient” to refer to the pseudo-gradient. Once all gradients are uploaded, the server aggregates them and produces a new model with parameters wt using the following rule: wt = wt−1 − (cid:80)N gt,i. The goal of federated learning is to minimize a weighted risk function over the N clients: F (w) = (cid:80)N EDi[l(f (x; w), y)], where l : R × Y −→ R is a loss function. ⊙ denotes the Hadamard product operator.\n\nFi(w) = (cid:80)N\n\nni i=1 ni\n\nni i=1 ni\n\nni i=1 ni\n\n(cid:80)N\n\n(cid:80)N\n\n(cid:80)N\n\ni=1\n\ni=1\n\ni=1\n\nThreat Model. The adversary generates a backdoor data sample x′ by embedding a trigger in a benign data sample x and correlating the trigger with a label y′, which is different from the label y of the benign data sample. We use D′ to denote the distribution of backdoor data samples. Then, the malicious clients connect to the federated learning system and insert backdoor data samples into the training set. Since the goal of federated learning is to minimize the risk over all clients’ datasets, the model can entangle the backdoor while trying to minimize the risk over backdoor samples on the malicious clients. Appendix A visualizes some backdoor samples.\n\n3\n\n(a) Failure Mode 1 (AND-Mask)\n\n(b) Failure Mode 2 (Trimmed-mean)\n\nFigure 1: Failure modes of AND-mask (a) and trimmed-mean estimator (b). Note that an aggregator fails if the malicious value flips the sign of the aggregation result compared to the true aggregate. Blue dots are benign values. Red crosses are malicious values. The orange box is an arithmetic mean and the orange triangle is a trimmed-mean. In (a), the adversary uses outliers to flip the sign of the arithmetic mean when the benign values have the same sign. (b) shows that the trimmed-mean estimator may bias toward the malicious value when the average is supposed to be zero, but the benign values have diverse signs.\n\nTo simplify tedious notation, we assume all users have the same number of samples i.e., ni = ni′, ∀i ̸= i′.\n\n4 METHOD\n\nThis section presents the proposed server-side defense operating on the gradients. We start with an overview of the idea, then introduce the two complementary components of our defense and outline the invariant aggregator steps.\n\n4.1 OVERVIEW\n\nIn the proposed defense, we aim at finding invariant directions to optimize the federated learning model, such that the model is generally utilitarian for most of clients and can exclude the directions that benefit a subset of potentially malicious clients. Since our defense operates with the gradients, we consider the invariant direction from a first-order perspective (Parascandolo et al., 2021). Expanding the loss function around the current weight w on client i, for a parameter update g, we have:\n\nEDi[l(x, y; w + g)] = EDi[l(x, y; w)] + EDi [∇wl(x, y; w)]⊤g + R2(w + g),\n\n(1)\n\nwhere R2(w + g) is a second-order Taylor remainder. With a reasonably small ∥g∥ (achievable with small learning rates), the remainder term R2(w + g) is negligible and the change of the loss function mainly depends on the first-order gradient and the parameter update g. Since learning the trigger benefits the malicious clients exclusively, there exists at least one gradient dimension k and one benign client i ∈ {1, ..., N − N ′} where ED′[∇wk l(x, y; w)] ̸= 0 and the dimension-wise gradient have inconsistent signs, i.e.,\n\nEDi[∇wk l(x, y; w)] × ED′[∇wk l(x′, y′; w)] ≤ 0.\n\n(2)\n\nIf the condition in Equation 2 is not true, then learning the trigger would always benefit the model on benign data, thereby contradicting common empirical observation. A more detailed analysis of the condition in Equation 2 is in Section 5.2. The proposed methods show how to treat the inconsistent signs and defend against the backdoor attack by only allowing invariant directions. A dimensionwise analysis is necessary because backdoor attacks can succeed by poisoning few inconsistent dimensions (Wu & Wang, 2021) without incurring much vector-wise difference, as Section 6.2 will show.\n\nWe consider two treatments for each gradient dimension to help the model avoid the direction specified by ED′[∇wl(x′, y′; w)]: (Treatment 1) setting the dimension with inconsistent signs to zero using an AND-mask (Parascandolo et al., 2021) such that no client benefits or (Treatment 2) employing a robust mean estimator (e.g., trimmed-mean (Xie et al., 2020b)) to remove the malicious values that cause the inconsistent sign. To achieve a better result, we combine these two treatments to avoid their failure modes. The following examples illustrate the failure mode of each treatment and motivate the combination of defense.\n\nFailure Mode 1. Figure 1a shows an example where the adversary may exploit a dimension by inserting outliers where the benign values have a consistent sign. The outliers can mislead the average toward the non-invariant direction. The robust mean estimator (treatment 2) can trim the outliers\n\n4\n\nand accurately estimate the mean. In contrast, treatment 1 can fail with a high sign consistency. Because it either lets the highly consistent dimension pass or has to be over-aggressive in zeroing out the dimensions, hurting the model’s accuracy on benign data.\n\nFailure Mode 2. Figure 1b shows some values with inconsistent signs, which treatment 1 can handle. However, this example can fail the robust mean estimator (treatment 2), whose result has the same sign as the malicious values.\n\nThe following sections shall detail the two treatments and discuss their complementary relationship.\n\n4.2 AND-MASK\n\n(cid:80)N\n\nThe AND-mask (Parascandolo et al., 2021) computes a dimension-wise mask by inspecting the sign consistency of each dimension across clients. For dimension k, the sign consistency is: | 1 i=1 sign(gi,k)|. If the sign consistency is below a given threshold τ , the mask element mk N\nis set to 0, otherwise, mk is set to 1. The mask along dimension k is defined as: Definition 1. (AND-Mask) For the kth dimension in the gradient vector, the corresponding mask mk is defined as:\n\nmk := 1\n\n|\n\nsign(gi,k)| ≥ τ\n\n.\n\n(3)\n\n(cid:34)\n\nN (cid:88)\n\ni=1\n\n(cid:35)\n\nOur defense then multiplies the mask m with the aggregated gradient ̄g element-wise, setting the inconsistent dimension to zero.\n\n4.3 TRIMMED-MEAN\n\nTo complement the AND-mask, our defense broadcasts the trimmed-mean estimator to each gradient dimension. The trimmed-mean estimator alleviates the outlier issue by removing the subset of largest and smallest elements before computing the mean. The largest and smallest elements appear on the two tails of a sorted sequence. Next, we define order statistics and the trimmed mean estimator.\n\nDefinition 2. (Order Statistics) (Xie et al., 2020b) By sorting the scalar sequence {xi : i ∈ {1, ..., N }, xi ∈ R}, we get x1:N ≤ x2:N ≤ ... ≤ xN :N , where xi:N is the ith smallest element in {xi : i ∈ {1, ..., N }}.\n\nThen, the trimmed-mean estimator removes a α × N elements from each tail of the sorted sequence.\n\nDefinition 3. (Trimmed Mean Estimator) (Xie et al., 2020b) For α ∈ [0, 1], the α-trimmed mean of the set of scalars xi:N ∈ {1, ..., N } is defined as follows:\n\nTrMean({x1, ..., xN }; α) =\n\n1 N − 2 · ⌈α · N ⌉\n\nN −⌈α·N ⌉ (cid:88)\n\ni=⌈α·N ⌉+1\n\nxi:N ,\n\n(4)\n\nwhere ⌈.⌉ denotes the ceiling function.\n\n4.4 OUR APPROACH: INVARIANT AGGREGATOR\n\nAlgorithm 1 outlines the steps of our server-side defense that perform aggregation of invariant updates from the clients. The solution is composed of the AND-mask (treatment 1) and trimmed-mean estimator (treatment 2). Our defense applies the two components separately based on the sign consistency of each dimension with a threshold τ .\n\nWe show how these two components, i.e., the AND-mask and the trimmed-mean estimator, complement each other. Our analysis considers a single dimension and starts with the robustness of the trimmed mean estimator, which improves the robustness of the AND-mask against outliers. The following theorem extends the robustness guarantee of a modified trimmed-mean estimator (Lugosi & Mendelson, 2021), which is shown in Appendix B, to the conventional trimmed-mean estimator (Definition 3).\n\n5\n\nAlgorithm 1 Server-side Defense\n\nInput:\n\nA set of reported gradients, {gi | i ∈ {1, ..., N }}; Hyper-parameters τ , α;\n\nAggregator:\n\n1: Compute the AND-mask m := 1\n\n(cid:104) | (cid:80)N 2: Compute the trimmed-mean ̄g := TrMean({g1, ..., gN }; α) under Definition 3; 3: return m ⊙ ̄g;\n\ni=1 sign(gi)| ≥ τ\n\nfollowing Definition 1;\n\n(cid:105)\n\nTheorem 4. With the trimmed-mean estimator in Definition 3, for a given set of samples x1, ..., xN , with a corruption level η = N ′ N , let a = xαN :N and b = xN −αN :N following Definition 2, x be a random variable with variance σ and ̄x be the estimated mean, with probability at least (cid:80)N −N ′ 12 ), we have:\n\nN and a confidence level δ, set the trim level α = 8η+12 log( 4\n\n(cid:1)0.99i0.01N −N ′−ic−4(1−4e\n\n(cid:0)N −N ′ i\n\ni=N −αN\n\n−αN\n\nδ )\n\n| ̄x − E[x]| ≤ (20α + 10\n\n√\n\nα + 2c)σ\n\n(5)\n\nfederated data distribution and the stochastic gradient estimation process.\n\nThe proof is in Appendix B. Theorem 4 bounds the estimation error of a trimmed-mean estimator, which can increase as the variable’s variance increases. Multiple factors can increase the variance, such as non-i.i.d. In practice, a threat analysis is necessary to specify the maximum number of malicious clients to be tolerated, when the number of malicious clients is unknown. Our goal is to prevent the outliers from misleading the sign. Therefore, the estimation error to expectation ratio, | ̄x−E[x]| , is particularly relevant. Since the estimator error for a given α depends on the variance, a high expectationE[x] σ , is desirable. Then, we show that AND-mask identifies the elements with high variance ratio, expectation-variance ratios, avoiding the robustness degradation of the trimmed-mean estimator. The following theorem suggests that dimensions with a higher expectation-variance ratio have high probabilities of passing AND-mask, and increasing the mask threshold τ increases the chance of filtering out dimensions with low expectation-variance ratios.\n\nE[x]\n\nTheorem 5. Given a non-zero expectation-variance ratio φ = probability at most (cid:80)min(N,N −2N ′+τ N )\n\n(cid:1)φ−2i, the sign consistency is below τ .\n\nσ , N ′ malicious elements, with\n\ni=N −2N ′−τ N\n\n(cid:0)N −N ′ i\n\nE[x]\n\nAppendix B provides the proof. For the φ = 0 case, we may use the probability of the estimated gradient sign being the same as the malicious gradient to replace φ−2. We do not propose directly using the sample mean-variance ratio due to a potential issue: using the sample mean-variance ratio can be over-aggressive when the benign value has a consistent sign but varying magnitudes. Our ablation study in Appendix D.2 shows that AND-mask can preserve more utility than the sample mean-variance ratio when combined with the trimmed-mean estimator.\n\n5 ROBUSTNESS ANALYSIS FOR A SIMPLIFIED MODEL\n\nTo further motivate our approach, we consider a more direct robustness analysis for a specific generative model. This analysis discusses how the features impact the gradient sign consistency, when the two failure modes in Section 4.1 appear, and why we need a combination of defenses guaranteed by Theorems 4 and 5. First, we outline some preliminaries useful for this analysis.\n\n5.1 PRELIMINARIES\n\nWe consider a binary prediction task y ∈ {0, 1} with a linear model h(x) = w⊤x and a decision rule ˆy = 1w⊤x≥0(w⊤x). The training procedure uses a Sigmoid activation function s(z) = 1 and a logistic loss function l(z, y) = −y · log(z) − (1 − y) · log(1 − z), where z = h(x).\n\n1+e−z\n\nData Model. We assume that the samples per class are balanced on benign clients. The data samples come from a non-i.i.d. Gaussian distribution with a diagonal covariance matrix. On the ith client, for the kth feature, we have:\n\n.\n\n(6)\n\n(cid:16)\n\nxk ∼ N\n\n(cid:17)\n\n(2y − 1) · μi,k, σi,k\n\n6\n\nBackdoor Attack. We consider a single feature backdoor attack where the trigger is the kth feature xk. A backdoor using a specific feature is common (Wang et al., 2020). To backdoor images, the adversary often selects a pixel pattern or semantic pattern (e.g., blue color on airplanes). For text data, the trigger could be a dedicated set of characters . The malicious clients attack in collusion using the same backdoor samples, meaning that μi,k = μi′,k ̸= 0, ∀i, i′ ∈ {N − N ′ + 1, N }. To simplify the notation, we omit the subscript i for simplicity while focusing on a single client. Since the trigger is not useful or does not appear on benign clients, we assume μi,k = 0, ∀i ∈ {1, N −N ′}\n\nObjective. The backdoor attack is effective if the model entangles the trigger-label correlation. For a non-zero μk of the trigger xk, wkμk > 0 is a necessary condition for a model to entangle the trigger-label correlation. Therefore, in the case when μk > 0, avoiding wk from increasing and enforcing wk to decrease while wk > 0 can mitigate the backdoor attack.\n\n5.2 CONNECTING FEATURES TO GRADIENTS\n\nWe define the invariance of a feature by measuring its feature-label correlation (e.g., positive or negative) consistency across clients.\n\nDefinition 6. For a given feature k, its invariance p is defined as: p =\n\n(cid:12) (cid:12) (cid:12)\n\n1\n\nN · (cid:80)N\n\n(cid:12) (cid:12) i=1 sign(μi,k) (cid:12).\n\nSimilarly, we define the sign consistency of a gradient dimension k: Definition 7. With a linear model h(x) = w⊤x, a Sigmoid activation function s, and N clients, a\n\nq-consistent gradient w.r.t. wk satisfies:\n\n1\n\nN · (cid:80)N\n\ni=1 sign\n\nEx,y∼Di\n\n(cid:16)\n\n(cid:2)∇wk l(s(w⊤x), y)(cid:3) (cid:17)(cid:12)\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) = q.\n\nUnder Definitions 6 and 7, we discuss the connection between feature invariance and dimensionwise gradient sign consistency. First, we need to analyze the behavior of the gradient sign per client.\n\nTheorem 8. For a linear model with a Sigmoid activation function s and the logistic loss l, under our Gaussian data model, on the kth feature with a non-zero μk, if wkμk ≤ 0, we have sign In addition, if μk = 0, we have (cid:16)\n\n(cid:2)∇wk l(s(w⊤ · x), y)(cid:3) (cid:17)\n\n= sign(μk).\n\nEx,y∼Di\n\n(cid:16)\n\nsign\n\nEx,y∼Di\n\n(cid:2)∇wk l(s(w⊤ · x), y)(cid:3) (cid:17)\n\n= sign(wk).\n\nThe proof is provided in Appendix B. The result in Theorem 8 is intuitive. With a non-zero μk, if sign(wk) agrees with sign(μk), the gradient sign can be indefinite because the weight wk can be either larger or smaller than the optimal w∗ k. Otherwise, the gradient has the same sign as μk. If μk = 0, wk shall shrink to 0. Then, we outline the conditions that lead to inconsistent signs. Corollary 9. Under Theorem 8, suppose xk represents the trigger, μk = 0 on the N − N ′ benign clients and the N ′ malicious clients share the same non-zero μk, if wk = 0, the gradient w.r.t. wk is N -consistent. In addition, if wkμk > 0, the gradient w.r.t. wk is at least (1 − N ′ N ′ N )-consistent. The gradient w.r.t. wk is 1-consistent if wkμk < 0.\n\nCounting the gradient signs yields the result. Corollary 9 suggests that if wkμk > 0 and the model entangles the backdoor, the expected gradient w.r.t. wk can align with μk on malicious clients and conflict with μk on benign clients. Then, employing the trimmed-mean estimator to remove the malicious values can recover the invariant direction pointed by benign clients (Theorem 4) and thereby shrink μk. If μk = 0, using AND-mask can mask out the gradients w.r.t. wk from the malicious clients (Theorem 5). If wk remains 0, the model parameterized by w is robust to the trigger on xk. It is worth noting that the gradients w.r.t. wk is 1-consistent and align to μk when wkμk < 0. Such a consistent gradient may overshoot and flip the sign of wk. Reducing the learning rate can alleviate overshooting and the trimmed-mean estimator will guarantee wk to shrink after the overshooting.\n\nConnection to Failure Modes. If wkμk > 0, the gradients w.r.t. wk have a consistent but non-zero expectation among benign clients, causing the failure mode 1 in Section 4.1. On the other hand, the gradient variance can diversify the estimated gradient sign and cause the failure mode 2.\n\nAppendix D.1 further provides simulation results under the linear regime (Section 5.1), showing that the AND-mask can prevent the adversary from exploiting wk and the trimmed-mean estimator helps shrink wk.\n\n7\n\n6 EXPERIMENTS\n\nWe evaluate our defense on three realistic tasks on three different data types: (1) object recognition with visual data, (2) sentiment analysis with text data, and (3) phishing email detection with tabular data. We employ the state-of-the-art edge-case backdoor attack (Wang et al., 2020) to generate backdoor samples and evaluate of defense and existing defenses against it.\n\nAdditional Results. The simulations, an ablation study, an evaluation of the hyper-parameter sensitivity, evaluations with additional attack strategies, and empirical verifications of the two failure modes can be found in the Appendix D.\n\n6.1 EXPERIMENTAL SETUP\n\nWe briefly summarize our setup and report more details in Appendix C.\n\nMetrics. Our experiments employ two metrics: the main task accuracy (AccM) estimated on the benign samples and the backdoor task accuracy (AccB) over backdoor samples. A defense is designed to reduce the model’s accuracy on backdoor task and maintain the utility on the main task.\n\nDatasets. The visual data of the object detection task and text data of the sentiment analysis task are from CIFAR-10 (Krizhevsky, 2009; McMahan et al., 2017) and Twitter (Caldas et al., 2018), respectively. Each phishing email data sample has 45 standardized numerical features of the sender that represent the sender reputation scores. A large reputation score may indicate a phishing email. The reputation scores come from peer-reviewers in a reputation system (Jøsang et al., 2007). The adversary may use malicious clients to manipulate the reputation.\n\nFederated Learning Setup. We consider horizontal federated learning (Kairouz et al., 2021) where the clients share the same feature and label spaces. The number of clients are 100 for the three tasks. The server sample 20 clients at each round on the CIFAR-10 and phishing email experiments. We reduce the sampled client number to 15 on the Twitter experiment due to limited hardware memory.\n\nBackdoor Attack Setup. The adversary employs the edge-case backdoor attack, where it selects the data samples with low marginal probability in their data distribution to create backdoor samples. The visual and text backdoor samples follow the previous work (Wang et al., 2020). For the tabular data, we select the 38th feature (reputation), whose value is 0 on most of the data samples. Then, we let the adversary manipulate the 38th feature to 0.2 that has a low probability density on phishing emails and flip the label to non-phishing.\n\nThe adversary can control 20% clients on the CIFAR-10 and phishing email experiments and 10% clients on the Twitter experiment. Section 6.2 explains the different experiment configurations. Such an adversary is considered strong in practice (Shejwalkar et al., 2022). We consider a strong adversary because defending against strong adversary yields robustness against weak adversary, whose effectiveness is already shown (Wang et al., 2020). The adversary only uses backdoor samples during training.\n\n6.2 RESULT AND COMPARISON TO PRIOR WORKS\n\nOur results. Table 1 summarizes the performance of each defense on three tasks. Our approach decreases the backdoor task accuracy by 61.6% on average. The edge-case backdoor attack on the text sentiment analysis task (Twitter) is more difficult to defend and our approach mitigates the accuracy increase on the backdoor task by 41.7%. We hypothesize that the text sentiment analysis task has few invariant and benign features. For example, the shape features (Sun et al., 2021) in object classification tasks can be invariant across objects. In contrast, the sentiment largely depends on the entire sentence instead of a few symbols or features. Then, we discuss the limitations of prior defenses.\n\nVector-wise. Common vector-wise defenses such as Krum estimate pair-wise similarities in terms of Euclidean distance (Blanchard et al., 2017) (Krum and multi-Krum) of cosime similarity (Nguyen et al., 2021) (multi-KrumC) between each gradient and others. The gradients that are dissimilar to others are removed. The vector-wise view is insufficient for defending against backdoor attacks because backdoor attacks can succeed by manipulating a tiny subset of parameters (e.g. 5%) (Wu & Wang, 2021) without incurring much vector-wise difference. In practice, we observe that the malicious gradients can get high similarity scores and circumvent vector-wise defenses.\n\n8\n\nTable 1: Accuracy of Aggregators under Edge-case Backdoor Attack. Our approach reduces the model accuracy on backdoor samples by 61.7% on average, mitigating the backdoor attack, and achieves a comparable utility on benign samples as the standard FedAvg aggregator.\n\nMethod\n\nCIFAR-10\n\nTwitter\n\nPhishing\n\nAccM\n\nAccB\n\nAccM\n\nAccB\n\nAccM\n\nAccB\n\n.679 ± .001 .717 ± .001 FedAvg .140 ± .001 .275 ± .012 Krum .541± .002 .923 ± .021 Multi-Krum .681 ± .002 .821 ± .001 Multi-KrumC Trimmed-Mean .687 ± .001 .512 ± .002 Krum Trimmed-Mean .682 ± .001 .607 ± .002 .301 ± .005 .000 ± .001 Sign-SGD .454 ± .003 .828 ± .003 Weak-DP .415 ± .001 .572 ± .002 Freezing Layers .667 ± .001 .109 ± .001 FoolsGold .685 ± .001 .853 ± .002 RFA .662 ± .001 .984 ± .001 SparseFed .718 ± .001 .000 ± .001 No Attack .677 ± .001 .001 ± .001 Ours\n\n.722 ± .001 .440 ± .001 .579 ± .001 .766 ± .002 .727 ± .001 .656 ± .008 .594 ± .002 .701 ± .001 .728 ± .001 .640 ± .016 .727 ± .001 .641 ± .001 .610 ± .003 .751 ± .076 .667 ± .001 .374 ± .002\n\nN\\A\n\nN\\A\n\n.726 ± .002 .357 ± .001 .718 ± .001 .704 ± .002 .667 ± .001 .608 ± .002 .731 ± .001 .095 ± .001 .687 ± .001 .296 ± .003\n\n.999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .333 ± .333 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .000 .667 ± .333 .999 ± .001 .999 ± .001\n\nN\\A\n\nN\\A\n\n.999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .999 ± .001 .000 ± .001 .999 ± .001 .000 ± .001\n\nNote: The numbers are average accuracy over three runs. Variance is rounded up.\n\nDimension-wise. Failure mode 2 in Section 4.1 shows the limitation of the trimmed-mean estimator, which was the most effective defense against the edge-case backdoor attack. We also include SignSGD with majority vote (Bernstein et al., 2019) as a defense, which binarizes the gradient and takes the majority vote as the aggregation result. However, Sign-SGD struggles to train a large federated model (e.g., Resnet-18 on CIFAR-10) and can suffer from failure mode 1 where the clients have diverse signs. Then, the adversary can put more weight on one side and mislead the voting result. Combination. A naive combination of multi-Krum and the trimmed-mean estimator fails to defend against the backdoor attack because neither multi-Krum nor the trimmed-mean estimator avoids the failure mode of the other. Weak-DP. The weak-DP defense (Sun et al., 2019) first bounds the gradient norms, then add additive noise (e.g., Gaussian noise) to the gradient vector. The edge-case backdoor attack can work without scaling up the gradients, circumventing the norm bounding. For the additive noise, we hypothesize that in some dimensions, the difference between malicious and benign gradients can be too large for the Gaussian noise to blur their boundary. Freezing Layers. Since we employ a pre-trained Resnet-18 (He et al., 2016) on CIFAR-10, freezing the convolution layers may avoid entangling the trigger. However, this approach lacks empirical robustness, possibly because the adversary can use semantic features (e.g., blue color on airplanes) that the pre-trained model already learns as triggers. Advanced Defenses. FoolsGold (Fung et al., 2020) down-weights an update if that update has a high cosine similarity with another update. There are many ways to diversify updates. Malicious clients may leverage the stochastic gradient estimation process or mix backdoor samples with benign samples, whose distribution can differ across clients. RFA (Pillutla et al., 2022) computes geometric medians as the aggregation result, which is shown to be ineffective (Wang et al., 2020). SparseFed (Panda et al., 2022) only accepts elements with large magnitude in the aggregation results. However, benign and malicious updates can contribute to large magnitudes.\n\n7 CONCLUSION AND FUTURE WORK\n\nThis paper shows how to defend against backdoor attacks by focusing on the invariant directions in the model optimization trajectory. Enforcing the model to follow the invariant direction requires AND-mask to compute the sign-consistency of each gradient dimension, which estimates how invariant a dimension-wise direction can be, and use the trimmed-mean estimator to guarantee the model follows the invariant direction within each dimension. Both theoretical and empirical results demonstrate the combination of AND-mask and the trimmed-mean estimator is necessary and effective. Further defending against more advanced backdoor attacks such as invisible backdoors that add calibrated noise to all the features (Li et al., 2021b; Manoj & Blum, 2021) can be interesting.\n\n9\n\nREFERENCES\n\nEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 2938–2948. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/bagdasaryan20a.html.\n\nJeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. In Jennifer Dy and Andreas signSGD: Compressed optimisation for non-convex problems. Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 560–569. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/bernstein18a.html.\n\nJeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with\n\nmajority vote is communication efficient and fault tolerant. In ICLR, 2019.\n\nPeva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with\n\nadversaries: Byzantine tolerant gradient descent. In NIPS, 2017.\n\nSebastian Caldas, Peter Wu, Tian Li, Jakub Konecn ́y, H. B. McMahan, Virginia Smith, and Ameet S.\n\nTalwalkar. Leaf: A benchmark for federated settings. ArXiv, abs/1812.01097, 2018.\n\nClement Fung, Chris J. M. Yoon, and Ivan Beschastnikh. The limitations of federated learnIn 23rd International Symposium on Research in Attacks, Intrusions ing in sybil settings. and Defenses (RAID 2020), pp. 301–316, San Sebastian, October 2020. USENIX AssociISBN 978-1-939133-18-2. URL https://www.usenix.org/conference/ ation. raid2020/presentation/fung.\n\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016.\n\nAudun Jøsang, Roslan Ismail, and Colin Boyd. A survey of trust and reputation systems for online\n\nservice provision. Decis. Support Syst., 43:618–644, 2007.\n\nPeter Kairouz, H. B. McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary B. Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim Y. El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri`a Gasc ́on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Za ̈ıd Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecn ́y, Aleksandra Korolova, Farinaz Koushanfar, Oluwasanmi Koyejo, Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ̈Ozg ̈ur, R. Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Xiaodong Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. ArXiv, abs/1912.04977, 2021.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learnIn Advances in Neural Information Processing\n\ning: Training clean models on poisoned data. Systems, 2021a.\n\nYuezun Li, Y. Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 16443–16452, 2021b.\n\nYingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and X. Zhang.\n\nTrojaning attack on neural networks. In NDSS, 2018.\n\nG ́abor Lugosi and Shahar Mendelson. Robust multivariate mean estimation: The optimality of trimmed mean. The Annals of Statistics, 49(1):393 – 410, 2021. doi: 10.1214/20-AOS1961. URL https://doi.org/10.1214/20-AOS1961.\n\n10\n\nNaren Manoj and Avrim Blum.\n\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 20373–20384. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ aaebdb8bb6b0e73f6c3c54a0ab0c6415-Paper.pdf.\n\nExcess capacity and backdoor poisoning.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 1273–1282. PMLR, 20–22 Apr 2017. URL https://proceedings.mlr.press/v54/ mcmahan17a.html.\n\nThien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen Mollering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, Farinaz Koushanfar, Ahmad-Reza Sadeghi, and T. Schneider. Flame: Taming backdoors in federated learning. 2021.\n\nAshwinee Panda, Saeed Mahloujifar, Arjun Nitin Bhagoji, Supriyo Chakraborty, and Prateek Mittal. Sparsefed: Mitigating model poisoning attacks in federated learning with sparsifiIn Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera (eds.), Proceedings cation. of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 7587–7624. PMLR, 28–30 Mar 2022. URL https://proceedings.mlr.press/v151/panda22a.html.\n\nGiambattista Parascandolo, Alexander Neitz, ANTONIO ORVIETO, Luigi Gresele, and Bernhard Sch ̈olkopf. Learning explanations that are hard to vary. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=hb1sDDSLbV.\n\nJeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL https://aclanthology.org/D14-1162.\n\nKrishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. IEEE Transactions on Signal Processing, 70:1142–1154, 2022. doi: 10.1109/TSP.2022.3153135.\n\nElan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk minIn International Conference on Learning Representations, 2021. URL https:\n\nimization. //openreview.net/forum?id=BbNIbVPJ-42.\n\nVirat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing board: A critical evaluation of poisoning attacks on production federated learning. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1354–1371, 2022. doi: 10.1109/SP46214.2022. 9833647.\n\nMingjie Sun, Zichao Li, Chaowei Xiao, Haonan Qiu, Bhavya Kailkhura, Mingyan Liu, and Bo Li. Can shape structure features improve model robustness under diverse adversarial settings? 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 7506–7515, 2021.\n\nZiteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H. B. McMahan. Can you really backdoor\n\nfederated learning? ArXiv, abs/1911.07963, 2019.\n\nBrandon Tran, Jerry Li, and Aleksander Madry.\n\nSpectral signatures in backdoor attacks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ 280cf18baf4311c92aa5a042336587d3-Paper.pdf.\n\nHaoxiang Wang, Haozhe Si, Bo Li, and Han Zhao. Provable domain generalization via invariantfeature subspace recovery. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 23018–23033. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/wang22x.html.\n\n11\n\nHongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jyyong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 16070–16084. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf.\n\nEmily Wenger, Josephine Passananti, Arjun Nitin Bhagoji, Yuanshun Yao, Haitao Zheng, and Ben Y. Zhao. Backdoor attacks against deep learning systems in the physical world. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6202–6211, 2021.\n\nDongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= 4cEapqXfP30.\n\nChulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against In International Conference on Learning Representations, 2020a. URL\n\nfederated learning. https://openreview.net/forum?id=rkgyS0VFvr.\n\nChulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. Crfl: Certifiably robust federated learning against backdoor attacks. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11372–11382. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr. press/v139/xie21a.html.\n\nCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Slsgd: Secure and efficient distributed onIn Machine Learning and Knowledge Discovery in Databases, pp.\n\ndevice machine learning. 213–228, Cham, 2020b. Springer International Publishing. ISBN 978-3-030-46147-8.\n\nXiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang. Sparse invariant risk minimization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 27222–27244. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/zhou22e.html.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a new method to defend against backdoor attacks. The proposed method is basically a new aggregation rule. Edge-case backdoor attacks are considered, and multiple simple baselines are evaluated.\n\n# Strength And Weaknesses\n\nStrengths\n\n- FL is vulnerable to backdoor attacks\n\n- A new aggregation rule is proposed. \n\nWeaknesses\n\n- Many robust aggregation rules have been proposed. This paper's proposed aggregation is just some variant of existing one, e.g., trimmed-mean. Therefore, the paper's novelty is limited. \n\n- Seems like only edge-case backdoor attacks are evaluated. This represents a quite limited backdoor case. Other backdoor attacks should be evaluated. \n\n- Some simple baselines are evaluated. Strong and more recent defenses (appeared in AI, security, and system venues) should be evaluated. \n\n- Adaptive attacks are not considered.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee the above strength and weakness.\n\n# Summary Of The Review\n\nThe paper has limited novelty, and experiments are also limited.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nQAID: QUESTION ANSWERING INSPIRED FEW-SHOT INTENT DETECTION\n\n♢ ♣\n\n♢\n\n♢\n\n♢\n\nAsaf Yehudai IBM Israel Research Lab {first.last}@ibm.com, {yosimass, doronc, boazc}.il.ibm.com\n\n, Yosi Mass , Hebrew University of Jerusalem\n\n, Matan Vetzler ♢\n\n, Doron Cohen\n\n, Koren Lazar\n\n♣\n\n♢\n\n, Boaz Carmeli\n\n♢\n\nABSTRACT\n\nIntent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the finetuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.\n\n1\n\nINTRODUCTION\n\nIntent detection (ID) is the task of classifying an incoming user query to one class from a set of mutually-exclusive classes, a.k.a. intents (Wang et al., 2014; Schuurmans & Frasincar, 2019; Liu et al., 2019a). This ability is a cornerstone for task-oriented dialogue systems as correctly identifying the user intent at the beginning of an interaction is crucial to its success. However, labeled data is required for training and manual annotation is costly. This calls for sample efficient methods, gaining high accuracy with minimal amounts of labeled data.\n\nRecent works tackling few-shot ID have relied on large-scale pre-trained language models, such as BERT (Devlin et al., 2018). These works leverage task-adaptive training and focus on pre-training a model on a large open-domain dialogue corpus and fine-tuning it for ID classification (Mehri et al., 2020; Wu et al., 2020a; Casanueva et al., 2020; Zhang et al., 2021a).\n\nAlternative approaches tried to learn query representation based on query-to-query matching (henceforth, Match-QQ systems) (Zhang et al., 2020; Mass et al., 2020; Mehri et al., 2021). Zhang et al. (2020); Mass et al. (2020) adopt pairwise-encoding systems with cross-attention to deploy K-Nearest-Neighbor (K-NN) (Fix & Hodges, 1989) classification schema where training queries are fully utilized for both training and inference stages. Nevertheless, those methods’ downside is the processing time combined with the difficulty of scaling to large number of intents (Liu et al., 2021c).\n\nThe need to efficiently compare an incoming query to a large set of possible answers resides at the core of any question answering (QA) retrieval system (henceforth, Match-QA systems) (Karpukhin et al., 2020). Recently, Khattab & Zaharia (2020) introduced ColBERT, which allows faster training and inference by replacing the cross-attention mechanism used by Match-QQ systems (Zhang et al., 2020; Mass et al., 2020; Nogueira & Cho, 2019) with a fast contextualized token-level similarity mechanism dubbed late interaction.\n\nIn this work, we present a Question Answering inspired Intent Detection system, named QAID. We start by formulating the ID task as a question-answering retrieval task by treating the utterances and the intent names as queries and answers, respectively. This reformulation allows us to introduce valuable additional signal from the intent names. Then, we adapts the efficient architecture of ColBERT while replacing its triplet function loss with batch contrastive loss which was proven to be more robust (Khosla et al., 2020) and performs well in various tasks (Gunel et al., 2021; Gao et al., 2021a), including ID classification (Zhang et al., 2021b). In contrast to ColBERT which compares a query to a pair of positive and negative documents, we also include queries as positive examples,\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nand so we compare the queries both to their answers and to other queries from the same intent. This allows QAID to represent similarly both queries and answers of the same intent. Therefore, our training method assumes the settings of both Match-QQ and Match-QA. In inference, QAID relies on the token-level similarity (late interaction) mechanism between incoming query and all intent names for its predictions (Khattab & Zaharia, 2020).\n\nOur contribution is thus threefold. (1) We show that few-shot intent detection can be successfully handled by QA systems when letting the intent name play the role of the answer. (2) We show how intent detection architectures can benefit from recent advancements in supervised batch contrastive training and late-interaction scores. (3) We report state-of-the-art results on three few-shot intent detection benchmarks.\n\n2 METHOD\n\nOur method addresses the few-shot intent detection task, in which we have C defined intents and the task is to classify an incoming user query, q, into one of the C classes. In our formulation, upon getting a new user query q, we need to retrieve the most suited intent name. We set balanced K-shot learning for each intent (Mehri et al., 2020; Casanueva et al., 2020; Zhang et al., 2020), i.e., the training data containing K examples per intent1.\n\nIn the following section, we describe the structure of our QAID framework and its training stages. First, in Section 2.1 we elaborate on the different components of QAID. Then, in Section 2.2 we present the two training stages: the self-supervised contrastive pre-training in 2.2.1 and the supervised batch contrastive fine-tuning in 2.2.2. Lastly, in Section 2.3 we briefly touch on our decision to formulate ID as a question retrieval task.\n\n2.1 REPRESENTATION LEARNING FRAMEWORK\n\nThe main components of our framework are:\n\n• Data Augmentation module, Aug(⋅). For each input query, q, we generate two random augmentations, ˆq = Aug(q), each of which represents a different view of the input, q. For our augmentation we use the combination of two simple and intuitive ‘corruption’ techniques (Gao et al., 2021a; Wu et al., 2020b; Liu et al., 2021b); (i) randomly masking tokens from q (Devlin et al., 2018); (ii) dropping a small subset of neurons and representation dimensions. Technique (i) is done before passing the query to the encoder and technique (ii) is done in the forward propagation through the encoder model.\n\n• Encoder model, Enc(⋅), which maps a query q, consisting q1, ..., qm tokens, to Enc(q) ∈ Rm×DE , where DE is the embedding dimension. In our experiments, it is either 768 or 1024. • Projection layer, P roj(⋅), a single linear layer that maps vectors of dimension DE to vectors of\n\ndimension DP = 128, followed by normalization to the unit hypersphere.\n\n• Token-level score, Score(⋅, ⋅), given two queries u = (u1, ..., um) and v = (v1, ..., vl), the relevance score of u regarding to v, denoted by Score(u, v), is calculated by the late interaction between their bags of projected contextualized representations, i.e z(u) = P roj(Enc(u)). Namely, the sum of the maximum token-wise cosine similarity of their projected representations (Khattab & Zaharia, 2020). Equation 1 shows the formulation of this score.\n\nScore(u, v) = ∑ i∈[m]\n\nmax j∈[l]\n\nz(u)i ⋅ z(v)j\n\n(1)\n\n2.2 TWO-STAGE CONTRASTIVE TRAINING\n\nIn both stages, given a batch of input samples Q = (q1, .., qn ), we first apply Aug followed by the encoding and projection layer, denoted by the z(⋅) function as described in the last section, and so we have XQ = z(Aug(Q)) ∈ R2n×DP , where the 2n is a result of the two random augmentations we applied to each query. In the self-supervised training, each two augmented queries are the only\n\n1In the rest of the paper we refer to the intent examples as queries and use intents and classes interchangeably\n\n2\n\nPublished as a conference paper at ICLR 2023\n\npositive examples for each other while all other queries are negative. In the supervised training phase, we also run the same process with the same encoder on the corresponding intent names, ), resulting in XA = z(Aug(A)) ∈ R2n×DP . XA together with XQ forms a A = (a1, ..., an training batch of 4n instances. In this supervised setting, all queries and intent names of the same intent are positive to each other while all others are negative.\n\n2.2.1 PRE-TRAINING\n\nWe use a task-adaptive pre-training stage to overcome the few-shot constraint, as done by most goaloriented dialogue works (Mehri et al., 2020; Zhang et al., 2020). Our pre-training aims to facilitate domain adaptation by two early-stage objectives: (1) Incorporate token-level domain knowledge into the model (Mehri et al., 2020; 2021); (2) Adopt queries’ representations to the dialog domain through data augmentation techniques and self-supervised contrastive training (Zhang et al., 2021b; 2022a).\n\nPractically, in a training batch containing 2n augmented queries, let t ∈ [2n] be the index of an arbitrary augmented query. Then in the self-supervised contrastive learning stage, the loss takes the following form:\n\nLself = − ∑\n\nlog\n\nt∈[2n]\n\nexp(Score(qt, qJ(t))/τ ) a∈A(t) exp(Score(qt, qa)/τ )\n\n∑\n\n(2)\n\nwhere t is called the anchor/pivot, J(t) is the index of the second augmented sample deriving from the same source sample (a.k.a positive), A(t) = {[2n] \\ t}, A(t) \\ J(t) are called the negative and τ ∈ R is a scalar temperature parameter that controls the penalty to negative queries (see step (a) in Figure 1).\n\n+\n\nMasked language modeling as an auxiliary loss: In addition to the self-supervised contrastive training, we pre-train also on the masked language modeling (MLM) task (Taylor, 1953), to further adjust sentence-level representation to the domain of the data. Moreover, this improves lexical-level representation which is essential for token-level similarity. Hence we define Lmlm as the average cross-entropy loss over all masked tokens.\n\nThe overall loss for the pre-training phase is LP T = Lself + λLmlm, where λ is a controllable hyperparameter.\n\n2.2.2 FINE-TUNING\n\nAt the fine-tuning stage, we only have a limited number of examples for each intent, and intents may be semantically similar, making the classification task difficult. To address the data scarcity, we utilize the explicit intent names as unique examples that serve as answers in our QA retrieval framework. This is similar to recent works that leverage label names for zero-shot or few-shot text classification (Meng et al., 2020; Basile et al., 2021). The intent name is usually assigned by a domain expert when designing a goal-oriented dialogue system. As such, it provides a semantic description of the intent that aims to discriminate it from other intents. Consequently, intent names may provide a signal that is otherwise difficult to extract from a small set of queries.\n\nAdditionally, we utilize the pre-trained model designed for queries’ representations and continue fine-tuning it on the few-shot examples with the permanent representation learning technique (Khosla et al., 2020) of supervised batch contrastive training (see step (b) in Fig. 1). In that way, each time our model pulls together two queries from the same class it simultaneously also pulls their intent names closer together as well. Formally our supervised contrastive loss has the form:\n\nLsup = ∑\n\nt∈[4n]\n\n−1 ∣P (t)∣\n\n∑\n\nlog\n\np∈P (t)\n\nexp(Score(qt, qp\n\n)/τ )\n\n∑\n\na∈A(t) exp(Score(qt, qa)/τ )\n\n(3)\n\nIn this formulation q may represent either an augmented query or its intent name, and since each instance has four views: two augmented queries and two augmented intent names, we have a total\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Schematic illustration of our method. Hat and bar represent augmentation, the subscript is a ruining index, superscript is the class index. (a) Self-supervised contrastive training with data augmentation to enhance quires’ representations. (b) Supervise contrastive fine-tuning to learn queryto-query and query-to-answer similarity. (c) Indexing the answers into Faiss index. (d) Compare incoming query against all answers in the index and predict the most similar one.\n\nof 4n samples. A(t) = {[4n] \\ t}; P (t) is the group of all samples that are positive to qt and is defined as all the augmented queries or intent names derived from the same label as qt.\n\nBesides the supervised contrastive loss we also train with a classification loss, Lclss, and an MLM loss, Lmlm. In total, the fine-tuning loss is LF T = Lsup + λclassLclass + λmlmLmlm, where λclass and λmlm are controllable hyperparameters.\n\nIndexing and Inference. After fine-tuning, we index the embeddings of all candidate answers (a.k.a intent names) outputted from the encoder into Faiss (Johnson et al., 2021), a library for large-scale vector-similarity search. see Khattab & Zaharia (2020) for more details. Then, at inference time, we compare the incoming query representation with all answers in the index and retrieve the most similar (see step (c) and (d) in Fig. 1).\n\n2.3 WHY TO FORMULATE THE TASK AS A QUESTION ANSWERING RETRIEVAL TASK?\n\nFor a few-shot intent detection model to be practical, it must be computationally efficient in both training and inference phases, while possibly handling a large number of intents (Qi et al., 2020). Here we are leveraging the recent success of dense passage retrieval for question answering that was shown to perform well and efficiently (Karpukhin et al., 2020; Khattab & Zaharia, 2020). Those systems can handle a large number of candidate answers by using (1) a dual-encoder framework with light comparison instead of the computationally demanding pairwise-encoding, (2) an indexation that enables a large-scale fast retrieval.\n\n3 EXPERIMENTAL SETUP\n\n3.1 DATASETS\n\nWe experiment with three widely studied few-shot intent detection datasets which represent the intent detection (ID) part of DialoGLUE benchmark.2 These datasets present challenging few-shot ID tasks with fine-grained intents that are semantically similar. Moreover, they facilitate the comparison with recent state-of-the-art baselines.\n\nClinc150 (Larson et al., 2019) contains 22,500 personal assistance queries classified into 150 intents across 10 domains. Banking77 (Casanueva et al., 2020) contains 13,242 online banking queries classified into 77 finegrained intents in a single domain.\n\n2For readily use: https://github.com/jianguoz/Few-Shot-Intent-Detection/tree/main/Datasets\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nHWU64 (Liu et al., 2019b) contains 11,106 personal assistant queries classified into 64 intents across 21 different domains. Table 1 reports the data splits of each dataset.\n\nDataset\n\n#Train\n\n#Vaild\n\n#Test\n\n#Intents\n\n#Domains\n\nCLINC150 BANKING77 HWU64\n\n15000 8622 8954\n\n3000 1540 1076\n\n4500 3080 1076\n\n150 77 64\n\n10 1\n21\n\nTable 1: Data statistics of the three intent detection datasets from DialoGLUE.\n\n3.2 MODELS AND BASELINES\n\nWe experiment with RoBERTa-base and RoBERTa-large encoders from the Hugging Face transformers library3 with ColBERT architecture.4 In the self-supervised pre-training stage, we utilize the training and validation sets of the six ID datasets from Zhang et al. (2021b); Mehri et al. (2020; 2021). For a fair evaluation, we exclude the test sets from our pre-training following the observation of Zhang et al. (2021b). We fix the number of embeddings per query at m = 32 same as Khattab & Zaharia (2020). For the MLM loss, we follow the masking strategy of Devlin et al. (2018). We also use this masking strategy to augment our input queries in addition to the representation-level augmentation through the encoder build in 10% dropout. For the contrastive loss, we use the implementation of Khosla et al. (2020)5. For the late interaction score, we normalize the score by the number of tokens in the summation. We train our encoder for 20 epochs with a batch size of 64, a −5, a temperature parameter τ of 0.07 (same as Khosla et al. (2020)) and λ = 0.1 learning rate of 1e as recommended in the literature.\n\nFor the fine-tuning stage, we train our models on 5- and 10-shot splits as available within the public dataset distribution. We train our model for 10 epochs starting from the pre-trained model checkpoint. We set the batch size to 32, where queries and answers are encoded separately by the same encoder. Answers are truncated by the longest one in the batch same as Khattab & Zaharia (2020). We apply the same masking schema to queries and answers as done in the pre-training. We set the temperature to 0.07. We also set λclass and λmlm to 0.1 and 0.05, respectively, making Lsup the main summand in LF T . We used those parameters as they were recommended in the literature and shown to perform best in hyperparameter tuning. Following previous works, we ran the experiments with five different seeds and report the average accuracy.\n\nFor our Faiss Index6 implementation, we use IVFScalarQuantizer (“InVerted File with Scalar Quantizer.”). To improve memory efficiency, every embedding is represented by 8 bytes. In our work, we use the full retrieval option that effectively retrieves all candidate answers. In cases of many intents, one can deploy fast retrieval of top candidate answers.\n\n3.3 BASELINES\n\nWe start by categorizing the baselines by three main representation and prediction methodologies. A Classifier architecture learns both query representation and a linear classification head via crossentropy loss. The classification head contains a single vector per class that is implicitly trained to represent the class and enable prediction at inference time. A Match-QQ architecture learns query representation via query-to-query similarity matching as it learns to increase the similarity between embeddings of queries from the same class while simultaneously decreasing the similarity between queries from disparate classes. During inference, an input query is matched against a large collection of training queries, and the intent in which its queries are the most similar to the input is predicted. A Match-QA architecture learns to match queries to answers. The model learns to increase the similarity between queries and their corresponding answers while simultaneously decreasing the similarity\n\n3https://github.com/huggingface/transformers 4https://github.com/stanford-futuredata/ColBERT/tree/colbertv1 5https://github.com/HobbitLong/SupContrast/blob/master/losses.py 6https://github.com/facebookresearch/faiss\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nbetween queries and other answers. At inference time, an incoming query is matched against all possible answers, and the most similar is predicted. In these terms, the pretraining of QAID is based on Match-QQ, its fine-tuning involves both Match-QQ and Match-QA, and its prediction is based on Match-QA. We experimented with prediction methods that include also Match-QQ, but found it preform slightly worse than inference with only Match-QA. We will elaborate more in Section §4.1.\n\n3.3.1 BASELINE MODELS\n\nWe compare our approach and results against strong baseline models reported in the literature. In the rest of the section, we discuss these models in more detail and align them with the paradigms mentioned above. Notably, some baseline models mix and match components across architectures.\n\n• Classifier: We fine-tune RoBERTa-base encoder with a feed-forward classification head as our\n\nclassifier baseline.\n\n• ColBERT (Khattab & Zaharia, 2020): Contextualized Late Interaction over BERT (ColBERT) is a state-of-the-art passage search and retrieval system. ColBERT provides a Match-QA baseline and is the basis for the QAID architecture. For training we use 20 triplets (query, pos answer, neg answer) for each query, with hard negatives, namely, we run the query against all answers using bm25 (Robertson & Zaragoza, 2009) and select the negatives from the most similar answers. • USE+CONVERT (Casanueva et al., 2020): USE (Yang et al., 2019) is a large multilingual dualencoder model pre-trained in 16 languages. CONVERT (Casanueva et al., 2020) is an intent detection model with dual encoders that are pre-trained on 654 million (input, response) pairs from Reddit.\n\n• CONVEBERT (Mehri et al., 2020): a BERT-base model which has been trained on a large opendomain dialogue corpus. CONVEBERT achieved improvements over a vanilla BERT architecture and state-of-the-art results on a few task-oriented dialogue tasks.\n\n• CONVEBERT+Combined (Mehri et al., 2021): a CONVEBERT-base model trained to improve similarity matching of training examples, i.e., Match-QQ. Additionally, the model trains with observers for transformer attention and conducts task-adaptive self-supervised learning with mask language modeling (MLM) on the intent detection datasets. Combined represents the best MLM+Example+Observers setting in the referenced paper.\n\n• DNNC (Zhang et al., 2020): Discriminative Nearest Neighbor Classification (DNNC) model is trained to find the best-matched example from the training set through similarity matching (Match-QQ). The model conducts data augmentation during training and boosts performance by pre-training on three natural language inference tasks.\n\n• CPFT (Zhang et al., 2021b): Contrastive Pre-training and Fine-Tuning (CPFT) is a two-stage intent-detection architecture. During the first stage, the model learns with a self-supervised contrastive loss on a large set of unlabeled queries. In the second stage, the model learns with supervised contrastive loss to pull together query representation from the same intent (Match-QQ). The inference is done via a classification head that is added and trained during the second stage.\n\n4 RESULTS\n\nTable 2 lists the results on the three datasets described in Section 3.1. QAID with RoBERTa-base achieved the best results across all datasets and shots. Notably, increasing the model size from RoBERTa-base to RoBERTa-large resulted in additional significant improvement across all datasets. For 5-shot, QAID with RoBERTa-base improves over CPFT, which achieved the best results reported so far, by more than 4 points on the BANKING77 dataset which is translated to 30.64% in error rate reduction (ERR). Similarly, QAID achieves ERR of 8.5% and 18.9% over CPFT for the CLINC150 and HWU64 respectively. We attribute our improvement to three key differences between our method and CPFT. (1) Our problem formulation kept the class representation the same during training and inference. In other words, we didn’t train a classification layer for inference. (2) Incorporating answers as data points contributes an additional discriminating signal. (3) The token-level late interaction score is shown to perform better as our ablation experiments demonstrate in Section 4.1. Moreover, our standard deviations (std) are consistently lower than those of DNNC and CPFT with the highest std of 0.15 and average std of 0.07. We believe the reason for\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nClassifier (RoBERTa-base) ColBERT USE+CONVERT (Casanueva et al., 2020) CONVBERT (Mehri et al., 2020) CONVBERT + Combined (Mehri et al., 2021) DNNC (Zhang et al., 2020) CPFT (Zhang et al., 2021b)\n\nQAID (RoBERTa-base)\n\nQAID (RoBERTa-large)\n\nCLINC150\n\nBANKING77\n\nHWU64\n\n5\n\n87.68 82.03 90.49 -\n- 91.02 92.34\n\n10\n\n91.22 88.10 93.26 92.10 93.97 93.76 94.18\n\n5\n\n74.46 72.71 77.75 -\n- 80.40 80.86\n\n10\n\n83.79 79.25 85.19 83.63 85.95 86.71 87.20\n\n5\n\n73.52 74.98 80.01 -\n- 80.46 82.03\n\n10\n\n82.62 81.78 85.83 83.77 86.28 84.72 87.13\n\n93.41\n\n94.64\n\n85.25\n\n88.83\n\n85.52\n\n87.98\n\n94.95\n\n95.71\n\n87.30\n\n89.41\n\n87.82\n\n90.42\n\nTable 2: Accuracy results on three ID datasets in 5-shot and 10-shot settings. Baseline results are from the original papers except for RoBERTa Classifier and ColBERT.\n\nthe low std in the results is the combination of batch contrastive loss with data augmentation and the fine-grained late-interaction similarity score. Accordingly, our improvements are significant ac- −4. An additional advantage of our method is its cording to an unpaired t-test with a p-value of 1e efficiency. QAID pre-training run-time takes about two hours and it has to run only once for all of our targets. Our fine-tuning takes only ten minutes on one NVIDIA V100 GPU, compared to three hours of fine-tuning of DNNC. Another important aspect of our results is the effect of scaling from RoBERTa-base to RoBERTa-large, which resulted in significant improvements in both 5 and 10shot scenarios across all datasets, aligned with results showing larger models generalize better from small data (Bandel et al., 2022). Moreover, in some cases scaling the model was more beneficial than additional examples. Namely, RoBERTa-large in 5-shot surpasses RoBERTa-base in 10-shot.\n\n4.1 ABLATION TESTS\n\nIn this section, we describe several ablation studies demonstrating the importance of our method components and the main factors that contribute to our improvement over ColBERT.\n\nWe present our ablation results in Table 3. We start by analyzing the improving effect of the pretraining (PT) and batch contrastive training on ColBERT. We can see that both stages boost the performances considerably across all settings. It is noticeable that the pre-training (row ColBERT with PT) improves more in the 5-shot than in the 10-shot setting with deltas of 4.59 and 2.00 points on average, respectively. This result is consistent with the observation that a model with better query representation is essential when only a few examples are available (Choshen et al., 2022). Batch contrastive training (row ColBERT with batch contrastive) improves performance in most settings, with an average improvement of 5.74 points over ColBERT. We attribute this improvement to two major enhancements that batch contrastive training introduces. The first is the shift from a model that learns only Match-QA to a model that learns both Match-QA and Match-QQ. The second is the improved technique of batch contrastive loss over triplet loss that allow to process many positive and negative examples at once and intrinsic ability to perform hard positive/negative mining (Khosla et al., 2020). We note that this change has a minor effect on the training time as it relies on representations calculated in the batch and has no effect on the inference time.\n\nIn addition, we study some modifications of QAID to understand their effect. To further investigate the effect of batch contrastive we train QAID with N-pairs loss (row QAID - N-pairs loss) with in-batch negatives, a widely used loss in retrieval models, e.g. DPR and RocketQA (Karpukhin et al., 2020; Qu et al., 2021). In this setting, each query has one positive example, which in our case is the intent name, and multiple irrelevant (negative) examples, either different queries or intent names. This method differs from our supervised batch contrastive loss which allows many positive examples. Our results show that replacing QAID supervised batch contrastive loss with N-pairs loss leads to a decrease of more than 1 point on average. These results support our claim that retrieval models can benefit from adopting supervised batch contrastive loss. When we conducted fine-tuning training with and without auxiliary tasks (MLM and classification losses), we found\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nthat the auxiliary tasks increased QAID accuracy by 0.43 on average. Interestingly, the increase was more pronounced as the dataset contained fewer domains, 0.72, 0.48, and 0.10 of average improvement on Banking77, Clinc150, and HWU64, respectively. We also examine the performance of our method without pre-training, row QAID w/o PT. Results indicate that our method achieves an average improvement of 3.53 points compared to CPFT without pre-training (CPFT w/o PT) in Table 3. This result emphasizes the superiority of our proposed method as a fine-tuning method. Additionally, to better understand the role of the data augmentation module in our training, we conduct an experiment where we did not apply the data augmentation module. Results, QAID w/o data augmentation, show that by augmenting the data we improve the results by about a third of a point on average. In the 5-shot setting, the improvement is about 0.75 points on average, and in the 10-shot setting, the effect is inconsistent. Those results can indicate that data augmentation is more beneficial where less data is available.\n\nWe experiment with replacing the similarity score in QAID with the cosine similarity of the CLS token (row QAID - Cosine Similarity) instead of the token-level late interaction (Khattab & Zaharia, 2020) (row QAID). We can see that using the token-level late interaction achieves higher results across all datasets and shots. We ascribe this improvement to the fine-grained nature of the late-interaction score that enables detailed token-level comparison. This score presents an efficient alternative to the costly cross-attention scoring that most Match-QQ methods use.\n\nFinally, we discuss our inference method. We experiment with indexing and predicting based on both queries and answers, i.e., using Match-QQ and Match-QA in the inference stage as we do in training. Inference based only on Match-QA achieve slightly better (0.07) results on average, with an average improvement of 0.18 and 0.17 on Banking77 and Clinc150, respectively, and an average decrease of 0.14 on HWU64. These results indicate that our training method achieves answer representation that reflects the distribution of both training queries and answers. Therefore, allowing a more efficient inference that relies only on the answers’ representations.\n\nModel\n\nColBERT ColBERT with PT ColBERT with batch contrastive\n\nCLINC150\n\nBANKING77\n\nHWU64\n\n5\n\n82.03 89.31 89.92\n\n10\n\n88.10 90.85 93.17\n\n5\n\n72.71 75.73 81.41\n\n10\n\n79.25 80.42 86.68\n\n5\n\n74.98 81.20 80.36\n\n10\n\n81.78 84.88 85.54\n\nCPFT w/o PT (Zhang et al., 2021b)\n\n88.19\n\n91.55\n\n76.75\n\n84.83\n\n76.02\n\n82.96\n\nQAID - N-pairs loss QAID w/o PT QAID w/o data augmentation QAID - cosine similarity QAID\n\n92.38 90.52 93.21 92.73 93.41\n\n93.70 93.26 94.64 93.71 94.64\n\n84.47 81.61 84.34 84.11 85.25\n\n87.25 86.20 88.46 87.24 88.83\n\n84.19 80.45 84.37 84.04 85.52\n\n87.13 86.48 88.46 87.70 87.98\n\nTable 3: Accuracy results of our ablation experiments.\n\n5 RELATED WORK\n\n5.1 FEW-SHOT INTENT DETECTION\n\nTask adaptive pre-training is a common strategy to face the data scarcity problem in few-shot intent detection classification. Predominant approach for task adaptive pre-training models leverages selfsupervised mask language modeling training on large dialogues datasets (a few hundred million dialogues) and on the domain itself to tackle few-shot intent detection (Casanueva et al., 2020; Mehri et al., 2020; 2021). Shnarch et al. (2022) showed that unsupervised clustering helps better than MLM pre-training. DNNC (Zhang et al., 2020) pre-trains their system on annotated pairs from natural language inference (NLI) leveraging BERT (Devlin et al., 2018) pairwise encoding. Then they model intent detection as a kNN problem with k = 1 where the pre-trained NLI model learns to predict the similarity score for pair of queries. However, this model is computationally expensive as it fully utilized training examples in both training and inference.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nThe CPFT work, suggested by Zhang et al. (2021b), shares some similarities with our approach. when the two main ones are: the two-stage training process and the use of batch contrastive loss. However, we have several key differences where we extend upon their method. Those differences make our method more effective and efficient, especially when the task involves a large number of intents. Firstly, we reformulate the few-shot ID classification task as a retrieval task. To that end, we adopt an efficient Dual-Encoder-based retrieval architecture - ColBERT (Khattab & Zaharia, 2020), and a late-interaction similarity score. Moreover, we treat the intent names as the answers we wish to retrieve. Secondly, we adjust the training method to learn both Match-QQ and Match-QA similarities. Finally, we adopt a retrieval-based inference based on the similarity between the incoming query and the intent names, therefore we are not required to train an additional classification head.\n\nZhang et al. (2022b) design two stage training with batch contrastive loss and add explicit regularization loss directing the feature space towards isotropy. They report high 5-way few-shot results on the same benchmarks we use. Nevertheless, when evaluating their model accuracy the results are much lower than ours (about 16% and 9% lower on Banking77 and HWU64 respectively).\n\n5.2\n\nINTENT NAME\n\nThe idea of exploiting class names was proposed in the setting of zero and few-shot classification by a few past works (Meng et al., 2020; Yin et al., 2019; Basile et al., 2021). Yin et al. (2019) propose to formulate text classification tasks as a textual entailment problem (Dagan et al., 2005). This mapping enables using a model trained on natural language inference (NLI) as a zero-shot text classifier for a wide variety of unseen downstream tasks (Gera et al., 2022). Zhong et al. (2021) map the classification tasks to a question-answering format, where each class is formulated as a question and given as a prompt, and the decoder probabilities of the “Yes” and “No” tokens correspond to a positive or negative prediction of the class.\n\nIn our work, we cast the classification problem to the task of question answering retrieval and treat a much larger number of classes than these works tackle, which is usually up to twenty.\n\n5.3 BATCH CONTRASTIVE LEARNING\n\nBatch contrastive training was shown to achieve improved representation and perform better than contrastive losses such as triplet, max-margin, and the N-pairs loss (Khosla et al., 2020). Gunel et al. (2021); Gao et al. (2021b) suggest incorporating batch contrastive learning to train the encoder in natural language processing tasks. Gao et al. (2021b) designed a simple contrastive learning framework through dropout augmentation. They trained on NLI data to achieve state-of-the-art results on unsupervised and full-shot supervised semantic textual similarity (STS) tasks (Agirre et al., 2012; 2015; 2016). Liu et al. (2021a) suggest MirrorBERT, a self-supervised framework with two types of random data augmentation: randomly erase or mask parts of the texts during tokenization, and representation-level augmentation through built-in encoder dropout. We differ from those works as we target the few-shot intent detection task. Moreover, we adjust the late interaction score from Khattab & Zaharia (2020) to achieve cross-attention-like similarity scores. We also showed that class names can serve as an additional augmentation that can be the base for inference prediction.\n\n6 CONCLUSIONS\n\nIn this paper, we present QAID, Question Answering inspired Intent Detection system, that models the few-shot ID classification as a question-answering retrieval task, where utterances serve as questions and intent names as answers. We train QAID with a two-stage training schema with batch contrastive loss. Results show that replacing ColBERT triplet loss with batch contrastive loss leads to a considerable improvement. We assert that a contributing factor to this effect is the shift to learning Match-QQ and Match-QA representations. We leave for further research to investigate this effect on retrieval tasks. Moreover, our results show that incorporating token-level similarity scores in contrastive loss outperforms the common cosine similarity score without a notable increase in training time. We encourage future research to utilize this type of contrastive loss in other tasks and investigate its effect. Finally, our results on three few-shot ID benchmarks show that QAID achieves state-of-the-art performance.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe thank Leshem Choshen and Ariel Gera for their helpful feedback as we pursued this research.\n\nREFERENCES\n\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 385–393, Montr ́eal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1051.\n\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I ̃nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot In Proceedings of the 9th International Workshop on Semantic Evaluation on interpretability. (SemEval 2015), pp. 252–263, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2045. URL https://aclanthology.org/S15-2045.\n\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual In Proceedings of the 10th International Workshop on Semantic and cross-lingual evaluation. Evaluation (SemEval-2016), pp. 497–511, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL https://aclanthology. org/S16-1081.\n\nElron Bandel, Yoav Goldberg, and Yanai Elazar. Lexical generalization improves with larger models and longer training. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 4398–4410, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.findings-emnlp.323.\n\nAngelo Basile, Guillermo P ́erez-Torr ́o, and Marc Franco-Salvador. Probabilistic ensembles of zero-\n\nand few-shot learning models for emotion classification. In RANLP, 2021.\n\nI ̃nigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli ́c. Efficient\n\nintent detection with dual sentence encoders. arXiv preprint arXiv:2003.04807, 2020.\n\nLeshem Choshen, Elad Venezian, Shachar Don-Yehia, Noam Slonim, and Yoav Katz. Where to start? analyzing the potential value of intermediate models. arXiv preprint arXiv:2211.00107, 2022.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\n\nchallenge. In Machine learning challenges workshop, pp. 177–190. Springer, 2005.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nEvelyn Fix and Joseph Lawson Hodges. Discriminatory analysis. nonparametric discrimination: Consistency properties. International Statistical Review/Revue Internationale de Statistique, 57 (3):238–247, 1989.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\n\nembeddings. ArXiv, abs/2104.08821, 2021a.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\n\nembeddings. arXiv preprint arXiv:2104.08821, 2021b.\n\nAriel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz, Liat Ein-Dor, and Noam Slonim. Zero-shot text classification with self-training. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1107–1119, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.emnlp-main.73.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learning for\n\npre-trained language model fine-tuning. ArXiv, abs/2011.01403, 2021.\n\nJeff Johnson, Matthijs Douze, and Herv ́e J ́egou. Billion-scale similarity search with gpus. IEEE\n\nTransactions on Big Data, 7(3):535–547, 2021. doi: 10.1109/TBDATA.2019.2921572.\n\nVladimir Karpukhin, Barlas O ̆guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n\nOmar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pp. 39–48, 2020.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020.\n\nStefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K Kummerfeld, Kevin Leach, Michael A Laurenzano, Lingjia Tang, et al. An evaluation dataset for intent classification and out-of-scope prediction. arXiv preprint arXiv:1909.02027, 2019.\n\nFangyu Liu, Ivan Vulic, Anna Korhonen, and Nigel Collier. Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders. In EMNLP, 2021a.\n\nFangyu Liu, Ivan Vulic, Anna Korhonen, and Nigel Collier. Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders. In EMNLP, 2021b.\n\nJiao Liu, Yanling Li, and Min Lin. Review of intent detection methods in the human-machine dialogue system. In Journal of Physics: Conference Series, volume 1267, pp. 012059. IOP Publishing, 2019a.\n\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser.\n\nlanguage understanding services for building conversational agents.\n\nBenchmarking natarXiv preprint\n\nural arXiv:1903.05566, 2019b.\n\nZhiwei Liu, Ziwei Fan, Yu Wang, and Philip S. Yu. Augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021c.\n\nYosi Mass, Boaz Carmeli, Haggai Roitman, and David Konopnicki. Unsupervised faq retrieval with question generation and bert. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 807–812, 2020.\n\nShikib Mehri, Mihail Eric, and Dilek Hakkani-Tur. Dialoglue: A natural language understanding\n\nbenchmark for task-oriented dialogue. arXiv preprint arXiv:2009.13570, 2020.\n\nShikib Mehri, Mihail Eric, and Dilek Z. Hakkani-T ̈ur. Example-driven intent prediction with ob-\n\nservers. In NAACL, 2021.\n\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. Text classification using label names only: A language model self-training approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9006– 9017, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.emnlp-main.724. URL https://aclanthology.org/2020.emnlp-main.724.\n\nRodrigo Nogueira and Kyunghyun Cho.\n\nPassage re-ranking with bert.\n\narXiv preprint\n\narXiv:1901.04085, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nHaode Qi, Lin Pan, Atin Sood, Abhishek Shah, Ladislav Kunc, Mo Yu, and Saloni Potdar. Benchmarking commercial intent detection services with practice-driven evaluations. arXiv preprint arXiv:2012.03929, 2020.\n\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5835–5847, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.466. URL https://aclanthology.org/2021.naacl-main.466.\n\nStephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389, 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL http://dx.doi.org/10.1561/1500000019.\n\nJetze Schuurmans and Flavius Frasincar. Intent classification for dialogue utterances. IEEE Intelli-\n\ngent Systems, 35(1):82–88, 2019.\n\nEyal Shnarch, Ariel Gera, Alon Halfon, Lena Dankin, Leshem Choshen, Ranit Aharonov, and Noam In Proceedings Slonim. Cluster & tune: Boost cold start performance in text classification. of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7639–7653, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.526. URL https://aclanthology.org/2022. acl-long.526.\n\nWilson L Taylor. “cloze procedure”: A new tool for measuring readability. Journalism quarterly,\n\n30(4):415–433, 1953.\n\nFang Wang, Zhongyuan Wang, Zhoujun Li, and Ji-Rong Wen. Concept-based short text classification and ranking. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pp. 1069–1078, 2014.\n\nChien-Sheng Wu, Steven C.H. Hoi, Richard Socher, and Caiming Xiong. TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 917–929, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.66. URL https://aclanthology.org/2020.emnlp-main.66.\n\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive\n\nlearning for sentence representation. ArXiv, abs/2012.15466, 2020b.\n\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, et al. Multilingual universal sentence encoder for semantic retrieval. arXiv preprint arXiv:1907.04307, 2019.\n\nWenpeng Yin, Jamaal Hay, and Dan Roth. Benchmarking zero-shot text classification: Datasets, In Proceedings of the 2019 Conference on Empirical evaluation and entailment approach. Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3914–3923, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1404. URL https: //aclanthology.org/D19-1404.\n\nHaode Zhang, Yuwei Zhang, Li-Ming Zhan, Jiaxin Chen, Guangyuan Shi, Xiao-Ming Wu, and Albert Lam. Effectiveness of pre-training for few-shot intent classification. arXiv preprint arXiv:2109.05782, 2021a.\n\nHaode Zhang, Haowen Liang, Yuwei Zhang, Li-Ming Zhan, Xiao-Ming Wu, Xiaolei Lu, and Albert Y. S. Lam. Fine-tuning pre-trained language models for few-shot intent detection: Supervised pre-training and isotropization. In NAACL, 2022a.\n\nHaode Zhang, Haowen Liang, Yuwei Zhang, Liming Zhan, Xiao-Ming Wu, Xiaolei Lu, and Albert Y. S. Lam. Fine-tuning pre-trained language models for few-shot intent detection: Supervised pre-training and isotropization, 2022b. URL https://arxiv.org/abs/2205.07208.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJian-Guo Zhang, Kazuma Hashimoto, Wenhao Liu, Chien-Sheng Wu, Yao Wan, Philip S Yu, Richard Socher, and Caiming Xiong. Discriminative nearest neighbor few-shot intent detection by transferring natural language inference. arXiv preprint arXiv:2010.13009, 2020.\n\nJianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang, and Philip Yu. Few-shot intent detection via contrastive pre-training and fine-tuning. arXiv preprint arXiv:2109.06349, 2021b.\n\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zeroIn Findings of the Associashot learning by meta-tuning on dataset and prompt collections. tion for Computational Linguistics: EMNLP 2021, pp. 2856–2878, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.244. URL https://aclanthology.org/2021.findings-emnlp. 244.\n\n13",
    "reference": "# Summary Of The Paper\n\nThe paper proposes QAID, Question Answering inspired Intent Detection system, which models the intention detection classification as a question-answering task. The model uses two stages of training: a pretraining for better query representation and finetuning on few-shot labels of query and answers (name of intents). Certain choices of model, such as token-level similarity, batched contrastive learning, and inference with answers only, are verified by detailed ablation studies. The proposed method achieves SOTA  performance on three intention detection dataset from DialoGLUE.\n\n# Strength And Weaknesses\n\nStrength: \n\n- The paper uses and adapts techniques from QA and answer retrieval to the task of ID. The method is effective and efficient on three benchmark datasets when compared with various baselines.\n- The detailed ablation study verifies the necessity of stages and component of proposed framework.\n- The paper is well-written and easy to follow.\n\nWeakness:\n- Given the number of intents being only a few hundreds at most, is Faiss really necessary for inference?\n- n has two meanings in section 2.1: one for batch size and another for number of tokens.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper is clearly written.\nQuality: The statements are well supported. The empirical results are solid.\nNovelty: The paper adapts the method from question answering to intent detection. The adaptation itself is novel although the proposed methods are a combination of existing techniques.\nReproducibility: It requires some effort to reproduce the results.\n\n# Summary Of The Review\n\nThe paper is inspired by the development of ColBERT in QA task and propose a variant of the model for the task of intent detection. The proposed model makes several adaptations including batch contrastive loss and  signal from the intent names. The proposed method achieves SOTA performance on three few-shot ID benchmarks. And the ablation study proves the necessity of various components in the system.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMULTI-TREATMENT EFFECT ESTIMATION WITH PROXY: CONTRASTIVE LEARNING AND RANK WEIGHTING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study the treatment effect estimation problem for continuous and multidimensional treatments, in the setting with unobserved confounders, but highdimension proxy variables for unobserved confounders are available. Existing methods either directly adjust the relationship between observed covariates and treatments or recover the hidden confounders by probabilistic models. However, they either rely on a correctly specified treatment assignment model or require strong prior of the unobserved confounder distribution. To relax these requirements, we propose a Contrastive regularizer (Cr) to learn the proxy representation that contains all the relevant information in unobserved confounders. Based on the Cr, we propose a novel Rank weighting method (Rw) to de-bias the treatment assignment. Combining Cr and Rw, we propose a neural network framework named CRNet to estimate the effects of multiple continuous treatments under unobserved confounders, evaluated by the Average Dose-Response Function. Empirically, we demonstrate that CRNet achieves state-of-the-art performance on both synthetic and semi-synthetic datasets.\n\n1\n\nINTRODUCTION\n\nCausal inference is widely applied for explanatory analysis and decision making, e.g., Precision Medicine (Raita et al., 2021), Advertisement (Lada et al., 2019), Education (Johansson et al., 2016) and Digital Economy (Nazarov, 2020). With accessible observation data, many existing algorithms accurately estimate the effect of binary treatment by adjusting the confounders (i.e., the common causes of treatments and outcomes) which rely on unconfoundedness assumption that all confounders are observed. However, continuous and multi-dimensional treatments and unmeasured confounders are common in practice. For instance, practitioners seek to develop precise medicine by studying the response of multiple drug dosages (i.e., treatment) on patient health state (i.e., outcome) (Shi et al., 2020). Besides, due to technique and manipulation issues, some key variables, associated with the treatments and outcomes, like patient’s immunity maybe missing in the historical data, which are referred to as unmeasured confounders. To detect and adjust unmeasured confounders, practitioners would record some proxy variables (noised unobserved confounders, e.g., antibodies) which don’t have a direct effect on treatments and outcome of interest but has a spurious association through shared common confounders (Fig. 1(a)).\n\nFigure 1: (a) Causal Structure of Raw Data, i.e., Y ⊥ T | U; (b) Target Relationship from proxy representation, i.e., Y (T) ⊥ U | E(X).\n\nIn continuous treatments setting, under unconfoundedness assumption, recent works discretize the continuous treatment into multi-valued treatment (Hill, 2011; Wager & Athey, 2018) to traditional models, or develop generalize balancing methods for continuous scenario (Hirano & Imbens, 2004; Vegetabile et al., 2021; Huling et al., 2021). Among them, state-of-the-art works (Wu & Fukumizu, 2021; Schwab et al., 2020; Nie et al., 2021) learn a low-dimensional representation for raw data and\n\n1\n\nUXObserved Covariate (Proxy)Unmeasured ConfoundersTObserved TreatmentObserved OutcomeYXTYProxyRepresentaionEUTYTYXUTYTYE(X)(a)CausalGraph(b)ProxyRepresentaionUnder review as a conference paper at ICLR 2023\n\nbalance it using minimizing mutual information, which discard the imbalance part of raw data and lose most information for predictive task in practice. In fact, the technique implements a trade-off decreasing the estimator variance at the price of increasing the bias. Furthermore, with unobserved confounders, if we control the proxy rather than unobserved variables, the effect estimation will induce additional bias, referred as recovery bias. To deal with this bias, instead of balancing representations and discarding information to block the relationship between observed covariates and treatments, we propose a novel Contrastive regularizer (Cr) to learn a proxy representation for capturing all the relevant information in unobserved confounders with contrastive learning (He et al., 2020; Chen et al., 2020; Grill et al., 2020) which regularize representation space by positive and negative pairs. In Cr, we define the positive pair is the pair of treatments and proxies from the same sample, and the negative pair is the pair of treatment from one sample and proxies from different samples. And with an ideally representation for confounders, we would adopt a balancing methods to eliminate confounding bias, such as generalized propensity score (Hirano & Imbens, 2004).\n\nHowever, one limitation is that the covariate balancing methods rely on the correct specified models. If we don’t have any prior for the models of propensity score, i.e., the conditional distribution of treatment conditioning on the covariates, the effect estimation would still be biased, especially for high-dimensional data and continuous treatment. Besides, balancing methods still suffer from extreme values problem. Although recent methods (Fong et al., 2018; Vegetabile et al., 2021; Huling et al., 2021) propose to clip the score value or optimize balancing weights directly, they still fail in complex data, especially, under multi-continuous treatment setting. So a balancing method that have no extreme values and adapted to unobserved confounders is urgently needed. Therefore, to control for bias from treatment assignment, we propose to rank the weights obtained from inverse propensity score for more effective balancing weighting. Based on the proxy representation learned above, we sort the propensity score based weights in descending order and record their rank (the order in sorted data) as rank weights (Rw), which is an effective and robust weights for treatment effect estimation, theoretically.\n\nCombining Contrastive regularizer (Cr) and Rank weighting (Rw) methods, we propose a neural network framework CRNet to alleviate the outcome approximate bias in estimating the Average Dose-Response Function (ADRF). CRNet can accurately estimate the effects of multiple continuous treatments with high-dimension proxy variables. Empirically, we demonstrate that CRNet achieves state-of-the-art performance on both synthetic and semi-synthetic datasets.\n\n2 RELATED WORK\n\nCausal effect identification with proxy methods Proxy (Guo et al., 2020) assumes that the unobserved confounders can be recovered from the observed covariates. CEVAE (Louizos et al., 2017), intact-VAE (Wu & Fukumizu, 2021) recover unobserved confounders with VAE (Kingma et al., 2019) constraint. Negative controls (Lipsitch et al., 2010) assume that there exist two negative control variables: one is related to treatments and confounders, and another is related to outcomes and confounders. DFPV (Xu et al., 2021) introduces neural networks to model the bridge function (Miao et al., 2018) for estimating the causal effect. The setting of this paper is similar to the proxy. But our method need no data distribution prior and outperforms others in performance.\n\nEstimation methods for continuous treatments For estimating the continuous treatment effect, a branch of methods include spline (Imai & Van Dyk, 2004), kernel methods (Flores et al., 2012), ensemble methods (Hill, 2011; Wager & Athey, 2018), representation-based methods (Schwab et al., 2020; Nie et al., 2021; Bica et al., 2020) model the relationship between treatments and outcomes. There is also a branch of methods (Hirano & Imbens, 2004; Imai & Van Dyk, 2004; Robins et al., 2000; Vegetabile et al., 2021; Arbour et al., 2021; Huling et al., 2021) aim at balancing the covariates shifts. Few previous works take into account of unobserved variables with continuous treatment assignment bias. In this paper, we propose the contrastive regularizer to gain the balancing methods with the presence of unobserved confounders. Also, we propose a new rank weighting method which have no extreme values and is not much sensitive to model misspecified. Combining Cr and Rw, we design a framework CRNet to estimate continuous treatment with proxy.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 PRELIMINARIES\n\nNotation For self-consistency, we use uppercase for random variables (e.g., A) and lowercase for their realization (e.g., a). We suggest bold the character A as a vector, otherwise a scalar. Given a variable Ap i , superscript p represents the dimension of A, and subscript i denotes the i-th sample of A. DA refers to the total number of dimensions of A. NA means the total number of samples of A. Besides, a real valued sample {Xp i , Yi} ∈ Rp+q+r+1 denotes a random sample with observed covariates Xi ∈ Rp,treatments Ti ∈ Rq, unobserved confounders Ui ∈ Rr and outcome i=1 refers to a set of {Xi, Ti, Ui, Yi} with n samples. E is denoted Yi ∈ R. And {Xi, Ti, Ui, Yi}n as expectation and P represents density distribution function. A calligraphic letter H is denoted as a hypothesis space.\n\ni , Tq\n\ni , Ur\n\n3.1 PROBLEM SETUP\n\nAs Fig. 1(a) shown, in this paper, we focus on the treatment effect estimation problem for Continuous and multi-dimensional Treatments, in the setting with unobserved confounders, but high-dimension Proxy variables for unobserved confounders are available (Briefly, CTP problem). Specifically, we formalize proxy as:\n\nDefinition 1 mally, X = f ∗(U, ε1), where the noise item ε1 ⊥ {T, U} and f ∗ means the true function.\n\n(Proxy) The observed covariate and the noise view of unobserved confounder. For-\n\nWithout loss of generality, we focus on estimating the Average Dose-Response Function (ADRF) curve in this paper. We denote ADRF ∗ as the true ADRF and define that\n\nDefinition 2 (ADRF) The potential outcome of continuous treatments over the population:\n\nADRF ∗ = E[Yi(Ti = t)] = E[φ∗(U, do(t))],\n\n(1)\n\nwhere do(t) means the do operation on treatment that do(t) ⊥ {U}.\n\n3.2 MOTIVATION\n\nTo analyse the complex CTP problem, we simplify the ADRF estimation considering an additive regression model φ(Xi|t) given the observed t with no sample selection bias following Imai et al. (2008):\n\nφ(Xi|t) = ˆφ(Ui|t) + ˆh(ε1i|t)\n\nAnd the the estimated\n\nˆADRF of φ(Xi|t) can be formulated as\n\nˆADRF = E[φ(Xi|t)],\n\nwhere t means the observed treatment that t ̸⊥ {U}.\n\nWe set\n\nˆADRF as baseline and define the estimation error as\n\n∆ = E[φ∗(U, do(t)) − φ∗(U|t)] + E[φ∗(U|t) − ˆφ(Ui|t)] − E[ˆh(ε1i|t)] Given the Eq.(4) (the detailed derivation process is in the Appendix), we denote the first error term ∆T = E[φ∗(U, do(t)) − φ∗(U|t)] as the bias from treatment assignment, the second term ∆Y = E[φ∗(U|t)− ˆφ(Ui|t)] as the bias from outcome approximate and the third term ∆ε1 = −E[ˆh(ε1i|t)] as the bias from the recovery of U. Thus, we decompose the estimation error ∆ into\n\n(4)\n\n∆ = ∆ε1 + ∆T + ∆Y\n\n(5)\n\nThen we the divide ADRF estimation with multi-continuous treatments and proxy problem into three component: 1. Reduce the bias of recovery error ∆ε1 on ADRF estimation. 2. Reduce the bias ∆T from treatment assignment of T on U. 3. Reduce the bias ∆Y from the outcome approximation.\n\n3.3 ASSUMPTIONS\n\nThroughout this paper, we assume the two common assumptions Assumption 1 Stable Unit Treatment Value Assumption, SUTVA and Assumption 2 Overlap/Positivity assumption (Imbens & Rubin, 2015) are satisfied. Moreover, we assume the following assumptions.\n\n3\n\n(2)\n\n(3)\n\nUnder review as a conference paper at ICLR 2023\n\nAssumption 3 (Latent unconfoundedness) The potential outcome is independent of treatment assignment given the unobserved confounders. Formally, Y (t) ⊥ T|U.\n\nAssumption 4 observed confoudner. Formally, X ⊥ {T, Y }|U.\n\n(Proxy assumption) The proxy is independent of treatment and outcome given un-\n\nTo eliminate the recovery error ∆ε1, following Louizos et al. (2017), we consider it as a self-supervised representation learning problem: Recovering latent representation U from P(t, X, y), which means estimating P(U|t, X, y), and we formulate this problem as P(y|t, X) = (cid:82) P(y|t, U)P(U|X, t)dU. (For the discussion of proxy identi-\n\nP(y|t, X, U)P(U|X, t)dU = (cid:82)\n\nU\n\nU\n\nfication, see Appendix). Then we make assumptions that\n\nAssumption 5 imately recovered solely from the observations {X, t, y}.\n\n(Recoverability) The density P(U, t, y) of the latent confounders U can be approx-\n\n(Proxy representation) With proxies X, there exists some representations E(X|t) Assumption 6 (briefly, E(X)) such that E(X) ∼ P (U), which means Y (t) ⊥ U | E(X) for a potential outcome with the specific treatments t.\n\n4 ESTIMATION\n\n4.1 CONTRASTIVE REGULARIZER\n\nBased on Assumption 6, the latent representation U will be obtained when approximating the representation E(X). Existing methods (Louizos et al., 2017; Bica et al., 2020) address this problem by VAE (Kingma et al., 2019), GAN (Goodfellow et al., 2020) etc. They all rely on strong prior of the density form of U. In this paper, inspired by Eq.(5), we propose a novel contrastive learning model to preserve U and eliminate ∆ε1 from data without explicit distribution prior.\n\nContrastive Learning There exists two functions f ∈ F and g ∈ G, which encode X representations f (X, ε1) and g(T, ε2), satisfying s(f (Xi, ε1), g(Ti, ε2)) >> s(f (Xj, ε1), g(Ti, ε2)), where i ̸= j. s(·, ·) is a function that measures the similarity between representations.\n\nContrastive learning approximates the latent representations by constructing contrastive samples (similar and dissimilar instances), by which similar instances are closer in the projection space, while dissimilar instances are further away in the projection space to maximize the lower bound of the mutual information.\n\nUnder CTP setting, even if we can control the observed proxies X, the spurious association derived from U still can not be completely eliminated based on traditional representation algorithms. Therefore, we no longer rely on the representation balancing algorithm to cut off the relationship between Xand T (even if we do, we cannot guarantee accurate estimation). Instead, we propose to strengthen the association between X and T using contrastive learning (Jaiswal et al., 2020) to model proxy representation E(X) with neural network E(·) to represent the information from U.\n\nThe essential part for the contrastive approach is the contrastive pairs for modeling representations. Inspired by Arbour et al. (2021); Li et al. (2020), we construct contrastive pairs with no need of discretizing the treatments in causal inference: X and T in original sample as positive pairs {(Xi, Ti)} and X and T in permuted sample (shuffle X and T of data to obtain the permuted data) as negative pairs {Xj, Ti}. Then, as Fig. 2 shown, we set E(X|t) = s(f (X), g(t)) and adopt the NLL (Negative Log-Likelihood) loss (Chen et al., 2020) to design a novel contrastive loss to model P(U|t, X):\n\nlCr(f (X), g(t)) = − log\n\ne(s(f (Xi),g(t))) j=1 e(s(f (Xj ),g(t)))\n\n(cid:80)N\n\n,\n\n(6)\n\nwhere s(f (X), g(t)) denotes the cosine similarity ∥f (X)∥∥g(t)∥ . In contrastive aspect, representations {g(T)} are queries and representations {f (X)} are keys. For a query g(Ti), the positive key is f (Xi) of the sample i and the cosine similarity s(f (Xi), g(Ti)) value in the numerator in Eq. (6) is high. In contrast, the representation f (Xj) is the negative key of g(Ti) where i ̸= j and the\n\nf (X)·g(t)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Contrastive regularizer. The covariates X are transformed to f (X) via MLPs F. In practice (Chen et al., 2020), the representation f (X) is not directly constrained by contrastive loss. f (X) transforms to pX through projection head PX . The treatments T are operated in a similar way. pT and pX are constrained by lCr(f (X), g(T)). For the sake of brevity, we use f (X) and g(T) in the context to represent pX and pT.\n\ncosine similarity s(f (Xj), g(Ti)) value in the denominator of Eq. (6) is low. Constrained by lCr, we capture the proxy representations {f (X), g(T)} from the data {X, T}.\n\nIn this section, we propose to strengthen association between treatments T and covariates X to recover unmeasured common causes U using lCr, which has two responsibilities: (1) strengthen the association between X and T in the same sample, (2) constrain the representation space using X and T in the permuted samples. Benefiting from contrastive learning, these two responsibilities complete each other. With contrastive learning constraints, learned representation E(X) refuse the information of ε1 and maintain the information of U, which means we eliminate the error item ∆ε1 in Eq.(5). Next, we consider the error term ∆T in Eq.(5).\n\n4.2 RANK WEIGHTING\n\n4.2.1 MULTIPLE TREATMENT SCORE WEIGHT\n\nEstimating ADRF given proxy representation, existing methods usually apply the balancing methods 1 to approximate the density P(T|U) for balancing score weights. That is, adopting the inverse of . However, ˆP(T|U) is sensitive to the approximated ˆP(T|U) as the sample weights: Wi = correct specified and nearly can not be estimated accurately, especially for high-dimensional U and multi-dimensional continuous T. To approximate the P(T|U) under CTP setting, we adopt a mixture density network (MDN, which uses neural network to learn the Gaussian mixture model, (Bishop, 1994)) to model ˆP(T|U) = (cid:81)Q ˆP (tq | U). As the Fig. 3 shown, we apply MDN to approximate ˆP(T|U) and obtain the\n\n1 ˆP(Ti|Ui)\n\nq=1\n\nsample weight Wi as\n\nWi =\n\n1 ˆP(Ti|Ui)\n\n, ˆP(Ti|U) =\n\nK (cid:88)\n\nk=1\n\nαkN (cid:0)Ti | μk, Σ2\n\nk\n\n(cid:1)\n\n(7)\n\n1Given that both matching and stratification methods can be considered as special cases of weighted methods, and that the first two methods require discretization of T when it is continuous values, this paper focuses on weighting methods.\n\n5\n\nXTf(X)g(T)F(...)G(...)XTf(X)g(T)F(...)G(...)Construct ParisInput Sample 1 ...Input Sample N...· · ·f(X)g(T)f(X)g(T)f(X)g(T)f(X)g(T)f(X)g(T)f(X)g(T)f(X)g(T)f(X)g(T)f(X)g(T)pXpTpXpTpXpTpXpTpXpTpXpTpositive pairsnegative pairsS(...)LCR=-log ———————————————————Exp( S( ) )Exp( S() )+Exp( S() )Contrastive loss for Sample Pairs:XTPX(...)PT(...)pXpTpXpTpXpTUnder review as a conference paper at ICLR 2023\n\nThe loss function of Rw is:\n\nlRw = −\n\n1 n\n\nn (cid:88)\n\ni=1\n\nlog\n\n(cid:40) K\n\n(cid:88)\n\nk=1\n\nαkN (cid:0)Ti | μk, Σ2\n\nk\n\n(cid:1)\n\n(cid:41)\n\n.\n\n(8)\n\nwhere K is the number of sub-Gaussian models N (·) in the Gaussian mixture model, (μk, Σk) is the mean vector and covariance matrix of the kth sub-Gaussian model, and αk is the probability that the observation belongs to the kth sub-Gaussian model.\n\nFigure 3: Rank weighting. The proxy representation E(X) are transformed to Gaussian mixture distribution with K Gaussian submodels N (αk, μk, Σ2 k) via MLPs MDN. Then we infer the estimated density ˆP(t|U) and sort it in descending order to get the rank weight Rw.\n\n4.2.2 RANK WEIGHT\n\nHowever, the sample weight W from ˆP(T|U) still suffers from extreme values problem. Although recent methods (Fong et al., 2018; Vegetabile et al., 2021; Huling et al., 2021) propose to clip the score value or optimize balance weights directly, they still face a dilemma when the data gets more complex, especially, under multi-continuous treatment setting. So a balancing method that have no extreme values and adapted to unobserved confounders is urgently needed. Therefore, based on the proxy representation learned above, the core contribution of this paper is the rank weights for more effective balancing weighting.\n\nMotivated by the balancing score problem, we normalize the IPW weights obtained from Eq. (7) {Wi}n i=1 ∈ [0, 1] and sort them in descending order. We record their rank (the order in sorted data) as Ri ∈ N and the difference between adjacent W as stride δ ∈ R. We define the IPW as ξ(R, δ) = 1 . It’s clear that when sample size n is limited, the large δ causes extreme values and when n → ∞, δ → 0, so we define the form of rank weight as ̃ξ(R) and propose\n\nˆP(t|U)\n\nProposition 1 There exists some rank weight ̃ξ(R) that when n → ∞, e− ̃ξ(R) Z = (cid:82) e− ̃ξ(R) is the normalizing constant of ̃ξ(R).\n\nZ → 1\n\nP(t|U) , where\n\nThe Proposition 1 shows that the causal effect estimation with rank weight approximate to the unbiased estimation of causal effect. We detail the definition and proposition in Appendix. When n is limited, to eliminate the stride, we set it to a constant δ = 1\n\nn , and obtain the rank weight\n\nˆξ(Ri) ≈ Ri.\n\n(9)\n\nThe Eq. (9) shows that Rw method is adapted to Cr and can be applied to data of any dimension because it only depends on the rank information of weights. when n → ∞, the rank weight approximates to P(t|U) . With limited data samples, the rank weight don’t rely on specified models and address extreme values problem.\n\n1\n\nFig. 3 shows the process of rank weighting: After training of MDN 2, we inference ˆP(T|U) = (cid:1). Then we sort ˆP(T|U) in descending order and get the rank weight Rwi = (cid:80)K ̃ξ(Ri). Then the ∆T of Eq.(5) can be eliminated by weighted regression with rank weight.\n\nk=1 αkN (cid:0)T | μk, σ2\n\nk\n\n2Note that the rank weight is not only adapted to MDN, it can be applied to any balancing weights.\n\n6\n\nE(X)MDN(...)α!μ!σ!\"· · ·Rw!P(t|U)α#μ$σ$\"Under review as a conference paper at ICLR 2023\n\n4.3 CRNET\n\nCombining the Contrastive regularizer and Rank weight, we propose a neural network framework CRNet to estimate ADRF under CTP setting to eliminate ∆Y . As Fig. 4 shown, the overall CRNet\n\nFigure 4: CRNet. For training procedure, the proxy representation E(X|t) = {f (X), g(t)} constrained by contrastive loss lCr(X, t) are concatenated and input to MLPs H and MDN D to obtain the estimated outcome ˆY and the rank weights Rw. The final objective is to minimize the weighted loss in Eq. 10. For inference procedure, the estimated ADRF is obtained by h(f (X), g(t)).\n\narchitecture contains three components: (1) a contrastive regularizer, which contains two MLPs heads that encode proxy and the observed treatments into representations {f (X), g(T)}. (2) A sample weight learner named tank weighting. This module optimizes the rank weights on lRw in Eq.(8) using the representations {f (X), g(T)}. (3) A base MLPs encoder H that concatenates f (X) and g(T) and transforms them as the estimated ˆY to approximate the observed Y by the weighted regression loss lf inal(W, X, T, Y ).\n\nCombining lCr(X, T) and lRw(W ), the final loss is defined as:\n\nlf inal =\n\nN (cid:88)\n\ni=1\n\nRwi ∗ (Yi − ˆYi)2 + α ∗ lEr(X, T) + β ∗ lRw(W ),\n\n(10)\n\nwhere Rwi is the rank weight optimized by lRw(W ). α and β are the hyperparameters of Cr and Rw, respectively.\n\n5 EXPERIMENTS\n\nTo evaluate the performance of CRNet for CTP problem, we compare 6 statistical methods and 5 deep-based methods as baselines in ten simulation data and four semi-synthesis data from IHDP & News. All experiments are implemented using PyTorch (Paszke et al., 2019) on Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz.\n\nBaselines We compare our model with following baselines: For statistical methods, we use (1) CausalForest (Wager & Athey, 2018), a random forest algorithm for causal inference. (2) Bart (Hill, 2011; Chipman et al., 2010), Bayesian Additive Regression Trees for causal infer- (3) GPS (Hirano & Imbens, 2004), a generalized propensity score for continuous treatence. ments. (4)CBGPS (Fong et al., 2018), a generalized covariate balancing propensity score (Imai & Ratkovic, 2014) for continuous treatments. (5)EB, a continuous treatment version of entropy balancing method (Hainmueller, 2012). (6)DCOWS (Huling et al., 2021), a balancing method based on the distance covariance (Sz ́ekely & Rizzo, 2009). For representation based methods, we apply (7) NN, a neural network with fully MLPs. (8) MDN (Bishop, 1994), a mixture density network for modelling the density. (9) DRNet (Schwab et al., 2020), a multi-head deep model stratified according to T, we use a modified version (Nie et al., 2021) for estimating ADRF. (10) VCNet (Nie et al., 2021), a deep model which considers T as a varying coefficient. (11) CEVAE (Louizos et al., 2017), a VAE-based model to constrain the representation of covariates.\n\nDatasets We evaluate the performance of CRNet in ten simulation data and four semi-synthesis data. For simulation experiments, we design 10 simulation datasets and named five of them DataX DT DX (e.g., DataX 1 5 means a simulation with 1 treatment, 5 covariates and no unobserved confounders). We name the other 5 of them DataU DT DX with unobserved confounders.\n\n7\n\nTXF(...)f(X)g(T)G(...)H(...)Construct Parisl!\"#$%l&’l()D(...)RwUnder review as a conference paper at ICLR 2023\n\nWe also conduct 4 semi-synthetic experiments on 2 real-world datasets: IHDP3 and News4. IHDP contains 747 observations on 25 covariates. Following Schwab et al. (2020), we sample 5000 samples with 2870 covariates from News. The semi-synthetic experiment on IHDP is named as IHDP DT. The other 3 semi-synthetic experiments on News are named as News DT. For detailed descriptions of datasets, models, and hyperparameters, see Appendix.\n\nMetrics For all experiments, we perform 30 replications (E = 30) to report the average mean squared error (MSE) and the standard deviations (SD) of the average dose-response function estimation. For correlation measurement, we adopt distance correlation (dCor) to evaluate the quality of the proxy representation E(X). dCor(X, Y ) = with Var(X) = dCov(X, X),\n\ndCov(X,Y )\n\n√\n\nd Var(X) dVar(Y )\n\ndCov(X, T) := 1 n2 (cid:80)n ̄ai· = 1\n\nn\n\nj=1 ai,j, ̄a·j = 1\n\nn\n\ni=1\n\n(cid:80)n\n\n(cid:80)n\n\nj=1 Ai,jBi,j, where Ai,j := ai,j − ̄ai. − ̄a.j + ̄a.., ai,j = ∥Xi − Xj∥2, (cid:80)n\n\ni,j=1 ai,j. And the form of Bi,j is similar.\n\nj=1 ai,j, ̄a.. = 1\n\n(cid:80)n\n\nn2\n\nExperimental results on simulation datasets To assess the performance of CRNet, we conduct simulation experiments increasing the dimensions of treatments and proxies. As Table 1 shown, all methods except GPS perform well in the low-dimensional DataU 1 5. All baselines fails when treatments are multiple in DataU 2 200 and DataU 5 200. Increasing the dimension of treatments as DataU 2 200 and DataU 5 200, we found that CEVAE, which performs well in low dimensions, fails in convergence, which has also been demonstrated in Rissanen & Marttinen (2021). The performance of CRNet outperforms others across different settings. We further verify the effectiveness of Cr module in CRNet below.\n\nE=30\n\nCausal Forest Bart GPS NN DRNet VCNet CEVAE CRNet\n\nTable 1: Results (MSE±SD) on simulation DataU DT DX DataU 1 5\n\nDataU 2 200\n\nDataU 1 200\n\nDataU 1 50\n\n3.962 ± 0.9519 3.905 ± 0.9021 42.25 ± 3.0993 1.914 ± 0.5765 1.982 ± 0.6883 1.457 ± 1.4142 0.944 ± 0.1382 0.865 ± 0.2212\n\n12.22 ± 15.530 9.445 ± 11.109 65.67 ± 53.597 2.542 ± 2.8093 3.422 ± 1.6802 3.502 ± 3.7841 3.308 ± 0.2950 1.360 ± 1.5138\n\n76.56 ± 79.527 129.9 ± 83.055 1030. ± 119.94 22.78 ± 11.503 17.24 ± 9.7990 12.95 ± 17.753 7.309 ± 0.6142 6.651 ± 9.4864\n\n82.30 ± 82.395 127.8 ± 54.624 1110. ± 140.00 26.71 ± 9.8489 21.77 ± 6.9279 27.22 ± 17.365 21.44 ± 1.2480 11.79 ± 10.047\n\nDataU 5 200\n\n96.89 ± 89.262 154.3 ± 66.021 1112. ± 140.51 22.87 ± 8.8573 23.95 ± 9.2497 25.28 ± 15.617 25.04 ± 1.9198 14.79 ± 15.678\n\nExperimental results on the effectiveness of Cr block In the setting with multiple continuous treatments and proxies, we propose the Cr to model E(X) to hold onto the information from unobserved confounders. Practitioners use representation-based approaches to map proxies into a lowdimension representation space which will lose information predictive of the predicted treatment variable. As shown in Fig. 5(a) 5, the correlation between E(X) and T from conventional methods is still weak. To retain the information predictive, Cr regularize the proxy representation E(X) by contrastive learning, the correlation from CRNet and CRNet(ft) 6 is strong. In the experiment of Fig. 5(b), we first train a U-to-T prediction network to obtain the representation T(U) to represent the relationship between T and U. X T(U) denotes the dCor of X and T(U). DRNet refers to the dCor between T(U) and representation f (X). Others are operated similarly. It demonstrates that Cr successfully regularized the representation E(X) between X and T and other methods not.\n\nExperimental results on the effectiveness of Rw block Our downstream block for estimation is Rw. It is reliable iff the proxy representation is accurately measured. To evaluate the performance of Rw block, we conduct 5 simulation experiments with no unobserved confounders. As shown in Table 2, all experiments use the same backbone NN. In all experiments, GPS and MDN which direct model the density of P(T|X) induce excessive bias in ADRF estimation. The direct rank weight without Cr performs well in all experiments. And with Cr, our rank weighting method outperforms other weighting methods. It demonstrates that our CRNet is state-of-the-art even with no unobserved confounders.\n\n3https://www.fredjo.com 4https://paperdatasets.s3.amazonaws.com/news.db 5X T refers the dCor between the proxies X and treatments T. 6CRNet(ft) means the correlation between f (X) and g(T).\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Correlation between X and T\n\n(b) Correlation between X and T(U)\n\nFigure 5: Correlation of treatments, unobserved confounders, and covariates. In both figures, the abscissa represents the sample size, and the ordinate represents the value of dCor.\n\nTable 2: Results (MSE±SD) on simulation DataX DT DX\n\nE=30\n\nDataX 1 5\n\nDataX 1 50\n\nDataX 1 200\n\nDataX 2 200\n\nDataX 5 200\n\nNN +GPS +CBGPS +EB +DCOWS +MDN +Rw +Cr+Rw\n\n0.317 ± 0.2181 1.055 ± 1.2795 0.323 ± 0.2911 0.552 ± 0.9203 0.308 ± 0.2841 0.589 ± 1.6048 0.268 ± 0.2762 0.105 ± 0.0308\n\n2.012 ± 2.3314 13.31 ± 25.384 1.353 ± 1.8173 2.933 ± 4.0231 1.680 ± 1.6652 13.11 ± 45.473 1.171 ± 0.8979 1.170 ± 1.0302\n\n5.440 ± 5.6123 36.58 ± 62.489 6.196 ± 6.9755 6.398 ± 5.6984 6.048 ± 5.3401 99.26 ± 246.86 3.436 ± 4.0289 3.043 ± 4.9246\n\n5.606 ± 7.3323 28.20 ± 33.284 5.472 ± 6.4689 4.217 ± 5.9709 4.727 ± 6.2233 12.03 ± 18.670 3.935 ± 4.1624 1.915 ± 5.1747\n\n12.75 ± 20.344 47.64 ± 70.285 8.019 ± 5.2724 8.210 ± 3.0670 9.490 ± 7.9573 18.44 ± 19.405 7.500 ± 3.9411 5.654 ± 2.8299\n\nExperimental results on real-world datasets We further verify the performance of CRNet in real-world datasets IHDP & News. As shown in Table 3, the traditional methods Bart and Causal Forest cannot estimate the treatment effect accurately and suffer from the high-dimensional proxy imbalanced between different treatments. CRNet obtain a high-quality representation E(X) and retain predictive information of the predicted treatments in representation using contrastive regularizer, but other deep-based methods fails to capture the rich information between high-dimensional covariates and treatments. Therefore, CRNet shows robust performance and achieves the state-ofthe-art in all real-world experiments.\n\nTable 3: Results (MSE±SD) on semi-simulation Real-Data DT\n\nE=30\n\nIHDP 1\n\nNews 1\n\nNews 2\n\nNews 5\n\nCausal Forest Bart GPS NN DRNet VCNet CEVAE CRNet\n\n0.576 ± 0.5651 0.514 ± 0.3728 3.198 ± 15.148 0.774 ± 1.5413 1.485 ± 1.4620 0.626 ± 0.7126 1.935 ± 0.7765 0.351 ± 0.2122\n\n24.60 ± 4.7884 6.200 ± 12.018 1.603 ± 0.2932 0.958 ± 0.1148 3.442 ± 1.3222 8.563 ± 9.4974 1.142 ± 0.0762 0.867 ± 0.2193\n\n20.08 ± 4.0395 11.83 ± 3.2380 2.293 ± 0.7984 2.435 ± 0.2597 6.486 ± 4.2931 7.578 ± 9.2410 3.289 ± 0.1860 2.115 ± 0.6143\n\n21.23 ± 4.7864 22.25 ± 17.902 6.085 ± 1.1495 7.435 ± 1.0865 10.18 ± 2.8848 10.56 ± 6.4256 9.660 ± 0.4348 5.081 ± 0.7098\n\n6 CONCLUSION\n\nFor CTP problem, we formulate the estimation error into three terms from recovery of unobserved confounder, treatment assignment and approximation of outcome. We propose the contrastive regularizer to constrain the proxy representation in representation space for the bias from recovery of unobserved confounder. Based on Cr, we propose a rank weighting method to eliminate the extreme values problem and alleviate the sensitivity problem to model misspecified in treatment assignment model. Combining Cr and Rw, we elaborate a CRNet adapted to CTP problem to reduce the outcome approximation bias. CRNet achieves the state-of-the-art performance in estimating ADRF of both synthetic and semi-synthetic data.\n\n9\n\n10002000300040005000training samples0.190.290.390.49d CorX_TDRNetVCNetCEVAECRNetCRNet(ft)10002000300040005000training samples0.140.240.340.44d CorX_T(U)DRNetVCNetCEVAECRNetUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDavid Arbour, Drew Dimmery, and Arjun Sondhi. Permutation weighting. In International Confer-\n\nence on Machine Learning, pp. 331–341. PMLR, 2021.\n\nIoana Bica, James Jordon, and Mihaela van der Schaar. Estimating the effects of continuous-valued interventions using generative adversarial networks. Advances in Neural Information Processing Systems, 33:16434–16445, 2020.\n\nChristopher M Bishop. Mixture density networks. 1994.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nHugh A Chipman, Edward I George, and Robert E McCulloch. Bart: Bayesian additive regression\n\ntrees. The Annals of Applied Statistics, 4(1):266–298, 2010.\n\nCarlos A Flores, Alfonso Flores-Lagunes, Arturo Gonzalez, and Todd C Neumann. Estimating the effects of length of exposure to instruction in a training program: the case of job corps. Review of Economics and Statistics, 94(1):153–171, 2012.\n\nChristian Fong, Chad Hazlett, and Kosuke Imai. Covariate balancing propensity score for a continuous treatment: Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1):156–177, 2018.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.\n\nRuocheng Guo, Lu Cheng, Jundong Li, P Richard Hahn, and Huan Liu. A survey of learning causality with data: Problems and methods. ACM Computing Surveys (CSUR), 53(4):1–37, 2020.\n\nJens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to pro-\n\nduce balanced samples in observational studies. Political analysis, 20(1):25–46, 2012.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nJennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational\n\nand Graphical Statistics, 20(1):217–240, 2011.\n\nKeisuke Hirano and Guido W Imbens. The propensity score with continuous treatments. Applied Bayesian modeling and causal inference from incomplete-data perspectives, 226164:73–84, 2004.\n\nJared D Huling, Noah Greifer, and Guanhua Chen. Independence weights for causal inference with\n\ncontinuous exposures. arXiv preprint arXiv:2107.07086, 2021.\n\nKosuke Imai and Marc Ratkovic. Covariate balancing propensity score. Journal of the Royal Statis-\n\ntical Society: Series B (Statistical Methodology), 76(1):243–263, 2014.\n\nKosuke Imai and David A Van Dyk. Causal inference with general treatment regimes: Generalizing the propensity score. Journal of the American Statistical Association, 99(467):854–866, 2004.\n\nKosuke Imai, Gary King, and Elizabeth A Stuart. Misunderstandings between experimentalists and observationalists about causal inference. Journal of the royal statistical society: series A (statistics in society), 171(2):481–502, 2008.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nGuido W Imbens.\n\nThe role of the propensity score in estimating dose-response functions.\n\nBiometrika, 87(3):706–710, 2000.\n\nGuido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sci-\n\nences. Cambridge University Press, 2015.\n\nAshish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia\n\nMakedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2020.\n\nFredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual infer-\n\nence. In International conference on machine learning, pp. 3020–3029. PMLR, 2016.\n\nDiederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations\n\nand Trends® in Machine Learning, 12(4):307–392, 2019.\n\nAkos Lada, Alexander Peysakhovich, Diego Aparicio, and Michael Bailey. Observational data for heterogeneous treatment effects with application to recommender systems. In Proceedings of the 2019 ACM Conference on Economics and Computation, pp. 199–213, 2019.\n\nYunzhe Li, Kun Kuang, Bo Li, Peng Cui, Jianrong Tao, Hongxia Yang, and Fei Wu. Continuous treatment effect estimation via generative adversarial de-confounding. In Proceedings of the 2020 KDD Workshop on Causal Discovery, pp. 4–22. PMLR, 2020.\n\nMarc Lipsitch, Eric Tchetgen Tchetgen, and Ted Cohen. Negative controls: a tool for detecting confounding and bias in observational studies. Epidemiology (Cambridge, Mass.), 21(3):383, 2010.\n\nChristos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. Advances in neural information processing systems, 30, 2017.\n\nWang Miao, Xu Shi, and Eric Tchetgen Tchetgen. A confounding bridge approach for double\n\nnegative control inference on causal effects. arXiv preprint arXiv:1808.04945, 2018.\n\nDmitry Nazarov. Causality: Intelligent valuation models in the digital economy. Mathematics, 8\n\n(12):2174, 2020.\n\nLizhen Nie, Mao Ye, Qiang Liu, and Dan Nicolae. Vcnet and functional targeted regularization for\n\nlearning causal effects of continuous treatments. arXiv preprint arXiv:2103.07861, 2021.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nYoshihiko Raita, Carlos A Camargo Jr, Liming Liang, and Kohei Hasegawa. Leveraging “big data” in respiratory medicine–data science, causal inference, and precision medicine. Expert Review of Respiratory Medicine, 15(6):717–721, 2021.\n\nSeveri Rissanen and Pekka Marttinen. A critical look at the consistency of causal estimation with deep latent variable models. Advances in Neural Information Processing Systems, 34:4207–4217, 2021.\n\nJames M Robins, Miguel Angel Hernan, and Babette Brumback. Marginal structural models and\n\ncausal inference in epidemiology, 2000.\n\nPatrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M Buhmann, and Walter Karlen. Learning counterfactual representations for estimating individual dose-response curves. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 5612–5619, 2020.\n\nXu Shi, Wang Miao, and Eric Tchetgen Tchetgen. A selective review of negative control methods\n\nin epidemiology. Current epidemiology reports, 7(4):190–202, 2020.\n\nG ́abor J Sz ́ekely and Maria L Rizzo. Brownian distance covariance. The annals of applied statistics,\n\n3(4):1236–1265, 2009.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nBrian G Vegetabile, Beth Ann Griffin, Donna L Coffman, Matthew Cefalu, Michael W Robbins, and Daniel F McCaffrey. Nonparametric estimation of population average dose-response curves using entropy balancing weights for continuous exposures. Health Services and Outcomes Research Methodology, 21(1):69–110, 2021.\n\nStefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.\n\nPengzhou Wu and Kenji Fukumizu.\n\nIntact-vae: Estimating treatment effects under unobserved\n\nconfounding. arXiv preprint arXiv:2101.06662, 2021.\n\nLiyuan Xu, Heishiro Kanagawa, and Arthur Gretton. Deep proxy causal learning and its application to confounded bandit policy evaluation. Advances in Neural Information Processing Systems, 34: 26264–26275, 2021.\n\nA APPENDIX\n\nA.1\n\nIDENTIFICATION\n\nGiven latent unconfoundedness and proxy assumption, we know that the causal effect is not identified conditioning on X. Because ε1 ̸⊥ U|X, then Y (t) ̸⊥ T|X. We also show this problem in ADRF adjustment formula. The true ADRF is identified as\n\nADRF ∗ = E[Y(t)]] = EU[E[Y(t) | U]]] = EU[E[Y(t) | T = t, U]] = EU[E[Y | T = t, U]]\n\n(11)\n\nWhen proxy exists, ADRF is identified as\n\nˆADRF = E[Y(t)]] = EX[E[Y(t) | X]]] = EX[E[Y(t) | T = t, X]] = EX[E[Y | T = t, X]]\n\n̸= EU[E[Y | T = t, U]] = ADRF ∗.\n\n(12)\n\nIt is clear that the adjustment formula of ADRF from proxy is different from that of true ADRF because Y (t) ̸⊥ T|X, it will induce the recovery error ∆ε1 in the ADRF estimation phase if the unmeasured confounder U is not correct specified. The performance of using proxy to estimate ADRF depends on the degree of recovery of U.\n\nA.2 PROOF OF EQUATION (4)\n\nWe set\n\nˆADRF as baseline and define the estimation error as\n\n∆ = ADRF ∗ − ˆADRF = E[Yi(Ti = t)] − E[φ(Xi|t)]\n\n= E[φ∗(U, do(t))] − E[ ˆφ(Ui|t) + ˆh(ε1i|t)] = E[φ∗(U, do(t))] − E[ ˆφ(Ui|t)] − E[ˆh(ε1i|t)] = E[φ∗(U, do(t))] − E[φ∗(U|t)] + E[φ∗(U|t)] − E[ ˆφ(Ui|t)] − E[ˆh(ε1i|t)] = E[φ∗(U, do(t)) − φ∗(U|t)] + E[φ∗(U|t) − ˆφ(Ui|t)] − E[ˆh(ε1i|t)]\n\n(13)\n\nA.3 ANALYSIS FOR RANK WEIGHT\n\nBased on the definition rank, we define the corresponding index function of rank I(Ri) = i. And we record the stride\n\nδRi =\n\n(cid:26) WI(Ri−1) − Wi Wi\n\n0 < Ri < n Ri = 0\n\nas the difference between two adjacent weights of the sorted data. Then we can build a sequence model of W as:\n\nWi =\n\n(cid:26) WI(Ri−1) − δRi δRi\n\n0 < Ri < n Ri = 0\n\n(14)\n\n(15)\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSupposing an extreme value example that the maximum weight is much larger than others, this is because there are many unmeasured weights between the largest weight and the second largest weight we obtained. Combining Eq.(15) and the continuity of the probability density, it is clear that the excessive stride causes the extreme value of the weights. Moreover, the sample weights with large stride will also cause the sensitivity to misspecified because slight misspecifed of the large stride induce significant bias. Then is just using the rank information enough to make a covariate balance? To answer this question, we formulate the model in Eq.(15) as:\n\nW = ψ(R, I(R), δ) = ξ(R, δ),\n\n(16)\n\nwhere W is the sample weight, R is the rank, I is the index function of R and δ is stride. We reduce the second line to third line because I is a deterministic function of R.\n\nBased on Eq.(14) we notice that in {Wi}n i=1 ∈ [0, 1], if n → ∞, then stride → 0 because WRi − WRi−1 → 0. It is similar to the unnormalized density. Therefore, we formulate an IPW via Gibbs Z → sampling: e−ξ(R,δ) Z\n\nZ = e− ̃ξ(R,δ) P (T|U) = e−W P(t|U) . So we propose that\n\n, where Z = (cid:82) e−ξ(R,δ). We set n → ∞, then e− ̃ξ(R))\n\n= 1\n\nZ\n\n1\n\nProposition There exists some rank weight ̃ξ(R) that when n → ∞, e− ̃ξ(R) Z = (cid:82) e− ̃ξ(R) is the normalizing constant of ̃ξ(R).\n\nZ → 1\n\nP(t|U) , where\n\nThe proposition and Eq.(16) show that when n → ∞, using rank weight ̃ξ(R) is enough to make a covariate balance because the weights Rw approximate to the IPW of density P(T|U). It means when n → ∞, the causal effect estimation with Rw approximates to the unbiased estimation of causal effect (for unbiased estimation with IPW of ˆP(T|U), see Imbens (2000)). And when the data size is limited, rank function ̃ξ(R) can effectively avoid the extreme value problem and alleviate the sensitivity to the model misspecified. So we direct eliminate delta by setting δ = 1 n to obtain ˆξ(R) ≈ Ri. The operation can be considered as enforce the distribution of delta is Uniform, which is biased. But as n → ∞, the obtained ˆξ(R) ≈ Ri is approximating to the true IPW.\n\nA.4 EXPERIMENTAL DETAILS\n\nThe rules for defining symbols in this section are the same as in the main body. Please note that when the superscript is specified as Ap=2, it means the dimension of A is 2, and when it is not specified (e.g., A2), it means the power of A is 2.\n\nA.4.1 DETAILS ON DATASETS\n\nThe dataset split and dimension information corresponding to the data name are expressed in Table 4.\n\nTable 4: Dataset description\n\nNtrain/Ntest DT DX DU DY\n\nDataU 1 5 DataU 1 50 DataU 1 200 DataU 1 200 DataU 1 200 DataX 1 5 DataX 1 50 DataX 1 200 DataX 1 200 DataX 1 200 IHDP 1 News 1 News 2 News 5\n\n1800/300 1800/300 1800/300 1800/300 1800/300 1800/300 1800/300 1800/300 1800/300 1800/300 672/75 3150/1350 3150/1350 3150/1350\n\n1 1\n1 2\n5 1\n1 1\n2 5\n1 1\n2 5\n\n5 50 200 200 200 5\n50 200 200 200 25 2870 2870 2870\n\n5 50 200 200 200 0\n0 0\n0 0\n25 20 20 20\n\n1 1\n1 1\n1 1\n1 1\n1 1\n1 1\n1 1\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nSynthetic datasets we construct synthetic datasets following EB (Vegetabile et al., 2021). For all simulation datasets, the true covariates Up=1···200 are constructed as: Up=1···200 ∼ N (0, 1).\n\nFor DataX 1 5, DataX 1 50, DataX 1 200 datasets, Tp=1 is constructed as:\n\nTp=1 = 0.5N (3, 1) + 0.5N (6, 0.5) + 1.5 ∗\n\np=3 (cid:88)\n\np=1\n\nXp\n\nAnd Y p=1 is constructed as:\n\nY p=1 =\n\n1\n\ne− (cid:80)p=2\n\np=1 Tp\n\n+ eXp=1\n\n+ 2.1 ∗ Xp=2 + 2.2 ∗ Xp=3\n\n+ 2.3 ∗ Xp=4 + Xp=5 + 4.0 ∗\n\np=200 (cid:88)\n\np=151\n\nXp + I(DT==5 ∗ (3 ∗ cos(\n\np=5 (cid:88)\n\np=3\n\nTp)\n\nFor DataU 1 5, DataU 1 50, DataU 1 200 datasets, Tp=1 is constructed as:\n\nTp=1 = 0.5N (3, 1) + 0.5N (6, 0.5) + 1.5 ∗\n\np=3 (cid:88)\n\np=1\n\nUp + 0.5 ∗\n\np=DU(cid:88)\n\np=151\n\nUp.\n\nAnd Y p=1 is constructed as: 1\n\nY p=1 =\n\n+ eUp=1\n\n+ 2.1 ∗ Up=2 + 2.2 ∗ Up=3 + 2.3 ∗ Up=4\n\np=1 Tp\n\ne− (cid:80)p=2 + Up=5 + I(DU > 5) ∗\n\nDU 50\n\n− I(DU == 200) ∗ 2\n\n+\n\n100 DU + 5\n\n∗\n\np=151 (cid:88)\n\np=6\n\nUp + 4.0 ∗\n\np=200 (cid:88)\n\np=151\n\nUp − 1\n\nwhere DU is the dimension of U and I is the indicator function. For DataU 2 200 and DataU 5 200 datasets, Tp=1···5 is constructed as:\n\nTp=1 = 0.5N (3, 1) + 0.5N (6, 0.5) + 1.5 ∗\n\np=4 (cid:88)\n\np=1\n\nUp + 0.4 ∗\n\np=DU(cid:88)\n\np=151\n\nUp,\n\nTp=2 = N (4, 1) + 1.5 ∗ Up=5.\n\nTp=3···5 = N (p, 0.5) +\n\n100+p (cid:88)\n\nq=100\n\nUq.\n\nAnd Y p=1 is constructed as: 1\n\nY p=1 =\n\n+ eUp=1\n\n+ 2.1 ∗ Up=2 + 2.2 ∗ Up=3 + 2.3 ∗ Up=4\n\np=1 Tp\n\ne− (cid:80)p=2 + Up=5 + I(DU > 5) ∗\n\nDU 50\n\n− I(DU == 200) ∗ 2\n\n+\n\n100 DU + 5\n\n∗\n\np=151 (cid:88)\n\np=6\n\nUp + 4.0 ∗\n\np=200 (cid:88)\n\np=151\n\nUp\n\n+ I(DT == 5) ∗ (0.1 ∗\n\np=5 (cid:88)\n\np=3\n\nTp + 2) − 1\n\nThe observed covariates Xp=1···200 of DataU 1 5, DataU 1 50, DataU 1 200, DataU 2 200 and DataU 5 200 are formulated as\n\nXp=1···5 = Up + linespace(0,\n\np 10\n\n, NX)\n\nwhere linespace(0, p\n\nXp=5···200 = Up=5···200, 10 , NX) means samples NX data from [0, p\n\n10 ].\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nIHDP The generation process of T and Y are formulated as:\n\nT =\n\n2Up=1 5Up=1 +\n\n0.1max(Up=3,5,6, 1) 2.1 + min(Up=3,5,6, 1)\n\n+ N (0, 0.25),\n\nY =\n\nsin(3T )UP T = 4 1.2 − T\n\n+\n\np=15 (cid:88)\n\np=8\n\nUp + N (0, 0.25).\n\nU are standardized to N (0, 1) and T are normalized to [0, 1].\n\nThe observed covariates Xp=1···25 are formulated as\n\nXp=1 = 0.2\n\n(Up=1)2 Up=2 , Xp=2 = sin(Up=3),\n\nXp=3 = cos(Up=4), Xp=4 = 0.5exp(Up=5), X5 = Up=6Up=7, Xp=5···25 = Up=5\n\nNews The generation process of T and Y are formulated as:\n\nTp=1···5 = sin(Up)tanh(Up) + N (0, 1),\n\nY p=1 =\n\n3Tp(Up + 1) + M(Up=31···100, 1) + N (0, 1) ε\n\n.\n\nFor News 2, ε = 2, for News 5 ε = 4. U are standardized to N (0, 1) and T are normalized to [0, 1].\n\nfX (Up) =\n\n\n\n \n\n0.2(Up)2 sin(Up) + 0.1 cos(Up) + 0.1 0.1(10 + abs(Up) abs(Up)\n\n, I{mod((p − 1), 5) ≡ 0} , I{mod((p − 1), 5) ≡ 1} , I{mod((p − 1), 5) ≡ 2} , I{mod((p − 1), 5) ≡ 3} , I{mod((p − 1), 5) ≡ 4}\n\nGiven fX, the observed covariates Xp=1···2870 are formulated as\n\nXp=1···20 = fX(Up), Xp=21···2870 = Up.\n\nA.4.2 DETAILS ON MODELS\n\n2 shown, F consists of 5 FCs with We construct CRNet with depth 5. {256, 128, 128, 128, 128} hidden units. G consists of 5 FCs with {32, 64, 64, 32, 32} hidden units. The MDN module consists of 3 FCs with {20, 20, 20}.\n\nAs Fig.\n\nNN consists of 4 FCs with {32, 32, 32, 1} hidden units. We implement GPS, Bart, CF and DRNet following Schwab et al. (2020). We improve on DRNet and implement VCNet following Nie et al. (2021). We implement CEVAE following Louizos et al. (2017). We normalize simulation data to [0,1] for the conditional density estimator in DRNet and VCNet.\n\nA.4.3 DETAILS ON HYPERPARAMETERS\n\nFor GPS , Bart and CF, we use the default hyperparameters as Schwab et al. (2020). For all representation-based models, we fixed the random seed and search for the best performance with SGD or Adam. We also adjust the learning rate with {0.1, 0.01, 0.001, 0.0001, 0.00001}. For DRNet and VCNet, we adjust the hyperparameters knots with {[0.33, 0.66], [0.2, 0.4, 0.6, 0.8], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]} and α with {100, 10, 1, 0.1, 0.01, 0.001}. For VCthe hyperparameters β with {10, 1, 0.1} and the learning rate of Net+TR, we adjust TR with {0.1, 0.01, 0.001}. for CRNet, we adjust hyperparameters α with {100, 10, 1, 0.1, 0.01, 0.001} and β = 1 consistently.\n\nBesides,\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper proposed a new model for causal proxy learning for average dose-response function estimation of multidimensional treatments.  This is done by based on combining contrastive regularizer to learn the proxy representation, and ranked weighting method to de-bias the treatment assignment mechanism estimation. The authors claims that the proposed method can avoid the deficiencies of existing methods that rely on correctly specified treatment assignment models and/or require strong prior of the unobserved confounder distribution.\n\n# Strength And Weaknesses\n\n**Strength**\n\n- The proposed method is new to the best of my knowledge. Under the proxy assumption, the paper proposes to combine contrastive learning of the proxies and the rank weight estimations of the propensity scores to estimate treatment effects.\n- The paper has relatively extensive set of baselines and plenty of tasks in the experimental analysis part. I also appreciate that the authors have detailed all synthetic functions in the appendix for reproducibility. \n\n**Weakness**\n- Missing important references and lacks clear motivation.  The problem setting of this paper is very closely related to (Yixin, 2021), which should definitely be mentioned and compared. I found it hard to be convinced by the author's argument that \"existing methods rely on correctly specified treatment assignment models and/or require strong prior of the unobserved confounder distribution\", since the identification results in (Wang, 2021) does not rely on any of these. Given those, the paper lacks key motivations for the Cr + Rw approach.\n\n- The claim that \"our method need no data distribution prior and outperforms others in performance\" is not very convincing. To some degree, the choice of similarity functions in the contrastive learning part already implicitly assumes certain prior preferences. Even assuming it's true that this method requires absolutely no prior, this comes at a cost of: 1), one needs to specify the similarity function as well; 2), the identifiability of both treatment effects and latent confounders become unclear. \n\n- Although the proposed method is new, I don't immediately see the key novelty of the paper. The contrastive learning is not a new method and has been used for learning latent representations for a long time; the rank weighting might have certain degree of novelty, however I don't see how this relates to the proxy causal learning setting. It looks more like a standalone estimator on this own and needs more theoretical analysis and experiment evaluation for justification.  \n- Lacks rigorous theoretical analysis. For example, the authors claim that \" learned representation E(X) refuse the information of $\\epsilon_1$ and maintain the information of $U$, which means we eliminate the error item...\", which is too heuristic and needs to be formally derived. Also, the paper lacks identifiability analysis for $U$ (or treatment effects). Without those analysis, Eq(11) that the authors proposes hardly qualifies as an \"identification\"; and simply dodging this issue by hand-waving \" The performance of using proxy to estimate ADRF depends on the degree of recovery of $U$\" is clearly unconvincing. \n\n**Minor issues**\n\n- In related works, the paper cited a bunch of methodologies and claims that the proposed method \"outperforms others in performance\". Unfortunately, many of those mentioned in this part are not used in experiments at all. The authors need to be careful on what their claims and avoid over-selling.\n\n- The writing of the paper is not very consistent. For some parts it is very clear and convincing, but for some other parts it is very confusing and I found it hard to understand what the authors are trying to do. I finally managed to understand most of it, but at the cost of reading it multiple times and making my own guess on a number of undefined terms and notations. \n\n**Reference**\n\nWang, Yixin, and David Blei. \"A proxy variable view of shared confounding.\" International Conference on Machine Learning. PMLR, 2021.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee above.\n\n# Summary Of The Review\n\nCausal proxy learning is generally a very interesting topic, and this paper proposes a new solution to it. However, the motivation for the design choices of the paper is quite unclear; and the theoretical justification is not very convincing.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nRANDOM LAPLACIAN FEATURES FOR LEARNING WITH HYPERBOLIC SPACE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDue to its geometric properties, hyperbolic space can support high-fidelity embeddings of tree- and graph-structured data, upon which various hyperbolic networks have been developed. Existing hyperbolic networks encode geometric priors not only for the input, but also at every layer of the network. This approach involves repeatedly mapping to and from hyperbolic space, which makes these networks complicated to implement, computationally expensive to scale, and numerically unstable to train. In this paper, we propose a simpler approach: learn a hyperbolic embedding of the input, then map once from it to Euclidean space using a mapping that encodes geometric priors by respecting the isometries of hyperbolic space, and finish with a standard Euclidean network. The key insight is to use a random feature mapping via the eigenfunctions of the Laplace operator, which we show can approximate any isometry-invariant kernel on hyperbolic space. Our method can be used together with any graph neural networks: using even a linear graph model yields significant improvements in both efficiency and performance over other hyperbolic baselines in both transductive and inductive tasks.\n\n1\n\nINTRODUCTION\n\nReal-world data contains various structures that resemble non-Euclidean spaces: for example, data with tree- or graph-structure such as citation networks (Sen et al., 2008), social networks (Hoff et al., 2002), biological networks (Rossi & Ahmed, 2015), and natural language (e.g., taxonomies and lexical entailment) where latent hierarchies exist (Nickel & Kiela, 2017). Graph-style data features in a range of problems—including node classification, link prediction, relation extraction, and text classification. It has been shown both theoretically and empirically (Bowditch, 2006; Nickel & Kiela, 2017; 2018; Chien et al., 2022) that hyperbolic space—the geometry with constant negative curvature—is naturally suited for representing (i.e. embedding) such data and capturing implicit hierarchies, outperforming Euclidean baselines. For example, Sala et al. (2018) shows that hyperbolic space can embed trees without loss of information (arbitrarily low distortion), which cannot be achieved by Euclidean space of any dimension (Chen et al., 2013; Ravasz & Barab ́asi, 2003).\n\nPresently, most well-known and -established deep neural networks are built in Euclidean space. The standard approach is to pass the input to a Euclidean network and hope the model can learn the features and embeddings. But this flat-space approach can encode the wrong prior in tasks for which we know the underlying data has a different geometric structure, such as the hyperbolic-space structure implicit in tree-like graphs. Motivated by this, there is an active line of research on developing ML models in hyperbolic space Hn. Starting from hyperbolic neural networks (HNN) by Ganea et al. (2018), a variety of hyperbolic networks were proposed for different applications, including HNN++ (Shimizu et al., 2020), hyperbolic variational auto-encoders (HVAE, Mathieu et al. (2019)), hyperbolic attention networks (HATN, Gulcehre et al. (2018)), hyperbolic graph convolutional networks (HGCN, Chami et al. (2019)), hyperbolic graph neural networks (HGNN, Liu et al. (2019)), and hyperbolic graph attention networks (HGAT Zhang et al. (2021a)). The strong empirical results of HGCN and HGNN in particular on node classification, link prediction, and molecular-and-chemicalproperty prediction show the power of hyperbolic geometry for graph learning.\n\nThese hyperbolic networks adopt hyperbolic geometry at every layer of the model. Since hyperbolic space is not a vector space, operations such as addition and multiplication are not well-defined; neither are matrix-vector multiplication and convolution, which are key components of a deep model\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthat uses hyperbolic geometry at every layer. A common solution is to treat hyperbolic space as a gyro-vector space by equipping it with a non-commutative, non-associative addition and multiplication, allowing hyperbolic points to be processed as features in a neural network forward. However, this complicates the use of hyperbolic geometry in neural networks because the imposition of an extra structure on hyperbolic space beyond its manifold properties—making the approach somehow non-geometric. A second problem with using hyperbolic points as intermediate features is that these points can stray far from the origin (just as Euclidean DNNs require high dynamic range Kalamkar et al. (2019)), especially for deeper networks. This can cause significant numerical issues when the space is represented with ordinary floating-point numbers: the representation error is unbounded and grows exponentially with the distance from the origin. Much careful hyperparameter tuning is required to avoid this “NaN problem” Sala et al. (2018); Yu & De Sa (2019; 2021). These issues call for a simpler and more principled way of using hyperbolic geometry in DNNs.\n\nIn this paper, we propose such a simple approach for learning with hyperbolic space. The insight is to (1) encode the hyperbolic geometric priors only at the input via an embedding into hyperbolic space, which is then (2) mapped once into Euclidean space by a random feature mapping φ : Hn → Rd that (3) respects the geometry of hyperbolic space in that its induced kernel k(x, y) = E[⟨φ(x), φ(y)⟩] is isometry-invariant, i.e. k(x, y) depends only on the hyperbolic distance between x and y, followed by (4) passing these Euclidean features through some downstream Euclidean network. This approach both avoids the numerical issues common in previous approaches (since hyperbolic space is only used once early in the network, numerical errors will not compound) and eschews the need for augmenting hyperbolic space with any additional non-geometric structure (since we base the mapping only on geometric distances in hyperbolic space). Our contributions are as follows:\n\n• In Section 4 we propose a random feature extraction called HyLa which can be sampled to be an unbiased estimator of any isometry-invariant kernel on hyperbolic space. This generalizes the classic method of random Fourier features proposed for Euclidean space by Rahimi et al. (2007). • In Section 5 we show how to adopt HyLa in an end-to-end graph learning architecture that simultaneously learns the embedding of the initial objects and the Euclidean graph learning model. • In Section 6, we evaluate our approach empirically. Our HyLa-networks demonstrate better performance, scalability and computation speed than existing hyperbolic networks: HyLa-networks consistently outperform HGCN, even on a tree dataset, with 12.3% improvement while being 4.4× faster. Meanwhile, we argue that our method is an important hyperbolic baseline to compare against due to its simple implementation and compatibility with any graph learning model.\n\n2 RELATED WORK\n\nHyperbolic space. n-dimensional hyperbolic space Hn is usually defined and used via a model, a representation of Hn within Euclidean space. Common choices include the Poincar ́e ball (Nickel & Kiela, 2017) and Lorentz hyperboloid model (Nickel & Kiela, 2018). We develop our approach using the Poincar ́e ball model, but our methodology is independent of the model and can be applied to other models. The Poincar ́e ball model is the Riemannian manifold (Bn, gp) with Bn = {x ∈ Rn : ∥x∥ < 1} being the open unit ball and the Riemannian metric gp and metric distance dp being\n\ngp(x) = 4(1 − ∥x∥2)−2ge\n\nand\n\ndp(x, y) = arcosh\n\n1 + 2\n\n(cid:16)\n\n∥x−y∥2 (1−∥x∥2)(1−∥y∥2)\n\n(cid:17)\n\n.\n\nwhere ge is the Euclidean metric. To encode geometric priors into neural networks, many versions of hyperbolic neural networks have been proposed. But while (matrix-) addition and multiplication are essential to develop a DNN, hyperbolic space is not a vector space with well-defined addition and multiplication. To handle this issue, several approaches were proposed in the literature. Gyrovector space. Many hyperbolic networks, including HNN (Ganea et al., 2018), HNN++ (Shimizu et al., 2020), HVAE (Mathieu et al., 2019), HGAT (Zhang et al., 2021a), and GIL Zhu et al. (2020), adopt the framework of gyrovector space as an algebraic formalism for hyperbolic geometry, by equipping hyperbolic space with non-associative addition and multiplication: M ̈obius addition ⊕ and M ̈obius scalar multiplication ⊗, which is defined for x, y ∈ Bn and a scalar r ∈ R\n\nx ⊕ y := (1+2⟨x,y⟩+∥y∥2)x+(1−∥x∥2)y\n\n1+2⟨x,y⟩+∥x∥2∥y∥2\n\n,\n\nr ⊗ x := tanh(r tanh−1(∥x∥)) x\n\n∥x∥ .\n\nHowever, M ̈obius addition and multiplication are complicated with a high computation cost; high level operations such as M ̈obius matrix-vector multiplication are even more complicated and numer-\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nically unstable (Yu & De Sa, 2021; Yu et al., 2022), due to the use of ill-conditioned functions like tanh−1. Also problematic is the way hyperbolic space is treated as a gyrovector space rather than a manifold, meaning this approach can not be generalized to other manifolds that lack this structure. Push-forward & Pull-backward. Since many operations are well-defined in Euclidean space but not in hyperbolic space, a natural idea is to map Hn to Rd via some mappings, apply well-defined operations in Rd, then map the results back to hyperbolic space. Many works, including HATN (Gulcehre et al., 2018), HGCN (Chami et al., 2019), HGNN (Liu et al., 2019), HGAT (Zhang et al., 2021a), Lorentzian GCN (Zhang et al., 2021b), and GIL Zhu et al. (2020), adopt this method:\n\n• pull the hyperbolic points to Euclidean space with a “pull-backward” mapping; • apply operations such as multiplication and convolution in Euclidean space; and then • push the resulting Euclidean points to hyperbolic space with a “push-forward” mapping.\n\nSince hyperbolic space and Euclidean space are different spaces, no isomorphic maps exist between them. A common choice of the mappings (Chami et al., 2019; Liu et al., 2019) is the exponential map expx(·) and logarithm map logx(·), where x is usually chosen to be the origin O. The exponential and logarithm maps are mappings between the hyperbolic space and its tangent space TxHn, which is an Euclidean space contains gradients. The exponential map maps a vector v ∈ TxHn at x to another point expx(v) in Hn (intuitively, expx(v) is the point reached by starting at x and moving in the direction of v a distance ∥v∥ along the manifold), while the logarithm map inverts this.\n\nThis approach is more straightforward and natural in the sense that hyperbolic space is only treated as a manifold object with no more structures added, so it can be generalized to general manifolds (although it does privilege the origin). However, expo(·) and logo(·) are still complicated and numerically unstable. Both push-forward and pull-backward mappings are used at every hyperbolic layer, which incurs a high computational cost in both the model forward and backward loop. As a result, this prevents hyperbolic networks from scaling to large graphs. Moreover, the Push-forward & Pull-backward mappings act more like nonlinearities instead of producing meaningful features. Kernel Methods and Horocycle Features. Cho et al. (2019) proposed hyperbolic kernel SVM for nonlinear classification without resorting to ill-fitting tools developed for Euclidean space. Their approach differs from ours in that they map hyperbolic points to another (higher-dimensional) hyperbolic feature space, rather than an Euclidean feature space. They also constructed feature mappings only for the Minkowski inner product kernel: it’s unknown how to construct feature mappings of their type for general kernels. Another work by Fang et al. (2021) develops several valid positive definite kernels in hyperbolic spaces and investigates their usages; they do not provide any samplingbased features to approximate these kernels. Wang (2020) constructed hyperbolic neuron models using a push-forward mapping along with the hyperbolic Poisson kernel Pn(x, ω) = ( 1−∥x∥2 ∥x−ω∥2 )n−1 for x ∈ Bn, ω ∈ ∂Bn as the backbone of an even more complicated feature function. Sonoda et al. (2022) theoretically proposes a continuous version of shallow fully-connected networks on noncompact symmetric space (including hyperbolic space) using Helgason-Fourier transform, where some network functions coincidentally share some similarities to features proposed in this paper.\n\n3 BACKGROUND\n\ni=1\n\n∂2f ∂x2 i\n\nLaplace operator (Euclidean). The Laplace operator ∆ on Euclidean space Rn is defined as the divergence of the gradient of a scalar function f , i.e., ∆f = ∇ · ∇f = (cid:80)n . The eigenfunctions of ∆ are the solutions of the Helmholtz equation −∆f = λf, λ ∈ R, and can form an orthonormal basis for the Hilbert space L2(Ω) when Ω ∈ Rn is compact (Gilbarg & Trudinger, 2015), i.e., a linear combination of them can represent any function/model that is L2-integrable. A notable parameterization for these eigenfunctions are the plane waves, given by f (x) = exp(i⟨ω, x⟩), where ω ∈ Rn and ⟨·, ·⟩ is the Euclidean inner product. A standard result given in Helgason (2022) states that any eigenfunction of ∆ can be written as a linear combination of these plane waves (Theorem A.1). The famous result of Rahimi et al. (2007) used these eigenfunctions to construct feature maps, called random Fourier features, for arbitrary shift-invariant kernels in Euclidean space. Theorem 3.1 (Bochner’s theorem, Rudin (2017)). For any shift-invariant continuous kernel k(x, y) = k(x − y) on Rn, let p(ω) be its Fourier transform and ξω(x) = exp(i⟨ω, x⟩). Then k is positive definite if and only if p ≥ 0, in which case if we draw w proportionally to p,\n\nk(x − y) = (cid:82)\n\nRn p(ω) exp(i⟨ω, x − y⟩) dω = k(0) · Eω∼p [ξω(x)ξω(y)∗].\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nSince both the probability distribution p(ω) and the kernel k(x − y) are real, the integral is unchanged when we replace the exponential with a cosine. Rahimi et al. (2007) leveraged this to produce real-valued features by setting zω,b(x) = 2 cos(⟨ω, x⟩ + b), arriving at a real-valued mapping that satisfies the condition Ew∼p,b∼Unif[0,2π] [zω,b(x)zω,b(y)] = k(x − y). Rahimi et al. (2007) approximated functions such as Gaussian, Laplacian and Cauchy kernels with this technique.\n\n√\n\nLaplace-Beltrami operator (Hyperbolic). The Laplace-Beltrami operator L is the generalization of the Laplace operator to Riemannian manifolds, defined as the divergence of the gradient for any twice-differentiable real-valued function f , i.e., Lf = ∇ · ∇f . In the n-dimensional Poincar ́e disk model Bn, the Laplace-Beltrami operator takes the form (Agmon, 1987) L = 1\n\n+ n−2\n\n2 (1 − ∥x∥2) (cid:80)n\n\n4 (1 − ∥x∥2)2 (cid:80)n\n\ni=1 xi\n\ni=1\n\n.\n\n∂ ∂xi\n\n∂2 ∂x2 i\n\nJust as in the Euclidean case, the eigenfunctions of L in hyperbolic space can be derived by solving the Helmholtz equation. We might hope to find analogs of the “plane waves” in hyperbolic space that are eigenfunctions of L. One way to approach this is via a geometric interpretation of plane waves.\n\nIn the Euclidean case, for unit vector ω and scalar λ, f (x) = exp(iλ⟨ω, x⟩) is called a “plane wave” because it is constant on each hyperplane perpendicular to ω. We can interpret ⟨ω, x⟩ as the signed distance from the origin O to the hyperplane ξ(ω, x) which contains x and is perpendicular to w (Figure 1).\n\nFigure 1: Euclidean hyperplane.\n\nIn the Poincar ́e ball model of hyperbolic space, the geometric analog of the hyperplane is the horocycle. For any z ∈ Bn and unit vector ω (i.e., ω ∈ ∂Bn), the horocycle ξ(ω, z) is the Euclidean circle that passes through ω, z and is tangent to the boundary ∂Bn at ω, as indicated in Figure 2. We let ⟨ω, z⟩H denote the signed hyperbolic distance from the origin O to the the horocycle ξ(ω, z). In the Poincar ́e ball model, this takes the form ⟨ω, z⟩H = log((1−∥z∥2)/∥z−ω∥2). If we define the “hyperbolic plane waves” exp(μ⟨ω, z⟩H ), where μ ∈ C. Unsurprisingly, they are indeed eigenfunctions of the hyperbolic Laplacian (Agmon, 1987).\n\nFigure 2: Hyperbolic horocycle.\n\nL exp(μ⟨ω, z⟩H ) = μ(μ − n + 1)e(μ⟨ω, z⟩H ). Since we are interested in finding real eigenfunctions (via the same exp-to-cosine trick used in Rahimi et al. (2007)), we restrict our attention to μ that yield a real eigenvalue. This happens when μ = n−1 2 + iλ for real λ, in which case the eigenvalue is μ(μ − n + 1) = −λ2 − (n − 1)2/4. Just as the Euclidean plane waves exp(i⟨ω, x⟩) span the eigenspaces of the Euclidean Laplacian, the same result holds for these “hyperbolic plane waves” (Theorem A.2).\n\n4 HYLA: EUCLIDEAN FEATURES FROM HYPERBOLIC EMBEDDINGS\n\nIn this section, we present HyLa, a feature mapping that can approximate an isometry-invariant kernel over hyperbolic space Hn in the same way that the random Fourier features of Rahimi et al. (2007) approximate any shift-invariant kernel over Rn. In place of the Euclidean plain waves, which are the eigenfunctions of the Euclidean Laplacian, here we derive our feature extraction using the hyperbolic plain waves, which are eigenfunctions of the hyperbolic Laplacian. Since the hyperbolic plane wave exp(( n−1 2 −iλ)⟨⟨ω, x⟩H ) is an eigenfunction of the real operator L with real eigenvalue, so will this function multiplied by any phase exp(−ib), as will its real part. Call the result of this\n\n(cid:1) cos (λ⟨ω, z⟩H + b) .\n\nHyLaλ,b,ω(z) = exp (cid:0) n−1\n\n2 ⟨ω, z⟩H (1) This parameterization, which we call HyLa (for Hyperbolic Laplacian features), yields real-valued eigenfunctions of the Laplace-Beltrami operator with eigenvalue −λ2−(n−1)2/4. HyLa eigenfunctons have the nice property that they are bounded in almost every direction, as ⟨ω, z⟩H approaches 0 as z approaches any point on the boundary of Bn except ω. Note that HyLa eigenfunctions are invariant to isometries of the space: any isometric transformation of HyLa yields another HyLa eigenfunction with the same λ but a transformed ω (depending on how the isometry acts on the boundary ∂Bn). It is easy to compute, parameterized by continuous instead of discrete parameters,\n\n4\n\nOwxξ(w,x)⟨w,x⟩ξ(w,z)wzO⟨w,z⟩Under review as a conference paper at ICLR 2023\n\nFigure 3: Visualization of the kernel k(x, y) when ρ(λ) = N (0, 0.52). (left) Distributions of k(x, y) and the heat kernel (at temperature t = 6) in 3D hyperbolic space; (middle) k(x, O) in 2D Poincar ́e disk model; (right) ⟨φ(x), φ(y)⟩ for HyLa features based on D = 1000 samples. and are analogous to random Fourier features. Moreover, HyLa can be extended to eigenfunctions on other manifolds (e.g. symmetric spaces) since we only use manifold properties of Hn.\n\nfor any λ ∈ R, under uniform sampling of ω and b,\n\nWe show that HyLaλ,b,ω(x) HyLaλ,b,ω(y) is an unbiased estimate of an isometry-invariant kernel k(x, y). Theorem 4.1. Let ω be sampled uniformly (under the Euclidean metric) from the boundary ∂Bn and let b be sampled uniformly from [0, 2π]. Then E [HyLaλ,b,ω(x) HyLaλ,b,ω(y)] = kλ(x, y) for any λ ∈ R and x, y ∈ Hn, where the function kλ is an isometry-invariant kernel given by\n\nthe product\n\nkλ(x, y) = 1\n\n2 · 2F1\n\n(cid:0) n−1\n\n2 + iλ, n−1\n\n2 − iλ; n\n\n2 ; 1\n\n2 (1 − cosh(dH(x, y)))(cid:1) ,\n\nwhere 2F1 is the hypergeometric function, defined via analytic continuation by the power series\n\n(|z| < 1).\n\n2F1 (a, b; c; z) = (cid:80)∞\n\n(a)n(b)n (c)n\n\nzn\n\nn! = 1 + ab\n\nz\n\n1! + a(a+1)b(b+1)\n\nc(c+1)\n\nz2 2! + · · ·\n\nn=0\n\nk(x, y) = 1 2\n\nc Note that despite the presence of an i in the formula, this kernel is clearly real because the hypergeometric function satisfies the properties 2F1(a, b; c; z) = 2F1(b, a; c; z) and 2F1(a, b; c; z)∗ = 2F1(a∗, b∗; c∗; z∗). In practice, as with random Fourier features, instead of choosing one single λ, we select them at random from some distribution ρ(λ). The resulting kernel will be an isometryinvariant kernel that depends on the distribution of λ, as follows: (cid:0) n−1\n\n2 − iλ; n This formula gives a way to derive the isometry-invariant kernel from a distribution ρ(λ); if we are interested in finding a feature map for some particular kernel, we can invert this mapping to get a distribution for λ which will produce the desired kernel. Theorem 4.2. Suppose that k(x, y) = k(dH(x, y)) is an isometry-invariant positive semidefinite kernel. Assume the existence of an associated density ρ(λ) with the kernel, then 2 − iλ)⟨ω, z⟩H\n\n2 (1 − cosh(dH(x, y)))(cid:1) · ρ(λ) dλ.\n\n∂Bn k(dH(z, O)) exp (cid:0)( n−1\n\nρ(λ) = λ tanh ( πλ\n\n2 + iλ, n−1\n\n(cid:82) ∞ −∞ 2F1\n\n(cid:1) dω dz.\n\n2 ) (cid:82)\n\n2 ; 1\n\n(2)\n\nBn\n\n(cid:82)\n\ni.e., ρ(λ) is the spherical transform of the kernel, and if we draw λ proportional to ρ, ω uniformly on ∂Bn, and b uniformly on [0, 2π], then\n\nk(0) · E [HyLaλ,b,ω(x) HyLaλ,b,ω(y)] = kλ(x, y) = k(x, y). Although Theorem 4.2 lets us find a HyLa distribution for any isometric kernel, for simplicity in this paper, because the closed-forms of many kernels in Hn are not available, rather than arriving at a distribution via this inverse, we will instead focus on the case where ρ is a Gaussian. This corresponds closely to a heat kernel (Grigor’yan & Noguchi, 1998), as illustrated in Figure 3 (left).\n\nWe will use HyLa eigenfunctions to produce Euclidean features from hyperbolic embeddings, using the same random-features approach as Rahimi et al. (2007). Concretely, to map from Hn to RD, we draw D independent samples λ1, . . . , λD from ρ, D independent samples ω1, . . . , ωD uniform from ∂Bn, and D independent samples b1, . . . , bD uniform from [0, 2π], and then output a feature map φ the kth coordinate of which is φk(x) = 1√ (x). It is easy to see that this will yield feature vectors with E [⟨φ(x), φ(y)⟩] = k(x, y) as given in Eq. 2.\n\nHyLaλk,bk,ωk\n\nD\n\nWe visualize the kernel k(x, O) for ρ(λ) = N (0, 0.25) in the 2-dimensional Poincar`e disk in Figure 3, evaluating the integral in Eq. 2 using Gauss–Hermite quadrature. In Figure 3 (right), we sample random HyLa features with D = 1000 and plot ⟨φ(x), φ(O)⟩. Visibly, the HyLa features approximate the kernel well. A discussion of the estimation error is provided in Appendix B.\n\nConnection to Euclidean Activation. There is a close connection between the HyLa eigenfunction and the Euclidean activations used in Euclidean fully connected networks. Given a data point x ∈ Rn, a weight ω ∈ Rn, a bias b ∈ R and nonlinearity σ, a Euclidean DNN activation can be\n\n5\n\n01234567d(x,y)0.00.20.40.60.81.0Kernel Valuek(x,y)Heat KernelUnder review as a conference paper at ICLR 2023\n\nwritten as σ(⟨ω, x⟩ + b) = σ(∥ω∥⟨ ω ∥ω∥ , x⟩ + b). In hyperbolic space, for z ∈ Bn, ω ∈ ∂Bn, λ ∈ R, b ∈ R and σ = cos, we can reformulate the HyLa eigenfunction as HyLaλ,b,ω(z) = σ (λ⟨ω, z⟩H + b) exp (cid:0) n−1 (cid:1), HyLa generalizes Euclidean activations to hyperbolic space, with an extra factor exp (cid:0) n−1\n\n(cid:1) from the curvature of Hn.\n\n2 ⟨ω, z⟩H\n\n2 ⟨ω, z⟩H\n\nFrom a functional perspective, any f ∈ L2(Hn) can be expanded as an infinite linear combination (integral form) of HyLa (Theorem 4.3 in Sonoda et al. (2022)). This statement holds whenever the non-linearity σ is a tempered distribution on R, i.e., the topological dual of the Schwartz test functions, including ReLU and cos. Though the features will not approximate a kernel on hyperbolic space if σ ̸= cos, this suggests variants of HyLa with other nonlinearities may be interesting to study.\n\n5 HYLA FOR GRAPH LEARNING\n\nIn this section, we show how to use HyLa to encode geometric priors for end-to-end graph learning.\n\nBackground on Graph Learning. A graph is defined formally as G = (V, A), where V represents the vertex set consisting of n nodes and A ∈ Rn×n represents the symmetric adjacency matrix. Besides the graph structure, each node in the graph has a corresponding d-dimensional feature vector: we let X ∈ Rn×d denote the entire feature matrix for all nodes. A fraction of nodes are associated with a label indicating one (or multiple) categories it belongs to. The node classification task is to predict the labels of nodes without labels or even of nodes newly added to the graph.\n\nAn important class of Euclidean graph learning model is the graph convolutional neural network (GCN) (Kipf & Welling, 2016; Defferrard et al., 2016). The GCN is widely used in graph tasks including semi-supervised learning for node classification, supervised learning for graph-level classification, and unsupervised learning for graph embedding. Many complex graph networks and GCN variants have been developed, such as the graph attention networks (GAT, Veliˇckovi ́c et al. (2017)), FastGCN (Chen et al., 2018), GraphSage (Hamilton et al., 2017), and others (Velickovic et al., 2019; Xu et al., 2018). An interesting work to understand GCN is simplifying GCN (SGC, Wu et al. (2019)): a linear model derived by removing the non-linearities in a K-layer GCN as: f (A, X) = softmax(SKXW), where S is the “normalized” adjacency matrix with added selfloops and W is the trainable weight. Note that the pre-processed features SKX can be computed before training, which enables large graph learning and greatly saves memory and computation cost.\n\nEnd-to-End Learning with HyLa. We propose a feature-extracted architecture via the following recipe: embed the data objects (graph nodes or features, detailed below) into some space (e.g., Euclidean or hyperbolic), map the embedding to Euclidean features X via the kernel transformation (e.g., RFF or HyLa), and finally apply an Euclidean graph learning model f (A, X). This recipe only manipulates the input of the graph learning model and hence, this architecture can be used with any graph learning model. The graph model and the hyperbolic embedding are learned simultaneously with backpropagation. In theory, the embedding space can be any desired space, just use the same Laplacian recipe to construct features. Below we only show the pipeline for hyperbolic space, while we also include Euclidean space (with random Fourier features) as a baseline in our experiments.\n\nAlgorithm 1 End-to-End HyLa\n\nDirectly Embed Graph Nodes. We embed each node into a low dimensional hyperbolic space Bd0 as hyperbolic embedding Z ∈ Rn×d0 for all nodes in the graph, which can be either a pretrained fixed embedding or as parameters learnt together with the subsequent graph learning model during training time. To compute with the HyLa eigenfunctions, first sample d1 points uniformly from the boundary ∂Bd0 to get Ω ∈ Rd1×d0 , then sample d1 eigenvalues and biases separately from N (0, s2) and Uniform([0, 2π]) to get Λ, B ∈ Rd1, where s is a scale constant. The resulting feature matrix (X ∈ Rn×d1 ) computation follows; please refer to Algorithm 1 for a detailed breakdown. The mapped features X are then fed into the chosen graph learning model f (A, X) for prediction.\n\ninput: n objects, Poincar ́e disk Bd0, HyLa feature dimension d1, adjacency matrix A, node feature matrix X, graph neural network f initialize Z ∈ Rn×d0 {hyperbolic embeddings} sample boundary pts matrix Ω ∈ Rd1×d0, eigenvalues Λ ∈ Rn×d1 and biases B ∈ Rn×d1 compute P=⟨Ω, Z⟩H∈Rn×d1{Horocycle distance} compute X = exp (cid:0) n−1 2 P(cid:1)cos(Λ · P + B) {HyLa} if embedding features: X = XX ∈ Rn×d1 return Y = f (A, X) {e.g., SGC}\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nEmbed Features. One can also embed the given features into hyperbolic space so as to derive the node features implicitly. Specifically, when a node feature matrix X ∈ Rn×d is given, we initialize a hyperbolic embedding for each of the d dimensions to derive hyperbolic embeddings Z ∈ Rd×d0. The Euclidean features X can be computed in the same manner following Algorithm 1. However, to get the new node feature matrix, an extra aggregation step X = XX ∈ Rn×d1 is required before fedding into a graph learning model.\n\nEmbedding graph nodes is better-suited for tasks where no feature matrix is available or meaningful features are hard to derive. However, the size of the embedding Z will be proportional to the graph size, hence it may not scale to very large graphs due to memory and computation constraints. Furthermore, this method can only be used in a transductive setting, where nodes in the test set are seen during training. In comparison, embedding features can be used even for large graphs since the dimension d of the original feature matrix is usually fixed and much lower than the number of nodes n. Note that as the hyperbolic embeddings are built for each feature dimension, they can be used in both transductive and inductive settings, as long as the test data shares the same set of features as the training data. One limitation is that its performance depends on the existence of a feature matrix X that contains sufficient information for learning.\n\nAny graph learning model can be used in the proposed feature-extracted architecture. In our experiments, we focus primarily on the simple linear graph network SGC, which takes the form f (A, X) = softmax(AKXW) with a trainable weight matrix W. Note that just as the vanilla SGC case, AK or AKX can be pre-computed in the same way before training and inference. For the purpose of end-to-end learning, we jointly learn the embedding parameter Z and weight W in SGC during the training time. It’s also possible to adopt a two-step approach, i.e., first pretrain a hyperbolic embedding following (Nickel & Kiela, 2017; 2018; Sala et al., 2018; Sonthalia & Gilbert, 2020; Chami et al., 2019), then fix the embedding and train the graph learning model only. We defer this discussion to Appendix D due to page limit.\n\n6 EXPERIMENTS\n\n6.1 NODE CLASSIFICATION\n\nTask and Datasets. The goal of this task is to classify each node into a correct category. We use transductive datasets: Cora, Citeseer and Pubmed (Sen et al., 2008), which are standard citation networks benchmarks, following the standard splits adopted in Kipf & Welling (2016). We also include datasets adopted in HGCN (Chami et al., 2019) for comparison: disease propagation tree and Airport. Yhe former contains tree networks simulating the SIR disease spreading model (Anderson & May, 1992), while the latter contains airline routes between airports from OpenFlights. To measure scalability, we supplement our experiment by predicting community structure on a large inductive dataset Reddit following Wu et al. (2019). More experimental details are provided in Appendix E.\n\nExperiment Setup. Since all datasets contain node features, we choose to embed features most of the time, since it applies to both small and large graphs, and transductive and inductive tasks. The only exception is the Airport dataset, which contains only 4 dimensional features—here, we use HyLa/RFF after embedding the graph nodes to produce better features X. We then use SGC model as softmax(AKXW), where both W and Z are jointly learned.\n\nBaselines. On Disease, Airport, Pubmed, Citeseer and Cora dataset, we compare our HyLa/RFFSGC model against both Euclidean models (GCN, SGC and GAT) and Hypebolic models (HGCN, LGCN and HNN) using their publicly released version in Table 1, where all hyperbolic models adopt a 16-dimensional hyperbolic space for consistency and a fair comparison. For the largest Reddit dataset, a 50-dimensional hyperbolic space is used. We also compare against the reported performance of supervised and unsupervised variants of GraphSAGE and FastGCN in Table 1. Note that GCN-based models (e.g., HGCN, LGCN) could not be trained on Reddit because its adjacency matrix is too large to fit in memory, unless a sampling way is used for training. Worthy to mention, despite of the fact that standard Euclidean GCN literature (Kipf & Welling, 2016; Wu et al., 2019) train the model for 100 epochs on the node classification task, most hyperbolic (graph) networks including HGCN, LGCN, GIL1 report results of training for (5-)thousand epochs with early stopping.\n\n1We cannot replicate results of GIL from their public code.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Test accuracy/Micro F1 Score (%) averaged over 10 runs on node classification task. Performance of some baselines are taken from their original papers. OOM: Out of memory.\n\nDataset Hyperbolicity δ\n\nDisease 0\n\nAirport 1.0\n\nPubmed 3.5\n\nCiteseer 5.0\n\nCora 11\n\nGCN SGC GAT HGCN LGCN HNN RFF-SGC HyLa-SGC\n\n81.3 ± 0.3 70.5 ± 0.8 69.7 ± 0.4 71.9 ± 0.1 81.0 ± 0.0 69.5 ± 0.2 72.5 ± 0.7 83.0 ± 0.7 70.4 ± 0.4 79.9 ± 0.2 64.0 ± 0.6 74.5 ± 0.9 78.0 ± 0.6 65.9 ± 0.8 81.3 ± 4.0 54.6 ± 0.4 52.0 ± 1.0 41.0 ± 1.8 81.6 ± 0.4 83.4 ± 1.9 71.3 ± 0.6 82.5 ± 0.5 86.8 ± 2.1 95.2 ± 0.5 80.3 ± 0.9 72.6 ± 1.0\n\n78.1 ± 0.2 81.4 ± 0.6 78.9 ± 0.0 80.6 ± 0.1 81.5 ± 0.3 79.0 ± 0.3 90.6 ± 0.2 80.3 ± 0.3 77.8 ± 0.7 57.6 ± 0.7 69.8 ± 0.4 80.5 ± 0.5 77.6 ± 0.1 94.8 ± 0.8\n\nModel\n\nTest F1\n\nOOM OOM OOM\n\nGCN HGCN LGCN SAGE-mean 95.0 SAGE-GCN 93.0 93.7 FastGCN 94.9 SGC 93.9 RFF-SGC 94.5 HyLa-SGC\n\nFigure 4: Visualization of node HyLa features on Cora, Airport and Disease datasets, where nodes of different classes are indicated by different colors. (left) t-SNE on Cora; (middle) t-SNE on Airport; (right) PCA on Disease.\n\nFor a fair comparison, we train all models for a maximum of 100 epochs with early stopping, except from HGCN2, whose results were taken from the original paper trained for 5,000 epochs.\n\nAnalysis. Our feature-extracted architecture is particularly strong and expressive to encode geometric priors for graph learning from Table 1. Together with SGC, HyLa-SGC outperforms state-ofthe-art hyperbolic models on nearly all datasets. In particular, HyLa-SGC beats not only Euclidean SGC/GCN, but also an attention model GAT, except on the Cora dataset with a comparable performance. For the tree network disease with lowest hyperbolicity (more hyperbolic), the improvements of HyLa-SGC over HGCN is about 12.3%! The results suggest that the more hyperbolic the graph is, the more improvements will be gained with HyLa. On Reddit, HyLa-SGC outperforms samplingbased GCN variants, SAGE-GCN and FastGCN by more than 1%. However, the performance is close to SGC, which may indicate that the extra weights and nonlinearities are unnecessary for this particular dataset. Notably, RFF-SGC, embedding into Euclidean space and using RFF, sometimes can be better than GCN/SGC, while HyLa-SGC is consistently bettter than RFF-SGC.\n\nVisualization. In Figure 4, we visualize the learned node Euclidean features using HyLa on Cora, Airport and Disease datasets with t-SNE (Van der Maaten & Hinton, 2008) and PCA projection. This shows that HyLa achieves great label class separation (indicated by different colors).\n\n6.2 TEXT CLASSIFICATION\n\nTask and Datsets. We further evaluate HyLa on transductive and inductive text classification task to assign documents labels. We conducted experiments on 4 standard benchmarks including R52 and R8 of Reuters 21578 dataset, Ohsumed and Movie Review (MR) follows the same data split as Yao et al. (2019); Wu et al. (2019). Detailed dataset statistics are provided in Table 4.\n\nExperiment Setup. In the transductive case, previous work Yao et al. (2019) and Wu et al. (2019) apply GCN and SGC by creating a corpus-level graph where both documents and words are treated as nodes in the graph. For weights and connections in the graph, word-word edge weights are calculated as pointwise mutual information (PMI) and word-document edge weights as normalized TF-IDF scores. The weights of document-document edges are unknown and left as 0. We follows the same data processing setup for the transductive setting, and embed the whole graph since only the adjacent matrix is available. In the inductive setting, we take the sub-matrix of the large matrix in\n\n2HGCN requires pretraining embeddings from a link prediction task to achieve reported results on node\n\nclassification task for Pubmed and Cora.\n\n8\n\n60402002040t-SNE-x60402002040t-SNE-y604020020406080t-SNE-x6040200204060t-SNE-y200204060801st Principal Direction4202462nd Principal DirectionUnder review as a conference paper at ICLR 2023\n\nTable 2: Test accuracy (%) averaged over 10 runs on transductive and inductive text classification task except from the LR mode. Bold numbers: best in both transductive and inductive setting; Underlined numbers: best in inductive setting.\n\nSetting\n\nMethods\n\nR8\n\nR52\n\nOhsumed\n\nMR\n\nTransductive\n\n93.5 ± 0.2 68.4 ± 0.6 76.7 ± 0.2 TextGCN 97.1 ± 0.1 94.0 ± 0.2 68.5 ± 0.3 75.9 ± 0.3 97.2 ± 0.1 TextSGC 73.1 ± 0.4 RFF-SGC 96.5 ± 0.3 67.2 ± 0.4 94.0 ± 0.5 76.2 ± 0.3 HyLa-SGC 96.9 ± 0.4 94.1 ± 0.3 67.3 ± 0.5\n\nInductive\n\nTextGCN 95.8 ± 0.3\n\nLR\n\n93.3 HyLa-LR 97.4 ± 0.2 93.5 ± 0.2 92.2 ± 0.2 97.0 ± 0.4 RFF-LR\n\n88.2 ± 0.7 85.6\n\n74.8 ± 0.3 57.7 ± 0.4 73.0 56.6 64.9 ± 0.3 75.5 ± 0.3 61.6 ± 0.3 76.0 ± 0.3\n\nthe transductive setting, including only the document-word edges as the node representation feature matrix X, then follow the procedure in Section 5 to embed features and apply HyLa/RFF to get X. Since the adjacency matrix of documents is unknown, we replace SGC with a logistic regression (LR) formalized as Y = softmax(XW). We train all models for a maximum of 200 epochs and compare it against TextSGC and TextGCN in Table 2.\n\nPerformance Analysis. In the transductive setting, HyLa-based models can match the performance of TextGCN and TextSGC. The corpus-level graph may contain sufficient information to learn the task, and hence HyLa-SGC does not seem to outperform baselines, but still has a comparable performance. HyLa shows extraordinary performance in the inductive setting, where less information is used compared to the (transductive) node level case, i.e. a submatrix of corpus-level graph. With a linear regression model, it can already outperform inductive TextGCN, sometimes even better than the performance of a transductive TextGCN model, which indicates that there is indeed redundant information in the corpus-level graph. From the results on the inductive text classification task, we argue that HyLa (with features embedding) is particularly useful in the following three ways. First, it can solve the OOM problem of classic GCN model in large graphs, and requires less memory during training, since there are limited number of lower level features (also X is usually sparse), and there is no need to build a corpus-level graph anymore. Second, it is naturally inductive as HyLa is built at feature level (for each word in this task), it generalizes to any unseen new nodes (documents) that uses the same set of words. Third, the model is simple: HyLa follows by a linear regression model, which computes faster than classical GCN models.\n\nEfficiency. Following Wu et al. (2019), we measure the training time of HyLa-based models on the Pubmed dataset, and we compare against both Euclidean models (SGC, GCN, GAT) and Hyperbolic models (HGCN, HNN). In Figure 5, we plot the timing performance of various models, taking into account the precomputation time of the models into training time. We measure the training time on a NVIDIA GeForce RTX 2080 Ti GPU and show the specific timing statistics in Appendix. HyLa-based models achieve the best performance while incurring a minor computational slowdown, which is 4.4× faster than HGCN.\n\nFigure 5: Performance over training time on Pubmed. HyLa-SGC achieves best performance with minor computation slowdown.\n\n7 CONCLUSION\n\nWe propose a simple and efficient approach to using hyperbolic space in neural networks, by deriving HyLa as an expressive feature using the Laplacian eigenfunctions. Empirical results on graph learning tasks show that HyLa can outperform SOTA hyperbolic networks. HyLa sheds light as a principled approach to utilizing hyperbolic geometry in an entirely different way to previous work. Possible future directions include (1) using HyLa with non-linear graph networks such as GCN to derive even more expressive models; and (2) adopting more numerically stable representations of the hyperbolic embeddings to avoid potential “NaN problems” when learning with HyLa.\n\n9\n\n0510152025303540relative training time70727476788082test accuracy (%)SGC1×GCN3.4×GAT33.8×HNN6.6×HGCN41.7×LGCN29.5×HyLa-SGC9.5×Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nShmuel Agmon. On the spectral theory of the laplacian on noncompact hyperbolic manifolds.\n\nJourn ́ees ́Equations aux d ́eriv ́ees partielles, pp. 1–16, 1987.\n\nRoy M Anderson and Robert M May. Infectious diseases of humans: dynamics and control. Oxford\n\nuniversity press, 1992.\n\nSilvere Bonnabel. Stochastic gradient descent on riemannian manifolds.\n\nIEEE Transactions on\n\nAutomatic Control, 58(9):2217–2229, 2013.\n\nBrian H Bowditch. A course on geometric group theory. 2006.\n\nInes Chami, Zhitao Ying, Christopher R ́e, and Jure Leskovec. Hyperbolic graph convolutional neural\n\nnetworks. Advances in neural information processing systems, 32:4868–4879, 2019.\n\nJie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via\n\nimportance sampling. arXiv preprint arXiv:1801.10247, 2018.\n\nWei Chen, Wenjie Fang, Guangda Hu, and Michael W Mahoney. On the hyperbolicity of small-\n\nworld and treelike random graphs. Internet Mathematics, 9(4):434–491, 2013.\n\nEli Chien, Puoya Tabaghi, and Olgica Milenkovic. Hyperaid: Denoising in hyperbolic spaces for\n\ntree-fitting and hierarchical clustering. arXiv preprint arXiv:2205.09721, 2022.\n\nHyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-margin classification in hyperbolic space. In The 22nd international conference on artificial intelligence and statistics, pp. 1832–1840. PMLR, 2019.\n\nMicha ̈el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29:3844–3852, 2016.\n\nPengfei Fang, Mehrtash Harandi, and Lars Petersson. Kernel methods in hyperbolic spaces.\n\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10665–10674, 2021.\n\nOctavian-Eugen Ganea, Gary B ́ecigneul, and Thomas Hofmann. Hyperbolic neural networks. arXiv\n\npreprint arXiv:1805.09112, 2018.\n\nDavid Gilbarg and Neil S Trudinger. Elliptic partial differential equations of second order, volume\n\n224. springer, 2015.\n\nAlexander Grigor’yan and Masakazu Noguchi. The heat kernel on hyperbolic space. Bulletin of the\n\nLondon Mathematical Society, 30(6):643–650, 1998.\n\nCaglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786, 2018.\n\nWilliam L Hamilton, Rex Ying, and Jure Leskovec.\n\nInductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017.\n\nSigurdur Helgason. Groups and geometric analysis: integral geometry, invariant differential oper-\n\nators, and spherical functions, volume 83. American Mathematical Society, 2022.\n\nPeter D Hoff, Adrian E Raftery, and Mark S Handcock. Latent space approaches to social network\n\nanalysis. Journal of the american Statistical association, 97(460):1090–1098, 2002.\n\nDhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. arXiv preprint arXiv:1609.02907, 2016.\n\nQi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. arXiv preprint\n\narXiv:1910.12892, 2019.\n\nEmile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye Teh. Continuous hierarchical representations with poincar ́e variational auto-encoders. Advances in neural information processing systems, 32, 2019.\n\nMaximillian Nickel and Douwe Kiela. Poincar ́e embeddings for learning hierarchical representa-\n\ntions. Advances in neural information processing systems, 30:6338–6347, 2017.\n\nMaximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In International Conference on Machine Learning, pp. 3779–3788. PMLR, 2018.\n\nAli Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines.\n\nIn NIPS,\n\nvolume 3, pp. 5. Citeseer, 2007.\n\nErzs ́ebet Ravasz and Albert-L ́aszl ́o Barab ́asi. Hierarchical organization in complex networks. Phys-\n\nical review E, 67(2):026112, 2003.\n\nRyan Rossi and Nesreen Ahmed. The network data repository with interactive graph analytics and\n\nvisualization. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n\nWalter Rudin. Fourier analysis on groups. Courier Dover Publications, 2017.\n\nFrederic Sala, Chris De Sa, Albert Gu, and Christopher R ́e. Representation tradeoffs for hyperbolic embeddings. In International conference on machine learning, pp. 4460–4469. PMLR, 2018.\n\nPrithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina\n\nEliassi-Rad. Collective classification in network data. AI Magazine, 29(3):93–106, 2008.\n\nRyohei Shimizu, Yusuke Mukuta, and Tatsuya Harada. Hyperbolic neural networks++. arXiv\n\npreprint arXiv:2006.08210, 2020.\n\nSho Sonoda, Isao Ishikawa, and Masahiro Ikeda.\n\nsymmetric space and ridgelet transform based on helgason-fourier analysis. arXiv:2203.01631, 2022.\n\nFully-connected network on noncompact arXiv preprint\n\nRishi Sonthalia and Anna Gilbert. Tree! i am no tree! i am a low dimensional hyperbolic embedding.\n\nAdvances in Neural Information Processing Systems, 33:845–856, 2020.\n\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n\nlearning research, 9(11), 2008.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\nPetar Velickovic, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. ICLR (Poster), 2(3):4, 2019.\n\nMing-Xi Wang. Laplacian eigenspaces, horocycles and neuron models on hyperbolic spaces. 2020.\n\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. SimIn International conference on machine learning, pp.\n\nplifying graph convolutional networks. 6861–6871. PMLR, 2019.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? arXiv preprint arXiv:1810.00826, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLiang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 7370–7377, 2019.\n\nTao Yu and Chris De Sa. Numerically accurate hyperbolic embeddings using tiling-based models.\n\nAdvances in neural information processing systems, 2019.\n\nTao Yu and Christopher M De Sa. Representing hyperbolic space accurately using multi-component\n\nfloats. Advances in Neural Information Processing Systems, 34, 2021.\n\nTao Yu, Went Guo, Jianan Canal Li, Tiancheng Yuan, and Christopher De Sa. Mctensor: A high-precision deep learning library with multi-component floating-point. arXiv preprint arXiv:2207.08867, 2022.\n\nSteve Zelditch. Eigenfunctions of the Laplacian on a Riemannian manifold, volume 125. American\n\nMathematical Soc., 2017.\n\nYiding Zhang, Xiao Wang, Chuan Shi, Xunqiang Jiang, and Yanfang Fanny Ye. Hyperbolic graph\n\nattention network. IEEE Transactions on Big Data, 2021a.\n\nYiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. Lorentzian graph convolutional\n\nnetworks. In Proceedings of the Web Conference 2021, pp. 1249–1261, 2021b.\n\nShichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao, and Bin Wang. Graph geometry inter-\n\naction learning. Advances in Neural Information Processing Systems, 33:7548–7558, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA THEOREMS\n\nTheorem A.1 (Helgason (2022); Zelditch (2017)). All smooth eigenfunctions of the Euclidean Laplacian ∆ on Rn are\n\nf (x) = (cid:82)\n\nSn−1 eiλ⟨ω,x⟩dT (ω),\n\nwhere λ ∈ C − {0} and T is an analytic functional (or hyperfunction), i.e., an element of the dual space of the space of analytic functions on Sn−1. Theorem A.2 (Helgason (2022); Zelditch (2017)). All smooth eigenfunctions of the Hyperbolic Laplacian L on Bn are\n\nf (z) = (cid:82)\n\n∂Bn exp((iλ + n−1\n\n2 )⟨ω, z⟩H ) dT (ω),\n\nwhere λ ∈ C and T is an analytic functional (or hyperfunction). Lemma A.3 (Expression of k(x, O)). Denote ζλ,ω(z) = exp (cid:0)( n−1 responding kernel kλ(x, O) for a particular value of λ defined as\n\n2 + iλ)⟨ω, z⟩H\n\n(cid:1), then the cor-\n\nkλ(x, O) = E [HyLaλ,b,ω(x) HyLaλ,b,ω(y)] =\n\n1 2\n\n· E\n\nω\n\n[ζλ,ω(O)∗ζλ,ω(x)] =\n\n1 2\n\n· E\n\nω\n\n[ζλ,ω(x)]\n\n(3)\n\ntakes the form\n\nkλ(x, O) =\n\n1 2\n\n· 2F1\n\n(cid:18) n − 1 2\n\n+ iλ,\n\nn − 1 2\n\n− iλ;\n\nn 2\n\n;\n\n1 2\n\n(1 − cosh(dH(x, O)))\n\n,\n\n(cid:19)\n\nwhere the expectation in Equation 3 is taken over ω drawn uniformly from the n-dimensional unit sphere.\n\nProof. Recall that for x in the n-dimensional Poincare ball Bn and ω ∈ ∂Bn,\n\n⟨ω, x⟩H = log\n\n(cid:32)\n\n1 − ∥x∥2 ∥x − ω∥2\n\n(cid:33)\n\n.\n\nLet γ = n−1\n\n2 + iλ, expanding Equation 3 out, (cid:34)\n\n(cid:32)\n\n(cid:32)\n\nkλ(x, O) =\n\n1 2\n\n· E\n\nω\n\nexp\n\nγ log\n\n(cid:32)\n\n· exp\n\nγ log\n\n(cid:32)\n\n· exp\n\nγ log\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n· exp\n\nγ log\n\n=\n\n=\n\n=\n\n1 2\n\n1 2\n\n1 2\n\nand if we let\n\n1 − ∥x∥2 ∥x − ω∥2 (cid:33)(cid:33)\n\n1 − ∥x∥2 1 + ∥x∥2\n\n(cid:33)(cid:33)(cid:35)\n\n(cid:34)\n\n(cid:32)\n\nexp\n\nγ log\n\nE ω\n\n1 − ∥x∥2 1 + ∥x∥2\n\n1 − ∥x∥2 1 + ∥x∥2\n\n(cid:33)(cid:33)\n\n\n\n(cid:32)\n\nE ω\n\n\n\n(cid:33)(cid:33)\n\n\n\n(cid:32)\n\nE ω\n\n\n\nu =\n\n−2 ∥x∥\n\n1 + ∥x∥2 ,\n\n(cid:33)(cid:33)(cid:35)\n\n(cid:32)\n\n1 + ∥x∥2 ∥x − ω∥2 (cid:33)−γ \n\n∥x − ω∥2 1 + ∥x∥2\n\n∥x∥2 − 2ωT x + ∥ω∥2 1 + ∥x∥2\n\n(cid:33)−γ  ,\n\nand let ˆx denote the unit vector in the direction of x, then this becomes\n\nkλ(x, O) =\n\n(cid:32)\n\n(cid:32)\n\n· exp\n\nγ log\n\n(cid:33)(cid:33)\n\n1 − ∥x∥2 1 + ∥x∥2\n\n1 2\n\n(cid:20)(cid:16)\n\nE ω\n\n1 + uˆxT ω\n\n(cid:17)−γ(cid:21)\n\n.\n\nExpanding this out using the Binomial Formula (which is valid here because |u| < 1), we get\n\nkλ(x, O) =\n\n=\n\n(cid:32)\n\n· exp\n\nγ log\n\n(cid:32)\n\n· exp\n\nγ log\n\n(cid:32)\n\n(cid:32)\n\n1 − ∥x∥2 1 + ∥x∥2\n\n1 − ∥x∥2 1 + ∥x∥2\n\n1 2\n\n1 2\n\n(cid:33)(cid:33)\n\n(cid:34) ∞ (cid:88)\n\nk=0\n\nE ω\n\n(cid:18)−γ k\n\n(cid:19)\n\nuk (cid:16)\n\nˆxT ω\n\n(cid:17)k(cid:35)\n\n(cid:33)(cid:33) ∞ (cid:88)\n\nk=0\n\n(cid:19)\n\n(cid:18)−γ k\n\nuk E\n\nω\n\n(cid:20)(cid:16)\n\n(cid:17)k(cid:21)\n\n.\n\nˆxT ω\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nSince this expected value is 0 for odd k, this becomes\n\nkλ(x, O) =\n\n(cid:32)\n\n(cid:32)\n\n· exp\n\nγ log\n\n1 2\n\n1 − ∥x∥2 1 + ∥x∥2\n\n(cid:33)(cid:33) ∞ (cid:88)\n\nk=0\n\n(cid:18)−γ 2k\n\n(cid:19)\n\nu2k E\n\nω\n\n(cid:20)(cid:16)\n\n(cid:17)2k(cid:21)\n\n.\n\nˆxT ω\n\nBut ˆxT ω has the same the distribution as the inner product of two uniform random unit vectors in n dimensions. The square of this is well known to be Beta-distributed with parameters ( 1 2 ). So we can write this as\n\n2 , n−1\n\nkλ(x, O) =\n\n(cid:32)\n\n(cid:32)\n\n· exp\n\nγ log\n\n1 2\n\n1 − ∥x∥2 1 + ∥x∥2\n\n(cid:33)(cid:33) ∞ (cid:88)\n\nk=0\n\n(cid:19)\n\n(cid:18)−γ 2k\n\nu2k E\n\nw\n\n(cid:2)wk(cid:3) ,\n\nwhere w ∼ Beta( 1\n\n2 , n−1\n\n2 ). Of course, the moments of the Beta distribution are well-known to be\n\n(cid:2)wk(cid:3) =\n\nE w\n\n(cid:1)(k)\n\n(cid:1)(k)\n\n,\n\n(cid:0) 1 2\n(cid:0) n 2\n\nwhere x(k) denotes the Pochhammer symbol representing the rising factorial. On the other hand,\n\n(cid:19)\n\n(cid:18)−γ 2k\n\n=\n\n(−γ)(2k) (2k)!\n\nwhere x(k) denotes the Pochhammer symbol representing the falling factorial. Since x(k) = (−1)k(−x)(k), we can write this in terms of rising factorials as\n\n(cid:19)\n\n(cid:18)−γ 2k\n\n=\n\n(γ)(2k) (2k)!\n\n.\n\nSo substituting everything in, we have\n\nkλ(x, O) =\n\n(cid:32)\n\n(cid:32)\n\n· exp\n\nγ log\n\n1 2\n\n1 − ∥x∥2 1 + ∥x∥2\n\n(cid:33)(cid:33) ∞ (cid:88)\n\nk=0\n\n(γ)(2k) (2k)!\n\n· u2k ·\n\n(cid:1)(k)\n\n(cid:1)(k)\n\n.\n\n(cid:0) 1 2\n(cid:0) n 2\n\nNext, observe that\n\n(cid:19)(k)\n\n(cid:18) 1 2\n\n=\n\nk−1 (cid:89)\n\nm=0\n\n(cid:18) 1 2\n\n(cid:19)\n\n+ m\n\n= 2−k\n\nk−1 (cid:89)\n\nm=0\n\n(2m + 1) = 4−k ·\n\n(2k)! k!\n\n.\n\nSo this becomes\n\nkλ(x, O) =\n\n(cid:32)\n\n(cid:32)\n\n· exp\n\nγ log\n\n1 2\n\n1 − ∥x∥2 1 + ∥x∥2\n\n(cid:33)(cid:33) ∞ (cid:88)\n\nk=0\n\n(γ)(2k) k!\n\n· u2k · 4−k ·\n\n1 (cid:1)(k)\n\n.\n\n(cid:0) n 2\n\nNext, we leverage the famous identity that\n\nγ(2k) = 4k (cid:16) γ\n\n2\n\n(cid:17)(k) (cid:18) γ + 1\n\n(cid:19)(k)\n\n2\n\nto get\n\nkλ(x, O) =\n\n=\n\n(cid:32)\n\n· exp\n\nγ log\n\n(cid:32)\n\n· exp\n\nγ log\n\n(cid:32)\n\n(cid:32)\n\n1 − ∥x∥2 1 + ∥x∥2\n\n1 − ∥x∥2 1 + ∥x∥2\n\n1 2\n\n1 2\n\n(cid:33)(cid:33) ∞ (cid:88)\n\nk=0\n\n(cid:33)(cid:33)\n\n(cid:17)(k)\n\n1 k!\n\n·\n\n(cid:16) γ 2\n\n(cid:18) γ + 1 2\n\n·\n\n(cid:19)(k)\n\n· u2k ·\n\n1 (cid:1)(k)\n\n(cid:0) n 2\n\n· 2F1\n\n(cid:18) γ + 1 2\n\n;\n\nγ 2\n\n;\n\nn 2\n\n(cid:19)\n\n.\n\n; u2\n\nSince\n\nu2 =\n\n4 ∥x∥2 1 + 2 ∥x∥2 + ∥x∥4 ,\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nit follows that\n\n1 − u2 =\n\n1 − 2 ∥x∥2 + ∥x∥4 1 + 2 ∥x∥2 + ∥x∥4 =\n\n(cid:33)2\n\n(cid:32)\n\n1 − ∥x∥2 1 + ∥x∥2\n\nand\n\n√\n\n1 − u2 − 1 √\n\n1 − u2\n\n2 formula\n\n=\n\n− ∥x∥2 1 − ∥x∥2 .\n\nRecall HypergeometricFunctions/Hypergeometric2F1/17/ShowAll.html) that\n\n(https://functions.wolfram.com/\n\nclassic\n\nthe\n\n(cid:19)\n\n= (1 − z)−a\n\n2F1\n\n(cid:18)\n\n2a, 2c − 2a − 1; c;\n\n; c; z\n\n√\n\n1 − z − 1 √\n\n2\n\n1 − z\n\n(cid:19)\n\n.\n\n(cid:18)\n\n2F1\n\n1 2\nSubstituting z = u2, a = γ\n\na, a +\n\n2 , and c = n 2 yields (cid:32)\n\n1 2\n\nkλ(x, O) =\n\n· 2F1\n\nγ, n − γ − 1;\n\n(cid:33)\n\nn 2\n\n;\n\n− ∥x∥2 1 − ∥x∥2\n\n=\n\n1 2\n\n· 2F1\n\n(cid:32)\n\nn − 1 2\n\n+ iλ,\n\nn − 1 2\n\n− iλ;\n\nn 2\n\n;\n\n− ∥x∥2 1 − ∥x∥2\n\n(cid:33)\n\n.\n\nTherefore,\n\n− ∥x∥2 1 − ∥x∥2 =\n\n− tanh2(D/2) 1 − tanh2(D/2)\n\n=\n\n− tanh2(D/2) sech2(D/2)\n\n= − sinh2(D/2) =\n\n1 2\n\n(1 − cosh(D)) ,\n\nwhere D = dH(x, O). So we get a final expression\n\nkλ(x, O) =\n\n1 2\n\n· 2F1\n\n(cid:18) n − 1 2\n\n+ iλ,\n\nn − 1 2\n\n− iλ;\n\nn 2\n\n;\n\n1 2\n\n(1 − cosh(dH(x, O)))\n\n.\n\n(cid:19)\n\nThe proof is done here. We further make a remark here. Recall that for the Poincar ́e ball model,\n\nd(x, O) = 2 log\n\n\n\n\n\n(cid:113)\n\n\n\n\n\n∥x∥ + 1\n\n1 − ∥x∥2 (cid:33)\n\n(cid:32)\n\n= log\n\n= log\n\n(∥x∥ + 1)2 1 − ∥x∥2 (cid:19)\n\n(cid:18) 1 + ∥x∥ 1 − ∥x∥\n\nTherefore,\n\n(cid:32)\n\nlog\n\n(cid:33)\n\n1 − ∥x∥2 1 + ∥x∥2\n\n= 2 artanh(∥x∥).\n\n= log\n\n= log\n\n(cid:18) 1 − tanh2(D/2) 1 + tanh2(D/2)\n\n(cid:18) sech2(D/2)\n\n1 + tanh2(D/2)\n\n(cid:19)\n\n(cid:19)\n\n(cid:18)\n\n(cid:18)\n\n= log\n\n= log\n\n1 sinh2(D/2) + cosh2(D/2) 1\nsinh2(D/2) + cosh2(D/2)\n\n(cid:19)\n\n(cid:19)\n\n= − log (cosh(D)) .\n\nAnd on the other hand,\n\nu =\n\n−2 tanh(D/2) 1 + tanh2(D/2)\n\n= tanh(D).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nthen we can also write the kernel as\n\nkλ(x, O) =\n\n(cid:18)\n\n· exp\n\n−\n\n(cid:18) n − 1 2\n\n1 2\n\n(cid:19)\n\n(cid:19)\n\n+ iλ\n\nlog (cosh(D))\n\n· 2F1\n\n(cid:18) n + 1 4\n\n+ i\n\nλ 2\n\n,\n\nn − 1 4\n\n+ i\n\nλ 2\n\n;\n\nn 2\n\n; tanh(D)2\n\n(cid:19)\n\n,\n\nwhere D = dH(x, O).\n\nLemma A.4 (Isometry-invariance). The kernel defined by\n\nkλ(x, y) =\n\n1 2\n\n·E\n\nω\n\n[ζλ,ω(x)∗ζλ,ω(y)] =\n\n1 2\n\n·E\n\nω\n\n(cid:20)\n\nexp\n\n(cid:18) (\n\nn − 1 2\n\n− iλ)⟨x, ω⟩H + (\n\n(cid:19)(cid:21)\n\n+ iλ)⟨y, ω⟩H\n\nn − 1 2\n\nis isometry-invariant.\n\nProof. Let g be any isometry of the space, then observe the geometric identity (Helgason, 2022):\n\n⟨g ◦ x, g ◦ ω⟩ = ⟨x, ω⟩ + ⟨g ◦ O, g ◦ ω⟩.\n\n(4)\n\nTake ω = g−1 ◦ ω in Equation 4, it follows that\n\n⟨g ◦ x, ω⟩ = ⟨x, g−1 ◦ ω⟩ + ⟨g ◦ O, ω⟩.\n\nTake x = g−1 ◦ O in Equation 4, it follows that\n\n0 = ⟨O, ω⟩ = ⟨g−1 ◦ O, ω⟩ + ⟨g ◦ O, g ◦ ω⟩,\n\ni.e.,\n\nreplace g−1 with g, then\n\nBy definition,\n\n⟨g−1 ◦ O, ω⟩ = −⟨g ◦ O, g ◦ ω⟩,\n\n⟨g ◦ O, ω⟩ = −⟨g−1 ◦ O, g−1 ◦ ω⟩.\n\nkλ(x, y) =\n\n1 2\n\n· E\n\nω\n\n(cid:20)\n\n(cid:18)\n\nexp\n\n(\n\nn − 1 2\n\n− iλ)⟨x, ω⟩H + (\n\nn − 1 2\n\n(cid:19)(cid:21)\n\n+ iλ)⟨y, ω⟩H\n\n.\n\n(5)\n\nNow assume, g ◦ y = O, then consider\n\n[ζλ,ω(g ◦ x)∗ζλ,ω(O)] = (cid:20)\n\n(cid:18)(cid:18) n − 1\n\n(cid:19)\n\n1 2\n\n· E\n\nω\n\n[ζλ,ω(g ◦ x)∗]\n\n(cid:19)(cid:21)\n\n− iλ\n\n⟨g ◦ x, ω⟩\n\nkλ(g ◦ x, O) =\n\n=\n\n=\n\n=\n\n=\n\n=\n\n1 2\n1 2\n\n1 2\n\n1 2\n\n1 2\n\n1 2\n\n· E\n\nω\n\n· E\n\nω\n\n· E\n\nω\n\n· E\n\nω\n\n· E\n\nω (cid:90)\n\n·\n\nexp\n\n(cid:20)\n\nexp\n\n(cid:20)\n\nexp\n\n(cid:20)\n\nexp\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2 (cid:18)(cid:18) n − 1\n\n2\n\nexp\n\nSn−1\n\n− iλ\n\n− iλ\n\n(cid:19)\n\n(cid:19)\n\n(cid:19)\n\n− iλ\n\n(cid:0)⟨x, g−1 ◦ ω⟩ + ⟨g ◦ O, ω⟩(cid:1)\n\n(cid:19)(cid:21)\n\n(cid:0)⟨x, g−1 ◦ ω⟩ − ⟨g−1 ◦ O, g−1 ◦ ω⟩(cid:1)\n\n(cid:19)(cid:21)\n\n(cid:0)⟨x, g−1 ◦ ω⟩ − ⟨y, g−1 ◦ ω⟩(cid:1)\n\n(cid:19)(cid:21)\n\n(cid:19)\n\n− iλ\n\n(cid:0)⟨x, g−1 ◦ ω⟩ − ⟨y, g−1 ◦ ω⟩(cid:1)\n\n(cid:19)\n\nρ1(ω)dω,\n\nwhere ρ1(ω) is a uniform distribution over the sphere, use ˆω = g−1 ◦ ω as a change of variable, then\n\nkλ(g ◦ x, O) =\n\n(cid:90)\n\nSn−1\n\n(cid:90)\n\n=\n\nSn−1\n\nexp\n\nexp\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:19)\n\n− iλ\n\n(cid:19)\n\n(cid:0)⟨x, g−1 ◦ ω⟩ − ⟨y, g−1 ◦ ω⟩(cid:1)\n\n(cid:19)\n\nρ1(ω)dω\n\n(cid:19)\n\n− iλ\n\n(⟨x, ˆω⟩ − ⟨y, ˆω⟩)\n\nρ1(g ◦ ˆω)d(g ◦ ˆω).\n\nWe claim that the mapping g acts on the boundary with the Jacobian given by\n\nd(g ◦ ˆω) d(ˆω)\n\n=\n\n1 2\n\n· exp((n − 1) · ⟨g−1 ◦ O, ˆω⟩) =\n\n(cid:32)\n\n(cid:13)g−1 ◦ O(cid:13) 1 − (cid:13) 2\n(cid:13) ∥g−1 ◦ O − ˆω∥2\n\n1 2\n\n·\n\n(cid:33)n−1\n\n,\n\n(6)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nthen\n\nkλ(g ◦ x, O) =\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\nSn−1\n\n(cid:90)\n\nSn−1\n\n(cid:90)\n\nSn−1\n\n(cid:90)\n\nSn−1\n\n(cid:90)\n\nSn−1\n\n·\n\n·\n\n·\n\n·\n\n·\n\nexp\n\nexp\n\nexp\n\nexp\n\nexp\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:18)(cid:18) n − 1\n\n2\n\n1 2\n\n1 2\n\n1 2\n\n1 2\n\n1 2\n\n(cid:19)\n\n(cid:19)\n\n− iλ\n\n(⟨x, ˆω⟩ − ⟨y, ˆω⟩)\n\nρ1(g ◦ ˆω)d(g ◦ ˆω)\n\n(cid:19)\n\n(cid:19)\n\n− iλ\n\n(⟨x, ˆω⟩ − ⟨y, ˆω⟩)\n\nρ1(g ◦ ˆω) exp((n − 1) · ⟨g−1 ◦ O, ˆω⟩)d(ˆω)\n\n(cid:19)\n\n(cid:19)\n\n− iλ\n\n(⟨x, ˆω⟩ − ⟨y, ˆω⟩) + (n − 1) · ⟨y, ˆω⟩\n\nρ1(g ◦ ˆω)d(ˆω)\n\n(cid:19)\n\n(cid:18)\n\n− iλ\n\n⟨x, ˆω⟩ +\n\nn − 1 −\n\n(cid:19)\n\n− iλ\n\n⟨x, ˆω⟩ +\n\n(cid:18) n − 1 2\n\nn − 1 2\n(cid:19)\n\n(cid:19)\n\n(cid:19)\n\n+ iλ\n\n⟨y, ˆω⟩\n\nρ1(g ◦ ˆω)d(ˆω)\n\n(cid:19)\n\n+ iλ\n\n⟨y, ˆω⟩\n\nρ1(g ◦ ˆω)d(ˆω).\n\nSince ρ1 is a uniform distribution, this is\n\nkλ(g ◦ x, O) =\n\n(cid:20)\n\nexp\n\n1 2\n\n· E\n\nˆω\n\n(cid:18)(cid:18) n − 1\n\n2\n\n(cid:19)\n\n− iλ\n\n⟨x, ˆω⟩ +\n\n(cid:18) n − 1 2\n\n(cid:19)\n\n(cid:19)(cid:21)\n\n+ iλ\n\n⟨y, ˆω⟩\n\n,\n\ncompared with Equation 5, it follows that kλ(g◦x, O) = kλ(x, y). Since kλ(g◦x, O) only depends on dH(g◦x, O) = dH(x, g−1◦O) = dH(x, y) from Lemma A.3, then kλ(x, y) is distance-invariant, and hence isometry-invariant.\n\nIt suffices to prove Equation 6, i.e.,\n\nd(g ◦ ω) d(ω)\n\n=\n\n(cid:32)\n\n1 − (cid:13) (cid:13)g−1 ◦ O(cid:13) 2\n(cid:13) ∥g−1 ◦ O − ω∥2\n\n(cid:33)n−1\n\n,\n\nclearly it holds when g is an rotation, it suffices to show this for a translation isometry. Denote Inv(x) = x ∥x∥2 , then in the Poincar`e Ball model, all translation isometry takes the form\n\nTa(x) = −a + (1 − ∥a∥2) Inv(Inv(x) − a),\n\nwhere both x, a in the Poincar`e Ball model and Ta(a) = O, T −1\n\na (O) = a. Thus,\n\nTa(ω) = −a+(1−∥a∥2) Inv(Inv(ω)−a) = −a+(1−∥a∥2) Inv(ω−a) = −a+(1−∥a∥2)\n\nω − a\n\n∥ω − a∥2 ,\n\nwhere we use the fact ∥ω∥ = 1. Since the integral is taken over the unit sphere with ∥ω∥ = 1, ∥Ta(ω)∥ = 1, we consider only the mapping of Ta restricted to the first n − 1 (free) dimensions, with an abuse of notation, regard ω as an n − 1 dimensional vector. Then the Jacobian of Ta(ω) with respect to ω is\n\ndTa(ω) = (1 − ∥a∥2)d(\n\nω − a\n\n∥ω − a∥2 ),\n\nwe can calculate that\n\nd(\n\nω − a\n\n∥ω − a∥2 ) =\n\n∥ω − a∥2 dω − (ω − a) · 2(ω − a)⊺dω ∥ω − a∥4\n\n=\n\n=\n\n1\n\n∥ω − a∥2 ·\n\n∥ω − a∥2 − 2(ω − a) · (ω − a)⊺ ∥ω − a∥2\n\ndω\n\ndω\n\n∥ω − a∥2 ·\n\n(cid:32)\n\nIn−1,n−1 −\n\n2(ω − a) · (ω − a)⊺ ∥ω − a∥2\n\n(cid:33)\n\n,\n\nthen\n\ndTa(ω) dω\n\n=\n\n1 − ∥a∥2 ∥ω − a∥2\n\n(cid:32)\n\nIn−1,n−1 −\n\n2(ω − a) · (ω − a)⊺ ∥ω − a∥2\n\n(cid:33)\n\n,\n\nnote the relation that\n\ndet(I + xy⊺) = 1 + x⊺y,\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nthen\n\ndet(In−1,n−1 −\n\n2(ω − a) · (ω − a)⊺ ∥ω − a∥2\n\n) = 1 −\n\n2(ω − a)⊺ · (ω − a) ∥ω − a∥2\n\n= 1 − 2 = −1,\n\nthen the absolute value of determinant of the Jacobian is\n\n| det(\n\ndTa(ω) dω\n\n)| = | det\n\n(cid:32)\n\n1 − ∥a∥2 ∥ω − a∥2 (In−1,n−1 −\n\n2(ω − a) · (ω − a)⊺ ∥ω − a∥2\n\n(cid:33) )\n\n|\n\n(cid:32)\n\n(cid:32)\n\n=\n\n=\n\n(cid:33)n−1\n\n1 − ∥a∥2 ∥ω − a∥2\n\n(cid:13)g−1 ◦ O(cid:13) 1 − (cid:13) 2\n(cid:13) ∥ω − g−1 ◦ O∥2\n\n(cid:33)n−1\n\n,\n\nwith which a change of variable would give\n\nd(g ◦ ω) d(ω)\n\n=\n\n(cid:32)\n\n(cid:13)g−1 ◦ O(cid:13) 1 − (cid:13) 2\n(cid:13) ∥g−1 ◦ O − ω∥2\n\n(cid:33)n−1\n\n,\n\nwhich finishes the proof.\n\nProof of Theorem 4.1. As a result of Lemma A.3 and Lemma A.4, the expression for kλ(x, y) follows:\n\nkλ(x, y) =\n\n1 2\n\n· 2F1\n\n(cid:18) n − 1 2\n\n+ iλ,\n\nn − 1 2\n\n− iλ;\n\nn 2\n\n;\n\n1 2\n\n(1 − cosh(dH(x, y)))\n\n,\n\n(cid:19)\n\nwhere 2F1 is the hypergeometric function, we can also apply the Euler transformation to get\n\n(cid:18) 1 2\n\n·\n\n1 2\n\nkλ(x, y) =\n\nIf we let\n\n(1 + cosh(dH(x, y)))\n\n(cid:19)1− n\n\n2\n\n· 2F1\n\n(cid:18) 1 2\n\n+ iλ,\n\n1 2\n\n− iλ;\n\nn 2\n\n;\n\n1 2\n\n(1 − cosh(dH(x, y)))\n\n.\n\n(cid:19)\n\nz =\n\n1 2\n\n(1 − cosh(dH(x, y)))\n\nthen this is written more succinctly as\n\nkλ(x, y) =\n\n1 2\n\n· (1 − z)1− n\n\n2 · 2F1\n\n(cid:18) 1 2\n\n+ iλ,\n\n1 2\n\n− iλ;\n\n(cid:19)\n\n; z\n\n.\n\nn 2\n\nWe can also write this as a Legendre function,\n\nkλ(x, y) =\n\n=\n\n· (1 − z)1− n\n\n2 · Γ\n\n· (z(1 − z))\n\n2−n 4\n\n1 2\n1 2\n\n(cid:17)\n\n(cid:16) n 2\n(cid:16) n 2\n\n· Γ\n\n· z\n\n2−n 4\n\n· (1 − z)\n\nn−2 4\n\n· P 1− n\n\n2\n\n− 1\n\n2 +iλ (1 − 2z)\n\n(cid:17)\n\n· P 1− n\n\n2\n\n− 1\n\n2 +iλ (1 − 2z)\n\nObserve that\n\nand similarly\n\nz(1 − z) =\n\n1 4\n\n(cid:0)1 − cosh2(dH(x, y))(cid:1) =\n\n(cid:0)− sinh2(dH(x, y))(cid:1)\n\n1 4\n\n1 − 2z = cosh(dH(x, y)).\n\nThis is manifestly real because\n\nkλ(x, y) =\n\n=\n\n1 2\n\n1 2\n\n·\n\n·\n\n∞ (cid:88)\n\nk−1 (cid:89)\n\nk=0\n\nm=0\n\n∞ (cid:88)\n\nk−1 (cid:89)\n\nk=0\n\nm=0\n\n(cid:0) n−1\n\n2 + m + iλ(cid:1) (cid:0) n−1\n\n2 + m − iλ(cid:1)\n\n(cid:0) n\n\n2 + m(cid:1) (1 + m)\n\n·\n\n1 2\n\n(1 − cosh(dH(x, y)))\n\n(cid:0) n−1\n\n2 + m(cid:1)2 + λ2 2 + m(cid:1) (1 + m)\n\n(cid:0) n\n\n·\n\n1 2\n\n(1 − cosh(dH(x, y))) .\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nIf we draw HyLa features using a distribution ρ over λ, then the resulting approximated kernel will be\n\nk(x, y) =\n\n=\n\n1 2\n\n1 2\n\n·\n\n·\n\n(cid:90) ∞\n\n−∞ (cid:90) ∞\n\n−∞\n\nkλ(x, y) · ρ(λ) dλ\n\n2F1\n\n(cid:18) n − 1 2\n\n+ iλ,\n\nn − 1 2\n\n− iλ;\n\nn 2\n\n;\n\n1 2\n\n(1 − cosh(dH(x, y)))\n\n· ρ(λ) dλ.\n\n(cid:19)\n\nProof of Theorem 4.2. Denote\n\nφλ(z) =\n\n(cid:90)\n\nSn−1\n\nζλ,ω(z)dω =\n\n(cid:90)\n\nSn−1\n\nexp\n\n(cid:18)\n\n(\n\nn − 1 2\n\n(cid:19)\n\n+ iλ)⟨ω, z⟩H\n\ndω,\n\nwhich are basic spherical functions. For any x, y ∈ Bn, assume gy is an isometry that maps y to the origin, i.e., gy ◦ y = O, denote ˆx = gy ◦ x, then kλ(x, y) = kλ(ˆx, O) for any λ, note that\n\nkλ(ˆx, O) =\n\n=\n\n=\n\n1 2\n\n1 2\n1 2\n\n·\n\n·\n\n·\n\n(cid:90)\n\nSn−1\n\nexp\n\n(cid:18)\n\n(\n\nn − 1 2\n\n(cid:19)\n\n+ iλ)⟨ω, ˆx⟩H\n\nρ1(ω)dω\n\n1 Area(Sn−1) 1\nArea(Sn−1)\n\n(cid:90)\n\nSn−1\n\nexp\n\n(cid:18)\n\n(\n\nn − 1 2\n\n(cid:19)\n\n+ iλ)⟨ω, ˆx⟩H\n\ndω\n\nφλ(ˆx),\n\nwhere we use the fact that ρ1(ω) is a uniform distribution over the sphere. Assume the existence of an associated density ρ(λ) with the kernel, then\n\nk(x, y) = k(dH(x, y)) = k(dH(ˆx, O))\n\n=\n\n=\n\n=\n\n=\n\n=\n\n(cid:90) ∞\n\n−∞ (cid:90) ∞\n\n−∞\n\nkλ(x, y) · ρ(λ) dλ\n\nkλ(ˆx, O) · ρ(λ) dλ\n\n1 2\n\n1 2\n\n·\n\n·\n\n(cid:90) ∞\n\n−∞\n\n1 Area(Sn−1) (cid:90) ∞ 1\nArea(Sn−1)\n\n−∞\n\nφλ(ˆx) · ρ(λ) dλ\n\nφλ(ˆx) · ρ(λ) dλ\n\n1 πArea(Sn−1)\n\n(cid:90) ∞\n\n−∞\n\nρ(λ) λ tanh ( πλ 2 )\n\n· φλ(ˆx) · |c(λ)|−2 dλ\n\nwhere |c(λ)|−2 = πλ spherical transform (Helgason, 2022) of\n\n2 tanh πλ\n\n2 when λ ∈ R. Note that the last integral is exactly the inverse 2 ) , hence it can be derived in the reverse direction\n\nρ(λ) λ tanh ( πλ\n\nby taking the spherical transform of k(dH(ˆx, O)), i.e.,\n\nρ(λ) λ tanh ( πλ 2 )\n\n(cid:90)\n\n=\n\nBn\n\nk(dH(ˆx, O))φ−λ(ˆx)dˆx.\n\nHence ρ(λ) can be derived as\n\nρ(λ) = λ tanh (\n\n= λ tanh (\n\nπλ 2\nπλ 2\n\n)\n\n)\n\n(cid:90)\n\nk(dH(ˆx, O))φ−λ(ˆx)dˆx\n\nBn\n\n(cid:90)\n\n(cid:90)\n\nBn\n\n∂Bn\n\nk(dH(z, O)) exp\n\n(cid:18) (\n\nn − 1 2\n\n(cid:19)\n\n− iλ)⟨ω, z⟩H\n\ndωdz.\n\nTherefore, given an isometry-invariant positive semidefinite kernel k(x, y) = k(dH(x, y)), we can compute ρ(λ) following the above expression if it exists, then the rest of the theorem follows.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Averaged estimation error of HyLa to the kernel\n\nB CONCENTRATION OF THE KERNEL ESTIMATION\n\nThe readers may wonder whether there is a concentration behavior using the random variable ⟨φ(x), φ(y)⟩ to approximate k(x, y). Unfortunately, the eigenfunction φ(x) itself is not a subGaussian so as to derive a concentration bound in a straightforward way, but we do provide a numerical experiment to measure the estimation behavior. For the estimation in Figure 3, we sampled 1, 000 different eigenfunctions.\n\nWe first fix a set of 1, 000 points xi by uniformly sampling over 2-dimensional hyperbolic space, then approximate the kernel k(o, xi) using ⟨φ(xi), φ(o)⟩ by sampling with an increasing number of eigenfunctions, ranging from 50 to 1, 000. At last, we compute the mean absolute error of the estimation ⟨φ(xi), φ(o)⟩ to the true kernel value k(o, xi). We plot the mean estimation error in Figure 6, which seems to be an exponentially decay, it’s an interesting future work to investigate this estimation error.\n\nC NODE EMBEDDING VS FEATURE EMBEDDING\n\nWhen HyLa is adopted at node level, i.e., each vertex/node vi in the graph is associated with a hyperbolic embedding parameter zi ∈ Bd0. Then the inner product of HyLa features ⟨φ(zi), φ(zj)⟩ of vertex vi, vj approximates some kernel k(zi, zj). The optimization of zi encourages learning of the kernel on the hyperbolic space to solve the task. When HyLa is adopted at feature level, i.e., each column dimension of the node feature X ∈ Rn×d is associated with a hyperbolic embedding parameter zi ∈ Bd0. The HyLa feature associated to each vertex/node vi is then computed as (cid:80)d k=1 Xik = 1 if a row-normalization is applied on the original node features.\n\nk=1 Xikφ(zk), where (cid:80)d\n\nTherefore, the inner product of two node HyLa features is\n\nd (cid:88)\n\n⟨\n\nk=1\n\nXikφ(zk),\n\nd (cid:88)\n\nl=1\n\nXjlφ(zl)⟩ =\n\nd (cid:88)\n\nk,l=1\n\nXikXjl⟨φ(zk), φ(zl)⟩,\n\n20\n\n02004006008001000Sampling Number0.020.030.040.050.060.070.080.09Averaged Estimation ErrorUnder review as a conference paper at ICLR 2023\n\nTable 3: Node classification Dataset statistics. # Nodes # Edges Classes Features Setting\n\nDataset\n\nTransductive\n\nCora Citeseer Pubmed\n\nDisease Airport\n\nInductive Reddit\n\n2,708 3,327 19,717\n\n1,044 3,188\n\n233K\n\n5,429 4,732 44,338\n\n1,043 18,631\n\n11.6M\n\n7 6\n3\n\n2 4\n\n41\n\n1,433 3,703 500\n\n1,000 4\n\n602\n\nTable 4: Text classification Dataset statistics. # Docs # Words Average Length Classes Dataset\n\nR8 R52 Ohsumed MR\n\n7, 674 9, 100 7400 10662\n\n7688 8892 14157 18764\n\n65.72 69.82 135.82 20.39\n\n8 52 23 2\n\nwhich in expectation equals a linear combination of kernels, i.e., (cid:80)d k,l=1 XikXjlk(zk, zl). Therefore, it captures a much more complicated kernel relation on the hyperbolic space than directly embedding nodes.\n\nD TWO-STEP APPROACH\n\nFor the purpose of end-to-end learning, in our experiments, we jointly learn the embedding parameter Z and weight W in SGC during the training time, as detailed in subsection E.2. It’s possible to adopt a two-step approach, i.e., first pretrain a hyperbolic embedding, then fix the embedding and train the graph learning model only. In the first step, for example, optimization-based methods (Nickel & Kiela, 2017; 2018) and combinatorial construction methods (Sala et al., 2018; Sonthalia & Gilbert, 2020) can be adopted by supervising the graph connectivity. However, these methods only utilize the graph structure information, but ignore the node feature information X, which leads to a natural performance degradation. In comparison, as shown in experiments and analyzed in Appendix C, our end-to-end learning of HyLa can be used to embed features and enables learning a complex kernel representation to avoid this shortcoming. Intuitively, the graph connectivity information can be too general for downstreaming tasks which rely more on semantic information. It’s not clear to us how to encode the semantic information (node features) into embedding following e.g., (Nickel & Kiela, 2017; 2018).\n\nAnother way (Chami et al., 2019) of deriving a pretrained hyperbolic embedding that might take semantic information into consideration is to train a link prediction model, however, this method is not efficient as HGCN, shown in Figure 5.\n\nE EXPERIMENT DETAILS\n\nE.1 TASK AND DATASET\n\nWe provide a detailed description/table of used datasets in Table 3 and Table 4.\n\n1. Citation Networks. Cora, Citeseer and Pubmed Sen et al. (2008) are standard citation network benchmarks, where nodes represent papers, connected to each other via citations. We follow the standard splits Kipf & Welling (2016) with 20 nodes per class for training, 500 nodes for validation and 1000 nodes for test.\n\n2. Disease propagation tree Chami et al. (2019). This is tree networks simulating the SIR disease spreading model Anderson & May (1992), where the label is whether a node was infected or not and the node features indicate the susceptibility to the disease. We use dataset splits of 30/10/60% for train/val/test set.\n\n3. Airport. We take this dataset from Chami et al. (2019). This is a transductive dataset where nodes represent airports and edges represent the airline routes as from OpenFlights. Airport contains 3,188 nodes, each node has a 4 dimensional feature representing geographic information (longitude, latitude and altitude), and GDP of the country where the airport belongs to. For node classification, labels are chosen to be the population of the country where the airport belongs to. We use dataset splits of 524/524 nodes for val/test set.\n\n4. Reddit. This is a much larger graph dataset built from Reddit posts, where the label is the community, or “subreddit”, that a post belongs to. Two nodes are connected if the same user comments on both. We use a dataset split of 152K/24K/55K follows Hamilton et al. (2017); Chen et al. (2018), similarly, we evaluate HyLa inductively by following Wu et al.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Hyper-parameters for node classification.\n\nDataset\n\nDisease Airport Pubmed Citeseer Cora Reddit\n\nd0\n\n16 16 16 16 16 50\n\nd1\n\n100 1000 100 500 100 1000\n\nK\n\ns\n\n5 2\n5 5\n2 2\n\n1.0 0.01 0.1 1.0 1.0 0.5\n\nlr1\n\n0.05 0.5 0.5 0.1 0.1 0.1\n\nlr2\n\n# Epochs\n\n0.0001 0.1 0.001 0.001 0.01 0.001\n\n100 100 200 100 100 100\n\nTable 6: Hyper-parameters for text classification.\n\nTransductive Setting\n\nInductive Setting\n\nDataset\n\nR8 R52 Ohsumed MR\n\nd0\n\n50 50 50 30\n\nd1\n\n500 500 500 500\n\ns\n\n0.5 0.5 0.5 0.5\n\nlr1\n\n0.01 0.1 0.01 0.1\n\nlr2\n\n0.0001 0.0001 0.0001 0.0001\n\nd0\n\n50 50 50 50\n\nd1\n\n500 1000 1000 500\n\ns\n\n0.5 0.5 0.1 0.5\n\nlr1\n\n0.001 0.008 0.001 0.01\n\nlr2\n\n0.0001 0.0001 0.0001 0.0001\n\n(2019): we train on a subgraph comprising only training nodes and test with the original graph.\n\nE.2 TRAINING DETAILS.\n\nWe use HyLa together with SGC model as softmax(AKXW), where the HyLa feature matrix X ∈ Rn×d1 is derived from the hyperbolic embedding Z ∈ Rn×d0 using Algorithm 1. Specifically, we randomly sample constants of HyLa features X by sampling the boundary points ω uniformly from the boundary ∂Bn, eigenvalue constants λ from a zero-mean s-standard-deviation Gaussian and biases b uniformly from [0, 2π]. These constants remain fixed throughout training.\n\nWe use cross-entropy as the loss function and jointly optimize the low dimensional hyperbolic embedding Z and linear weight W simultaneously during training. Specifically, Riemannian SGD optimizer Bonnabel (2013) (of learning rate lr1) for Z and Adam Kingma & Ba (2014) optimizer (of learning rate lr2) for W. RSGD naturally scales to very large graph because the graph connectivity pattern is sufficiently sparse. We adopt early-stopping as regularization. We tune the hyper-parameter via grid search over the parameter space. Each hyperbolic embedding is initialized around the origin, by sampling each coordinate at random from [−10−5, 10−5].\n\nE.3 HYPER-PARAMETERS\n\nWe provide the detailed values of hyper-parameters for node classification and text classification in Table 5 and Table 6 respectively. Particularly, we fix K = 2 for the text classification task and train the model for a maximum of 200 epochs without using any regularization (e.g. early stopping). Also note that in the transductive text classification setting, HyLa is used at node level, hence the size of parameters will be proportional to the size of graph, in which case, d0 and d1 can not be too large so as to avoid OOM. In the inductive text classification setting, there is no such constraint as the dimension of lower level features is not very large itself. Please check the code for more details.\n\nF TIMING\n\nWe show the specific training timing statistics of different models on Pubmed dataset in Table 7. Particularly for HGCN model, in order to achieve the report performances, we follow the same training procedure using public code, which is divided into two stages: (1) a link prediction task on the dataset to learn hyperbolic embeddings, and (2) use the pretrained embeddings to train a MLP classifier. Hence, we add the timing of both stages as the timing for HGCN.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Training time on Pubmed.\n\nModel\n\nTiming (seconds)\n\nSGC GCN GAT HNN HGCN LGCN HyLa-SGC 3.51\n\n0.37 1.25 12.52 2.41 15.41 10.93\n\n23",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors suggest a simpler method: learn a hyperbolic embedding of the input, map it once to Euclidean space using a mapping that encodes geometric priors by respecting the isometries of hyperbolic space, and end with a standard Euclidean network.\n\n# Strength And Weaknesses\n\nThe idea is quite interesting, but the experimental results are strange, especially for the baselines in table 1 which are much lower than their real performance. \nFor scalability, the author only experiments on some small datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear and easy to read.\n\n# Summary Of The Review\n\nIn this paper, the authors propose a straightforward approach, learn a hyperbolic embedding of the input, then map it once to Euclidean space using a mapping that preserves the isometries of hyperbolic space while encoding geometric priors, and finally, finish with a standard Euclidean network. However, I am a little concerned about the experimental results.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nANALOG BITS: GENERATING DISCRETE DATA USING DIFFUSION MODELS WITH SELF-CONDITIONING\n\nTing Chen, Ruixiang Zhang†, Geoffrey Hinton Google Research, Brain Team {iamtingchen,ruixiangz,geoffhinton}@google.com\n\nABSTRACT\n\nWe present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete/categorical image generation, we significantly improve previous state-of-the-art on both CIFAR10 (which has 3K discrete 8-bit tokens) and IMAGENET 64×64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.\n\n1\n\nINTRODUCTION\n\nState-of-the-art generative models for discrete data, such as discrete images and text, are based on autoregressive modeling (Van den Oord et al., 2016; Salimans et al., 2017; Parmar et al., 2018; Child et al., 2019; Roy et al., 2021; Jun et al., 2020; Sutskever et al., 2014; Brown et al., 2020; Chowdhery et al., 2022), where the networks, often Transformers (Vaswani et al., 2017), are trained to predict each token given its preceding ones in a sequential manner or with causal attention masks. One major drawback of such approaches is that they typically require computation and memory that is quadratic to the dimension of data (e.g., sequence length or image size), leading to difficulties in modeling large images or sequences. Another drawback is that, during generation, autoregressive models generate one token at a time so the total number of sequential sampling steps is often the same as the dimension of data, making it slow in generating large images or long sequences.\n\nIn contrast, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020), or score-based generative models (Song & Ermon, 2019; 2020; Song et al., 2021), can model much higher dimensional data without running into computation and memory issues. During generation, diffusion models iteratively refine samples with a high degree of parallelism, so the total number of sequential sampling steps can be much less than the dimension of data. However, state-of-the-art diffusion models (Dhariwal & Nichol, 2021; Ho et al., 2022; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022) can only generate continuous data (mainly real valued pixels), and have not yet achieved results competitive with autoregressive models in generating discrete/categorical data, such as generating discrete/categorical images (Hoogeboom et al., 2021; Austin et al., 2021).\n\nIn this work, we propose a simple and generic approach for enabling continuous state diffusion models to generate discrete data. The key ingredient in our approach is analog bits: real numbers used to model the bits that represent the discrete data. Analog bits can be directly modeled by continuous state diffusion models, without requiring a discrete state space or re-formulation of the continuous\n\n†Work done as a student researcher at Google. Code at https://github.com/google-research/pix2seq.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Bit Diffusion: modeling discrete data using continuous diffusion models with analog bits.\n\ndiffusion process. At sampling time, the generated analog bits can be decoded into discrete variables by a simple thresholding operation. Our approach, as illustrated in Figure 1, is based on the following high-level conjecture. With strong continuous generative models (diffusion models in particular), it should not be too difficult to generate highly concentrated bimodal data where each real-valued analog bit is close to a binary bit. To reduce the prediction loss (such as negative log likelihood), the network has to model structures among analog bits that can actually lead to meaningful discrete variables after thresholding.\n\nBesides analog bits, we further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals that greatly improve the sample quality. We evaluate the proposed approach on both discrete image generation, and image-conditional text / caption generation. On discrete CIFAR-10 and IMAGENET 64×64, the proposed Bit Diffusion model significantly improves both existing discrete diffusion models but also the best autoregressive model. For example, on categorical CIFAR-10, the best autoregressive model (Jun et al., 2020) obtains a FID of 12.75, while our model (with 1/3 of the model size of the autoregressive model, using 100 instead of 3072 sequential inference steps) achieves a much better 6.93. For image captioning on MS-COCO dataset, our model achieves a result competitive with a strong autoregressive captioner based on a Transformer.\n\n2 METHOD\n\nPreliminaries We start with a short introduction to diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020; 2021). Diffusion models learn a series of state transitions to map noise (cid:15) from a known prior distribution to x0 from the data distribution. To learn this (reverse) transition from the noise distribution to the data distribution, a forward transition from x0 to xt is first defined: xt = (cid:112)γ(t) x0 + (cid:112)1 − γ(t) (cid:15),\n\n(1)\n\nwhere (cid:15) ∼ N (0, I), t ∼ U(0, T ) is a continuous variable, and γ(t) is a monotonically decreasing function from 1 to 0. Instead of directly learning a neural net to model the transition from xt to xt−∆, one can learn a neural net f (xt, t) to predict x0 (or (cid:15)) from xt, and estimate xt−∆ from xt and estimated ̃x0 (or ̃(cid:15)). This training of f (xt, t) is based on denoising with a (cid:96)2 regression loss: Lx0 = Et∼U (0,T ),(cid:15)∼N (0,1)(cid:107)f ((cid:112)γ(t) x0 + (cid:112)1 − γ(t) (cid:15), t) − x0(cid:107)2.\n\n(2)\n\nTo generate samples from a learned model, it follows a series of (reverse) state transition xT → xT −∆ → · · · → x0. This can be achieved by iteratively applying denoising function f on each state xt to estimate x0, and then make a transition to xt−∆ with the estimated ̃x0 (using transition rules such as those specified in DDPM (Ho et al., 2020) or DDIM (Song et al., 2020)). Note that state transitions in these diffusion models assume a continuous data space and state space. Therefore, one cannot directly apply it to model and generate discrete/categorical data.\n\nAnalog Bits A discrete data variable from an alphabet of size K can be represented using n = (cid:100)log2 K(cid:101) bits, as {0, 1}n. Due to the discreteness, existing work has to re-formulate continuous diffusion models by adopting a discrete data space and state space (Sohl-Dickstein et al., 2015; Hoogeboom et al., 2021; Austin et al., 2021). In contrast, we propose to simply cast the binary bits {0, 1}n into real numbers Rn for the continuous diffusion models 1. We term these real numbers analog bits since they learn to share the same bimodal values as binary bits but are modeled as real numbers. To draw samples, we follow the same procedure as sampling in a continuous diffusion model, except that we apply a quantization operation at the end by simply thresholding the generated analog bits. This yields binary bits which can be then converted into original discrete/categorical variables. Notably, there is no hard constraint to force the model to generate exact binary bits, but we\n\n1After casting as real numbers, one may also transform them by shifting and scaling from 0, 1 to −b, b.\n\n2\n\nToken_id=50 1 0 1diffusion models-1.0 1.0 -1.0 1.0int2bitcast as real numbersas input to-1.05 1.01 -1.02 0.98with shift & scale0 1 0 1thresholdingtoken_id=5bit2intgenerateAnalog BitsTraining SamplingPublished as a conference paper at ICLR 2023\n\nxt+∆\n\nxt\n\nxt−∆\n\nxt+∆\n\nxt\n\nxt−∆\n\n· · ·\n\n ̃x0\n\n ̃x(cid:48) 0\n\n· · ·\n\n· · ·\n\n ̃x0\n\n· · ·\n\n ̃x(cid:48) 0\n\n(a) Standard reverse diffusion steps.\n\n(b) Self-Conditioning on the previous x0 estimate.\n\nFigure 2: An illustration of reverse diffusion sampling steps (a) without or (b) with Self-Conditioning. ̃x0 denotes the estimation of data sample by the denoising network f at a sampling step. We propose to condition the network directly on its previously generated/estimated samples.\n\nexpect a strong continuous generative model to generate real numbers that exhibit very clear bimodal concentrations and this is what happens in our experiments.\n\nFor simplicity, we use the same regression loss function (Eq. 2) for modeling analog bits. However, it is possible to use other loss functions such as the cross entropy loss. We also note that the binary encoding mechanism for constructing analog bits is extensible as well (e.g., one-hot encoding). Extensions of loss functions and binary encoding are described in the appendix B.\n\nSelf-Conditioning Conditioning is a useful technique for improving diffusion models (Nichol & Dhariwal, 2021; Ho et al., 2022). However, a typical conditioning variable is either from some external sources, such as class labels (Nichol & Dhariwal, 2021) or low-resolution images from another network (Nichol & Dhariwal, 2021; Saharia et al., 2021; Ho et al., 2022). Here we propose a technique for the model to directly condition on previously generated samples of its own during the iterative sampling process, which can significantly improve the sample quality of diffusion models.\n\nIn a typical diffusion sampling process, the model iteratively predicts x0 (or (cid:15)) in order to progress the chain of mapping noise into data. However, as shown in Figure 2a, the previously estimated ̃x0 is simply discarded when estimating x0 from a new time step, i.e. the denoising function f (xt, t) does not directly depend on a previously estimated ̃x0. Here we consider a slightly different denoising function of f (xt, ̃x0, t) that also takes previous generated samples as its input, illustrated in Figure 2b. A simple implementation of Self-Conditioning is to concatenate xt with previously estimated ̃x0. Given that ̃x0 is from the earlier prediction of the model in the sampling chain, this comes at a negligible extra cost during sampling. In order to train the denoising function f (xt, ̃x0, t), we make some small changes to the training. With some probability (e.g., 50%), we set ̃x0 = 0 which falls back to modeling without Self-Conditioning. At other times, we first estimate ̃x0 = f (xt, 0, t) and then use it for Self-Conditioning. Note that we do not backpropagate through the estimated ̃x0 so the overall increase of training time is small (e.g., less than 25%).\n\nAsymmetric Time Intervals Besides Self-Conditioning, we identify another factor, time step t, that can also impact Bit Diffusion models. Time step t is an integral part of both denoising network f (xt, t) as well as the state transitions. During a typical reverse diffusion process, the model takes symmetric time intervals (i.e., ∆ as in t → t − ∆) for both the state transition and time reduction itself, resulting in the same/shared t for both arguments of f (xt, t). However, we find that, when taking large reverse steps, using asymmetric time intervals, implemented via a simple manipulation of time scheduling at generation, can lead to improved sampling quality for Bit Diffusion models.\n\nMore specially, with asymmetric time intervals during the sampling process, we have f (xt, t(cid:48)), where t(cid:48) = t + ξ and ξ is a small non-negative time difference parameter. Note that training remains unchanged, and the same/shared t is used for both arguments of the f (xt, t). Figure 3 illustrates the effect with a trained Bit Diffusion model, where it is asked to take two reversing steps from a state xt constructed using the forward diffusion, and it shows that asymmetric time intervals reduce the number of noisy pixels (after thresholding and converting back to discrete variables).\n\n ̃xt=0.1|xt=0.6\n\nf ( ̃xt=0.1, t(cid:48)=0.1)\n\nf ( ̃xt=0.1, t(cid:48)=0.3)\n\nf ( ̃xt=0.1, t(cid:48)=0.5)\n\nf ( ̃xt=0.1, t(cid:48)=0.7)\n\nFigure 3: When taking a large reverse step from xt=0.6 to xt=0.1 in Bit Diffusion with maximum time T = 1.0, we see that asymmetric time intervals with a positive time difference ξ improve the denoising quality of xt=0.1 (by reducing the number of noisy pixels).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nPutting it together Algorithm 1 and 2 summarize the training and sampling algorithms for the proposed Bit Diffusion model with Analog Bits, Self-Conditioning, and Asymmetric Time Intervals (via the td parameter). The proposed changes to the existing diffusion models are highlighted in blue. Note that unlike standard diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Nichol & Dhariwal, 2021), we use a continuous time parameterization between 0 and 1 instead of a fixed discrete time for maximal flexibility but they perform similarly. More details of the algorithm (including some important functions) can be found in Appendix A.\n\nAlgorithm 1 Bit Diffusion training algorithm.\n\nAlgorithm 2 Bit Diffusion sampling algorithm.\n\ndef train_loss(x):\n\n# Binary encoding: discrete to analog bits. x_bits = int2bit(x).astype(float) x_bits = (x_bits * 2 - 1) * scale\n\n# Corrupt data. t = uniform(0, 1) eps = normal(mean=0, std=1) x_crpt = sqrt(gamma(t)) * x_bits +\n\nsqrt(1 - gamma(t)) * eps\n\n# Compute self-cond estimate. x_pred = zeros_like(x_crpt) if self_cond and uniform(0, 1) > 0.5:\n\nx_pred = net(cat([x_crpt, x_pred], -1), t) x_pred = stop_gradient(x_pred)\n\n# Predict and compute loss. x_pred = net(cat([x_crpt, x_pred], -1), t) loss = (x_pred - x_bits)**2 return loss.mean()\n\ndef generate(steps, td=0):\n\nx_t = normal(mean=0, std=1) x_pred = zeros_like(x_t)\n\nfor step in range(steps):\n\n# Get time for current and next states. t_now = 1 - step / steps t_next = max(1 - (step+1+td) / steps, 0)\n\n# Predict x_0. if not self_cond:\n\nx_pred = zeros_like(x_t)\n\nx_pred = net(cat([x_t,x_pred],-1), t_now)\n\n# Estimate x at t_next. x_t = ddim_or_ddpm_step(\n\nx_t, x_pred, t_now, t_next)\n\n# Binary decoding to discrete data. return bit2int(x_pred > 0)\n\n3 EXPERIMENTS\n\nWe experiment with two different discrete data generation tasks, namely discrete/categorical image generation, and image captioning (image-conditional text generation).\n\n3.1 EXPERIMENTAL SETUP AND IMPLEMENTATION DETAILS\n\nDatasets We use CIFAR-10 (Krizhevsky et al., 2009) and IMAGENET 64×64 (Deng et al., 2009) 2 for image generation experiments. We adopt widely used FID (Heusel et al., 2017) as the main evaluation metric, and it is computed between 50K generated samples and the whole training set. For image captioning, following (Chen et al., 2022), we use MS-COCO 2017 captioning dataset (Lin et al., 2014).\n\nBinary encoding Each pixel consists of 3 sub-pixels (RGB channels), and each sub-pixel is an integer in [0, 256) representing the intensity. Standard continuous generative models cast RGB channels as real numbers and normalize them in [−1, 1]. For discrete image generation, we consider three discrete encoding for sub-pixels, namely UINT8, GRAY CODE, and UINT8 (RAND). In UINT8, we use 8-bit binary codes converted from the corresponding sub-pixel integer in [0, 256). In GRAY CODE, we assign 8-bit binary codes uniquely to each sub-pixel integer such that two adjacent integers only differ by 1 bit. And in UINT8 (RAND), we assign 8-bit binary codes to every sub-pixel integer by randomly shuffling the integer-to-bits mapping in UINT8. The binary codes in UINT8 and GRAY CODE are loosely correlated with its original sub-pixel intensities, while UINT8 (RAND) has no correlation so each sub-pixel is a categorical variable. The details of the binary codes and their correlations with sub-pixel intensity can be found in the appendix C. We shift and scale the binary bits from 0, 1 to −1, 1 for the analog bits.\n\nFor image captioning, we follow (Chen et al., 2022), and use sentencepiece (Kudo & Richardson, 2018) with a vocabulary of size 32K to tokenize the captions. After tokenization, we encode each token into 15 analog bits using the binary codes converted from the corresponding integer. We set the maximum number of tokens to 64 so the total sequence length is 960 bits. Since we directly model bits, it is also possible to directly work with their byte representations without a tokenizer, but we leave this for future work.\n\n2Following (Brock et al., 2018; Nichol & Dhariwal, 2021), we center crop and area downsample images to\n\n64×64.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of FIDs on unconditional and class-conditional CIFAR-10. Note that both UINT8 and GRAY CODE are only partial/weakly ordinal (see Appendix C). Our Bit Diffusion achieves state-of-the-art FIDs in generating discrete images, beating the best autoregressive model.\n\nMethod\n\nState space\n\nFID (Unconditional)\n\nFID (Conditional)\n\nOn continuous pixels (as reference):\n\nDDPM (Ho et al., 2020) DDPM (our reproduction)\n\nOn discrete (partial) ordinal pixels:\n\nD3PM Gauss+Logistic (Austin et al., 2021) τ LDR-10 (Campbell et al., 2022) Bit Diffusion on UINT8 Bit Diffusion on GRAY CODE\n\nOn categorical pixels:\n\nContinuous Continuous\n\nDiscrete Discrete Continuous Continuous\n\nD3PM uniform (Austin et al., 2021) D3PM absorbing (Austin et al., 2021) Autoregressive Transformer (Jun et al., 2020) Bit Diffusion on UINT8 (RAND)\n\nDiscrete Discrete Discrete Continuous\n\n3.17 3.14\n\n7.34 3.74 3.48 3.86\n\n51.27 30.97 12.75 6.93\n\n- 2.95\n\n- -\n2.72 2.94\n\n- -\n- 6.43\n\nTable 2: Comparison of FIDs on class-conditional IMAGENET 64×64. The corresponding samples can be found in Figure 4 and 11.\n\nDDPM (our repo.) on continuous pixels\n\nBit Diffusion on UINT8\n\nBit Diffusion on GRAY CODE\n\nBit Diffusion on UINT8 (RAND)\n\n3.43\n\n4.84\n\n5.14\n\n8.76\n\nArchitecture We use the U-Net architecture (Ho et al., 2020; Nichol & Dhariwal, 2021; Ronneberger et al., 2015) for image generation. For CIFAR-10, we use a single channel dimension of 256, 3 stages and 3 residual blocks (He et al., 2016) per stage, with a total of 51M parameters. We only use dropout (Srivastava et al., 2014) of 0.3 for continuous diffusion models on CIFAR-10. For IMAGENET 64×64, following (Nichol & Dhariwal, 2021), we use a base channel dimension of 192, multiplied by 1,2,3,4 in 4 stages and 3 residual blocks per stage, which account for a total of 240M parameters 3. For UINT8 (RAND) encoding, we find the following “softmax factorization” architectural tweak on the final output layer can lead to a better performance. Instead of using a linear output layer to predict analog bits directly, we first predict a probability distribution over 256 classes per sub-pixel (with each class corresponds to one of the 256 different 8-bit codes), and then map class distribution into analog bits by taking weighted average over all 256 different 8-bit codes.\n\nFor image captioning, we follow the architecture used in (Chen et al., 2021; 2022), with a pre-trained image encoder using the object detection task, for both autoregressive baseline as well as the proposed method. Both decoders are randomly initialized 6-layer Transformer (Vaswani et al., 2017) decoder with 512 dimension per layer. For the autoregressive decoder, the token attention matrix is offset by the causal masks, but it is non-masked all-to-all attention for our Bit Diffusion.\n\nOther settings We train our models with the Adam optimizer (Kingma & Ba, 2014). For CIFAR-10, we train the model for 1.5M steps with a constant learning rate of 0.0001 and batch size of 128. For IMAGENET 64×64, we train the model for 500K steps with a constant learning rate of 0.0002 4 and batch size of 1024. For Bit Diffusion, we use Self-Conditioning by default, unless otherwise specified. We use an exponential moving average of the weights during training with a decay factor of 0.9999. For our best image generation results, we sweep over a few sampling hyper-parameters, such as sampler (DDIM vs DDPM), sampling steps in {100, 250, 400, 1000}, and time difference in {0., 0.01, 0.1, 0.2, 0.5}.\n\n3.2 DISCRETE IMAGE GENERATION\n\nWe compare our model against state-of-the-art generative models (Ho et al., 2020; Austin et al., 2021; Campbell et al., 2022; Jun et al., 2020) on generating discrete CIFAR-10 images in Table 1. Our model\n\n3Our model is about 30M parameters smaller than that used in (Nichol & Dhariwal, 2021) as we drop the\n\nmiddle blocks for convenience, which may have a minor effect on performance.\n\n4For UINT8 (RAND) encoding, we use learning rate of 0.0001 instead.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) DDPM on continuous pixels (FID=3.43)\n\n(b) Bit Diffusion on UINT8. (FID=4.84)\n\n(c) Bit Diffusion on GRAY CODE (FID=5.14)\n\n(d) Bit Diffusion on UINT8 (RAND) (FID=8.76)\n\nFigure 4: Class-conditional generations on continuous v.s. discrete ImageNet 64×64. Each row represents random samples conditioned on a class, and the classes are adopted from (Nichol & Dhariwal, 2021), namely, 9: ostrich, 11: goldfinch, 130: flamingo, 141: redshank, 154: pekinese, 157: papillon, 97: drake and 28: spotted salamander. More samples from random classes are shown in Figure 11.\n\nachieves better results compared to both existing discrete diffusion models and the best autoregressive model. When compared to continuous diffusion models (i.e., DDPM), our Bit Diffusion models on UINT8 and GRAY CODE can achieve similar performance.\n\nDiscrete generation of IMAGENET 64×64 is significantly harder than CIFAR-10, and we have not found other competing methods that report FIDs, so we only compare the proposed method against DDPM on continuous pixels. Results are shown in Table 2. We find that the diffusion model on continuous pixels has the best FID while the diffusion model on UINT8 (RAND), i.e., categorical data, has the worst FID, indicating the increase of hardness when removing intensity/order information in sub-pixels. Note that, in these experiments, there is no extra model capacity to compensate for the loss of intensity/order information since the model sizes are the same. Figure 4 shows generated images of different diffusion models on continuous and discrete IMAGENET 64×64. Despite the differences in FIDs, visually these samples look similar.\n\nAblation of Self-Conditioning Figure 5 shows the effectiveness of the Self-Conditioning technique in both Bit Diffusion and continuous diffusion models. Note that the experiments are performed in three settings, namely CIFAR-10 with UINT8, CIFAR-10 with UINT8 (RAND), and IMAGENET\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n(a) CIFAR-10, UINT8.\n\n(b) CIFAR-10, UINT8 (RAND).\n\n(c) IMAGENET 64×64.\n\nFigure 5: Self-conditioning is a generic technique that not only greatly improves Bit Diffusion but also leads to improved results for continuous diffusion models.\n\n(a) UINT8.\n\n(b) UINT8 (RAND).\n\nFigure 6: Effect of time difference in class-conditional IMAGENET 64×64. Optimal time difference shrinks to zero as the number of sampling steps increases. For 100 sampling steps, non-zero time difference leads to improved FIDs.\n\n64×64 with continuous pixels, where the only difference for pairs in each setting is whether the Self-Conditioning is used. For CIFAR-10, we find that Self-Conditioning greatly improves the performance across different binary encodings. We also notice that for Bit Diffusion, predicting x0 is much more effective than predicting (cid:15). For IMAGENET 64×64, we find that the proposed Self-Conditioning also leads to improved FIDs for continuous diffusion (i.e., DDPM). Therefore, we conclude that Self-Conditioning by itself is a generic technique that can benefit diffusion models on both continuous and discrete data.\n\nAblation of asymmetric time intervals Figure 6 shows the FID on generated IMAGENET 64×64 samples as we vary the time difference parameter during the sampling process. We find that as the number of steps increases (from 100 to 400), the optimal time difference shrinks to 0. For 100 steps, a non-zero time difference leads to a significant improvement of FID. We also note that for Bit Diffusion on UINT8 (RAND), using 400 sampling steps actually leads to a drastically worse sample quality than using 100 steps. This is related to how the Self-Conditioning is applied and we present alternative Self-Conditioning sampling strategies in the Appendix G, some of which lead to improved FIDs at a cost of longer sampling time.\n\nConcentration of generated analog bits Figure 7 visualizes the distribution of generated analog bits from 64 generated images on IMAGENET 64×64. Although there is no hard constraint on the analog bits being binary / bimodal, the generated ones are highly concentrated on two modes, which makes the thresholding / quantization easy and robust.\n\n(a) UINT8.\n\n(b) GRAY CODE.\n\n(c) UINT8 (RAND).\n\nFigure 7: Histogram distribution (50 bins) of the analog bits from 64 randomly generated IMAGENET 64×64 samples at ̃x0, with 100 DDIM steps. Most of the generated analog bits are very concentrated.\n\n7\n\nFID0102030Predict epsPredict x0W/o Self-CondW/ Self-CondFID0100200300400Predict epsPredict x0W/o Self-CondW/ Self-CondFID0246810Predict epsPredict x0W/o Self-CondW/ Self-CondTime differenceFID0102030400.000.250.500.751.00 DDIM (100 steps) DDIM (400 steps) DDPM (100 steps) DDPM (400 steps)Time differenceFID8102040600.000.250.500.751.00 DDIM (100 steps) DDIM (400 steps) DDPM (100 steps) DDPM (400 steps)1.00.50.00.51.0Generated analog bits01020304050Percentage1.00.50.00.51.0Generated analog bits01020304050Percentage1.00.50.00.51.0Generated analog bits01020304050PercentagePublished as a conference paper at ICLR 2023\n\n3.3\n\nIMAGE CAPTIONING\n\nWe compare our Bit Diffusion model with an autoregressive Transformer baseline (Chen et al., 2022). As mentioned, both models have similar architectures, with an object detection pretrained (Chen et al., 2021) image encoder, and a randomly initialized Transformer (Vaswani et al., 2017) decoder. Table 3 presents the main comparison. Overall, our model achieves similar performance as the autoregressive model. We find that generally it only needs about 10 steps for the model to achieve good results, despite that there are a total of maximum 960 bits for caption that the model has to model. We find that the asymmetric time intervals play an important role in the final performance of our model, as demonstrated in Table 4, especially when sampling steps are fewer.\n\nTable 3: Image captioning results on MS-COCO dataset with a randomly initialized text decoder.\n\nMethod\n\nBLEU-4 CIDEr ROUGE-L\n\nAutoregressive Transformer\n\nBit Diffusion (5 steps) Bit Diffusion (10 steps) Bit Diffusion (20 steps) Bit Diffusion (40 steps)\n\n33.9\n\n31.5 34.5 34.7 34.4\n\n1.18\n\n1.00 1.13 1.15 1.15\n\n0.57\n\n0.55 0.57 0.58 0.57\n\nTable 4: Asymmetric time intervals significantly improves the performance of Bit Diffusion.\n\n0.0\n\n17.1 17.6 20.0 20.7\n\n1.0\n\n27.8 26.3 27.9 27.5\n\n2.0\n\n30.8 30.7 30.6 30.7\n\n5 steps 10 steps 20 steps 40 steps\n\nTime difference 4.0\n\n5.0\n\n3.0\n\n31.5 32.6 32.0 32.2\n\n31.6 33.4 32.3 32.9\n\n31.5 34.0 33.9 33.2\n\n6.0\n\n31.5 34.3 34.4 33.8\n\n7.0\n\n8.0\n\n31.5 34.5 34.7 34.4\n\n31.6 34.6 34.5 34.4\n\nTable 5 provides some generated samples of our model when different inference steps are used. The model makes mistakes when the sampling steps are too few, and the mistakes may not always be interpretable due to that the model directly predicts the bits behind the tokenized word pieces and a small difference in bits can lead to total different words.\n\nTable 5: Generated image captions under different number of sampling steps.\n\nSteps=1: A group of diets in for foraa\n\nSteps=2: A group of elephants in in a\\ufffd.\n\nSteps=5: A group of elephants standing in a Irish.\n\nSteps=10: A group of elephants standing by a fence.\n\nSteps=1: A says man fora You a\\u0000.\n\nSteps=2: A says man a skateboard’ aa.\n\nSteps=5: A man on a skateboard on a skateboard.\n\nSteps=10: A man is doing a trick on a skateboard.\n\n4 RELATED WORK\n\nAutoregressive models for discrete data Autoregressive models have demonstrated state-of-the-art results when it comes to generating discrete data. In particular, text generation, or language modeling, is dominated by autoregressive approaches (Sutskever et al., 2014; Brown et al., 2020; Chowdhery et al., 2022). Autoregressive models are also applied to discrete/categorical image generation (Van den Oord et al., 2016; Salimans et al., 2017; Parmar et al., 2018; Child et al., 2019; Roy et al., 2021; Jun et al., 2020; Chen et al., 2020a), where they work well on small image resolutions. However, the computation cost and memory requirement increase drastically (typically in a quadratic relation) as\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nthe size of sequence or the image resolution increase, so it becomes very challenging to scale these approaches to data with large dimensions.\n\nDiffusion models for discrete data State-of-the-art diffusion models (Dhariwal & Nichol, 2021; Ho et al., 2022; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022) cannot generate discrete or categorical data. Existing extensions of these continuous diffusion models to discrete data are based on both discrete data space and state space (Sohl-Dickstein et al., 2015; Hoogeboom et al., 2021; Austin et al., 2021; Campbell et al., 2022). Compared to discrete state space, continuous state space is more flexible and potentially more efficient. Our approach is also compatible with both discrete and continuous time, and does not require re-formulation of existing continuous models, thus it is simpler and can potentially be plugged into a broader family of generative models.\n\nAnother line of discrete diffusion models is based on the embedding of discrete data (Li et al., 2022). One can also consider our binary encoding with analog bits as a simple fixed encoder, and the decoding / quantization of bimodal analog bits is easy and robust via a simple thresholding operation. In contrast, the quantization of real numbers in generated continuous embedding vectors may contain multiple modes per dimension, leading to potential difficulty in thresholding/quantization.\n\nNormalizing Flows for discrete data Normalizing Flows (Rezende & Mohamed, 2015; Dinh et al., 2017; Kingma & Dhariwal, 2018) are a powerful family of generative models for high-dimensional continuous distributions based on some invertible mapping. However, straightforward application of flow-based models on categorical data is limited due to the inherent challenges on discrete support. Discrete flows (Tran et al., 2019; Hoogeboom et al., 2019; Lindt & Hoogeboom, 2021) introduce invertible transformations of random variables in discrete space without the need of computing the log-determinant of Jacobian. Other works (Lippe & Gavves, 2021; Hoogeboom et al., 2021; Tan et al., 2021) introduce various embedding methods for transforming discrete data into continuous space with disjoint support, which can be interpreted as a variational inference problem (Theis et al., 2015) with different dequantization distribution families. Several works (Kingma et al., 2016; Ziegler & Rush, 2019; Zhang et al., 2020) also explore normalizing flows on discrete data under the Variational Autoencoders (Kingma & Welling, 2013) framework by enriching the prior. Compared to our diffusion-based approach, these models suffer from strict invertible restrictions on network architecture, thus limiting their capacity.\n\nOther generative models for discrete data Other generative models, such as Varational Autoencoders (VAE) (Kingma & Welling, 2013), Generateive Adversarial Networks (GAN) (Goodfellow et al., 2014; Yu et al., 2017; Che et al., 2017; Hjelm et al., 2017; Fedus et al., 2018) have also been applied to generate discrete data. These methods have not yet achieved the level of performance as autoregressive models on tasks such as discrete image generation or text generation, in terms of sample quality or data likelihood. Potentially, the proposed analog bits can also be applied to these continuous generative models, by having the networks directly model and generate analog bits, but it is not explored in this work.\n\nOther related work The proposed Self-Conditioning technique shares some similarities with selfmodulation in GANs (Chen et al., 2018a) (where the earlier latent state can directly modulate the later latent states) and SUNDAE (Savinov et al., 2021) (where an inference step is incorporated for denoising).\n\n5 CONCLUSION\n\nWe introduce a simple and generic technique that enables continuous state diffusion models to generate discrete data. The main idea is to encode discrete or categorical data into bits and then model these bits as real numbers that we call analog bits. We also propose two simple techniques, namely Self-Conditioning (i.e., condition the diffusion models directly on their previously generated samples) and Asymmetric Time Intervals, that lead to improved sample quality. We demonstrate that our approach leads to state-of-the-art results in discrete / categorical image generation, beating the best autoregressive model. In an image-conditional text generation task on MS-COCO dataset, we also achieve competitive results compared to autoregressive models. One limitation of our approach, similar to other existing diffusion models, is that they still require a significant number of inference steps for generating good (image) samples. However, we expect that future improvements from diffusion models for continuous data can also transfer to discrete data using analog bits.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe would like to thank Priyank Jaini, Kevin Swersky for providing helpful feedback to our draft. Our implementation is partially based on the Pix2Seq codebase, and we thank Lala Li, Saurabh Saxena, for their contributions to the Pix2Seq codebase.\n\nREFERENCES\n\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981– 17993, 2021. (Cited on 1, 2, 5, 9)\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image\n\nsynthesis. arXiv preprint arXiv:1809.11096, 2018. (Cited on 4)\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. (Cited on 1, 8)\n\nAndrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. arXiv preprint arXiv:2205.14987, 2022. (Cited on 5, 9)\n\nTong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983, 2017. (Cited on 9)\n\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pp. 1691–1703. PMLR, 2020a. (Cited on 8)\n\nTing Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative adversarial\n\nnetworks. arXiv preprint arXiv:1810.01365, 2018a. (Cited on 9)\n\nTing Chen, Martin Renqiang Min, and Yizhou Sun. Learning k-way d-dimensional discrete codes for compact embedding representations. In International Conference on Machine Learning, pp. 854–863. PMLR, 2018b. (Cited on 14)\n\nTing Chen, Lala Li, and Yizhou Sun. Differentiable product quantization for end-to-end embedding compression.\n\nIn International Conference on Machine Learning, pp. 1617–1626. PMLR, 2020b. (Cited on 14)\n\nTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\n\nframework for object detection. arXiv preprint arXiv:2109.10852, 2021. (Cited on 5, 8)\n\nTing Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A unified sequence\n\ninterface for vision tasks. arXiv preprint arXiv:2206.07669, 2022. (Cited on 4, 5, 8)\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\n\narXiv preprint arXiv:1904.10509, 2019. (Cited on 1, 8)\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. (Cited on 1, 8)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. (Cited on 4)\n\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural\n\nInformation Processing Systems, 34:8780–8794, 2021. (Cited on 1, 9)\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In International\n\nConference on Learning Representations, 2017. (Cited on 9)\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via filling in the_. arXiv\n\npreprint arXiv:1801.07736, 2018. (Cited on 9)\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. (Cited on 9)\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. (Cited on 5)\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. (Cited on 4)\n\nR Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, and Yoshua Bengio. Boundary-\n\nseeking generative adversarial networks. arXiv preprint arXiv:1702.08431, 2017. (Cited on 9)\n\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep\n\nGenerative Models and Downstream Applications, 2021. (Cited on 19)\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\n\nInformation Processing Systems, 33:6840–6851, 2020. (Cited on 1, 2, 4, 5)\n\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:47–1, 2022. (Cited on 1, 3, 9)\n\nEmiel Hoogeboom, Jorn Peters, Rianne Van Den Berg, and Max Welling. Integer discrete flows and lossless\n\ncompression. Advances in Neural Information Processing Systems, 32, 2019. (Cited on 9)\n\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34: 12454–12465, 2021. (Cited on 1, 2, 9)\n\nHeewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, and Ilya Sutskever. Distribution augmentation for generative modeling. In International Conference on Machine Learning, pp. 5006–5019. PMLR, 2020. (Cited on 1, 2, 5, 8)\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n\n2014. (Cited on 5)\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\n(Cited on 9)\n\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in\n\nneural information processing systems, 31, 2018. (Cited on 9)\n\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016. (Cited on 9)\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. (Cited on\n\n4)\n\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and\n\ndetokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. (Cited on 4)\n\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm\n\nimproves controllable text generation. arXiv preprint arXiv:2205.14217, 2022. (Cited on 9)\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pp. 740–755. Springer, 2014. (Cited on 4)\n\nAlexandra Lindt and Emiel Hoogeboom. Discrete denoising flows. arXiv preprint arXiv:2107.11625, 2021.\n\n(Cited on 9)\n\nPhillip Lippe and Efstratios Gavves. Categorical normalizing flows via continuous transformations. In Interna-\n\ntional Conference on Learning Representations, 2021. (Cited on 9)\n\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022. (Cited on 16)\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. (Cited on 1, 9)\n\nAlexander Quinn Nichol and Prafulla Dhariwal.\n\nImproved denoising diffusion probabilistic models.\n\nIn\n\nInternational Conference on Machine Learning, pp. 8162–8171. PMLR, 2021. (Cited on 3, 4, 5, 6)\n\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pp. 4055–4064. PMLR, 2018. (Cited on 1, 8)\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\n\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. (Cited on 1, 9)\n\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference\n\non machine learning, pp. 1530–1538. PMLR, 2015. (Cited on 9)\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234–241. Springer, 2015. (Cited on 5)\n\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. (Cited on 1, 8)\n\nChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image\n\nsuper-resolution via iterative refinement. arXiv preprint arXiv:2104.07636, 2021. (Cited on 3)\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. (Cited on 1, 9)\n\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017. (Cited on 1, 8)\n\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. Step-unrolled\n\ndenoising autoencoders for text generation. arXiv preprint arXiv:2112.06749, 2021. (Cited on 9)\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\n\nunits. arXiv preprint arXiv:1508.07909, 2015. (Cited on 14)\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015. (Cited on 1, 2, 4, 9)\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\n\narXiv:2010.02502, 2020. (Cited on 1, 2)\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances\n\nin Neural Information Processing Systems, 32, 2019. (Cited on 1)\n\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in\n\nneural information processing systems, 33:12438–12448, 2020. (Cited on 1)\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. (Cited on 1, 2)\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1): 1929–1958, 2014. (Cited on 5)\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Advances\n\nin neural information processing systems, 27, 2014. (Cited on 1, 8)\n\nShawn Tan, Chin-Wei Huang, Alessandro Sordoni, and Aaron Courville. Learning to dequantise with truncated\n\nflows. In International Conference on Learning Representations, 2021. (Cited on 9)\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nLucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv\n\npreprint arXiv:1511.01844, 2015. (Cited on 9)\n\nDustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole. Discrete flows: Invertible generative\n\nmodels of discrete data. Advances in Neural Information Processing Systems, 32, 2019. (Cited on 9)\n\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. (Cited on 1, 8)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. (Cited on 1, 5, 8)\n\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. (Cited on 9)\n\nZijun Zhang, Ruixiang Zhang, Zongpeng Li, Yoshua Bengio, and Liam Paull. Perceptual generative autoencoders.\n\nIn International Conference on Machine Learning, pp. 11298–11306. PMLR, 2020. (Cited on 9)\n\nZachary Ziegler and Alexander Rush. Latent normalizing flows for discrete sequences.\n\nIn International\n\nConference on Machine Learning, pp. 7673–7682. PMLR, 2019. (Cited on 9)\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA MORE DETAILS OF ALGORITHM 1 AND 2\n\nAlgorithm 3 and 4 provide more detailed implementations of functions in Algorithm 1 and 2.\n\nAlgorithm 3 Binary encoding and decoding algorithms (in Tensorflow).\n\nimport tensorflow as tf\n\ndef int2bit(x, n=8):\n\n# Convert integers into the corresponding binary bits. x = tf.bitwise.right_shift(tf.expand_dims(x, -1), tf.range(n)) x = tf.math.mod(x, 2) return x\n\ndef bit2int(x):\n\n# Convert binary bits into the corresponding integers. x = tf.cast(x, tf.int32) n = x.shape[-1] x = tf.math.reduce_sum(x * (2 ** tf.range(n)), -1) return x\n\nAlgorithm 4 xt estimation with DDIM / DDPM updating rules.\n\ndef gamma(t, ns=0.0002, ds=0.00025):\n\n# A scheduling function based on cosine function. return numpy.cos(((t + ns) / (1 + ds)) * numpy.pi / 2)**2\n\ndef ddim_step(x_t, x_pred, t_now, t_next):\n\n# Estimate x at t_next with DDIM updating rule. γnow = gamma(t_now) γnext = gamma(t_next) x_pred = clip(x_pred, -scale, scale) γnow * x_pred) eps =\n\n* (x_t -\n\n1√\n\n√\n\nγnext * x_pred +\n\n1 − γnext * eps\n\n√\n\n1−γnow √\n\nx_next = return x_next\n\ndef ddpm_step(x_t, x_pred, t_now, t_next):\n\n# Estimate x at t_next with DDPM updating rule. γnow = gamma(t_now) αnow = gamma(t_now) / gamma(t_next) σnow = sqrt(1 - αnow) z = normal(mean=0, std=1) x_pred = clip(x_pred, -scale, scale) γnow * x_pred) eps =\n\n* (x_t -\n\n1√\n\n√\n\nαnow * (x_t - 1−αnow\n\n1−γnow\n\n√\n\n* eps) + σnow * z\n\n1−γnow\n\nx_next = return x_next\n\n1√\n\nB ALTERNATIVE BINARY ENCODING AND LOSS FUNCTIONS\n\nB.1 ANALOG BITS BASED ON ONE-HOT ENCODING\n\nAn alternative binary encoding to the base-2 encoding of the discrete data used in the main paper, is the one-hot encoding, where a discrete variable is represented as a vector whose length is the same as the vocabulary size K, with a single slot being 1 and the rest being 0. The resulting one-hot vector can be similarly treated as analog bits and modeled by continuous state diffusion models. To obtain discrete variables corresponding to the generated analog bits, we use an arg max operation over all candidate categories, instead of the thresholding operation in base-2 analog bits. Note that the one-hot encoding requires K bits, which is less efficient compared to base-2 encoding that only requires (cid:100)log2 K(cid:101) bits, especially for large K. 5\n\n5Although, one can reduce the vocabulary size K by using sub-tokenization (e.g., subword (Sennrich et al.,\n\n2015)), or learned discrete codes (Chen et al., 2018b; 2020b).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nB.2 SIGMOID CROSS ENTROPY LOSS\n\nAs we use (cid:96)2 loss by default for its simplicity and compatibility with continuous diffusion models. The proposed Bit Diffusion models can work with other loss functions too. Since the analog bits are bimodal, we can use the following sigmoid cross entropy loss:\n\nLx0,xt,t = log σ(x0f (xt, t)), where we assume x0 ∈ {−1, 1}n, and σ is a sigmoid function. During the sampling process, we use 2σ(f (xt, t)) − 1 as the output of denoising network.\n\nB.3 SOFTMAX CROSS ENTROPY LOSS\n\nFor one-hot analog bits, one could also add a softmax activation function for the output of denosing network f , and use the following softmax cross entropy loss:\n\nLx0,xt,t = x0 log softmax(f (xt, t)), where we assume x0 ∈ {0, 1}n which is the one-hot representation.\n\nB.4 PRELIMINARY EXPERIMENTS\n\nTable 6 presents FIDs of Bit Diffusion models with different types of analog bits and loss functions on unconditional CIFAR-10. Note that it is possible some of these results can be improved by more tuning of hyper-parameters or tweaks of the network, but we do not focus on them in this work.\n\nTable 6: FIDs of Bit Diffusion models with different types of analog bits and loss functions on unconditional CIFAR-10.\n\n(cid:96)2 loss\n\nLogistic loss\n\nSoftmax loss\n\nONE HOT UINT8 GRAY CODE UINT8 (RAND)\n\n46.32 3.48 3.86 6.93\n\n26.82 3.53 3.71 49.29\n\n29.49 -\n- -\n\nC ON BINARY ENCODING OF PIXELS: UINT8, GRAY CODE, UINT8 (RAND)\n\nIn the main paper, we describe three different types of binary encodings of pixels. Here we provide additional detail on how we generate UINT8 (RAND): we first apply a random permutation to 256 sub-pixel values, and then assign the binary binary bits of permuted integers to the non-permuted integers. For example, assume 0 is mapped to 228 after the permutation, the analog bits of 0 would be the binary bits of 228. The random permutation is generated by numpy.random.seed(42); numpy.random.shuffle(numpy.arange(256)).\n\n(a) UINT8.\n\n(b) GRAY CODE.\n\n(c) UINT8 (RAND).\n\nFigure 8: Correlation between (absolute) difference in subpixel intensity and the Hamming distance of the corresponding binary bits.\n\nFigure 8 show the correlation between Hamming distance of three different binary encodings we use and the (absolute) difference of sub-pixel intensity. This is done by taking every pair of subpixel\n\n15\n\n12345678Hamming distance050100150200250Abs difference in intensity12345678Hamming distance050100150200250Abs difference in intensity12345678Hamming distance050100150200250Abs difference in intensityPublished as a conference paper at ICLR 2023\n\nintegers (in [0, 256)), compute their absolute difference, as well as the Hamming distance between the corresponding binary bits. We find that both UINT8 and GRAY CODE exhibit partial correlation between the two quantities (with different correlation patterns), meaning that these codes partially contain the order information about the original sub-pixel intensity. However, UINT8 (RAND) exhibits no correlation between hamming distance and sub-pixel intensity, indicating the order information is fully removed, thus can be considered as categorical data.\n\nD A TOY EXAMPLE ON CONTINUOUS MODELING OF DISCRETE VARIABLES\n\nAn intuitive toy example of how a continuous generative model can generate binary data is given in Figure 9, where a mapping from prior distribution at xT to data distribution at x0 is shown. With a deterministic sampler (such as DDIM), it is straight-forward how they can represent any Bernoulli distribution by dividing the prior into two regions of probability densities corresponding to the Bernoulli distribution. For stochastic samplers, they can achieve a similar effect but the mapping from noise to data is stochastic. For an arbitrary discrete variable, represented as m-dimensional Bernoulli distribution, the mapping from continuous noise distribution to the target Bernoulli distribution also exists but it is more complicated (and difficult to visualize).\n\n(a) Mapping from N (0, 1) to Bernoulli distribution with P (x0 = 1) = 0.5.\n\n(b) Mapping from N (0, 1) to Bernoulli distribution with P (x0 = 1) = 0.7.\n\nFigure 9: A toy example on continuous modeling of discrete variables.\n\nE ON OTHER SAMPLERS FOR CONTINUOUS DIFFUSION MODELS\n\nAs our models are based on continuous diffusion models, in theory our models are able to incorporate faster samplers. To this end, we conduct preliminary exploration of using DPM-Solver (Lu et al., 2022) for sampling some of our models.\n\nWe find that DPM-Solver provides a boost to diffusion models based on analog bits, similar to what it is able to do for continuous data. This shows a potential of our model enjoying faster continuous sampler while other baselines (e.g., D3PM) may not be able to do due to their use of discrete states. Table 7 below shows the FID scores of bit diffusion models on ImageNet-64x64 under different binary encoding schemes. We find that the DPM-Solver is able to provide a significant reduction in function evaluations for bit diffusion on discrete/categorical data (with 30 NFEs it gets comparable FIDs as 100 NFEs of DDIM), similar to that in continuous diffusion models.\n\nFurthermore, we also find that self-conditioning continues to provide a boost with DPM-solver. For example, the table 8 shows FID scores of diffusion models on ImageNet 64x64 (continuous rgb values). And we find that the self-conditioning consistently improves the performance of DPM-Solver with fixed number of function evaluations.\n\nF EXTRA RANDOM SAMPLES ON CIFAR-10 AND IMAGENET 64×64\n\nFigure 10 shows random samples (non cherry-picked) from unconditional diffusion models on CIFAR-10 with continuous pixels and analog bits.\n\nFigure 11 shows random samples (non cherry-picked) from class-conditional diffusion models on IMAGENET 64×64 with continuous pixels and analog bits.\n\n16\n\nxT−110.50.5x0xT−110.30.7x0Published as a conference paper at ICLR 2023\n\nTable 7: Comparison of Continuous Diffusion Samplers. FIDs on ImageNet 64×64 shown below.\n\nSamplers\n\nUINT8 Gray Code UINT8 (RAND)\n\nDDIM @ 1000 NFE DDPM @ 1000 NFE DDIM @ 400 NFE DDPM @ 400 NFE DDIM @ 100 NFE DDPM @ 100 NFE\n\nDPM-Solver @ 30 NFE DPM-Solver @ 50 NFE\n\n5.51 7.71 5.00 4.84 8.80 13.04\n\n7.85 6.46\n\n5.14 6.91 5.52 5.37 11.31 12.77\n\n9.64 7.61\n\n58.45 64.08 38.44 40.91 8.76 9.25\n\n10.39 10.96\n\nTable 8: The effect of Self-Conditioning for sampling with DPM-Solver. FIDs on ImageNet 64×64 shown below.\n\nModel\n\nDPM-Solver @ 20 NFE DPM-Solver @ 30 NFE\n\n(cid:15) prediction, w/o self-conditioning (cid:15) prediction, w/ self-conditioning\n\nx0 prediction, w/o self-conditioning x0 prediction, w/ self-conditioning\n\n6.10 4.24\n\n12.13 6.94\n\n5.58 4.15\n\n11.05 6.43\n\n(a) DDPM on continuous pixels (FID=3.14)\n\n(b) Bit Diffusion on UINT8 (FID=3.71)\n\n(c) Bit Diffusion on GRAY CODE (FID=3.88)\n\n(d) Bit Diff. on UINT8 (RAND) (FID=6.93)\n\nFigure 10: Random samples from unconditional models trained on CIFAR-10. (a) is for continuous image generation, (b), (c), and (d) are for discrete/categorial image generation.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n(a) DDPM on continuous pixels (FID=3.43)\n\n(b) Bit Diffusion on UINT8. (FID=4.84)\n\n(c) Bit Diffusion on GRAY CODE (FID=5.14)\n\n(d) Bit Diffusion on UINT8 (RAND) (FID=8.76)\n\nFigure 11: Random samples from class-conditional models trained on IMAGENET 64×64. (a) is for continuous image generation, (b), (c), and (d) are for discrete/categorial image generation.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nG ON SAMPLING STRATEGIES WITH SELF-CONDITIONING\n\nG.1 METHOD\n\nIn this section, we present extensions to the default sampling strategy with Self-Conditioning in Algorithm 2. The default sampling strategy utilizes data estimate from the previous step as the conditional input to the denoising network for producing data estimate at the current step. While this is both simple and effective, we observe that, for UINT8 (RAND) encoding of pixels, as the number of sampling steps increases (with both DDIM or DDPM samplers), the generated samples tend to be over-smoothed. We propose the following two extensions of the default sampling strategy to mitigate the issues and provide improvements when using larger sampling steps.\n\nSelf-Conditioning based on Momentum Estimate The first extension to the default sampling strategy is to adopt an exponential moving average over the previous data estimate to provide a more reliable conditioning input, similar to a momentum optimizer. The detailed procedure is shown in algorithm 5, where the differences from the default sampling strategy are highlighted in blue. Note that the default sampling strategy can also be considered as a special case of this generalized form in that the momentum is set to zero.\n\nSelf-Conditioning based on Self-Guidance One potential issue with the default sampling strategy is the slight discrepancy of the Self-Conditioning signal during training and inference/sampling. Specifically, during training, the Self-Conditioning signal is the data estimate from the same time step, while, during sampling, it is from the past time step(s). Therefore, here we propose an approach that also use the same step data estimate for self-conditioning, which comes at the cost of extra forward pass over the denoising network at sampling time. Specifically, we conduct two forward passes of denoising network per sampling step, one with zero data estimate and the other with current data step estimate, and then we use a weighted combination, similar to (Ho & Salimans, 2021), of both prediction to form the final prediction at the current step. The detailed procedure is given in algorithm 6 with differences to the default sampling strategy highlighted.\n\nAlgorithm 5 Sampling with Self-Conditioning based on Momentum Estimate.\n\ndef generate(steps, td=0, momentum=0.):\n\nx_t = normal(mean=0, std=1) x_pred = zeros_like(x_t) x_accu = zeros_like(x_t)\n\nfor step in range(steps):\n\n# Get time for current and next states. t_now = 1 - step / steps t_next = max(1 - (step + 1 + td) / steps, 0)\n\n# Predict x_0 (with self-cond). x_accu = momentum * x_accu + (1 - momentum) * x_pred x_pred = net(cat([x_t, x_accu], -1), t_now)\n\n# Estimate x at t_next. x_t = ddim_or_ddpm_step(x_t, x_pred, t_now, t_next)\n\n# Binary decoding: analog bits to discrete data. x_int = bit2int(x_pred > 0) return x_int\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 6 Sampling with Self-Conditioning based on Self-Guidance.\n\ndef generate(steps, td=0, guide_w=3.0):\n\nx_t = normal(mean=0, std=1) x_pred = zeros_like(x_t)\n\nfor step in range(steps):\n\n# Get time for current and next states. t_now = 1 - step / steps t_next = max(1 - (step + 1 + td) / steps, 0)\n\n# Predict x_0 wo/ self-cond. x_pred_uncond = net(cat([x_t, zeros_like(x_t)], -1), t_now) # Predict x_0 w/ self-cond. x_pred_selfcond = net(cat([x_t, x_pred_uncond], -1), t_now) # Apply self-guidance. x_pred = guide_w * x_pred_selfcond + (1.0 - guide_w) * x_pred_uncond\n\n# Estimate x at t_next. x_t = ddim_or_ddpm_step(x_t, x_pred, t_now, t_next)\n\n# Binary decoding: analog bits to discrete data. x_int = bit2int(x_pred > 0) return x_int\n\nG.2 EXPERIMENTS\n\nTable 9 reports the best FID scores across various sampling strategies discussed here (as well as samplers, sampling steps, time difference in asymmetric time intervals).\n\nTable 9: Best FIDs of Bit Diffusion models with different Self-Conditioning sampling strategies on conditional IMAGENET 64×64.\n\nUINT8\n\nGRAY CODE\n\nUINT8 (RAND)\n\nDefault sampling (momentum= 0) Momentum Estimate Self-Guidance\n\n4.84 4.85 5.15\n\n5.14 5.14 5.65\n\n8.76 8.51 7.87\n\nFigure 12 shows FIDs on conditional IMAGENET 64×64 with UINT8 encoding, using Momentum Estimate with different sampling steps. We find that the momentum on the data estimate is only helpful when sampling steps are larger.\n\nFigure 13 shows FIDs on conditional IMAGENET 64×64 with UINT8 encoding, using Self-Guidance with different sampling steps. We find that a guidance weight between 3.0 and 5.0 is generally preferable and robust to other hyper-parameters (such as sampler choice, sampling steps, and time difference).\n\nG.3 SAMPLES\n\nFigure 14 and 15 provide generated samples from different sampling strategies with 100 and 1000 DDIM sampling steps, respectively.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n(a) DDIM, number of sampling steps < 500.\n\n(b) DDIM, number of sampling steps > 500.\n\n(c) DDPM, number of sampling steps < 500.\n\n(d) DDPM, number of sampling steps > 500.\n\nFigure 12: FID on conditional IMAGENET 64×64 with UINT8 (RAND) encoding using selfcondition sampling based on momentum estimate. The statistics of FID scores in each group are aggregated over the number of sampling steps in {100, 200, 400, 600, 800, 1000}, time difference in {0.0, 0.2, 0.4, 0.6, 0.8}.\n\n(a) DDIM, number of sampling steps < 500.\n\n(b) DDIM, number of sampling steps > 500.\n\n(c) DDPM, number of sampling steps < 500.\n\n(d) DDPM, number of sampling steps > 500.\n\nFigure 13: FID on conditional IMAGENET 64×64 with UINT8 (RAND) encoding using self-condition sampling based on self-guidance. The statistics of FID scores in each group are aggregated over the number of sampling steps in {100, 200, 400, 600, 800, 1000}, time difference in {0.0, 0.1}.\n\n21\n\n0.00.10.20.30.40.50.60.70.80.9Momentum020406080100120140FID Score0.00.10.20.30.40.50.60.70.80.9Momentum020406080100120140FID Score0.00.10.20.30.40.50.60.70.80.9Momentum020406080100120140FID Score0.00.10.20.30.40.50.60.70.80.9Momentum020406080100120140FID Score1.02.03.04.05.06.07.08.09.010.0Self-guidance Weight010203040506070FID Score1.02.03.04.05.06.07.08.09.010.0Self-guidance Weight010203040506070FID Score1.02.03.04.05.06.07.08.09.010.0Self-guidance Weight020406080100120FID Score1.02.03.04.05.06.07.08.09.010.0Self-guidance Weight020406080100120FID ScorePublished as a conference paper at ICLR 2023\n\n(a) W/o Self-Conditioning, FID=90.7.\n\n(b) W/ previous estimate (momentum=0), FID=12.3.\n\n(c) W/ previous estimate (momentum=0.5), FID=35.5.\n\n(d) W/ prev. estimate (momentum=0.9), FID=132.6.\n\n(e) W/ same-step estimate (w = 3.0), FID=15.6.\n\n(f) W/ same-step estimate (w = 5.0), FID=9.3.\n\nFigure 14: Random samples of Bit Diffusion with UINT8 (RAND) on categorical IMAGENET 64×64 using various Self-Conditioning sampling strategies. Different plots share the same set of xT . Sampling with 100 steps of DDIM without asymmetric time intervals.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(a) W/o Self-Conditioning, FID=73.60.\n\n(b) W/ previous estimate (momentum=0), FID=58.6.\n\n(c) W/ previous estimate (momentum=0.5), FID=46.0.\n\n(d) W/ previous estimate (momentum=0.9), FID=11.7.\n\n(e) W/ same-step estimate (w = 3.0), FID=11.8.\n\n(f) W/ same-step estimate (w = 5.0), FID=9.6.\n\nFigure 15: Random samples of Bit Diffusion with UINT8 (RAND) on categorical IMAGENET 64×64 using various Self-Conditioning sampling strategies. Different plots share the same set of xT . Sampling with 1000 steps of DDIM without asymmetric time intervals.\n\n23",
    "reference": "# Summary Of The Paper\n\nThe authors of this paper introduce a method to use diffusion models to generate discrete data. The crux of the method rests on the observation that discrete data can be represented with binary bits and binary bits can be uses as real numbers to train a continuous diffusion model; the output at the end of the diffusion process can be rounded to obtain binary output bits.\n\n# Strength And Weaknesses\n\nStrengths:\n* The authors introduce two techniques - self-conditioning and asymmetric time intervals. Both of these seem useful for both continuous and discrete diffusion models. Other concurrent work has also shown that using self-conditioning improves results of diffusion models.\n* Figure 7 shows that the generated bits are concentrated on two modes without adding explicit constraints and this validates the authors' hypothesis that modeling binary bits is easy. \n* For image captioning, the AnalogBits achieves the same performance as auto-regressive models with just 10 diffusion steps!\n\nWeaknesses:\n* Discrete diffusion models are especially suitable for NLP tasks given the discrete nature of text. Despite, proposing a method for discrete data, the authors only presents results on one NLP task (image captioning). Additional experimental results on other NLP texts would help strengthen this paper.\n* For encoding NLP tokens as analog bits, each token is mapped to a random set of 15 analog bits. As a results, there is no semantic meaning in the new representation (similar tokens don't have similar bit representations). The authors show that using UINT8 is much better when compared to using random bits for discrete image generation. Exploring ways to represent semantic meaning in bit representations of tokens would be an interesting extension to the presented work.\n* Gray codes perform worse than UINT8 and no explanation is give as to why this is the case.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe method is clearly explained and the necessary ablations have been conducted.\n\n# Summary Of The Review\n\nDiffusion models have shown strong performance in continuous domains. However, there are many domains that are not continuous (like text) and adapting diffusion models to discrete settings will aid in exploring whether state-of-the-art results can be improved by using diffusion models for discrete domains. The method in this paper also converts discrete domain data to a \"continuous representation\" and therefore additional work on continuous diffusion models can easily be ported and used for the discrete setting. Furthermore, self-conditioning and asymmetric time intervals are useful general techniques that will be of interest to the broader diffusion model community.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGFLOWNETS AND VARIATIONAL INFERENCE\n\nNikolay Malkin∗, Salem Lahlou∗, Tristan Deleu∗ Mila, Universit ́e de Montr ́eal\n\nXu Ji, Edward Hu Mila, Universit ́e de Montr ́eal\n\nKatie Everett Google Research\n\nDinghuai Zhang Mila, Universit ́e de Montr ́eal\n\nYoshua Bengio Mila, Universit ́e de Montr ́eal, CIFAR\n\nABSTRACT\n\nThis paper builds bridges between two families of probabilistic algorithms: (hierarchical) variational inference (VI), which is typically used to model distributions over continuous spaces, and generative flow networks (GFlowNets), which have been used for distributions over discrete structures such as graphs. We demonstrate that, in certain cases, VI algorithms are equivalent to special cases of GFlowNets in the sense of equality of expected gradients of their learning objectives. We then point out the differences between the two families and show how these differences emerge experimentally. Notably, GFlowNets, which borrow ideas from reinforcement learning, are more amenable than VI to off-policy training without the cost of high gradient variance induced by importance sampling. We argue that this property of GFlowNets can provide advantages for capturing diversity in multimodal target distributions. Code: https://github.com/GFNOrg/GFN_vs_HVI.\n\n1\n\nINTRODUCTION\n\nMany probabilistic generative models produce a sample through a sequence of stochastic choices. Non-neural latent variable models (e.g., Blei et al., 2003), autoregressive models, hierarchical variational autoencoders (Sønderby et al., 2016), and diffusion models (Ho et al., 2020) can be said to rely upon a shared principle: richer distributions can be modeled by chaining together a sequence of simple actions, whose conditional distributions are easy to describe, than by performing generation in a single sampling step. When many intermediate sampled variables could generate the same object, making exact likelihood computation intractable, hierarchical models are trained with variational objectives that involve the posterior over the sampling sequence (Ranganath et al., 2016b).\n\nThis work connects variational inference (VI) methods for hierarchical models (i.e., sampling through a sequence of choices conditioned on the previous ones) with the emerging area of research on generative flow networks (GFlowNets; Bengio et al., 2021a). GFlowNets have been formulated as a reinforcement learning (RL) algorithm – with states, actions, and rewards – that constructs an object by a sequence of actions so as to make the marginal likelihood of producing an object proportional to its reward. While hierarchical VI is typically used for distributions over real-valued objects, GFlowNets have been successful at approximating distributions over discrete structures for which exact sampling is intractable, such as for molecule discovery (Bengio et al., 2021a), for Bayesian posteriors over causal graphs (Deleu et al., 2022), or as an amortized learned sampler for approximate maximum-likelihood training of energy-based models (Zhang et al., 2022b). Although GFlowNets appear to have different foundations (Bengio et al., 2021b) and applications than hierarchical VI algorithms, we show here that the two are closely connected.\n\nAs our main theoretical contribution, we show that special cases of variational algorithms and GFlowNets coincide in their expected gradients. In particular, hierarchical VI (Ranganath et al., 2016b) and nested VI (Zimmermann et al., 2021) are related to the trajectory balance and detailed balance objectives for GFlowNets (Malkin et al., 2022; Bengio et al., 2021b). We also point out the differences between VI and GFlowNets: notably, that GFlowNets automatically perform gradient variance reduction by estimating a marginal quantity (the partition function) that acts as a baseline and allow off-policy learning without the need for reweighted importance sampling.\n\nOur theoretical results are accompanied by experiments that examine what similarities and differences emerge when one applies hierarchical VI algorithms to discrete problems where GFlowNets\n\n∗Equal contribution. Contact: nikolay.malkin@mila.quebec.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nhave been used before. These experiments serve two purposes. First, they supply a missing hierarchical VI baseline for problems where GFlowNets have been used in past work. The relative performance of this baseline illustrates the aforementioned similarities and differences between VI and GFlowNets. Second, the experiments demonstrate the ability of GFlowNets, not shared by hierarchical VI, to learn from off-policy distributions without introducing high gradient variance. We show that this ability to learn with exploratory off-policy sampling is beneficial in discrete probabilistic modeling tasks, especially in cases where the target distribution has many modes.\n\n2 THEORETICAL RESULTS\n\n2.1 GFLOWNETS: NOTATION AND BACKGROUND\n\nWe consider the setting of Bengio et al. (2021a). We are given a pointed1 directed acyclic graph (DAG) G = (S, A), where S is a finite set of vertices (states), and A ⊂ S × S is a set of directed edges (actions). If s→s′ is an action, we say s is a parent of s′ and s′ is a child of s. There is exactly one state that has no incoming edge, called the initial state s0 ∈ S. States that have no outgoing edges are called terminating. We denote by X the set of terminating states. A complete trajectory is a sequence τ = (s0→ . . . →sn) such that each si→si+1 is an action and sn ∈ X. We denote by T the set of complete trajectories and by x τ the last state of a complete trajectory τ.\n\nGFlowNets are a class of models that amortize the cost of sampling from an intractable target distribution over X by learning a functional approximation of the target distribution using its unnormalized density or reward function, R : X → R+. While there exist different parametrizations and loss functions for GFlowNets, they all define a forward transition probability function, or a forward policy, PF (− | s), which is a distribution over the children of every state s ∈ S. The forward policy is typically parametrized by a neural network that takes a representation of s as input and produces the logits of a distribution over its children. Any forward policy PF induces a distribution over complete trajectories τ ∈ T (denoted by PF as well), which in turn defines a marginal distribution over terminating states x ∈ X (denoted by P⊤\n\nF):\n\nPF (τ = (s0→ . . . →sn)) =\n\nP⊤\n\nF (x) =\n\nn−1 (cid:214)\n\nPF (si+1 | si)\n\ni=0\n\n∑︁\n\nτ ∈ T:xτ =x\n\nPF (τ)\n\n∀τ ∈ T ,\n\n∀x ∈ X.\n\n(1)\n\n(2)\n\nGiven a forward policy PF, terminating states x ∈ X can be sampled from P⊤ ries τ from PF (τ) and taking their final states x τ. F (x) ∝ R(x). Because the sum in (2) GFlowNets aim to find a forward policy PF for which P⊤ is typically intractable to compute exactly, training objectives for GFlowNets introduce auxiliary objects into the optimization. For example, the trajectory balance objective (TB; Malkin et al., 2022) introduces an auxiliary backward policy PB, which is a learned distribution PB (− | s) over the parents of every state s ∈ S, and an estimated partition function Z, typically parametrized as exp(log Z) where log Z is the learned parameter. The TB objective for a complete trajectory τ is defined as\n\nF by sampling trajecto-\n\nLTB (τ; PF, PB, Z) =\n\nZ · PF (τ) R(x τ)PB (τ | x τ) where PB (τ | x τ) = (cid:206) (s→s′ ) ∈ τ PB (s | s′). If LTB is made equal to 0 for every complete trajectory F (x) ∝ R(x) for all x ∈ X and Z is the inverse constant of proportionality: Z = (cid:205)x ∈ X R(x). τ, then P⊤ The objective (3) is minimized by sampling trajectories τ from some distribution and making gradient steps on (3) with respect to the parameters of PF, PB, and log Z. The distribution from which τ is sampled amounts to a choice of scalarization weights for the multi-objective problem of minimizing (3) over all τ ∈ T . If τ is sampled from PF (τ) – note that this is a nonstationary scalarization – we say the algorithm runs on-policy. If τ is sampled from another distribution, the algorithm runs off-policy; typical choices are to sample τ from a tempered version of PF to encourage exploration (Bengio et al., 2021a; Deleu et al., 2022) or to sample τ from the backward policy PB (τ|x) starting from given terminating states x (Zhang et al., 2022b). By analogy with the RL nomenclature, we call the behavior policy the one that samples τ for the purpose of obtaining a stochastic gradient, e.g, the gradient of the objective LTB in (3) for the sampled τ.\n\nlog\n\n(3)\n\n(cid:19) 2\n\n(cid:18)\n\n,\n\n1A pointed DAG is one with a designated initial state.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nOther objectives have been studied and successfully used in past works, including detailed balance (DB; proposed by Bengio et al. (2021b) and evaluated by Malkin et al. (2022)) and subtrajectory balance (SubTB; Madan et al., 2022). In the next sections, we will show how the TB objective relates to hierarchical variational objectives. In §C, we generalize this result to the SubTB loss, of which both TB and DB are special cases.\n\n2.2 HIERARCHICAL VARIATIONAL MODELS AND GFLOWNETS\n\nVariational methods provide a way of sampling from distributions by means of learning an approximate probability density. Hierarchical variational models (HVMs; Ranganath et al., 2016b; Sobolev & Vetrov, 2019; Vahdat & Kautz, 2020; Zimmermann et al., 2021)) typically assume that the sample space is a set of sequences (z1, . . . , zn) of fixed length, with an assumption of conditional independence between zi−1 and zi+1 conditioned on zi, i.e., the likelihood has a factorization q(z1, . . . , zn) = q(z1)q(z2|z1) . . . q(zn|zn−1). The marginal likelihood of zn in a hierarchical model involves a possibly intractable sum,\n\nq(zn) =\n\n∑︁\n\nq(z1)q(z2|z1) . . . q(zn|zn−1).\n\nz1,...,zn−1\n\nThe goal of VI algorithms is to find the conditional distributions q that minimize some divergence between the marginal q(zn) and a target distribution. The target is often given as a distribution with intractable normalization constant: a typical setting is a Bayesian posterior (used in VAEs, variational EM, and other applications), for which we desire q(zn) ∝ plikelihood (x|zn) pprior (zn). The GFlowNet corresponding to a HVM: Sampling sequences (z1, . . . , zn) from a hierarchical model is equivalent to sampling complete trajectories in a certain pointed DAG G. The states of G at a distance of i from the initial state are in bijection with possible values of the variable zi, and the action distribution is given by q. Sampling from the HVM is equivalent to sampling trajectories from the policy PF (zi+1|zi) = q(zi+1|zi) (and PF (z1|s0) = q(z1)), and the marginal distribution q(zn) is the terminating distribution P⊤ F. The HVM corresponding to a GFlowNet: Conversely, suppose G = (S, A) is a graded pointed DAG2 and that a forward policy PF on G is given. Sampling trajectories τ = (s0→s1→ . . . →sL) in G is equivalent to sampling from a HVM in which the random variable zi is the identity of the (i + 1)-th state si in τ and the conditional distributions q(zi+1|zi) are given by the forward policy PF (si+1|si). Specifying an approximation of the target distribution in a hierarchical model with n layers is thus equivalent to specifying a forward policy PF in a graded DAG. The correspondence can be extended to non-graded DAGs. Every pointed DAG G = (S, A) can be canonically transformed into a graded pointed DAG by the insertion of dummy states that have one child and one parent. To be precise, every edge s→s′ ∈ A is replaced with a sequence of l′ − l(s) edges, where l(s) is the length of the longest trajectory from s0 to s, l′ = l(s′) if s′ ∉ X, and l′ = maxs′′ ∈ S l(s′′) otherwise. This process is illustrated in §A. We thus restrict our analysis in this section, without loss of generality, to graded DAGs.\n\nThe meaning of the backward policy: Typically, the target distribution is over the objects X of the last layer of a graded DAG, rather than over complete sequences or trajectories. Any backward policy PB on the DAG turns an unnormalized target distribution R over X into an unnormalized distribution over complete trajectories T :\n\n∑︁\n\n∀τ ∈ T PB (τ) ∝ R(x τ)PB (τ | x τ), with unknown partition function ˆZ =\n\nR(x).\n\n(4)\n\nx ∈ X\n\nThe marginal distribution of PB over terminating states is equal to R(x)/ ˆZ by construction. Therefore, if PF is a forward policy that equals PB as a distribution over trajectories, then P⊤ F (x) = R(x)/ ˆZ ∝ R(x).\n\nVI training objectives: In its most general form, the hierarchical variational objective (‘HVI objective’ in the remainder of the paper) minimizes a statistical divergence D f between the learned and the target distributions over trajectories:\n\nLHVI, f (PF, PB) = D f (PB ∥PF) = Eτ∼PF\n\n(cid:20)\n\nf\n\n(cid:18) PB (τ) PF (τ)\n\n(cid:19)(cid:21)\n\n.\n\n(5)\n\n2We recall some facts about partially ordered sets. A pointed graded DAG is a pointed DAG in which all complete trajectories have the same length. Pointed graded DAGs G are also characterized by the following equivalent property: the state space S can be partitioned into disjoint sets S = (cid:195)L Sl, with S0 = {s0}, called layers, such that all edges s→s′ are between states of adjacent layers (s ∈ Si,s′ ∈ Si+1 for some i).\n\nl=0\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nTwo common objectives are the forward and reverse Kullback-Leibler (KL) divergences (Mnih & Gregor, 2014), corresponding to f ↦→ − log t for DKL (PF ∥PB), respectively. Other f -divergences have been used, as discussed in Zhang et al. (2019b); Wan et al. (2020). Note that, similar to GFlowNets, (5) can be minimized with respect to both the forward and backward policies, or can be minimized using a fixed backward policy.\n\n↦→ t log t for DKL (PB ∥PF) and f\n\n: t\n\n: t\n\nDivergences between two distributions over trajectories and divergences between their two marginal distributions over terminating states distributions are linked via the data processing inequality, assuming f is convex (see e.g. Zhang et al. (2019b)), making the former a sensible surrogate objective for the latter: F) ≤ D f (PB ∥PF)\n\nD f (R/ ˆZ ∥P⊤\n\nTable 1: A comparison of algorithms for approximating a target distribution in a hierarchical variational model or a GFlowNet. The gradients used to update the parameters of the sampling distribution and of the auxiliary backward policy approximate the gradients of various divergences between distributions over trajectories.\n\n(6) When both PB and PF are learned, the divergences with respect to which they are optimized need not be the same, as long as both objectives are 0 if and only if PF = PB. For example, wake-sleep algorithms (Hinton et al., 1995) optimize the generative model PF using DKL (PB ∥PF) and the posterior PB using DKL (PF ∥PB). A summary of common combinations is shown in Table 1.\n\nPF (sampler) PB (posterior) DKL (PF ∥PB) DKL(PF ∥PB) REVERSE KL DKL (PB ∥PF) DKL (PB ∥PF) FORWARD KL DKL(PB ∥PF) DKL (PF ∥PB) WAKE-SLEEP (WS) REVERSE WAKE-SLEEP DKL(PF ∥PB) DKL (PB ∥PF) On-policy TB\n\nDKL(PF ∥PB)\n\nSurrogate loss\n\nAlgorithm\n\nsee §2.3\n\nWe remark that tractable unbiased gradient estimators for objectives such as (5) may not always exist, as we cannot exactly sample from or compute the density of PB (τ) when its normalization constant ˆZ is unknown. For example, while the REINFORCE estimator gives unbiased estimates of the gradient with respect to PF when the objective is REVERSE KL (see §2.3), other objectives, such as FORWARD KL, require importance-weighted estimators. Such estimators approximate sampling from PB by sampling a batch of trajectories {τi } from another distribution π (which may equal PF) PB ( τi ) and weighting a loss computed for each τi by a scalar proportional to π ( τi ) . Such reweighted importance sampling is helpful in various variational algorithms, despite its bias when the number of samples is finite (e.g., Bornschein & Bengio, 2015; Burda et al., 2016), but it may also introduce variance that increases with the discrepancy between PB and π.\n\n2.3 ANALYSIS OF GRADIENTS\n\nThe following proposition summarizes our main theoretical claim, relating the GFN objective of (3) and the variational objective of (5). In §C, we extend this result by showing an equivalence between the subtrajectory balance objective (introduced in Malkin et al. (2022) and empirically evaluated in Madan et al. (2022)) and a natural extension of the nested variational objective (Zimmermann et al., 2021) to subtrajectories. A special case of this equivalence is between the Detailed Balance objective (Bengio et al., 2021b) and the nested VI objective (Zimmermann et al., 2021).\n\nProposition 1 Given a graded DAG G, and denoting by θ, φ the parameters of the forward and backward policies PF, PB respectively, the gradients of the TB objective (3) satisfy:\n\n∇φ DKL(PB ∥PF) =\n\n∇θ DKL(PF ∥PB) =\n\n1 2\n1 2\n\nEτ∼PB [∇φLTB(τ)],\n\nEτ∼PF [∇θ LTB (τ)].\n\n(7)\n\n(8)\n\nThe proof of the extended result appears in §C. An alternative proof is provided in §B. While (8) is the on-policy TB gradient with respect to the parameters of PF, (7) is not the on-policy TB gradient with respect to the parameters of PB, as the expectation is taken over PB, not PF. The on-policy TB gradient can however be expressed through a surrogate loss (cid:104)\n\n(cid:105)\n\nEτ∼PF [∇φLTB(τ)] = ∇φ\n\nD\n\nlog2 (PB ∥PF) + 2(log Z − log ˆZ)DKL (PF ∥PB)\n\n,\n\n(9) log2 is the pseudo- f -divergence\n\nwhere ˆZ = (cid:205)x ∈ X R(x), the unknown true partition function. Here D defined by f (x) = log(x)2, which is not convex for large x. (Proof in §B.)\n\nThe loss in (7) is not possible to optimize directly unless using importance weighting (cf. the end of §2.2), but optimization of PB using (7) and PF using (8) would yield the gradients of REVERSE WAKE-SLEEP in expectation.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nScore function estimator and variance reduction: Optimizing the reverse KL loss DKL (PF ∥PB) with respect to θ, the parameters of PF, requires a likelihood ratio (also known as REINFORCE) estimator of the gradient (Williams, 1992), using a trajectory τ (or a batch of trajectories), which takes the form:\n\nΔ(τ) = ∇θ log PF (τ; θ)c(τ), where c(τ) = log\n\nPF (τ) R(x τ)PB (τ | x τ)\n\n(10)\n\n(Note that the term ∇θ c(τ) that is typically present in the REINFORCE estimator is 0 in expectation, since Eτ∼PF [∇θ log PF (τ)] = (cid:205)τ PF ( τ ) ∇θ PF (τ) = 0.) The estimator of (10) is known to exhibit high variance norm, thus slowing down learning. A common workaround is to subtract a baseline b from c(τ), which does not bias the estimator. The value of the baseline b (also called control variate) that most reduces the trace of the covariance matrix of the gradient estimator is\n\nPF ( τ )\n\nb∗ =\n\nEτ∼PF [c(τ)∥∇θ log PF (τ; θ)∥2] Eτ∼PF [∥∇θ log PF (τ; θ)∥2]\n\n,\n\ncommonly approximated with Eτ∼PF [c(τ)] (see, e.g., Weaver & Tao (2001); Wu et al. (2018)). This approximation is itself often approximated with a batch-dependent local baseline, from a batch of trajectories {τi }B\n\ni=1:\n\n1 B\nA better approximation of the expectation Eτ∼PF [c(τ)] can be obtained by maintaining a running average of the values c(τ), leading to a global baseline. After observing each batch of trajectories, the running average is updated with step size η:\n\nblocal =\n\nc(τi)\n\n(11)\n\ni=1\n\nB ∑︁\n\nη\n\nbglobal ← (1 − η)bglobal + ηblocal. (12) This coincides with the update rule of log Z in the minimization of LTB (PF, PB, Z) with a learning 2 for the parameter log Z (with respect to which the TB objective is quadratic). Consequently, rate (8) of Prop. 1 shows that the update rule for the parameters of PF, when optimized using the REVERSE KL objective, with (12) as a control variate for the score function estimator of its gradient, is the same as the update rule obtained by optimizing the TB objective using on-policy trajectories. While learning a backward policy PB can speed up convergence (Malkin et al., 2022), the TB objective can also be used with a fixed backward policy, in which case the REVERSE KL objective and the TB objective differ only in how they reduce the variance of the estimated gradients, if the trajectories are sampled on-policy. In § 4, we experimentally explore the differences between the two learning paradigms that arise when PB is learned, or when the algorithms run off-policy.\n\n3 RELATED WORK\n\n(Hierarchical) VI: Variational inference (Zhang et al., 2019a) techniques originate from graphical models (Saul et al., 1996; Jordan et al., 2004), which typically include an inference machine and a generative machine to model the relationship between latent variables and observed data. The line of work on black-box VI (Ranganath et al., 2014) focuses on learning the inference machine given a data generating process, i.e., inferring the posterior over latent variables. Hierarchical modeling exhibits appealing properties under such settings as discussed in Ranganath et al. (2016b); Yin & Zhou (2018); Sobolev & Vetrov (2019). On the other hand, works on variational auto-encoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014) focus on generative modeling, where the inference machine – the estimated variational posterior – is a tool to assist optimization of the generative machine or decoder. Hierarchical construction of multiple latent variables has also been shown to be beneficial (Sønderby et al., 2016; Maaløe et al., 2019; Child, 2021).\n\nWhile earlier works simplify the variational family with mean-field approximations (Bishop, 2006), modern inference methods rely on amortized stochastic optimization (Hoffman et al., 2013). One of the oldest and most commonly used ideas is REINFORCE (Williams, 1992; Paisley et al., 2012) which gives unbiased gradient estimation. Follow-up work (Titsias & L ́azaro-Gredilla, 2014; Gregor et al., 2014; Mnih & Gregor, 2014; Mnih & Rezende, 2016) proposes advanced estimators to reduce the high variance of REINFORCE. The log-variance loss proposed by Richter et al. (2020) is equivalent in expected gradient of PF to the on-policy TB loss for a GFlowNet with a batch-optimal value of log Z. On the other hand, path-wise gradient estimators (Kingma & Welling, 2014) have much lower variance, but have limited applicability. Later works combine these two approaches for particular distribution families (Tucker et al., 2017; Grathwohl et al., 2018).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nBeyond the evidence lower bound (ELBO) objective used in most variational inference methods, more complex objectives have been studied. Tighter evidence bounds have proved beneficial to the learning of generative machines (Burda et al., 2016; Domke & Sheldon, 2018; Rainforth et al., 2018; Masrani et al., 2019). As KL divergence optimization suffers from issues such as mean-seeking behavior and posterior variance underestimation (Minka, 2005), other divergences are adopted as in expectation propagation (Minka, 2001; Li et al., 2015), more general f -divergences (Dieng et al., 2017; Wang et al., 2018; Wan et al., 2020), their special case α-divergences (Hern ́andez-Lobato et al., 2016), and Stein discrepancy (Liu & Wang, 2016; Ranganath et al., 2016a). GFlowNets could be seen as providing a novel pseudo-divergence criterion, namely TB, as discussed in this work.\n\nWake-sleep algorithms: Another branch of work, starting with Hinton et al. (1995), proposes to avoid issues from stochastic optimization (such as REINFORCE) by alternatively optimizing the generative and inference (posterior) models. Modern versions extending this framework include reweighted wake-sleep Bornschein & Bengio (2015); Le et al. (2019) and memoised wakesleep (Hewitt et al., 2020; Le et al., 2022). It was shown in Le et al. (2019) that wake-sleep algorithms behave well for tasks involving stochastic branching.\n\nGFlowNets: GFlowNets have been used successfully in settings where RL and MCMC methods have been used in other work, including molecule discovery (Bengio et al., 2021a; Malkin et al., 2022; Madan et al., 2022), biological sequence design (Malkin et al., 2022; Jain et al., 2022; Madan et al., 2022), and Bayesian structure learning (Deleu et al., 2022). A connection of the theoretical foundations of GFlowNets (Bengio et al., 2021a;b) with variational methods was first mentioned by Malkin et al. (2022) and expanded in Zhang et al. (2022a; 2023).\n\nA concurrent and closely related paper (Zimmermann et al., 2022) theoretically and experimentally explores interpolations between forward and reverse KL objectives.\n\n4 EXPERIMENTS\n\nThe goal of the experiments is to empirically investigate two main observations consistent with the above theoretical analysis:\n\nObservation 1. On-policy VI and TB (GFlowNet) objectives can behave similarly in some cases, when both can be stably optimized, while in others on-policy TB strikes a better compromise than either the (mode-seeking) REVERSE KL or (mean-seeking) FORWARD KL VI objectives. This claim is supported by the experiments on all three domains below.\n\nHowever, in all cases, notable differences emerge. In particular, HVI training becomes more stable near convergence and is sensitive to learning rates, which is consistent with the hypotheses about gradient variance in §2.3.\n\nObservation 2. When exploration matters, off-policy TB outperforms both on-policy TB and VI objectives, avoiding the possible high variance induced by importance sampling in off-policy VI. GFlowNets are capable of stable off-policy training without importance sampling. This claim is supported by experiments on all domains, but is especially well illustrated on the realistic domains in §4.2 and §4.3. This capability provides advantages for capturing a more diverse set of modes.\n\nObservation 1 and Observation 2 provide evidence that off-policy TB is the best method among those tested in terms of both accurately fitting the target distribution and effectively finding modes, where the latter is particularly important for the challenging molecule graph generation and causal graph discovery problems studied below.\n\n4.1 HYPERGRID: EXPLORATION OF LEARNING OBJECTIVES\n\nIn this section, we comparatively study the ability of the variational objectives and the GFlowNet objectives to learn a multimodal distribution given by its unnormalized density, or reward function, R. We use the synthetic hypergrid environment introduced by Bengio et al. (2021a) and further explored by Malkin et al. (2022). The states form a D-dimensional hypergrid with side length H, and the reward function has 2D flat modes near the corners of the hypergrid. The states form a pointed DAG, where the source state is the origin s0 = 0, and each edge corresponds to the action of incrementing one coordinate in a state by 1 (without exiting the grid). More details about the environment are provided in § D.1. We focus on the case where PB is learned, which has been shown to accelerate convergence (Malkin et al., 2022).\n\nIn Fig. 1, we compare how fast each learning objective discovers the 4 modes of a 128 × 128 grid, with an exploration parameter R0 = 0.001 in the reward function. The gap between the learned distribution P⊤ F and the target distribution is measured by the Jensen-Shannon divergence (JSD)\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Top: The evolution of the JSD between the learned sampler P⊤ F and the target distribution on the 128 × 128 grid, as a function of the number of trajectories sampled. Shaded areas represent the standard error evaluated across 5 different runs (on-policy left, off-policy right). Bottom: The average (across 5 runs) final learned distribution P⊤ F for the different algorithms, along with the target distribution. To amplify variation, the plot intensity at each grid position is resampled from the Gaussian approximating the distribution over the 5 runs. Although WS, FORWARD KL, and REVERSE WS (off-policy) find the 4 target modes, they do not model them with high precision, and produce a textured pattern at the modes, where it should be flat.\n\nbetween the two distributions, to avoid giving a preference to one KL or the other. Additionally, we show graphical representations of the learned 2D terminating states distribution, along with the target distribution. We provide in § E details on how P⊤ F and the JSD are evaluated and how hyperparameters were optimized separately for each learning algorithm.\n\nExploration poses a challenge in this environment, given the distance that separates the different modes. We thus include in our analysis an off-policy version of each objective, where the behavior policy is different from, but related to, the trained sampler PF (τ). The GFlowNet behavior policy used here encourages exploration by reducing the probability of terminating a trajectory at any state of the grid. This biases the learner towards sampling longer trajectories and helps with faster discovery of farther modes. When off-policy, the HVI gradients are corrected using importance sampling weights.\n\nFor the algorithms that use a score function estimator of the gradient (FORWARD KL, REVERSE WS, and REVERSE KL), we found that using a global baseline, as explained in §2.2, was better than using the more common local baseline in most cases (see Fig. D.1). This brings the VI methods closer to GFlowNets and thus factors out this issue from the comparison with the GFlowNet objectives. We see from Fig. 1 that while FORWARD KL and WS – the two algorithms that use DKL (PB ∥PF) as the objective for PF – discover the four modes of the distribution faster, they converge to a local minimum and do not model all the modes with high precision. This is due to the mean-seeking behavior of the forward KL objective, requiring that P⊤ F puts non-zero mass on terminating states x where R(x) > 0. Objectives that use the reverse KL to train the forward policy (REVERSE KL and REVERSE WS) are mode-seeking and can thus have a low loss without finding all the modes. The TB GFlowNet objective offers the best of both worlds, as it converges to a lower value of the JSD, discovers the four modes, and models them with high precision. This supports Observation 1. Additionally, in support of Observation 2, while both the TB objective and the HVI objectives benefit from off-policy sampling, TB benefits more, as convergence is greatly accelerated.\n\nWe supplement this study with a comparative analysis of the algorithms on smaller grids in §D.1.\n\n4.2 MOLECULE SYNTHESIS\n\nWe study the molecule synthesis task from Bengio et al. (2021a), in which molecular graphs are generated by sequential addition of subgraphs from a library of blocks (Jin et al., 2020; Kumar\n\n7\n\n0k200k400k600k800k1000kTrajectories sampled104103102101100JSDTBWSForward KLReverse KLReverse WS0k200k400k600k800k1000kTrajectories sampledOn PolicyReverse KLReverse WSWSForward KLTBOff PolicyTarget DistributionPublished as a conference paper at ICLR 2023\n\nFigure 2: Correlation between marginal sampling log-likelihood and log-reward on the molecule generation task for different learning algorithms, showing the advantage of off-policy TB (red) against on-policy TB (orange) and both on-policy (blue) and off-policy HVI (green). For each hyperparameter setting on the x-axis (α or β), we take the optimal choice of the other hyperparameter (β or α, respectively) and plot the mean and standard error region over three random seeds.\n\net al., 2012). The reward function is expressed in terms of a fixed, pretrained graph neural network f that estimates the strength of binding to the soluble epoxide hydrolase protein (Trott & Olson, 2010). To be precise, R(x) = f (x) β, where f (x) is the output of the binding model on molecule x and β is a parameter that can be varied to control the entropy of the sampling model.\n\nBecause the number of terminating states is too large to make exact computation of the target distribution possible, we use a performance metric from past work on this task (Bengio et al., 2021a) to evaluate sampling agents. Namely, for each molecule x in a held-out set, we compute log P⊤ F (x), the likelihood of x under the trained model (computable by dynamic programming, see § E), and F (x) and log R(x). This value should equal 1 for a perfect evaluate the Pearson correlation of log P⊤ sampler, as log P⊤\n\nF (x) and log R(x) would differ by a constant, the log-partition function log ˆZ.\n\nIn Malkin et al. (2022), GFlowNet samplers using the DB and TB objectives, with the backward policy PB fixed to a uniform distribution over the parents of each state, were trained off-policy. Specifically, the trajectories used for DB and TB gradient updates were sampled from a mixture of the (online) forward policy PF and a uniform distribution at each sampling step, with a special weight depending on the trajectory length used for the termination action.\n\nWe wrote an extension of the published code of Malkin et al. (2022) with an implementation of the HVI (REVERSE KL) objective, using a reweighted importance sampling correction. We compare the off-policy TB from past work with the off-policy REVERSE KL, as well as on-policy TB and REVERSE KL objectives. (Note that on-policy TB and REVERSE KL are equivalent in expectation in this setting, since the backward policy is fixed.) Each of the four algorithms was evaluated with four values of the inverse temperature parameter β and of the learning rate α, for a total of 4×4×4 = 64 settings. (We also experimented with the off-policy FORWARD KL / WS objective for optimizing PF, but none of the hyperparameter settings resulted in an average correlation greater than 0.1.) The results are shown in Fig. 2, in which, for each hyperparameter (α or β), we plot the performance for the optimal value of the other hyperparameter. We make three observations:\n\n• In support of Observation 2, off-policy REVERSE KL performs poorly compared to its on-policy counterpart, especially for smoother distributions (smaller values of β) where more diversity is present in the target distribution. Because the two algorithms agree in the expected gradient, this suggests that importance sampling introduces unacceptable variance into HVI gradients.\n\n• In support of Observation 1, the difference between on-policy REVERSE KL and on-policy TB is quite small, consistent with their gradients coinciding in the limit of descent along the full-batch gradient field. However, REVERSE KL algorithms are more sensitive to the learning rate.\n\n• In support of Observation 2, off-policy TB gives the best and lowest-variance fit to the target distribution, showing the importance of an exploratory training policy, especially for sparser reward landscapes (higher β).\n\n8\n\n481016reward exponent 0.60.40.20.00.20.40.60.8logPTF(x) to logR(x) correlationReverse KL on-policyTB on-policyReverse KL off-policyTB off-policy104103learning rate Published as a conference paper at ICLR 2023\n\nTable 2: Comparison of the Jensen-Shannon divergence for Bayesian structure learning, showing the advantage of off-policy TB over on-policy TB and on-policy or off-policy HVI. The JSD is F (G). measured between the true posterior distribution p(G | D) and the learned approximation P⊤\n\nObjective\n\n(Modified) Detailed Balance Off-Policy Trajectory Balance On-Policy Trajectory Balance On-Policy REVERSE KL (HVI) Off-Policy REVERSE KL (HVI)\n\n3 5.32 ± 4.15 × 10−6 3.70 ± 2.51 × 10−7 0.022 ± 0.007 0.022 ± 0.007 0.014 ± 0.008\n\nNumber of nodes\n\n4 2.05 ± 0.70 × 10−5 9.35 ± 2.99 × 10−6 0.123 ± 0.028 0.125 ± 0.027 0.605 ± 0.019\n\n5 4.65 ± 1.08 × 10−4 5.44 ± 2.47 × 10−4 0.277 ± 0.040 0.306 ± 0.042 0.656 ± 0.009\n\n4.3 GENERATION OF DAGS IN BAYESIAN STRUCTURE LEARNING\n\nFinally, we consider the problem of learning the (posterior) distribution over the structure of Bayesian networks, as studied in Deleu et al. (2022). The goal of Bayesian structure learning is to approximate the posterior distribution p(G | D) over DAGs G, given a dataset of observations D. Following Deleu et al. (2022), we treat the generation of a DAG as a sequential decision problem, where directed edges are added one at a time, starting from the completely disconnected graph. Since our goal is to approximate the posterior distribution p(G | D), we use the joint probability R(G) = p(G, D) as the reward function, which is proportional to the former up to a normalizing constant. Details about how this reward is computed, as well as the parametrization of the forward policy PF, are available in §D.3. Note that similarly to §4.2, and following Deleu et al. (2022), we leave the backward policy PB fixed to uniform. We only consider settings where the true posterior distribution p(G | D) can be computed exactly by enumerating all the possible DAGs G over d nodes (for d ≤ 5). This allows us to exactly compare the posterior approximations, found either with the GFlowNet objectives or HVI, with the target posterior distribution. The state space grows rapidly with the number of nodes (e.g., there are 29k DAGs over d = 5 nodes). For each experiment, we sampled a dataset D of 100 observations from a randomly generated ground-truth graph G★; the size of D was chosen to obtain highly multimodal posteriors. In addition to the (Modified) DB objective introduced by Deleu et al. (2022), we also study the TB (GFlowNet) and the REVERSE KL (HVI) objectives, both on-policy and off-policy.\n\nIn Table 2, we compare the posterior approximations found using these different objectives in terms of their Jensen-Shannon divergence (JSD) to the target posterior distribution P(G | D). We observe that on the easiest setting (graphs over d = 3 nodes), all methods accurately approximate the posterior distribution. But as we increase the complexity of the problem (with larger graphs), we observe that the accuracy of the approximation found with Off-Policy REVERSE KL degrades significantly, while the ones found with the off-policy GFlowNet objectives ((Modified) DB & TB) remain very accurate. We also note that the performance of On-Policy TB and On-Policy REVERSE KL degrades too, but not as significantly; furthermore, both of these methods achieve similar performance across all experimental settings, confirming our Observation 1, and the connection highlighted in § 2.2. The consistent behavior of the off-policy GFlowNet objectives compared to the on-policy objectives (TB & REVERSE KL) as the problem increases in complexity (i.e., as the number of nodes d increases, requiring better exploration) also supports our Observation 2. These observations are further confirmed when comparing the edge marginals P(Xi → X j | D) in Fig. D.3 (§D.3), computed either with the target posterior distribution or with the posterior approximations.\n\n5 DISCUSSION AND CONCLUSIONS\n\nThe theory and experiments in this paper place GFlowNets, which had been introduced and motivated as a reinforcement learning method, in the family of variational methods. They suggest that off-policy GFlowNet objectives may be an advantageous replacement to previous VI objectives, especially when the target distribution is highly multimodal, striking an interesting balance between the mode-seeking (REVERSE KL) and mean-seeking (FORWARD KL) VI variants. This work should prompt more research on how best to choose the behavior policy in off-policy GFlowNet training, seen as a means to efficiently explore and discover modes.\n\nWhereas the experiments performed here focused on the realm of discrete variables, future work should also investigate GFlowNets for continuous action spaces as potential alternatives to VI in continuous-variable domains. We make some first steps in this direction in the Appendix (§F). While this paper was under review, Lahlou et al. (2023) introduced theory for continuous GFlowNets and showed that some of our claims extend to continuous domains.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nAUTHOR CONTRIBUTIONS\n\nN.M., X.J., D.Z., and Y.B. observed the connection between GFlowNets and variational inference, providing motivation for the main ideas in this work. N.M., X.J., and T.D. did initial experimental exploration. S.L., N.M., and D.Z. contributed to the theoretical analysis. S.L. and N.M. extended the theoretical analysis to subtrajectory objectives. D.Z. reviewed the related work. S.L. performed experiments on the hypergrid domain. N.M. performed experiments on the molecule domain and the stochastic control domain. T.D., E.H., and K.E. performed experiments on the causal graph domain. All authors contributed to planning the experiments, analyzing their results, and writing the paper.\n\nACKNOWLEDGMENTS\n\nThe authors thank Moksh Jain for valuable discussions about the project.\n\nThis research was enabled in part by computational resources provided by the Digital Research Alliance of Canada. All authors are funded by their primary institution. We also acknowledge funding from CIFAR, Genentech, Samsung, and IBM.\n\nREFERENCES\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint 1806.01261, 2018.\n\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS), 2021a.\n\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward Hu, Mo Tiwari, and Emmanuel Bengio.\n\nGFlowNet foundations. arXiv preprint 2111.09266, 2021b.\n\nChristopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n\nDavid M. Blei, Michael I. Jordan, Thomas L. Griffiths, and Joshua B. Tenenbaum. Hierarchical topic models and the nested Chinese restaurant process. Neural Information Processing Systems (NIPS), 2003.\n\nJ ̈org Bornschein and Yoshua Bengio. Reweighted wake-sleep. International Conference on Learn-\n\ning Representations (ICLR), 2015.\n\nYuri Burda, Roger Baker Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.\n\nInternational Conference on Learning Representations (ICLR), 2016.\n\nRewon Child. Very deep VAEs generalize autoregressive models and can outperform them on im-\n\nages. International Conference on Learning Representations (ICLR), 2021.\n\nTristan Deleu, Ant ́onio G ́ois, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer, and Yoshua Bengio. Bayesian structure learning with generative flow networks. Uncertainty in Artificial Intelligence (UAI), 2022.\n\nAdji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John William Paisley, and David M. Blei. Variational inference via χ upper bound minimization. Neural Information Processing Systems (NIPS), 2017.\n\nJustin Domke and Daniel Sheldon. Importance weighting and variational inference. Neural Infor-\n\nmation Processing Systems (NeurIPS), 2018.\n\nDan Geiger and David Heckerman. Learning Gaussian networks. In Uncertainty Proceedings 1994,\n\npp. 235–243. Elsevier, 1994.\n\nWill Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Kristjanson Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. International Conference on Learning Representations (ICLR), 2018.\n\nKarol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep AutoRe-\n\ngressive networks. International Conference on Machine Learning (ICML), 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJos ́e Miguel Hern ́andez-Lobato, Yingzhen Li, Mark Rowland, Thang D. Bui, Daniel Hern ́andezLobato, and Richard E. Turner. Black-box alpha divergence minimization. International Conference on Machine Learning (ICML), 2016.\n\nLuke B. Hewitt, Tuan Anh Le, and Joshua B. Tenenbaum. Learning to learn generative programs\n\nwith memoised wake-sleep. Uncertainty in Artificial Intelligence (UAI), 2020.\n\nGeoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The “wake-sleep” algorithm for\n\nunsupervised neural networks. Science, 268 5214:1158–61, 1995.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Neural Infor-\n\nmation Processing Systems (NeurIPS), 2020.\n\nMatthew D. Hoffman, David M. Blei, Chong Wang, and John William Paisley. Stochastic variational\n\ninference. Journal of Machine Learning Research (JMLR), 14:1303–1347, 2013.\n\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena InterSimine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. national Conference on Machine Learning (ICML), 2022.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Chapter 11. junction tree variational autoencoder for molecular graph generation. Drug Discovery, pp. 228–249, 2020. ISSN 2041-3211.\n\nMichael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and Lawrence K. Saul. An introduction\n\nto variational methods for graphical models. Machine Learning, 37:183–233, 2004.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational Bayes. International Conference\n\non Learning Representations (ICLR), 2014.\n\nJack Kuipers, Giusi Moffa, and David Heckerman. Addendum on the scoring of Gaussian directed\n\nacyclic graphical models. The Annals of Statistics, 42(4):1689–1691, 2014.\n\nAshutosh Kumar, Arnout Voet, and Kam Y.J. Zhang. Fragment based drug design: from experimen-\n\ntal to computational approaches. Current medicinal chemistry, 19(30):5128–5147, 2012.\n\nSalem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hern ́andez-Garc ́ıa, L ́ena N ́ehale Ezzine, Yoshua Bengio, and Nikolay Malkin. A theory of continuous generative flow networks. arXiv preprint 2301.12594, 2023.\n\nTuan Anh Le, Adam R. Kosiorek, N. Siddharth, Yee Whye Teh, and Frank Wood. Revisiting reweighted wake-sleep for models with stochastic control flow. Uncertainty in Artificial Intelligence (UAI), 2019.\n\nTuan Anh Le, Katherine M. Collins, Luke B. Hewitt, Kevin Ellis, N. Siddharth, Samuel J. Gershman, and Joshua B. Tenenbaum. Hybrid memoised wake-sleep: Approximate inference at the discretecontinuous interface. International Conference on Learning Representations (ICLR), 2022.\n\nYingzhen Li, Jos ́e Miguel Hern ́andez-Lobato, and Richard E. Turner. Stochastic expectation propa-\n\ngation. Neural Information Processing Systems (NIPS), 2015.\n\nQiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference\n\nalgorithm. Neural Information Processing Systems (NIPS), 2016.\n\nIlya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. Interna-\n\ntional Conference on Learning Representations (ICLR), 2017.\n\nLars Maaløe, Marco Fraccaro, Valentin Li ́evin, and Ole Winther. BIVA: A very deep hierarchy of latent variables for generative modeling. Neural Information Processing Systems (NeurIPS), 2019.\n\nKanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning GFlowNets from partial episodes for improved convergence and stability. arXiv preprint 2209.12782, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in GFlowNets. Neural Information Processing Systems (NeurIPS), 2022.\n\nVaden Masrani, Tuan Anh Le, and Frank D. Wood. The thermodynamic variational objective. Neural\n\nInformation Processing Systems (NeurIPS), 2019.\n\nThomas P. Minka. Expectation propagation for approximate Bayesian inference. arXiv preprint\n\n1301.2294, 2001.\n\nThomas P. Minka. Divergence measures and message passing. 2005.\n\nAndriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. Inter-\n\nnational Conference on Machine Learning (ICML), 2014.\n\nAndriy Mnih and Danilo Jimenez Rezende. Variational inference for Monte Carlo objectives. Inter-\n\nnational Conference on Machine Learning (ICML), 2016.\n\nJohn William Paisley, David M. Blei, and Michael I. Jordan. Variational Bayesian inference with\n\nstochastic search. International Conference on Machine Learning (ICML), 2012.\n\nTom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood, and Yee Whye Teh. Tighter variational bounds are not necessarily better. International Conference on Machine Learning (ICML), 2018.\n\nRajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. Artificial Intelli-\n\ngence and Statistics (AISTATS), 2014.\n\nRajesh Ranganath, Dustin Tran, Jaan Altosaar, and David M. Blei. Operator variational inference.\n\nNeural Information Processing Systems (NIPS), 2016a.\n\nRajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models.\n\nInternational\n\nConference on Machine Learning (ICML), 2016b.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning (ICML), 2014.\n\nLorenz Richter, Ayman Boustati, Nikolas N ̈usken, Francisco J. R. Ruiz, and ̈Omer Deniz Akyildiz. VarGrad: A low-variance gradient estimator for variational inference. Neural Information Processing Systems (NeurIPS), 2020.\n\nLawrence K. Saul, T. Jaakkola, and Michael I. Jordan. Mean field theory for sigmoid belief net-\n\nworks. Journal of Artificial Intelligence Research, 4:61–76, 1996.\n\nArtem Sobolev and Dmitry Vetrov. Importance weighted hierarchical variational inference. Neural\n\nInformation Processing Systems (NeurIPS), 2019.\n\nCasper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder\n\nvariational autoencoders. Neural Information Processing Systems (NIPS), 2016.\n\nMichalis K. Titsias and Miguel L ́azaro-Gredilla. Doubly stochastic variational Bayes for non-\n\nconjugate inference. International Conference on Machine Learning (ICML), 2014.\n\nOleg Trott and Arthur J Olson. AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of Computational Chemistry, 31(2):455–461, 2010.\n\nGeorge Tucker, Andriy Mnih, Chris J. Maddison, John Lawson, and Jascha Narain Sohl-Dickstein. REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models. Neural Information Processing Systems (NIPS), 2017.\n\nArash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. Neural Information\n\nProcessing Systems (NeurIPS), 2020.\n\nNeng Wan, Dapeng Li, and Naira Hovakimyan. f-divergence variational inference. Neural Informa-\n\ntion Processing Systems (NeurIPS), 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nDilin Wang, Hao Liu, and Qiang Liu. Variational inference with tail-adaptive f-divergence. Neural\n\nInformation Processing Systems (NeurIPS), 2018.\n\nLex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning.\n\nUncertainty in Artificial Intelligence (UAI), 2001.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine Learning, 8(3):229–256, 1992.\n\nCathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M. Bayen, Sham M. Kakade, Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent factorized baselines. International Conference on Learning Representations (ICLR), 2018.\n\nMingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. International Conference\n\non Machine Learning (ICML), 2018.\n\nCheng Zhang, Judith B ̈utepage, Hedvig Kjellstr ̈om, and Stephan Mandt. Advances in variational inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41:2008–2026, 2019a.\n\nDinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative\n\nmodels with GFlowNets. arXiv preprint 2209.02606v1, 2022a.\n\nDinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML), 2022b.\n\nDinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative\n\nmodels with GFlowNets and beyond. arXiv preprint 2209.02606, 2023.\n\nMingtian Zhang, Thomas Bird, Raza Habib, Tianlin Xu, and David Barber. Variational f-divergence\n\nminimization. arXiv preprint 1907.11891, 2019b.\n\nHeiko Zimmermann, Hao Wu, Babak Esmaeili, Sam Stites, and Jan-Willem van de Meent. Nested\n\nvariational inference. Neural Information Processing Systems (NeurIPS), 2021.\n\nHeiko Zimmermann, Fredrik Lindsten, Jan-Willem van de Meent, and Christian A. Naesseth. A\n\nvariational perspective on generative flow networks. arXiv preprint 2210.07992, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA CANONICAL CONSTRUCTION OF A GRADED DAG\n\nFigure A.1: Illustration of the process by which a DAG (left) can turn into a graded DAG (right). Nodes with a double border represent terminating states. Nodes with a dashed border represent dummy states added to make the DAG graded.\n\nFig. A.1 shows the canonical conversion of a DAG into a graded DAG as described in §2.2. Note that this operation is idempotent: applying it to a graded DAG yields the same graded DAG.\n\nB PROOFS\n\nWe prove Prop. 1.\n\nProof For a complete trajectory τ ∈ T , denote by c(τ) = log\n\nPF ( τ )\n\nR ( xτ ) PB ( τ | xτ ) . We have the following:\n\n∇θ c(τ) = ∇θ log PF (τ) (13) ∇φc(τ) = −∇φ log PB (τ | x τ) = −∇φ log PB (τ) (14) Denoting by f1 : t ↦→ t log t and f2 : t ↦→ − log t, which correspond to the forward and reverse KL divergences respectively, and starting from\n\nLHVI, f2 (PF, PB) = D K L (PF ∥PB) = Eτ∼PF\n\nLHVI, f1 (PF, PB) = D K L (PB ∥PF) = Eτ∼PB\n\n(cid:20)\n\n(cid:20)\n\nlog\n\nlog\n\n(cid:21)\n\n(cid:21)\n\nPF (τ) PB (τ) PB (τ) PF (τ)\n\n= Eτ∼PF [c(τ)] + log ˆZ,\n\n= − (cid:0)Eτ∼PB [c(τ)] + log ˆZ (cid:1) ,\n\nwe obtain:\n\n∇θ LHVI, f2 (PF, PB) = ∇θ Eτ∼PF [c(τ)] = Eτ∼PF [∇θ log PF (τ)c(τ) + ∇θ c(τ)], ∇φLHVI, f1 (PF, PB) = −∇φEτ∼PB [c(τ)] = −Eτ∼PB [∇φ log PB (τ)c(τ) + ∇φc(τ)].\n\nFrom (13) and (14), we obtain:\n\nEτ∼PF [∇θ c(τ)] = Eτ∼PF [∇θ log PF (τ)] =\n\nHence, for any scalar Z > 0, we can write:\n\nPF (τ)∇θ log PF (τ) =\n\n∑︁\n\nτ ∈ T\n\n∇θ PF (τ) = ∇θ 1 = 0\n\n∑︁\n\nτ ∈ T\n\nEτ∼PF [∇θ c(τ)] = 0 = Eτ∼PF [∇θ log PF (τ) log Z]\n\nand similarly\n\nEφ∼PF [∇φc(τ)] = 0 = Eτ∼PB [∇φ log PB (τ) log Z].\n\nPlugging these two equalities back in the HVI gradients above, we obtain:\n\n∇θ LHVI, f2 (PF, PB) = Eτ∼PF [∇θ log PF (τ) log\n\n∇φLHVI, f1 (PF, PB) = −Eτ∼PB [∇θ log PB (τ) log\n\nZ PF (τ) R(x τ)PB (τ | x τ) Z PF (τ) R(x τ)PB (τ | x τ)\n\n]\n\n]\n\n14\n\ns0s1s2s3s4s5s6s7s8s0s1s2s7s5s4s8s6s3Published as a conference paper at ICLR 2023\n\nThe last two equalities hold for any scalar Z (that does not depend on the parameters of PF, PB, and that does not depend on any trajectory). In particular, the equations hold for the parameter Z of the Trajectory Balance objective. It thus follows that:\n\n∇θ LHVI, f2 (PF, PB) =\n\n∇φLHVI, f1 (PF, PB) =\n\nEτ∼PF\n\n∇θ\n\n(cid:34)\n\n(cid:34)\n\nEτ∼PB\n\n∇θ\n\n1 2\n\n1 2\n\n(cid:18)\n\n(cid:18)\n\nlog\n\nZ PF (τ) R(x τ)PB (τ | x τ)\n\nlog\n\nZ PF (τ) R(x τ)PB (τ | x τ)\n\n(cid:19) 2(cid:35)\n\n(cid:19) 2(cid:35)\n\n=\n\n=\n\n1 2\n\n1 2\n\nEτ∼PB [∇θ LTB(τ; PF, PB, Z)]\n\nEτ∼PB [∇φLTB(τ; PF, PB, Z)]\n\nAs an immediate corollary, we obtain that the expected on-policy TB gradient does not depend on the estimated partition function Z.\n\nNext, we will prove the identity (9), which we restate here:\n\nEτ∼PF [∇φLTB (τ)] = ∇φ\n\n(cid:104)\n\nD\n\nlog2 (PB ∥PF) + 2(log Z − log ˆZ)DKL (PF ∥PB)\n\n(cid:105)\n\n.\n\n(15)\n\nProof The RHS of (15) equals\n\n(cid:34)\n\n∇φ\n\nEτ∼PF\n\n∇φ\n\n(cid:34)\n\n(cid:20)\n\n=Eτ∼PF\n\n=Eτ∼PF\n\n(cid:34) (cid:18)\n\n(cid:32) (cid:18)\n\nlog\n\nPB (τ | x τ)R(x τ) ˆZ PF (τ)\n\nlog\n\nPB (τ | x τ)R(x τ) ˆZ PF (τ)\n\n(cid:19) 2\n\n(cid:19) 2\n\n+ 2(log Z − log ˆZ) log\n\n+ 2(log Z − log ˆZ) log\n\n(cid:35) (cid:35)\n\n(cid:33)(cid:35)\n\nPF (τ) ˆZ PB (τ | x τ)R(x τ)\n\nPF (τ) ˆZ PB (τ | x τ)R(x τ)\n\n2∇φ log PB (τ | x τ) log\n\n(cid:20)\n\n=2Eτ∼PF\n\n∇φ log PB (τ | x τ) log\n\n=Eτ∼PF [∇φLTB (τ)]\n\nPB (τ | x τ)R(x τ) ˆZ PF (τ) PB (τ | x τ)R(x τ) Z PF (τ)\n\n− 2(log Z − log ˆZ)∇φ log PB (τ | x τ)\n\n(cid:21)\n\n(cid:21)\n\nC A VARIATIONAL OBJECTIVE FOR SUBTRAJECTORIES\n\nIn this section, we extend the claim made in Prop. 1 to connect alternative GFlowNet losses to other variational objectives. Prop. 1 is thus a partial case of Prop. 2. This provides an alternative proof to Prop. 1.\n\nThe detailed balance objective (DB): The loss proposed in (Bengio et al., 2021b) parametrizes a GFlowNet using its forward and backward policies PF and PB respectively, along with a state flow function F, which is a positive function of the states, that matches the target reward function on the terminating states. It decomposes as a sum of transition-dependent losses:\n\n∀s→s′ ∈ A LDB (s→s′; PF, PB, F) =\n\n(cid:18)\n\nlog\n\nF (s)PF (s′ | s) F (s′)PB (s | s′)\n\n(cid:19) 2\n\n, where F (s′) = R(s′) if s′ ∈ X.\n\n(16)\n\nThe subtrajectory balance objective (SubTB): Both the DB and TB objectives can be seen as special instances of the subtrajectory balance objective (Malkin et al., 2022; Madan et al., 2022). Malkin et al. (2022) suggested instead of defining the state flow function F for every state s, a state flow function could be defined on a subset of the state space S, called the hub states. The loss can be decomposed into a sum of subtrajectory-dependent losses:\n\n∀τ = (s1, . . . , sn) ∈ T partial LSubTB(τ; PF, PB, F) =\n\n(cid:18)\n\nlog\n\nF (s1)PF (τ) F (sn)PB (τ | st )\n\n(cid:19) 2\n\n,\n\n(17)\n\nwhere PF (τ) is defined for partial trajectories similarly to complete trajectories (2), PB (τ | s) = (cid:206) (s→s′ ) ∈ τ PB (s | s′), and we again fix F (x) = R(x) for terminating states x ∈ X). The SubTB objective reduces to the DB objective for subtrajectories of length 1 and to the TB objective for complete trajectories, in which case we use Z to denote F (s0).\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA variational objective for transitions: From now on, we work with a graded DAG G = (S, A), in which the state space S is decomposed into layers: S = (cid:195)L HVI provides a class of algorithms to learn forward and backward policies on G. Rather than learning these policies (PF and PB) using a variational objective requiring distributions over complete trajectories, nested variational inference (NVI; Zimmermann et al., 2021)), which combines nested importance sampling and variational inference, defines an objective dealing with distributions over transitions, or edges. To this end, it makes use of positive functions Fk of the states sk ∈ Sk, for k = 0, . . . , L − 1, to define two sets of distributions ˇpk and ˆpk over edges from Sk to Sk+1:\n\nl=0 Sl, with S0 = {s0} and SL = X.\n\nˆpk (sk→sk+1) ∝ Fk (sk)PF (sk+1 | sk)\n\nˇpk (sk→sk+1) ∝\n\n(cid:26)R(sL)PB (sk | sL) Fk+1 (sk+1)PB (sk | sk+1)\n\nk = L − 1 otherwise\n\n.\n\nLearning the policies PF, PB and the functions Fk is done by minimizing losses of the form:\n\nLNVI(PF, PB, F) =\n\nL−1 ∑︁\n\nk=0\n\nD f ( ˇpk ∥ ˆpk)\n\n(18)\n\n(19)\n\nThe positive function Fk plays the same role as the state flow function in GFlowNets (in the DB objective in particular). Before drawing the links between DB and NVI, we first propose a natural extension of NVI to subtrajectories.\n\nC.1 A VARIATIONAL OBJECTIVE FOR SUBTRAJECTORIES Consider a graded DAG G = (S, A) where S = (cid:195)L l=0 Sl, S0 = {s0}, SL = X. Amongst the L + 1 layers l = 0, . . . , L, we consider K + 1 ≤ L + 1 special layers, that we call junction layers, of which the states are called hub states. We denote by m0, . . . , mK the indices of these layers, and we constrain m0 = 0 to represent the layer comprised of the source state only, and mK = L representing the terminating states X. On each non-terminating junction layer mk ≠ L, we define a state flow function Fk : Smk → R∗ +. Given any forward and backward policies PF and PB respectively, consistent with the DAG G, the state flow functions define two sets of distributions ˇpk and ˆpk over partial trajectories starting from a state smk ∈ Smk and ending in a state smk+1 ∈ Smk+1 (we denote by Tk the set comprised of these partial trajectories, for k = 0 . . . K − 1):\n\n∀τk = (smk → . . . →smk+1) ∈ Tk ∀τk = (smk → . . . →smk+1) ∈ Tk\n\nˆpk (τk) ∝ Fk (smk )PF (τk), ˇpk (τk) ∝ Fk+1 (smk+1)PB (τk | smk+1),\n\n(20) (21)\n\nwhere FK is fixed to the target reward function R.\n\nLemma 1 If ˆpk = ˇpk for all k = 0 . . . K − 1, then the forward policy PF induces a terminating state distribution P⊤\n\nF that matches the target unnormalized distribution (or reward function) R.\n\nProof Consider a complete trajectory τ = (sm0→ . . . →sm1→ . . . → . . . sm2→ . . . → . . . →smK ). And let τk = (smk → . . . →smk+1), for every k < K. Denote by ˆZk and ˇZk the partition functions (constant of proportionality in (18)) of ˆpk and ˇpk respectively, for every k < K. It is straightforward to see that for every 0 < k < K:\n\nˆZk+1 = ˇZk =\n\n∑︁\n\nFk+1 (smk+1)\n\nsmk+1\n\n∈ Smk+1\n\nK −1 (cid:214)\n\nk=0\n\nˆpk (τk) =\n\n(cid:206)K −1 k=0 (cid:206)K −1 k=0 Fk+1(smk+1) ˇZk\n\nFk (smk ) ˆZk\n\nPF (τ),\n\nPB (τ | smK ).\n\nK −1 (cid:214)\n\nˇpk (τk) =\n\n(cid:206)K −1 k=0\n\n(22)\n\n(23)\n\n(24)\n\n(cid:206)K −1 k=0 Because ˆpk = ˇpk for all k = 0 . . . K − 1, then both right-hand sides of (23) and (24) are equal. Combining this with (22), we obtain:\n\nk=0\n\n∀τ ∈ T\n\nF0 (s0) ˆZ0 (cid:32)(cid:32) (cid:32)(cid:32) (cid:123)(cid:122) (cid:125) (cid:124) =1\n\nPF (τ) =\n\nR(x τ) (cid:205)x ∈ X R(x)\n\nPB (τ | x),\n\n(25)\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nwhich implies the TB constraint is satisfied for all τ ∈ T . Malkin et al. (2022) shows that this is a sufficient condition for the terminating state distribution induced by PF to match the target reward function R, which completes the proof. Similar to NVI, we can use Lemma 1 to define objective functions for PF, PB, Fk, of the form:\n\nLSubNVI, f (PF, PB, F0:K −1) =\n\nK −1 ∑︁\n\nk=1\n\nD f ( ˇpk ∥ ˆpk)\n\n(26)\n\nNote that the SubNVI objective of (26) matches the NVI objective (Zimmermann et al., 2021) when all layers are junction layers (i.e. K = L, and mk = k for all k ≤ L), and matches the HVI objective of (5) when only the first and last layers are junction layers (i.e. K = 1, m0 = 0, and m1 = L).\n\nC.2 AN EQUIVALENCE BETWEEN THE SUBNVI AND THE SUBTB OBJECTIVES Proposition 2 Given a graded DAG G as in §2.1, with junction layers m0 = 0, m1, . . . , mK = L as in § C.1. For any forward and backward policies, and for any positive function Fk defined for the hubs, consider ˆpk and ˇpk defined in (20) and (21). The subtrajectory variational objectives of (26) are equivalent to the subtrajectory balance objective (17) for specific choices of the f -divergences. Namely, denoting by θ, φ the parameters of PF, PB respectively:\n\nEτk ∼ ˇpk [∇φLSubTB(τk; PF, PB, F)] = 2∇φ D f1 ( ˇpk ∥ ˆpk) Eτk ∼ ˆpk [∇θ LSubTB(τk; PF, PB, F)] = 2∇θ D f2 ( ˇpk ∥ ˆpk)\n\n(27) (28)\n\nwhere F = F0:K −1, and f1 : t ↦→ t log t and f2 : t ↦→ − log t.\n\nProof For a subtrajectory τk = (smk → . . . →smk+1) ∈ Tk, let c(τk) = log\n\nFirst, note that because ˆZk and ˇZk are not functions of φ, θ ((23)):\n\nFk (smk ) PF ( τk )\n\nFk+1 (smk+1\n\n) PB ( τk |smk+1\n\n∇φc(τk) = −∇φ log\n\n∇θ c(τk) = ∇θ log\n\nFk+1(smk+1)PB (τk | smk+1) ˇZk Fk (smk )PF (τk) ˆZk\n\n= ∇φ log ˆpk (τk)\n\n= −∇φ log ˇpk (τk)\n\n17\n\n) .\n\n(29)\n\n(30)\n\nPublished as a conference paper at ICLR 2023\n\nWe will prove (27). The proof of (28) follows the same reasoning, and is left as an exercise for the reader.\n\nD f1 ( ˇpk ∥ ˆpk) = D K L ( ˇpk ∥ ˆpk)\n\n∇φ D f1 ( ˇpk ∥ ˆpk) = ∇φ\n\n∑︁\n\nτk ∈ Tk\n\nˇpk (τk) log\n\nˇpk (τk) ˆpk (τk)\n\n= −∇φ\n\n∑︁\n\nτk ∈ Tk\n\nˇpk (τk)c(τk) + ∇φ log\n\nˆZk ˇZk (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125)\n\n(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)\n\n(cid:124)\n\n(cid:123)(cid:122) =0, according to (23)\n\n= −\n\n= −\n\n∑︁\n\nτk ∈ Tk ∑︁\n\nτk ∈ Tk\n\n(∇φ ˇpk (τk)c(τk) + ˇpk (τk)∇φc(τk))\n\n( ˇpk (τk)∇φ log ˇpk (τk)c(τk) + ˇpk (τk)∇φc(τk))\n\n= −Eτk ∼ ˇpk [∇φ log ˇpk (τk)c(τk)] +\n\n∑︁\n\nτk ∈ Tk\n\nˇpk (τk)∇φ log ˇpk (τk)\n\nfollowing (29)\n\n= −Eτk ∼ ˇpk [∇φ log PB (τk | smk+1)c(τk)] + ∇φ\n\n∑︁\n\nˇpk (τk)\n\nτk ∈ Tk\n\n(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)\n\n(cid:124)\n\n(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)\n\n(cid:125)\n\n(cid:123)(cid:122) =0\n\n(cid:20)\n\n= Eτk ∼ ˇpk\n\n∇φ log PB (τk | smk+1) log\n\n(cid:34)\n\nEτk ∼ ˇpk\n\n∇φ\n\n(cid:18)\n\nlog\n\nFk (smk )PF (τk) Fk+1 (smk+1)PB (τk | smk+1)\n\nEτk ∼ ˇpk [∇φLSubTB(τk; PF, PB, F)]\n\n=\n\n=\n\n1 2\n\n1 2\n\nFk+1(smk+1)PB (τk | smk+1) Fk (smk )PF (τk) (cid:19) 2(cid:35)\n\n(cid:21)\n\nAs a special case of Prop. 2, when the state flow function is defined for s0 only (and for the terminating states, at which it equals the target reward function), i.e. when K = 1, the distribution ˆp0 (τ) and PF (τ) are equal, and so are the distributions ˇp0(τ) and PB (τ). We thus obtain the first two equations of Prop. 1 as a consequence of Prop. 2.\n\nD ADDITIONAL EXPERIMENTAL DETAILS\n\nD.1 HYPERGRID EXPERIMENTS\n\nDetails about the environment For completeness, we provide more details about the environment, as explained in Malkin et al. (2022). In a D-dimension hypergrid of side length H, the state space S is partitioned into the non-terminating states So = {0, . . . , H − 1}D and terminating states X = S⊤ = {0, . . . , H − 1}D. The initial state is 0RD = (0, . . . , 0) ∈ So, and in addition to the transitions from a non-terminating state to another (by incrementing one coordinate of the state), an “exit” action is available for all s ∈ So, that leads to a terminating state s⊤ ∈ S⊤. The reward at a terminating state s⊤ = (s1, . . . , sD)⊤ is:\n\nR(s⊤) = R0 + 0.5\n\nD (cid:214)\n\nd=1\n\n(cid:20)(cid:12) (cid:12) (cid:12) (cid:12)\n\nsd H − 1\n\n1\n\n− 0.5\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n∈ (0.25, 0.5]\n\n(cid:21)\n\n+ 2\n\nD (cid:214)\n\nd=1\n\n(cid:20)(cid:12) (cid:12) (cid:12) (cid:12)\n\nsd H − 1\n\n(cid:12) (cid:12) − 0.5 (cid:12) (cid:12)\n\n1\n\n∈ (0.3, 0.4)\n\n(cid:21)\n\n, (31)\n\nwhere R0 is an exploration parameter (lower values indicate harder exploration). Architectural details The forward and backward policies are parametrized as neural networks with 2 hidden layers of 256 units each. The neural networks take as input a one-hot representation of a a state (also called K-hot, or multi-hot representations), which is a H×D vector including exactly D ones and (H − 1)D zeros, and output the logits of PF and PB respectively. Forbidden actions (e.g. when a coordinate is already maxed out at H − 1) are masked out by setting the corresponding logits to −∞ after the forward pass. Unlike Malkin et al. (2022), we do not tie the parameters of PF and PB.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure D.1: A comparison of the the type of baseline used (local or global) for the three HVI algorithms that use a score function estimator of the gradient.\n\nBehavior policy The behavior policy is obtained from the forward policy PF by subtracting a scalar ε from the logits output by the forward policy neural network. The value of ε is decayed from εinit to 0 following a cosine annealing schedule (Loshchilov & Hutter, 2017), and the value ε = 0 is reached at an iteration Tmax. The values of εinit and Tmax were treated as hyperparamters.\n\nHyperparameter optimization Our experiments have shown that HVI objectives were brittle to the choice of hyperparameters (mainly learning rates), and that the ones used for Trajectory Balance in Malkin et al. (2022) do not perform as well in the larger 128 × 128 grid we considered. To obtain a fair comparison between GFlowNets and HVI methods, a particular care was given to the optimization of hyperparameters in this domain. The optimization was performed in two stages:\n\n1. We use a batch size of 64 for all learning objectives, whether on-policy or off-policy, and the Adam optimizer with secondary parameters set to their default values, for the parameters of PF, the parameters of PB, and log Z (which is initialized at 0). The learning rates of PF, PB, log Z, along with a schedule factor γ < 1 by which they are multiplied when the JSD plateaus for more than 500 iterations (i.e. 500 × 64 trajectories sampled), were sought after separately for each combination of learning objective and sampling method (on-policy or off-policy), using a Bayesian search with the JSD evaluated at 200K trajectories as an optimization target. The choice of the baseline for HVI methods (except WS, that does not have a score function estimator of the gradient) was treated as a hyperparameter as well.\n\n2. All objectives were then trained for 106 trajectories using all the combinations of hyperparameters found in the first stage, for 5 seeds each. The final set of hyperparameters for each objective and sampling mode was then chosen as the one that leads to the lowest area under the JSD curve (approximated with the trapezoids method).\n\nFor off-policy runs, Tmax was defined as a fraction 1/n of the total number of iterations (which is equal to 106/64). The value of n and εinit was optimized the same way as the learning rate and the schedule, as described above.\n\nIn Fig. D.1, we illustrate the differences between the two types of baselines considered (global and local) for the 3 algorithms that use a score function estimator of the gradient, both on-policy and off-policy.\n\nSmaller environments: The environment studied in the main body of text (128 × 128, with R0 = 10−3) already illustrates some key differences between the Forward and Reverse KL objectives. As a sanity check for the HVI methods that failed to converge in this challenging environment, we consider two alternative grids: 64×64 and 8×8×8×8, both with an easier exploration parameter (R0 = 0.1), and compare the 5 algorithms on-policy on these two extra domains. Additionally, for the two-dimensional domain (64 × 64), we illustrate in Fig. D.2 a visual representation of the average distribution obtained after sampling 106 trajectories, for each method separately. Interestingly, unlike the hard exploration domain, the two algorithms with the mode-seeking KL (REVERSE KL and REVERSE WS) converge to a lower JSD than the mean-seeking KL algorithms (FORWARD KL and WS), and are on par with TB.\n\nD.2 MOLECULE EXPERIMENTS\n\nMost experiment settings were identical to those of Malkin et al. (2022), in particular, the reward model f the held-out set of molecules used to compute the performance metric, the GFlowNet model\n\n19\n\n3×1014×101On PolicyReverse KL1012×1013×1014×101Reverse WS102101Forward KLlocalglobal2.25×1012.5×1012.75×1013×1013.25×1013.5×1013.75×101Off Policy102101102101Published as a conference paper at ICLR 2023\n\nFigure D.2: Top: The evolution of the JSD between the learned sampler P⊤ F and the target distribution on the 8 × 8 × 8 × 8 grid left and the 64 × 64 grid right. Trajectories are sampled on-policy. Shaded areas represent the standard error evaluated across 5 different runs Bottom: The average (across 5 runs) final learned distribution P⊤ F for the different algorithms, along with the target distribution. To amplify variation, the plot intensity at each grid position is resampled from the Gaussian approximating the distribution over the 5 runs.\n\narchitecture (a graph neural network introduced by by Bengio et al. (2021a)), and the off-policy exploration rate. All models were trained with the Adam optimizer and batch size 4 for a maximum of 50000 batches. The metric was computed after every 5000 batches and the last computed value of the metric was reported, which was sometimes not the value after 50000 batches when the training runs terminated early because of numerical errors.\n\nD.3 BAYESIAN STRUCTURE LEARNING EXPERIMENTS\n\nBayesian Networks A Bayesian Network is a probabilistic model where the joint distribution over d random variables {X1, . . . , Xd } factorizes according to a directed acyclic graph (DAG) G:\n\np(X1, . . . , Xd) =\n\nd (cid:214)\n\np(Xi | PaG (Xi)),\n\ni=1 where PaG (Xi) is the set of parent variables of Xi in the graph G. Each conditional distribution in the factorization above is also associated with a set of parameters θ ∈ Θ. The structure G of the Bayesian Network is often assumed to be known. However, when the structure is unknown, we can learn it based on a dataset of observation D: this is called structure learning.\n\nStructure of the state space We use the same structure of graded DAG G as the one described in (Deleu et al., 2022), where each state of G is itself a DAG G, and where actions correspond to adding one edge to the current graph G to transition to a new graph G′. Only the actions maintaining the acyclicity of G′ are considered valid; this ensures that all the states are well-defined DAGs, meaning that all the states are terminating here (we define a distribution over DAGs). Similar to the hypergrid environment, the action space also contains an extra action “stop” to terminate the generation process, and return the current graph as a sample of our distribution; this “stop” action is denoted G → G⊤, to follow the notation introduced in §2.1.\n\nReward function Our objective in Bayesian structure learning is to approximate the posterior distribution over DAGs p(G | D), given a dataset of observations D. Since our goal is to find a forward policy PF for which P⊤ F (G) ∝ R(G) (see §2.1), we can define the reward function as the joint distribution R(G) = p(G, D) = p(D | G) p(G), where p(G) is a prior over graphs (assumed to be uniform throughout the paper), and p(D | G) is the marginal likelihood. Since the marginal likelihood involves marginalizing over the parameters of the Bayesian Network\n\np(D | G) =\n\n∫\n\nΘ\n\np(D | θ, G) p(θ | G) dΘ,\n\nit is in general intractable. We consider here a special class of models, called linear-Gaussian models, where the marginal likelihood can be computed in closed form; for this class of models, the log-marginal likelihood is also called the BGe score (Geiger & Heckerman, 1994; Kuipers et al., 2014) in the structure learning literature.\n\n20\n\n0k200k400k600k800k1000kTrajectories sampled105104103102101100JSD8x8x8x8TBReverse KLWSForward KLReverse WS0k200k400k600k800k1000kTrajectories sampled64x64Reverse KLReverse WSWSForward KLTBTarget DistributionPublished as a conference paper at ICLR 2023\n\nFigure D.3: Comparison of edge marginals computed using the target posterior distribution and using the posterior approximations found either with the GFlowNet objectives, or REVERSE KL. Performance is reported as the Root Mean Square Error (RMSE) between the marginals (lower is better).\n\nFor each experiment, we sampled a dataset D of 100 samples from a randomly generated Bayesian network. The (ground truth) structure of the Bayesian Network was generated following an Erd ̋osR ́enyi model, with about d edges on average (to encourage sparsity on such small graphs with d ≤ 5). Once the structure is known, the parameters of the linear-Gaussian model were sampled randomly from a standard Normal distribution N (0, 1). See (Deleu et al., 2022) for more details about the data generation process. For each setting (different values of d) and each objective, we repeated the experiment over 20 different seeds. Forward policy Deleu et al. (2022) parametrized the forward policy PF using a linear transformer, taking all the d2 possible edges in the graph G as an input, and returning a probability distribution over those edges, where the invalid actions were masked out. We chose to parametrize PF using a simpler neural network architecture, based on a graph neural network (Battaglia et al., 2018). The GNN takes the graph G as an input, where each node of the graph is associated with a (learned) embedding, and it returns for each node Xi a pair of embeddings ui and vi. The probability of adding an edge Xi → X j to transition from G to G′ (given that we do not terminate in G) is then given by\n\nPF (G′ | G, ¬G⊤) ∝ exp(u⊤\n\ni v j ),\n\nassuming that Xi → X j is a valid action (i.e., it doesn’t introduce a cycle in G), and where the normalization depends only on all the valid actions. We then use a hierarchical model to obtain the forward policy PF (G′ | G), following (Deleu et al., 2022):\n\nPF (G′ | G) = (1 − PF (G⊤ | G))PF (G′ | G, ¬G⊤).\n\nRecall that the backward policy PB is fixed here, as the uniform distribution over the parents of G (i.e. all the graphs were exactly one edge has been removed from G).\n\n(Modified) Detailed Balance objective For completeness, we recall here the modified Detailed Balance (DB) objective (Deleu et al., 2022) as a special case of the DB objective (Bengio et al., 2021b; see also (16)) when all the states of G are terminating (which is the case in our Bayesian structure learning experiments):\n\nL ( M ) DB (G → G′; PF, PB) =\n\n(cid:18)\n\nlog\n\nR(G′)PB (G | G′)PF (G⊤ | G) R(G)PF (G′ | G)PF (G⊤ | G)\n\n(cid:19) 2\n\n.\n\nOptimization Following (Deleu et al., 2022), we used a replay buffer for all our off-policy objectives ((Modified) DB, TB, and REVERSE KL). All the objectives were optimized using a batch size of 256 graphs sampled either on-policy from PF, or from the replay buffer. We used the Adam optimizer, with the best learning rate found among {10−6, 3 × 10−6, 10−5, 3 × 10−5, 10−4}. For the TB objective, we learned log Z using SGD with a learning rate of 0.1 and momentum 0.8.\n\nEdge marginals distribution p(G | D) and the posterior approximation P⊤\n\nIn addition to the Jensen-Shannon divergence (JSD) between the true posterior F (G) (see § E for details about how this\n\n21\n\n(Modified)DBOff-PolicyTBOn-PolicyTBOn-PolicyHVIOff-PolicyHVI0.00.10.20.30.40.50.6RMSENumberofnodes:d=3(Modified)DBOff-PolicyTBOn-PolicyTBOn-PolicyHVIOff-PolicyHVINumberofnodes:d=4(Modified)DBOff-PolicyTBOn-PolicyTBOn-PolicyHVIOff-PolicyHVINumberofnodes:d=5Edge marginalsPublished as a conference paper at ICLR 2023\n\ndivergence is computed), we also compare the edge marginals computed with both distributions. That is, for any edge Xi → X j in the graph, we compare\n\np(Xi → X j | D) =\n\n∑︁\n\np(G | D)\n\nand\n\nP⊤\n\nF (Xi → X j ) =\n\nG | Xi ∈PaG (X j )\n\n∑︁\n\nP⊤\n\nF (G).\n\nG | Xi ∈PaG (X j )\n\nThe edge marginal quantifies how likely an edge Xi → X j is to be present in the structure of the Bayesian Network, and is of particular interest in the (Bayesian) structure learning literature. To measure how accurate the posterior approximation P⊤ F is for the different objectives considered F (Xi → X j ), here, we use the Root Mean Square Error (RMSE) between p(Xi → X j for all possible pairs of nodes (Xi, X j ) in the graph.\n\n| D) and P⊤\n\nFig. D.3 shows the RMSE of the edge marginals, for different GFlowNet objectives and REVERSE KL (denoted as HVI here for brevity). The results on the edge marginals largely confirm the observations made in § 4.3: the off-policy GFlowNet objectives ((Modified) DB & TB) consistently perform well across all experimental settings; On-Policy TB & On-Policy REVERSE KL perform similarly and degrade as the complexity of the experiment increases (as d increases); and Off-Policy REVERSE KL has a performance that degrades the most as the complexity increases, where the edge marginals given by P⊤ | D) accurately.\n\nF (Xi → X j ) do not match the true edge marginals p(Xi → X j\n\nE METRICS Evaluation of the terminating state distribution P⊤ F: When the state space is small enough (e.g. graphs with d ≤ 5 nodes in the Structure learning experiments, or a 2-D hypergrid with length 128, as in the Hypergrid experiments), we can propagate the flows in order to compute the terminating F from the forward policy PF. This is done using a flow function F defined state distribution P⊤ recursively:\n\nF (s′) =\n\n(cid:26)1 (cid:205)s∈ Par (s′ ) F (s)PF (s′ | s)\n\nif s′ = s0 otherwise\n\nP⊤\n\nF is then given by:\n\nP⊤\n\nF (s⊤) ∝ F (s)PF (s⊤ | s),\n\n(32)\n\n(33)\n\nThe recursion can be carried out by dynamic programming, by enumerating the states in any topological ordering consistent with the graded DAG G. In particular, computation of the flow at a given terminating state s is linear in the number of states and actions that lie on trajectories leading to s, and computation of the full distribution P⊤\n\nF is linear in |S| + |A|.\n\nEvaluation of the Jensen-Shannon divergence (JSD) Similarly, when the state space is small enough, the target distribution P⊤ = R/Z ∗ can be evaluated exactly, given that the marginalization is over X only. The JSD is a symmetric divergence, thus motivating our choice. The JSD can directly be evaluated as:\n\nJSD (P⊤∥P⊤\n\nF) =\n\n=\n\n1 2\n1 2\n\n(cid:0)DKL (P⊤∥ M) + DKL (P⊤\n\n(cid:18)\n\n∑︁\n\ns∈ So\n\nP⊤(s) log\n\nF ∥ M)(cid:1) where M = (P⊤ + P⊤ 2P⊤ F (s) P⊤(s) + P⊤\n\nF (s) log\n\n+ P⊤\n\nF (s)\n\n2P⊤(s) P⊤(s) + P⊤\n\nF)/2\n\nF (s)\n\n(cid:19)\n\n(34)\n\n(35)\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nF EXTENSION TO CONTINUOUS DOMAINS\n\nAs a first step towards understanding GFlowNets with continuous action spaces, we perform an experiment on a stochastic control problem. The goal of this experiment is to explore whether the observations in the main text may hold in continuous settings as well. We consider an environment in which an agent begins at the point x0 = (0, 0) in the plane and makes a sequence of K = 10 steps over the time interval [0, 1], through points x0.1, x0.2, . . . , x1. Each step from xt to xt+0.1 is Gaussian with learned mean depending on xt and t and with fixed variance; the variance is isotropic with standard deviation 1 K . Equivalently, the agent samples the √\nEuler-Maruyama discretization with interval Δt = 1\n\nK of the Itˆo stochastic differential equation\n\n2\n\ndxt = f (xt , t) dt +\n\ndwt ,\n\n1 2\n\n(36)\n\nwhere wt is the two-dimensional Wiener process. The choice of the drift function f determines the marginal density of the final point, x1. We aim to find f such that this marginal density is proportional to a given reward function, in this case a quantity proportional to the density function of the standard 8gaussians distribution, shown in Fig. F.2. We scale the distribution so that the modes of the 8 Gaussian components are at a distance of 2 from the origin and their standard deviations are 0.25. In GFlowNet terms, the set of states is S = {(0, 0)} ∪ {(x, t) : x ∈ R2, t ∈ {0.1, 0.2, . . . , 1}}. States with t = 1 are terminating. There is an action from (x, t) to (x′, t′) if and only if t′ = t + Δt. The forward policy is given by a conditional Gaussian:\n\nx′ − x; f (x, t)Δt, PF ((x′, t + Δt) | (x, t)) = N (cid:169) (cid:173)\n\n(cid:171)\n\n(cid:33) 2\n\n(cid:32) √\n\nΔt 2\n\n.\n\n(cid:170) (cid:174)\n\n(cid:172)\n\nWe impose a conditional Gaussian assumption on the backward policy as well, i.e.,\n\nPB ((x, t) | (x′, t + Δt)) =\n\n(cid:26)N (cid:0)x − x′; μB (x′, t + Δt)Δt, σ2 1\n\nB (x′, t + Δt)Δt(cid:1)\n\nt ≠ 0 t = 0\n\n,\n\n(37)\n\n(38)\n\nwhere μB and log σ2 B are learned. Notice that all the policies, except the backward policy from time 1\nK to time 0, now represent probability densities; states can have uncountably infinite numbers of children and parents. We parametrize the three functions f , μB, log σ2 B as small (two hidden layers, 64 units per layer) MLPs taking as input the position x and an embedding of the time t. Their parameters can be optimized using any of the five algorithms in Table 1 of the main text.3 Fig. F.1 shows the marginal densities of xt (estimated using KDE) for different t in one well-trained model, as well as some sampled points and paths.\n\nIn addition to training on policy, we consider exploratory training policies that add Gaussian noise to the mean of each transition distribution. We experiment with adding standard normal noise scaled by σexp, where σexp ∈ {0, 0.1, 0.2}. Fig. F.2 compares the marginal densities obtained using different algorithms with on-policy and offpolicy training. The algorithms that use a forward KL objective to learn PB – namely, REVERSE WS and FORWARD KL – are not shown because they encounter NaN values in the gradients early in training, even when using a 10× lower learning rate than that used for all other algorithms (10−3 for the parameters of f , μB, log σ2\n\nB and 10−1 for the log Z parameter of the GFlowNet).\n\nThese results suggest that the observations made for discrete-space GFlowNets in the main text may continue to hold in continuous settings. The first two rows of Fig. F.2 show that off-policy exploration is essential for finding the modes and that TB achieves a better fit to the target distribution. Just as in Fig. 1, although all modes are found by WAKE-SLEEP, they are modeled with lower precision, appearing off-centre and having an oblong shape, which is reflected in the slightly higher MMD.\n\n3We conjecture (and strongly believe under mild assumptions) but do not prove that the necessary GFlowNet theory continues to hold when probabilities are placed by probability densities; the results obtained here are evidence in support of this conjecture.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure F.1: Above: KDE (2560 samples, bandwidth 0.25) of the agent’s position after i steps for i = 0, 1, . . . , 10 (t = 0, 0.1, . . . , 1) for a model trained with off-policy TB, showing a close match to the target distribution (also convolved with the KDE kernel for fair comparison). Below: A sample of 2560 points from the trained model and the trajectories taken by 128 of the points.\n\nOn-Policy σexp = 0\n\nOff-Policy\n\nσexp = 0.1\n\nσexp = 0.2\n\nAlgorithm\n\nTB\n\nMMD\n\n0.1111\n\n0.0005\n\n0.0075\n\nREVERSE KL\n\nMMD\n\n0.0027\n\n0.0059\n\n0.0036\n\nWAKE-SLEEP\n\nMMD\n\n0.0008\n\n0.0036\n\n0.0043\n\nTarget\n\nFigure F.2: KDE of learned marginal distributions with various algorithms and exploration policies and MMD with Gaussian kernel exp(−∥x − y∥2) estimated using 2560 samples.\n\n24\n\ns0s1s2s3s4s5s6s7s8s9s10target",
    "reference": "# Summary Of The Paper\n\nThis paper studies the relationship between variational inference and GFlowNet. This work finds out these two algorithms are equivalent in the sense of the expected gradients of their learning objectives. Moreover, they demonstrate the superiority of the GFlowNet on off-policy training.\n\n# Strength And Weaknesses\n\nStrength:\n- The connection between the variational inference and GFlowNet is a new finding. This paper finds the intrinsic property, variance reduction, that makes the GFlowNet stand out. They also verify that GFlowNet is capable of stable off-policy training without importance sampling.\n- This paper supplies a missing baseline result for hierarchical VI algorithms on modeling discrete random variables.\n\nWeaknesses:\n-   As the author mentioned in the conclusion section, this paper only studies the performance of GFlowNet and VI algorithms on modeling discrete random variables (since GFlowNet was mostly studied on such regime), which could be limited considering that VI algorithms are mostly used for continuous variables.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is well-written and explains clearly the relationship between variational inference and GFlowNet in certain cases. Moreover, this paper conducted plenty of experiments on hypergrid, molecule synthesis, and DAG generation to verify the superiority of the GFlowNet on off-policy training, which should be reproducible with the provided code. \n\nTypo:\nA typo in page 3, line 7, q(z_1,...,z_n)=q(z_1)q(z_2|z_1)...q(z_n|z_{n-1}) rather than x_1\n\n# Summary Of The Review\n\nOverall this paper is well-written and provides an interesting connection between GFlownet and VI algorithms, and demonstrate the advantage of GFlowNet on off-policy training.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSTEP\n\nONE SUPERVISED LEARNING\n\nTOWARDS\n\nSUSTAINABLE\n\nSELF-\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAlthough increasingly training-expensive, most self-supervised learning (SSL) models have repeatedly been trained from scratch but not fully utilized, since only a few SOTAs are employed for downstream tasks. In this work, we explore a sustainable SSL framework with two major challenges: i) learning a stronger new SSL model based on the existing pretrained SSL model, also called as “ base” model, in a cost-friendly manner, ii) allowing the training of the new model to be compatible with various base models. We propose a Target-Enhanced Conditional (TEC) scheme which introduces two components to the existing maskreconstruction based SSL. Firstly, we propose patch-relation enhanced targets which enhances the target given by base model and encourages the new model to learn semantic-relation knowledge from the base model by using incomplete inputs. This hardening and target-enhancing help the new model surpass the base model, since they enforce additional patch relation modeling to handle incomplete input. Secondly, we introduce a conditional adapter that adaptively adjusts new model prediction to align with the target of different base models. Extensive experimental results show that our TEC scheme can accelerate the learning speed, and also improve SOTA SSL base models, e.g., MAE and iBOT, taking an explorative step towards sustainable SSL.\n\n1\n\nINTRODUCTION\n\nSelf-supervised learning (SSL) has achieved overwhelming success in unsupervised representation learning, with astonishingly high performance in many downstream tasks like classification (Zhou et al., 2022a;b), object detection, and segmentation (Bao et al., 2021; He et al., 2022). In SSL, a pretext task is first built, e.g., instance discrimination task (He et al., 2020; Chen* et al., 2021) or masked image modeling (MIM) (Bao et al., 2021; He et al., 2022), and then pseudo labels are generated via the pretext task to train a network model without requiring manual labels. Though successful, SSL is developing towards a direction of requiring increasingly large training costs, e.g., 200 training epochs in MoCo (He et al., 2020) while 16,00 epochs in MAE (He et al., 2022) to release its potential. Unfortunately, most researchers only have limited computational budgets and often cannot afford to train large SSL models. Moreover, the pretrained non-SOTA SSL models are rarely used in practice, since SOTA is updated frequently and a previous one quickly becomes useless, wasting huge training resources. Thus, a sustainable SSL framework is much demanded.\n\n83.6%\n\nJust like how human experience is enriched and passed from one generation to the next in human society, we try to let an SSL model inherit the knowledge from a pretrained SSL base model to achieve superior representation learning ability for “sustainable” learning and also to improve learning efficiency than training a new SSL model from scratch. Fig. 1 illustrates the sustainable SSL for more clarity, in which we call the new SSL model to be trained as the new model and the pretrained SSL model as the base model. To surpass the base model, in sustainable SSL, the new model exploits not only the implicit base model knowledge but also the absent knowledge in the base model. Such a learning process follows a fully self-supervised manner and differs from the self-training schemes (Xie et al., 2020; Yalniz et al., 2019) that require labels for supervised learn-\n\nFigure 1: The concept of sustainable SSL.\n\n84.1%\n\n84.8%\n\n85.1%\n\n1\n\nPretrained base SSL modelNewSSL model...MAEiBOTDataGuideGuideNew model surpasses Base model.Under review as a conference paper at ICLR 2023\n\ning. This process can be regarded as a special case of Knowledge Distillation (KD) (Hinton et al., 2015; Gou et al., 2021), which targets at learning a more powerful new model based on the base model in a self-supervised setting.\n\ni\n\nI\n\nG M\n\nT O\nB\n\nIn this work, we take an explorative step towards sustainable SSL by efficiently learning from existing pretrained SSL models and surpassing them. In this work, to achieve this challenging goal, we encourage the new model to learn not only knowledge of the base model but also more semantic-related new knowledge. We therefore choose a mask-reconstruction (He et al., 2022) SSL scheme to train the new model, in which the base model generates reconstruction targets from the full input images and the new model tries to predict the generated targets from randomly masked image input. With this pretext task, the new model is forced to learn the semantics of the full input and its patch relations so that the new model can reason the desired full information from an incomplete input. As illustrated by Fig. 2, the attentions of iBOT (Zhou et al., 2022a) miss some semantic regions, e.g., ears, while TEC with iBOT as the base model captures all semantics and well distinguishes all different components of an input image. Because of its more powerful ability to capture comprehensive semantic, TEC helps achieve the challenging sustainable SSL, and actually can provide rich and flexible semantics for downstream tasks.\n\nFigure 2: Self-attention visualization. Different colors denote attentions of different heads. Black means no attention.\n\nC E\nT\n\nHowever, different SSL base models could have various properties due to their various training targets and training strategies, e.g., iBOT models with more category semantics while MAE models with more image details (He et al., 2022). So it is important to build high-qualified and compatible reconstruction targets from the base model so that the new model learns these targets in a complementary manner. A good model target should reveal the semantic relations among patches, e.g., the relation between car wheels and car body, so that new model can learn these general relation patterns and adapts to downstream tasks. To this end, we propose to enhance the target quality of the base model by using two complementary reconstruction targets: a) the patch-dim normalization which normalizes base model targets along patch dimension to enhance the relations among input patches, and b) patch attention maps with rich semantics to filter out possible noise and establish the correlation between the whole image semantic and the patch semantic. For target compatibility, we introduce conditional adapters into the new model so that new model predictions can be adaptable to various base models with different properties. Given a base model target, adapters conditionally active and adjust mid-level features of the new model to predict the target more effectively. These adapters are discarded after pretraining but can serve parameter-efficient finetuning (Jia et al., 2022; Chen et al., 2022b) if kept.\n\nthe above method for sustainable SSL We call as Target-Enhanced Conditional (TEC) maskreconstruction. As shown in Fig. 3, on ImageNet, TEC without any extra training data improves the SSL base model by a remarkable margin, e.g., MAE (He et al., 2022) and iBOT (Zhou et al., 2022a). For taking iBOT with 1600 epochs as base instance, model, TEC with only 800 training epochs makes 1.0% improvement. Moreover, we also find that TEC can significantly accelerate the SSL learning process and saves training cost. For example, training TEC for only 100 epochs with random initialization and a 300-epochs-trained MAE base model outperforms MAE trained with 1600 epochs. This work takes one step closer to sustainable SSL, and we hope our initial effort will inspire more works in the future to sustainably improve SSL in a cost-friendly manner.\n\n)\n\n%\n\n(\n\ny c\na r\nu c\nc A\n1 -\np o\nT\n\nt e\nN e\ng a\n\nm\n\nI\n\nTECiBOT\n\nTECMAE\n\nTECMAE300ep\n\niBOT\n\nMAE\n\nMAE\n\nPretraining Epochs on ViT-B/16\n\nFigure 3: Top1 accuracy on ImageNet-1k. TEC models have the same color with their base model.\n\n2\n\n3008001600 82.583.083.584.084.585.0 Under review as a conference paper at ICLR 2023\n\nZf\n\nZe\n\nZa\n\nYf\n\n′ q\n\nZ\n\nAs\n\n′ A\nc\n\nZq\n\nZk\n\nFigure 4: The overall framework of the proposed TEC. The pretrained SSL base model in TEC generates patch-relation enhanced reconstruction targets, i.e., patch-dim normalized features and semantic attention maps. The new ViT encoder takes in masked image and the class token enhanced by the input adapter, and then sequentially passes the generated features into encoder adapters and the multi-target decoder to predict the targets given by base model.\n\n2 METHOD\n\n2.1 OVERALL FRAMEWORK\n\nAn overall framework of the proposed target-enhanced conditional (TEC) mask-reconstruction method is illustrated in Fig. 4. TEC follows (He et al., 2022; Bao et al., 2021) to use Vision Transformer (ViT) (Dosovitskiy et al., 2020) for implementation. Under the mask-reconstruction framework (He et al., 2022), as shown in Fig. 4, TEC consists of a new ViT encoder to be pretrained, conditional adapters for conditional pretraining, a multi-target decoder for reconstruction targets prediction, an SSL pretrained ViT encoder as the base model, and a target-enhancing module to construct patch-relation enhanced reconstruction targets from the base model. Specifically, the base model is an SSL-pretrained ViT encoder (e.g., in MAE (He et al., 2022)) and is used to generate the latent semantic of a full image. Then target-enhancing module enhances the latent semantic to construct two complementary reconstruction targets as the supervision of the new model. The new ViT encoder together with adapters takes in masked images, and generates adapted latent semantics that are then fed into the multi-target decoder to predict the base model targets. After pretraining, the new ViT encoder is kept for downstream tasks while other parts are removed. At below, we will introduce the conditional pretraining aided by adapters in Section 2.2 which helps the new model effectively predict base model targets, and elaborate on the target-enhancing module to generate high-qualified base model targets in Section 2.3.\n\n2.2 CONDITIONAL PRETRAINING\n\nAs aforementioned, base models often have different properties, e.g., more global category semantic in iBOT while more local details in MAE. So the prediction of the new model should be compatible with any given base model. To resolve a similar issue on vanilla image pixel reconstruction, the works (Wang et al., 2022a; Dong et al., 2022; Gao et al., 2022) manually select certain features from the mid-level layers of the encoder by trial and error to better align with the image pixel target. However, it is almost impossible to manually select features from certain fixed layers that are compatible with different base models because of their possible different properties. So to better predict base model targets, the new model should have conditional adaptation ability regarding a given SSL base model.\n\nGiven a fixed pretrained model, the parameter-efficient fine-tuning scheme introduces trainable extra modules with a small number of parameters into this pretrained model for adapting it to downstream tasks in both vision (Jia et al., 2022; Chen et al., 2022b) and NLP (Houlsby et al., 2019; Li & Liang, 2021; Liu et al., 2021) domains. For example, the prompting scheme (Li & Liang, 2021; Liu et al., 2021; Jia et al., 2022) concatenates learnable input tokens, e.g., class token, with patch tokens to activate certain semantic features of a fixed ViT model that are suitable for specific downstream tasks. Also, inserting lightweight adapter modules (e.g., MLP (Houlsby et al., 2019; Chen et al., 2022b) and residual blocks (Li et al., 2022b)) into a fixed model can modulate mid-level features of the model to predict features required by the downstream task. Inspired by these parameter-efficient fine-tuning schemes, we introduce the adaptation scheme into the pretraining stage to handle the diversities of base models by equipping the new model with conditional adapters. Since our adapters are only used for pretraining and will be removed during finetuning, they do not increase extra\n\n3\n\nEncoder adapterCATMulti-targets decoderAttention mapsFeature mapsQKPatch selectionPatchesAttention SelectionConditionalAdapterTarget-EnhancingModuleBaseVIT encoderViTblocksCATEncoder adapter...New VIT encoder......Pred.Class tokenPatch-dim normSUMInput adapterUnder review as a conference paper at ICLR 2023\n\ninference costs. Actually, Tab. 4 shows that keeping these adapters in the inference phase enhances the parameter-efficient finetuning ability of the model. At below, we will introduce how to apply adapters, i.e., input and encoder adapters, into the new model encoder. Input adapter. For ViT networks, one often concatenates a class token with the input patch tokens to learn the global semantics of the whole input. Since the prompting scheme shows the adaption ability of the class token, we propose to further enhance the feature adaption ability of the class token by adding an input adapter. As shown in Fig. 5, the input adapter which is implemented by a small two-layer MLP layer enhances the representation ability of the class token so that the class token can better activate features in the new model according to the base model targets. Specifically, the class token T ∈ RC of the ViT is processed by the MLP layer to obtain an enhanced class token T\n\n′ n\nFigure 5: The input adapter and encoder adapter.\n\n∈ RC:\n\nBlocks\n\nZn\n\nZ\n\nT\n\nT\n\n′\n\n′\n\n′\n\nT\n\n= MLP(T ),\n\n′\n\n′\n\nas the new class token, meaning no extra cost is brought by MLP(T ).\n\nwhere C is the embedding dimension. During pretraining, T is appended to the patch tokens. MLP enhances the representation ability of T and enables the new model to better predict base model targets. For inference, since MLP(T ) is shared by all input samples, one can compute it in advance to get T Encoder adapter. To modulate mid-level features in the new model so that it can adapt to the base model targets, we apply a simple MLP with residual connection (Chen et al., 2022b) as our encoder adapter in the pretraining phase. As we hope to remove adapters after pretraining for higher inference efficiency, we need to keep the encoder network topology unchanged after removing adapters. So we put the input of adapters in the middle of the encoder and merge all adapter outputs at the end of the encoder. As shown in Fig. 5, given features X = {Xi, i = 1, . . . , D} from each encoder block where D is the encoder block number, we first uniformly divide them into N groups, in which each group contains 3 blocks by default. Within the nth group, we merge features from all blocks:\n\nThen we feed the feature Zn into an adapter and obtain an overall feature Ze:\n\nZn = FC(Concat(Xi, ..., Xj)).\n\nZ\n\n′\n\nn = Zn + MLP(Zn),\n\nZe =\n\n(cid:88)N\n\nn=1\n\nZ\n\n′\n\nn,\n\n(1)\n\nwhere MLP is a small MLP of two fully-connected layers. The adapted features are then fed into the multi-target decoder to predict base model targets, which will be introduced in Section 2.3.\n\n2.3 PATCH-RELATION ENHANCED RECONSTRUCTION TARGETS\n\nTo better exploit the knowledge of base models for sustainable SSL, our target-enhancing module constructs two complementary targets with enhanced patch relations: 1) feature-level targets with patch dimension normalization to strengthen the relations among patches; 2) semantic attention maps to learn relations between semantic patches and other patches. The feature-level targets reveal the semantics of certain patches, while attention maps focus more on relations among patch feature. Patch-dim normalized feature-level targets. Given a base model, we propose to normalize its target along the patch dimension to enhance the spatial patch relations. Specifically, for an input, assume its base model target is Y ∈ RL×C where L and C respectively denote patch number and channel dimension. Then we normalize Y along the patch dimension:\n\ni t\na R\n\no\n\nYf = (Y − μL)/σL,\n\n(2)\n\nPatch Similarity\n\nFigure 6: The patch similarity distribution of MAE.\n\nwhere μL and σL are respectively mean and variance along the patch dimension. For MIM, this patch-dim normalization can better enhance the spatial relations among tokens than the widely used feature normalization (Wei et al., 2022c;a; Baevski et al., 2022) along channel dimension. This is because as observed in Fig. 6, when using MAE for pretraining, the base model features of different patches actually have similar values and thus have high similarity score, since they all reveal the global semantic of the image. This cannot well reveal the spatial relations among these patches. As a result, the new model can easily reconstruct the feature\n\n4\n\nEncoder adapterFCMLPCATClstokenInput adapterMLP0.00.20.40.60.81.00.00.10.20.30.4No NormPatch-dim NormChannel-dim NormUnder review as a conference paper at ICLR 2023\n\ntarget of masked patches due to their high similarity with visible patches, but does not well learn spatial relations among patches. Normalization along channel dimension can hardly enhance the patch relations as it only consider the mean and variance within a patch. Actually, channel-dim normalization even enlarges the similarity among patches as revealed in Fig. 6. In contrast, patch-dim normalization ensures values within each channel has a clear difference and enhances the possible inherent spatial relations among patches by obviously reducing the similarity among patches as shown in Fig. 6. Moreover, Tab. 6(c) shows that our normalization can significantly improves the new model performance. After normalization, following (He et al., 2022), the new model uses a fully-connected layer followed by the decoder to generate Zf for predicting the base model target Yf on masked patches:\n\nLfea = ∥M ◦ (Yf − Zf )∥2 2,\n\n(3)\n\nwhere M is the mask matrix and ◦ denotes the element-wise product. Semantic attention-level targets. Self-attention in pretrained ViT models has a powerful capability of capturing semantic relations among patch tokens (Caron et al., 2021; Li et al., 2022c; Ziegler & Asano, 2022). We then propose to utilize the self-attention map as a type of reconstruction target for MIM to further enhance the semantic relation modeling capability of the new model. According to previous investigations on the effects of self-attention map in KD (Wu et al., 2022; Wang et al., 2022b), not all attention map contains useful semantic relations, and severe noisy attentions even hinder student learning. Accordingly, it is necessary to select parts of attention maps to reduce the possible severe noise and also help reduce training costs.\n\n′\n\n′\n\nHere we utilize the base model class token that contains sufficient global semantics to select top similar patch tokens, which filters out the possible noises. Specifically, given the attention maps Ac ∈ RH×L between class token and patch tokens from the last ViT block in base model where L and H respectively denote patch number and head number, we average the attention map Ac c ∈ R1×L. Then, as shown in Fig. 4, we select top-k patches along head dimension to obtain A c, and then compute the attention map Ap ∈ RH×k×L among the top-k with the largest values in A patches and all patch tokens. Considering the importance of the class token, we further concatenate attention maps between itself and selected Ap to obtain our final reconstruction targets, i.e., As ∈ RH×(k+1)×L. Note, when we compute As, a temperature τ is added before the Softmax operation to adjust the attention sharpness. For the new model, we respectively use two fully-connected layers to project its decoder output into two predictions Zq ∈ RL×C and Zk ∈ RL×C. We select the same q ∈ Rk×C. Then we concatenate the class token cls in new patches as in As from Zq to form Z q, cls]⊤Zk) ∈ RH×(k+1)×L. model with Z ′ Finally, we compute the predicted entropy loss between the prediction Za and the target As as\n\nq and compute the KQ attention map Za = Softmax([Z\n\n′\n\n′\n\nLatt = −As log Za.\n\n(4)\n\nMulti-target decoder. Due to the different properties of two reconstruction targets, namely feature target and attention target, one decoder in a new model for prediction is insufficient to handle them simultaneously and often results in prediction conflict. But using separate decoders for each target would increase the trainable parameters and thus slow down the training. To solve this issue, we use a simple decoder adaptation scheme that constructs target-specific inputs and then feeds them into a shared decoder. Specifically, we feed the output feature Ze (see Eqn. 1) of the new model encoder into a fully-connected layer and then fill the masked tokens with a learnable mask token f . Then similarly, given Ze, we also use a fully-connected layer and a learnable mask to obtain Z token to obtain Z m into a shared transformer-based decoder for predicting the feature and attention map targets of the base model. Unlike the large semantic gap between encoder output and vanilla image in MAE, the base model target has similar semantics to the new model predictions. So a shallow 2-layer decoder is enough and better than the 8-layer decoder used in MAE. This design also greatly reduces the training cost.\n\nm. Next, we respectively feed Z\n\nf and Z\n\n′\n\n′\n\n′\n\n′\n\n3 EXPERIMENTS We evaluate our TEC on ImageNet-1k (Deng et al., 2009) by pretraining randomly initialized ViT (Dosovitskiy et al., 2020) with a 16×16 patch size and 224×224 image resolution for 300/800 epochs via AdamW (Loshchilov & Hutter, 2017) of 4,096 batchsize. To ensure the improvement is from TEC, we do not use any explicit/implicit extra training data and the base models that are stronger than new model. Indeed, we respectively use iBOT (Zhou et al., 2022a) and MAE (He\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison with existing SSL methods under ImageNet-1k finetuning using ViT. † and gray color mean the usage of implicit /explicit extra data. The pretraining epoch number of TEC denotes the one from randomly initialized weights under the guidance of base models, and does not include that of the base model. Compared results are obtained from their reported results.\n\nModel\n\nMethod\n\nEpoch\n\nGuidance\n\nTop1 acc.\n\nViT-Base\n\nDeit III (Touvron et al., 2022) DINO (Caron et al., 2021) MoCov3 (Chen* et al., 2021) MixMIM (Liu et al., 2022b) MFM (Xie et al., 2022a) BEiT (Bao et al., 2021) SplitMask (El-Nouby et al., 2021) ConMIM (Yi et al., 2022) SimMIM (Xie et al., 2022b) SIM (Tao et al., 2022) CAE (Chen et al., 2022c) MaskFeat (Wei et al., 2022a) LoMaR (Chen et al., 2022a) BootMAE (Dong et al., 2022) data2vec (Baevski et al., 2022) Mugs (Zhou et al., 2022b) MVP (Wei et al., 2022b) PeCo (Dong et al., 2021) CMAE (Huang et al., 2022) Ge2-AE (Liu et al., 2022a) FD-CLIP (Wei et al., 2022c) MAE (He et al., 2022) FD-MAE (Wei et al., 2022c) TEC TEC iBOT-ImageNet-22K iBOT (Zhou et al., 2022a) SemMAE (Li et al., 2022a) TEC TEC\n\nViT-Large MAE (He et al., 2022)\n\nTEC\n\n800 300 300 300 300 800 300 800 800 1600 1600 1600 1600 800 800 1600 300 800 1600 800 300 1600 300 300 800 -\n1600 800 300 800\n\n1600 300\n\nSupervised NA NA RGB Frequency DALLE† NA Momentum RGB Momentum DALLE† HOG RGB RGB+Momentum Momentum NA CLIP† Perceptual codebook RGB RGB+Frequency CLIP† RGB MAE MAE MAE Momentum Momentum iBOT iBOT iBOT\n\nRGB MAE\n\n83.8 82.8 83.2 83.2 83.1 83.2 83.6 83.7 83.8 83.8 83.9 84.0 84.1 84.2 84.2 84.3 84.4 84.5 84.7 84.8 84.9 83.6 83.8+0.2 84.7+1.1 84.8+1.2 84.4 84.1 84.5+0.4 84.8+0.7 85.1+1.0\n\n85.9 86.5+0.6\n\nTable 2: Semantic segmentation on ADE20k using Upernet and ViT-B.\n\nTable 3: Instance segmentation on COCO using Cascade MaskRCNN and ViT-B.\n\nMethod\n\nBEiT PeCo GE2-AE CAE CMAE\n\nMAE TECMAE\n\niBOT TECiBOT\n\nEpoch\n\nmIoU\n\nMethod\n\nAPbbox\n\nAPmask\n\n800 800 800 1600 1600\n\n1600 800\n\n1600 800\n\n47.1 48.5 48.9 50.2 50.1\n\n48.1 49.9\n\n50.0 51.0\n\nImplementation from (Zhou et al., 2022a)\n\niBOT\n\nTECiBOT\n\n51.2\n\n52.7\n\n44.2\n\n45.4\n\nImplementation from (Li et al., 2022b)\n\nMAE\n\nTECMAE\n\n54.0\n\n54.6\n\n46.7\n\n47.2\n\net al., 2022) pretrained ViT model on ImageNet-1k as our base model. Base models are obtained from their official publicly released versions. We use the same masking strategy in MAE, e.g., 75% random masked ratio. See more training details in Appendix.\n\n3.1 PERFORMANCE COMPARISON\n\n3.1.1 COMPARISON ON IMAGENET Finetuning on ImageNet-1k. Tab. 1 summarizes the finetuning performance on ImageNet-1k. One can observe that with iBOT as base model, TEC surpasses the base model by 0.7% under 300 training epochs from random initialization, and further makes 1.0% improvement for 800 epochs. Similarly, TEC respectively brings 1.1% and 1.2% improvement over the MAE base model under 300/800 training epochs. These results show that thanks to the proposed target-enhanced conditional MIM scheme, TEC actually can further improve the strong MIM-based methods, e.g., MAE and iBOT used here. Besides, Tab. 1 also shows that under similar or even less training cost, TEC outperforms other SOTA SSLs, including methods trained by implicitly extra data, e.g., MVP (Wei et al., 2022b)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Top1 accuracy on the ImageNet-1k dataset under parameter-efficient finetuning.\n\nTable 5: Semi-supervised semantic segmentation on the ImageNet-S dataset.\n\nMethod\n\nEpoch\n\nSettings\n\nTop 1 acc.\n\nPretrain Method\n\nEpoch\n\nmIoUval\n\nMAE\n\n1600\n\nLinear probing\n\nTECMAE\n\n800\n\nLinear probing +Input adapter FT +Encoder adapter FT\n\n68.0\n\n69.8 72.6 79.9\n\nSSL\n\nSSL+FT\n\nMAE TECMAE\n\nMAE TECMAE\n\n1600 800\n\n1600+100 800+100\n\n38.3 42.9\n\n61.0 62.0\n\nand FD-CLIP (Wei et al., 2022c). More surprisingly, TEC with only ImageNet-1k data has an improvement of 0.7% over iBOT trained on ImageNet-22k, indicating more effectiveness of TEC pretraining over more training data. To the best of our knowledge, TEC sets a new SOTA 85.1% on ViT-B when solely using ImageNet-1k, showing the potential of sustainable SSL learning. We also investigate the scaling ability of TEC by using ViT-Large, and observe that TEC surpasses the MAE pretrained base model by 0.6% with 300 epochs from random initialization. Parameter-efficient finetuning on ImageNet-1k. Parameter-efficient finetuning methods, e.g., linear probing, aims to finetune a small amount of parameters for adapting to a downstream task. We test TEC under the linear probing setting which only finetunes a linear classifier at the top of a frozen pretrained model. Tab. 4 reports the classification accuracy of ViT-B on ImageNet-1k under linear probing. TEC improves the MAE base model by 1.8%, showing more category-related semantic information in the learned new model. Indeed, our input and encoder adapters used for pretraining can also be used for parameter-efficient finetuning. Finetuning input adapter via prompting brings a remarkable improvement of 4.6%, and finetuning both input and encoder adapters makes 11.9% improvement over the MAE base model. This also shows the benefit of our proposed adapters. Semantic segmentation on ImageNet-S. To test the pixel-level representation ability of TEC pretrained models, we conduct semantic segmentation finetuning on ImageNet-S (Gao et al., 2021) which has pixel-level training labels on ImageNet. We use ViT-B as the segmentation model without extra segmentation head, since the pretraining and finetuning data have no domain shift. Tab. 5 shows that TECMAE improves MAE base model by 4.6% on mIoU. When using supervised ImageNet fully-finetuned pretraining, TECMAE achieves a gain of 1.0% over MAE. 3.1.2 TRANSFER LEARNING ON DOWNSTREAM TASKS Here we investigate the transfer learning ability of TEC models on downstream tasks. Semantic segmentation. For semantic segmentation on the ADE20k (Zhou et al., 2018) dataset, we use Upernet (Xiao et al., 2018) with ViT-B as the segmentation model. Tab. 2 shows that TECiBOT surpasses the iBOT base model by 1.0% mIoU, and TECMAE achieves a 1.8% improvement over its MAE base model. Thus, TEC pretrained models show greater transfer learning abilities on semantic segmentation compared to their base models. Also, TEC shows remarkable advantages over strong competitors with fewer pretraining epochs. For example, it outperforms MAE, CAE (Chen et al., 2022c), and CMAE (Huang et al., 2022) by 2.9%, 0.8%, and 0.9%, achieving new SOTA. Instance segmentation. For instance segmentation on COCO (Lin et al., 2014), for fairness, we apply the Cascade MaskRCNN (Cai & Vasconcelos, 2019) implemented by iBOT (Zhou et al., 2022a) and ViTDet (Li et al., 2022b) for TEC with iBOT/MAE base models. Tab. 3 shows that with the implementation from iBOT, TEC surpasses the iBOT base model by 1.5% on box AP and 1.2% on mask AP; and when using the implementation from ViTDet, TEC also achieves a gain of 0.6% on box AP and 0.5% on mask AP, indicating stable improvements of TEC.\n\n3.2 ABLATION AND ANALYSIS\n\nWe give the ablation study and analysis of our TEC. By default, models are pretrained with 300 epochs and evaluated with the finetuning on ImageNet-1k. Conditional pretraining. The conditional adapters aid the SSL pretraining under different base models. Tab. 6(a) shows adapters stably improve the performance by 0.4% and 0.2% when using MAE and iBOT as base models. To observe the adaptation difference to base models, we show the average proportion of encoder adapters contributing to the encoder output in Fig. 7, i.e., Z\n\n′\n\nTECiBOT TECMAE\n\nn o\n\ni t\nr o\np o\nr\n\nP\n\ne g\na r\ne v\nA\n\nViT Encoder Layers Figure 7: The average proportion of encoder adapters contributing to the encoder output Ze.\n\nn/Ze in Eqn. 1. iBOT base model requires adapters to provide more features\n\n7\n\n0246810120.150.200.250.300.350.40 Under review as a conference paper at ICLR 2023\n\nTable 6: Ablation study on ImageNet-1K fully finetuning setting using ViT-B.\n\n(a) Ablation study of proposed modules.\n\nPatch-norm. feature\n\nSemantic attention\n\nAdapters\n\nMAE base\n\niBOT base\n\n✓ ✓\n✓ ✓\n\nBase model performance\n\n✓\n\n✓\n\n✓ ✓\n\n83.6%\n\n84.2% 84.3% 84.6% 84.7%\n\n84.1%\n\n84.5% 84.7% 84.7% 84.8%\n\n(b) Effect of adapters.\n\n(c) Patch-norm features.\n\n(d) Init. with base model pretrain.\n\nTop1 acc.\n\nTop1 acc.\n\nMAE base No adapter + input adapter + encoder adapter\n\n83.6 84.2 84.3 84.6\n\nMAE base NA Feature dim. Patch dim.\n\n83.6 83.9 83.9 84.2\n\niBOT base Load Not load\n\nTop1 acc.\n\n84.1 84.4 84.8\n\n(e) TEC accelerates MAE training.\n\n(f) Effect of semantic-related patch attention.\n\nEpoch\n\nTop1 acc.\n\nMAE TECMAE1600ep MAE TECMAE300ep TECMAE300ep\n\n1600 300 300 100 300\n\n83.6 84.7+1.1 82.9 83.9+1.0 84.3+1.4\n\niBOT base No attention Cls token only All attention Attention select\n\nTop1 acc.\n\n84.1 84.5 84.5 84.6 84.7\n\nfrom deeper layers, while MAE base model makes adapters focus more on shallow layers, which is constant with their properties, i.e., iBOT base model has more high-level category semantics while MAE model has more low-level image details. Feature normalization on different dimensions. We normalize target features on the patch dimension to stress the relative relations among patches, which differs from existing methods that normalize features on the channel dimension. In Tab. 6(c), normalizing on patch dimension achieves a 0.3% gain than channel-dim normalization. In contrast, the channel-dim normalization has no effect compared to the unnormalized version. Channel-dim normalization emphasizes the feature difference in channels. Instead, our patch-dim normalization stresses the relations among patches, which is compatible with the patch prediction in the MIM scheme. Tab. 6(a) shows training with patch-dim normalized feature has the 0.6%/0.4% gain over MAE/iBOT base models, showing its robustness over base models. Semantic-related attention. The KQ attention maps naturally contain the semantic relations among patches and thus are used as the base model targets with enhanced patch-relation properties. Tab. 6(a) shows that using attention maps further improves the models trained with patch-dim normalization. Tab. 6(f) compares the effects of different types of attention maps. Only using the attention maps of the class token has no improvement, while the attention of semantic-related patches brings a 0.2% gain over the baseline. Therefore, it is the relation among patches that helps the MIM training. Compared to using all attention maps, using the selected semantic-related attention maps brings a larger gain by reducing the noise. Accelerating the training process of base models. By default, we use the fully pretrained SSL models as base models. To verify if TEC can improve an unconverged SSL model, we use a 300epoch MAE pretrained ViT-B as the base model and train TEC with 100/300 epoch from random initialization. As shown in Tab. 6(e), the 300-epoch pretrained MAE gives a 82.9% Top.1 accuracy. In comparison, TECMAE300ep achieves 84.3%/83.9% with 300/100 epochs, surpassing the 300-epoch MAE base models with 1.4%/1.0%. Notably, TECMAE300ep even outperforms the 1600epoch pretrained MAE by 0.3% with only 100 epoch training, showing TEC can significantly accelerate the training process of the base model. Still, the TECMAE1600ep taught with 1600-epoch MAE base model further improves the TECMAE300ep by 0.4%, indicating our sustainable learning scheme relies on good base models to achieve better performance. Initializing new model with base model weights or not. The new models in the TEC framework are trained from random initialization. Tab. 6(d) compares the new model performance with/without loading the pretrained weights of the base model. The randomly initialized new model outperforms the model loaded with pretrained base model weights by 0.4%. We assume that randomly initialized\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nnew models avoid the local minima of the base model, and the new model learns a different weight distribution from the base model.\n\n4 RELATED WORKS\n\nSelf-supervised learning. Self-supervised learning enables representation pretraining without human annotation by training with pretext tasks, e.g., instance discrimination task (ID) and masked image modeling task (MIM). ID learn high category-related representations by pulling close representations from multiple views of one image Chen et al. (2020); Grill et al. (2020); Chen & He (2021); Zbontar et al. (2021); Caron et al. (2020). Extracting representations from multi-view requires larger training cost than supervised training. MIM learns semantics by reconstructing the information of masked patches from unmasked parts, which learns more spatial semantic details than ID. Due to the incomplete input, MIM usually requires longer training epochs than ID to converge. (Huang et al., 2022; Wang et al., 2022a) explore combining the advantages of MIM and ID to further improve performance. Recently, (Kong & Zhang, 2022) reveals both MIM and ID learn occlusion-invariant features. We observe a trend that these SSL methods require increasingly large computing costs to achieve SOTA, which hinders the development of new SSL methods. To remedy this, we explore sustainable SSL by learning from pretrained SSL models. Masked image modeling on various targets. The reconstruction targets guide the MIM learning on different semantic spaces. MIM has explored various reconstruction targets, e.g., RGB pixels and tokenizers. To make images similar to the discretized language in NLP (Devlin et al., 2018), Beit (Bao et al., 2021) utilizes the DALLE pretrained tokenizer (Ramesh et al., 2021) as the prediction target. CAE (Chen et al., 2022c) further decouples this pretext task prediction with the encoder. MAE and SimMIM (Xie et al., 2022b) show using RGB images as targets achieves competitive fully finetuning performance. MaskFeat (Wei et al., 2022a) reveals hand-designed HOG feature (Dalal & Triggs, 2005) is an effective target form. Ge2-AE (Liu et al., 2022a) and MFM (Xie et al., 2022a) find the image frequency domain can be complementary to RGB image targets. The perceptual codebook in PeCo (Dong et al., 2021) helps the model learn semantic information. The online momentum network (He et al., 2020) is used by iBOT and data2vec (Baevski et al., 2022) to provide updated prediction targets. BootMAE (Dong et al., 2022) takes advantage of RGB images and online network targets. (Yang et al., 2022) enhances the distillation from a large teacher model to a compact student model with the masking scheme. MVP (Wei et al., 2022b) introduces rich semantics learned from vision-language pretraining using the CLIP pretrained model (Ramesh et al., 2021) as the target. Unlike these works that stress the unique properties of a specific reconstruction target, we show that all SSL pretrained models can serve as good base models with the help of target-enhancement in TEC. The adapters and target-enhancing scheme in TEC enables the good adaptability to various base model targets. Self-supervised knowledge distillation. The sustainable SSL can be regarded as a special case of the self-supervised knowledge distillation as they both learn from SSL pretrained models. Reversed KD (Yuan et al., 2020) shows a weak teacher model can benefit the student in the supervised setting. ClusterFit (Yan et al., 2020) conducts training on the clustered pseudo-labels to reduce the overfitting to the pretext tasks. SEED (Fang et al., 2021) distillates knowledge from large SSL models to small models with contrastive loss. (Navaneet et al., 2022) uses MLP heads for feature regression to distill large SSL teachers to compact student models. (Xu et al., 2021) groups instances with teacher models and transfers the instance-relation knowledge to student models. As an exception, (Wei et al., 2022c) shows feature distillation improves contrastive-based SSL models but brings marginal gain over the SOTA MAE (He et al., 2022) model. Our sustainable SSL focuses on the new model outperforming the base model in a self-supervised manner. We show in Appendix that our TEC method is advantageous over several SOTA self-supervised distillation methods. 5 CONCLUSION This work explores sustainable self-supervised learning by learning from pretrained SSL models. We propose a target-enhanced conditional mask-reconstruction learning scheme to learn from and surpass existing SSL models. The adapters help to adapt the new model to various base models during pretraining and can also serve as parameter-efficient finetuning modules. We utilize the maskreconstruction scheme as the basis for surpassing base models, and we construct prediction targets with enhanced patch-level relations to aid the MIM pretraining. Our method further improves the strong MIM pretrained methods, e.g., MAE and iBOT, proving the feasibility of sustainable learning. This work takes an initial step towards sustainable SSL, and we will explore a more general multiround sustainable SSL framework in the future.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022.\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: high quality object detection and instance segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 43(5): 1483–1498, 2019.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In IEEE International Conference on Computer Vision (ICCV), 2021.\n\nCheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143, 2021.\n\nJun Chen, Ming Hu, Boyang Li, and Mohamed Elhoseiny. Efficient self-supervised vision pretrain-\n\ning with local masked reconstruction. arXiv preprint arXiv:2206.00790, 2022a.\n\nShoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. arXiv preprint\n\nAdaptformer: Adapting vision transformers for scalable visual recognition. arXiv:2205.13535, 2022b.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML), pp. 1597–1607. PMLR, 2020.\n\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022c.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE Confer-\n\nence on Computer Vision and Pattern Recognition (CVPR), June 2021.\n\nXinlei Chen*, Saining Xie*, and Kaiming He. An empirical study of training self-supervised vision\n\ntransformers. arXiv preprint arXiv:2104.02057, 2021.\n\nMMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox\n\nand benchmark. https://github.com/open-mmlab/mmsegmentation, 2020.\n\nNavneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’05), volume 1, pp. 886–893. Ieee, 2005.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248–255. Ieee, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Bootstrapped masked autoencoders for vision bert pretraining. In European Conference on Computer Vision (ECCV), 2022.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv ́e Jegou, and Edouard arXiv preprint\n\nGrave. Are large-scale datasets necessary for self-supervised pre-training? arXiv:2112.10740, 2021.\n\nZhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. Seed: Self-supervised distillation for visual representation. arXiv preprint arXiv:2101.04731, 2021.\n\nPeng Gao, Teli Ma, Hongsheng Li, Jifeng Dai, and Yu Qiao. Convmae: Masked convolution meets\n\nmasked autoencoders. arXiv preprint arXiv:2205.03892, 2022.\n\nShanghua Gao, Zhong-Yu Li, Ming-Hsuan Yang, Ming-Ming Cheng, Junwei Han, and Philip Torr.\n\nLarge-scale unsupervised semantic segmentation. arXiv preprint arXiv:2106.03149, 2021.\n\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A\n\nsurvey. International Journal of Computer Vision (IJCV), 129(6):1789–1819, 2021.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo ́Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R ́emi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16000–16009, 2022.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2(7), 2015.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning (ICML), pp. 2790–2799. PMLR, 2019.\n\nZhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui Shen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners. arXiv preprint arXiv:2207.13532, 2022.\n\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n\nSer-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.\n\nXiangwen Kong and Xiangyu Zhang. Understanding masked image modeling via learning occlusion\n\ninvariant feature. arXiv preprint arXiv:2208.04164, 2022.\n\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning (ICML), pp. 3519–3529. PMLR, 2019.\n\nGang Li, Heliang Zheng, Daqing Liu, Bing Su, and Changwen Zheng. Semmae: Semantic-guided\n\nmasking for learning masked autoencoders. arXiv preprint arXiv:2206.10207, 2022a.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\n\npreprint arXiv:2101.00190, 2021.\n\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer back-\n\nbones for object detection. arXiv preprint arXiv:2203.16527, 2022b.\n\nZhong-Yu Li, Shanghua Gao, and Ming-Ming Cheng. Exploring feature self-relation for self-\n\nsupervised transformer. arXiv preprint arXiv:2206.05184, 2022c.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European\n\nDoll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. Conference on Computer Vision (ECCV), pp. 740–755, 2014.\n\nHao Liu, Xinghua Jiang, Xin Li, Antai Guo, Deqiang Jiang, and Bo Ren. The devil is in the frequency: Geminated gestalt autoencoder for self-supervised visual pre-training. arXiv preprint arXiv:2204.08227, 2022a.\n\nJihao Liu, Xin Huang, Yu Liu, and Hongsheng Li. Mixmim: Mixed and masked image modeling\n\nfor efficient visual representation learning. arXiv preprint arXiv:2205.13137, 2022b.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nKL Navaneet, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Simreg: Regression as a simple yet effective tool for self-supervised knowledge distillation. arXiv preprint arXiv:2201.05131, 2022.\n\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, et al. Knowledge inheritance for pre-trained language models. arXiv preprint arXiv:2105.13880, 2021.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), pp. 8821–8831. PMLR, 2021.\n\nChenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image modeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204, 2022.\n\nHugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In European Confer-\n\nence on Computer Vision (ECCV), 2022.\n\nLuya Wang, Feng Liang, Yangguang Li, Wanli Ouyang, Honggang Zhang, and Jing Shao. Repre: Improving self-supervised vision transformer with reconstructive pre-training. arXiv preprint arXiv:2201.06857, 2022a.\n\nShaoru Wang, Jin Gao, Zeming Li, Jian Sun, and Weiming Hu. A closer look at self-supervised\n\nlightweight vision transformers. arXiv preprint arXiv:2205.14443, 2022b.\n\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14668–14678, 2022a.\n\nLonghui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, and Qi Tian. Mvp: Multimodality-guided\n\nvisual pre-training. arXiv preprint arXiv:2203.05175, 2022b.\n\nYixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022c.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nHaiyan Wu, Yuting Gao, Yinqi Zhang, Shaohui Lin, Yuan Xie, Xing Sun, and Ke Li. Self-supervised models are good teaching assistants for vision transformers. In International Conference on Machine Learning (ICML), pp. 24031–24042. PMLR, 2022.\n\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In European Conference on Computer Vision (ECCV), pp. 418–434, 2018.\n\nJiahao Xie, Wei Li, Xiaohang Zhan, Ziwei Liu, Yew Soon Ong, and Chen Change Loy. Masked frequency modeling for self-supervised visual pre-training. arXiv preprint arXiv:2206.07706, 2022a.\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10687–10698, 2020.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9653–9663, 2022b.\n\nHaohang Xu, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Bag of instances aggregation boosts self-supervised distillation. In International Conference on Learning Representations (ICLR), 2021.\n\nI Zeki Yalniz, Herv ́e J ́egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-\n\nsupervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019.\n\nXueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, and Dhruv Mahajan. Clusterfit: Improving generalization of visual representations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6509–6518, 2020.\n\nZhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, and Chun Yuan. Masked gener-\n\native distillation. arXiv preprint arXiv:2205.01529, 2022.\n\nKun Yi, Yixiao Ge, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, and Xiaohu Qie. Masked image modeling with denoising contrast. arXiv preprint arXiv:2205.09616, 2022.\n\nLi Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label smoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3903–3911, 2020.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ́ephane Deny. Barlow twins: Self-supervised\n\nlearning via redundancy reduction. CoRR, abs/2103.03230, 2021.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision (IJCV), 2018.\n\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.\n\nibot: Image bert pre-training with online tokenizer. International Conference on Learning Representations (ICLR), 2022a.\n\nPan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: A multi-granular self-supervised learning framework. In arXiv preprint arXiv:2203.14415, 2022b.\n\nAdrian Ziegler and Yuki M Asano. Self-supervised learning of object parts for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14502– 14511, 2022.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nTowards general sustainable SSL. This work aims to take one step towards sustainable SSL based on existing pretrained SSL models. To make more steps towards sustainable SSL, we use the TEC pretrained models as the base model for a new round of TEC pretraining. Tab. 7 shows that the second-round TEC trained with the first-round TEC base model achieves 85.2%. The possible reason for the smaller improvement in the second round is caused by the limited network capacity or two rounds of TEC pretraining learns similar knowledge.\n\nTraining cost comparison. Accelerating the training of a larger language model with the help of a smaller pretrained model (Qin et al., 2021; Chen et al., 2021) has been proven possible in the NLP field. We follow them to add the FLOPs, training time, and parameters comparison as shown in Tab. 8. TEC requires shorter training time to achieve better performance than the base models. For example, TEC outperforms iBOT/MAE by 0.7%/1.1% Top 1 accuracy with only 7%/20% training time. TEC has a similar number of parameters with MAE, because the shallow decoder saves parameters while adapters increase parameters. Both TEC and MAE have a larger number of parameters compared to iBOT due to the extra decoder. But benefiting from the decoder, they only process the visible patches in the encoder, thus requiring a smaller training cost than iBOT. As only a part of the model requires gradients in some SSL methods, e.g. the base model in TEC and online model in iBOT, which requires no backward cost to compute gradients, we compare the FLOPs for network parts with/without gradients. Benefiting from only processing the unmasked patches in the encoder and the shallow two-layer decoder, TEC requires smaller training FLOPs with gradients than iBOT and MAE. The extra FLOPs (FLOPs without gradients) of the base model in TEC are smaller than the online network in iBOT because no extra head is needed for the base model in TEC. Compared to MAE, the extra FLOPs of the base model can be partly balanced by the smaller FLOPs with gradients in TEC. Therefore, TEC has a similar training time with MAE for each training iteration.\n\nComparison on parameter-efficient finetuning. Tab. 9 reports the accuracy of 1) learning probing and 2) adapter finetuning which only finetunes the adapter and the linear classifier for parameterefficient finetuning. One can observe that 1) the linear probing performance of TEC relies on the base model, and 2) adapter finetuning significantly improves the performance. Indeed, most MIMbased models, e.g. BEIT and MAE, have much lower linear probing performance, since they do not use the global semantic learning loss, e.g. clustering loss or InforNCE instance discriminative loss. This also explains the lower performance of TEC compared with the global semantic learning methods, e.g. iBOT. But by finetuning the adaptors and also linear classifier, TEC improves iBOT with a remarkable margin of 3.9%. This is because as shown in Fig. 2, iBOT focuses more on distinguishing the patches related to global semantics and ignores the semantics of other patches, while TEC can group the patches into several semantic groups and further identify the semantics of each group. In this way, finetuning adapters help activate the semantic groups that are related to global semantics required by the downstream tasks, thus improving the model’s discriminability on global semantics and showing good parameter-efficient finetuning performance.\n\nDetailed schematic diagrams. We show more detailed schematic diagrams of conditional adapters in Fig. 10 and attention selection in Fig. 11 for a better understanding of these modules.\n\nComparison with self-supervised distillation methods. We compare several recently proposed self-supervised distillation methods on the fully finetuning performance on ImageNet. Tab. 10 shows the remarkable improvement of TEC over other self-supervised distillation methods. For using MAE ViT-B as the base model, TEC outperforms FD by a noticeable gain of 0.9%. When comparing with MaskFeat which also applies the MIM scheme, TEC has a gain of 0.6% when using MoCov3 ViT-B as the base model.\n\nTable 7: Towards general sustainable SSL using the TEC as the new base model.\n\nModel\n\niBOT TECiBOT TEC\n\nBase\n\nEpoch\n\nTop1 acc.\n\n- iBOT TECiBOT\n\n1600 800 800\n\n84.1 85.1 85.2\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Training cost comparison.\n\nMethod\n\nEpoch\n\nTime (8xA100)\n\nFLOPs (with grad)\n\nFLOPs (no grad)\n\nParameters\n\nTop 1 acc.\n\nVIT-B iBOT TECiBOT MAE TECMAE\n\n- 1600 300 1600 300\n\n- 361h 25h 125h 25h\n\n17.6G 19.2G 8.3G 9.8G 8.3G\n\n- 19.2G 17.6G 0G 17.6G\n\n86.6M 96.3M 118.6M 111.9M 118.6M\n\n- 84.1 84.8 83.6 84.7\n\nTable 9: Top1 accuracy on the ImageNet-1k dataset under linear probing (LP), adapter finetuning (Adapter FT), and fully finetuning (Fully FT). Method\n\nFully FT Top 1 acc.\n\nTop 1 acc.\n\nSettings\n\nEpoch\n\nBEiT SimMIM BootMAE CAE SemMAE CMAE Ge2-AE\n\nMAE TECMAE TECMAE\n\niBOT TECiBOT TECiBOT\n\n800 800 800 800 800 800 800\n\n1600 800 800\n\n1600 800 800\n\nLP LP LP LP LP LP LP\n\nLP LP Adapter FT\n\nLP LP Adapter FT\n\n56.7 56.7 66.1 68.6 68.7 73.9 75.3\n\n68.0 69.8 79.9\n\n79.8 78.0 81.9\n\n83.2 83.8 84.2 83.8 84.5 84.7 84.8\n\n83.6 84.7 84.7\n\n84.1 84.8 85.1\n\n′\n\nVisualization of patch-dim normalized feature-level targets and semantic attention-level targets. We visualize the selected semantic attention-level targets from the iBOT base model in Fig. 8. The averaged attention maps of the class token (A c) mostly focus on the high-semantic objects, thus making the selected patches belong to the high-semantic objects. The attention maps of selected patches contain the semantic relation between high-semantic objects and other regions. Different patches have some unique attention parts that differ from other patches. The attention maps of these selected patches focus on similar semantic objects but are complementary in some parts, which explains why using attention maps of selected patches is better than only using class token attention maps as shown in Tab. 6(f). We show the visualization of patch-dim normalized feature-level targets of iBOT base model in Fig. 9. Patch-dim normalized features are more distinguishable compared to the original and channel-dim normalized features. The spatial relation among feature patches is more clearly shown by the patch-dim normalization.\n\nPretraining settings on ImageNet-1k. We use the standard ViT network implemented in MAE. We give the pretraining settings in Tab. 11, which follows the pretraining settings in MAE. Due to the different properties of SSL base models, we set different parameters of semantic-related attention for MAE and iBOT base models, as shown in Tab. 12.\n\nInspired by (Baevski et al., 2022), we utilize the average output features of last two blocks of the base model as the feature target. We measure the CKA similarity (Kornblith et al., 2019) of each mid-layer block to the output of the last block, and we observe the high feature similarity of last two blocks.\n\nTable 10: Comparison with self-supervised distillation methods.\n\nMethod\n\nMAE FDMAE TECMAE\n\nMoCov3 MaskFeatMoCov3 TECMoCov3\n\nBase\n\n- MAE-ViT-B MAE-ViT-B\n\n- MoCov3-ViT-B MoCov3-ViT-B\n\nArch\n\nViT-B ViT-B ViT-B\n\nViT-B ViT-B ViT-B\n\nEpoch\n\nTop 1 acc.\n\n1600 300 300\n\n300 300 300\n\n83.6 83.8 84.7\n\n83.2 83.9 84.5\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nImage\n\nA\n\n′ c\n\nMulti-head attention maps of class token, top-3 patch, and top-9 patch.\n\nFigure 8: Visualization of the selected semantic attention-level targets from the iBOT base model.\n\nFully finetuning settings on ImageNet-1k. We give the fully finetuning settings on ImageNet-1k in Tab. 13. We observe that TECs trained with different base models may have different properties. Since these base models have different finetuning layer decay values, we set different layer decay values for TECs trained with different base models.\n\nParameter-efficient finetuning settings on ImageNet-1k. Following MAE, the linear probing settings are shown in Tab. 14. For parameter-efficient finetuning with the input adapter, we use the same training settings as used by the liner probing in Tab. 14. When finetuning with the encoder adapters, we use the same training settings as used by the fully finetuning in Tab. 13 due to more parameters are contained in encoder adapters.\n\nSemi-supervised semantic segmentation finetuning on ImageNet-S. We give the training settings of semi-supervised semantic segmentation finetuning on ImageNet-S in Tab. 15. We set different learning rates and layer decay weights for models initialized with pretrained weights with/without fully finetuning.\n\nDownstream task settings. For semantic segmentation on ADE20K, we use the MMSegmentation (Contributors, 2020) implementation of Upernet. The training configurations follow the MAE training configurations in MMSegmentation. Specifically, the models are trained for 160k iterations with the batch-size of 16 on 8 GPUs. The AdamW optimizer is used with the initial learning rate of 1e-4, weight decay of 0.05, 1,500 warmup iterations, and poly learning rate decay schedule. The ViT-B with 16×16 patch size is used as the backbone. The image size is set to 512×512 and the\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nNo\n\nPatch\n\nChannel\n\nNo\n\nPatch\n\nChannel\n\nNo\n\nPatch\n\nChannel\n\nFigure 9: Visualization of patch-dim normalized feature-level targets from the iBOT base model.\n\nFigure 10: The details of adapters for conditional pretraining.\n\ndefault data augmentation in MMSegmentation is utilized, i.e., random crop, random flip, and photo metric distortion. A layer decay rate of 0.65 is utilized for the ViT backbone.\n\nFor instance segmentation on COCO using Cascade MaskRCNN and ViT-B, we follow the training configurations of iBOT and ViTDet for TECiBOT and TECMAE. The ViT-B with 16×16 patch size is used as the backbone. TECiBOT follows the training strategy of iBOT using the MMDetection implementation. We train the model with the 3× schedule using the batch-size of 16 on 8 GPUs. The AdamW optimizer is used with the initial learning rate of 1e-4, weight decay of 0.05, and layer decay of 0.65. The learning rate is multiplied by 0.1 at the 27th and 33rd epoch. We train the model with 512×512 image size. The random flip and random resize crop augmentation is applied. For TECMAE, we use the ViTDet implementation of Cascade MaskRCNN. The model is trained with 100 epoch with the batch-size of 64 on 32 GPUs. The AdamW optimizer is used with the initial learning rate of 1e-4, weight decay of 0.1, and layer decay of 0.6. The learning rate is multiplied by 0.1 at 89th and 96th epoch. The input image size is 1024 × 1024. The random flip and random resize crop augmentation is applied.\n\n17\n\nClass token(C×L)T (C×1)Input adapterMLPClass tokenT! (C×1)Concat. (C×(L+1))Encoder adapterFCMLPEncoder adapterCATViTblocksCAT...New VIT encoder......SUM(3C×(L+1))Z\"(C×(L+1))Z\"!(C×(L+1))Z#(C×(L+1))Under review as a conference paper at ICLR 2023\n\nFigure 11: The details of the semantic attention map selection.\n\nTable 11: Pretraining settings.\n\nConfiguration\n\nOptimizer Base learning rate Weight decay Optimizer momentum Batch size Learning rate schedule Warmup epochs Augmentation\n\nValue\n\nAdamW 1.5e-4 0.05 β1, β2=0.9, 0.95 4096 Cosine decay 40 RandomResizedCrop\n\nTable 12: Parameters of semantic-related attention. Base model\n\nk\n\nτ\n\nMAE (ViT-Base) MAE (ViT-Large) iBOT (ViT-Base)\n\n1.8 1.4 1.0\n\n15 15 9\n\nTable 13: Settings of fully finetuning and parameter-efficient finetuning with encoder adapters.\n\nConfiguration\n\nOptimizer Base learning rate Min learning rate Weight decay Optimizer momentum Layer-wise lr decay Batch size Learning rate schedule Warmup epochs Training epochs Augmentation Label smoothing Mixup Cutmix Drop path\n\nValue\n\nAdamW 5e-4 (B), 1e-3(L) 1e-6 (B), 1e-5(L) 0.05 β1, β2=0.9, 0.999 0.55 (MAE-B), 0.65 (iBOT-B), 0.65 (MAE-L) 1024 Cosine decay 20 (B), 5 (L) 100 (B), 50 (L) RandAug (9, 0.5) 0.1 0.8 1.0 0.1\n\n18\n\nA!\" (1×L)Base Model Attention SelectionA# (H×k×L)Get Patch Atten. Select Top-k patchesA! (H×L)Avg.A$ (H×(k+1)×L)A! (H×L)Concat. Pred.Z% (H×(k+1)×L)Mul. Base Model Attention Maps(H×(L+1)×(L+1))New Model Feature MapsKZ& (L×C)Z’ (L×C)Z&\",cls (H×(k+1)×C()Select patchesZ’ (H×L×C()QNewModel Attention Selection((L+1)×C)Remove CLS TokenRemove CLS TokenUnder review as a conference paper at ICLR 2023\n\nTable 14: Settings of linear probing and parameter-efficient finetuning with the input adapter.\n\nConfiguration\n\nOptimizer Base learning rate Weight decay Optimizer momentum Batch size Learning rate schedule Warmup epochs Training epochs Augmentation\n\nValue\n\nLARS 0.1 0\n0.9 16384 Cosine decay 10 90 RandomResizedCrop\n\nTable 15: Settings of semantic segmentation finetuning on ImageNet-S.\n\nConfiguration\n\nOptimizer Base learning rate Weight decay Optimizer momentum Layer-wise lr decay Batch size Learning rate schedule Warmup epochs Training epochs Augmentation Drop path\n\nValue\n\nAdamW 5e-4 (SSL), 1e-4 (SSL+FT) 0.05 β1, β2=0.9, 0.999 0.60 (SSL), 0.45 (SSL+FT) 256 Cosine decay 5\n100 RandomResizedCrop 0.1\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper proposes an unsupervised method for improving already trained self-supervised models. The method, TEC, consists in reconstructing the masked patch representations of an image. The target, i.e. the feature representation of the dropped patches, are obtained from a pre-trained SSL model.\n\n# Strength And Weaknesses\n\n**Strengths**\n\n- Extensive comparisons with sota and thorough presentation of the related works.\n\n- The experimental results and absolute numbers reported in this paper are strong. This paper advances the state of the art in self-supervised representation learning.\n\n- In my opinion, the strongest result is in Table 7d, where the paper shows that MAE-300 epochs + TEC-100 epochs (i.e. 400 epochs)is competitive with MAE-1600 epochs pre-training. This convinces me that the method can speed up SSL pre-training.\n\n- Many ablations are presented.\n\n**Weaknesses**\n\n- Unconvincing “sustainability” ability of the paper.\nThis work starts from the observation that even though SSL is progressing in computer vision, models are not really used in practice for downstream tasks. The paper argues that this is because (i) pre-training SSL costs are too high and (ii) new and better SSL methods are appearing very frequently, preventing the field to stabilize on a framework.\nHowever this argumentation is flawed for the following reasons.\n(i) Models with extremely high pre-training budgets are being adopted in practice (GPTs, CLIP, models trained on JFT), meaning that expensive pretraining costs do no prevent the pretrained models to be used subsequently.\n(ii) The fact that new SSL methods arrive might just be a signal that SSL in vision is not fully mature yet and that further progress needs to be made to clearly beat supervised learning (which is still the most used pre-training in vision).\n\n- Table 6 is the weakest part of this paper. It shows that TEC does not achieve “sustainability” as advertised in Figure 1.\nUnlike the sustainability property advertised by the paper in Figure 1, Table 6 shows that a TEC model cannot be a based model for a new TEC-training round (marginal improvement of +0.1%).\n\n- Table 1 should include comparison with strong supervised baseline like DeiT-3. In addition, this is very misleading in Table 1 not to count the base model epochs into the total number of epochs. In other words, when training TEC-MAE for 300 epochs, the column should be 1900 epochs (1600 + 300) instead of 300.\n\n- Linear evaluation is 10 points below sota (iBOT for example). I find it weird that authors don’t report sota numbers in this Table 4.\n\n- The method is complex with many added components (input and encoder adapters, use of self-attention maps for feature selection). Complexity might not go towards sustainability.\n\n- This is not really a new SSL method but rather a feature distillation or enhancement one. Should compare with this literature (in last paragraph of related work) more and start from the same backbone as theirs for fair comparison.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**: the paper and figures is difficult to read and the contributions are not well stated. Also, I find that presenting the method in the scope of sustainability is rather unconvincing. The paper would benefit from some re-writing to emphasize more the fact that it is a kind of “self-supervised distillation method” (falling under the same umbrella as methods in the last paragraph of the related work section).\n\n**Novelty**: The different components of the method are not novel: reconstructing dropped patches based with feature representations from another model as target (BeiT, MaskFeat), leveraging the salient attention maps given by a ViT (LeoPart), distilling from a pre-trained SSL model (clusterFit, maskfeat).\nHowever we can argue that the novelty lies into their combination.\n\n**Reproducibility**: As a practitioner, I would argue that the paper does not give enough implementation details for allowing the reproduction of the reported results (for example there is no implementation details about data augmentation nor about the downstream tasks). It wouldn't be a problem if the paper mentions that code will be made publicly available but there is no such mention in the current version.\n\n# Summary Of The Review\n\nFor the reasons stated above (weaknesses section + problem of reproducibility), I lean towards rejection of the paper.\n\nThat being said, I still think the paper is promising and some reported results are strong. I will be willing to upgrade my initial assessment based on the authors’ rebuttal.\n\nIn any case (acceptance or rejection), I definitely think the paper should be re-written to tone down the “sustainability” aspect and present the contribution through the scope of “distillation” or “feature enhancing”.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDOWNSTREAM DATASETS MAKE SURPRISINGLY GOOD PRETRAINING CORPORA\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFor most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gains are attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning. In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around 10×–500× less data), outperforming the latter on 7 and 5 datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks, including the GLUE benchmark. Besides classification tasks, self-pretraining also provides benefits on structured output prediction tasks such as span based question answering and commonsense inference, often providing more than 50% of the performance boosts provided by pretraining on the BookWiki corpus. Our results hint that in many scenarios, performance gains attributable to pretraining are driven primarily by the pretraining objective itself and are not always attributable to the use of external pretraining data in massive amounts. These findings are especially relevant in light of concerns about intellectual property and offensive content in web-scale pretraining data.\n\n1\n\nINTRODUCTION\n\nFor training predictive models operating on natural language data, the current best practice is to pretrain models on large unlabeled upstream corpora to optimize self-supervised objectives, for example, masked language modeling (MLM); the resulting weights are then used to initialize models that are subsequently trained (finetuned) on the labeled downstream data available for the task at hand. Large-scale pretrained models typically provide significant performance boosts when compared to models trained directly on the downstream task (with random initializations) (Peters et al., 2018; Devlin et al., 2019; Chiang & Lee, 2020; Krishna et al., 2021). Upstream corpora tend to be significantly larger than the downstream corpora and the success of this approach is often attributed to its ability to leverage these massive upstream corpora (Liu et al., 2019; Yang et al., 2019). For example, the seminal BERT model (Devlin et al., 2019) was pretrained using the BookWiki corpus which is a combination of English Wikipedia and BooksCorpus (Zhu et al., 2015), totaling 13GB of plain text. Subsequent models have moved on to web-scale data. For example, XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2020)), were trained on 158GB, 160GB and 750GB of data, respectively.\n\nAs upstream corpus size and downstream performance have gone up, popular attempts at explaining these gains have focused on themes of “knowledge transfer” from the upstream corpus, attributing them to shared linguistic structure, semantics (Lina et al., 2019; Tenney et al., 2019), and facts about the world (Petroni et al., 2019). However, since the introduction of large-scale pretraining corpora occurred together with the invention of self-supervised pretraining objectives (e.g. masked language modeling (Devlin et al., 2019) and replaced token detection (Clark et al., 2019)), it remains unclear to what extent large-scale corpora are integral to these leaps in performance. For several tasks, especially summarization, recent works have managed to achieve surprising performance gains in settings where\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Aggregate performance of an ELECTRA model across 10 finetuning datasets when it is (i) randomly initialized (ii) pretrained on upstream corpus (BookWiki) (iii) pretrained on the finetuning dataset itself\n\nthe upstream corpus is created synthetically with arbitrary symbols, but the pretraining objective is designed to capture some of the structure of the task (Krishna et al., 2021; Wu et al., 2022).\n\nIn this work, we ask just how much of pretraining’s benefits could be realized in the absence of upstream corpora by pretraining directly on the downstream corpora (with the same self-supervised objectives). We find that this approach, which we call self-pretraining, often rivals the performance boosts conferred by off-the-shelf models pretrained on large upstream corpora (Figure 1), even outperforming them on 7 out of 10 datasets. Prior research has shown that additional self-supervised pretraining of off-the-shelf models using the downstream data can give further gains (Gururangan et al., 2020). Yao et al. (2022) showed that one can use the downstream data to retrieve a tiny subset of a large general corpus for pretraining effciently without sacrificing performance. Our study goes further, showing that even when starting from random initializations, and without using any external data beyond the downstream data itself, self-pretraining can rival standard practices. Since self-pretraining requires the same data that must already be available for downstream finetuning, the benefits of pretraining in this case cannot be attributed to transfer of knowledge from the upstream corpus. Instead, these benefits can only be attributed to the pretraining objective, which is possibly able to learn some inductive biases better than the finetuning objective (e.g. linguistic knowledge Tenney et al. (2019)), or perhaps simply initialize network parameters such that their statistics lead to better optimization during finetuning (Wu et al., 2022). While we note that similar observations have been made in the computer vision community (El-Nouby et al., 2021), we argue that it is especially important to establish these phenomena in the language domain, for which building on self-supervised pretrained models is now the ubiquitous practice of the vast majority of practitioners.\n\nTo understand differences in predictions with different pretraining strategies (i.e., between selfpretrained and off-the-shelf models), we analyse the errors made by these models on the same downstream data. Despite similar performance of these models, we find that self-pretrained and off-the-shelf models make significantly less correlated errors when compared to two independently finetuned models pretrained with either strategy. However, we observe that these uncorrelated mistakes do not transfer to improvements in the ensemble performance.\n\nWe find that models pretrained on one downstream dataset often perform surprisingly well when finetuned to other downstream datasets. Notably, the downstream datasets in our study come from a wide variety of domains such as news, online forums, tweets, reviews etc (Table 1). Nevertheless, we find that pretraining on any of these downstream datasets delivers significant performance gains on most datasets (greater than half of off-the-shelf model’s gains in 88% of cases) irrespective of domain. However, the best performance on a downstream dataset is usually achieved by the model pretrained on that dataset itself. Models pretrained on downstream datasets perform well on the GLUE benchmark too, despite having considerably less long-term dependencies as compared to standard upstream corpora. For example, the MNLI corpus consists of 2-sentence input texts that are concatenated in random order.\n\nIn addition to classification tasks, we also experiment with tasks such as span-based question answering, named entity recognition, and grounded commonsense inference. Self-pretraining delivers around 40-80% of the performance boost compared to models pretrained on the BookWiki corpus across ELECTRA and Roberta models. Hence, self-pretraining can perform better than fine-tuning randomly initialized models even for tasks that require prediction of more complex structured output than a single label, and for tasks whose solution relies on commonsense knowledge.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nOverall, our contributions can be summarized as follows:\n\n• Evaluation of self-pretraining across 10 downstream classification tasks and two pretraining techniques (ELECTRA and RoBERTa), with comparisons to off-the-shelf pretrained models.\n\n• Analysis of the out-of-distribution performance of models pretrained on one downstream\n\ndataset and finetuned on other downstream datasets including the GLUE benchmark.\n\n• Demonstration of self-pretraining’s efficacy on more complex tasks than classification such as tasks requiring structured output prediction and tasks requiring commonsense reasoning.\n\n2 RELATED WORK\n\nSelf-Pretraining in Computer Vision Most relevant to our work, two recent/concurrent works in computer vision explore self-pretraining (He et al., 2022; El-Nouby et al., 2021). In a contemporary work, He et al. (2022) showed that pretraining with a Masked AutoEncoder (MAE) objective (analogue of MLM objective for image datasets) significantly boosts the performance of ViT models on the Imagenet-1K dataset. In another related paper, El-Nouby et al. (2021) showed that pretraining solely on small-scale downstream datasets for object detection and segmentation tasks reaches the performance of Imagenet-pretrained models. Our work establishes that a similar phenomenon is observed for NLP tasks too across a wide range of datasets.\n\nPretraining on Downstream Data in NLP Task-Adaptive PreTraining (TAPT (Gururangan et al., 2020)) consists of taking off-the-shelf pretrained models like BERT and RoBERTa and engaging in further pretraining on the downstream datasets before finetuning them to the task at hand. TAPT has been shown to improve performance of off-the-shelf models in a variety of works (Logeswaran et al., 2019; Han & Eisenstein, 2019; Chakrabarty et al., 2019). By contrast, our work pretrains models only on the downstream dataset, enabling a head-to-head comparison between the performance of off-the-shelf and self-pretrained models, and (in some situations) challenging the necessity of upstream corpora altogether.\n\nClaims about Knowledge transfer Many works claim that pretraining extracts generally useful knowledge from the upstream corpus such as linguistic patterns (Lina et al., 2019; Tenney et al., 2019; Manning et al., 2020) and facts (Petroni et al., 2019), and that this accounts for the performance gains that they enjoy on downstream tasks. Several works, e.g., in the probing literature (Tenney et al., 2019; Manning et al., 2020; Petroni et al., 2019), demonstrate that from the internal representations of a model, it is easy (e.g., via linear models) to predict certain linguistic features or real-world facts. However, these studies do not clarify the mechanism by which these observations relate to performance gains on downstream tasks. Tenney et al. (2019) recognizes this limitation, stating “the observation of a (linguistic) pattern does not tell us how it is used”. Our work suggests that to the extent that such knowledge extraction plays a role in pretraining’s benefits, sufficient knowledge is often present in the downstream dataset and need not be transferred from huge upstream corpora.\n\nChallenges to the Knowledge Transfer Narrative Multiple previous works have questioned whether knowledge transfer can fully account for the efficacy of pretraining. Improvements in performance on downstream NLP tasks have resulted from pretraining on other modalities like music and code (Papadimitriou & Jurafsky, 2020), sequences of meaningless symbols (Chiang & Lee, 2020; Krishna et al., 2021; Wu et al., 2022), and language denatured via shuffling of words (Sinha et al., 2021). On the other hand, models pretrained on language have shown improved performance on tasks dealing with other modalities such as image classification Lu et al. (2021) and reinforcement learning for games Reid et al. (2022). By contrast, we show that without surplus upstream data of any modality, self-pretraining alone can often perform comparably or even better than standard pretraining with a large upstream corpus. In a similar vein with these papers, our work suggests that a large portion of pretraining’s success may come from alternative, unexplored mechanisms which have more to do with the pretraining objective than knowledge transfer from upstream corpora.\n\n3 EXPERIMENTAL SETUP\n\nOur experiments center around the ELECTRA model (Clark et al., 2019) and the RoBERTa-base model (Liu et al., 2019). On the broadest set of experiments, for which we can only afford to train one\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nDataset\n\nSize (MB) Classes Domain\n\nTask\n\nAGNews (Zhang et al., 2015) QQP (Wang et al., 2018) Jigsaw Toxicity (Kaggle.com, 2018) MNLI (Williams et al., 2018) Sentiment140 (Go et al., 2009) PAWS (Zhang et al., 2019) DBPedia14 (Zhang et al., 2015) Discovery (Sileo et al., 2019) Yahoo Answertopics (Zhang et al., 2015) Amazon Polarity (Zhang et al., 2015)\n\n27 43 59 65 114 139 151 293 461 1427\n\n4 News 2 Online forum questions 6 Wikipedia comments 3 Diverse 5 Tweets 2 Wikipedia 14 Wikipedia 174 Web crawl\n\n10 Online forum answers Product reviews\n\n2\n\ntopic classification paraphrase detection toxicity detection natural language inference sentiment classification paraphrase detection topic classification discourse marker prediction topic classification sentiment classification\n\nTable 1: The suite of downstream datasets used in this work along with their training set sizes\n\nmodel, we employ ELECTRA because it performs better than BERT/RoBERTa given comparable compute budgets (Clark et al., 2019). In particular, we use the small variant of ELECTRA (14 million parameters), which performs similarly to BERT-base on GLUE (difference of about 2 points) while training much faster (Clark et al., 2019). However, we replicate many of these results on the larger RoBERTa-base model revealing similar results and thus establishing the generality of our findings.\n\nDuring pretraining, a text sequence is fed into the model with some tokens masked out. While MLMonly models like RoBERTa only have a generator network that predicts the content of the masked tokens, ELECTRA has an additional discriminator module that predicts if those predictions were correct. Both the generator and the discriminator networks’ parameters are updated simultaneously during pretraining. After pretraining, the generator is discarded and the discriminator is used as an encoder for finetuning on downstream tasks.\n\nWe experimented with 10 different downstream datasets (Table 1). We chose these datasets in our testbed to span different dataset sizes ranging from 27 megabytes to about 1.4 gigabytes of text in the training split. These datasets are for different tasks such as topic classification, sentiment classification, natural language inference etc., and are created using data sourced from diverse domains. Most of them are multi-class classification tasks except Jigsaw Toxicity which is a multi-label classification task, and Sentiment140 which is modeled as a regression task. For finetuning a pretrained model on any dataset, we passed the input through the model, took the vector representation of the CLS token in the final layer, and passed it through a classification head with one hidden layer to get the output.\n\n4 SELF-PRETRAINING PERFORMANCE\n\nIn our first set of experiments, we compare self-pretraining’s performance with other pretraining techniques. For each dataset, we pretrain an ELECTRA model on text from its training split and then finetune it on the same training data using the associated labels. To create a pretraining corpus from a downstream dataset, we concatenate the input text from each of the examples, assembling them in random order. We evaluate the performance of each finetuned model on the corresponding dataset’s test split1. For all datasets, we evaluate performance by accuracy, except for Sentiment140 and Jigsaw Toxicity, for which we use Pearson correlation and micro-averaged AUC scores, respectively (these are not multi-class classification problems).\n\nNotably, all self-pretrained models deliver significant performance boosts on their respective datasets (Table 2), and over half of them perform even better than the off-the-shelf model. We measured a model’s benefit as the performance boost that it achieves over a randomly initialized model, divided by the boost achieved by the off-the-shelf ELECTRA model against the same baseline. The average benefit of self-pretraining across all datasets is 103.70%. We do not see a clear correlation between the size of the dataset and the performance of self-pretraining. For example, the highest benefit of 131.33% is achieved for the smallest dataset (AGNews), which is merely 27MB in size, while the minimum benefit is achieved on the Discovery dataset, which is the third largest dataset measuring 293MB. For each downstream dataset, we also pretrain a model on a randomly sampled subset of Wikipedia of the same size as the dataset’s training corpus, and finetune it on the downstream task. This approach (called WikiSub) provides a size-adjusted comparision between using separate upstream data vs the downstream data for pretraining. We see that self-pretraining performs better\n\n1For QQP and MNLI we just use the validation split because test set labels are private.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nthan WikiSub in majority of cases and when it performs worse (MNLI and Discovery datasets), the performance gap is much smaller than the gap between offshelf and self-pretrained models (Table 2).\n\nWe also evaluated the alternate pretraining technique TAPT as described in Gururangan et al. (2020). In this technique, we take the off-the-shelf ELECTRA model, which has already been pretrained on the upstream BookWiki corpus, and further pretrain it on the downstream dataset for 100 epochs. Self-pretraining outperforms TAPT on 6 datasets, notably including the two datasets where it outperformed the off-the-shelf models by the greatest benefit margin - AGNews and Yahoo Answertopics. Interestingly, TAPT performs worse than off-the-shelf model on the same 3 datasets where selfpretraining performs worse than off-the-shelf model (except Sentiment140). None of the three pretraining approaches seem to be uniformly better than any other.\n\nFinally, we also evaluate the self-pretrained models on the GLUE benchmark and report results on the dev set 2. The performance of the models on their pretraining dataset does not seem to be correlated with its GLUE score. The GLUE score also does not monotonically go up with increasing dataset size, indicating that the data domain makes some difference. For example, the Amazon Polarity corpus scores just 66.14 on GLUE despite being about 1.4 gigabytes in size, while AGNews which is 27MB in size, scores 74.30. The highest GLUE score is achieved by pretraining on Yahoo Answertopics.\n\nDataset\n\nSize(MB) RandInit SelfPretrain Offshelf Benefit% WikiSub TAPT GLUE\n\nAGNews QQP Jigsaw Toxicity MNLI Sentiment140 PAWS DBPedia14 Discovery Yahoo Answertopics Amazon Polarity\n\n27 43 59 65 114 139 151 293 461 1427\n\n91.75 82.93 97.83 65.49 63.75 50.00 98.59 17.00 61.94 93.86\n\n94.34 90.66 98.49 78.39 67.04 97.53 99.22 22.38 65.26 96.27\n\n93.72 90.34 98.53 82.29 66.95 97.30 99.11 24.55 64.55 96.13\n\n131.33 104.34 94.99 76.77 102.91 100.49 121.17 71.22 127.31 106.49\n\n93.51 89.16 98.35 78.64 65.52 97.42 99.18 22.47 64.37 95.82\n\n94.07 90.64 98.48 79.26 65.65 97.85 99.23 23.58 65.05 96.16\n\n74.30 75.43 76.65 78.28 72.67 74.65 70.38 77.26 79.53 66.14\n\nTable 2: Performance of ELECTRA-small models pretrained with different techniques on various downstream datasets and on the GLUE benchmark (dev set). For reference, a randomly initialized model scores 53.20 and the off-the-shelf model scores 79.43 on GLUE.\n\n5 CROSS DATASET FINETUNING\n\nIn this set of experiments, we investigated if the models pretrained on a dataset are only useful for that specific task, or are they useful across the whole spectrum of tasks that we consider. We took each model pretrained on a dataset in our testbed and finetuned and evaluated it on all other datasets in the testbed. The performance benefits provided in all cases are shown as a heatmap in Figure 2.\n\nWe found that for almost all downstream datasets, pretraining on any other dataset provides significant advantage (Figure 2). In most cases, pretraining on the downstream dataset itself performs the best. Among datasets where self-pretraining performs better than off-the-shelf model (i.e. the diagonal entry is greater than 1), pretraining on datasets of larger size does not help further. However, for the datasets where self-pretraining’s benefit is much less than 100% (i.e. MNLI and Discovery), pretraining on a larger dataset (e.g., Yahoo Answertopics) performs better than self-pretraining.\n\nAmong all the pretrained models, a few models perform consistently good or bad across different downstream datasets (Figure 2). For example, the model pretrained on Yahoo Answertopics gets the highest average score of 0.90 across all datasets, while the PAWS-pretrained model gives the lowest aggregate score of 0.64. Similarly, there are downstream datasets that are benefited consistently by either a large or a small margin by pretraining on different datasets. For example, performance on QQP and PAWS receives huge boosts by pretraining on most datasets. In contrast, performance on sentiment140 is low for most pretrained models, even dropping below 20% for 3 pretraining datasets.\n\nNext, we perform an ablation to investigate that given a fixed dataset to finetune on, is it better to pretrain on the exact same data (i.e., using the same set of inputs), or is it better to pretrain on different data with an identical distribution. To test this hypothesis, we divided downstream datasets into two\n\n2Following Clark et al. (2019) we exclude the WNLI task from the results.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Performance benefits of models pretrained on each dataset, upon finetuning on each downstream dataset. Each value is the ratio of performance gains achieved by model pretrained on the row’s dataset vs off-the-shelf model, relative to random initialization, upon finetuning on the column’s dataset.\n\nMNLI\n\nQQP\n\nDiscovery\n\nYahoo Answertopics\n\nA A 76.00 B 75.93\n\nB 76.42 75.05\n\nA A 84.28 B 88.73\n\nB 84.79 88.41\n\nA A 18.78 B 19.99\n\nB 18.61 19.98\n\nA A 64.18 B 64.09\n\nB 64.34 64.18\n\nTable 3: Performance when splitting the dataset into two equal-sized subsets A and B and then pretraining on one (row) and finetuning on another (column)\n\nsubsets (denoted as A and B) by randomly splitting the training dataset into half. We pretrained one model on each subset and then finetuned them on both subsets separately. The validation and test sets used for finetuning are the same as in the original dataset.\n\nWe do not see any consistent benefits with pretraining and finetuning on the same dataset (Table 3). Instead, we found consistent patterns where models pretrained on one split (either A or B) outperformed models pretrained on the other, irrespective of the split used for finetuning. This suggests that the pretraining data has greater influence on the final performance than the finetuning data. Additionally, we observe that finetuning the superior pretrained model, using the downstream split other than the one used for pretraining, performs the best, suggesting overall exposure to more data helps.\n\n6 DIFFERENCE IN OUTPUTS OF SELF-PRETRAINED AND OFF-THE-SHELF\n\nMODELS\n\nSince self-pretrained models and off-the-shelf models perform similarly in terms of classification accuracy, a natural question to ask is: do these models make errors on the same set of inputs? To answer this question, we investigate the difference in predictions made by models pretrained with different strategies across all multi-class classification tasks. In particular, given model fA and fB,\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nEnsemble Accuracy\n\nError Inconsistency\n\nDataset\n\n2×SelfPretrain\n\n2×Offself\n\nSelfPretrain + Offself\n\n2×SelfPretrain\n\n2×Offself\n\nSelfPretrain + Offself\n\nAGNews QQP MNLI PAWS DBPedia14 Discovery Yahoo Amazon\n\n94.66 90.92 78.51 97.70 99.28 22.98 65.32 96.40\n\n94.17 90.74 82.37 97.45 99.19 25.25 64.69 96.24\n\n94.54 91.63 82.31 97.75 99.24 25.02 65.64 96.51\n\n1.76 4.57 6.94 0.96 0.38 7.85 5.27 1.26\n\n3.50 5.27 6.42 1.30 0.48 9.18 5.49 1.58\n\n4.01 8.91 14.82 2.07 0.51 12.66 9.55 2.48\n\nTable 4: Performance of ensemble models of self-pretrained and off-the-shelf models. For ensembling, we aggregate predictions of models after calibration with Temperature Scaling (Guo et al., 2017). We observe that in most of the datasets, SelfPretrain + Off-the-shelf ensembling does not improve over ensembles of two models with the same pre-training strategy, despite relatively higher error inconsistency of SelfPretrain + Off-the-shelf models.\n\nwe compute error inconsistency, defined as follows:\n\n(cid:80)n\n\ni=1 (1 [fA(xi) ̸= yi ∧ fB(xi) = yi] + 1 [fA(xi) = yi ∧ fB(xi) ̸= yi)] n\n\n,\n\nwhere {xi, yi}n i=1 is the test set. Intuitively, error inconsistency captures the fraction of examples where exactly one model is correct. This definition has been commonly used to estimate diversity in model prediction (Gontijo-Lopes et al., 2022; Geirhos et al., 2020). Across all the multi-class classification tasks, in addition to computing error inconsistency between self-pretrained and offthe-shelf model, for baseline comparison, we also tabulate error inconsistency between: (i) two independently finetuned versions of a self-pretrained model; and (ii) two independently finetuned versions of the off-the-shelf model.\n\nCompared to error inconsistency between two models with the same pretraining dataset, we observe that models trained with different pretraining datasets have high error inconsistency in predictions (Table 4). Note that for models with comparative performance, high error inconsistency highlights the high disagreement in predictions. This demonstrates that while different pretraining datasets produce similarly performing models in terms of overall accuracy, the model predictions are relatively dissimilar. Our observations here align with investigations in vision tasks, where Gontijo-Lopes et al. (2022) observed that different models trained with different pretraining datasets produced uncorrelated errors.\n\nSince different pretraining datasets produce models with uncorrelated errors, we now seek to ensemble these models to check if uncorrelated mistakes can be converted to a correct prediction. When the models make different predictions, in particular, when one model is correct and another is incorrect, the ensemble prediction will be dominated by the model with higher confidence in their prediction. As before, we consider ensembles of (i) two independently finetuned versions of a self-pretrained model; (ii) two independently finetuned off-the-shelf models; and (iii) a finetuned version each of the self-pretrained and off-the-shelf models.\n\nWe make the following observations: First, as expected we observe that ensembling improves model performance as compared to a single model (Table 4). Second, despite having larger error inconsistency, we do not observe any significant improvements in ensembles of self-pretrained and off-the-shelf model as compared to ensembles of two models with the same pretraining strategy (Table 4). This is in contrast with findings on vision tasks where Gontijo-Lopes et al. (2022) observed that larger error inconsistency led to larger improvement in ensemble performance.\n\n7 ABLATIONS WITH OTHER PRETRAINING ARCHITECTURES\n\nWe conducted our experiments so far with ELECTRA-small architecture because it is faster to pretrain than other popular models, yet delivers good downstream performance (Clark et al., 2019)\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Variation in performance of ELECTRA models with change in number of layers and hidden size (— randomly initialized, — self-pretrained, — BookWiki-pretrained)\n\n(e.g. comparable to BERT-base on GLUE benchmark). But we also conduct experiments with more architectures and model sizes to test the efficacy of self-pretraining more broadly.\n\nWe experiment with the RoBERTa model which uses only the masked language modeling objective, rather than ELECTRA’s complex objective. We use the RoBERTa-base architecture, which has a much larger parameter count of 110 million, compared to ELECTRA-small’s 14 million. Due to resource constraints, we pretrained the RoBERTa models for fewer iterations as outlined in Warstadt et al. (2020). For reference, we pretrain a RoBERTa-base model on the BookWiki corpus for the same number of iterations. Our results show again that self-pretraining performs comparably to pretraining on BookWiki corpus, delivering over 85% of pretraining benefit on 9 out of 10 datasets, and outperforming the model pretrained on BookWiki corpus (Table 5) on 5 datasets.\n\nDataset\n\nRandInit\n\nSelfPretrain BookWiki Benefit % TAPT\n\nAGNews QQP Jigsaw Toxicity MNLI Sentiment140 PAWS DBPedia14 Discovery Yahoo Answertopics Amazon Polarity\n\n91.91 76.50 97.32 31.82 56.68 50.00 98.57 17.36 61.11 89.02\n\n94.28 88.68 97.72 75.12 68.55 97.34 99.21 25.85 65.96 96.68\n\n94.22 90.18 98.03 80.90 60.19 97.08 99.24 26.30 64.58 96.11\n\n102.27 89.05 56.02 88.23 338.26 100.55 95.98 94.91 139.80 108.13\n\n94.07 90.64 98.48 79.26 65.65 97.85 99.23 23.58 65.05 96.16\n\nTable 5: Performance of RoBERTa-base models pretrained with different techniques on downstream datasets.\n\nIn addition to experimenting with a base-sized architecture (110M parameters), we also experiment with architectures which are even smaller than ELECTRA-small. We train ELECTRA models of smaller size by either reducing the number of layers in the generator and discriminator, or reducing the hidden dimension of the discriminator3. As the models get smaller, self-pretraining continues to significantly outperform random initialization and often outperforms pretraining on BookWiki corpus (Figure 3). Interestingly, the relative performance of self-pretrained and BookWiki-pretrained models tends to stay the same across model size. For example, for QQP self-pretraining is always best and for MNLI BookWiki-pretraining is always best irrespective of number of layers or hidden size.\n\n3In ELECTRA, the generator’s hidden size is already much smaller than that of the discriminator by design.\n\nSo we do not reduce it further, in order to have a reasonably well-performing generator.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n8 PERFORMANCE ON STRUCTURED PREDICTION AND COMMONSENSE NLI\n\nWhile the bulk of our experiments were on a variety of classification tasks, we also experiment with some tasks beyond simple classification. We experiment with three types of tasks: (i) span based question answering, (ii) named entity recognition (NER), and (iii) grounded commonsense inference. For question answering we use the SQuAD dataset (Rajpurkar et al., 2016) (v1.1) and report the F1-score. For NER, we use the CONLL-2012 NER task which uses annotations from Ontonotes v5.0 (Weischedel et al., 2013) involving 18 kinds of named entities. To measure performance, we use the overall F1 score 4. We include SWAG (Zellers et al., 2018) and HellaSwag (Zellers et al., 2019) for multiple-choice sentence completion.\n\nFor Electra-small models, we see that for each of these datasets self-pretraining achieves more than 70% pretraining benefit, and for Roberta-base model the benefit is 40-80% (Table 6). Even for the SWAG and HellaSwag datasets, which are designed to use rely on commonsense inference of pretrained models, we see performance boosts by pretraining using only the task’s training set.\n\nDatasets\n\nSize(MB)\n\nELECTRA-small\n\nRoberta-base\n\nRI\n\nSP\n\nOS Benefit%\n\nRI\n\nSP\n\nBW Benefit%\n\nSQuAD SWAG HellaSwag CONLL-2012\n\n19 22 30 6.4\n\n15.82 27.55 29.27 54.49\n\n63.01 60.56 39.14 75.66\n\n75.96 73.76 42.91 82.65\n\n78.47 71.43 72.36 75.18\n\n14.93 27.95 24.53 63.65\n\n67.23 45.18 31.03 72.64\n\n81.89 70.37 34.28 86.25\n\n78.11 40.62 66.67 39.78\n\nTable 6: Performance of ELECTRA and Roberta models pretrained with different techniques. RI: random initialization, SP: self-pretraining, OS: off-the-shelf; BW: pretrained on BookWiki by us.\n\n9 CONCLUSION AND FUTURE WORK\n\nIn this work, we showed that pretraining models only on text from the downstream dataset performs comparably to pretraining on a huge upstream corpus for a wide variety of datasets. The errors made by such self-pretrained models on the downstream tasks are significantly different from the ones made by the off-the-shelf models pretrained on upstream corpora. Our results suggest that the importance of learning from surplus upstream data for improving downstream task performance may have been overestimated. Crucially, our experiments also do not show that upstream data does not help at all or knowledge transfer does not occur, but simply questions to what extent it is responsible for downstream gains. For example, the impressive zero-shot performance very large language models such as GPT-3 (Brown et al., 2020) clearly suggests knowledge transfer is involved. One direction of future work would be to investigate how the performance of self-pretraining compares of pretraining on upstream corpora as the model sizes go up by orders of magnitude.\n\nWe found that the quantity and quality of data required for pretraining to provide significant benefit (over a randomly initialized model trained only with a supervised loss) is quite low. Downstream datasets which are tiny in comparison to typical upstream corpora, still function as useful pretraining corpora for getting performance gains across a wide range of datasets. Additionally, many of these datasets (e.g. MNLI, QQP) do not contain high quality long-term dependencies and yet manage to perform well even on the GLUE benchmark for language understanding. A possible future work could be to characterize the nature of long-term dependencies in pretraining corpora and quantify their impact on downstream performance boosts.\n\nSince self-pretraining does not involve any upstream corpus, it prevents exposure of the model to potentially undesirable contents in the large upstream corpus, while still delivering large performance benefits. Research has demonstrated the negative influence of web-sourced pretraining corpora on models, such as generating toxic language (Gehman et al., 2020) or reflecting racial biases in predictions (Ahn & Oh, 2021). For use cases that require avoding such issues, self-pretraning can provide a viable alternative to standard pretraining. In future work, we hope to compare how self-pretrained models and off-the-shelf models perform on these negative measures such as toxicity and social biases.\n\n4We use the seqeval library for evaluation (https://github.com/chakki-works/seqeval)\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJaimeen Ahn and Alice Oh. Mitigating language-dependent ethnic bias in bert. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 533–549, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nTuhin Chakrabarty, Christopher Hidey, and Kathleen McKeown. Imho fine-tuning improves claim\n\ndetection. In Proceedings of NAACL-HLT, pp. 558–563, 2019.\n\nCheng-Han Chiang and Hung-yi Lee. Pre-training a language model without human language. arXiv\n\npreprint arXiv:2012.11995, 2020.\n\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\n\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv ́e Jegou, and Edouard arXiv preprint\n\nGrave. Are large-scale datasets necessary for self-supervised pre-training? arXiv:2112.10740, 2021.\n\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3356–3369, 2020.\n\nRobert Geirhos, Kristof Meding, and Felix A Wichmann. Beyond accuracy: quantifying trial-by-trial behaviour of cnns and humans by measuring error consistency. Advances in Neural Information Processing Systems, 33:13890–13902, 2020.\n\nAlec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision.\n\nCS224N project report, Stanford, 1(12):2009, 2009.\n\nRaphael Gontijo-Lopes, Yann Dauphin, and Ekin D Cubuk. No one representation to rule them all: Overlapping features of training methods. In International Conference on Learning Representations (ICLR), 2022.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\n\nnetworks. In International conference on machine learning, pp. 1321–1330. PMLR, 2017.\n\nSuchin Gururangan, Ana Marasovi ́c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342–8360, 2020.\n\nXiaochuang Han and Jacob Eisenstein. Unsupervised domain adaptation of contextualized embeddings for sequence labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4238–4248, 2019.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nKaggle.com. classify jigsaw-toxic-comment-classification-challenge, 2018.\n\nToxic online\n\nclassification\n\ncomments.\n\ncomment\n\ntoxic\n\nchallenge:\n\nand https://www.kaggle.com/c/\n\nIdentify\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKundan Krishna, Jeffrey P Bigham, and Zachary C Lipton. Does pretraining for summarization require knowledge transfer? In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 3178–3189, 2021.\n\nYoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The inductive bias of in-context learning: Rethinking pretraining example design. In International Conference on Learning Representations, 2021.\n\nYongjie Lina, Yi Chern Tana, and Robert Frankb. Open sesame: Getting inside bert’s linguistic\n\nknowledge. ACL 2019, pp. 241, 2019.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. 2019.\n\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. Zero-shot entity linking by reading entity descriptions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3449–3460, 2019.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\n\nence on Learning Representations, 2018.\n\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\n\ncomputation engines. arXiv preprint arXiv:2103.05247, 2021.\n\nChristopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the National Academy of Sciences, 117(48):30046–30054, 2020.\n\nIsabel Papadimitriou and Dan Jurafsky. Learning music helps you read: Using transfer to study linguistic structure in language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6829–6839, 2020.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https: //aclanthology.org/N18-1202.\n\nFabio Petroni, Tim Rockt ̈aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, 2016.\n\nMachel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help offline reinforcement\n\nlearning? arXiv preprint arXiv:2201.12122, 2022.\n\nDamien Sileo, Tim Van De Cruys, Camille Pradel, and Philippe Muller. Mining discourse markers for unsupervised sentence representation learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3477–3486, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/N19-1351.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. arXiv preprint arXiv:2104.06644, 2021.\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593–4601, 2019.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, 2018.\n\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu, and Samuel Bowman. Learning which In features matter: Roberta acquires a preference for linguistic generalizations (eventually). Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 217–235, 2020.\n\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23, 2013.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122, 2018.\n\nYuhuai Wu, Felix Li, and Percy Liang. Insights into pre-training via simpler synthetic tasks. arXiv\n\npreprint arXiv:2206.10139, 2022.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.\n\nXingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. Nlp from scratch without large-scale pretraining: A simple and efficient framework. In International Conference on Machine Learning, pp. 25438–25451. PMLR, 2022.\n\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset\n\nfor grounded commonsense inference. In EMNLP, 2018.\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791–4800, 2019.\n\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text\n\nclassification. In NIPS, 2015.\n\nYuan Zhang, Jason Baldridge, and Luheng He. Paws: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1298–1308, 2019.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19–27, 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 THE ROLE OF SENTENCE ORDER IN PRETRAINING CORPORA\n\nFor virtually all pretrained models like BERT, ELECTRA, XLNet, the sentences in the pretraining corpora are ordered as they naturally occur in some document such as Wikipedia article. Devlin et al. (2019) mention in their work : “It is critical to use a document-level corpus rather than a shuffled sentence-level corpus (...) in order to extract long contiguous sequences.” However, for many of our pretraining corpora made from downstream datasets, the sentence taken in order do not form a coherent document or narrative text. For example, in the MNLI or QQP corpora, neighboring sentences will simply be premise-hypothesis pairs or potential paraphrase candidates.\n\nDespite the sentence order not forming a coherent document, many pretraining corpora achieve high performance boosts on the GLUE language understanding benchmark (Table 7). For example, MNLI achieves around 96% of the performance boost of the off-the-shelf model (Table 7). Interestingly, shuffling the sentences in these corpora leads to a large drop in performance (Table 7). This suggests that there is some value to keeping the sentence order in a way that puts sentences from the same example in datasets like MNLI and QQP next to each other. A likely explanation of this is in Levine et al. (2021) where authors showed that including similar sentences in the same input sequence when pretraining should lead to improved performance via theoretical analysis and empirical experiments.\n\nWe test if GLUE performance can be improved by artificially re-ordering a set of sentences to promote the occurrence of similar sentences together. We rearrange the sentences in the sentence-shuffled versions of pretraining corpora to encourage content overlap among neighboring sentences, and see if this can recover some of the drops in performance that occurred due to shuffling. Our algorithm creates the corpus by iteratively appending sentences to it, such that at each step the new sentence is the one with maximum TF-IDF similarity with the previous sentence. Such a way of constructing a corpus by similarity based retrieval has been used in past works (Levine et al., 2021; Yao et al., 2022), with the main difference that they retrieved sentences from external corpora similar to the ones present in the downstream dataset, whereas we simply use it to reorder sentences already present in the downstream dataset for pretraining We also make sure that the algorithm does not accidentally recover the original order of sentences (e.g. by matching the premise-hypothesis pairs originally in the MNLI dataset).\n\nWe experiment with 5 different datasets and find that the sentence-reordering scheme improves performance compared to random sentence order for all of them except QQP. For Discovery and DBPedia14 datasets, it scores even higher than our standard sentence ordering scheme which preserves the adjacency and order of sentences within each datapoint. This shows that re-ordering sentences to promote content similarity between neighboring sentences, can potentially improve GLUE score, without introducing any new information or narrative structure.\n\nA.2\n\nIMPLEMENTATION DETAILS FOR PRETRAINING AND FINETUNING\n\nHyperparameters for pretraining For pretraining ELECTRA-small models, we use the standard hyperparameters (Table 8) as described in Clark et al. (2019). For the Roberta-base models, training\n\nPretraining Dataset\n\nRandom Standard TF-IDF(Ours)\n\nNone (RandomInit)\n\nSentiment140 DBpedia14 Discovery MNLI QQP\n\n-\n\n- 72.82 71.79 62.80 71.09\n\nBookWiki (Off-the-shelf)\n\n-\n\n53.20\n\n72.67 70.38 77.26 78.28 75.43\n\n79.43\n\n-\n\n75.29 75.44 78.94 76.33 69.57\n\n-\n\nTable 7: GLUE scores achieved by different strategies for ordering sentences from the downstream dataset used for pretraining. Random: randomly ordered sentences; Standard: sentences within a datapoint occur contiguously in original order; TF-IDF: sentences reordered using content similarity.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nwith the standard hyperparameters with our computing resources would be prohibitively slow, and so we used hyperparameters from Warstadt et al. (2020) which require lesser time to train (Table 8). For task-adaptive pretraining(TAPT), we follow Gururangan et al. (2020) and further pretrain off-the-shelf models for 100 epochs on the downstream task’s training set, with the first 6% of the resulting total updates used for learning rate warmup.\n\nHyperparameters for finetuning For finetuning the models on the 10 downstream datasets, we use hyperparameters as shown in Table 9. We use the AdamW optimizer (Loshchilov & Hutter, 2018) for finetuning. We use early stopping based on validation set performance. The validation metric used is mean squared error for the sentiment140 dataset (regression), average binary crossentropy for the jigsaw dataset (multi-label classification), and accuracy for all other datasets (multi-class classification). The patience parameter for early stopping is set to 3 epochs. For finetuning ELECTRAsmall models on the GLUE datasets, we use the standard learning rate of 1e-4 following Clark et al. (2019).\n\nDetails about use of downstream datasets All downstream datasets used in this paper were sourced from the Huggingface library5. For the Yahoo Answertopics dataset, we use only the text from the answer (not the question) as input to the models (both for pretraining and finetuning). For the PAWS dataset, we use the version called “Unlabeled PAWSwiki” in Zhang et al. (2019), which is actually not unlabeled but has silver labels. We preferred that version over others because of its larger size. For datasets which had a train and test split but no validation split (e.g. Yahoo Answertopics), we extracted 5000 random datapoints from the the train split to make the validation split. If a dataset had a train and validation split but no test split (e.g. Unlabeled PAWSwiki), we designated the validation split to be the test split, and created a new validation set by extracting 5000 random datapoints from the train set.\n\nHyperparameter\n\nELECTRA\n\nRoberta\n\nSize (Parameter count) Training steps Warmup steps Batch size Peak learning rate Sequence length\n\nSmall (14M) Base (110M)\n\n1M 10K 128 5e-4 128\n\n100K 6K 512 5e-4 512\n\nTable 8: Hyperparameters used for pretraining models\n\nHyperparameter\n\nELECTRA Roberta\n\nTraining epochs Batch size Learning rate Max sequence length\n\n20 32 {1e-4,1e-5} 512\n\n20 32 2e-5 512\n\nTable 9: Hyperparameters used for finetuning models on 10 downstream tasks\n\nA.3 SOFTWARE PACKAGES AND HARDWARE USED\n\nFor pretraining ELECTRA models, we used Nvidia’s implementation of the ELECTRA codebase 6, run using Nvidia’s Tensorflow cotainer image 21.07 7. For pretraining Roberta models, we used the official implementation in the Fairseq library8. For finetuning experiments, we used the AllenNLP library for training and evaluation routines, coupled with the Huggingface library for the model architectures.\n\n5https://huggingface.co/docs/datasets/index 6https://github.com/NVIDIA/DeepLearningExamples/tree/master/\n\nTensorFlow2/LanguageModeling/ELECTRA\n\n7https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/\n\nrel_21-07.html\n\n8https://github.com/facebookresearch/fairseq\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nWe used a collection of Nvidia V100 (32GB) and A6000(48GB) GPUs for our experiments. Pretraining an ELECTRA-small model takes around 1.5 days on 2 GPUs while pretraining a Roberta-base model takes around 1.5 days on 4 GPUs.\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper studies the effectiveness of self-pretraining, meaning applying pretraining *objectives* like masked language modeling to *datasets* specific for a single task. Remarkably, self-pretraining often achieves performance comparable to the conventional approach of pre-training on generalist web-scale text. This casts doubt on the proposition that knowledge transfer from large text corpora is at the core of why pre-training is so effective in NLP.\n\nContributions:\n- The paper compares self-pretraining to using an off-the-shelf model like ELECTRA, and finds that the two often achieve comparable results.\n- Pre-training on other task-targeted datasets (rather than generalist web corpora) is also shown to provide significant gains\n- Self-pretrained models are observed to have a different distribution of errors compared to off-the-shelf pretrained models, but simple ensembling is not able to take advantage of this to get better results\n\n# Strength And Weaknesses\n\nStrengths:\n- Experiments include a variety of text domains and dataset sizes, and the core findings of the paper hold across all data conditions\n- Both off-the-shelf and randomly-initialized models are included as points of comparison in the experiments, providing appropriate context for interpreting the effectiveness of self-pretraining\n- Understanding the reasons underlying the effectiveness of pre-training can have far-reaching implications given the current state of the field. Providing concrete evidence that casts doubt on the popular claims of \"knowledge transfer\" contributes to a experimentally-grounded understanding of the topic.\n\nWeaknesses:\n- The ensembling results appear to have ensembling take place over probabilities, rather than discrete predictions -- at least that appears to be the case given that only two models are being ensembled. Is there any way to apply majority-vote style ensembling instead, to get around the issues with uncalibrated model confidence levels?\n- The discussion on how to construct artificial long-term dependencies is very interesting, but it also is a little bit orthogonal to the remainder of the paper; at least with the current framing. The paper would be stronger if it could provide a more cohesive narrative in terms of incoporating this section.\n- The use of BERT-base scale models naturally leaves open the question of whether anything changes at larger scale. While the fine-tuning paradigm becomes effective at these scales, recent research suggests that zero-shot capabilities tend to emerge at larger model sizes. Given that zero-shot capabilities are naturally connected to investigating the role of knowledge transfer in pre-training, there is a natural unresolved question of whether self-pretraining would fall behind if the experimental setup is upscaled by at least an order of magnitude. This is, of course, understandably hard to assess without enormous compute expenditures.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is easy to follow and the writing is clear. A comprehensive investigation of self-pretraining is (to my knowledge) novel in the NLP literature, and the related work is addressed in the paper.\n\n# Summary Of The Review\n\nOverall the paper contains a thorough and comprehensive experimental evaluation of self-pretraining, and the result that self-pretraining can be as effective as off-the-shelf models can have important implications for our understanding of why the pre-training paradigm is as effective as it is.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nNEURAL IMPLICIT SHAPE EDITING USING BOUNDARY SENSITIVITY\n\nArturs Berzins Department of Mathematics and Cybernetics SINTEF arturs.berzins@sintef.no\n\nMoritz Ibing & Leif Kobbelt Visual Computing Institute RWTH Aachen University\n\nABSTRACT\n\nNeural fields are receiving increased attention as a geometric representation due to their ability to compactly store detailed and smooth shapes and easily undergo topological changes. Compared to classic geometry representations, however, neural representations do not allow the user to exert intuitive control over the shape. Motivated by this, we leverage boundary sensitivity to express how perturbations in parameters move the shape boundary. This allows to interpret the effect of each learnable parameter and study achievable deformations. With this, we perform geometric editing: finding a parameter update that best approximates a globally prescribed deformation. Prescribing the deformation only locally allows the rest of the shape to change according to some prior, such as semantics or deformation rigidity. Our method is agnostic to the model its training and updates the NN in-place. Furthermore, we show how boundary sensitivity helps to optimize and constrain objectives (such as surface area and volume), which are difficult to compute without first converting to another representation, such as a mesh.\n\n1\n\nINTRODUCTION\n\nA neural field is a neural network (NN) mapping every point in a domain of interest, typically of 2 or 3 dimensions, to one or more outputs, such as a signed distance function (SDF), occupancy probability, opacity or color. This allows to represent smooth, detailed, and watertight shapes with topological flexibility, while being compact to store compared to classic implicit representations (Davies et al., 2020). When the NN is trained not on a single shape but instead an entire collection, each shape is encoded in a latent vector, which is an additional input to the NN (Park et al., 2019; Chen & Zhang, 2019; Mescheder et al., 2019). As a result, neural fields are receiving increased interest as a geometric representation in numerous applications, such as shape generation (Park et al., 2019), shape completion (Chibane et al., 2020), shape optimization (Remelli et al., 2020), scene representation (Sitzmann et al., 2020), and view synthesis (Mildenhall et al., 2020). Some pioneering works have also investigated geometry processing, like smoothing and deformation, on neural implicit shapes (Yang et al., 2021; Remelli et al., 2020; Mehta et al., 2022; Guillard et al., 2021), but these can be computationally costly or resort to intermediate mesh representations. In part, this difficulty stems from the shape being available only implicitly as the sub-level set of the field.\n\nWhile intuitive (often synonymous with local) geometric control is a key design principle of classic explicit or parametric representations (like meshes, splines, or subdivision schemes), it is not trivial to edit even classic implicit representations, especially ones with global functions (Bærentzen & Christensen, 2002). Previous works on neural implicit shape editing have focused on the shape semantics, i.e. changing part-level features based on the whole shape structure, but achieve this through tailored training procedures or architectures or resort to intermediate mesh representations.\n\nWe instead propose a framework which unifies geometric and semantic editing and which is agnostic to the model and its training and modifies the given model in-place akin to classic representations. To treat the geometry, not the field, as the primary object we consider boundary sensitivity to relate changes in the function parameters and the implicit shape. This allows us to express and interpret a basis for the displacement space.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nIn this framework, the user supplies a target displacement on (a part) of the shape boundary in the form of deformation vectors or normal displacements at a set of surface points. Employing boundary sensitivity we find the parameter update which best approximates the prescribed deformation. In geometric editing, we prescribe an exact geometric update on the entirety of the boundary. Akin to local control in classic representations, we especially study the case where the prescribed displacement is local and the rest of the boundary is fixed. In semantic editing only a part of the boundary is prescribed a target displacement. The remaining unconstrained displacement is determined by leveraging the generalization capability of the model as an additional prior, producing semantically consistent results on the totality of the shape. Another prior often used in shape editing is based on deformation energy, such as as-rigid-as-possible (Sorkine & Alexa, 2007) or as-Killing-as-possible (Solomon et al., 2011), which generates physically plausible deformations minimizing stretch and bending. This is not trivially applicable to implicit surfaces, as there is no natural notion of stretch due to ambiguity in tangent directions. We discuss a few options to resolve this ambiguity and demonstrate that boundary sensitivity can be leveraged to optimize directly in the space of expressible deformations. Lastly, we use level-set theory and boundary sensitivity to constrain a class of functionals, which are difficult to compute without first converting to another representation, such as a mesh. As a specific use-case, we consider fixing the volume of a shape to prevent shrinkage during smoothing.\n\n2 RELATED WORK\n\nImplicit Shape Representations and Manipulation Implicit shape representations or level-sets have been widely used in fields such as computer simulation (Sethian & Smereka, 2003), shape optimization (van Dijk et al., 2013), imaging, vision, and graphics (Vese, 2003; Tsai & Osher, 2003). Classically, an implicit function is represented as a linear combination of either a few global basis functions, such as in blobby models (Muraki, 1991), or many local basis functions supported on a (potentially adaptive) volumetric grid (Whitaker, 2002). While few global bases use less memory, the task of expressing local displacements is generally ill-posed (Whitaker, 2002). Hence, methods for interactive editing of implicit shapes are formulated for grid-supported level-sets (Museth et al., 2002; Bærentzen & Christensen, 2002). These methods use a prescribed velocity field, for which the level-set-equation – a partial differential equation (PDE) modelling the evolution the surface – is solved using numerical schemes on the discrete spatiotemporal grid.\n\nIn neural fields, a NN is used to represent an implicit function. Different from classic Neural Fields implicit representations, these are non-linear and circumvent the memory-expressivity dilemma Davies et al. (2020). In addition, automatic differentiation also provides easy access to differential surface properties, such as normals and curvatures, useful for smoothing and deformation (Yang et al., 2021; Mehta et al., 2022; Atzmon et al., 2021) or more exact shape fitting (Novello et al., 2022). Early works propose to use vanilla multilayer perceptrons (MLPs) to learn occupancy probability (Mescheder et al., 2019; Chen & Zhang, 2019) or signed distance functions (Park et al., 2019; Atzmon & Lipman, 2020), whose level-sets define the shape boundary. Conditioning the NN on a latent code as an additional input allows to decode a collection of shapes with a single NN (Chen & Zhang, 2019; Mescheder et al., 2019; Park et al., 2019). Later works build upon these constructions by introducing a spatial structure on the latent codes using (potentially adaptive) grids, which affords more spatial control when generating novel shapes (Ibing et al., 2021) and allows to reconstruct more complex shapes and scenes (Jiang et al., 2020; Peng et al., 2020; Chibane et al., 2020). In this work, we develop a method to interactively modify shapes generated by any of these methods.\n\nNeural Shape Manipulation Although there are many previous works on the deformation of shapes with NNs, we focus only on methods that use neural implicit representations. These can roughly be sorted into two groups based on their guiding principle. Methods in the first group manipulate shapes based on a semantic principle. Hertz et al. (2022) create a generative framework with part-level control consisting of three NNs decomposing and mixing shapes in the latent space. Elsner et al. (2021) encourage the latent code to act as geometric control points, allowing to manipulate the geometry by moving the control points. Chen et al. (2021) demonstrate how to interpolate between shapes while balancing their semantics by choosing which layer’s features to track. Similar to our method, this is agnostic to the training and the model. Hao et al. (2020) learn a joint latent space between an SDFs and its coarse approximation in the form of a union of spherical primitives. This way modifications of the spheres can be translated to the best matching change of the high-fidelity shape.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nIn our work, we allow a similar editing approach, but through insights from boundary sensitivity, we are able to apply such manipulation to arbitrary NNs.\n\nMethods in the second group are based on some well defined energy. Yang et al. (2021) demonstrate shape smoothing using a curvature-based loss. Similarly, shape deformation is performed by optimizing the deformation energy for which another invertible correspondence NN is needed. Niemeyer et al. (2019) train an additional NN as a spatio-temporally continuous velocity field, along which the points of a shape are integrated.\n\nBoundary Sensitivity Boundary sensitivity has already been introduced in the context of implicit neural shapes in several previous works. Atzmon et al. (2019) use it to translate geometric losses defined on points sampled from a classifier or SDF level-sets to parameter updates during training. Atzmon et al. (2021) use approximate Killing vector fields (AKVFs) and boundary sensitivity during training to encourage latent space interpolation to act as physically plausible deformation. Neural implicit shape manipulation has also been achieved by leveraging meshes as an intermediate representation to benefit from the well studied geometry processing operations on those. Remelli et al. (2020) propose differentiable mesh extraction to then propagate gradients from a differentiable operation on the mesh through to the implicit NN. Mehta et al. (2022) study this link in more detail using classic theory of level-sets, using boundary sensitivity to translate several mesh-based operations to SDF updates. Guillard et al. (2021) builds upon differentiable mesh extraction for sketch-based editing by translating an image-based error to shape updates. Similar to our work, Sketch2Mesh uses boundary sensitivity only for the editing, being model and training agnostic. While our method trivially generalizes to meshes, we require only points sampled from the surface. We further show how to restrict the editing process by introducing explicit constraints or by using the NN’s semantic prior.\n\n3 BOUNDARY SENSITIVITY\n\nLet f be a differentiable function mapping a spatial coordinate x ∈ D ⊂ RD to a scalar f (x; Θ) ∈ R on a domain of interest D. Let the vector Θ ∈ RP gather the parameters of f . For a given Θ, the sign of f implicitly defines the shape Ω(Θ) = {x ∈ D|f (x; Θ) ≤ 0} and its boundary Γ(Θ) = {x ∈ D|f (x; Θ) = 0}. Consider the variation of Ω(Θ) by a displacement field (also known as velocity field) δx : D (cid:55)→ RD, denoted by Ωδx(Θ) = {x + δx(x) ∈ D|f (x + δx(x); Θ) ≤ 0}. If δx is sufficiently small, the initial and the perturbed shapes are diffeomorphic (Allaire et al., 2004), meaning there is a one-toone correspondence between the points of both. Consider the displacement field δx induced by a sufficiently small parameter perturbation δΘ and the perturbed shape Ωδx(Θ) = Ω(Θ + δΘ). To compute δx, consider the total derivative of the boundary condition f (x; Θ) = 0:\n\ndf (x; Θ) = ∇xf ⊤ dx + ∇Θf ⊤ dΘ = 0 ∀x ∈ Γ .\n\n(1)\n\nWe can replace the infinitesimal increments dx and dΘ with sufficiently small perturbations ∇xf ⊤δx + ∇Θf ⊤δΘ = 0. Assuming bounded gradients ∇xf = ∇xf (x; Θ) ∈ RD and ∇Θf = ∇Θf (x; Θ) ∈ RP allows to express δx = δx(x; Θ, δΘ) as\n\nδx =\n\n−∇xf ∇Θf ⊤δΘ ∥∇xf ∥2\n\n+ δxt .\n\n(2)\n\nThis is what we refer to as boundary sensitivity – the movement of the boundary δx caused by the parameter perturbations δΘ. Equation 2 consists of normal and tangent components δx = δxn + δxt = nδxn + tδxt. With n = ∇xf /∥∇xf ∥ the outward facing normal, we rewrite the normal part as the weighted sum of the basis b = b(x; Θ) ∈ RP\n\nδxn = b⊤δΘ , b =\n\n−∇Θf ∥∇xf ∥\n\n.\n\n(3)\n\nFor each positive parameter perturbation δΘp, p ∈ {1..P }, a positive gradient ∂f /∂Θp implies that the boundary moves inward along the normal, since the value of f at x increases. The total movement\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ncaused by all parameter perturbations is their superposition. We show some basis functions in Figures 2 and 3.\n\nThe tangential component δxt is ambiguous since per definition ∇xf ⊤δxt = 0. This is transport of the surface along itself which has no effect on the implicit representation. However, this ambiguity can be resolved to recover δxt based on an additional assumption. In the context of classic implicit representations two such assumptions considered are that normals of points do not change during the deformation (Jos & Schmidt, 2011) and that they are nearly-isometric (Tao et al., 2016).\n\nEditing Let δ ̄x : ̄Γ (cid:55)→ RD be a prescribed displacement on (a part of) the boundary ̄Γ ⊆ Γ. EC = (cid:82) ̄Γ ||δ ̄x − δx||2 ds is the energy associated with the deviation from the prescribed displacement. The task in editing is to find a parameter update δΘ inducing the displacement δx which minimizes this energy. Since a parameter update can cause movement only in the normal direction, it can only approximate the normal component of the target.\n\nδΘ = argmin\n\nδ ̃Θ\n\n= argmin\n\nδ ̃Θ\n\n(cid:90)\n\n ̄Γ\n\nEC = argmin\n\nδ ̃Θ\n\n(cid:90)\n\n ̄Γ\n\n∥δ ̄xn − δxn + δ ̄xt − δxt∥2 ds\n\n∥δ ̄xn − b⊤δ ̃Θ∥2 ds +\n\n(cid:90)\n\n ̄Γ\n\n∥δ ̄xt − δxt∥2 ds\n\n(4)\n\nIn practice, the displacement is provided as a finite set of surface vectors at the locations {xi}i=1..I ⊂ ̄Γ and the integral is estimated with the sum: (cid:88)\n\nδΘ = argmin\n\n∥δ ̄xn(xi) − b(xi)⊤δ ̃Θ∥2 .\n\n(5)\n\nδ ̃Θ\n\ni\n\nThis is a linear-least-squares problem BδΘ = δ ̄y with B = [b⊤(xi)]⊤ ∈ RI×P and δ ̄y = [δ ̄xn(xi)] ∈ RI .\n\nTo improve the numerical stability one often uses Tikhonov regularization which penalizes the norm of the solution minδΘ EC + λ∥δΘ∥2 for some small positive regularization constant λ. In our setting, Tikhonov regularization serves an additional purpose: small δΘ are necessary for the valididity of the linear expansion in Equation 1. Furthermore, especially in semantic editing, we might sample less points than parameters I < P , which would lead to an underdetermined system if regularization is not used. Lastly, regularization can also control how similar the final shape is to the source shape, as indicated in Figure 9.\n\nTarget Deformation We sample points xi on the boundary ̄Γ via iterative rejection sampling on the domain of interest or near the farthest point samples of the existing points. This is a sufficiently efficient method for our needs, although more advanced methods exist (Hart et al., 2002). Target deformations are then prescribed on the sampled points. If these are not given as the magnitude along the normal deformation δ ̄xn, but as a vector δ ̄x, we project them δ ̄xn = n⊤δ ̄x. With partially prescribed targets, one must be careful about the target being unintentionally restrictive if the normals at the sampled points are nearly orthogonal to the target vector. This can be remedied by additionally filtering the points based on their normals.\n\nLarge Displacements Despite the boundary sensitivity in Equation 1 being valid only for small displacements due to the assumption of locally constant gradients, large displacements can be achieved via several iterations. The initially sampled surface points are moved by the computed δx in order to obtain the samples of the next iteration. To obtain the respective target deformations we split the initially user-provided target (either scalar along normal or vector) into equal parts. If the target is a vector, for each iteration we project the divided target vector onto the current normal. In the demonstrated geometric and semantic editing applications a small number of iterations (<15) is sufficient. Note, that the number of iterations will in general scale with the magnitude of the desired deformation.\n\nConstraints Furthermore, we demonstrate the use of boundary sensitivity to fix a value of a functional during parameter updates. If the functional is expressed as a surface or volume integral,\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nthis can be done without computing the integrals themselves. An example of this is constant area, which can simulate the behaviour of unstretchable, but bendable materials, such as rope in 2D or textile in 3D. Similarly, volume preservation is characteristic to incompressible materials (Desbrun & Gascuel, 1995) and is desirable in smoothing to prevent shrinkage (Taubin, 1995).\n\nTo this end, we loosely introduce shape derivatives and refer to Allaire et al. (2021) for more detail. H ′(Ω)(δx) denotes a shape derivative of the functional H(Ω) ∈ R if the expansion H(Ωδx) = H(Ω) + H ′(Ω)(δx) holds for small δx. We rewrite this using perturbations as δH(Ω)(δx) = H(Ωδx) − H(Ω). For several functionals the shape derivatives are known analytically. For the functional H(Ω) = (cid:82)\n\nΩ h dx defined as a volume integral of h = h(x) ∈ R the shape derivative is\n\nδH(Ω)(δx) =\n\n(cid:90)\n\nΩ\n\ndiv (hδx) dx =\n\n(cid:90)\n\nΓ\n\nhδx⊤n dx\n\n(6)\n\nwhere the last equality is due to the divergence theorem on a bounded and Lipschitz domain Ω. Inserting the boundary sensitivity from Equation 2 we arrive at\n\nδH = δΘ⊤\n\n(cid:90)\n\nΓ\n\nhb dx = b⊤\n\nH δΘ\n\n(7)\n\nH )δΘ.\n\nwhere bH := (cid:82) Γ h(x)b(x) dx ∈ RP again acts as a basis, but for the perturbed integral quantity. δH = 0 can now be enforced either as soft constraint or as a hard constraint by projecting any parameter update δΘ onto the RP −1 linear subspace where the integral stays constant δΘδH=0 = (I − bH b⊤ Analogously, for a functional G(Ω) = (cid:82) Γ g dx defined as a surface integral of g = g(x) ∈ R the shape derivative as a perturbation is δG(Ω)(δx) = (cid:82) Γ (∂g/∂n + κg) δx⊤n dx, where ∂g/∂n is the directional derivative of g along n and κ = κ(x) = div(n(x)) ∈ R is the mean-curvature. The corresponding basis for δG = b⊤\n\nGδΘ is bG = (cid:82)\n\nΓ (∂g/∂n + κg) b dx.\n\n4 APPLICATIONS\n\nHaving established the framework for editing implicit shapes through boundary sensitivity, we consider its applications. We emphasize geometric and semantic editing since our framework unifies both while being agnostic to the architecture and training of the model. We also address deformationrigidity-based editing since it classically is a widely used approach to shape editing. Lastly, we demonstrate the use of constraints with an example of volume-preserving smoothing.\n\nIn all cases, we include Tikhonov regularization with λ = 0.1 (see Appendix B). However, the behavior of regularization depends on the number of points I and the magnitude of the prescribed displacement. In turn, λ influences the number of required iterations and the residual.\n\n4.1 GEOMETRIC EDITING\n\nIn geometric editing, the displacement is prescribed on the entirety of the boundary. In this section, we focus on studying the case where the prescribed displacement is local and the rest of the boundary is fixed, akin to local control in classic representations. Deformation in Section 4.3 and smoothing in Section 4.4 are examples of geometric editing with a global target displacement.\n\nFor each considered shape, we train a separate network to fit the value and surface normals of the SDF. All networks share the same architecture: 3 hidden layers of 32 neurons each with sin activations. In total, there are P = 2273 learnable parameters, all of which are manipulated during editing. We demonstrate several examples of geometric editing in Figure 1 where we displace parts of both man-made and organic neural implicit shapes. In addition, we quantify and plot the relative geometric error between the computed shape and prescribed target normalized by the largest target displacement (δxn − δ ̄xn)/ maxx∈Γ |δ ̄xn|. When the displacements are, loosely speaking, natural to the shape, the approximations recover the target well. However, not arbitrarily complex displacements can be approximated as in the example of inscribing letters. The characteristic length of the target is much smaller than that of the shape, especially in the relevant region. We hypothesize that the good memory-expressiveness trade-off of neural fields requires the NN to allocate geometric complexity\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Geometric editing. Top: non-zero target displacement is prescribed on the highlighted regions and the remaining boundary is fixed with δ ̄xn = 0. Bottom: the resulting shape and normalized error. Natural displacements are approximated well. A counter-example is given in the last column: the prescribed displacements (imprinting letters) are too complex and outside the limits of expressible deformations, resulting in a coarse dent in the general area.\n\nFigure 2: Basis functions of several neural implicit shapes trained on a single geometry. Each column depicts a weight from each layer increasing in depth left-to-right. Red/blue denote positive/negative deformation w.r.t. the outward normal. These bases do not strongly encode semantic or geometric meaning, different from latent basis functions in generative models in Figure 3.\n\nwhere it is needed during training. To illustrate this, some basis functions are shown in Figure 2. As can be seen, their characteristic length is similar to that of the shape features. As all possible edits are a combination of these basis functions, it is unlikely that modifications on a much smaller scale can be reconstructed faithfully.\n\n4.2 SEMANTIC EDITING\n\nIn representation learning the aim is to explain observations with a small number of latent variables (Bengio et al., 2013), which we will denote by l. Generative models f (x; l, Θ), such as (variational) auto-encoders or generative adversarial networks, attempt to map novel latent codes to novel outputs within the same distribution as the observations. When interpolating between two latent codes, the appearance of the generated output changes continuously (Shen et al., 2020a).\n\nFor a generative neural implicit shape model, the basis for the boundary movement is locally described by b = −∇lf /∥∇xf ∥ (Equation 3). Figure 3 illustrates a few of such basis functions for the generative decoder of the IM-Net model (Chen & Zhang, 2019). The decoder is trained as part of an auto-encoder reconstructing the entire ShapeNet dataset (Chang et al., 2015) from l ∈ R256\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Basis functions of IM-Net. They partially encode semantics, such as segmentation and symmetries. Each column represents the same basis function. For similar shapes, such as the two chairs, the bases are qualitatively similar. Red/blue denote positive/negative deformation w.r.t. the normal.\n\nlatent variables. We use a pretrained model available at https://github.com/czq142857/ IM-NET-pytorch, which is implemented as an MLP with 8 layers of 1024 neurons each and leaky-ReLU activations.\n\nThese basis functions can be observed to encode semantics, such as rotation and reflection symmetries, and segment semantically meaningful parts, such as windscreen on the car, rim of the lamp and different surfaces on chairs. For similar parts, such as the two chairs, the bases show similar behaviour.\n\nHowever, it is known that not all directions in the latent space are (equally) viable (Chen et al., 2022; Vyas et al., 2021), hence, a similar argument can be made for the bases. Furthermore, while each basis function might cause meaningful deformation, they are not necessarily disentangled, i.e. each basis function does not explain a single generative factor, unless latent disentanglement is used in training (Bengio et al., 2013; Shen et al., 2020b; Tschannen et al., 2018). This could further increase the interpretability of the basis.\n\nDespite this, with our method we can intuitively traverse the latent space by considering the link between geometric and latent variable changes. We present several examples in Figure 4, where the highlighted areas ̄Γ of an initial shape are prescribed a local, high-level displacement, such as shortening a leg of a chair or squeezing the sides of the boat. We sample roughly 100 points in these areas, prescribe the same target vector at each point, and leave the remaining boundary unconstrained. After projecting the target onto the current normal and finding the best fit parameter update according to Equation 5, we repeat this process for a few (< 15) iterations to achieve visually obvious changes. User input is provided only at the beginning. Note that we only consider the latent parameters for our basis and leave all network parameters unchanged. Despite only prescribing local geometric deformation, we observe global and semantically consistent changes. Not only are obvious symmetries preserved, but also the morphology of the shape can change significantly, such as with the boat.\n\nIn Appendix A, we repeat the same set of experiments with the DualSDF (Hao et al., 2020) architecture. Each model is trained on a single ShapeNet category. This gives better generative results and a more semantically pronounced basis.\n\nCompared to geometric editing, semantic editing has the computational advantage of sampling points on just a small part of the boundary ̄Γ. Furthermore, the number of latent parameters is much smaller than the number of NN parameters, leading to very small least-squares systems. Per iteration, the method only requires a single forward- and backward-pass through the model, altogether being fit for interactive use.\n\n4.3 RIGID EDITING\n\nAs a final approach to editing, we briefly address deformation-rigidity since it classically is a widely used approach to shape editing. On the one hand, deformation energy is one of the many alternative priors to semantic prior discussed previously. A simple approach is to consider pure bending energy\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Semantic editing. Top: non-zero target displacement is prescribed on the highlighted regions ̄Γ and the rest is unconstrained. Bottom: the result after < 15 iterations. A semantically plausible result is produced from a plausible locally prescribed displacement. A counter-example is given in the last column: prescribing each wing to move in opposite directions is not a semantically viable displacement and the method fails to approximate the displacements and produce meaningful results.\n\nsince it can be computed only from the implicit representation itself. On the other hand, typical applications, such as as-rigid-as-possible (Sorkine & Alexa, 2007) or as-Killing-as-possible (Solomon et al., 2011), also include the stretching energy. This is not trivially applicable to implicit surfaces, as there is no natural notion of stretch due to ambiguity in the tangent directions. As discussed in Section 3, there are several choices for recovering tangential displacements, but we consider perhaps the simplest one – using the tangential projection δxt(x; Ξ) = (cid:0)I − n(x)n(x)⊤(cid:1) ft(x; Ξ) of a separate sufficiently smooth NN ft(x; Ξ) : R3 (cid:55)→ R3 parameterized in Ξ.\n\nTo quantify the deformation energy, we leverage Killing vector fields (KVFs) which generate isometric deformations (Solomon et al., 2011). δx is a KVF if it has an anti-symmetric Jacobian J(δx) = J(δx)(x) ∈ R3×3 everywhere on the boundary: J(δx) + J(δx)⊤ = 0 ∀x ∈ Γ. A vector fields’ deviation from being Killing can be measured with its Killing energy EK(δx, Γ) = (cid:82)\n\nΓ∥J(δx) + J(δx)⊤∥2 dx.\n\nClassically, the Jacobian is computed using a discrete operator on a spatial discretization, while Atzmon et al. (2021) formulates AKVF on a neural implicit directly. We follow this approach and seek an AKVF respecting the prescribed boundary deformations δ ̄x(xi) weighted by some small α > 0: minδ ̃x EK + αEC.\n\nBoundary sensitivity allows to search for the energy minimizing deformation directly in the space of expressible deformations and to directly optimize the normal component over δΘ using Equation 2. The EC in Equation 4 is expressed trivially but can now also accommodate the tangential component. To express EK, we can leverage J being a linear operator J(δx) = J(δxn) + J(δxt). Here J(δxn) = J(b⊤δΘ) = J(b⊤)δΘ with J(b⊤) ∈ R3×3×P and J(δxt) = J(ft) straight-forward.\n\nFigure 5 illustrates the results of AKVF deformation. Comparing these results with a mesh-based method, after sufficient iterations we achieve qualitatively similar results and comparable energies: Emesh K = 11, EK = 6 for the cactus. However, due to the need to train an additional NN and perform second-order differentiation, our approach is about an order of magnitude slower than the mesh-based LSTSQ solver.\n\nK = 254, EK = 320 for the bunny and Emesh\n\n4.4 VOLUME PRESERVING SMOOTHING\n\nAs an example of fixing the value of a surface or volume integral, we fix the volume itself during smoothing. Fixing the volume of an implicit shape is difficult without resorting to an intermediate representation, such as a mesh (Remelli et al., 2020; Mehta et al., 2022), since there is no trivial way to differentiably compute the volume of an implicit shape. Smoothing on neural fields has been previously formulated using gradient descent on an objective penalizing the deviation from a specified mean-curvature (Yang et al., 2021) and as mean-curvature flow computed on an intermediate mesh (Mehta et al., 2022). We also use mean-curvature flow without resorting to the intermediate mesh, but\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Rigid editing. Target displacement is prescribed in the highlighted regions ̄Γ and the rest is unconstrained. In both cases, the bottom of the shapes is anchored.\n\nFigure 6: Select iterations of smoothing via mean-curvature flow without and with a volume constraint. The volume after 86 iterations decreases by 80% and 1.6%, respectively. The constraint is achieved by projecting the parameter update onto the locally linear isochronic subspace found using boundary sensitivity.\n\nthe proposed volume-constraint is agnostic to the choice of the smoothing method. We first compute the mean curvature by expanding κ(x) = div(n(x)). From there, we can reuse the geometric editing framework with the globally prescribed target displacement δxn(x) = −κ(x) ∀ x ∈ ̄Γ = Γ. Note, that this approach induces the correct geometric flow as discussed by Mehta et al. (2022). To fix the volume V while smoothing, we simply set h = 1 in the bases introduced in Equation 7. We enforce volume preservation by projecting the parameter update onto the isochronic subspace. Figure 6 compares smoothing with and without this constraint. After 86 iterations, the volume decreases by 1.6% with and 80% without the constraint. Ultimately, the shapes converge to a sphere and a singular point, respectively.\n\n5 CONCLUSIONS\n\nWith implicit neural shapes becoming a widespread representation, we have demonstrated a unifying approach to perform geometric and semantic editing without the need for tailored training or architectures, while being simple to implement, and, especially in the case of semantic editing, fast and fit for interactive use. While we touched upon optimizing directly in the deformation space with a rigidity-prior, using other priors for unconstrained and tangential deformations remains an interesting problem. We used signed-distance and occupancy fields, but the editing framework can also be extended to other neural fields, where NeRFs especially provide tantalizing options. We hope that formulating a basis for the deformation space allows future work to further study and build models with desirable properties, such as interpretability, tailored degrees-of-freedom, linear-independence, and compactness or use them for segmentation or symmetry detection.\n\nACKNOWLEDGMENTS\n\nThis was supported by the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreement number 860843.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nGrégoire Allaire, François Jouve, and Anca-Maria Toader. Structural optimization using sensitivity analysis and a level-set method. Journal of Computational Physics, 194(1):363–393, 2004. ISSN 0021-9991. doi: https://doi.org/10.1016/j.jcp.2003.09.032.\n\nGrégoire Allaire, Charles Dapogny, and François Jouve. Chapter 1 - shape and topology optimization. In Andrea Bonito and Ricardo H. Nochetto (eds.), Geometric Partial Differential Equations - Part II, volume 22 of Handbook of Numerical Analysis, pp. 1–132. Elsevier, 2021. doi: https: //doi.org/10.1016/bs.hna.2020.10.004.\n\nMatan Atzmon and Yaron Lipman. Sal: Sign agnostic learning of shapes from raw data. In IEEE/CVF\n\nConference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nMatan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, and Yaron Lipman. Controlling neural level sets. In Advances in Neural Information Processing Systems, pp. 2032–2041, 2019.\n\nMatan Atzmon, David Novotny, Andrea Vedaldi, and Yaron Lipman. Augmenting implicit neural shape representations with explicit deformation fields. arXiv preprint arXiv:2108.08931, 2021.\n\nJ. A. Bærentzen and N. J. Christensen. Volume sculpting using the level-set method. In International Conference on Shape Modelling and Applications (SMI) 2002, may 2002. URL http://www2. compute.dtu.dk/pubdb/pubs/704-full.html.\n\nYoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:1798–1828, 2013.\n\nAngel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University — Toyota Technological Institute at Chicago, 2015.\n\nNutan Chen, Patrick van der Smagt, and Botond Cseke. Local distance preserving auto-encoders\n\nusing continuous k-nearest neighbours graphs. ArXiv, abs/2206.05909, 2022.\n\nYunlu Chen, Basura Fernando, Hakan Bilen, Thomas Mensink, and Efstratios Gavves. Neural feature matching in implicit 3d representations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1582–1593. PMLR, 18–24 Jul 2021.\n\nZhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5939–5948, 2019.\n\nJulian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for 3d shape reconstruction and completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6970–6981, 2020.\n\nThomas Davies, Derek Nowrouzezahrai, and Alec Jacobson. On the effectiveness of weight-encoded\n\nneural implicit 3d shapes. arXiv preprint arXiv:2009.09808, 2020.\n\nMathieu Desbrun and Marie-Paule Gascuel. Animating soft substances with implicit surfaces. In Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’95, pp. 287–290, New York, NY, USA, 1995. Association for Computing Machinery. ISBN 0897917014. doi: 10.1145/218380.218456. URL https://doi.org/10.1145/ 218380.218456.\n\nTim Elsner, Moritz Ibing, Victor Czech, Julius Nehring-Wirxel, and Leif Kobbelt. Intuitive shape\n\nediting in latent space, 2021.\n\nBenoit Guillard, Edoardo Remelli, Pierre Yvernay, and Pascal Fua. Sketch2mesh: Reconstructing and editing 3d shapes from sketches. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13023–13032, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZekun Hao, Hadar Averbuch-Elor, Noah Snavely, and Serge Belongie. Dualsdf: Semantic shape manipulation using a two-level representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\n\nJohn C. Hart, Ed Bachta, Wojciech Jarosz, and Terry Fleury. Using particles to sample and control more complex implicit surfaces. In SMI ’02: Proceedings of the Shape Modeling International 2002 (SMI’02), pp. 129, Washington, DC, USA, August 2002. IEEE Computer Society. doi: 10/dfw2ss.\n\nAmir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, and Daniel Cohen-Or. Spaghetti: Editing\n\nimplicit shapes through part aware generation. arXiv preprint arXiv:2201.13168, 2022.\n\nMoritz Ibing, Isaak Lim, and Leif Kobbelt. 3d shape generation with grid-based implicit functions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13559–13568, 2021.\n\nChiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, and Thomas Funkhouser. Local implicit grid representations for 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\n\nStam Jos and Ryan Schmidt. On the velocity of an implicit surface. ACM Transactions on Graphics,\n\n30:1–7, 2011. ISSN 15577368. doi: 10.1145/1966394.1966400.\n\nIshit Mehta, Manmohan Chandraker, and Ravi Ramamoorthi. A level set theory for neural implicit\n\nevolution under explicit flows. arXiv preprint arXiv:2204.07159, 2022.\n\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. In Proceedings of the\n\nOccupancy networks: Learning 3d reconstruction in function space. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4460–4470, 2019.\n\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n\nShigeru Muraki. Volumetric shape description of range data using “blobby model”. In Proceedings of the 18th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’91, pp. 227–235, New York, NY, USA, 1991. Association for Computing Machinery. ISBN 0897914368. doi: 10.1145/122718.122743.\n\nKen Museth, David E. Breen, Ross T. Whitaker, and Alan H. Barr. Level set surface editing operators. ACM Trans. Graph., 21(3):330–338, jul 2002. ISSN 0730-0301. doi: 10.1145/566654.566585.\n\nMichael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In International Conference on Computer Vision, October 2019.\n\nTiago Novello, Guilherme Schardong, Luiz Schirmer, Vinicius da Silva, Helio Lopes, and Luiz Velho. Exploring differential geometry in neural implicits, 2022. URL https://arxiv.org/abs/ 2201.09263.\n\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 165–174, 2019.\n\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In European Conference on Computer Vision, pp. 523–540. Springer, 2020.\n\nEdoardo Remelli, Artem Lukoianov, Stephan Richter, Benoit Guillard, Timur Bagautdinov, In Pierre Baque, and Pascal Fua. Meshsdf: Differentiable iso-surface extraction. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 22468–22478. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ fe40fb944ee700392ed51bfe84dd4e3d-Paper.pdf.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJ. A. Sethian and Peter Smereka. Level set methods for fluid interfaces. Annual Review of Fluid\n\nMechanics, 35(1):341–372, 2003. doi: 10.1146/annurev.fluid.35.101101.161105.\n\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9240–9249, 2020a.\n\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9243–9252, 2020b.\n\nVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Proc. NeurIPS, 2020.\n\nJustin Solomon, Mirela Ben-Chen, Adrian Butscher, and Leonidas Guibas. As-killing-as-possible vector fields for planar deformation. Computer Graphics Forum, 30(5):1543–1552, 2011. doi: https://doi.org/10.1111/j.1467-8659.2011.02028.x.\n\nOlga Sorkine and Marc Alexa. As-Rigid-As-Possible Surface Modeling. In Alexander Belyaev and Michael Garland (eds.), Geometry Processing. The Eurographics Association, 2007. ISBN 978-3-905673-46-3. doi: 10.2312/SGP/SGP07/109-116.\n\nMichael Tao, Justin Solomon, and Adrian Butscher. Near-isometric level set tracking. In Proceedings of the Symposium on Geometry Processing, SGP ’16, pp. 65–77. Eurographics Association, 2016.\n\nG. Taubin. Curve and surface smoothing without shrinkage. In Proceedings of IEEE International\n\nConference on Computer Vision, pp. 852–857, 1995. doi: 10.1109/ICCV.1995.466848.\n\nRichard Tsai and Stanley Osher. Level set methods and their applications in image science. Commu-\n\nnications in mathematical sciences, 1, 12 2003. doi: 10.4310/CMS.2003.v1.n4.a1.\n\nMichael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based\n\nrepresentation learning. arXiv preprint arXiv:1812.05069, 2018.\n\nNico van Dijk, Kurt Maute, Matthijs Langelaar, and Fred Keulen. Level-set methods for structural topology optimization: A review. Structural and Multidisciplinary Optimization, 48, 09 2013. doi: 10.1007/s00158-013-0912-y.\n\nLuminita Vese. Multiphase Object Detection and Image Segmentation, pp. 175–194. Springer New York, New York, NY, 2003. ISBN 978-0-387-21810-6. doi: 10.1007/0-387-21810-6_10. URL https://doi.org/10.1007/0-387-21810-6_10.\n\nShantanu Vyas, Ting-Ju Chen, Ronak R. Mohanty, Peng Jiang, and Vinayak R. Krishnamurthy. Latent embedded graphs for image and shape interpolation. Computer-Aided Design, 140:103091, 2021. ISSN 0010-4485. doi: https://doi.org/10.1016/j.cad.2021.103091. URL https://www. sciencedirect.com/science/article/pii/S0010448521001020.\n\nRoss T Whitaker. Isosurfaces and level-set surface models. School of Computing, University of Utah,\n\n2002.\n\nGuandao Yang, Serge Belongie, Bharath Hariharan, and Vladlen Koltun. Geometry processing with\n\nneural fields. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA SEMANTIC EDITING WITH DUALSDF\n\nWe repeat the experiments from Section 4.2 with a different architecture, namely, DualSDF (Hao et al., 2020). DualSDF learns a joint latent space for a fine-scale shape and its coarse approximation as a union of spheres. When a sphere is manipulated, an optimization process is run to find a latent code that explains the new sphere configuration, i.e. coarse shape. From the updated latent code the fine-scale shape can be generated.\n\nDifferent to IM-Net, DualSDF is trained on individual ShapeNet categories. We use pretrained models on planes and chairs available at https://github.com/zekunhao1995/DualSDF. In addition, we train another model on cars following the provided training procedure on all 3515 car shapes available after preprocessing the ShapeNet category.\n\nFigure 7 shows a few select basis functions of the three different DualSDF models. Figure 8 show the semantic editing results with our method, compared with the method described in DualSDF. For both methods, the shapes are generated with the same generative network. The difference lies only in how the edit is prescribed. In our approach we prescribe the movement directly on the sampled surface, while in DualSDF we move the spherical primitives, selecting them according to a similar design intent.\n\nBoth DualSDF and our method achieve plausible, though different, results. Both methods fail to adhere to the semantically implausible updates prescribed in the last column. However, the main benefit of our approach is that we are able to apply such manipulation to arbitrary NNs, whereas DualSDF needs a second NN for the coarse approximation and task-specific training.\n\nFigure 7: Select basis functions of three different DualSDF models. The semantics in most basis functions are not as prominent as the ones shown here. The two sets of chairs show the same respective basis functions, which can be seen to have the same semantic quality.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Semantic editing. The first two rows are similar to Figure 4, but use the generative network of the DualSDF architecture. The third and fourth rows show the prescribed deformation and the result using the editing procedure as described by Hao et al. (2020).\n\nB EFFECT OF TIKHONOV REGULARIZATION ON SEMANTIC EDITING\n\nFigure 9: Effect of Tikhonov regularization on semantic editing. Left: source shape is prescribed an inward displacement on the highlighted regions while the rest is unconstrained. The three figures on the right have decreasing amount of Tikhonov regularization λ = 101, 10−1, 10−3. Stronger regularization better preserves similarity to the source shape, which is especially noticeable on the unconstrained front of the boat remaining wider. Stronger regularization also requires much more iterations to converge to a similar result (860 for λ = 101 compared to < 10 for the other two) since the the constraint violation EC is weighted less heavily than regularization.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nC SPLITTING LARGE DEFORMATIONS\n\nFigure 10: Effect of splitting a large target deformation in geometric editing. Displayed are the results after splitting the target into 1,2,4,8,16 equal parts. As the figure illustrates, this helps accurately recover large deformations which violate the first-order approximation. For comparison, the black silhouette in the background shows the target.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper introduces a method for interactively editing 3D shapes defined as the level sets of neural implicit functions. Specifically, the approach quantifies the distribution of changes to this level set (the shape boundary) w.r.t. the parameters governing the neural implicits (either the network weights for a model regressed to fit a single shape, or the latent variables of a generative model). This is referred to as \"boundary sensitivity\". Based on this, the authors demonstrate several example of shape editing guided by high-level sparse user input, as well as introducing deformation constraints (rigidity, volume preservation...) into the framework. While this sort of inverse control has been demonstrated for other representations, this is, to the best of my knowledge, a novel contribution for neural implicits.\n\n# Strength And Weaknesses\n\nThe paper is well-written and develops an interesting method for computing the boundary sensitivity of an implicit field. Various applications and examples are presented to highlight the utility of the approach. In general, I am positive about this paper.\n\nOne weakness is that the paper does not really go into how and where the method fails. E.g. for what magnitude of displacements does the linearity assumption lead to incorrect results? If the network is very complex (very high-dimensional), are the geometric edits induced by perturbing network weights still smooth enough to look plausible? How does this latter aspect play with the explicit constraints induced in Sections 4.3 and 4.4?\n\nAnother weakness is that it does not sufficiently compare to baselines, as pointed out in other reviews.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is fairly clear. The work appears to be high-quality and original.\n\nI was curious if large user-specified local edits can be handled by dividing them into small incremental changes and then time-stepping the method, i.e. to move the bunny's ear by 10 units, break it into 10 steps of 1 unit each and recursively apply the method to obtain the final integrated deformation of the bunny.\n\n# Summary Of The Review\n\nI think the paper develops a nice mathematically justified approach for tying implicit field boundary changes to parameter updates, using this to solve various inverse control tasks. I think it can be better positioned w.r.t. evaluations and prior work.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nOUTPUT DISTRIBUTION OVER THE ENTIRE INPUT SPACE: A NOVEL PERSPECTIVE TO UNDERSTAND NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nUnderstanding the input-output mapping relationship in the entire input space contributes a novel perspective to a comprehensive understanding of deep neural networks. In this paper, we focus on binary neural classifiers and propose to first uncover the histogram about the number of inputs that are mapped to certain output values and then scrutinize the representative inputs from a certain output range of interest, such as the positive-logit region that corresponds to one of the classes. A straightforward solution is uniform sampling (or exhaustive enumeration) in the entire input space but when the inputs are high dimensional, it can take almost forever to converge. We connect the output histogram to the density of states in physics by making an analogy between the energy of a system and the neural network output. Inspired by the Wang-Landau algorithm designed for sampling the density of states, we propose an efficient sampler that is driven to explore the under-explored output values through a gradient-based proposal. Compared with the random proposal in Wang-Landau algorithm, our gradientbased proposal converges faster as it can propose the inputs corresponding to the under-explored output values. Extensive experiments have verified the accuracy of the histogram generated by our sampler and also demonstrated interesting findings. For example, the models map many human unrecognizable images to very negative logit values. These properties of a neural model are revealed for the first time through our sampled statistics. We believe that our approach opens a new gate for neural model evaluation and shall be further explored in future works.\n\n1\n\nINTRODUCTION\n\nUnderstanding the input-output mapping relationship in the entire input space contributes a novel perspective to a comprehensive understanding of deep neural networks. Existing methods approximate such mapping relations through the evaluation on a certain subset of the entire input space, such as measuring the accuracy on in-distribution test sets Dosovitskiy et al. (2021); Tolstikhin et al. (2021); Steiner et al. (2021); Chen et al. (2021); Zhuang et al. (2022); He et al. (2015), out-ofdistribution (OOD) test sets (Liu et al., 2020; Hendrycks & Gimpel, 2016; Hendrycks et al., 2019; Hsu et al., 2020; Lee et al., 2017; 2018), and adversarial test sets Szegedy et al. (2013); Rozsa et al. (2016); Miyato et al. (2018); Kurakin et al. (2016). However, none of the existing evaluations can offer a comprehensive understanding that covers the entire input space, including all kinds of inputs mentioned above and even those human unrecognizable inputs as shown in Fig 1a.\n\nAs a pilot study, we focus on binary classification — given a trained binary classifier, we aim to uncover a histogram that counts how many samples in the entire input space are mapped to certain logit values, i.e., the distribution of the output values, as shown in Fig 1b. A straightforward solution is uniform sampling (or exhaustive enumeration) in the entire input space but when the inputs are high dimensional, it can take almost forever to converge. Therefore, it calls for a novel efficient sampling method over a neural model’s output space. Note that, as a side product of the sampling procedure, one can expect that this histogram also offers fine-grained information such as some representative input samples corresponding to a certain range of output values.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Different types of input samples\n\n(b) The histogram of an example binary classifier\n\nFigure 1: Input types and the example output histogram when the task is binary classification between digits 0 and 1. The entire input space covers all possible gray-scale images of the same shape. y is the output (logit) of input x.\n\nWe connect the output histogram problem to the density of states (DOS) problem in physics by making an analogy between the system energy and neural network output, as shown in the right figure. If one follows the physics language to describe our problem, the input x to the neural network can be viewed as the configuration x of the system; the neural network output (e.g., logit values in binary classifier) y(x) corresponds to the energy function E(x); the desired output histogram can be obtained through the DOS (a.k.a., the entropy, S(E(x)), the log scale of DOS), which is the count of the configurations given the energy value. Note that the density of states by definition is over the entire input space, which aligns perfectly with our objective.\n\nInspired by the Wang-Landau algorithm (Wang & Landau, 2001) designed for density of states, we propose an efficient sampler that is driven to explore the under-explored output values through a gradient-based proposal. If the binary classifier is well-trained, it is reasonable to believe that in-distribution inputs (images) which usually have certain semantic structures are concentrated in certain output ranges. If one follows the random proposal in the Wang-Landau algorithm, it is difficult to propose the inputs with meaningful structure (or even in-distribution inputs), thus possibly preventing the sampler from exploring the corresponding output values. Thus, we propose to apply a gradient-based proposal called Gibbs-with-Gradients (GWG) (Grathwohl et al., 2021) which proves to be efficient to propose in-distribution inputs for a trained model.\n\nWith the help of this new sampler, we can reveal some new understanding of the models for the entire input space. First, in our experiments on a real-world dataset, the dominant output values are very negative and correspond to the human-unrecognizable inputs. This indicates the models may map an overwhelmingly large number of unrecognizable images to the overconfident prediction probabilities. Second, we can derive the relative difference between the dominant peak of output values and the other output values, especially those where the in-distribution inputs correspond to. The output values where the in-distribution inputs correspond to are also dominated by the humanunrecognizable inputs. This result presents significant challenges to the OOD detection problems. Third, we observe a clear trend of the representative samples in a CNN model and speculate it simply utilizes the background to predict the labels of the digits.\n\nOur contributions are summarized as follows.\n\n• We work on the challenging problem to uncover the output distribution over the entire input space.\n\nSuch output distributions offer a novel perspective to understand deep neural networks.\n\n• We connect this output distribution problem to the density of states problem in physic and successfully tailor Wang-Landau algorithm using a gradient-based proposal, which is a must-have component to sample the entire output space as much as possible, improving the efficiency.\n\n• We conduct extensive experiments on toy and real-world datasets based on CNN and ResNet-18 to confirm the correctness of our proposed sampler and discover novel and interesting findings.\n\nWe believe that our approach opens a new gate for neural model evaluation and shall be further explored in future works. For example, one can can utilize our sampler to estimate the intrinsic ratio of in-distribution samples given a range of interest with human evaluation as shown in Sec. 5.3.\n\n2\n\nyEPhysicsMLE=∑!\",$%J\"$x\"x$+μ∑$h$x$IsingmodelNeural Networkxconfigurationx(e.g., images)Energy: E(x)Output: y(x)Density of states (DOS): S(E(x))Output histogram: S(y(x))Under review as a conference paper at ICLR 2023\n\n2 PROBLEM DEFINITION\n\nIn the traditional setting, binary neural classifiers model the class distribution through logit z. A neural classifier parameterized by θ learns pθ(z|x) = δ(z − yθ(x)) through a function yθ : x → z ∈ R, where x ∈ Ω, Ω ⊆ {0, ..., N }D for images, and δ is the Dirac delta function. Ω is aligned with Gibbs-With-Gradient’s setting to be discrete.\n\nWhat the above model does not define is the distribution of the data x. This paper aims to obtain the output value distribution of binary classifiers in the entire input space: Ω = {0, ..., N }D. Here we assume that the data distribution p(x) follows the uniform distribution over the domain Ω of x and denote its measure by μ. We define the joint distribution\n\npθ(z, x) = pθ(z|x)μ(x)\n\nOur goal is only the logit (output) distribution. We marginalize the above joint distribution to define the density given the logit z:\n\npθ(z) =\n\n(cid:88)\n\nΩ\n\npθ(z|x)μ(x) =\n\n(cid:88)\n\nx∈Ω\n\nδ(z − yθ(x))\n\nTo sample from the distribution pθ(z), we can first sample xi ∼ Uniform(Ω), then condition on the sampled xi, sample zi ∼ pθ(z|xi). While uniform sampler in principle can resolve our problem, it takes almost forever to converge.\n\n3 METHOD\n\nIn this section, we first discuss the connection between our problem to density of states (DOS), introduce both Wang-Landau algorithm and Gibbs-with-Gradient as background, and present our new sampler Gradient-Wang-Landau algorithm.\n\n3.1 CONNECTION TO DENSITY OF STATES (DOS) IN PHYSICS\n\nIn statistical physics, given the energy function E : x → E ∈ R , the DOS ρ(E) is defined as (cid:88)\n\nρ(E) =\n\nδ(E − E(x))\n\nwhere δ is the Dirac delta function and Ω is the domain of x where x is valid. The DOS is treated as a probability distribution in the energy space, whose log-probability is defined as the entropy S:\n\nx∈Ω\n\nρ(E) = exp(S(E))\n\nBoltzmann constant is assumed to be 1 in our setting. DOS is meaningful because many physical quantities depend on energy or its integration but not the specific input x.\n\nWe connect the output histogram to DOS in physics by making an analogy between the system energy E = E(x) and neural network output z = y(x). This connection is based on the observation that the energy function in physics maps an input configuration to a scalar-valued energy; similarly, a binary neural classifier maps an image to a logit. Both the logit and energy are treated as the direct output of the mapping. Other quantities, such as the loss, are derived from the output. The desired output histogram can be obtained similarly through sampling the DOS (a.k.a., the entropy S(E(x)) or S(y(x)) in the log scale) which is the count of the configurations given the energy value. The output histogram and DOS are defined in the entire input space.\n\n3.2 WANG-LANDAU ALGORITHM AND GIBBS-WITH-GRADIENT\n\nWang-Landau algorithm is a Markov chain Monte Carlo sampler that samples DOS. Since the true distribution ρ(E) is what we are interested in sampling but its formula/model is unknown, we need to approximate it. The Wang-Landau algorithm uses a histogram to store the current estimation ̃S. It improves the sampling efficiency by sampling the inverted distribution:\n\np(x) ∝ exp(− ̃S(E(x)))\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nBy sampling p(x), we can get an ensemble of E via E(·) whose probability distribution is:\n\nπ(E) =\n\n(cid:88)\n\nδ(E − E(x))\n\nx∼p(x)\n\nWhen ̃S(E) approaches S(E), the energy distribution π(E) approaches to E-independent constant for all the accessible energy E.\n\nGibbs-With-Gradient (GWG) is used for energy-based models (EBM) by sampling\n\nlog p(x) = f (x) − log Z,\n\nwhere f (x) is the unnormalized log-probability, Z is the partition function, and x is discrete. Typical Gibbs sampler iterates every dimension xi of x, computes the conditional probability p(xi|x1, ...xi−1, xi+1, ..., xD), and samples according to this conditional probability.\n\nWhen the training data x are natural images and the EBM learns x decently well, the traditional Gibbs sampler wastes much of the computation. For example, most pixel-by-pixel iterations over xi in MNIST dataset will be on the background which should stay black. GWG proposes a smart proposal that picks the pixel xi that is more likely to change, such as the pixels around the edge between the bright and dark region of the digits.\n\n3.3 WANG-LANDAU WITH GRADIENT PROPOSAL\n\nDirectly applying Wang-Landau algorithm is not enough as it uses random proposal, because a trained neural model learns preferred mapping through the loss function. For example, a binary classifier should map the training inputs to either the sufficiently positive or negative logit values which ideally should correspond to the extremely rare but semantically meaningful inputs. After the sampler explores and generates the peak centered at 0 where most random samples correspond to as shown in Fig. 1b, it is almost impossible for the sampler with a random proposal to propose the inputs with meaningful structure (or even in-distribution inputs) so that the other possible output values are explored. Of course, whether those output values correspond to in-distribution inputs is only confirmable after sampling. In summary, it is extremely difficult for the random proposal in Wang-Landau algorithm to explore (almost all) the possible output values.\n\nWe propose to use the framework of Wang-Landau algorithm but replace the proposal distribution with the Gibbs-With-Gradients (GWG) sampler which has a gradient proposal, since the gradient proposal takes the advantage of model’s learned weights to propose inputs. In order to sample the distribution of the output prediction through GWG, we define log-probability f (x) as:\n\nf (x) = S(y(x))\n\nwhere S is the count for the bin corresponding to y(x). The fixed f (·) in the original GWG is changing in our sampling process given the input x, since the formula for S is unknown and we can only estimate the output distribution as we did in Wang-Landau algorithm. Moreover, the GWG requires the gradient of f , but the S is not differentiable since it is approximated through discrete bins. We adopt a first-order differentiable interpolation for the discrete histogram of entropy.\n\nIn summary, similar to the original Wang-Landau algorithm, we first initialize two histograms with all of their bins to 0. One of these histograms is for entropy S, and the other histogram, H, is a counter of how many times the sampler workers visited a specific bin and H is also for the flatness check. We first preset the number of iterations. When H passes the flatness check, it enters the next iteration loop with the step counter reset to 0. Every step in the while loop until the flatness check passes, we interpolate the entropy in the histogram to get a differentiable interpolation and take the derivative of the negation of the entropy with respect to the output z and the inputs x through the chain rule. GWG uses this gradient to propose the next input that is likely to have lower entropy and be accepted by the sampler. his procedure drives the sampler to visit rare samples whose logit values correspond to the lower entropy until S converges. This proposal also goes through a Monte-Carlo accept-reject procedure in the GWG. Once the flatness is met, the bins of H are reset to 0 and the step size is halved before a new iteration. Our proposed algorithm is in Alg. 1 in Appendix.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4 RELATED WORKS AND DISCUSSIONS\n\nPerformance Characterization has long been explored even before the era of deep learning (Haralick, 1992; Klette et al., 2000; Thacker et al., 2008). The input-output relationship has been explored for simple functions (Hammitt & Bartlett, 1995) and mathematical morphological operators (Gao et al., 2002; Kanungo & Haralick, 1990). Compared to existing performance characterization approaches (Ramesh et al., 1997; Bowyer & Phillips, 1998; Aghdasi, 1994; Ramesh & Haralick, 1992; 1994), our work focuses on the output distribution (Greiffenhagen et al., 2001) of a neural network over the entire input space (i.e., not task specific) following the blackbox approach (Courtney et al., 1997; Cho et al., 1997) where the system transfer function from input to output is unknown. Our setting shall be viewed as the most general forward uncertainty quantification case (Lee & Chen, 2009) where the model performance is characterized when the inputs are perturbed (Roberts et al., 2021). To our best knowledge, we demonstrate for the first time that the challenging task of sampling the entire input space for modern neural networks is feasible and efficient by drawing the connection between neural network and physics models. Our proposed method can offer samples to be further integrated with the performance characterization methods mentioned above.\n\nDensity Estimation and Energy Landscape Mapping Previous works in density estimation focus on data density Tabak & Turner (2013); Liu et al. (2021), where class samples are given and the goal is to estimate the density of samples. Here we are not interested in the density of the given dataset, but the density of all the valid samples in the pixel space for a trained model. Hill et al. (2019); Barbu & Zhu (2020) have done the pioneering work in sampling the energy landscape for energy-based models. Their methods specifically focus on the local minimum and barriers of the energy landscape. We can relax the requirement and generalize the mapping on the “output” space where either sufficiently positive or sufficiently negative output (logit) values are meaningful in binary classifiers and other models.\n\nOpen-world Model Evaluation Though many neural models have achieved the SOTA performance, most of them are only on in-distribution test sets (Dosovitskiy et al., 2021; Tolstikhin et al., 2021; Steiner et al., 2021; Chen et al., 2021; Zhuang et al., 2022; He et al., 2015; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Huang et al., 2017; Zagoruyko & Komodakis, 2016). Openworld settings where the test set distribution differs from the in-distribution training set create special challenges for the model. While the models have to detect the OOD samples from in-distribution samples (Liu et al., 2020; Hendrycks & Gimpel, 2016; Hendrycks et al., 2019; Hsu et al., 2020; Lee et al., 2017; 2018; Liang et al., 2018; Mohseni et al., 2020; Ren et al., 2019), we also expect sometimes the model could generalize what it learns to OOD datasets (Cao et al., 2022; Sun & Li, 2022). It has been discovered that models have over-confident predictions for some OOD samples that obviously do not align with human judgments (Nguyen et al., 2015). The OOD generalization becomes more challenging because of this discovery, because the models may not be as reliable as we thought they were. Adversarial test sets Szegedy et al. (2013); Rozsa et al. (2016); Miyato et al. (2018); Kurakin et al. (2016); Xie et al. (2019); Madry et al. (2017) also present special challenges as models decisions are different from those of humans. Having a full view of input-output relation with all the above different kinds of test sets under consideration is important.\n\nSamplers MCMC samplers (Chen et al., 2014; Welling & Teh, 2011; Li et al., 2016; Xu et al., 2018) are developed to scale to big datasets and sample efficient with gradients. Recently, GibbsWith-Gradients (GWG) (Grathwohl et al., 2021) is proposed to pick the promising pixel(s) as the proposal. To further improve sampling efficiency, CSGLD (Deng et al., 2020) drives the sampler to explore the under-explored energy using similar idea as Wang-Landau algorithm (Wang & Landau, 2001). The important difference between our problem setting and the previous ones solved by other MCMC samplers is the function or model as distribution to be sampled from is unknown. Wang-Landau algorithm utilizes previous approximation of the distribution to drive the sampler to explore the under-explored energy regions. This algorithm can be more efficient through parallelization (Vogel et al., 2013; Cunha-Netto et al., 2008), bin-free (Junghans et al., 2014; Li & Eisenbach, 2017) and extended to multi-dimensional outputs (Zhou et al., 2006). While the previous samplers can be applied to high dimensional inputs, the energy functions written by physicists are relative simple and symmetric. However, modern neural networks are complex and hard to characterize performance (Roberts et al., 2021). We assume agnostic of the output properties of the model and thus apply the Wang-Landau algorithm to sample the entropy as a function of energy but with the gradient proposal in GWG to make the sampler more efficient. Similar to GWG, our sampler can propose\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nthe inputs corresponding to the under-explored regions of outputs. Improvements of efficiency can benefit from a patch of pixel changes.\n\n5 EXPERIMENTS\n\nIn this section, we apply our proposed Gradient Wang-Landau sampler to inspect a few neural network models and present the discovered output histogram together with representative samples. The dataset and model training details are introduced in Sec. 5.1. We first empirically confirm our sampler performance through a toy example in Sec. 5.2. We then discuss results for modern binary classifiers in Sec. 5.3 and Sec. 5.4. Hyperparameters of the samplers tested in are Appendix C.\n\n5.1 DATASETS, MODELS, AND OTHER EXPERIMENT SETTINGS Datasets As aforementioned, we focus on binary classification. Therefore, we derive two datasets from the MNIST datasets by only including samples with labels {0, 1}. The training and test splits are the same as those in the original MNIST dataset.\n\n• Toy is a simple dataset with 5×5 binary input images we construct. It is designed to make feasible the bruteforce enumeration over the entire input space (only 25×5 different samples). We center crop the MNIST samples from {0, 1} classes and resize them to 5 × 5 images. We compute the average of the pixel values and use the average as the threshold to binarize the images — the pixel value lower than this threshold becomes 0; otherwise, it becomes 1. The duplicates are not removed for accuracy after resizing since PyTorch does not find duplicate row indices.\n\n• MNIST-0/1 is an MNIST dataset whose samples only have the 0,1 labels. To align with the GWG setting, the inputs are discrete and not Z-normalized. Therefore, in this dataset, the input x is 28 × 28 dimensional with discrete pixel values from {0, ..., 255}.\n\nNeural Network Models for Evaluation Since the focus of this paper is not to compare different neural architectures, given the relatively small datasets we have, we train two types of models, a simple CNN and ResNet-18 (He et al., 2015). Each pixel of the inputs is first transformed to the one-hot encoding and passed to a 3-by-3 convolution layer with 3 channel output. The CNN model contains 2 convolution layers with 3-by-3 filter size. The output channels are 32 and 128. The final features are average-pooled and passed to a fully-connected layer for the binary classification.\n\nPlease keep in mind that our goal in this experiment section is to showcase that our proposed sampler can uncover some novel interesting empirical insights for neural network models. Models with different architectures, weights due to different initialization, optimization, and/or datasets will lead to different results. Therefore, our results and discussions are all model-specific. Specifically, we train a simple CNN model to classify the 5×5 binary images in the Toy dataset (CNN-Toy). The test accuracy of this CNN-Toy model reaches 99.7%, which is almost perfect. We train a simple CNN model to classify the 28 × 28 grey-scale images in the MNIST-0/1 dataset (CNN-MNIST-0/1). The test accuracy of CNN-MNIST-0/1 model is 97.8%. We train a ResNet-18 model to classify the 28 × 28 grey-scale images in the MNIST-0/1 dataset (ResNet-18-MNIST-0/1). The test accuracy of ResNet-18-MNIST-0/1 model is 100%.\n\nSampling Methods for Comparison We compare several different sampling methods (including our proposed method) to obtain the output histogram over the entire input space.\n\n• Enumeration generates the histogram by enumerating all the possible pixel values as inputs. This\n\nis a rather slow but the most accurate method.\n\n• In-dist Test Samples generates the histogram of the inputs based on the fixed test set.This is commonly used in machine learning evaluation. It is based on a very small and potentially biased subset of the entire input space.\n\n• Wang-Landau algorithm (WL) generates the histogram the Wang-Landau algorithm with the random proposal. Specifically, we randomly pick one pixel at a time and change it to any valid (discrete) value as in this implementation 1.\n\n• Gradient Wang-Landau (GWL) generates the histogram by our proposed sampler of Wang-\n\nLandau algorithm with gradient proposal.\n\n1https://www.physics.rutgers.edu/ ̃haule/681/src_MC/python_codes/\n\nwangLand.py\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Output histograms of CNN-Toy obtained by different sampling methods. The indistribution samples are only a very small portion in the output histogram. We also present the representative samples obtained by GWL given different logit values.\n\nFigure 3: Output histograms of CNN-MNIST-0/1 obtained by different sampling methods. The blue scale is for GWL and the red scale is for In-distribution Test Samples. We also present the representative samples obtained by GWL given different logit values (more in Fig. 6 in Appendix)\n\n5.2 RESULTS OF CNN-TOY\n\nGiven the CNN-Toy model, we apply Enumeration, GWL, and In-dist Test Samples to obtain the output histograms, as shown in Fig. 2. Note that our GWL method samples the relative entropy of different energy values as duplicate x may be proposed. After normalization with the maximum entropy, the GWL histogram almost exactly matches the Enumeration histogram which is the ground truth histogram. This confirms the accuracy of our GWL sampler and we can apply it further to more complicated models with confidence.\n\nRemarkably, this histogram is quite different from the expectation we presented in Fig. 1b — this histogram is even not centered at 0 or has the expected subdominant peaks on both the positive and negative sides. Instead, the dominant peak is so wide that it covers almost the entire spectrum of the possible output values. From a coarse-grained overview, most of the samples are mapped to the center of logit −5 with a decay from −5 to both sides in the CNN-Toy model. This shows the CNN-Toy model is biased to predict more samples to the negative logit values.\n\nIn Fig. 2, we also present the representative samples obtained by GWL given different logit values in the CNN-Toy model. The visualization results suggest that the CNN-Toy model probably learns the digit “1” for positive logit values as the center pixels of the representative samples are white (see the three representative samples with logit values from 0 to 20) and “0” for the very negative logit values as the center pixels of the representative samples are black (see two representative samples with logit values from -20 to -30). From this example, one can see that the output histogram over the entire input space can offer a comprehensive understanding of the neural network models, helping researchers better understand critical questions such as the distribution of the outputs, where the model maps the samples to, and what the representative samples with high likelihood are.\n\n5.3 RESULTS OF CNN-MNIST-0/1\n\nHistogram Results by GWL The trial of applying GWL on the CNN-Toy model is encouraging and we now apply GWL to the CNN-MNIST-0/1 that is trained on a real-world dataset. The results are shown in Fig. 3. As our GWL reveals, the output histogram of CNN-MNIST-0/1, similar to CNN-Toy’s histogram, does not have the subdominant peaks. It is also different from the presumed case in Fig. 1b. Compared with the output histogram of the CNN-Toy model (i.e., Fig. 2), this\n\n7\n\n302010010y(x)10505S(y(x))Relative entropy S of output values for Toy datasetEnumeration (ground truth)GWL (ours)In-dist test samples5040302010010y(x)40003000200010000S(y(x))Relative Entropy of output valuesGWLin-dist test samples010203040Under review as a conference paper at ICLR 2023\n\n(a) S of GWL(ours) for steps.\n\n(b) S of GWL(ours) per iteration.\n\n(c) S of WL per iteration.\n\nFigure 4: Intermediate output histogram S per iteration. (a) GWL gradually explores the logit values in the first iteration. (b) GWL discovers the output histogram well within 2 iterations. (c) The original WL explores the output distribution much slower.\n\ntime, the peak is on the negative boundary and the histogram is skewed towards the negative logit values. S almost linearly decays to the positive logit values. While the in-distribution samples have logit values between −20 and 15 as we expect, these samples are exponentially (i.e., e1300 at logit value -20 to e3100 at logit value 13, thousands in log scale) less often found than the majority samples whose logit values are around −55. From a fine-grained view, the CNN-MNIST-0/1 model tends to map the human-unrecognizable samples to the very negative logit values. While previous work (Nguyen et al., 2015) showed the existence of the overconfident prediction samples, our result shows a rough but quantitative performance of this CNN which can serve as a baseline for further improvements. One may notice that in Fig. 3, there is still some output values (e.g., the rightmost positive logit region) that are not yet covered by our GWL sampler. We believe that this calls for more future work to follow on more advanced efficient samplers.\n\nGWL is much more efficient than WL Since WL takes a much longer time to converge, we are not able to obtain the converged results from WL. For the comparison purpose, we inspect the intermediate S results of the GWL and WL samplers, as shown in Fig. 4. As one can see from Fig. 4a, in the first iteration, GWL has already been able to gradually explore the logit values efficiently from the most dominant output value around −55 to the positive logit values. Within only two iterations, as shown in Fig. 4b, GWL can discover the output histogram covering the value range from −55 to 13. On the other hand, as presented in Fig. 4c, in the first two iterations, the original WL can only explore the output ranges from around −55 to −45; in the 3rd iteration, WL converges significantly slower and never ends in a reasonable time. This result indicates that the GWL converges much faster than the original WL and is able to explore a much more diverse range of output values.\n\nManual inspection on more representative samples As show in Fig. 3, for the CNN-MNIST0/1 model, GWL can effectively sample input images from logit values ranging from -55 to 13. We further group these logit values per 100 bins (100 bins correspond to a difference of 10 in logit value) in S, resulting in about 7 groups. For every group, we sample 200 representative input images. To make sure they are not correlated, we sample every 1000 steps. For demonstration purposes, we randomly pick 10-out-of-200 samples from every group in Fig. 6 in Appendix. We manually inspect the sufficiently positive group (e.g., the last column in Fig. 6) and the sufficiently negative groups (e.g., the first five columns in Fig. 6) , and there are no human recognizable samples of digits. We also observe an interesting pattern that as the logit value increases, more and more representative samples have black background. This result suggests that the CNN-MNIST-0/1 model may heavily rely on the background to classify the images (Xiao et al., 2020). We conjecture that is because the samples in the most dominant peak are closer to class 0 samples than class 1 samples (Appendix. D). In summary, although CNN-MNIST-0/1 holds a very high in-distribution test accuracy, it is far from a robust model because it does not truly understand the semantic structure of the digits.\n\nDiscussion Fig. 3 presents challenges to the OOD detection methods that may be more modeldependent than we thought before. If the model cannot map most of the human unrecognizable samples with high uncertainty, the likelihood-based OOD detection methods (Liu et al., 2020; Hendrycks & Gimpel, 2016) cannot perform well for samples in the entire input space. Fig. 6 shows the inputs with the in-distribution output values (output logits of the red plot) of the CNN model may not uniquely correspond to in-distribution samples. More rigorous experiments to a definite conclusion are yet required as future work.\n\n8\n\n50050100150y(x)020004000S(y(x))Histogram plot for different stepssteps=1000steps=5000steps=10000steps=200006040200y(x)020004000S(y(x))Histogram plot per iteration (itt)itt=0itt=1itt=26040200y(x)05S(y(x))1e6Histogram plot per iteration (itt)itt=0itt=1itt=2Under review as a conference paper at ICLR 2023\n\n(a) Results with random re-initialization.\n\n(b) Results with test set re-initialization.\n\nFigure 5: Output histograms of ResNet-18-MNIST-0/1 obtained by different sampling methods. There may be a sharp local minima in the output landscape causing a cliff around the logit value of -33 and making GWL “trapped”. We have tried two variants to address the “trapped” issue via (a) random re-initialization and (b) test set re-initialization. The blue scale is for GWL and the red scale is for In-distribution Test Samples. We also present the representative samples obtained by GWL given different logit values.\n\n5.4 RESULTS OF RESNET-18-MNIST-0/1\n\nWhen applying our GWL samplers to the ResNet-18-MNIST-0/1 model, we observe that the sampler can easily get trapped in some output regions. This is a fairly common phenomenon for WangLandau-based samplers, as reported in (Vogel et al., 2018). We follow the common practice to re-initialize the sampler to the random samples every time it gets trapped. We let those workers run 1000 steps (10,000 pixels selected with replacement) before counting to S again. As shown in Fig. 5a, the smallest logit values in ResNet-18-MNIST-0/1 are around -220, much lower than those of CNN-MNIST-0/1. A wide range of negative logit values corresponds to human unrecognizable inputs and there is no obvious pattern observed in contrast to CNN-MNIST-0/1’s results.\n\nInterestingly, we observe a cliff around the logit value of -33. We try another variant for reinitialization: we re-initialize using the test set samples every time it gets trapped in certain output value. This time, as shown in Figure 5b, it can explore the output values larger than -33. We believe there may be a sharp local minima in the output landscape, similar to the case discussed before Vogel et al. (2018).\n\nBecause of the “trapping” issue and the complexity of ResNet-18 over CNN, we have to relax the flatness check a bit to let GWL converge for the first iteration. Because of the less rigorous flatness check, we do not draw conclusions about ResNet-18-MNIST-0/1 evaluation of the relative entropy differences. Compared with the CNN-MNIST-0/1 model, ResNet-18-MNIST-0/1 has more interesting phenomena and further exploration is needed to understand these phenomena.\n\n6 CONCLUSION\n\nWe aim to get a full picture of the input-output relationship of a model through the inputs valid in the pixel space. We propose to use a histogram to better understand the input-output distribution. When the inputs are high-dimensional, enumeration or uniform sampling is either impossible or takes too long to converge. We connect the density of states in physics to this histogram sampling problem. We propose to use an efficient sampler to achieve this goal. We confirm empirically this can be achieved and uncover some new aspects of neural networks.\n\nFor future work, it is interesting to develop a new and more efficient sampler that has theoretical guarantees to acquire this input-output relationship in order to sample with more pixels, such as the ImageNet (Deng et al., 2009). Most importantly, with this new sampler, we can develop new insights into network architectures developed in the last decade for open-world applications.\n\n7 REPRODUCIBILITY\n\nWe provide fairly amount of information to re-implement our sampler. The data processing is in the Sec. 5.1 and algorithm is in Appendix B. The hyperparameters and sampling details are also listed in the Sec. 5. We also provide different time stamp of steps for our samplers to indicate what to expect during the sampling procedure in Fig. 4. Of course, the Wang-Landau algorithm we adopted\n\n9\n\n200150100500y(x)2000150010005000S(y(x))Relative Entropy of output valuesGWLin-dist test samples01020304050200150100500y(x)10005000S(y(x))Relative Entropy of output valuesGWLin-dist test samples01020304050Under review as a conference paper at ICLR 2023\n\nis the prototypical one and it subjects to some issues reported in its follow-up works, such as the discontinuity of the boundaries between bins and trapping in one of the bins. These problems lead to some issues in our experiments and we discussed them in Sec. 5.4. More advanced algorithms have been developed to resolve these issues.\n\n8 ETHICS STATEMENT\n\nOur method aims to provide a comprehensitve understanding of the neural models. This work will be applicable to many applications, such as those in the safety and trusty-worthy machine learning. As a pilor study, we do not anticipate the negative aspects of our work.\n\nREFERENCES\n\nFarzin Aghdasi. Digitization and analysis of mammographic images for early detection of breast\n\ncancer. PhD thesis, University of British Columbia, 1994.\n\nAdrian Barbu and Song-Chun Zhu. Mapping the energy landscape. In Monte Carlo Methods, pp.\n\n367–420. Springer, 2020.\n\nKevin Bowyer and P Jonathon Phillips. Empirical evaluation techniques in computer vision. IEEE\n\nComputer Society Press, 1998.\n\nKaidi Cao, Maria Brbic, and Jure Leskovec. Open-world semi-supervised learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=O-r8LOR-CCA.\n\nTianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo.\n\nIn\n\nInternational conference on machine learning, pp. 1683–1691. PMLR, 2014.\n\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets\n\nwithout pretraining or strong data augmentations. arXiv preprint arXiv:2106.01548, 2021.\n\nKyujin Cho, Peter Meer, and Javier Cabrera. Performance assessment through bootstrap. Transactions on Pattern Analysis and Machine Intelligence, 19(11):1185–1198, 1997.\n\nIEEE\n\nPatrick Courtney, Neil Thacker, and Adrian F Clark. Algorithmic modelling for performance evalu-\n\nation. Machine Vision and Applications, 9(5):219–228, 1997.\n\nAntˆonio Gonc ̧alves da Cunha-Netto, AA Caparica, Shan-Ho Tsai, Ronald Dickman, and David Paul Landau. Improving wang-landau sampling with adaptive windows. Physical Review E, 78(5): 055701, 2008.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\n\nWei Deng, Guang Lin, and Faming Liang. A contour stochastic gradient langevin dynamics algorithm for simulations of multi-modal distributions. In Advances in Neural Information Processing Systems, 2020.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\n\nXiang Gao, Visvanathan Ramesh, and Terry Boult. Statistical characterization of morphological operator sequences. In European Conference on Computer Vision, pp. 590–605. Springer, 2002.\n\nWill Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, and Chris Maddison. Oops i took a gradient: Scalable sampling for discrete distributions. In International Conference on Machine Learning, pp. 3831–3841. PMLR, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMichael Greiffenhagen, Dorin Comaniciu, Heinrich Niemann, and Visvanathan Ramesh. Design, analysis, and engineering of video monitoring systems: An approach and a case study. Proceedings of the IEEE, 89(10):1498–1517, 2001.\n\nAM Hammitt and EB Bartlett. Determining functional relationships from trained neural networks.\n\nMathematical and computer modelling, 22(3):83–103, 1995.\n\nRobert M Haralick. Performance characterization in computer vision.\n\nIn BMVC92, pp. 1–8.\n\nSpringer, 1992.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. arXiv preprint arXiv:1512.03385, 2015.\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\n\nexamples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\n\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier In International Conference on Learning Representations, 2019. URL https://\n\nexposure. openreview.net/forum?id=HyxCxhRcY7.\n\nMitch Hill, Erik Nijkamp, and Song-Chun Zhu. Building a telescope to look into high-dimensional\n\nimage spaces. Quarterly of Applied Mathematics, 77(2):269–321, 2019.\n\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized ODIN: Detecting outIn Proceedings of the\n\nof-distribution image without learning from out-of-distribution data. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10951–10960, 2020.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.\n\nChristoph Junghans, Danny Perez, and Thomas Vogel. Molecular dynamics in the multicanonical ensemble: Equivalence of wang–landau sampling, statistical temperature molecular dynamics, and metadynamics. Journal of chemical theory and computation, 10(5):1843–1847, 2014.\n\nTapas Kanungo and Robert M Haralick. Character recognition using mathematical morphology. In\n\nProc. of the Fourth USPS Conference on Advanced Technology, pp. 973–986, 1990.\n\nReinhard Klette, H Siegfried Stiehl, Max A Viergever, and Koen L Vincken. Performance charac-\n\nterization in computer vision. Springer, 2000.\n\nAlexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world,\n\n2016.\n\nKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers\n\nfor detecting out-of-distribution samples. arXiv preprint arXiv:1711.09325, 2017.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, pp. 7167–7177, 2018.\n\nSang Hoon Lee and Wei Chen. A comparative study of uncertainty propagation methods for black-\n\nbox-type problems. Structural and multidisciplinary optimization, 37(3):239–253, 2009.\n\nChunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic gradient langevin dynamics for deep neural networks. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nYing Wai Li and Markus Eisenbach. A histogram-free multicanonical monte carlo algorithm for the basis expansion of density of states. In Proceedings of the Platform for Advanced Scientific Computing Conference, pp. 1–7, 2017.\n\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In 6th International Conference on Learning Representations, ICLR 2018, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nQiao Liu, Jiaze Xu, Rui Jiang, and Wing Hung Wong. Density estimation using deep generative neural networks. Proceedings of the National Academy of Sciences, 118(15):e2101344118, 2021.\n\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detec-\n\ntion. Advances in Neural Information Processing Systems, 2020.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993, 2018.\n\nSina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizable out-of-distribution detection. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):5216–5223, April 2020. ISSN 2159-5399. doi: 10.1609/aaai.v34i04.5966.\n\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427–436, 2015.\n\nV Ramesh and RM Haralick. A methodology for automatic selection of iu algorithm tuning param-\n\neters. In ARPA Image Understanding Workshop, 1994.\n\nVisvanathan Ramesh and Robert M Haralick. Random perturbation models and performance charIn Proceedings 1992 IEEE Computer Society Conference on\n\nacterization in computer vision. Computer Vision and Pattern Recognition, pp. 521–522. IEEE Computer Society, 1992.\n\nVisvanathan Ramesh, RM Haralick, AS Bedekar, X Liu, DC Nadadur, KB Thornton, and X Zhang. Computer vision performance characterization. RADIUS: Image Understanding for Imagery Intelligence, pp. 241–282, 1997.\n\nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and In Advances in\n\nBalaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. Neural Information Processing Systems, pp. 14680–14691, 2019.\n\nDaniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\n\npreprint arXiv:2106.10165, 2021.\n\nAndras Rozsa, Ethan M Rudd, and Terrance E Boult. Adversarial diversity and hard positive genIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n\neration. Workshops, pp. 25–32, 2016.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nAndreas Steiner, Alexander Kolesnikov, , Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021.\n\nYiyou Sun and Yixuan Li. Open-world contrastive learning. arXiv preprint arXiv:2208.02764, 2022.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.\n\nEsteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms.\n\nCommunications on Pure and Applied Mathematics, 66(2):145–164, 2013.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nNeil A Thacker, Adrian F Clark, John L Barron, J Ross Beveridge, Patrick Courtney, William R Crum, Visvanathan Ramesh, and Christine Clark. Performance characterization in computer vision: A guide to best practices. Computer vision and image understanding, 109(3):305–334, 2008.\n\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, arXiv preprint and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. arXiv:2105.01601, 2021.\n\nThomas Vogel, Ying Wai Li, Thomas W ̈ust, and David P Landau. Generic, hierarchical framework for massively parallel wang-landau sampling. Physical review letters, 110(21):210603, 2013.\n\nThomas Vogel, Ying Wai Li, and David P. Landau. A practical guide to replica-exchange ISSN 1742-6588. doi: 10.1088/1742-6596/1012/\n\nwang—landau simulations. 1012, 4 2018. 1/012003. URL https://www.osti.gov/biblio/1479789.\n\nFugao Wang and David P Landau. Efficient, multiple-range random walk algorithm to calculate the\n\ndensity of states. Physical review letters, 86(10):2050, 2001.\n\nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.\n\nIn Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681–688, 2011.\n\nKai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of\n\nimage backgrounds in object recognition. arXiv preprint arXiv:2006.09994, 2020.\n\nCihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. In Proceedings of the\n\nImproving transferability of adversarial examples with input diversity. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2730–2739, 2019.\n\nPan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. Advances in Neural Information Processing Systems, 31, 2018.\n\nSergey Zagoruyko and Nikos Komodakis.\n\nWide residual networks.\n\narXiv preprint\n\narXiv:1605.07146, 2016.\n\nChenggang Zhou, T. C. Schulthess, Stefan Torbr ̈ugge, and D. P. Landau. Wang-landau algorithm for continuous models and joint density of states. Phys. Rev. Lett., 96:120201, Mar 2006. doi: 10.1103/PhysRevLett.96.120201. URL https://link.aps.org/doi/10.1103/ PhysRevLett.96.120201.\n\nJuntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. ICLR, 2022.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX A: REPRESENTATIVE INPUTS\n\nHere we list more representative samples of the CNN-MNIST-0/1 scenario. The samples are bounded by a black box of boundaries.\n\nFigure 6: More representative samples of the CNN-MNIST-0/1 model obtained by GWL at different logit values, grouped by logit values. We further group these logit values per 100 bins (100 bins correspond to a difference of 10 in logit value) in S, resulting in about 10 groups. The output values in the first column are within the range [-55,-45) and the second is from [-45,-35], etc.\n\n14\n\n-50.6-43.2-31.5-18.3-13.8-0.510.3-54.2-43.9-28.9-18.5-13.90.68.8-46.7-44.2-32.8-17.4-13.9-3.511.2-49.3-39.0-26.0-20.2-9.80.77.9-49.1-39.6-33.4-23.4-11.00.86.3-47.9-35.7-32.2-18.6-7.6-0.79.3-47.2-43.1-33.7-16.2-14.93.410.4-45.1-38.1-28.8-23.4-12.23.914.8-53.1-36.7-33.5-22.5-5.7-1.98.8-48.7-43.8-31.7-22.2-8.00.25.8Under review as a conference paper at ICLR 2023\n\nB APPENDIX B: GRADIENT WANG-LANDAU ALGORITHM\n\nHere we provide the algorithms of the GWL algorithm. The input and output are listed. The hyperparameters are determined mostly by the toy-example.\n\nAlgorithm 1 Our proposed Gradient Wang-Landau (GWL)\n\nRequire: pretrained model y: x → z, flat histogram H = 0, entropy histogram S = 0, increment/step-size lnf, number of iterations T, test set Dte, GWG sampler GW G(z, S), interpolation function g(z, S) for t = 1, 2, ..., T do\n\nx ∼ Dte while H is not flat do\n\nz = y(x) Sin = g(z, S) x ∼ GW G(z, −Sin) ̃z = round(z) S[ ̃z] ← S[ ̃z]+lnf H[ ̃z] ← H[ ̃z]+1\n\nend while lnf ← lnf/2 H ← 0\n\nend for return S(y)\n\n▷ Get the continuous interpolation entropy Sin at output z ▷ Take the negation of Sin for Wang-Landau exploration ▷ Round z to the nearest z′ that corresponds to one of the bins\n\n▷ Reset all the bins in counter H to 0\n\nC HYPER-PARAMETERS AND IMPLEMENTATION DETAILS FOR GWL AND\n\nWL\n\nThe hyper-parameters for GWL and WL are extremely similar, if not identical, as the only major difference between GWL and WL is the gradient proposal versus the random proposal. We first preset a large enough range of output values for the sampler to explore the trained neural network models. In our experiments, we found that the output (logit) values of the binary classifiers typically fall in the range of -300 to 100 (based on ResNet). Therefore, we use this range for all experiments. For flatness histogram H, the bin window size is set to be 1, resulting in 400 bins. The histogram H is considered flat if the difference between maximum bin value and minimum bin value is smaller than the average bin value. For output histogram S, we set the bin window size to be 0.1, resulting in 4000 bins. Instead of updating one bin at a time for S, we update the neighbor bins with exponential decay. We use the linear interpolation to approximate the bins for continuous queries. We iterate 5 times with test set initialization. Every step the GWG tries to at most update 10 pixels.\n\nD SAMPLES SIMILARITY\n\nThe samples in the most dominant peak may be closer to class 0 than to class 1. We compute the L2 pixel-wise distance from the uniform noise image to the samples of class 1 and 0 respectively. The mean L2 distance from uniform noise to 0 is around 0.3121 and that from uniform noise to 1 is around 0.3236. The distance between 1 and 0 samples is 0.1652. This result shows the samples in the most dominant peak are closer to class 0 samples than class 1 samples. More rigorous experiments to a definite conclusion is yet required as future work.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper attempts to analyze a binary neural classifier's input-output relationship.\nA straightforward, brute-force sampling is forbidden due to the high dimension and continuity of input distribution.\nTo achieve efficiency, the authors reformulate the problem as density of states (DOS) sampling in physics, and propose to apply a gradient-based proposal which facilitates the exploration of under-explored output values.\nThe main contributions are:\n\n- making the output histogram problem tractable.\n- analyzing the input-output relationship in the entire input space and revealing some interesting properties by sampled statistics.\n- introducing a new perspective to understand the behavior of neural network classifiers.\n\n# Strength And Weaknesses\n\n**[Strength]**\n\n- The idea of trying to understand neural networks through input-output mapping relationships is groundbreaking; it also makes sense to draw on solutions from a related field (condensed matter physics).\n- Various experiments well illustrate that the statistics sampled by the proposed method (GWL) are close to the ground truth (enumeration) and significantly better than directly sampling (from in-dist test samples).\n\n**[Weaknesses]**\n\n- While it is possible to describe the output histogram problem using a domain specific approach (physical language), I still wonder if there is another way (perhaps a more general sampling algorithm) to solve this problem? The authors do not seem to discuss this sufficiently in the Related Work section.\n- Are the properties of neural network input-output mapping observed by the authors in their experiments (e.g., mapping unrecognizable human samples to very negative logit values) consistent or contradictory to some previous literature? Making some comparisons would make the overall analysis a bit more comprehensive.\n- Fig. 6 is interesting, but I don't particularly understand why logit values for larger samples look \"darker\" implying that the *background* is more important than the *foreground* for the classification task? Perhaps the authors could give a more detailed explanation on this?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe articles are clearly written and of good quality.\n\nThe use of a physical perspective for interpretable analysis of the mapping of neural networks is novel.\n\n# Summary Of The Review\n\nIn general I think it is novel and interesting to use a physical perspective to understand the mapping of neural networks. However, there is very little discussion or comparison of related work throughout the paper (e.g., other non-physically inspired sampling algorithms, or other work on the analysis of neural network mappings; see the Weakness section above). This results in this work looking less than complete, and weaken their conclusions. I am inclined to marginally reject it.\n\n========== post rebuttal ==========\n\n> We update section 5.3 (and in appendix D) to present some of our interpretation of the results, such as why the black background appears when the logit gets larger.\n> In the related work section, we compare the methods with MCMC samplers and position our method in terms of performance characterization literature.\n\nThank you for clarifying this and adding Section 5.3 as well as the supplementary material for more explanation. I feel that the two most significant flaws (inadequate discussion of the related work and too few explanations of the Fig. 6) have been addressed, thus improving my rating.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDATA CONTINUITY MATTERS: IMPROVING SEQUENCE MODELING WITH LIPSCHITZ REGULARIZER\n\nEric Qu∗ Duke Kunshan University Kunshan, Jiangsu 215316, China zhonghang.qu@duke.edu\n\nXufang Luo†& Dongsheng Li Microsoft Research Asia Shanghai 200232, China {xufluo, dongsheng.li}@microsoft.com\n\nABSTRACT\n\nSequence modeling is a core problem in machine learning, and various neural networks have been designed to process different types of sequence data. However, few attempts have been made to understand the inherent data property of sequence data, neglecting the critical factor that may significantly affect the performance of sequence modeling. In this paper, we theoretically and empirically analyze a generic property of sequence data, i.e., continuity, and connect this property with the performance of deep models. First, we empirically observe that different kinds of models for sequence modeling prefer data with different continuity. Then, we theoretically analyze the continuity preference of different models in both time and frequency domains. To further utilize continuity to improve sequence modeling, we propose a simple yet effective Lipschitz Regularizer, that can flexibly adjust data continuity according to model preferences, and bring very little extra computational cost. Extensive experiments on various tasks demonstrate that altering data continuity via Lipschitz Regularizer can largely improve the performance of many deep models for sequence modeling.1\n\n1\n\nINTRODUCTION\n\nSequence modeling is a central problem in many machine learning tasks, ranging from natural language processing (Kenton & Toutanova, 2019) to time-series forecasting (Li et al., 2019). Although simple deep models, like MLPs, can be used for this problem, various model architectures have been specially designed to process different types of real-world sequence data, achieving vastly superior performance to simple models. For instance, the vanilla Transformer shows great power in natural language processing (Wolf et al., 2020), and its variant Informer (Zhou et al., 2021) is more efficient in time-series forecasting tasks. And a recent work Structured State Space sequence model (S4) (Gu et al., 2021) reaches SoTA in handling data with long-range dependencies. However, few attempts have been made to understand the inherent property of sequence data in various tasks, neglecting the critical factor which could largely influence the performance of different types of deep models. Such investigations can help us to understand the question that what kind of deep model is suitable for specific tasks, and is essential for improving deep sequence modeling.\n\nIn this paper, we study a generic property of sequence data, i.e., continuity, and investigate how this property connects with the performance of different deep models. Naturally, all sequence data can be treated as discrete samples from an underlying continuous function with time as the hidden axis. Based on this view, we apply continuity to describe the smoothness of the underlying function, and further quantify it with Lipschitz continuity. Then, it can be noticed that different data types have different continuity. For instance, time-series or audio data are more continuous than language sequences, since they are sampled from physical continuous signals evolved through time.\n\nFurthermore, we empirically observe that different deep models prefer data with different continuity. We design a sequence-to-sequence task to show this phenomenon. Specifically, we generate\n\n∗Work done during an internship in Microsoft Research Asia. †Corresponding author. 1Code is available at https://EricQu.site/LipReg/\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) High Continuity\n\n(b) Low Continuity\n\nFigure 1: A Sequence-to-sequence task to show that different deep models prefer data with different continuity. The first row shows input and output sequences. We generate input sequences with different continuities (left column: high continuity; right column: low continuity) and learn a mapping function using different models (second row: S4; third row: Transformer). We can see that S4 prefers more continuous sequences, while Transformer prefers more discrete sequences. And adjusting continuity according to the preferences of models with Lipschitz Regularizer can largely improve their performances. More details of this experiment are in Appendix A.\n\ntwo kinds of input sequences with different continuity, and map them to output sequences using exponential moving average. Then, we use two different deep models to learn this mapping. Each model has an identical 1D convolution embedding layer, and a separate sequence processing module. One uses the S4 model (Gu et al., 2021) and the other uses vanilla Transformer (Vaswani et al., 2017) (with the same number of layers and hidden dimensions). The results of this experiment are shown in Figure 1. It can be observed that the S4 model achieves significantly better performance with more continuous inputs, and the Transformer performs better with more discrete inputs. Note that, essentially, they are learning the same mapping only with different data continuity. This clearly shows different models prefer different data continuity.\n\nInspired by the above observation, we hypothesize that it is possible to enhance model performance by changing the data continuity according to their preferences. To make the proposed method simple and applicable for different deep models, we derive a surrogate that can be directly optimized for the Lipschitz continuity, and use it as a regularizer in the loss function. We call the proposed surrogate Lipschitz Regularizer, which depicts the data continuity and can also be used to adjust it.\n\nThen, we investigate the data continuity property for different models and how to use Lipschitz Regularizer to change data continuity according to the model preference. We provide in-depth analyses in both time and frequency domains. On the one hand, Lipschitz continuity describes the continuity of sequences over time, which is a feature in the time domain. Here, we investigate two models. One is a continuous-time model S4, and the other model Informer is based on self-attention. As for S4, since the fitting error of the S4 model is bounded by the Lipschitz constant, S4 prefers smoother input sequences with smaller Lipschitz constant. Hence, we make the inputs of S4 layers more continuous by adding the Lipschitz Regularizer to the loss function. Experiment results on the Long Range Arena benchmark demonstrate that Lipschitz Regularizer can largely improve\n\n2\n\n0255075100125150175200Index0.00.20.40.60.81.0ValueOutputInput0255075100125150175200Index0.350.400.450.500.550.600.650.700.75ValueGround TruthS40255075100125150175200Index0.40.50.60.70.8ValueGround TruthTransformerwith Lip Regularizer0255075100125150175200Index0.00.20.40.60.81.0ValueOutputInput0255075100125150175200Index0.350.400.450.500.550.60ValueGround TruthS4with Lip Regularizer0255075100125150175200Index0.350.400.450.500.55ValueGround TruthTransformerPublished as a conference paper at ICLR 2023\n\nthe performance of the S4 model, especially for tasks with discrete inputs. Conversely, Informer is built upon self-attention, which is designed to process some tokenized discrete data, e.g., text, so Informer prefers less continuous sequences. Therefore, we decrease the continuity of input sequences by subtracting the Lipschitz Regularizer from the loss function. Prediction performance and empirical analyses on many time-series tasks well prove the superiority of the Lipschitz Regularizer. Also, we observe the same results on the task mentioned above as shown in Figure 1.\n\nOn the other hand, for the frequency domain, we find that Lipschitz Regularizer represents the expectation of the frequency of data’s underlying function. Here, we take the ReLU network as the studied case, and theoretically justify that Lipschitz Regularizer is related to the spectral bias - a phenomenon that neural networks tend to prioritize learning the low-frequency modes. We then propose to use Lipschitz Regularizer by subtracting it from the loss function to mitigate the spectral bias. In this way, neural networks are forced to learn high-frequency parts, and convergence can be accelerated since information in different frequency bands can be learned simultaneously.\n\nIn summary, Lipschitz Regularizer can be used to flexibly adjust data continuity for a wide range of deep models which have a preference for data continuity. It improves various models with very little extra computational cost, shedding a light on inherent data property analyses for sequence modeling.\n\n2 RELATED WORK\n\nDeep Neural Networks for Sequence Modeling Sequence modeling plays a critical role in many machine learning problems. Many general architectures, including MLPs (Rahaman et al., 2019), RNNs (Mikolov et al., 2010), and CNNs (Bai et al., 2018), can all be used for sequence modeling. And recently, two types of models show great power in addressing challenges for sequence modeling, such as handling complex interactions and long-range dependencies. The first type is self-attention-based models. For instance, the vanilla Transformer (Vaswani et al., 2017), Informer (Zhou et al., 2021), and Performer (Choromanski et al., 2020) all show great performance on natural language processing, time-series forecasting, and speech processing, respectively. Another type is continuous-time models, which are built upon the view that inputs are sampled from continuous functions. They include but not limited to Neural ODE (Chen et al., 2018), Lipschitz RNN (Erichson et al., 2020), State-space model (Gu et al., 2021). In this paper, we do not aim at proposing novel models like previous works, but we focus on understanding intrinsic preference for input sequences. We show these two types of models both have a preference for the data continuity property, and we utilize it to promote their performance.\n\nLipschitz Continuity of Neural Networks The Lipschitz continuity is a general property for any function, and is widely used for analyzing different kinds of neural networks, including MLPs (Zhang et al., 2021; Gouk et al., 2021), CNNs (Zou et al., 2019), self-attention-based networks (Dasoulas et al., 2021), graph neural networks (Gama et al., 2020) and GANs (Gulrajani et al., 2017). It becomes an essential property of neural networks, and can be used in various ways, such as improving adversarial robustness (Meunier et al., 2022) and proving generalization bounds (Sokoli ́c et al., 2017). In this paper, we focus on the Lipschitz continuity of the underlying function of sequence data, and use it as a data property, but not a property of models.\n\n3 LIPSCHITZ CONTINUITY OF SEQUENCE DATA\n\nIn this section, we first show the measure for continuity of sequence data, and how it can be used as a regularizer. We give a definition for Lipschitz Regularizer here, and leave detailed analyzes and usages of it in specific models in the rest of the sections. Then, we provide views for Lipschitz Regularizer in both time and frequency domains.\n\nTo define the measure for the continuity of sequence data, we view inputs as signals, and data points in the sequence are discrete samples of an underlying continuous function with certain time steps. Next, we calculate the Lipschitz constant of the underlying function, which is widely used as the measure for continuity. Specifically, suppose the sequence is x0, x1, . . . , xn, and the underlying function is defined as f (t0) = x0, f (t1) = x1, . . . , f (tn) = xn, where t0, t1, . . . , tn are time steps.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nThen, if we let t0 = 0, t1 = 1, . . . , tn = n, the Lipschitz constant Lf of function f is\n\nLf =\n\nmax ti,tj ∈{0,1,...,n}\n\n|f (ti) − f (tj)| |ti − tj|\n\n=\n\nmax i,j∈{0,1,...,n}\n\n|xi − xj| |i − j|\n\n.\n\n(1)\n\nBy Mean Value Theorem, for i, j ∈ {0, 1, . . . , n} and j − i > 1, we could always find an interval [k, k + 1] of time step 1 such that i ≤ k ≤ j − 1, |xi−xj |\n\n|i−j| ≤ |xk+1 − xk|. Therefore, we have\n\nLf =\n\nmax i,j∈{0,1,...,n}\n\n|xi − xj| |i − j|\n\n=\n\nmax k∈{0,1,...,n−1}\n\n|xk+1 − xk|.\n\n(2)\n\nHowever, since we would like to adjust this continuity according to the preferences of different models, this measure should be easy to be optimized, but it is hard to pass gradients due to the max operator. To help with the optimization process, we design a surrogate by taking the average over all terms and changing the norm to l2. Moreover, since we simply use this surrogate as a regularizer in the loss function to flexibly adjust the continuity for various models, we name this term as Lipschitz Regularizer, and its formal definition is given as follows.\n\nDefinition 3.1. (Lipschitz Regularizer) Suppose the sequence is x0, x1, . . . , xn. We define the Lipschitz Regularizer in the following equation:\n\nLLip =\n\n1 n\n\nn−1 (cid:88)\n\ni=0\n\n(xi+1 − xi)2\n\n(3)\n\n3.1 VIEW LIPSCHITZ REGULARIZER IN TIME AND FREQUENCY DOMAINS\n\nWe then provide two views for Lipschitz Regularizer. On the one hand, Lipschitz Regularizer is a feature for sequence data in the time domain, representing the continuity of sequences over time. Thus, it can be used to alter the continuity of input sequences to specific models. As shown in Figure 1, different deep models have different preferences for data continuity. We can use the Lipschitz Regularizer to manually make the sequences more or less continuous, and therefore improve the performance of the model. An example of increasing the continuity to improve the performance of a continuous-time model is described in §4.1. A converse example of decreasing the continuity to improve the performance of an attention-based model is described in §4.2.\n\nOn the other hand, from the frequency perspective, Lipschitz Regularizer directly relates to the frequency of the function, and can be used to alter modes with different frequencies. Specifically,\n\nn−1 (cid:88)\n\n(xi+1 − xi)2 ≈\n\ni=0\n\n(cid:19)2\n\n(cid:90)\n\nR\n\n(cid:18) df (t) dt\n\ndt =\n\n(cid:90)\n\nR\n\n(2πiξ)2 ˆf 2(ξ)(−dξ) = 4π2CEp(ξ)[ξ2]\n\n(4)\n\nwhere ξ is the frequency of the Fourier transform of f . p(ξ) = ˆf 2(ξ)/C is the normalized squared Fourier transform of f , where ˆf (ξ) := (cid:82) f (x)e−i2πξxdx. Details of the derivation are presented in Appendix G.1. Essentially, the Lipschitz Regularizer of sequence data represents the exception of the frequency of the data’s underlying function. Besides, previous literature shows that neural networks tend to prioritize the learning of low-frequency parts of the target function (Rahaman et al., 2019). We find that Lipschitz Regularizer can be utilized to mitigate this phenomenon by emphasizing high-frequency parts, which allows the network to fit all spectra simultaneously and results in a faster convergence rate. The details of this discussion are in §5.1.\n\n4 TIME DOMAIN\n\nIn this section, we view Lipschitz Regularizer in the time domain, and show how it can be used to make the sequence more discrete or continuous over time, catering to the preference of different models. Generally, to change the continuity of the input sequence to different models with the Lipschitz Regularizer, we apply it on the output of the embedding layer before the sequence is sent to different models. We describe the details of two different models in the following sections.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Accuracy of the S4 model and its variant with our proposed Lipschitz Regularizer (S4 + Emb + Lip) in LRA. S4 + Emb is set to ablate the effect of the extra embedding layer.\n\nListOps\n\nText\n\nRetrieval\n\nImage\n\nImage-c\n\nPath\n\nPath-c\n\nPathX PathX-c\n\nS4 S4 + Emb S4 + Emb + Lip\n\n59.53 58.94 61.37\n\n86.51 87.12 89.74\n\n91.07 90.28 93.83\n\n88.54 87.25 89.19\n\n84.27 85.13 88.43\n\n94.02 92.37 93.52\n\n89.11 90.32 91.39\n\n96.03 93.87 95.72\n\n92.41 92.81 94.36\n\n4.1 STATE SPACE MODEL\n\nThe State Space Model is a classic model in control engineering. Gu et al. (2021) extended it to the deep sequence model, and proposed the S4 model. S4 is a continuous-time sequence model. It advances SoTA on long-range sequence modeling tasks by a large margin. An S4 layer can be denoted as follows:\n\n ̇x = Ax + Bu\n\ny = Cx + Du,\n\n(5)\n\nwhere u is the input function, x is the hidden state , y is the output. A, B, C, D are trainable matrices.\n\nThe critical and essential design in the S4 layer is the transition matrix A, which is initialized with the HiPPO matrix. The HiPPO matrix makes the S4 layer optimally remember the history of the input’s underlying function, so the S4 model can substantially outperform previous methods on long-range sequence modeling tasks. Particularly, the HiPPO matrix is designed to find the best polynomial approximation of the input’s underlying function given a measure that defines the optimal history, and a memory budget which is the hidden dimension in the model. Each measure corresponds to an optimal HiPPO matrix.\n\nTheoretical Analyses To connect the continuity property with the S4 model, we provide the following intuition here while a formal proposition along with its proof in Appendix G.2. Generally, the error rate of HiPPO-LegS projection decreases when the sequence is more continuous/smooth (Gu et al., 2021). Here, LegS denotes the scaled Legendre measure, which assigns uniform weights to all history. This is also true for S4 layers, since the HiPPO matrix is the most critical design in the S4 layer. However, in many tasks, such as natural language processing, the input sequence are not particularly smooth. This will deteriorate the performance of the S4 model.\n\nLipschitz Regularizer can be used to solve the above problem, because it can adjust the continuity of sequences. Specifically, since we cannot directly manipulate the underlying function of the input sequence, we add a 1D convolutional layer that does not change the sequence length as an embedding layer before the S4 layer, and then apply Lipschitz Regularizer to the output of the embedding layer as follows:\n\nL(y, ˆy, ˆl) = LS4(y, ˆy) + λLLip(ˆl), (6) where y is the ground-truth, and ˆy is the output of the S4 model. ˆl is the output of the embedding layer, and LS4 is the original loss of the S4 model. λ is a hyperparameter to control the magnitude of the Lipschitz Regularizer. By using Equation (6) as the loss function, the input of the S4 layers becomes more continuous, so the error of the HiPPO-LegS projection and S4 layer can be reduced, leading to better model performance.\n\nExperiments To demonstrate the effectiveness of the Lipschitz Regularizer, we use a modified version of the Long Range Arena (LRA) (Tay et al., 2020) benchmark with harder tasks. The descriptions of the original LRA are in Appendix . In addition to the original LRA, we create 3 harder tasks with more discrete sequences. Particularly, we notice that among these 6 tasks, 3 of them use pixels as inputs (i.e., Image and Pathfinder), which could be more continuous than texts in the other 3 tasks. So we design Image-c, Path-c, and PathX-c, in which the contrast of images is increased, and the increasing degree is randomly sampled from 50% to 100% for each image.\n\nWe test three methods on the modified LRA. The first one is the original S4 model (denoted as S4). The second one is the S4 model with a 1D convolutional layer as the embedding layer, and Lipschitz Regularizer is applied to the outputs of the embedding layer (denoted as S4 + Emb +\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nLip). Furthermore, we also design the third model to ablate the effect of the extra embedding layer. Here, we use the S4 model with the same embedding layer as the previous method, and Lipschitz Regularizer is not applied (denoted as S4 + Emb). Hyperparameter λ is chosen from {1, 2, 3, 4, 5} when the model performs best on the validation set.\n\nResults are listed in Table 1, and we have the following observations. (1) It is obvious that our method (i.e., S4 + Emb + Lip) significantly outperforms other methods in almost all tasks, especially in tasks with discrete inputs, such as Text and Retrieval. Improved performance in Image-c, Pathc, and PathX-c shows that Lipschitz Regularizer can mitigate the influence of increased contrasts. These results well demonstrate the effectiveness of the Lipschitz Regularizer, indicating that it can make input sequences of the S4 layer more continuous, and better cater to the preference of the S4 model. (2) Comparing the results of S4 on Image/Path(X) and Image-c/Path(X)-c, it can be observed that the performance of the S4 model degenerates with the increasing contrasts of images. The cause is the deceased continuity against the preference of the S4 model, verifying that the S4 model indeed prefers continuous inputs. (3) Only adding the extra embedding layer (S4 + Emb) makes the accuracy decrease in 4 out of 7 tasks, indicating that improvements come from the effect of the Lipschitz Regularizer, but not the extra layer. Besides, this extra embedding layer is also the main reason causing the performance drop in Path and PathX dataset. In Appendix B.2, the visualization for the output vector of the embedding layer shows that this embedding layer may overly and incorrectly blur or even erase some informative shapes in the original picture, causing some necessary information lost, and the model confused.\n\n4.2 TRANSFORMER-BASED MODELS\n\nIn this section, we show that Lipschitz Regularizer can improve the performance of Transformerbased models when inputs are continuous. In particular, we choose time-series forecasting tasks whose inputs are highly continuous, and we use three Transformer-based models, i.e. vanilla Transformer (Vaswani et al., 2017), Informer (Zhou et al., 2021) and Autoformer (Wu et al., 2021) to evaluate the effectiveness of Lipschitz Regularizer. Although these models already have a good performance on time-series forecasting tasks, due to the preference of Transformer-based models for discrete sequences (shown in Figure 1) and highly continuous inputs, we can still apply Lipschitz Regularizer to further improve the model by decreasing the continuity of input sequences. Specifically, since all three models have an embedding layer, we directly apply Lipschitz Regularizer to the output of the embedding layer as follows:\n\nL(y, ˆy, ˆl) = LTransformer(y, ˆy) − λLLip(ˆl), where y is the ground-truth, and ˆy is the output of the respective model. ˆl is the output of the embedding layer, and LTransformer is the original loss of the Transformer-based model. λ controls the magnitude of the Lipschitz Regularizer. Note that different from the usage in the S4 model, here we subtract Lipschitz Regularizer to make the input discrete, and cater to the model preference.\n\n(7)\n\nExperiments We use 5 datasets in this experiment and their descriptions are in Appendix C.1. Evaluation metrics are Mean Square Error (MSE) and Mean Absolute Error (MAE). Hyperparameter λ is chosen from {1, 2, 3, 4, 5, 6, 7, 8} when the model performs best on the validation set.\n\nResults of Transformer, Informer, Autoformer, and these models with Lipschitz Regularizer (denoted as Transformer + Lip, Informer + Lip and Autoformer + Lip) are shown in Table 2, and results of multivariate experiments are in Appendix C.2. We can see that the models with Lipschitz Regularizer generally outperform the original models on most of the tasks. This indicates that Transformer-based models prefer discrete sequences and reducing input continuity with Lipschitz Regularizer can be helpful for them. We also note that the Lipschitz Regularizer is more effective on vanilla Transformer than the models with specialized designs for time series forecasting. This indicates the vanilla Transformer is more sensitive to data continuity, and special designs in Informer and Autoformer may mitigate it.\n\nBesides, to investigate whether Lipschitz Regularizer changes the data continuity, we also show curves tracking the Lipschitz constant of the output of the embedding layer during training in Figure 2. Curves show that with the Regularizer, the Lipschitz constant becomes larger than it is in the original model, and continuously increases during training. Results demonstrate that Lipschitz Regularizer can indeed change the continuity, and thus improve the model.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Univariate time-series forecasting results of 3 Transformer-based models and training them with Lipschitz Regularizer (indicated by + Lip). Note in this table, prediction window sizes are converted to lengths of sequences used in the model.\n\nMethods\n\nTransformer\n\nTransformer + Lip\n\nInformer\n\nInformer + Lip\n\nAutoformer\n\nAutoformer + Lip\n\nMetric\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n1 h\nT T\nE\n\n2 h\nT T\nE\n\n1\n\nm T\nT E\n\nr e\nh\n\nt a\ne\n\nW\n\nL C\nE\n\n24 48 168 336 720\n\n24 48 168 336 720\n\n24 48 96 288 672\n\n24 48 168 336 720\n\n48 168 336 720 960\n\n0.07047 0.18902 0.39773 0.41523 0.65586\n\n0.09449 0.15016 0.25197 0.22258 0.21932\n\n0.01279 0.08974 0.05341 0.22354 0.40726\n\n0.00223 0.00422 0.00537 0.00524 0.00933\n\n0.26161 0.32283 0.47213 0.48477 0.46930\n\n0.20586 0.37046 0.55569 0.56902 0.75324\n\n0.24259 0.30996 0.41087 0.38170 0.38844\n\n0.08410 0.25869 0.17696 0.40455 0.55824\n\n0.03468 0.04106 0.05975 0.05772 0.07630\n\n0.37762 0.41766 0.50381 0.52413 0.51409\n\n0.07019 0.16716 0.30811 0.41324 0.62233\n\n0.07560 0.13229 0.21046 0.20867 0.18445\n\n0.01210 0.02872 0.05182 0.13780 0.40726\n\n0.00154 0.00292 0.00319 0.00417 0.00272\n\n0.24277 0.29221 0.37916 0.46524 0.43643\n\n0.20570 0.34974 0.48183 0.56402 0.73160\n\n0.20989 0.29278 0.37453 0.37298 0.35793\n\n0.08312 0.12820 0.15017 0.29825 0.55826\n\n0.02497 0.03026 0.04464 0.03673 0.03823\n\n0.36460 0.39677 0.45903 0.51649 0.49698\n\n0.09842 0.15845 0.18314 0.22164 0.26883\n\n0.09309 0.15483 0.23193 0.26321 0.27722\n\n0.03016 0.06944 0.19414 0.40140 0.51164\n\n0.11676 0.17822 0.26585 0.29713 0.35875\n\n0.23894 0.44680 0.48892 0.54026 0.58225\n\n0.24747 0.31907 0.34619 0.38720 0.43506\n\n0.24015 0.31445 0.38947 0.41659 0.43063\n\n0.13717 0.20255 0.37236 0.55355 0.64390\n\n0.25142 0.31846 0.39764 0.41571 0.46647\n\n0.35891 0.50315 0.52840 0.57059 0.60782\n\n0.08882 0.12615 0.10579 0.11810 0.13131\n\n0.08626 0.13684 0.30071 0.24875 0.23646\n\n0.01815 0.05848 0.13336 0.30266 0.27543\n\n0.11256 0.19134 0.25138 0.24748 0.26479\n\n0.17306 0.18765 0.37758 0.44496 0.37740\n\n0.23674 0.28333 0.25552 0.26959 0.28731\n\n0.22559 0.28936 0.43671 0.40827 0.39648\n\n0.09147 0.19686 0.30091 0.46864 0.45377\n\n0.23844 0.32408 0.37400 0.37725 0.39214\n\n0.30488 0.31497 0.43689 0.50387 0.45817\n\n0.05567 0.07860 0.09232 0.10462 0.12069\n\n0.11136 0.15137 0.20403 0.22188 0.25612\n\n0.02317 0.04130 0.05432 0.11893 0.09156\n\n0.00740 0.01002 0.01038 0.00729 0.00960\n\n0.57845 0.52339 0.53511 0.80028 0.76603\n\n0.18596 0.22324 0.24037 0.25484 0.27791\n\n0.26315 0.30316 0.35646 0.37417 0.40089\n\n0.11778 0.15783 0.18033 0.27181 0.23690\n\n0.06422 0.07648 0.07082 0.06492 0.08758\n\n0.55653 0.53251 0.54372 0.66756 0.65389\n\n0.05504 0.07422 0.08983 0.10461 0.12394\n\n0.09345 0.14945 0.18370 0.21195 0.25604\n\n0.02300 0.03931 0.05258 0.07521 0.09280\n\n0.00736 0.00978 0.00528 0.00566 0.00925\n\n0.52893 0.42961 0.51762 0.91144 0.64859\n\n0.18495 0.21398 0.23544 0.25483 0.27833\n\n0.25515 0.30129 0.33714 0.36425 0.40085\n\n0.10107 0.15601 0.17605 0.21728 0.23621\n\n0.06329 0.07727 0.05638 0.05888 0.07136\n\n0.54479 0.48926 0.52391 0.69692 0.62054\n\nCount\n\n2\n\n49\n\n4\n\n46\n\n6\n\n44\n\nFigure 2: The Lipschitz constant of the output of the embedding layer during the training process of Informer + Lip. The experiment is the univariate ETTh2 with the prediction window size of 24h.\n\nFigure 3: MSE of Informer + Lip on univariate EETh2 with different λ. Colors represent different window sizes.\n\nWe also show an ablation study for the hyperparameter λ in Figure 3. We can observe that (1) MSE increases when λ < 0, while decreases when λ > 0. Since positive λ reduces the data continuity, we can conclude that Informer prefers discrete sequences, and Lipschitz Regularizer can reduce the continuity and cater to the preference; (2) MSEs do not have a large variance for different positive λ, indicating that the performance improvement is not sensitive to hyperparameter changes.\n\n5 FREQUENCY DOMAIN\n\nIn this section, we study how continuity affects the performance of deep models from the frequency perspective. We take the ReLU network as the study case, and provide theoretical analyses and experiment results to show the effectiveness of applying the Lipschitz Regularizer on ReLU networks.\n\n7\n\n012345Epoch0.100.150.200.250.300.350.400.45Lipschitz ConstantLipschitz RegularizerOriginal420240.100.150.200.250.300.35MSE2448168336720Published as a conference paper at ICLR 2023\n\n5.1 RELU NETWORK\n\nA ReLU network g : Rd → R with L hidden layers of width d1, . . . , dL is defined as:\n\ng(x) =\n\n(cid:16)\n\nT (L+1) ◦ σ ◦ T (L) ◦ · · · ◦ σ ◦ T (1)(cid:17)\n\n(x),\n\n(8)\n\nwhere T (k) : Rdk−1 → Rdk is an affine function (d0 = d, dL+1 = 1) and σ is the ReLU function.\n\nTheoretical Analyses In the previous literature, Rahaman et al. (2019) showed that the lowfrequency part of the sequence data is learned faster by the ReLU network, and such phenomenon is called the “spectral bias”. We claim that the Lipschitz Regularizer could help mitigate the spectral bias. Intuitively, when the Lipschitz constant of the ReLU network increases, we expect that the model can learn more information in high-frequency parts. We provide a formal proposition and its proof on this intuition in Appendix G.3. This inspires us to balance frequency modes via changing the Lipschitz continuity of functions. Besides, suppose we use a ReLU network to learn a sequenceto-sequence mapping, where values of data in the input sequence (length n) increase linearly in the interval (0, 1) with step size 1 n , and the output is generated by the mapping function h(t). Note that since values of data in the input sequence increase linearly, the Lipschitz constant of the ReLU network is the same as the output sequence. Therefore, we design the decayed Lipschitz Regularizer as follows:\n\nL(y, ˆy) = LMSE(y, ˆy) − λe−εtLLip(ˆy), (9) where y is the ground-truth generated by h(t) and ˆy is the prediction. LMSE is the MSE Loss. λ and ε are hyperparameters that control the magnitude and decay rate of the Lipschitz Regularizer, respectively.\n\nWe further explain the reasons why this regularizer can mitigate spectral bias in two aspects. First, by Equation (4), the added term could be seen as a direct penalty to the low-frequency part of the output sequence. Since the value of data in the input sequence increases linearly, this is equivalent to penalizing the low-frequency part of the ReLU network, and prioritizing the learning of the highfrequency part.\n\nIn another perspective, Rahaman et al. (2019) claimed that the origin of the spectral bias is the gradually increasing parameter norm, and Lipschitz Regularizer can intentionally relieve it. Specifically, Fourier components of the ReLU network ˆgθ(ξ) is bounded by O(Lg), and Lg is bounded by the parameter norm, which can only increase by a small step during the optimization step. Hence, gradually increasing parameter norms can hinder the learning of high-frequency parts at the early optimization stage. Besides, due to the fact that Lipschitz Regularizer can intentionally change Lg, subtracting Lipschitz Regularizer as Equation (9) can enlarge the parameter norm, making it possible for optimizing both high and low-frequency parts. This can be seen as a warm-up process for the network where the parameter norm increases at the beginning of optimization, and then the convergence can be significantly accelerated, since modes of all frequencies can be learned simultaneously after the warm-up.\n\nExperiments We choose a mapping task to evaluate the proposed Lipschitz Regularizer. Specifically, we try to learn the mapping function whose input is the sequence with linearly increasing values, and output is a highly periodic sequence. Given frequencies K = {k1, k2, . . . , kn}, amplitudes A = {a1, a2, . . . , an}, and phases Φ = {φ1, φ2, . . . , φn}, the mapping function is defined as h(x) = (cid:80)n\n\ni=1 Ai sin(2πkix + φi).\n\nIn this experiment, we take n = 10 and frequencies K = {5, 10, . . . , 45, 50}, amplitudes A = {0.1, 0.2, . . . , 1}. The phases are uniformly sampled from 0 to 2π, i.e., φi ∼ U (0, 2π). The input samples in the sequence are uniformly placed over (0, 1) with the number of samples N = 100, and the output is generated by h(x).\n\nAs for the model, we use a 6-layer deep ReLU network with the hidden dimension set to 256 for all layers. To verify the effectiveness of the proposed Lipschitz Regularizer, we train two identical networks with the same training procedure. One is trained with the decayed Lipschitz Regularizer and the other without it. We set hyperparameter λ ∈ {1, 2, 3, 4, 5} and ε ∈ {0.00001, 0.0001, 0.001, 0.01, 0.1} when the model performs best on the validation set.\n\nWe show the frequency and MSE of ReLU networks during the training process in Figure 4. From Figure 4 (a) and (b), we notice that low-frequency parts are learned first in both networks, but\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) Frequency of a ReLU network\n\n(b) Frequency of an identical ReLU network with Lipschitz Regularizer\n\n(c) MSE of the ReLU network w and w/o Lipschitz Regularizer\n\nFigure 4: Evolution of the frequency and MSE of ReLU networks during the training process. In (a) and (b), color indicates the normalized amplitude of the Fourier component at the corresponding frequency, i.e., | ˆgθ(ki)|/Ai. Lipschitz Regularizer enables faster learning of high frequencies and faster convergence.\n\nFigure 5: Predictions of ReLU networks (first row: without Lipschitz Regularizer; second row: with Lipschitz Regularizer) during the training process. With Lipschitz Regularizer, high-frequency parts can be learned faster.\n\nwith the decayed Lipschitz Regularizer, high frequencies can be learned significantly faster. Figure 4 (c) shows that Lipschitz Regularizer can accelerate convergence. We also show predictions of two models during the training process in Figure 5, which gives a more intuitive result, indicating that high frequencies can be learned faster when we use decayed Lipschitz Regularizer to warmup optimization. All results demonstrate that Lipschitz Regularizer enables almost simultaneous learning for all frequencies, so spectral bias can be relieved in this way, and the convergence is accelerated.\n\n6 SUMMARY\n\nWe investigate a generic property of sequence data, i.e., continuity, which is closely related to the performance of different models, and propose Lipschitz Regularizer to flexibly adjust the continuity for various models. We first empirically observe that the different deep models prefer different data continuity. Then, from both time and frequency domains, we provide in-depth theoretical and experimental studies for specific models. For the time domain, we show that the continuoustime model S4 prefers continuous sequences, while the Transformer-based model Informer prefers discrete inputs. We use Lipschitz Regularizer to adjust the data continuity for both of them and largely improve their performance by catering to their preference. For the frequency domain, we show that Lipschitz Regularizer can help mitigate the spectral bias, and accelerate convergence for ReLU networks. In general, Lipschitz Regularizer is available for any sequence modeling tasks and models which have a preference for data continuity, and can accordingly facilitate learning for various models with very little computational cost.\n\n9\n\n5101520253035404550Frequency [Hz]50000400003000020000100000Training Iteration0.00.20.40.60.81.05101520253035404550Frequency [Hz]50000400003000020000100000Training Iteration0.00.20.40.60.81.00100002000030000400005000060000Training Iteration0.000.250.500.751.001.251.501.752.00MSEReLU Networkwith Lipschitz Regularizer0.00.20.40.60.81.02101234Iteration 1000.00.20.40.60.81.02101234Iteration 50000.00.20.40.60.81.02101234Iteration 100000.00.20.40.60.81.02101234Iteration 600000.00.20.40.60.81.021012340.00.20.40.60.81.021012340.00.20.40.60.81.021012340.00.20.40.60.81.02101234Published as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors would like to thank Yifei Shen and Yansen Wang for their helpful discussions and insights. The authors also want to thank our reviewers for providing all the valuable feedback and suggestions.\n\nREFERENCES\n\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\n\nand recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\n\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\n\ndifferential equations. Advances in neural information processing systems, 31, 2018.\n\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.\n\nGeorge Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, pp. 2456–2466. PMLR, 2021.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\n\nN Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2020.\n\nFernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks.\n\nIEEE Transactions on Signal Processing, 68:5680–5695, 2020.\n\nHenry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural net-\n\nworks by enforcing lipschitz continuity. Machine Learning, 110(2):393–416, 2021.\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R ́e. Hippo: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33: 1474–1487, 2020.\n\nAlbert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured\n\nstate spaces. In International Conference on Learning Representations, 2021.\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017.\n\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171– 4186, 2019.\n\nMakerere AI Lab. Bean disease dataset, January 2020. URL https://github.com/\n\nAI-Lab-Makerere/ibean/.\n\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural information processing systems, 32, 2019.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLaurent Meunier, Blaise J Delattre, Alexandre Araujo, and Alexandre Allauzen. A dynamical system perspective for lipschitz neural networks. In International Conference on Machine Learning, pp. 15484–15500. PMLR, 2022.\n\nTomas Mikolov, Martin Karafi ́at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pp. 1045–1048. Makuhari, 2010.\n\nNasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference on Machine Learning, pp. 5301–5310. PMLR, 2019.\n\nJure Sokoli ́c, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep\n\nneural networks. IEEE Transactions on Signal Processing, 65(16):4265–4280, 2017.\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nPete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv\n\npreprint arXiv:1804.03209, 2018.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ́emi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art In Proceedings of the 2020 conference on empirical methods in natural language processing. natural language processing: system demonstrations, pp. 38–45, 2020.\n\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419–22430, 2021.\n\nBohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. Towards certifying l-infinity robustness using neural networks with l-inf-dist neurons. In International Conference on Machine Learning, pp. 12368–12379. PMLR, 2021.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11106–11115, 2021.\n\nDongmian Zou, Radu Balan, and Maneesh Singh. On lipschitz bounds of general convolutional\n\nneural networks. IEEE Transactions on Information Theory, 66(3):1738–1759, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nTABLE OF CONTENTS\n\nA\n\nDETAILS OF EXPERIMENT IN THE INTRODUCTION .\n\nA.1 UNIVARIATE .\n\n.\n\nA.2 MULTIVARIATE .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nB\n\nDETAILS OF EXPERIMENT FOR THE S4 MODEL .\n\nB.1 DATASET DESCRIPTION .\n\n.\n\n.\n\n.\n\n.\n\n.\n\nB.2 ANALYSIS FOR THE PATHFINDER TASK .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nC\n\nDETAILS OF EXPERIMENT FOR THE TRANSFORMER-BASED MODEL .\n\nC.1 DATASET DESCRIPTION .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nC.2 RESULTS OF MULTIVARIATE TIME-SERIES FORECASTING .\n\nC.3 CASES IN TIME-SERIES FORECASTING TASKS\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nFINE-TUNE SWIN TRANSFORMER WITH LIPSCHITZ REGULARIZER .\n\nD\n\nE\n\nF\n\nAPPLY LIPSCHITZ REGULARIZER TO THE SPEECH CLASSIFICATION TASK .\n\nNEURAL ODE WITH LIPSCHITZ REGULARIZER .\n\nG MATHEMATICAL DERIVATIONS\n\n.\n\n.\n\nG.1 DERIVATION OF EQUATION (4) .\n\n.\n\n.\n\nG.2 CONTINUITY AND THE S4 MODEL.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nG.3 CONTINUITY AND THE RELU NETWORK .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n12\n\n12\n\n12\n\n15\n\n15\n\n15\n\n16\n\n16\n\n16\n\n16\n\n19\n\n20\n\n20\n\n21\n\n21\n\n21\n\n22\n\nA DETAILS OF EXPERIMENT IN THE INTRODUCTION\n\nA.1 UNIVARIATE\n\nWe present a sequence-to-sequence task in the Introduction section, and show more details here. In this experiment, we generate two types of input sequences with different continuity (each has 1000 samples), and map them to output with the exponential moving average h(t):\n\nh(t) =\n\n2 1 + w\n\n(cid:18)\n\nxt +\n\n1 −\n\n(cid:19)\n\n2 1 + w\n\nh(t − 1)\n\nwhere x1, x2, . . . , xN is the input sequence, w is the window size (set to 50). We choose the exponential moving average because it is a sequence-to-sequence mapping that makes use of the contextual information. The Lipschitz constant of the input and output sequence is shown in Table 3. Note that the high Lipschitz constant represents low continuity, while the low Lipschitz constant represents high continuity. Then, we train the S4 model and the Transformer model with generated input and output sequences. Each model has a 1D convention embedding layer with kernel size 5, stride 1, and padding 2. Both Transformer and S4 have 1 separated layer with the hidden dimension set to 16. We also apply Lipschitz Regularizer to the output of the embedding layer and train models again. MSE of these 4 models is shown in Table 3. We could observe that S4 performs better with continuous inputs and the Transformer is better with discrete inputs. Also, Lipschitz Regularizer can improve the performance of S4 and Transformer by changing the data continuity into their prefers ones.\n\nA.2 MULTIVARIATE\n\nWe repeat the above experiment with multivariate data. Specifically, we also generate high and low continuity input sequences with dimension 16 (each has 1000 samples). The input sequences are\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Results of the experiment in the Introduction.\n\nLipschitz Constant\n\nInput Output\n\nMSE\n\nS4 S4 + Lip Transformer Transformer + Lip\n\nHigh Continuity Low Continuity\n\n0.0543 0.0107\n\n0.00014 -\n0.00003 0.00038\n\n0.2706 0.0142\n\n0.00567 0.00045 0.00222 -\n\nmapped to output with exponential moving average. Same models are trained on the generated data. Results are shown in Table 4. We also randomly sample four dimensions and corresponding curves are shown in Figure 6, 7, 8, 9. Our findings and conclusions in the univariate experiment also hold in the multivariate case.\n\nTable 4: Results of the experiment in the Introduction running with multivariate data.\n\nLipschitz Constant\n\nAverage MSE over all variates\n\nHigh Continuity Low Continuity\n\nInput Output\n\nS4 S4 + Lip Transformer Transformer + Lip\n\n0.0708 0.0138\n\n0.00072 -\n0.00014 0.00418\n\n0.3592 0.0171\n\n0.00963 0.00141 0.01368 -\n\nFigure 6: Results of the S4 model with high continuity multivariate data for the experiment in the Introduction.\n\n13\n\n0.20.30.40.50.60.7Value0255075100125150175200Index0.20.30.40.50.60.7Value0255075100125150175200IndexGround TruthS4Published as a conference paper at ICLR 2023\n\nFigure 7: Results of the Transformer model with high continuity multivariate data for the experiment in the Introduction.\n\nFigure 8: Results of the S4 model with low continuity multivariate data for the experiment in the Introduction.\n\nFigure 9: Results of the Transformer model with low continuity multivariate data for the experiment in the Introduction.\n\n14\n\n0.20.30.40.50.60.7Value0255075100125150175200Index0.20.30.40.50.60.7Value0255075100125150175200IndexGround TruthTransformerwith Lip Regularizer0.300.350.400.450.500.550.600.650.70Value0255075100125150175200Index0.300.350.400.450.500.550.600.650.70Value0255075100125150175200IndexGround TruthS4with Lip Regularizer0.400.450.500.550.600.650.70Value0255075100125150175200Index0.400.450.500.550.600.650.70Value0255075100125150175200IndexGround TruthTransformerPublished as a conference paper at ICLR 2023\n\nB DETAILS OF EXPERIMENT FOR THE S4 MODEL\n\nB.1 DATASET DESCRIPTION\n\nIn this experiment, we test the Lipschitz Regularizer on the S4 model. The testing dataset is the Long Range Arena (LRA) benchmark. Specifically, LRA contains various long sequence modeling tasks with sequence lengths ranging from 1K to 16K, which is very challenging for deep models. Tasks in LRA include (1) ListOps, in which the model needs to calculate the answer with a list of algebraic operations, (2) Text, a binary classification task with byte-level texts from IMDB reviews, (3) Retrieval, a document retrieval task with byte-level texts and documents from ACL Anthology Network, (4) Image, an image classification task with the image in CIFAR-10 dataset flattened into the pixel sequence, (5) Path (referred as Pathfinder in Gu et al. (2021)), in which the model needs to deduce whether two points in the image are connected by dashed lines, and the image is also flattened into the pixel sequence, (6) PathX, a harder version of Path, where the dimension of input images increased from 32 × 32 to 128 × 128.\n\nB.2 ANALYSIS FOR THE PATHFINDER TASK\n\nWe notice that the Lipschitz Regularizer causes deteriorated performance on the Pathfinder dataset, so here, we provide detailed analyzes to explore the reason. We show a case in the experiment of the S4 model on the Path dataset in Figure 10. We can see that the performance drop is mainly caused by the embedding layer. As explained in § 4.1, since we cannot directly manipulate the underlying function of the input sequence, we add an extra embedding layer before the S4 layer. However, changes from figure a1 to a2 show that this embedding layer may overly and incorrectly blur or even erase some informative shapes in the original picture, causing some necessary information lost, and making the model confused. Although Lipschitz Regularizer can slightly relieve this issue, the necessary path information is not as obvious as it is in the original image. The performance of these 3 models (i.e., S4, S4 + Emb, S4 + Emb + Lip) in Table 1 also matches this finding. Moreover, Figure b1, b2, and b3 show that when the contrast is increased, these shapes are not likely to be erased since their pixels all have high gray values. Hence, Lipschitz Regularizer can improve model performance on the Path-c task.\n\nFigure 10: A case in the experiment of the S4 model on the Path dataset. In this task, the model needs to deduce whether two points in the image are connected by a dashed line. (a1) An image randomly sampled from the Path dataset. (b1) The image in (a1) with 100% contrast increased. (a2, b2) Average of the output vector of the embedding layer in a trained S4 + Emb model, with a1 and b1 as the input, respectively. (a3, b3) Average of the output vector of the embedding layer in a trained S4 + Emb + Lip model, with a1 and b1 as the input, respectively.\n\n15\n\n(a1)(a2)(a3)(b1)(b2)(b3)Published as a conference paper at ICLR 2023\n\nC DETAILS OF EXPERIMENT FOR THE TRANSFORMER-BASED MODEL\n\nC.1 DATASET DESCRIPTION\n\nIn this experiment, we show that the Lipschitz Regularizer can improve the performance of Transformer-based models, including Transformer, Informer, and Autoformer. We use 3 realworld datasets: ETT (Electricity Transformer Temperature), ECL (Electricity Consuming Load), and Weather. ETT has 3 separate datasets, i.e., {ETTh1, ETTh2} for 1-hour-level with 2 separated countries, and ETTm1 for 15-minute-level. And we use multiple prediction window sizes, including {24h, 48h, 168h, 336h, 720h, 960h} for ETTh, ECL, Weather, and {6h, 12h, 24h, 72h, 168h} for ETTm.\n\nC.2 RESULTS OF MULTIVARIATE TIME-SERIES FORECASTING\n\nResults of multivariate time-series forecasting of three Transformer-based models are presented in Table 5. We can see that Lipschitz Regularizer can improve the model performance in most cases, showing that altering data continuity is also helpful in multivariate time-series forecasting tasks. Besides, we can also observe that the improvement by Lipschitz Regularizer is slightly less significant than which in univariate time-series forecasting. The reason may be that we apply the same regularizer to input sequences of all variates. However, these input sequences may have different continuities and need regularizers with different weights. Future work might be adding trainable weights to the regularizer of different input sequences.\n\nTable 5: Multivariate time-series forecasting results of 3 Transformer-based models and training them with Lipschitz Regularizer (indicated by + Lip). Note in this table, prediction window sizes are converted to lengths of sequences used in the model.\n\nMethods\n\nTransformer\n\nTransformer + Lip\n\nInformer\n\nInformer + Lip\n\nAutoformer\n\nAutoformer + Lip\n\nMetric\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n1 h\nT T\nE\n\n2 h\nT T\nE\n\n1\n\nm T\nT E\n\nr e\nh\n\nt a\ne\n\nW\n\nL C\nE\n\n24 48 168 336 720\n\n24 48 168 336 720\n\n24 48 96 288 672\n\n24 48 168 336 720\n\n48 168 336 720 960\n\n0.59446 0.82961 1.05050 1.40297 1.05294\n\n0.83025 1.28728 5.66520 5.11314 3.00465\n\n0.28475 0.45711 0.68831 0.88883 1.17478\n\n0.14897 0.26360 0.49273 0.66636 0.89504\n\n0.24832 0.26446 0.27420 0.28641 0.32819\n\n0.57293 0.70984 0.83959 0.99042 0.80128\n\n0.71355 0.92881 1.91462 1.81923 1.44033\n\n0.35112 0.45664 0.60444 0.71069 0.83787\n\n0.23293 0.33987 0.48432 0.58604 0.69045\n\n0.35046 0.36343 0.37078 0.37610 0.39887\n\n0.57268 0.71223 1.04581 1.04308 1.04062\n\n0.31699 0.65413 3.56351 2.95696 2.52934\n\n0.28900 0.43928 0.53434 0.83600 0.92590\n\n0.21319 0.22625 0.42120 0.60194 0.50826\n\n0.23187 0.23862 0.25691 0.27986 0.30984\n\n0.56055 0.65060 0.84110 0.80437 0.80127\n\n0.42514 0.64771 1.45387 1.37183 1.26787\n\n0.34211 0.44483 0.51490 0.66682 0.73367\n\n0.28302 0.31913 0.41574 0.57205 0.52560\n\n0.33168 0.33770 0.36375 0.36451 0.38930\n\n0.57727 0.68461 0.93119 1.12811 1.21454\n\n0.72025 1.45708 3.48945 2.72290 3.46729\n\n0.32315 0.49426 0.67758 1.05643 1.19203\n\n0.33501 0.39546 0.60843 0.70204 0.83106\n\n0.34399 0.36820 0.38061 0.40616 0.45993\n\n0.54945 0.62487 0.75159 0.87302 0.89606\n\n0.66539 1.00062 1.51457 1.33987 1.47321\n\n0.36881 0.50311 0.61353 0.78565 0.92626\n\n0.38091 0.45890 0.56714 0.61955 0.73079\n\n0.39289 0.42427 0.43101 0.44335 0.54774\n\n0.53483 0.68292 1.02153 0.99066 1.20265\n\n0.33658 1.81144 3.05333 2.35546 3.72312\n\n0.35075 0.44407 0.47372 1.03285 0.90468\n\n0.32660 0.38138 0.60603 0.67344 0.65801\n\n0.25829 0.37272 0.34641 0.35887 0.44522\n\n0.51756 0.61618 0.79883 0.75376 0.88891\n\n0.43113 1.08344 1.44302 1.28684 1.66671\n\n0.39111 0.45275 0.47680 0.80819 0.72072\n\n0.38280 0.43057 0.56406 0.62094 0.60372\n\n0.35700 0.43410 0.42147 0.43815 0.50400\n\n0.39562 0.41434 0.46037 0.51710 0.47690\n\n0.26384 0.31247 0.46657 0.47924 0.47352\n\n0.35616 0.44404 0.55732 0.58302 0.56261\n\n0.15944 0.21638 0.30901 0.35603 0.42178\n\n0.18320 0.22662 0.22740 0.29826 0.27856\n\n0.43466 0.44104 0.46466 0.49936 0.49172\n\n0.34252 0.37143 0.46249 0.48042 0.48400\n\n0.40320 0.44776 0.49872 0.50759 0.50822\n\n0.24224 0.29847 0.37048 0.39652 0.43347\n\n0.29892 0.33740 0.33667 0.38835 0.37727\n\n0.37109 0.38277 0.46020 0.50278 0.50977\n\n0.25465 0.31090 0.46982 0.49315 0.46775\n\n0.35807 0.43239 0.55357 0.56224 0.56208\n\n0.16573 0.21580 0.31191 0.34620 0.42083\n\n0.18281 0.21555 0.23392 0.36776 0.23146\n\n0.40143 0.41764 0.47120 0.48853 0.50879\n\n0.33327 0.37014 0.46203 0.48750 0.48066\n\n0.40252 0.44631 0.49706 0.50469 0.50788\n\n0.24907 0.29788 0.37294 0.38616 0.43125\n\n0.29758 0.32441 0.34069 0.33395 0.31658\n\nCount\n\n4\n\n46\n\n12\n\n38\n\n13\n\n38\n\nC.3 CASES IN TIME-SERIES FORECASTING TASKS\n\nWe present some cases to provide some deeper analyses for Lipschitz Regularizer. Here, we plot the last dimension of forecasting results for a qualitative comparison. In all figures of this section, the first 96 data points are inputs of the model, and others are forecasting results.\n\nUnivariate Forecasting We show three examples of univariate forecasting in Figure 11, 12, 13. Figure 11 and 12 show how the Lipschitz Regularizer improves the performance of the model, while\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13 shows a negative case. In Figure 13, the main problem is that the model trained with Lipschitz Regularizer captures a larger decrease than the original data when the time is in 100-200.\n\nFigure 11: Univariate forecasting example of Transformer on the Weather dataset with the prediction window size set to 720. Left figure shows the result of the original Transformer (MSE: 0.00933, MAE: 0.07630). Right figure shows the result of the Transformer trained with Lipschitz Regularizer (λ = 1, MSE: 0.00272, MAE: 0.03823).\n\nFigure 12: Univariate forecasting example of Autoformer on the ETTh2 dataset with the prediction window size set to 168. Left figure shows the result of the original Autoformer (MSE: 0.20403, MAE: 0.35646). Right figure shows the result of the Autoformer trained with Lipschitz Regularizer (λ = 5, MSE: 0.18370, MAE: 0.33714).\n\nFigure 13: Univariate forecasting example of Autoformer on the ECL dataset with the prediction window size set to 720. Left figure shows the result of the original Autoformer (MSE: 0.80028, MAE: 0.66756). Right figure shows the result of the Autoformer trained with Lipschitz Regularizer (λ = 5, MSE: 0.91144, MAE: 0.69692).\n\nMultivariate Forecasting We show four examples of multivariate forecasting in Figure 14, 15, 16, 17. Figure 14 and 16 show the cases where the Lipschitz Regularizer improves the performance of the model, while Figure 15 and 17 show two negative cases. For Figure 15, the sudden change in the input is abnormal, which influences the model with Lipschitz Regularizer more than the original\n\n17\n\n01002003004005006007008000.050.000.050.100.15GroundTruthPrediction01002003004005006007008000.000.020.040.060.080.100.120.14GroundTruthPrediction0501001502002502.01.51.00.50.0GroundTruthPrediction0501001502002502.01.51.00.50.0GroundTruthPrediction010020030040050060070080032101234GroundTruthPrediction0100200300400500600700800432101234GroundTruthPredictionPublished as a conference paper at ICLR 2023\n\nmodel. As for Figure 17, both models did not capture the pattern in the input data. This figure shows that Lipschitz Regularizer makes the output slightly more continuous, therefore causing the increased error.\n\nFigure 14: Multivariate forecasting example of Transformer on the ETTm1 dataset with the prediction window size set to 672. Left figure shows the result of the original Transformer (MSE: 1.17478, MAE: 0.83787). Right figure shows the result of the Transformer trained with Lipschitz Regularizer (λ = 1, MSE: 0.92590, MAE: 0.73367).\n\nFigure 15: Multivariate forecasting example of Transformer on the Weather dataset with the prediction window size set to 24. Left figure shows the result of the original Transformer (MSE: 0.14897, MAE: 0.23293). Right figure shows the result of the Transformer trained with Lipschitz Regularizer (λ = 1, MSE: 0.21319, MAE: 0.28302).\n\nFigure 16: Multivariate forecasting example of Autoformer on the ECL dataset with the prediction window size set to 720. Left figure shows the result of the original Autoformer (MSE: 0.28641, MAE: 0.37610). Right figure shows the result of the Autoformer trained with Lipschitz Regularizer (λ = 1, MSE: 0.25691, MAE: 0.36375).\n\n18\n\n01002003004005006007008001.41.21.00.80.60.40.20.00.2GroundTruthPrediction01002003004005006007008001.41.21.00.80.60.40.20.0GroundTruthPrediction0204060801001200.050.000.050.100.150.20GroundTruthPrediction0204060801001200.050.000.050.100.15GroundTruthPrediction010020030040050060070080010123GroundTruthPrediction010020030040050060070080010123GroundTruthPredictionPublished as a conference paper at ICLR 2023\n\nFigure 17: Multivariate forecasting example of Autoformer on the ETTh1 dataset with the prediction window size set to 720. Left figure shows the result of the original Autoformer (MSE: 0.47690, MAE: 0.49172). Right figure shows the result of the Autoformer trained with Lipschitz Regularizer (λ = 1, MSE: 0.50977, MAE: 0.50879).\n\nD FINE-TUNE SWIN TRANSFORMER WITH LIPSCHITZ REGULARIZER\n\nIn this section, we investigate whether the proposed Lipschitz Regularizer can be used to improve large pre-trained models by fine-tuning its embedding layer on down-stream tasks. Here, we use a regular setting that the model is pre-trained on a large image dataset, and then fine-tuned it on the down-stream image classification task. Moreover, considering that Transformer-based models have shown great power in computer vision domains (Dosovitskiy et al., 2020; Liu et al., 2021), and these models process images by splitting them into patches and feeding the model a sequence of patch embeddings, we choose a typical model, i.e., Swin Transformer (Liu et al., 2021), for experiments in this section.\n\nGenerally, because input tokens of the Transformer are local image patches, they tend to be continuous, which might not match the preference of the Transformer model. Therefore, we apply the Lipschitz Regularizer to make them more discrete. Specifically, we use the Lipschitz Regularizer for outputs of the embedding layer, and change the loss as follows:\n\nL(y, ˆy, ˆl) = LSwin(y, ˆy) − λLLip(ˆl),\n\n(10)\n\nwhere y is the ground-truth, and ˆy is the output of the Swin Transformer. ˆl is the output of the embedding layer, and LSwin is the original loss of the Swin Transformer. λ controls the magnitude of the Lipschitz Regularizer.\n\nWe use the pre-trained Swin Transformer on ImageNet2, and fine-tune it on the image classification task with the Beans dataset (Lab, 2020), containing bean leaf images of diseased and healthy leaves. We only fine-tune the embedding layer and the last linear layer, and freeze other parts of the model.\n\nTable 6: Results of fine-tuning Swin Transformer with Lipschitz Regularizer on an image classification task. Test accuracy with different λ is reported.\n\nλ\n\n5\n\n1\n\n0\n\n-1\n\n-5\n\nTest Accuracy\n\n0.9398\n\n0.9549\n\n0.9248\n\n0.9098\n\n0.8947\n\nWe show the validation accuracy during training in Figure 18 and list the testing accuracy in Table 6. By setting λ to zero, we obtain the result of the baseline. We can see that the model performance can be improved by the Lipschitz Regularizer, showing great potential for changing data continuity in the fine-tuning setting and vision tasks. Besides, results also show that setting λ to positive values (i.e., 5 and 1) benefits model performance, while setting it to negative values (i.e., -5 and -1) degenerates the performance. This verifies our intuition that the Transformer-based model prefers more discrete inputs.\n\n2The pre-trained model is acquired at https://huggingface.co/microsoft/swin-base-patch4-window7-224\n\n19\n\n01002003004005006007008002.22.01.81.61.41.21.0GroundTruthPrediction01002003004005006007008002.22.01.81.61.41.21.0GroundTruthPredictionPublished as a conference paper at ICLR 2023\n\nFigure 18: Validation accuracy of fine-tuning Swin Transformer in each epoch with different values of λ.\n\nE APPLY LIPSCHITZ REGULARIZER TO THE SPEECH CLASSIFICATION TASK\n\nIn this section, we show the effectiveness of the Lipschitz Regularizer on the speech classification task by applying it to a Transformer-based model. As we discussed in § 1, the Transformer-based model prefers discrete inputs. However, voice signals are highly continuous since they are sampled from a continuous physical process with a high sample rate. This inspires us to use the Lipschitz Regularizer to make it more discrete and therefore more preferable for Transformer-based models.\n\nSpecifically, following the settings in Gu et al. (2021), we investigate the Performer model (Choromanski et al., 2020) on the Speech Commands (SC) dataset (Warden, 2018). We test the Performer model on two versions of SC. One is MFCC, where the sequence is pre-processed into standard MFCC features (length 161). Another is Raw, which contains unprocessed signals (length 16000). The Lipschitz Regularizer is applied after the embedding layer, and changes the loss as follows:\n\nL(y, ˆy, ˆl) = LPer(y, ˆy) − λLLip(ˆl), (11) where y is the ground-truth, and ˆy is the output of the Performer model. ˆl is the output of the embedding layer, and LPer is the original loss of the Performer model. λ controls the magnitude of the Lipschitz Regularizer.\n\nTable 7: Results of the Performer model and Lipschitz Regularizer on the Speech Commands dataset. Test accuracy for MFCC and Raw speech data is reported.\n\nλ\n\n0\n\n1\n\n3\n\nMFCC 80.63 30.89 Raw\n\n81.13 35.72\n\n83.21 33.86\n\nResults are shown in Table 7. The column with λ = 0 represents the baseline. The performance of the Performer model is increased by the Lipschitz Regularizer, which further verifies our claim that Transformer-based models prefer discrete inputs.\n\nF NEURAL ODE WITH LIPSCHITZ REGULARIZER\n\nIn this section, we apply the Lipschitz Regularizer to the Neural ODE model (Chen et al., 2018) to see the effect of the regularizer on the model. Similar to the state-space model, Neural ODE is also a continuous-time model, which treats the input as samples from a continuous function. We expect that the Neural ODE will perform better when the input is more continuous.\n\nWe adopt the experiment of fitting time series using the latent ODE in its original paper (Chen et al., 2018). Essentially, the neural network is a generative latent function time-series model, predicting\n\n20\n\n020406080100Epochs0.700.750.800.850.900.951.00Val Accuracy=5=1=0=1=5Published as a conference paper at ICLR 2023\n\nthe solution to an ODE, and the input data of this experiment is sampled from a randomly generated ODE with the same generation process as Chen et al. (2018). The network is a variational autoencoder, which consists of an RNN encoder and a Neural ODE decoder. To alter the continuity of the input to the Neural ODE, we directly apply Lipschitz Regularizer to the output of the RNN encoder as follows:\n\nL(y, ˆy, ˆl) = LODE(y, ˆy) − λLLip(ˆl), (12) where y is the ground-truth, and ˆy is the output of the model. ˆl is the output of the RNN encoder, and LODE is the original loss of the Neural ODE. λ controls the magnitude of the Lipschitz Regularizer.\n\nThe MSE during training is shown in Figure 19. We can observe that Neural ODE performs better when data is more continuous. Predictions of 9 independent runs are presented in Figure 20. We can see that the model has better fitting results when we use Lipschitz Regularizer to make inputs more continuous.\n\nFigure 19: The MSE of the Neural ODE model with Lipschitz Regularizer during training.\n\nG MATHEMATICAL DERIVATIONS\n\nG.1 DERIVATION OF EQUATION (4)\n\nn−1 (cid:88)\n\ni=0\n\n(xi+1 − xi)2 =\n\nn−1 (cid:88)\n\n(cid:18) f (ti+1) − f (ti) ti+1 − ti\n\n(cid:19)2\n\n(cid:18) df (t) dt\n\n(cid:19)2\n\ndt\n\ni=0 (cid:90)\n\nR\n\n(cid:90)\n\nR\n\n≈\n\n=\n\n= 4π2\n\n(cid:90)\n\nξ2 ˆf 2(ξ)dξ\n\n(2πiξ)2 ˆf 2(ξ)(−dξ)\n\n(13)\n\nR (cid:90)\n\n= 4π2C\n\nˆf 2(ξ) C\n= 4π2CEp(ξ)[ξ2]\n\nξ2\n\nR\n\ndξ\n\nG.2 CONTINUITY AND THE S4 MODEL\n\nProposition G.1. Suppose f1, f2 : R+ → R are two differentiable functions of input sequences, and their Lipschitz constant are Lf1 and Lf2. The HiPPO matrix with scaled Legendre measure (LegS) is denoted as HiPPO-LegS. Let the error of the HiPPO-LegS projection of f1, f2 at time t be δ1(t), δ2(t), respectively. Let ˆδ1(t) = tLf1, ˆδ2(t) = tLf2. For any time t, suppose Lf1 ≤ Lf2 , we have δ1(t) = O(ˆδ1(t)), δ2(t) = O(ˆδ2(t)), and ˆδ1(t) ≤ ˆδ2(t).\n\n21\n\n2500500075001000012500150001750020000Epochs0100200300400MSE=1=0=1Published as a conference paper at ICLR 2023\n\nProof. By Gu et al. (2020, Proposition 6), the LegS measure, which uniformly weighs all history, has the following property. Suppose the HiPPO-LegS projection for the target function f (t) at time t is p(t) = projt(f ), then the error δf (t) = (cid:13) N ), where Lf is the Lipschitz constant of f (t), and the maximum polynomial degree is N − 1. So, we have δ1(t) = O(ˆδ1(t)), δ2(t) = O(ˆδ2(t)), and ˆδ1(t) ≤ ˆδ2(t). Therefore, the error rate of HiPPOLegS projection decreases with the Lipschitz constant, so with smaller Lipschitz constant, we expect smaller projection error.\n\n(cid:13)f≤t − p(t)(cid:13)\n\n(cid:13) = O(tLf /\n\n√\n\nG.3 CONTINUITY AND THE RELU NETWORK\n\nProposition G.2. Suppose there are two ReLU networks gθ1, gθ2 with identical architecture, and the Lipschitz constant of them are L1, L2, respectively. Let h1(ξ) = L1/∥ξ∥n+1, h2(ξ) = L2/∥ξ∥n+1, where ξ is the frequency, and ˆgθ(ξ) is the Fourier component of gθ. Suppose L1 ≤ L2, we have ˆgθ1(ξ) = O(h1(ξ)), ˆgθ2(ξ) = O(h2(ξ)), and h1(ξ) ≤ h2(ξ).\n\nProof. By Rahaman et al. (2019, Theorem 1), for a ReLU network gθ with parameter θ, its Fourier component is,\n\nˆgθ(ξ) =\n\nd (cid:88)\n\nn=0\n\nGn(θ, ξ) ∥ξ∥n+1\n\n(14)\n\nwhere the numerator Gn(θ, ·) : Rd → C is bounded by O(Lg). So, we have ˆgθ1(ξ) = O(h1(ξ)), ˆgθ2 (ξ) = O(h2(ξ)), and h1(ξ) ≤ h2(ξ). Therefore, with smaller Lipschitz constant, we expect smaller ˆgθ(ξ).\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(a) Apply Lipschitz Regularizer with λ = 1.\n\n(b) The original Neural ODE model (λ = 0).\n\n(c) Apply Lipschitz Regularizer with λ = −1.\n\nFigure 20: Results of generative latent function with Neural ODE and Lipschitz Regularizer. Points are input sequences, and the color of points indicates their time. The blue line is the ground truth and the orange line is the prediction. The gap between points and the orange line indicates training loss, and the gap between the orange line and the blue line indicates the test loss.\n\n23\n\n54321012.01.51.00.50.00.510121012321013210164202432101231.21.00.80.60.40.20.03210101230.50.00.51.01.52.02.5101210120.80.60.40.20.00.24321010.750.500.250.000.250.500.751.000.50.00.51.065432102.01.51.00.50.01.51.00.50.00.51.01.5210123210132101642024210121.21.00.80.60.40.20.00.23210101230.00.51.01.52.02.51.51.00.50.00.51.01.51010.60.40.20.00.24321010.750.500.250.000.250.500.751.000.50.00.51.05432102.01.51.00.50.01.00.50.00.51.01.5101432101321016420242101231.251.000.750.500.250.000.253210101230121.00.50.00.51.01.51.51.00.50.00.51.01.50.60.40.20.00.24321010.750.500.250.000.250.500.750.50.00.51.0",
    "reference": "# Summary Of The Paper\n\nThis paper comprehensively investigates the continuity of sequential data from both theoretical and empirical perspectives. The authors observe that different model structures may prefer data with different continuity. Then, they give a theoretical analysis in both time and frequency domains. To further improve sequence modeling, they propose Lipschitz Regularizer which can flexibly adjusts data continuity based on the model preferences. Experimental results on different model structures and tasks demonstrate the effectiveness of Lipschitz Regularizer for many deep models in sequence modeling.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1) In my view, this paper is both comprehensive and deep. Now, there are many effective model structures such as Transformer and state space models for sequence modeling. But few works try to investigate the reasons why they work well in specific tasks. This paper starts from continuity, which is an intrinsic property of data, and clearly reveals the relationship between task data and model structures with theoretical supports. The authors provide meaningful insights into different tasks and model designs, which may guide the future research of sequence modeling.\n\n2) In addition to analyzing the phenomenon, the authors also propose a simple and effective method by adjusting the data continuity according to model preferences. Empirical results show its superior performance on challenging tasks like LRA, which are convincing for me.\n\n3) This paper is well-organized and easy to follow. I really enjoy reading this paper and learn a lot from it.\n\nWeaknesses:\n\n1) The authors only conduct experiments on LRA and time-series forecasting tasks in the analysis of Section 4. I wonder whether the authors try to use Lipschitz Regularizer in NLP or audio tasks such as language modeling because they are also attractive sequence modeling tasks.\n\n2) Typo: Preformer –> Performer in line 7 of Section 2; Removing ) in line 1 of Section 5.1.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe overall quality of this paper is satisfactory. The comprehensive analysis and proposed method are novel. The reproducibility is high since the authors submit codes.\n\n# Summary Of The Review\n\nIn my view, this paper is of high quality. I will surely recommend acceptance. But if other reviewers raise serious problems in this paper, I will be still open to change my mind.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTEST-TIME ADAPTATION WITH SLOT-CENTRIC MODELS\n\nAnonymous authors Paper under double-blind review\n\nFigure 1: Image and point-cloud instance segmentation with Slot-TTA. Slot-TTA parse completely novel scenes into familiar entities via slow inference, i.e., gradient descent on the reconstruction error of the scene example under consideration. Left: Slot-TTA outperform Mask2Former (Cheng et al., 2021), a SOTA 2D image segmentor, on segmenting novel images by gradient descent on image synthesis of neighboring image views. Right: Slot-TTA outperform a state-of-the-art 3D-DETR detector by 30% in instance segmentation accuracy in out-of-distribution 3D point clouds, when trained on the same training data.\n\nABSTRACT\n\nWe consider the problem of segmenting scenes into constituent objects and their parts. Current supervised visual detectors, though impressive within their training distribution, often fail to segment out-of-distribution scenes into their constituent entities. Recent test-time adaptation methods use auxiliary self-supervised losses to adapt the network parameters to each test example independently and have shown promising results towards generalization outside the training distribution for the task of image classification. In our work, we find evidence that these losses can be insufficient for instance segmentation tasks, without also considering architectural inductive biases. For image segmentation, recent slot-centric generative models break such dependence on supervision by attempting to segment scenes into entities in a self-supervised manner by reconstructing pixels. Drawing upon these two lines of work, we propose Slot-TTA, a semi-supervised instance segmentation model equipped with a slot-centric image or point-cloud rendering component, that is adapted per scene at test time through gradient descent on reconstruction or novel view synthesis objectives. We show that test-time adaptation greatly improves instance segmentation in out-of-distribution scenes. We evaluate Slot-TTA in several 3D and 2D scene instance segmentation benchmarks and show substantial out-of-distribution performance improvements against state-of-the-art supervised feed-forward detectors and self-supervised domain adaptation models.\n\n1\n\n3D-DETROursInputInputMask2FormerOursUnder review as a conference paper at ICLR 2023\n\n1\n\nINTRODUCTION\n\nWhile significant progress has been made in machine scene perception and segmentation within the last decade, object (and part) detectors continue to generalize poorly outside their training distribution (Geirhos et al., 2020; Hendrycks et al., 2021). Consider the unfamiliar entity shown in Figure 1 (last row on the right). We can intuitively reason about meaningful parts that this shape could be broken into. Yet, a state-of-the-art 3D detection transformer (Misra et al., 2021) trained to segment chairs in a supervised manner struggles with this decomposition, even though the entitiy contains familiar (chair) parts. This lack of generalization requires us to build systems that can robustly adapt to such changes in distribution.\n\nTest-time adaptation (TTA) (Ghifary et al., 2016; Sun et al., 2020; Wang et al., 2020) describes a setting where a model adapts to changes in distribution at test-time, at the cost of additional computation. In recent years, a variety of methods based on TTA have been proposed, focusing on few-shot adaptation (Ren et al., 2018) where the network is given access to a few labelled examples, or unsupervised domain adaptation (UDA) (Zhang, 2021) where the network is given access to many unlabelled examples from the new distribution. Of particular relevance is a specific UDA setting where model parameters are adapted independently to each unlabelled example in the test-set. This setting has been previously referred to as single-example UDA, and here we also refer to it as slow inference since it is similar to a human taking more time to parse a difficult example. Existing approaches for this setting typically devise a self-supervised loss that aligns well with the task of image classification and then optimize this loss during test-time adaptation (Sun et al., 2020; Gandelsman et al., 2022; Bartler et al., 2022; Grill et al., 2020). However, despite their success for image classification, these approaches do not provide adequate support for other scene understanding tasks, and in particular scene segmentation, as we showcase in Section 4.1.\n\nOne potentially important aspect to supporting TTA for other scene understanding tasks is the inductive bias of the underlying architecture. In the context of instance segmentation, there has been a lot of recent development in building models that segment scenes into entities in an unsupervised way by optimizing a reconstruction objective (Eslami et al., 2016; Greff et al., 2016; Van Steenkiste et al., 2018; Goyal et al., 2021; Du et al., 2020; Locatello et al., 2020; Zoran et al., 2021). These methods differ in details but share the notion of incorporating a fixed set of entities, also known as slots or object files. Each slot extracts information about a single entity during encoding, and is “synthesized” back to the input domain during decoding. Their ability to distinguish visual objects at a representation level makes them a particularly promising candidate for TTA for instance segmentation tasks.\n\nIn light of the above, we propose Test-time adaptation with slot-centric models (Slot-TTA), a semisupervised slot-centric approach that combines Slot Attention (Locatello et al., 2020) (in the 2D image or point clouds setting) or Object Scene Representation Transformer (Sajjadi et al., 2022a) (in multiview image setting) with a supervised segmentation loss to enable it to leverage instance-level image or point cloud annotations. Slot-TTA is trained jointly to synthesize and segment scenes. At test time, the model adapts without supervision to a single test sample by optimizing the self-supervised objective alone. Different from fully-unsupervised object-centric generative models, Slot-TTA uses annotations at training time to help it develop the notion of what an object is, which lets it scale to more complex visual settings. Different from existing TTA methods, Slot-TTA uses a slot-centric architecture and self-supervised synthesis loss that better aligns with the task of instance segmentation. Different from state-of-the-art detectors, Slot-TTA is equipped with reconstruction feedback that allows it to adapt at test time without supervision, i.e. without using additional annotated data. Indeed, we show that test-time adaptation via image or point cloud synthesis in Slot-TTA enables successfully parsing completely unfamiliar scenes composed of familiar entities (Figure 3).\n\nWe test Slot-TTA’s instance segmentation ability on the following datasets: PartNet (Mo et al., 2019), MultiShapeNet-Hard (Sajjadi et al., 2022b) Multi-Shape and Plating. We evaluate Slot-TTA’s ability to parse out-of-distribution scenes and compare it against state-of-the-art entity-centric generative models (Locatello et al., 2020; Sajjadi et al., 2022a), program synthesis models (Tian et al., 2019), 3D unsupervised part discovery models (Wu et al., 2020) and supervised visual detectors (Cheng et al., 2021; Misra et al., 2021) trained with labeled data to segment objects. We show improvements over all baselines in Slot-TTA ability to segment novel scenes. Additionally, we ablate different design choices of Slot-TTA. We will make our code and datasets publicly available to the community.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nEntity-centric generative models for scene decomposition Entity-centric (or object-centric) models use architectural inductive biases to represent perceptual inputs, such as an observation of a visual scene, in terms of separate object variables, often referred to as slots or object files (Greff et al., 2020; Sabour et al., 2017; Kosiorek et al., 2018; Engelcke et al., 2019; Goyal et al., 2020; Ke et al., 2021; Burgess et al., 2019; Greff et al., 2019; Zablotskaia et al., 2020; Rahaman et al., 2020). Prominent examples of such models include MONet (Burgess et al., 2019), GENESIS (Engelcke et al., 2019), IODINE (Greff et al., 2019), and Slot Attention (SA) (Locatello et al., 2020), which are trained in a fully-unsupervised setting via a simple auto-encoding objective. Object representations and scene decomposition emerge via the inductive bias of the model architecture (and in some cases, additional regularizers). However, without any form of supervision, scene decompositions can be ambiguous, which is particularly challenging for complex real-world scenes or in the presence of complicated textures. In Slot-TTA, we aid the competition mechanism in SA to address this issue by jointly training with a supervised segmentation loss. OSRT (Sajjadi et al., 2022a) is a cross-view geometry-free encoder-decoder method, that segments an image into objects through reconstructing novel viewpoints. OSRT combines SA with SRT (Sajjadi et al., 2022b), a view synthesis model that uses transformer encoder and decoders to fuse information across views, as well as the camera pose, without any explicit 3D information. Our multi-view RGB Slot-TTA builds upon their architecture.\n\nTest-time adaptation In test-time adaptation, model parameters are updated at test-time to better generalize to the distribution shift. In recent years, there has been significant development in this direction. Methods such as pseudo labelling and entropy minimization (Shin et al., 2022; Wang et al., 2020; Bateson et al., 2022) have demonstrated that supervising the model using its own confident predictions could help improve its accuracy. Adaptive BatchNorm methods (Khurana et al., 2021; Chang et al., 2019) have shown that updating the BatchNorm parameters using the new examples can help adaptation. Despite these successes, these methods by definition are data inefficient as they require confident predictions or a batch of examples to adapt. Self-supervised learning (SSL) (Sun et al., 2020; Bartler et al., 2022; Gandelsman et al., 2022) based methods on the other hand, have empirically shown to be data efficient. During training, they jointly train using the task and SSL loss, and during test-time, they train only using the SSL loss. All of the methods in the SSL setting thus far focus on the task of classification and mainly differ in terms of the SSL loss used. For example TTT (Sun et al., 2020) uses rotation angle prediction as their SSL loss, MT3 (Bartler et al., 2022) uses a BYOL (Grill et al., 2020) loss and TTT-MAE (Gandelsman et al., 2022) uses Masked autoencoding loss (Pathak et al., 2016). In our work, we show that these losses do not generalize to segmentation, and how we might need specific architectural biases to close the gap.\n\nWe describe additional related work on Unsupervised 3D Part Discovery and Shape program synthesis in supplementary Section 10\n\n3 METHOD\n\nFigure 2: Model architecture for multi-view images. Given multi-view RGB images as input. Slot-TTA (here using OSRT (Sajjadi et al., 2022a) as a backbone) maps them to a set of token features, which are then mapped to a set of slot vectors. Conditioned on the camera-viewpoint Slot-TTA then decodes each slot into its respective segmentation mask and RGB image. It then uses weighted averaging to render the RGB image for the whole scene as seen from the camera viewpoint. On the training dataset, we jointly optimize using reconstruction and segmentation loss. On the test set, we optimize only using the reconstruction loss. We use a similar training pipeline for other input modalities.\n\n3\n\nSlot DecoderWeighted AverageN slotsToken featuresMask Segmentation LossEncoderSlot DecoderSlot DecoderSlot DecoderCamera informationRGB Reconstruction Loss Input RGB viewsPredicted RGB and Mask of each Slot for Target View 1Predicted Target View 1Ground Truth Target View 1 Optimized only on the training set Optimized on both training and test set Under review as a conference paper at ICLR 2023\n\nThe goal of Slot-TTA is to segment out-of-distribution scenes into objects and parts annotated in the training set. We consider three different settings: (i) segmenting 2D multi-view RGB images, (ii) 2D single-view RGB images, and (iii) segmenting 3D point clouds. In each setting, the model encodes the scene as a set of slot vectors (capturing information about individual objects), and decodes them back to either 3D point clouds or (novel-view) RGB images (depending on the setting). To compute slots, Slot-TTA uses Slot Attention (SA) (Locatello et al., 2020), where visual features are softly partitioned across slots through iterative attention. In the following, we first give a brief overview of SA in Section 3.1, followed by a detailed description of Slot-TTA in Section 3.2.\n\n3.1 BACKGROUND\n\nCurrent state-of-the-art detectors and segmentors instantiate slots (i.e. the query vectors) from 2D visual feature maps or 3D point feature clouds (Carion et al., 2020; Misra et al., 2021). Most works use iterative cross-attention (features to slots) and self-attention (slot-to-slots) operations (Carion et al., 2020) to map a set of N input feature vectors to a set of K slot vectors. Attention-based competition amongst slots and iterative routing popularized in Goyal et al. (2021); Locatello et al. (2020) encourages a single location in the input to be assigned to a unique slot vector. Given a visual scene encoded as a set of feature vectors M ∈ RN ×C and K randomly initialized slots sampled from a multivariate Gaussian distribution with a diagonal covariance S ∼ N (μ, Diag(σ2)) ∈ RK×D, where μ, σ ∈ RC are learnable parameters of the Gaussian, Slot Attention (Locatello et al., 2020) computes an attention map a between the feature map M and the slots S:\n\na = Softmax(k(M) · q(S)T , axis=“slots”) ∈ RN ×K.\n\n(1)\n\nk, q, and v are learnable linear transformations that map inputs and slots to a common dimension D. The softmax normalization over slots ensures competition amongst them to attend to a specific feature vector in M. Updates to the slots are computed based on the input features they attend to:\n\nupdates = aT v(M ) ∈ RK×C, where ai,k =\n\nai,k (cid:80)N −1 i=0 ai,k\n\n(2)\n\nwhich are then fed into a GRU (Cho et al., 2014): S = GRU(state = S, input = updates). We iterate 3 times over equations 1 and 2. For detailed description, please refer to Locatello et al. (2020).\n\n3.2 TEST-TIME ADAPTATION WITH SLOT-CENTRIC MODELS (SLOT-TTA)\n\nWe first describe the encoders and decoders that form the foundation of Slot-TTA for each modality. Further we detail how we train Slot-TTA and perform test time adaptation through slow inference.\n\n3.2.1 ENCODING AND DECODING BACKBONES\n\nPosed multi-view 2D RGB images As shown in Figure 2, Slot-TTA builds upon the architecture of OSRT (Sajjadi et al., 2022a), which is an object-centric, geometry-free novel view synthesis method. Given a set of multi-view RGB images as input, a CNN encodes each input image Ii into a feature grid, which is then flattened into a set of tokens with camera pose and ray direction information added in each of the tokens, similar to SRT (Sajjadi et al., 2022b). These are then encoded into a set of latent features using a transformer (Vaswani et al., 2017) Enc with multiple self-attention blocks z = Enc(CNN(Ii)). The latent features z are then mapped into a set of slots S using Slot Attention (Section 3.1). For decoding, we adopt the spatial broadcast decoder (Watters et al., 2019) formulation, where a render MLP takes as input the slot vector Sk and the pixel location p parameterized by the camera position and the ray direction pointing to the pixel to be decoded. It outputs an RGB color ck and an unnormalized alpha score ak for each pixel location ck, ak = Dec(p, Sk). The ak’s are normalized using a Softmax and used as weights to aggregate the predicted RGB values ck for each slot. A camera viewpoint conditioned decoder allows us to render novel viewpoints, for which we show novel view rendering results in our supplementary video. We ablate other decoder choices, such as the Slot Mixer decoder (Sajjadi et al., 2022a) in supplementary Section 9.1.\n\nSingle-view 2D RGB images For this setting, Slot-TTA uses a ResNet-18 (He et al., 2016) to encode the input RGB image into a feature grid. We then add positional vectors to the feature grid and map to a set of slot vectors using Slot Attention. Similar to the multi-view setting, each slot\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nvector is decoded to the RGB image and an alpha mask using an MLP renderer. We parameterize pixel location p as (x, y) points on the grid instead of camera position as the above setting.\n\n3D point clouds To adapt to 3D point clouds, Slot-TTA uses a 3D point transformer (Zhao et al., 2021) which maps the 3D input points to a set of M feature vectors of C dimensions each. We set M to 128 and C to 64 in our experiments. Point feature vectors are mapped to slots with Slot Attention. Slot-TTA decodes 3D point clouds from each slot using implicit functions (Mescheder et al., 2019). Specifically, each decoder takes in as input the slot vector Sk and an (X, Y, Z) location and returns the corresponding occupancy score ok,x,y,z = Dec(Sk, (x, y, z)) , where Dec is a multi-block ResNet MLP similar to that of Lal et al. (2021). We then max-pool over the slot dimension k to get an occupancy value ox,y,z for each 3D point in the scene.\n\n3.2.2 TRAINING AND TEST-TIME ADAPTATION\n\nSlot-TTA assumes entity-level supervision in the form of segmentation masks, and can also exploit unlabelled data via a reconstruction loss objective.\n\nTraining for joint segmentation and reconstruction We train all the parameters of our model to jointly optimize image / point cloud reconstruction or novel view image synthesis objectives and the task segmentation objective over all the n examples in the training set, where x represents the input scene and y the segmentation labels:\n\nmin θ\n\n1 n\n\nn (cid:88)\n\ni=1\n\nλslseg(xi, yi; θ) + λrlrecon(xi; θ)\n\n(3)\n\nIn the case of RGB images, for reconstruction, we minimize the mean squared error between the predicted and ground truth RGB images. For segmentation, we supervise the alpha masks ai of each slot as provided by the decoders. We use Hungarian matching (Kuhn, 1955) (combinatorial optimization algorithm that solves assignment problems) to associate the ground truth masks with the predicted masks, and upon association we apply a categorical cross-entropy loss lseg . In the case of 3D point clouds we supervise the predicted occupancy probability ok. We use a binary cross-entropy loss for lrecon. For lseg we use Hungarian matching with a categorical cross-entropy loss similar to other modalities. We weight the segmentation and reconstruction loss by λs and λr.\n\nTest-time adaptation In this work, we refer to a single forward pass through our trained model without any test-time adaptation as fast inference (same as regular inference). We call the process of test-time adapting the model on each example independently slow inference, using only the reconstruction objective of Eq. 3. We use this terminology to emphasize that the only difference between both settings is the added computation time which results in an effective speed difference between the two inference schemes. We adapt only the encoder parameters θenc in our model, which we found to improve results compared to adapting the entire model as shown in our supplementary Section 9.1. We train for 150 steps per example using the Adam optimizer (Kingma & Ba, 2014) and a learning rate of 1e-4.\n\n4 EXPERIMENTS\n\nSlow Inference (TTA)\n\nin Figure 3: Slot-TTA: 3D segmentation improves over gradient steps despite only optimizing for 3D reconstruction.\n\nWe test Slot-TTA capability for segmenting posed multi-view RGB images, single-view RGB images and 3D point clouds. Our experiments aim to answer the following questions: (i) How does Slot-TTA compare against state-of-the-art 2D and 3D segmentation models (Luo et al., 2020; Yu et al., 2021; Cheng et al., 2021)? (ii) How does slow inference through reconstruction feedback affect segmentation accuracy in Slot-TTA and its variants? (iii) How much does supervision during training contribute to segmentation performance?\n\n5\n\nInput & TargetSlow InferenceUnder review as a conference paper at ICLR 2023\n\nEvaluation metric In addition to the loss, we use Adjusted Random Index (ARI) as our evaluation metric for segmentation accuracy (Rand, 1971). ARI calculates the similarity between two-point clusters while being invariant to the ordering of the cluster centers. For this, we use the publicly available implementation of Kabra et al. (2019).\n\n4.1 SEGMENTING RGB IMAGES IN MULTI-VIEW SCENES\n\nDataset We evaluate Slot-TTA on the MultiShapeNet (MSN) dataset from SRT (Sajjadi et al., 2022b). The dataset is constructed by rendering 51K ShapeNet objects using Kubric (Greff et al., 2022) against 382 photo-realistic HDR backgrounds so that there is no overlap of objects between the train and test sets. In addition to having different object instances in training and test sets, we further re-generate data in the MSN dataset so that train and tests sets differ in the number of objects present: scenes with 5-7 object instances are in the training set and scenes with 16-30 objects are in the test set. Each instance is sampled from a random pose. This lets us measure how well our model and the baselines perform under this distribution shift. Please refer to supplementary Section 7 for further dataset details and visualization of samples from the train-test split. Additionally in Table 6, we test our model on a different distribution shift, where instead of increasing the number of instances in the test-set we introduce new object categories from Google Scanned objects dataset Downs et al. (2022). Thus showing Slot-TTA improves performance across different distribution shifts.\n\nMethod\n\nin-dist (5-7 instances)\n\nout-of-dist (16-30 instances)\n\nFast Infer.\n\nSlow Infer.\n\nFast Infer.\n\nSlow Infer.\n\nSlot-TTA-w/o supervision\n\nMask2Former Mask2Former+BYOL Mask2Former+Recon\n\nSlot-TTA (Ours)\n\n0.32\n\n0.93 0.93 0.93\n\n0.92\n\n0.30\n\nN/A 0.95 0.92\n\n0.95\n\n0.33\n\n0.74 0.75 0.74\n\n0.70\n\n0.29\n\nN/A 0.74 0.67\n\n0.83\n\nTable 1: Instance Segmentation accuracy (higher is better) in the multi-view RGB setup for in-distribution test set of 5-7 object instances and out-of-distribution 16-30 object instances.\n\nBaselines We compare to three baselines: (i) Mask2Former (Cheng et al., 2021), a state-of-the-art 2D image segmentor which adapts detection transformers (Carion et al., 2020) to image segmentation by using multiscale segmentation decoders with masked attention. (ii) Mask2Former+BYOL which combines the segmentation model of (Cheng et al., 2021) with test time adaptation using BYOL selfsupervised loss of Bartler et al. (2022). (iii) Mask2Former+Recon which combines the segmentation model of Cheng et al. (2021) with rendering submodules and image reconstruction loss for test-time adaptation.\n\nFigure 4: Test-time adaptation via slow inference in Slot-TTA for multi-view scenes. In the right top we visualize the RGB loss (blue curve) and the segmentation ARI accuracy (red curve). As can be seen, during slow inference the segmentation accuracy improves as reconstruction loss reduces.\n\n6\n\nGround truth target RGBBefore adaptationAfter adaptationReconstruction LossSegmentation AccuracyUnder review as a conference paper at ICLR 2023\n\nResults We show quantitative segmentation results of our model and baselines on target camera viewpoints in Table 1 and qualitative TTA results in Figure 4. In Slot-TTA-w/o supervision, instead of training jointly for reconstruction and segmentation, we train using only cross-view image synthesis, similar to OSRT (Sajjadi et al., 2022a).\n\nIt can be observed that: (i) Slot-TTA-Slow outperforms the feedforward Mask2Former-Fast, especially for out-of-distribution scenes; (ii) adding self-supervised losses of SOTA image classification methods (Bartler et al., 2022) to Mask2Former (eg. Mask2Former+BYOL) does not suffice to adapt them effectively at test time and (iii) Slot-TTA without supervision, which is identical to OSRT is not competitive with supervised models for object segmentation.1\n\nFor additional qualitative comparisons between fast and slow inference in the multi-view setting, please refer to Figure 11 in Supplementary. For extensive ablations of Slot-TTA please refer to Table 5 in Supplementary. For novel-view renderings please refer to our supplementary video.\n\n4.2 SEGMENTING SINGLE-VIEW RGB IMAGES\n\nAs a proof of concept, in this section, we test our model and the baseline Mask2Former in segmenting single RGB images comprised of multiple samples from five shapes of distinct colors, organized in heavily occluded configurations, a dataset we create and we call Multi-Shape. Our training set consists of images with 3-5 object instances, while the test set consists of images with 10-16 object instances. For this setting, we report the ARI scores for the foreground objects only, since in this dataset the background occupies a large image area and a method that assigns most pixels to background already achieves a very high ARI. We find the performance accuracy ordering of the methods to be the same.\n\nAs can be seen in Table 2, before TTA Mask2former and Mask2former+Recon outperform our method. After TTA, our method significantly outperforms the baselines.\n\nMethod\n\nin-dist (3-5 instances)\n\nout-of-dist(10-16 instances)\n\nFast Infer.\n\nSlow Infer.\n\nFast Infer.\n\nSlow Infer.\n\nMask2Former Mask2Former+Recon\n\nSlot-TTA (Ours)\n\n0.96 0.95\n\n0.96\n\nN/A 0.94\n\n0.95\n\n0.44 0.43\n\n0.39\n\nN/A 0.47\n\n0.69\n\nTable 2: Foreground instance segmentation accuracy (higher is better) for single-view RGB images. Indistribution images have 3-5 objects and out-of-distribution images have 10-16 objects.\n\nPlease refer to Section 9.2 for qualitative results in our Multi-Shape dataset, where we showcase some success and failure cases in Slot-TTA with slow inference.\n\nAdditionally we test Slot-TTA on a real-world salad plating dataset which we collected and will publicly release. As can be viewed at [https://sites.google.com/corp/view/slottta], our model effectively segments the scene into objects and amodally reconstructs each one, despite heavy occlusions.\n\n4.3 SEGMENTING 3D POINT CLOUDS\n\nWe test Slot-TTA in segmenting 3D object point clouds into parts, for within distribution and out-ofdistribution object categories.\n\nWe consider two segmentation supervision setups: (i) Supervision from a dataset of generic 3D part primitives. (ii) Supervision from labelled 3D object point-clouds of a related object category.\n\n4.3.1 SUPERVISION FROM A DATASET OF GENERIC 3D PART PRIMITIVES\n\nDataset We use the part primitive dataset introduced by Shape2Prog (Tian et al., 2019) (akin to generalized cylinders of Marr (1982)), which consists of differently sized cubes, cuboids, and discs. Thus, our supervised training set consists of scenes that contain a single primitive part, resized and\n\n1Although OSRT performs poorly in the ARI metric, it achieves substantially better results in terms of\n\nforeground-ARI (yet still not competitive). This is because it is unable to segment out the background.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ntranslated in different 3D locations of a blank 3D canvas, while the test set consists of unseen chairs and tables from ShapeNet, each composed of 6 to 16 parts. Our goal is to quantify to what extent our model can compose generic parts into shape compositions via slow inference, as we motivated in Figure 1. We assume access to unlabelled 3D chair point clouds during joint training.\n\nBaselines We compare our model against the following baselines: (i) PQ-Nets of Wu et al. (2020) assume access to a set of primitive 3D parts for pre-training. Specifically they first learn a primitive part decoder, then they learn a sequential encoder that encodes the 3D point cloud into a 1D latent vector and sequentially decodes parts using the pretrained part decoder. We use the publicly available code to train the model. (ii) Shape2Prog of Tian et al. (2019) is a shape program synthesis method that is trained supervised to predict shape programs from object 3D point clouds. The program represents the part category, location, and the symmetry relations among the parts (if any).\n\nWe further evaluate Slot-TTA w/o supervision, an ablative version of our model that is trained without any supervised pre-training on the generic part dataset, but is only trained using a reconstruction objective for autoencoding the Chair dataset. This version of our model coincides with the previous work of Slot Attention of Locatello et al. (2020) but instead implemented for 3D.\n\nMethod\n\nin-dist (Chairs)\n\nout-of-dist (Tables)\n\nFast Infer.\n\nSlow Infer.\n\nFast Infer.\n\nSlow Infer.\n\nShape2Prog (Tian et al., 2019) PQ-Nets (Wu et al., 2020)\n\nSlot-TTA w/o supervision\n\nSlot-TTA (Ours)\n\n0.28 0.20\n\n0.41\n\n0.57\n\n0.53 0.31\n\n0.35\n\n0.62\n\n0.23 0.17\n\n0.47\n\n0.60\n\n0.40 0.21\n\n0.38\n\n0.69\n\nTable 3: Instance Segmentation accuracy (higher is better) in the test set of Chair category (in-distribution) and Table category (out-of-distribution) when trained using the supervision of generic primitives.\n\nResults We show quantitative results of our model and the baselines in Table 3. It can be observed that: (i) Slot-TTA significantly outperform PQ-Nets (Wu et al., 2020), which maps the input object 3D pointcloud into a 1D latent vector, suggesting that segregation into slot like entities using attention as in Slot-TTA is beneficial; (ii) Slot-TTA-Fast outperforms Slot-TTA w/o supervision-Fast by a large margin, indicating that the additional supervised data is beneficial and correctly integrated; and (iii) slow inference through reconstruction feedback helps in the presence of supervision and hurts in the absence of it. Such trade-off between reconstruction and segmentation in generative models for scene decomposition has previously been pointed out in Engelcke et al. (2020), which is also supported by our findings.\n\nIn supplementary, please, refer to Section 9.3.1for further ablations and qualitative comparison against baselines. Refer to the supplementary video for the intermediate visualizations of slow inference. Finally please refer to Figure 7.3.1 for visualization of the primitive dataset.\n\n4.3.2 SUPERVISION FROM A RELATED OBJECT CATEGORY\n\nDataset In this setup, we test our model and baselines in segmenting the test objects of the Chair and Table categories in the PartNet benchmark (Mo et al., 2019), with access to ground-truth pointcloud segmentation of the Chair category in PartNet. We train Slot-TTA in a semi-supervised way combining a reconstruction loss and a supervised segmentation loss as described in Section 3.2.2.\n\nBaselines We compare our model against the following baselines: (i) Learning2Group of (Luo et al., 2020) progressively groups points into segments by learning pairwise grouping decisions parameterized by features of the two point clusters. We used the publicly available code and trained the model in the training set of the Chair category. (ii) 3D-DETR a variant of state-of-the-art 3D object detection model of Misra et al. (2021) for 3D point-cloud instance segmentation. Please refer to supplementary Section 8.4 for a detailed descriptions on all the baselines.\n\nWe consider the following ablative versions of Slot-TTA: (i) Slot-TTA w/o supervision is trained without the supervised part segmentation loss; and (ii) Slot-TTA w/o SlotAttention, which does not\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nin-dist (Chair)\n\nout-dist (Table)\n\nFast Infer.\n\nSlow Infer.\n\nFast Infer.\n\nSlow Infer.\n\n3D-DETR (Misra et al., 2021) Learning2Group (Luo et al., 2020)\n\nSlot-TTA w/o supervision Slot-TTA w/o SlotAttention\n\nSlot-TTA (Ours)\n\n0.67 0.62\n\n0.40 0.58\n\n0.64\n\nN/A N/A\n\n0.44 0.55\n\n0.66\n\n0.41 0.46\n\n0.31 0.31\n\n0.49\n\nN/A N/A\n\n0.48 0.44\n\n0.61\n\nTable 4: Instance Segmentation accuracy (higher is better) in the test set of Chair category (in-distribution) and Table category (out-of-distribution) when trained using the supervision of Chair category.\n\nuse Slot Attention for mapping point features to slots. Instead it maps 3D point features to slots via iterative layers of cross (query to point) and self (query-to-query) attention layers on learnable query vectors similar to 3D-DETR and DETR (Carion et al., 2020). Please note that slots and queries represent the same thing, but we use the terminology of DETR (Carion et al., 2020) in this case.\n\nResults We report the fast and slow inference results for all ablative versions of our model. Our baselines in this case, 3D-DETR and Learning2Group (Luo et al., 2020) are feedforward in nature, they are not equipped with decoders, and thus cannot be evaluated with slow inference. We show quantitative results in Table 4 and qualitative results in Figure 5. Slot-TTA significantly outperform the baselines, and Slot-TTA-Slow results in a significant boost in performance (∼ 30%) over the feedforward inference in our model, Slot-TTA-Fast.\n\nFigure 5: Out-of-distribution 3D segmentation results for Slot-TTA and baselines. Left: 3D segmentation results on out-of-distribution shapes for Slot-TTA and baselines. Right: 3D segmentation results for out-ofdistribution categories of PartNet for Slot-TTA trained semi-supervised only on the Chair category.\n\nWe draw the following conclusions from Table 4: (i) Generic primitives generalize a lot better than category-specific supervision. Slot-TTA in Table 3 (that uses the generic parts dataset) outperforms Slot-TTA in Table 4 (that use the category-specific supervision) in OOD generalization on the Table category. (ii) Competition amongst slots helps. Slot-TTA outperforms Slot-TTA w/o SlotAttention, thus showing competition amongst slot vectors during encoding helps generalization. (iii) Slow inference through reconstruction feedback helps OOD generalization of Slot-TTA. Our baselines 3D-DETR and Learning2Group (Luo et al., 2020) are feed-forward in nature, they lack any form of reconstruction feedback, and thus cannot adapt as our model through such feedback. In our supplementary Section 9.3.2 we extend Table 4 to all remaining 14 ShapeNet categories as the test categories. Further in Table 8 in supplementary we report results on bounding box detection instead of instance segmentation, and showcase a similar improvement in performance. Thus showcasing our method generalizes beyond segmentation tasks.\n\n5 CONCLUSION\n\nWe presented Slot-TTA, a novel semi-supervised instance segmentation model equipped with a slot-centric image or point-cloud rendering component. Slot-TTA is capable of test-time adaptation on a single unlabeled example (i.e. slow inference) through gradient descent on reconstruction or novel view synthesis objectives. We have shown how slow inference for Slot-TTA greatly improves segmentation in out-of-distribution scenes, and compares favorably to other (unsupervised) segmentation approaches, including other forms of test time adaptation.\n\nThere exist several promising directions for improving Slot-TTA, which reflect its current limitations. Currently, Slot-TTA does not model pairwise interactions between slots, while such cross-talk could be beneficial for adaptation. Further, while a scene decomposition into entities and parts is inherently hierarchical, such structure is currently not reflected in Slot-TTA as the backbones it considers capture a flat, non-hierarchical, list of entities.\n\n9\n\n3D-DETR [5]Learning2Group [45]OursInputCupboardLampDisplayLaptopUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY STATEMENT\n\nTo ensure the reproducibility of our work, we will make our code and datasets publicly available to the community. Additionally, in our supplementary section, we try our best to mention all the information that could aid reproducibility. Specifically, in Section 7 we mention all the dataset preparation details. In Section 8 we in-depth specify the implementation details of our method and the baselines, including the hyperparameter values and computational details.\n\nREFERENCES\n\nAlexander Bartler, Andre Bühler, Felix Wiewel, Mario Döbler, and Bin Yang. Mt3: Meta testtime training for self-supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics, pp. 3080–3090. PMLR, 2022.\n\nMathilde Bateson, Hervé Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments\n\nfor image segmentation. In MICCAI, pp. 736–745. Springer, 2022.\n\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pp. 213–229. Springer, 2020.\n\nAngel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University — Toyota Technological Institute at Chicago, 2015.\n\nWoong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-specific In Proceedings of the IEEE/CVF\n\nbatch normalization for unsupervised domain adaptation. conference on Computer Vision and Pattern Recognition, pp. 7354–7362, 2019.\n\nZhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha Chaudhuri, and Hao Zhang. Bae-net: branched autoencoder for shape co-segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8490–8499, 2019.\n\nZhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net: Generating compact meshes via binary space partitioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 45–54, 2020.\n\nBowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. CoRR, abs/2112.01527, 2021. URL https://arxiv.org/abs/2112.01527.\n\nKyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n\nBoyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, and Andrea In Proceedings of the IEEE/CVF\n\nTagliasacchi. Cvxnet: Learnable convex decomposition. Conference on Computer Vision and Pattern Recognition, pp. 31–44, 2020.\n\nTheo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. Learning elementary structures for 3d shape generation and matching. arXiv preprint arXiv:1908.04725, 2019.\n\nLaura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. arXiv preprint arXiv:2204.11918, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYilun Du, Kevin Smith, Tomer Ulman, Joshua Tenenbaum, and Jiajun Wu. Unsupervised discovery\n\nof 3d physical objects from video. arXiv preprint arXiv:2007.12348, 2020.\n\nKevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381, 2020.\n\nMartin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052, 2019.\n\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Reconstruction bottlenecks in object-centric\n\ngenerative models. arXiv preprint arXiv:2007.06245, 2020.\n\nSM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, and Geoffrey E Hinton. Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint arXiv:1603.08575, 2016.\n\nYossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked\n\nautoencoders. arXiv preprint arXiv:2209.07522, 2022.\n\nLin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao Zhang. Sdm-net: Deep generative network for structured deformable mesh. ACM Transactions on Graphics (TOG), 38(6): 1–15, 2019.\n\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665–673, 2020.\n\nKyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7154–7164, 2019.\n\nKyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3d shape. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4857–4866, 2020.\n\nMuhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep reconstruction-classification networks for unsupervised domain adaptation. In European conference on computer vision, pp. 597–613. Springer, 2016.\n\nAnirudh Goyal, Alex Lamb, Phanideep Gampa, Philippe Beaudoin, Sergey Levine, Charles Blundell, Yoshua Bengio, and Michael Mozer. Object files and schemata: Factorizing declarative and procedural knowledge in dynamical systems. arXiv preprint arXiv:2006.16225, 2020.\n\nAnirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and\n\nBernhard Schölkopf. Recurrent independent mechanisms. In ICLR, 2021.\n\nKlaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, Jürgen Schmidhuber, and Harri Valpola. Tagger: Deep unsupervised perceptual grouping. arXiv preprint arXiv:1606.06724, 2016.\n\nKlaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning, pp. 2424–2433. PMLR, 2019.\n\nKlaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. On the binding problem in artificial\n\nneural networks. arXiv preprint arXiv:2012.05208, 2020.\n\nKlaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: A scalable dataset generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3749–3761, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021.\n\nRishabh Kabra, Chris Burgess, Loic Matthey, Raphael Lopez Kaufman, Klaus Greff, Malcolm Reynolds, and Alexander Lerchner. Multi-object datasets. https://github.com/deepmind/multiobject-datasets/, 2019.\n\nHiroharu Kato and Tatsuya Harada. Learning view priors for single-view 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9778–9787, 2019.\n\nNan Rosemary Ke, Aniket Didolkar, Sarthak Mittal, Anirudh Goyal, Guillaume Lajoie, Stefan Bauer, Danilo Rezende, Yoshua Bengio, Michael Mozer, and Christopher Pal. Systematic evaluation of causal discovery in visual model based reinforcement learning. arXiv preprint arXiv:2107.00848, 2021.\n\nAnsh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image\n\ntest-time adaptation. arXiv preprint arXiv:2112.02355, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAdam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat: Generative modelling of moving objects. Advances in Neural Information Processing Systems, 31: 8606–8616, 2018.\n\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\n\nquarterly, 2(1-2):83–97, 1955.\n\nTejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. Picture: A probabilistic programming language for scene perception. In Proceedings of the ieee conference on computer vision and pattern recognition, pp. 4390–4399, 2015.\n\nShamit Lal, Mihir Prabhudesai, Ishita Mediratta, Adam W. Harley, and Katerina Fragkiadaki. Co-\n\nconets: Continuous contrastive 3d scene representations, 2021.\n\nYikai Li, Jiayuan Mao, Xiuming Zhang, William T Freeman, Joshua B Tenenbaum, Noah Snavely, and Jiajun Wu. Multi-plane program induction with 3d box priors. arXiv preprint arXiv:2011.10007, 2020.\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525–11538, 2020.\n\nTiange Luo, Kaichun Mo, Zhiao Huang, Jiarui Xu, Siyu Hu, Liwei Wang, and Hao Su. Learning to group: A bottom-up framework for 3d part discovery in unseen categories. arXiv preprint arXiv:2002.06478, 2020.\n\nD. Marr. Vision: A Computational Investigation into the Human Representation and Processing of\n\nVisual Information. W.H. Freeman and Co., New York, NY, 1982.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. In Proceedings of the\n\nOccupancy networks: Learning 3d reconstruction in function space. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4460–4470, 2019.\n\nIshan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2906–2917, 2021.\n\nKaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 909–918, 2019.\n\nChengjie Niu, Jun Li, and Kai Xu. Im2struct: Recovering 3d shape structure from a single rgb image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4521–4529, 2018.\n\nDespoina Paschalidou, Ali Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning 3d shape parsing beyond cuboids. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10344–10353, 2019.\n\nDespoina Paschalidou, Luc Van Gool, and Andreas Geiger. Learning unsupervised hierarchical part decomposition of 3d objects from a single rgb image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1060–1070, 2020.\n\nDespoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, and Sanja Fidler. Neural parts: Learning expressive 3d shape abstractions with invertible neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3204–3215, 2021.\n\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2536–2544, 2016.\n\nNasim Rahaman, Anirudh Goyal, Muhammad Waleed Gondal, Manuel Wuthrich, Stefan Bauer, Yash Sharma, Yoshua Bengio, and Bernhard Schölkopf. S2rms: Spatially structured recurrent modules. arXiv preprint arXiv:2007.06533, 2020.\n\nWilliam M Rand. Objective criteria for the evaluation of clustering methods. Journal of the American\n\nStatistical association, 66(336):846–850, 1971.\n\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprint arXiv:1803.00676, 2018.\n\nSara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. arXiv\n\npreprint arXiv:1710.09829, 2017.\n\nMehdi SM Sajjadi, Daniel Duckworth, Aravindh Mahendran, Sjoerd van Steenkiste, Filip Paveti ́c, Mario Luˇci ́c, Leonidas J Guibas, Klaus Greff, and Thomas Kipf. Object scene representation transformer. arXiv preprint arXiv:2206.06922, 2022a.\n\nMehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luˇci ́c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6229–6238, 2022b.\n\nInkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk-Jin Yoon. Mm-tta: Multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, 2022.\n\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nYonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T Freeman, Joshua B Tenenbaum, and Jiajun Wu. Learning to infer and execute 3d shape programs. arXiv preprint arXiv:1901.02875, 2019.\n\nShubham Tulsiani, Hao Su, Leonidas J Guibas, Alexei A Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2635–2643, 2017.\n\nSjoerd Van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully\n\ntest-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.\n\nNicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. arXiv preprint arXiv:1901.07017, 2019.\n\nRundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, and Baoquan Chen. Pq-net: A generative part seq2seq network for 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nChun-Han Yao, Wei-Chih Hung, Varun Jampani, and Ming-Hsuan Yang. Discovering 3d parts from image collections. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12981–12990, 2021.\n\nHong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance fields.\n\narXiv preprint arXiv:2107.07905, 2021.\n\nPolina Zablotskaia, Edoardo A Dominici, Leonid Sigal, and Andreas M Lehrmann. Unsupervised video decomposition using spatio-temporal iterative inference. arXiv preprint arXiv:2006.14727, 2020.\n\nYoushan Zhang. A survey of unsupervised domain adaptation for visual recognition. arXiv preprint\n\narXiv:2112.06745, 2021.\n\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16259–16268, 2021.\n\nDaniel Zoran, Rishabh Kabra, Alexander Lerchner, and Danilo J Rezende. Parts: Unsupervised segmentation with slots, attention and independence maximization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10439–10447, 2021.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nThe structure of this appendix is as follows: In Section 7 we cover the details on the datasets. In Section 8 we specify further implementation details. In Section 9 we provide additional qualitative and quantitative results for the experiments in Section 4 of our main paper.\n\nAlong with this we also provide a video file in the supplementary zip, which visualizes the intermediate reconstruction and parsing results of Slot-TTA during the slow inference stage and we also visualize our multi-view rendering results.\n\n7 DATASETS\n\n7.1 MULTI-VIEW RGB\n\nFigure 6: We visualize samples from the train-test split used by us in experiment Section 4.1. Different rows correspond to different scenes and different columns correspond to different viewpoints.\n\nWe use the MultiShapeNet-Hard dataset of Scene Representation Transformer, a complex photorealistic dataset for Novel View Synthesis (Sajjadi et al., 2022b). Our train split consists of 5-7 ShapeNet objects placed at random locations and orientations in the scene. The backgrounds are sampled from 382 realistic HDR environment maps. Our test set consists of 16-30 objects placed at novel arrangements. We sample objects from a pool of 51K ShapeNet objects across all categories, we divide the pool into train and test such that the test set consists of objects not seen during training. The train split has 200K scenes, and the test set consists of 4000 scenes, each with 10 views. We had to regenerate the dataset for this specific train-test split.\n\n7.2 SINGLE-VIEW RGB\n\nFigure 7: We visualize the samples of our MultiShape dataset.\n\nMulti-Shape is a dataset built by us for proof-of-concept. It consists of 5 shapes of distinct colors uniformly placed at a random location in a 2D canvas. Our training set consist of 3-5 object instances, while the test set consists of a highly occluded setting with 10-16 object instances.\n\n7.3 POINT CLOUD\n\nFor all the tasks, we subsample the input point clouds to a standard size of 2048 points.\n\n7.3.1 GENERIC PRIMITIVE PART DATASET.\n\nWe use the primitive dataset of Tian et al. (2019) as supervision in Experiment Section 4.3.1. The dataset consists of 200K primitive instances sampled from the primitive templates that are visualized\n\n15\n\nTraining SetTest SetTest SetTraining SetTraining SetTest SetTest SetTraining SetUnder review as a conference paper at ICLR 2023\n\nFigure 8: We visualize all the generic primitive templates of Tian et al. (2019), as you can see, they mainly consist of Cubes, Cuboids, and Discs.\n\nFigure 9: We visualize the Synthetic whole shape dataset of Tian et al. (2019). Shape2Prog supervises their model using the annotations from this dataset.\n\nin Figure 8. Examples are sampled from the templates by changing their sizes and placing them uniformly in random locations, similar to Tian et al. (2019).\n\n7.3.2 SYNTHETIC WHOLE SHAPE DATASET.\n\nThis dataset was generated by Shape2Prog (Tian et al., 2019). The segmentation labels from this dataset are used by them as a supervision signal for later generalizing to PartNet shapes. The dataset consists of about 120K synthetically generated Chairs and Tables, we visualize some of these synthetically generated tables and chairs in Figure 9. Note that neither Slot-TTA nor the baselines have access to this dataset.\n\n16\n\nCategory-specific primitivesUnder review as a conference paper at ICLR 2023\n\n7.3.3 PARTNET DATASET.\n\nWe use the official level-3 train-test split of PartNet (Mo et al., 2019). We use the train split of Chair category as our training set, we consider test split of Table category in PartNet our test categories. We use this as the train-test split in Experiment Section 4.3.2. We set the value of number of slots K as 16 for this dataset.\n\n8\n\nIMPLEMENTATION DETAILS\n\n8.1 POSED MULTI-VIEW 2D RGB IMAGES\n\nTraining details and computational complexity. We use a batch size of 256 in this setting. We set our learning rate as 10−4. We use an Adam optimizer with β1 = 0.9, β2 = 0.999. For training, our model takes about 4 days to converge using 64 TPUv2 chips. Our slow inference for each example takes about 10 seconds on a single TPUv2 chip. Similarly, a forward pass through our model takes about 0.1 seconds. During training, instead of decoding all the pixels, we decode only a sample of them. Specifically, we randomly pick 1024 pixel locations for each example in the batch during each iteration of training. During test-time adaptation, instead of uniformly sampling pixel locations, we use an error-weighted sampling strategy which we describe below.\n\nInputs. During training and test-time adaptation, our model takes in as input multi-view RGB images along with their ground-truth egomotion. For each scene, we randomly select 5 input and target views, and make sure there is no overlap between the two sets, as a result rendering novel viewpoints each time. Note that although we stick to 5 viewpoints, Slot-TTA can take a variable number of viewpoints as input. We use a resolution of 128x128 for our input and target images.\n\nEncoder. Here we follow the original implementation of OSRT (Sajjadi et al., 2022b). The model encodes each input image Ii, its camera extrinsic and intrinsics into a set representation via a shared CNN and transformer backbone. Specifically, the CNN outputs a feature grid for each image conditioned on the camera extrinsic and intrinsics, which are then flattened into a set of flat patch embeddings. The patch embeddings are then processed by a transformer that outputs a set of latent embeddings. The latent embeddings have a dimensionality of 1535. The CNN consists of 3 blocks of convolutions, with a ReLU activation after each convolution. The transformer contains 5 blocks of Multi-Head Self-attention.\n\nSlot Attention. The latent embeddings from the encoder are then mapped to a Slot Attention module. We use the original implementation by Locatello et al. (2020), however instead of initializing the slots from a multi-variate gaussian we have them as learnable embedding vectors. We keep our slot vectors dimensionality as 1536. We set the number of slots as 32 in this setting.\n\nDecoder. We use the broadcast decoder of Sajjadi et al. (2022a) for decoding the slots to their RGB image conditioned on the target viewpoints. Our slot decoder consists of a 4-layer MLP with a hidden dimensionality of 1536 and ReLU activation. Our target viewpoints are parameterized using 6D light-field parametrization of camera position and normalized ray direction.\n\nError-conditioned pixel sampling To accelerate test-time adaptation, we sparsely sample a subset of pixels from the target images, where we prioritize the pixels with a high reconstruction error. To this end, we calculate the reconstruction error over all pixels and apply a Softmax with a temperature τ = 0.01 along the pixel dimension.\n\n8.2 SINGLE-VIEW 2D RGB IMAGES\n\nTraining details and computational complexity. We use a batch size of 16 in this setting. We set our learning rate as 10−4. We use an Adam optimizer with β1 = 0.9, β2 = 0.999. For training, our model takes about 12 hours to converge using V100 GPU. Our slow inference for each example takes about 6 seconds on the same GPU. Similarly, a forward pass through our model takes about 0.06 seconds. We decode the whole image instead of sampling a subset of pixels like in the above section.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nInputs. We use a single 2D RGB image of resolution 128 x 128 as our input. We normalize our input in the range of -0.5 to 0.5 before passing it through our encoder.\n\nEncoder. We use ResNet-18 (He et al., 2016) as our encoder backbone, which takes as input the RGB image and outputs a feature grid. We add positional vectors to the feature grid, which are grid locations normalized in the range of -1.0 to -0.1.\n\nSlot Attention. We use the exact implementation of Locatello et al. (2020). In this setting, we set the number of slots as 16 as that’s the maximum number of objects in Multi-Shape dataset.\n\nDecoder. We use a 4-layer MLP decoder, that takes in as input a slot vector and a 2D location and outputs it’s corresponding RGB value and alpha score.\n\n8.3\n\n3D POINT CLOUDS\n\nTraining details and computational complexity. We use a batch size of 8 for point cloud input. We set our learning rate as 40−4. We use the Adam optimizer with β1 = 0.9, β2 = 0.999. Our model takes 24 hours (approximately 200k iterations) to converge. Our slow inference per example takes about 1 min (500 iterations). A forward pass through the proposed model takes about 0.15 secs. We use a single V100 GPU for training and inference.\n\nInputs. We subsample the input point clouds to a standard size of 2048 points.\n\nEncoder. We adopt the point transformer (Zhao et al., 2021) architecture as our encoder. Point transformer encoder is essentially layers of self attention blocks. Specifically a self attention block includes sampling of query points and updating them using their N most neighbouring points as key/value vectors. In the architecture we specifically apply 5 layers of self attention which look as follows: 2048-16-64, 2048-16-64, 512-16-64, 512-16-64, 128-16-64, 128-16-64. We use the notation of S-N -C, where S is the number of subsampled query points from the point cloud, N is the number of neighbouring points and C is the feature dimension. We thus get an output feature map of size 128 × 64.\n\nDecoder. We obtain point occupancies by querying the slot feature vector slotk at discrete locations (x, y, z) specifically ox,y,z = Dec(slotk, (x, y, z)). The architecture of Dec is similar to that of Lal et al. (2021). Given slotk, which is one of the slot feature vector. We encode the coordinate (x, y, z) into a 64-D feature vector using a linear layer. We denote this vector as z. The inputs slotk and z are then processed as follows:\n\noutk = RBi(RBi−1(· · · RB1(z + F C1(slotk)) · · · ) + F Ci−1(slotk)) + F Ci(slotk)).\n\n(4)\n\nWe set i = 3. F Ci is a linear layer that outputs a 64 dimensional vector. RNi is a 2 layer ResNet MLP block (He et al., 2016). The architecture of ResNet block is: ReLU, 64-64, ReLU, 64-64. Here, i − o represents a linear layer, where i and o are the input and output dimension. Finally outk is then passed through a ReLU activation function followed by a linear layer to generate a single value for occupancy.\n\n8.4 BASELINES\n\nMask2former (Cheng et al., 2021) Mask2former is a recent state-of-the-art 2D RGB segmentation network, that scales transformer-based 2D-DETR (Carion et al., 2020) for the task of segmentation. They improve 2D-DETR’s transformer decoder by adding masked and multi-scale attention, which helps them achieve SOTA results on panoptic, instance and semantic segmentation on the COCO dataset. We use their publicly available code to train on MultiShapeNet dataset. We use a batch size of 256 and train their network on 8 V100s GPUS for 4 days until convergence. We set the number of slots in their network as 32, similar to our model.\n\n3D-DETR. 3D-DETR is a version of 3DETR (Misra et al., 2021)(a 3D state-of-the-art object detection method) scaled to the task of instance segmentation, we build this architecture on top of the idea of 2D-DETR (Carion et al., 2020).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Given point clouds as input the encoder backbone featurizes the points into N feature vectors, we then do iterative self and cross attention using K learnable query heads, similar to Carion et al. (2020). We then compute attention of the updated queries wrt to the encoded feature vectors. We then concatenate these attentions along the batch dimension and pass them through a FPN-style transformer that increases their resolution and outputs each query mask logits. We then do hungarian matching and binary cross entropy loss\n\nThe base encoder is a 5 layer architecture of 1024-16-64, 1024-16-64, 1024-16-64, 1024-16-64, 512-16-64, following the notation of S,N ,C where S is the number of sampled query points, N is the number of selected neighbouring points and C is the feature dimension. We have 16 learnable queries with six encoder-decoder layers of transformer attention, each layer consists of 8 heads. We then compute multi-head cross-attention between each query vector and the encoded 3d points. This gives us a map of size M × 512 × 16, where M is the number of heads in the multi-head attention.\n\nEach attention map is individually upsampled through a PointTransformer decoder of the architecture 512-16-64, 1024-16-64, 1024-16-64, which gives us the final instance segmentation mask per query, similar to DETR (Carion et al., 2020). We then use Hungarian matching to match the predicted masks against the ground truth masks and then apply binary cross entropy loss for each match. We aggregate the losses from each query and backpropagate. Figure 10 visualizes the architecture of 3D-DETR. Note that we do not follow the 2 stage training of Carion et al. (2020), rather we train their model end-to-end for instance segmentation, similar to our model. We have found this trick to save compute and still not harm the end results.\n\nLearning2Group. (Luo et al., 2020) Learning2Group progressively groups points into segments by learning pairwise grouping decisions parameterized by features of the point clusters to be grouped. Given the intermediate grouping decisions are not supervised, and the non-differentiability of their grouping functions they use reinforcement learning gradients for training using supervision from the final segmentation. We use their open-sourced architecture and code for comparision with our mode. We train their model using our datasets from scratch.\n\nShape2Prog (Tian et al., 2019) Shape2Prog is a shape program synthesis method that is trained supervised to predict shape programs from object 3D point clouds. Shape2Prog introduced two synthetically generated datasets that helped the model parse 3D pointclouds from ShapeNet Chang et al. (2015) into shape programs without any supervision: i) Generic primitive set (Figure 8) we discussed earlier in which they use to pre-train their part decoders, and ii) Synthetic whole shape dataset of chairs and tables (Figure 9) generated programmatically alongside its respective groundtruth programs. Their model requires supervised pre-training on the dataset of synthetic whole shapes paired with programs. We therefore use their publicly available model weights trained on synthetic whole shapes to further train on PartNet Chairs. Note that no other baseline nor Slot-TTA assumes access to the synthetic whole shape dataset. We use their open-sourced architecture, code and pretrained checkpoints for comparision with our model. We change the value of number of blocks similar to the number of slots in our model for each dataset.\n\n19\n\nTrainingCross Attention With HeadsHead features T = 1Attention MapHead features T = 2Hungarian Matching + Binary CrossEntropy LossEncoderAttention MapAttention MapSpatial Attention MapsC o\nn c\na t\ne n\na t\neFPN-Style Transformer P O\nI N\nT A\nR G\nM A\nXUnder review as a conference paper at ICLR 2023\n\nPQ-Nets. (Wu et al., 2020) PQ-Nets is a sequential encoder-decoder architecture, that takes 3D point cloud as input and sequentially encodes it into multiple 1D latents which are then decoded to part point clouds. It achieves this decomposition by pre-training their decoder to predict part point clouds. We use their open-sourced architecture and code for comparision with our model. We train their model using our datasets from scratch. We change the value of number of slots in their model based on the maximum number of parts in the dataset.\n\n9 ADDITIONAL EXPERIMENTS\n\n9.1 SEGMENTING RGB IMAGES IN MULTI-VIEW SCENES\n\nMethod\n\nin-dist (5-7 instances)\n\nout-of-dist (16-30 instances)\n\nbefore TTA\n\nafter TTA\n\nbefore TTA\n\nafter TTA\n\nSlot-TTA-SlotMixer_Decoder Slot-TTA-SRT_Decoder\n\nSlot-TTA-tta_All_param Slot-TTA-tta_Norm_param Slot-TTA-tta_Slot_param\n\nSlot-TTA w/o Weighted_Sample\n\nSlot-TTA (Ours)\n\n0.94 0.92\n\nN/A N/A N/A\n\nN/A\n\n0.92\n\n0.89 0.88\n\n0.92 0.94 0.94\n\n0.93\n\n0.95\n\n0.65 0.60\n\nN/A N/A N/A\n\nN/A\n\n0.70\n\n0.72 0.63\n\n0.82 0.79 0.76\n\n0.81\n\n0.83\n\nTable 5: ARI Segmentation accuracy (higher is better) in the in-distribution test set of 5-7 object instances and out-of-distribution 16-30 object instances.\n\nMethod\n\nin-dist (ShapeNet categories)\n\nout-of-dist (GSO categories)\n\nbefore TTA\n\nafter TTA\n\nbefore TTA\n\nafter TTA\n\nMask2Former Mask2Former+BYOL Mask2Former+Recon\n\nSlot-TTA (Ours)\n\n0.93 0.93 0.93\n\n0.92\n\nN/A 0.95 0.92\n\n0.95\n\n0.93 0.92 0.92\n\n0.92\n\nN/A 0.93 0.91\n\n0.95\n\nTable 6: ARI Segmentation accuracy (higher is better) in the in-distribution test set of ShapeNet object categoriesChang et al. (2015) and out-of-distribution test set of GSO object categories Downs et al. (2022).\n\nIn Table 6, we tested our model on a different distribution shift. In the test set instead of increasing the number of instances in the scene in Table 1, we introduced instances from new object categories. Specifically the MSN Sajjadi et al. (2022b) train-set consists of ShapeNet object categoriesChang et al. (2015) (Tables, Chairs etc), whereas the new test-set consists of Google Scanned Object Downs et al. (2022) (GSO) categories (Shoes, Stuffed toys etc). We find that our model gets a score of 0.92 before TTA and 0.95 after TTA, whereas mask2former and its TTA counterparts get a score of 0.93, clearly demonstrating the benefit of slot-centric test-time adaptation over a state-of-the-art baseline.\n\nWe conduct various ablations of Slot-TTA in Table 1. In Figure 11, we show additional qualitative results comparing Slot-TTA-Fast and Slot-TTA-Slow.\n\n(i) We ablate different decoder choices in the topmost section where instead of using the broadcast decoder we use the Scene representation transformer (SRT) decoder (Sajjadi et al., 2022b) which we refer to as Slot-TTA-SRT_Decoder or the SlotMixer decoder (Sajjadi et al., 2022a), referred to as Slot-TTA-SlotMixer_Decoder.\n\n(ii) We ablate what parameters to adapt at test time. As it’s unclear since TENT (Wang et al., 2020) optimizes BatchNorm or LayerNorm parameters, but TTT (Sun et al., 2020) optimizes the shared parameters between the SSL and the task-specific branch, which in our case will be all the parameters in the network. In Table 5, Slot-TTA-tta_All_param is when we adapt all the\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nnetwork parameters, Slot-TTA-tta_Norm_param adapts only the Layer or BatchNorm parameters and Slot-TTA-tta_Slot_param adapts only the learnable slot embeddings. We find that optimizing only the encoder parameters works the best for our setting.\n\n(iii) Further, we ablate error-conditioned pixel sampling where Slot-TTA w/o Weighted_Sample refers to our model that uses uniform sampling instead of the error weighted sampling.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: On the left, we visualize Slot-TTA-Fast. In the middle, we visualize Slot-TTA-Slow. In the first row we visualize the ground truth target RGB views. In the second and third row we visualize Slot-TTA predicted target RGB views and their segmentation masks. On the right-most column we visualize the RGB loss and segmentation accuracy when doing slow inference. Same setting as Section 4.1\n\n22\n\nBefore AdaptationAfter AdaptationLoss & AccuracyUnder review as a conference paper at ICLR 2023\n\n9.2 SEGMENTING SINGLE-VIEW RGB IMAGES\n\nIn Figure 12 we qualitatively compare Slot-TTA-Slow with Slot-TTA-Fast. We show that slow inference can help discover objects missed by Slot-TTA. We also show some failure cases where slow inference could override the object-centric bottleneck to achieve higher reconstruction accuracy.\n\nFigure 12: Success and Failure cases of slow-inference on Multi-Shape dataset. Same setting as Section 4.2\n\n9.3 SEGMENTING 3D POINT CLOUDS\n\n9.3.1\n\nSUPERVISION FROM A DATASET OF GENERIC 3D PART PRIMITIVES\n\nIn this Section we show additional qualitative and quantitative results for Section 4.3.1. Neither Slot-TTA nor the baselines have access to ground-truth 3D segmentations during training of Chair or Table category; as a result, they may output 3D parts of coarser or finer resolution. PartNet contains three different levels of ground-truth segmentation labels with progressively finer segmentation granularity. In Table 7, we further extend Table 3 to include different levels in Chair and Table category. Here (X/Y /Z) refers to (level 1/level 2/level 3) scores respectively. We pick the best performing level in fast inference and specifically report it’s slow inference results. We further qualitatively compare our model against Shape2Prog (best performing baseline) in Figure 13\n\nMethod\n\nin-dist (Chairs)\n\nout-of-dist (Tables)\n\nFast Infer.\n\nSlow Infer.\n\nFast Infer.\n\nSlow Infer.\n\nShape2Prog Tian et al. (2019) PQ-Nets Wu et al. (2020)\n\n0.28/0.21/0.26 0.20/0.18/0.16\n\nSlot-TTA\n\n0.51/0.48/0.57\n\n0.53 0.31\n\n0.62\n\n0.21/0.23/0.23 0.17/0.14/0.16\n\n0.51/0.55/0.60\n\n0.40 0.21\n\n0.69\n\nTable 7: ARI Segmentation accuracy (higher is better) in the test set of Chair (in-distribution) and Table category of PartNet(out-of-distribution). Slot-TTA significantly outperform all of the baselines.\n\n23\n\nInput RGBBefore adaptationAfter adaptationSuccesses - ARI increases by 20-30%Failures - ARI reduces by 10%Under review as a conference paper at ICLR 2023\n\nFigure 13: Additional segmentation results on out-of-distribution categories when supervised from generic primitives. Same setting as Section 4.3.1\n\n24\n\nInputShape2Prog (Supervised)GFS-Nets-FastGFS-Nets-SlowUnder review as a conference paper at ICLR 2023\n\n9.3.2\n\nSUPERVISION FROM A RELATED OBJECT CATEGORY\n\nIn this section we further qualitatively and quantitatively compare our model against baselines on the task of instance segmentation. We follow the same experimental setup of Section 4.3.2, where we use Chair category segmentation supervision and test for cross-category generalization. We test the segmentation accuracy of the models on other categories of PartNet in Table 9. Note that a few of these categories don’t share any common parts with the Chair category that’s why the absolute scores of Slot-TTA and all other baselines are very low on these categories. We find this specific metric to be not well-defined, as generalizing from Chairs to Shoes or Vase, might not make sense, but to a furniture category like a Table or Bed does. Inspite of this overall, we find the mean score of Slot-TTA to outperform all other baselines.\n\nFurther in Table 8, we tested our model on an object detection task, where we predict the bounding box for each instance. We find that our model gets a similar performance improvement as our instance segmentation task after test-time adaptation. Specifically the box mIoU improves from 0.51 to 0.63 after TTA, whereas our 3D-DETR baseline gets a score of 0.53. Thus showcasing that our method generalizes beyond the task of instance segmentation.\n\nMethod\n\nin-dist (Chair)\n\nout-dist (Table)\n\nFast Infer.\n\nSlow Infer.\n\nFast Infer.\n\nSlow Infer.\n\n3D-DETR (Misra et al., 2021)\n\nSlot-TTA\n\n0.68\n\n0.62\n\nN/A\n\n0.65\n\n0.53\n\n0.51\n\nN/A\n\n0.63\n\nTable 8: Object detection box mIoU accuracy (higher is better) in the test set of Chair category (in-distribution) and Table category (out-of-distribution) when trained using the supervision of Chair category.\n\nMethod 3D-DETR Learning2Group Slot-TTA-Fast Slot-TTA-Slow Method 3D-DETR Learning2Group Slot-TTA-Fast Slot-TTA-Slow Method 3D-DETR Learning2Group Slot-TTA-Fast Slot-TTA-Slow\n\nClock 0.12 0.18 0.04 0.05\n\nDishwasher 0.08 0.10 0.04 0.06\n\nDisplay Bottle 0.25 0.19 0.26 0.45 Lamp Microwave Refrigerator 0.21 0.17 0.10 0.24\n\n0.17 0.1 0.04 0.10\n\n0.01 0.14 0.01 0.01\n\n0.21 0.30 0.16 0.53 Knife 0.23 0.28 0.24 0.38 Mean 0.20 0.22 0.15 0.27\n\nDoor 0.19 0.17 0.09 0.12 TrashCan 0.17 0.15 0.11 0.30\n\nEarphone 0.21 0.25 0.18 0.32 Vase 0.14 0.25 0.10 0.15\n\nFaucet 0.28 0.28 0.25 0.37 Bed 0.26 0.32 0.28 0.41\n\nTable 9: ARI Segmentation scores (higher is better) on the test set of the other level-3 categories(out-ofdistribution) of PartNet. All the models are trained using the training set of the Chair category in PartNet.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Additional segmentation results in out-of-distribution categories when supervised from a related object category. Same setting as Section 4.3.2\n\n10 ADDITIONAL RELATED WORK\n\nShape program synthesis and analysis-by-synthesis Slot-TTA is also related to works in analysisby-synthesis Kulkarni et al. (2015), program synthesis for shape prediction Tian et al. (2019); Ellis et al. (2020); Li et al. (2020), as well as earlier works on Computer Vision, such as Marr’s 3D sketch Marr (1982) which involves representing a scene in terms of generalized cylinders and their syntactic relations to each other. In place of data-driven Markov Chain Monte Carlo search of analysis-by-synthesis methods that require good initialization, our slow inference searches in the space of primitives by gradient descent. In contrast to program synthesis methods, it does not require a predefined domain-specific language (DSL) or program annotations for visual structures Li et al. (2020), rather, it discovers compositions over primitives via its slow inference.\n\nUnsupervised 3D Part Discovery There are numerous methods that attempt the decomposition of complex 3D shapes into primitive parts without primitive supervisionKato & Harada (2019); Genova et al. (2019); Paschalidou et al. (2020); Gao et al. (2019); Deprelle et al. (2019); Tulsiani et al. (2017); Deng et al. (2020); Chen et al. (2019). Traditional primitives include cuboids Tulsiani et al. (2017); Niu et al. (2018), superquadrics Paschalidou et al. (2019; 2020), and convexes Deng et al. (2020); Chen et al. (2020). Genova et al. (2020) proposes a 3D representation that decomposes space into a structured set of implicit functions Genova et al. (2019). Neural Parts Paschalidou et al. (2021) represents arbitrarily complex genus-zero shapes and thus yields comparatively expressive parts. However the resulting parts of these methodsGenova et al. (2020); Paschalidou et al. (2021) are still not semantically meaningful and the decomposition is highly dependent on the number of parts initialized. The work of Yao et al. (2021) does 3D part reconstruction directly from a 2D image input without access to any ground-truth 3D shapes for training. However, both Yao et al. (2021) and Paschalidou et al. (2021) take as input the number of parts, and different decompositions are predicted with varying part numbers. There is no clear way to select the right number of parts. In our case, parts can be quite complex: pairs of parallel surfaces, quadruplets of legs, as we use implicit\n\n26\n\nInstance Segmentation3D-DETRGFS-Nets-Slow3D-DETRGFS-Nets-SlowUnder review as a conference paper at ICLR 2023\n\nfunctions to represent them. Moreover Slot-TTA’s dynamic attention-based routing allows it to infer different number of parts for each input scene.\n\n27",
    "reference": "# Summary Of The Paper\n\nThe paper aims to tackle the out-of-domain generalization challenge for instance segmentation of 2D images and 3D point clouds. The proposed method combines the recent slot attention-based model for segmentation and the standard test-time-augmentation approach with the auxiliary reconstruction task. The proposed method significantly outperforms the prior art in the out-of-domain setting on standard benchmarks. The ablation studies show the importance of the proposed module and learning setting.\n\n# Strength And Weaknesses\n\nStrength\n- It's an interesting idea to take the slot-centric approach to do TTA with the image reconstruction auxiliary task, which is not done before.\n\n- Strong performance on popular datasets from two modalities.\n\n- Detailed ablation studies and model description for design justification and reproducibility.\n\nWeakness\n- Lack of novelty. The proposed method combines two existing works: slot-attention-based instance segmentation model and the TTA method with image reconstruction auxiliary loss, without providing much new insights.\n\n- Biased experiment setting. For image instance segmentation, the benchmark dataset (sec. 7.1-7.2) has few instances in the training set and much more instances in the test set. Such experiment design favors instance segmentation models for crowded scenes (i.e., slot-attention based method) over popular maskformer models. Thus, it's unclear if the performance gain over the baseline (maskformer+reconstruction) majorly comes from the inductive bias of the backbone selection instead of the TTA framework. Also, It's unclear if the proposed method is indeed better on common images (e.g., MS-COCO). The paper result can be more convincing if it shows comparison on natural images.\n\n- Naming. (1) Task: the proposed method is specifically designed for the \"instance segmentation\" task, but the paper uses \"segmentation\" in many places, which can be confusing. (2) Title: It's more accurate to use \"image reconstruction\" instead of \"image synthesis\". The title can be better to emphasize on its key insight: slot-centric reconstruction. (3) the model name. GFSNet is generic, as any TTA methods that require finetuning can be named as GFSNet.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Clarity&quality: it's easy to follow overall. The method section can be better with figures for the model architecture for three different settings.\n\n- Novelty: It's incremental without much new insights.\n\n- Reproducibility: good.\n\n# Summary Of The Review\n\nOverall, the proposed method achieves strong performance on the selected multiple datasets. However, I have concerns on (1) its novelty as it simply combines two existing ideas; (2) usability in real world as the dataset images are heavily crowded with synthetic objects, which makes it unclear if the performance gains mostly comes from the TTA framework or the segmentation backbone that is better suited for the crowded scene. The paper will be much stronger if it provides results on popular 2D image instance segmentation benchmarks.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFEDPD: DEFYING DATA HETEROGENEITY THROUGH PRIVACY DISTILLATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nModel performance of federated learning (FL) typically suffers from data heterogeneity, i.e., data distribution varies with clients. Advanced works have already shown great potential for sharing client information to mitigate data heterogeneity. Yet, some literature shows a dilemma in preserving strong privacy and promoting model performance simultaneously. Revisiting the purpose of sharing information motivates us to raise the fundamental questions: Which part of the data is more critical for model generalization? Which part of the data is more privacy-sensitive? Can we solve this dilemma by sharing useful (for generalization) features and maintaining more sensitive data locally? Our work sheds light on data-dominated sharing and training, in a way that we decouple original training data into sensitive features and generalizable features. To be specific, we propose a Federated Privacy Distillation framework named FedPD to alleviate the privacy-performance dilemma. Namely, FedPD keeps the distilled sensitive features locally and constructs a global dataset using shared generalizable features in a differentially private manner. Accordingly, clients can perform local training on both the local and securely shared data for acquiring high model performance and avoiding the leakage of not distilled privacy. Theoretically, we demonstrate the superiority of the sharing-only useful feature strategy over sharing raw data. Empirically, we show the efficacy of FedPD in promoting performance with comprehensive experiments.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL), as an emerging protection paradigm, receives increasing attention recently (Kairouz et al., 2021; Li et al., 2021b; Yang et al., 2019), which preserves data privacy without transmitting pure data. In general, distributed clients collaboratively train a global model by aggregating gradients (or model parameters). However, distributed data can cause heterogeneity issues (McMahan et al., 2017; Li et al., 2022; 2020; Zhao et al., 2018), due to diverse computing capability and non-IID data distribution across federated clients. It results in unstable convergence and degraded performance.\n\nTo address the challenge of heterogeneity, the seminal work, federated averaging (FedAvg) (McMahan et al., 2017), proposes weighted averaging to overcome Non-IID data distribution when sharing selected local parameters in each communication round. Despite addressing the diversity of computing and communication, FedAvg still struggles with the client drift issue (Karimireddy et al., 2020). Therefore, recent works try to resolve this issue by devising new learning objectives (Li et al., 2020), designing new aggregation strategies (Yurochkin et al., 2019) and constructing information for sharing (Zhao et al., 2018; Yoon et al., 2021). Among these explorations, sharing relevant information across clients provides a straightforward and promising approach to mitigate data heterogeneity.\n\nHowever, recent works point out a dilemma in preserving strong privacy and promoting model performance. Specifically, (Zhao et al., 2018) show that a limited amount of sharing data could significantly improve training performance. Unfortunately, sharing raw data, synthesized data, logits and statistical information (Luo et al., 2021; Goetz & Tewari, 2020; Hao et al., 2021; Karimireddy et al., 2020) can incur high privacy risks. To protect clients’ privacy, differential privacy (DP) provides a de facto standard way for provable security quantitatively. The primary concern in applying DP is about performance degradation (Tramer & Boneh, 2020). Thus, solving the above dilemma can contribute to promoting model performance while preserving strong privacy.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n1.1 SYSTEMATIC OVERVIEW OF FEDPD\n\nTo solve the dilemma, we revisit the purpose of sharing information: sharing raw data benefits model generalization while violating privacy leakage. This motivates us to raise the fundamental questions:\n\n(1) Is it necessary to share complete raw data features to mitigate data heterogeneity? We find that some data features are more important than others to train a global model. Therefore, an intuitive approach is to divide the data features into two parts: one part for model generalization, named generalizable features, and the other part with clients’ privacy, named sensitive features. Then, the dilemma can be solved by sharing generalizable features and keeping sensitive features locally throughout the training procedure. The insight is that the sensitive features in the data are kept locally, and the generalizable features intrinsically related to generalization are shared across clients. Accordingly, numerous decentralized clients can share generalizable features without privacy concerns and construct a global dataset to perform local training.\n\n(2) How to divide data features into generalizable features and sensitive features? It is challenging to identify which part of the data is more important for model generalization and which part is more privacy-sensitive. To resolve this challenge, we propose a novel framework named Federated Privacy Distillation (FedPD). FedPD introduces a competitive mechanism by decomposing x ∈ Rd with dimension d into generalizable features xg ∈ Rd and sensitive features xs ∈ Rd, i.e., x = xg + xs. In FedPD, sensitive features xs aim to cover almost all information in the data x, while the generalizable features xg compete with xs for extracting sufficient information to train models such that models trained on xg can generalize well. Consequently, the sensitive features are almost the same as the data while models trained on generalizable features generalize well.\n\n(3) What is the difference between sharing raw data features and partial features? To ensure that sharing the generalizable features xg cannot expose FL to the danger of privacy leakage, we follow the conventional style in applying differential privacy to protect generalizable features xg shared across clients. Our trick is that most information in data has been distilled as sensitive features xs, which is very secure and kept locally. In other words, we only need a relatively small noise to protect xg, without the need to fully protect the raw data x, yet achieving a much stronger privacy than the straightforward protection (i.e., directly sharing x with differential privacy). Intuitively, sharing partial information in the data is more accessible to preserve privacy than sharing complete information, which is fortunately consistent with our theoretical analysis.\n\n1.2 OUR RESULTS AND CONTRIBUTION\n\nTo tackle data heterogeneity, we propose a novel framework with privacy, which constructs a global dataset using securely shared data and performs local training on both the local and shared data, shedding new light on data-dominated sharing schemes. To show the efficacy, we deploy FedDP on four popular FL algorithms, including FedAvg, FedProx, SCAFFOLD, and FedNova, and conduct experiments on various scenarios with respect to different amounts of devices and varying degrees of heterogeneity. Our extensive results show that FedPD achieves considerable performance gains on different FL algorithms. Our solution not only improves model performance in FL but also provides strong security, which is theoretically guaranteed from the lens of differential privacy.\n\nOur contributions are summarized as follows:\n\n• We raise a foundation question: whether it is necessary to share complete raw data features\n\nwhen sharing privacy data for mitigating data heterogeneity in FL.\n\n• We answer the question by proposing a plug-and-play framework named FedPD, where raw data features are divided into generalizable features and sensitive features. In FedPD, the sensitive features are distilled in a competitive manner and kept locally, while the generalizable features are shared in a differentially private manner to construct a global dataset.\n\n• We give a new perspective on employing differential privacy that adds noise to partial data features instead of the complete raw data features, which is theoretically superior to the raw data sharing strategy.\n\n• Extensive experiments demonstrate that FedPD can considerably improve the performance\n\nof FL models.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: FL Framework with the plug-in FedPD. Clients generate generalizable features and add noise protection to get protected generalizable features xp during privacy distillation process. The protected generalizable features xp are collected from numerous distributed clients to construct a globally shared dataset while sensitive features xs are kept locally. During local training procedure, local raw data and a subset of globally shared data jointly train the local model for global aggregation. LA denotes the Eq. 1 and LF is the Eq. 3 in our paper.\n\n2 RELATED WORK\n\nFederated Learning with heterogeneous data. The classic FL algorithm FedAvg (McMahan et al., 2017) suffers from serious performance degradation when meeting severe Non-IID data. To address the data heterogeneity problem, a series of works propose a new learning objective to calibrate the updated direction of local training from being too far away from the global model, including FedProx(Li et al., 2020), FedIR (Hsu et al., 2020), SCAFFOLD (Karimireddy et al., 2020) and MOON (Li et al., 2021a). And some works propose designing new model aggregation schemes like FedAvgM (Hsu et al., 2019),FedNova (Wang et al., 2020b),FedMA (Wang et al., 2020a),FedBN (Li et al., 2021c).\n\nAnother promising direction is sharing some data, which mainly focuses on synthesizing and sharing data of different clients to mitigate client drift (Zhao et al., 2018; Jeong et al., 2018; Long et al., 2021). To avoid privacy leakage caused by sharing data, some methods share the statistics of data (Yoon et al., 2021; Shin et al., 2020), which still contains some raw data content. Some methods distribute intermediate features (Hao et al., 2021), logits (Chang et al., 2019; Luo et al., 2021), or the learned new embedding (Tan et al., 2022). Although these tactics enhance privacy at some degree, advanced attacks can still successfully reconstruct raw data given shared data (Zhao et al., 2020). Unlike prior research, we exploit DP to ensure privacy of shared data and then analyze privacy-performance trade-off.\n\nDifferential privacy in federated learning. Recent works on model memorization and gradient leakage confirm that model parameters are seemingly secure (Carlini et al., 2019). Training with differential privacy (Zhu et al., 2019; Nasr et al., 2019) is a feasible solution to avoid some attacks, albeit at some loss in utility. Differential privacy quantifies what extent individual privacy in a statistical dataset is preserved while releasing the established model over specific datasets.\n\nIn FL, training with differential privacy, i.e., adding noise to the model/data, originally aims to protect local information of each client (Yuan et al., 2019; Thakkar et al., 2019). Some works analyze the relation between convergence and utility in FL (Huang et al., 2020; Wei et al., 2020). A series of works in DP add noise to gradients or model parameters in FL to protect model privacy (Kim et al., 2021; van der Hoeven, 2019; Triastcyn & Faltings, 2019; Sun et al., 2021). Unlike model-based protection, our work aims to protect data privacy and mitigate the client drift issue. We provide a detailed discussion of exciting works in Appendix A.5.\n\n3\n\nServerLocaltrainingClassifier FGloballyshareddata (xp,yp)Rawdata(xl,yl)LFRaw dataProtected generalizable featuresGloballyshareddataClient NAuxiliaryclassifier ALocalclassifier FLocaltrainingPrivacy distillationClient 1Client 2Client 2Client NLocaltrainingPrivacy distillationLocaltrainingPrivacy distillationxp=ProtectionxgxPrivacy distillationxg=x−xsClassifier AProtected generalizable features(xp,y)LARaw dataG(4;θ)xsxgUnder review as a conference paper at ICLR 2023\n\n3 METHODOLOGY AND DETAILED CONSTRUCTION\n\nThis section elaborates Federated Privacy Distillation (FedPD), which is illustrated in Figure 1. Our insight is to keep sensitive features on the client’s side locally and share generalizable features globally in a differentially private manner. FedPD endows each client to use its local raw data features and generalizable features from others during local training, thus defying data heterogeneity.\n\n3.1 DIVISION OF TWO TYPES OF FEATURES FOR PRIVATE DATA\n\nDifferential privacy (DP) is promising in FL protections, but sharing all raw data in a DP manner typically causes performance degradation. Recall that the goal of sharing information is to benefit the model generalization rather than to collect private information. Therefore, we suggest to share useful features (generalizable features) in data while keeping most features locally (sensitive features), such that shared features benefits global generalization and locally kept features avoids privacy leakage.\n\nIdeally, if we could identify the sensitive features xs and the generalizable features xg, we could be able to solve the privacy-performance dilemma. Intuitively, sensitive features xs contain most information of data, while generalizable features xg contains the nonsensitive part that can help global generalization in FL. To resolve the dilemma in protecting privacy and promoting performance, we can keep the sensitive features locally while sharing generalizable feature protected under differentially private guarantee. The major challenge here is that the intersection of two types of features as aforementioned may not be the empty set, making it challenging to distill privacy.\n\n3.2 PRIVACY DISTILLATION\n\nTo address this issue, we propose a competitive mechanism to perform privacy distillation. Therein, the generalizable features aim to train models for generalizing well on the raw data, while the sensitive features compete with the generalizable features to construct the raw data. Consequently, the sensitive features is almost the same as the data while models trained on generalizable features generalizing well on the raw data. We propose two approaches to instantiate the competitive mechanism for privacy distillation, i.e., making generalizable features useful for model generalization while keeping sensitive features almost the same as the raw data, i.e., covering almost all information of raw data.\n\n3.2.1 OPTIMIZATION VIEW\n\nA straightforward approach is to distill private information in a meta manner (Finn et al., 2017). Specifically, we employ a generative model, e.g., a variational auto-encoder (VAE), G(·; θ) parameterized with θ to achieve the goal of covering all information of raw data, i.e., xs = G(·; θ) aims to reconstruct x. Meanwhile, to ensure the generalizable features, xg = x − xs = x − G(·; θ) = xg(θ), useful for model generalization, we train an auxiliary classifier A(·; w) parameterized with w using xg such that A(·; w) trained on xg performs well on the raw data x. Then, we can formalize the task of privacy distillation into the following optimization problem as:\n\nmin θ\n\ns.t. ˆw(θ) = arg min\n\nw\n\nE\n\nE\n\n(x, y) L(A(x; ˆw(θ)), y) + H(xg(θ)), (xg(θ), y) L(A((xg(θ); w), y), xg(θ) = x − G(x; θ).\n\n(1)\n\nHere, y is the label of the sample x and the generalizable features xg(θ), ˆw(θ) is a function of θ denoting the parameters of classifier A(x; ·), H(xg(θ)) is the information entropy of xg(θ), and L(·, ·) represents the cross-entropy loss. We can see that every possible parameter θ is paired with a model trained on the corresponding generated data xg(θ). Thus, solving the optimization problem is equivalent to searching for parameters θ to generate the generalizable features xg(θ) with minimum information entropy. Moreover, the model A((xg(θ); w), y) trained using (xg(θ), y) can perform well on the raw data.\n\nHowever, the proposed non-convex optimization problem is non-trival. We employ a simple yet effective trick widely used in reinforcement learning (Mnih et al., 2015). Specifically, we alternatively update G(x; θ) over x via stochastic gradient descent and update A(xg(θ); w) over xg(θ). Moreover, we minimize an upper bound of H(xg(θ)) with the variance of xg(θ) following (Ahuja et al., 2021).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.2.2 GENERALIZATION VIEW\n\nBesides the optimization approach, we also provide a generalization view to distill privacy. In a high level, we aim to train a model A(·; w) using xg such that A(·; w) can generalize well on x, i.e., samples drawn from a different distribution. Therefore, we should model how the performance on the generated data transfers to the raw data. To derive a detailed connect between these two distributions, the metric to measure the generalization performance should be defined clearly. According to the margin theory (Koltchinskii & Panchenko, 2002) that maximizing the margin between data points and the decision boundary achieves strong generalization performance, we relate such a margin to the generalization performance: Definition 3.1 (Margin). We define the margin for a classifier A(·; w) on a distribution P with a distance metric d: Mm(A, P) = E\n\n(x, y) ∼ P inf\n\nA(x′) ̸= y d(x′, x).\n\nBuilt upon the defined margin that quantifies the degree of generalization performance, we can quantify the generalization performance of A(·; w) on a given distribution. To be specific, large margin means strong generalization performance.\n\nA recent work (Tang et al., 2022) shows that the margin is intrinsically related to the distribution discrepancy in the representation space, i.e., the distance between distributions sampling x and that sampling xg. Thus, we propose minimizing the distribution discrepancy of the generated distribution and the raw distribution in the representation space:\n\nmin θ\n\nE\n\n(x, y) L(A(x; w), y) + L(A(xg(θ); w), y) + H(xg(θ)) + d(r(xg(θ)), r(x)).\n\n(2)\n\nwhere d is the distance metric used in the definition of margin and r(xg(θ)) stands for the representation of xg(θ) generated by the classifier A.\n\n3.3 DIFFERENTIALLY PRIVATE GENERALIZABLE FEATURES\n\nThe proposed privacy distillation methods make it possible to keep most (private) information locally while sending the generalizable features to the server. However, for ease of calculation of information entropy, we employ the variance of generalizable features as a surrogate, which may cause privacy leakage. This breaks the original intention of federated learning in protecting privacy. Thus, the shared generalizable features should be protected. Accordingly, the server can construct a global dataset using these generalizable features and send the dataset back to clients for local training.\n\nTo avoid privacy leakage, additional noise (e.g., Gaussian or Laplacian) is added to generalizable features xg, i.e., xp ≜ xg + N (0, σ2). Then, clients send xp to the server to construct a globally shared dataset. Using the global dataset, clients can train classifier F (·; φ) parameterized by φ with the local and shared data, :\n\nmin φ\n\nLF (φ) = E\n\n(x, y)L(F (x; φ), y) + E\n\n(xp, y)L(F (xp; φ), y).\n\n(3)\n\nAlgorithm 1 summarizes the training procedure of FedAvg with FedPD. To make sure the framework can be used without privacy concern, we further provide the corresponding analysis. Before that, we introduce the definition of differential privacy, which we used for adding i.i.d noise to generalizable features. Definition 3.2. (Differential Privacy). A randomized mechanism M provides (ε, δ)-differential privacy (DP) if for any two neighboring datasets D and D′ that differ in a single entry, ∀S ⊆ Range(M),\n\nPr(M(D) ∈ S) ≤ eε · Pr(M(D′) ∈ S) + δ.\n\nwhere ε is the privacy budget and δ is the failure probability.\n\nOur added noise to xg is proportional to the sensitivity, as defined in Definition 3.3. The concept of sensitivity is originally used for sharing a dataset for achieving (ε, δ)-differential privacy. Later, we follow Theorem 3.4 to analyze the privacy on globally shared data. Definition 3.3. (Sensitivity). The sensitivity of a query function F : D → R for any two neighboring datasets D, D′ is,\n\n∆ = max D,D′\n\n∥F(D) − F(D′)∥.\n\nwhere ∥ · ∥ denotes L1 or L2 norm.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 FedAvg with FedPD server input: initial φ0, communication round R client k’s input: local epochs E, local datasets Dk, learning rate ηk\n\nInitialization: server distributes the initial model φ0 to all clients, Globally shared dataset Ds generating. ← Detail in Algorithm 2 Server Executes: for each round r = 1, 2, · · · , R do\n\nserver samples a subset of clients Sr ⊆ {1, ..., K}, n ← (cid:80) client k samples a subet of globally shared dataset Dk\n\ni∈Sr\n\n|Di|\n\nr ⊆ Ds (|Dk\n\nr | = |Dk|)\n\nserver communicates φr to selected clients k ∈ Sr and sampled sharing data Dk for each client k ∈ Sr in parallel do k,E−1 ← ClientTraining(k, φr, Dk r )\n\nr\n\nφr+1 end for φr+1 ← (cid:80)|Sr|\n\nk∈Sr\n\nend for\n\n|Di|\n\nn φr\n\nk,E−1\n\nClientTraining(k, φ, Dk for each local epoch j with j = 0, · · · , E − 1 do φk,j+1 ← φk,j − ηk∇φLF (φ), i.e., Eq. 3\n\nr ):\n\nend for Return φ to server\n\nTheorem 3.4. For any ε > 0, δ ∈ [0, 1], and ˆδ ∈ [0, 1], the class of (ε, δ)-differentially private mechanisms satisfies (ˆεˆδ, 1 − (1 − ˆδ)Πi(1 − δi))-differential privacy under k-fold adaptive composition for 2k log(1/ˆδ)}.\n\nkε2/ˆδ), (eε −1)εk/(eε +1)+ε\n\nˆεˆδ = min{kε, (eε −1)εk/(eε +1)+ε\n\n2k log(e +\n\n(cid:113)\n\n(cid:113)\n\n(cid:114)\n\nSince xs is kept by the corresponding client, an adversary views nothing, which can be regarded as adding a sufficiently large noise on x to make it random enough. Considering all clients’ data as a whole, we use a relatively small σ (i.e., σc < σd + σc) for achieving much smaller privacy loss, summarized in Theorem 3.5.\n\nTheorem 3.5. Given identical privacy requirement, σc of FedDP is much less than σ that is supposedly added to raw data in conventional FL.\n\nGiven (ε, δ)-DP at each client side, we utilize composition theorem to analyze overall privacy in FedPD. In summary, FedPD protects two types of data features using two different protective manners, i.e., small noise for generalizable features and extremely large noise for sensitive features, and thus attains higher model performance and stronger security in the same time.\n\n4 EXPERIMENTS AND EVALUATION\n\n4.1 EXPERIMENT SETUP\n\nFederated Non-IID Datasets. We conduct experiments over various popular image classification datasets, including CIFAR-10, CIFAR100 (Krizhevsky et al., 2009), Fashion-MNIST(FMNIST) (Xiao et al., 2017), and SVHN (Netzer et al., 2011). We use latent dirichlet sampling (LDA) (Hsu et al., 2019) to simulate Non-IID distribution with 10 and 100 clients. The primary thought is to draw a q ∼ Dir(αp) from Dirichlet distribution, where α controls the heterogeneity degree. Here, the less α is, the more severe Non-IID distribution generate. In our experiments, we partition our datasets with two different degrees by LDA including α = 0.1 and α = 0.05. Besides, in order to prove that our framework works well under with Non-IID partitions. We also test other two kinds of partition strategy: (1) #C = k (McMahan et al., 2017; Li et al., 2022): each client only has k different labels from dataset, and k controls the unbalanced degree. (2) Subset method (Zhao et al., 2018): each client\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results with/without FedPD on CIFAR-10\n\ncentralized training ACC = 95.48% w/(w/o) FedPD\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nα = 0.1, E = 1, K = 10 (Target ACC =79%)\n\nα = 0.05, E = 1, K = 10 (Target ACC =69%)\n\nFedAvg FedProx\n\n92.34(79.35) 92.12(83.06) SCAFFOLD 89.66(83.67) 92.23(80.95)\n\nFedNova\n\n12.99↑ 9.06↑ 5.99↑ 11.28↑\n\n39(284) 62(192) 34(288) 33(349)\n\n×7.3(×1.0) ×4.6(×1.5) ×8.4(×1.0) ×8.6(×0.8)\n\n90.02(69.36) 90.73(78.98) 81.04(37.87) 91.21(65.08)\n\n20.66↑ 11.75↑ 43.17↑ 26.13↑\n\n×9.2(×1.0) 44(405) ×8.4(×2.0) 48(203) 37(None) ×10.9(None) 32(None) ×12.7(None)\n\nα = 0.1, E = 5, K = 10 (Target ACC =85%)\n\nα = 0.1, E = 1, K = 100 (Target ACC =49%)\n\nFedAvg FedProx\n\n93.24(83.79) 91.39(82.32) SCAFFOLD 92.34(85.31) 92.85(86.21)\n\nFedNova\n\n9.45 ↑ 8.97 ↑ 7.03 ↑ 6.64 ↑\n\n17(261) ×15.4(×1.0) 76(None) ×3.4(None) ×17.0(×4.0) ×8.4(×2.2)\n\n15(66) 31(120)\n\n84.06(49.72) 87.01(50.01) 79.60(52.76) 86.64(45.97)\n\n34.34↑ 37.00↑ 26.84↑ 40.67↑\n\n×5.9(×1.0) 163(967) ×7.6(×1.2) 127(831) ×5.7(×1.5) 171(627) 199(None) ×4.9(None)\n\n“Round” means the communication rounds that arrive at the target accuracy. ↓ and ↑ indicates smaller (larger) values are better. “None” implies not attaining the target accuracy during the entire training process. All the “Speedup” is calculated by comparing with vanilla FedAvg “Round” in different Non-IID partition scenarios.\n\nhas all classes from the data, but one dominant class far away outnumbers other classes. These three partition methods mainly include label skew and quantity skew. The visualization of data distribution is shown in Figure 4 in Appendix A.1.\n\nModels, Metrics and Baselines. We use ResNet-18 (He et al., 2016) both in content extractor during sharing data generation process and classifier in FL. And we exploit β-VAE (Higgins et al., 2016) for privacy distiller for privacy-preserving. We evaluate the model performance on two popular metrics in FL, i.e. the communication rounds to reach the target accuracy and the best accuracy in whole training process. Note that the target accuracy is set as the best accuracy of vanilla FedAvg in different scenarios. We conduct FedAvg (McMahan et al., 2017) and some other popular methods including FedProx (Li et al., 2020), SCAFFOLD (Karimireddy et al., 2020), FedNova (Wang et al., 2020b), with or without FedPD, to explore the potency of our method. We conduct all algorithms with local epochs E = 1 and E = 5. The detailed hyper-parameters of each FL algorithm and privacy distillation in different datasets are listed in A.3.1.\n\n4.2 EXPERIMENTAL RESULTS\n\nMain Results. The results on CIFAR-10, CIFAR-100, FMNIST, and SVHN are shown respectively in Tables 1, 2, 5, and 6, which demonstrates that FedPD has a significant performance gain. We also show the convergence speed of different algorithms on CIFAR-10 with a = 0.1, E = 1, M = 10 in Figure 2a,1 which shows that FedPD can also greatly improve the convergence rate.\n\nTable 2: Results with/without FedPD on CIFAR-100\n\ncentralized training ACC = 75.56% w/(w/o) FedPD\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nα = 0.1, E = 1, K = 10 (Target ACC =67%)\n\nα = 0.05, E = 1, K = 10 (Target ACC =61%)\n\nFedAvg FedProx\n\n69.64(67.84) 70.02(65.34) SCAFFOLD 70.14(67.23) 70.48(67.98)\n\nFedNova\n\n1.8↑ 4.68 ↑ 2.91↑ 2.5↑\n\n283(495) ×1.7 (×1.0) 233(None) ×2.1(None) 198(769) ×2.5(× 0.6) ×3.4(×1.1) 147(432)\n\n68.49(62.01) 69.03(61.29) 69.32(58.78) 68.92(60.53)\n\n6.48↑ 7.74↑ 10.54↑ 8.39↑\n\n137(503) ×3.7(×1.0) ×3.6(1.0) 141(485) 81(None) ×6.2(None) 87(None) ×5.8(None)\n\nα = 0.1, E = 5, K = 10 (Target ACC =69%)\n\nα = 0.1, E = 1, K = 100 (Target ACC =48%)\n\nFedAvg FedProx\n\n70.96(69.34) 69.66(62.32) SCAFFOLD 70.76(70.23) 69.98(69.78)\n\nFedNova\n\n1.62↑ 7.34↑ 0.53↑ 0.2↑\n\n79(276)\n\n×3.5(×1.0) 285(None) ×1.0(None) ×2.6(×1.6) 108(174) ×3.1(×1.0) 89(290)\n\n60.58(48.21) 67.69(48.78) 66.67(51.03) 67.62(48.03)\n\n12.37↑ 18.91↑ 15.64↑ 19.59↑\n\n448(967) ×2.2(×1.0) 200(932) ×4.8(×1.0) 181(832) ×5.3(×1.2) 198(976) ×4.9(×1.0)\n\n1More figures of convergence speed of other experiments are shown in appendix A.4.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Experiment results of different Non-IID partition methods on CIFAR-10 with 10 clients.\n\nTable 4: Experiment results with different noise adding in CIFAR-10.\n\nPartition Method\n\nα = 0.1 #C = 2 Subset\n\nTest Accuracy w/(w/o) FedPD\n\nFedAvg\n\nFedProx\n\nSCAFFOLD\n\nFedNova\n\nNoise Type\n\nTest Accuracy on Different Noise\n\nFedAvg\n\nFedProx\n\nSCAFFOLD FedNova\n\n92.34(79.35) 89.23/42.54 90.29/39.53\n\n92.12(83.06) 88.17/58.45 89.11/32.87\n\n89.66(83.67) 84.43/46.82 89.92/35.26\n\n92.23(80.95) 89.54/45.42 90.00/38.52\n\nGaussian Noise Laplacian Noise\n\n92.34 92.30\n\n92.12 91.36\n\n89.66 91.24\n\n92.23 91.73\n\n(a) Test Accuracy on CIFAR-10 with α = 0.1, E = 1, K = 10 and Gaussian Filter for better visualization.\n\n(b) Test Accuracy on FMNIST with different noise level σ2 ,obtaining various privacy ε(lower ε is preferred).\n\n(c) Two clasifiers trained on different data form and test accuracy on x, xs, and xg respectively.\n\nFigure 2: Experiments of the relationship between privacy and performance.\n\n(a) Globally Shared Data xp\n\n(b) Model Inversion Attack xg\n\n(c) Model Inversion Attack xp\n\nFigure 3: Model Inversion Attack Results. White-Box attack globally shared data xp and generalizable features xg, respectively. The result of being attacked is in (b) and (c) to compare with shared data xp in (a)\n\nPrivacy and performance. To explore the relationship between privacy level ε and performance, we conduct experiments with different σ2. As shown in Figure 2b, the performance decreases with the increasing protection strength. Another Laplacian noise report comparable results with Gaussian noise listed in Table 4. In conclusion, we suggest sacrificing part of the privacy when encountering limited communication resources. Another question is, can the globally shared data be inferred by some attack methods? To answer this question, we resort model inversion attack (He et al., 2019), widely used in the literature to reconstruction our shared data. The results on Figure 3b indicates that only privacy distillation still have risk of privacy leakage. Figure 3c also be a strong testimony for the differential privacy of noise adding on generalizable features. Furthermore, FedPD can give a strong private information protection. The original image can be found in Appendix A.2\n\nDifferent number of clients. Table 1, Table 2, Table 5, and Table 6 show that FedPD strengthen the performance and speed up the convergence both in 10 and 100 clients. Especially 100 clients in CIFAR-10 and CIFAR100 have a noteworthy enhancement. The reason may be that FL on CIFAR-10 and CIFAR100 with 100-clients has more diverge data distribution than FMNIST. With FedPD, the missed data knowledge can be well replenished.\n\nDifferent data heterogeneity. Table 1, Table 2, Table 5, and Table 6 show that high Non-IID degree (α=0.05) achieve a better improvement than lower unbalanced degree (α=0.1), which also indicates that FedPD can well defend against data heterogeneity. Moreover, Table 3 shows that other two kinds of heterogeneity partition cause more performance decline compared with LDA (α = 0.1), and FedPD attains comparable improvement with LDA α = 0.1, indicating FedPD is insensitive to other Non-IID data distribution.\n\n8\n\n02505007501000Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova0.10.20.3Noise Level 292939495Test AccuracyAcc357Privacy xxgTraining Set 206090Test Accuracy [%]Test SetxxgxsUnder review as a conference paper at ICLR 2023\n\nTable 5: Results with/without FedPD on FMNIST\n\ncentralized training ACC = 95.64% w/(w/o) FedPD\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nα = 0.1, E = 1, K = 10 (Target ACC =86%)\n\nα = 0.05, E = 1, K = 10 (Target ACC =78%)\n\nFedAvg FedProx\n\n92.34(86.73) 92.09(87.73) SCAFFOLD 91.62(86.31) 92.39(87.03)\n\nFedNova\n\n5.61↑ 4.36 ↑ 3.89↑ 5.36↑\n\n14(121) 32(129) 29(147) 18(88)\n\n×8.6(×1.0) ×2.1(×0.9) ×4.2(× 0.8) ×6.7(×1.4)\n\n90.69(78.34) 89.68(82.03) 80.48(76.63) 89.72(79.98)\n\n12.35↑ 7.65↑ 3.85↑ 9.74↑\n\n16(420) 16(44)\n\n×26.3(×1.0) ×26.3(9.5) 139(None) ×6.2(None) ×26.3(× 0.8)\n\n16(531)\n\nα = 0.1, E = 5, K = 10 (Target ACC =87%)\n\nα = 0.1, E = 1, K = 100 (Target ACC =90%)\n\nFedAvg FedProx\n\n92.26(87.43) 91.79(86.63) SCAFFOLD 92.92(87.21) 92.30(87.67)\n\nFedNova\n\n4.83↑ 5.16↑ 5.71↑ 4.63↑\n\n19(276) ×14.5(×1.0) 34(None) ×8.1(None) ×34.5(×2.5) ×34.5(×1.5)\n\n8(112) 8(187)\n\n92.71(90.21) 92.82(90.17) 90.28(84.87) 91.04(85.32)\n\n2.5↑ 2.65↑ 5.41↑ 5.72↑\n\n×2.8(×1.0) 243(687) ×2.4(×1.4) 284(501) 952(None) ×0.7 (None) 589(None) ×1.2(None)\n\nTable 6: Results with/without FedPD on SVHN\n\ncentralized training ACC = 96.56% w/(w/o) FedPD\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nACC↑\n\nGain↑\n\nRound ↓\n\nSpeedup↑\n\nα = 0.1, E = 1, K = 10 (Target ACC =88%)\n\nα = 0.05, E = 1, K = 10 (Target ACC =82%)\n\nFedAvg FedProx\n\n93.21(88.34) 91.80(86.23) SCAFFOLD 88.41(80.12) 92.98(89.23)\n\nFedNova\n\n4.87↑ 5.574↑ 8.29↑ 3.75↑\n\n×2.5(×1.0) 105(264) 233(None) ×1.1(None) 357(None) ×0.(None) ×2.3(×1.0) 113(276)\n\n93.49(82.76) 93.21(79.43) 90.27(75.87) 93.05(82.32)\n\n10.73↑ 13.78↑ 14.4↑ 10.73↑\n\n194(365) 37(None) 64(None) 37(731)\n\n×1.9(×1.0) ×9.9(None) ×5.7(None) ×9.9(×0.5)\n\nα = 0.1, E = 5, K = 10 (Target ACC =87%)\n\nα = 0.1, E = 1, K = 100 (Target ACC =89%)\n\nFedAvg FedProx\n\n93.77(87.24) 91.15(77.21) SCAFFOLD 93.78(80.98) 93.66(89.03)\n\nFedNova\n\n6.53↑ 13.94↑ 12.8↑ 4.63↑\n\n×1.2(×1.0) 105(128) 142(None) ×0.9(None) 20(None) ×6.4(None) ×2.5(×0.7) 52(177)\n\n91.04(89.32) 91.41(88.76) 92.73(88.32) 84.05(81.87)\n\n1.72↑ 2.65↑ 4.41↑ 2.18↑\n\n763(623) 733(645) 507(687)\n\n×0.8(×1.0) ×0.8(×1.0) ×1.2(×0.9) None(None) None(None)\n\nDifferent local epochs. To test the effect of local epoch E, we choose E = 1 and E = 5 with the same Non-IID degree (α = 0.1) and client number (K = 10). We run 1000 rounds with 1 epoch local training and 400 rounds for 5 epochs local update. The results show that FedPD is robust to the local epochs.\n\nOther Facts of FedPD. For a intuitive understanding of why we utilize xg as a substitute of raw data x without drastic performance degradation. We train two different networks separately on x and xg on CIFAR-10 and test them on x, xs, and xg, respectively. The results presented in Figure 2c. As we can see, the most useful features for downstream tasks are contained in xg. More experimental details are presented in Appendix A.4\n\n5 CONCLUDING REMARKS\n\nIn this paper, we observe that model gains a substantial performance assisted by generalizable features. Later we conduct DP to protect generalizable features and contruct a globally shared dataset for defying heterogeneity in FL. Our contribution lies in not only improving model performance in NonIID scenarios, but also inspiring a new viewpoint on data-dominated secure sharing, e.g., distillation data before knowledge learning. We expect that our work could simulated further data-dominated sharing in FL or other popular learning algorithms.\n\nOur framework shows suprior results against model inversion attack, yet we have not finished exploring data poisoning attack, given the shared data. We conduct preliminary experiments on data poisoning attacks, in which some clients send Gaussian noise to the server, causing performance degradation and slow convergence. Limited storage or communication resources may limited the power of FedPD, since FedPD introduces extra storage overhead. We leave it as our future work to explore the storage-friendly FedPD.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nETHIC STATEMENT\n\nThis paper does not raise any ethical concerns. This study does not involve any human subjects, practices to data set releases, potentially harmful insights, methodologies and applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, and research integrity issues.\n\nREPRODUCIBILITY STATEMENT\n\nTo make all experiments reproducible, we have listed all detailed hyper-parameters of each FL algorithm and privacy distillation on different datasets in A.3.1. Due to the privacy concerns, we will upload the anonymous link of source codes and instructions during the discussion phase to make it only visible to reviewers. All definitions can be found in Section 3. And the complete proof can be found in Appendix A.6.\n\nREFERENCES\n\nKartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-ofdistribution generalization. Advances in Neural Information Processing Systems, 34:3438–3450, 2021.\n\nKuntai Cai, Xiaoyu Lei, Jianxin Wei, and Xiaokui Xiao. Data synthesis via differentially private\n\nmarkov random fields. Proceedings of the VLDB Endowment, 14(11):2190–2202, 2021.\n\nNicholas Carlini, Chang Liu, ́Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security, 2019.\n\nHongyan Chang, Virat Shejwalkar, Reza Shokri, and Amir Houmansadr. Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer. arXiv preprint arXiv:1912.11279, 2019.\n\nAntoine Chatalic, Vincent Schellekens, Florimond Houssiau, Yves-Alexandre De Montjoye, Laurent Jacques, and R ́emi Gribonval. Compressive learning with privacy guarantees. Information and Inference: A Journal of the IMA, 11(1):251–305, 2022.\n\nYae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence\n\nanalysis and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243, 2020.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.\n\nJack Goetz and Ambuj Tewari.\n\nFederated learning via synthetic data.\n\narXiv preprint\n\narXiv:2008.04489, 2020.\n\nWeituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen, and Lawrence Carin Duke. Towards fair federated learning with zero-shot data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3310–3319, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nZecheng He, Tianwei Zhang, and Ruby B Lee. Model inversion attacks against collaborative inference. In Proceedings of the 35th Annual Computer Security Applications Conference, pp. 148–162, 2019.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data\n\ndistribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.\n\nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Federated visual classification with real-world\n\ndata distribution. In European Conference on Computer Vision, pp. 76–92. Springer, 2020.\n\nZonghao Huang, Rui Hu, Yuanxiong Guo, Eric Chan-Tin, and Yanmin Gong. DP-ADMM: admm-\n\nbased distributed learning with differential privacy. In IEEE TIFS, 2020.\n\nEunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data. arXiv preprint arXiv:1811.11479, 2018.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020.\n\nMuah Kim, Onur G ̈unl ̈u, and Rafael F. Schaefer. Federated learning with local differential privacy:\n\nTrade-offs between privacy, utility, and communication. In ICASSP. IEEE, 2021.\n\nVladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the\n\ngeneralization error of combined classifiers. The Annals of Statistics, 30(1):1–50, 2002.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nQinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10713–10722, 2021a.\n\nQinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: vision, hype and reality for data privacy and protection. IEEE Transactions on Knowledge and Data Engineering, 2021b.\n\nQinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pp. 965–978. IEEE, 2022.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020.\n\nXiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning\n\non non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021c.\n\nTao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351–2363, 2020.\n\nYunhui Long, Boxin Wang, Zhuolin Yang, Bhavya Kailkhura, Aston Zhang, Carl Gunter, and Bo Li. G-pate: Scalable differentially private data generator via private aggregation of teacher discriminators. Advances in Neural Information Processing Systems, 34:2965–2977, 2021.\n\nMi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity: Classifier calibration for federated learning with non-iid data. Advances in Neural Information Processing Systems, 34:5972–5984, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nMilad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In SP, 2019.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nMyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. arXiv preprint arXiv:2006.05148, 2020.\n\nLichao Sun, Jianwei Qian, and Xun Chen. LDP-FL: practical private aggregation in federated learning\n\nwith local differential privacy. In IJCAI, 2021.\n\nYue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In AAAI Conference on Artificial Intelligence, volume 1, pp. 3, 2022.\n\nZhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xin He, Bo Han, and Xiaowen Chu. Virtual homogeneity learning: Defending against data heterogeneity in federated learning. arXiv preprint arXiv:2206.02465, 2022.\n\nOm Thakkar, Galen Andrew, and H. Brendan McMahan. Differentially private learning with adaptive clipping. CoRR, abs/1905.03871, 2019. URL http://arxiv.org/abs/1905.03871.\n\nFlorian Tramer and Dan Boneh. Differentially private learning needs better features (or much more\n\ndata). arXiv preprint arXiv:2011.11660, 2020.\n\nAleksei Triastcyn and Boi Faltings. Federated learning with bayesian differential privacy. In Big\n\nData. IEEE, 2019.\n\nDirk van der Hoeven. User-specified local differential privacy in unconstrained adaptive online\n\nlearning. In NeurIPS, 2019.\n\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.\n\nFederated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020a.\n\nJianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611–7623, 2020b.\n\nKang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek, and H. Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. In IEEE TIFS, 2020.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking\n\nmachine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nQiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1–19, 2019.\n\nTehrim Yoon, Sumin Shin, Sung Ju Hwang, and Eunho Yang. Fedmix: Approximation of mixup\n\nunder mean augmented federated learning. arXiv preprint arXiv:2107.00233, 2021.\n\nDanni Yuan, Xiaoyan Zhu, Mingkui Wei, and Jianfeng Ma. Collaborative deep learning for medical\n\nimage analysis with differential privacy. In GLOBECOM, 2019.\n\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pp. 7252–7261. PMLR, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nLin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. Fine-tuning global model via data-free knowledge distillation for non-iid federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10174–10183, 2022.\n\nNanxuan Zhao, Zhirong Wu, Rynson WH Lau, and Stephen Lin. What makes instance discrimination\n\ngood for transfer learning? arXiv preprint arXiv:2006.06606, 2020.\n\nYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated\n\nlearning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.\n\nLigeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In NeurIPS, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 VISUAL\n\nWe show the visualization of data distribution in Figure 4. The LDA partition and the #C = 2 partition have the label skew and the quantity skew simultaneously. And the Subset partition only has the label skew.\n\n(a) LDA (α = 0.1)\n\n(b) #C = 2\n\n(c) Subset\n\nFigure 4: Data distribution in various FL heterogeneity scenario. Different colors denote different labels and the length of each line denote data number.\n\nA.2 GLOBALLY SHARED DATA\n\nWe display the globally shared data xp from four different datasets and the raw data x to compare our privacy protection. Firstly, the raw data in Figure 3 shown in Figure 5.\n\nFigure 5: Raw Data in Model Inversion Attack.\n\nFigure 6: Globally Shared Data xp on CIFAR-10.\n\nA.3 MORE DETAILS OF FEDPD\n\nAlgorithm 1 give us an intuitive explanation of how we deploy FedPD on FL algorithm e.g., FedAvg and Algorithm 2 illustrates the procedure to generate globally shared data.\n\n14\n\nclientclientclientUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Globally Shared Data Generation Server input: generation process communication round T , noise mean μ, noise level σ2 Client k’s input: local epochs Q, local datasets Dk\n\nInitialization: server distributes the initial model w0, θ0 to all clients, Server Executes: for each round t = 1, 2, · · · , T do\n\nserver samples a subset of clients Sg ⊆ {1, ..., K}, n ← (cid:80) server communicates wt, θt to all clients for each client k ∈ Sg in parallel do\n\nk,q−1 ← ClientGenerationTraining(k, wt, θt, μ, σ2)\n\n|Di|\n\ni∈Sg\n\nk,q−1θt+1\n\nwt+1 end for wt+1, θt+1 ← (cid:80)|Sg|\n\n|Di|\n\nn wt\n\nk,Q−1, (cid:80)|Sg|\n\nk∈Sg\n\n|Di|\n\nn θt\n\nk,Q−1\n\nk∈Sg\n\nend for for all clients k with k = 0, · · · , K do\n\nk ← SharedDataGeneration(wk, θk, μ, σ2)\n\nk to server to construct globally shared dataset Ds\n\nDs send Ds\n\nend for\n\nClientGenerationTraining(k, w, θ, μ, σ2): for each local epoch q with q = 0, · · · , Q − 1 do\n\nwk,q+1, θk,q+1 ← PrivacyDistillation(wk, θk, μ, σ2) using Eq.1\n\nend for Return wk,Q−1, θk,Q−1 to server\n\nA.3.1 HYPER-PARAMETERS\n\nWe fine-tuned learning rates in 0.0001, 0.001, 0.01, 0.1 and report the best results and corresponding learning rate. In most case, we use 0.01 as the learning rate except SCAFFOLD and FedNova in SVHN under the α = 0.1, E = 1, K = 100 setting, the learning rate is 0.0001 and 0.001, respectively. Batch size is set as 64 in when K = 10 and 32 for K = 100. The number of clients selected for aggregation on server side is 5 per round for K = 10, and 10 for K = 100. The noise level in our experients is N (0, 0.15)\n\nA.3.2 TRICK FOR FEDPD\n\nFigure 7: The pink line indicates 6.25% dataset to train VAE and Auxiliary 200 rounds jointly and the whole dataset for only VAE G(·; θ) training 200 rounds in the following. Furthermore, the yellow one uses the same data in the former 200 rounds as the pink line but the complete dataset to train Auxiliary Classifier A(·; ω).\n\nIn addition, we provide an insight experiment on the need for mixupdata (Zhang et al., 2017) augmentation in our approach shown in Figure 7. As we can see, the absence of data leads to poor generalization of the auxiliary classifier A on x and adequate data for VAE G still has a bad effects.\n\n15\n\n0100200300400Round20406080Test Accuracyonly train G(;)only train A(;)Under review as a conference paper at ICLR 2023\n\n(a) Test Accuracy on CIFAR-10 with different noise level σ2\n\n(b) Test Accuracy on SVHN with different noise level σ2\n\n(c) Test Accuracy on CIFAR-100 with different noise level σ2\n\nFigure 8: Privacy-Performance results on different datasets\n\n(a) α = 0.1, E = 1, K = 10\n\n(b) α = 0.1, E = 5, K = 10\n\n(c) α = 0.1, E = 1, K = 100\n\nFigure 9: Convergence comparison on CIFAR-10.\n\n(a) α = 0.1, E = 1, K = 10\n\n(b) α = 0.1, E = 5, K = 10\n\n(c) α = 0.1, E = 1, K = 100\n\nFigure 10: Convergence comparison on CIFAR-100.\n\n(a) α = 0.1, E = 1, K = 10\n\n(b) α = 0.1, E = 5, K = 10\n\n(c) α = 0.05, E = 1, K = 10\n\nFigure 11: Convergence comparison on SVHN.\n\nA.4 MORE EXPERIMENT RESULTS\n\nA.5 MORE RELATED WORK\n\nFederated Learning with heterogeneous data. In FL, all distributed clients jointly train a model across various distributed datasets for user privacy protection, while local data is not accessible to other clients. FedAvg (McMahan et al., 2017) is the first work proposed to reduce communication overhead and preserve privacy by more local training epochs and fewer communication rounds. However, some studies (Zhao et al., 2018; Li et al., 2022) have pointed out that the divergence between FedAvg and centralized training is slight in the IID case. But, in heterogeneous distribution, there is a considerable divergence between the different clients and centralized training, and the gap\n\n16\n\n0.10.20.3Noise Level 2958575Test AccuracyAcc468Privacy 0.10.20.3Noise Level 290929496Test AccuracyAcc468Privacy 0.10.20.3Noise Level 2657075Test AccuracyAcc468Privacy 02505007501000Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova0100200300400Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova02505007501000Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova02505007501000Round0204060Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova0100200300400Round0204060Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova02505007501000Round0204060Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova02505007501000Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova0100200300400Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova02505007501000Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNovaUnder review as a conference paper at ICLR 2023\n\n(a) α = 0.1, E = 1, K = 100\n\n(b) α = 0.1, E = 5, K = 10\n\n(c) α = 0.05, E = 1, K = 10\n\nFigure 12: Convergence comparison on FMNIST.\n\naccumulates during the FedAvg weighted aggregation, leading to the performance degradation of FL models.\n\nRecently, a series work propose new learning objective to calibrate the update direction of local training from being too far away from the global model. FedProx (Li et al., 2020) adds a L2 distance as the regularization term in the objective function and provides a theoretical guarantee of convergence. Similarly, a novel objective function is also introduced in FedIR (Hsu et al., 2020) over a mini-batch by self-normalized weights to address the non-identical class distribution. SCAFFOLD (Karimireddy et al., 2020) restricts the model using previous information. Besides, MOON (Li et al., 2021a) introduces constrastive learning at the model level to correct the divergence between clients and server.\n\nMeanwhile, recent works propose designing new model aggregation schemes. FedAvgM (Hsu et al., 2019) performs momentum on the server side. FedNova (Wang et al., 2020b) adopts normalized averaging method to eliminate objective inconsistency. A study (Cho et al., 2020) also indicates that biasing client selection with higher local loss can speed up the convergence rate. The coordinate-wise averaging of weights also induce noxious performance. FedMA (Wang et al., 2020a) conducts Bayesian non-parametric strategy for heterogeneous data. FedBN (Li et al., 2021c) focus on feature shift Non-IID and perform local batch normalization before averaging models.\n\nAnother existing direction for tackling data heterogeneity is sharing data. This line of works mainly to assemble the data of different clients to construct a global IID dataset, mitigating client drift by replenishing the lack of information of clients (Zhao et al., 2018). Existing methods include synthesizing data based on the raw data by GAN (Jeong et al., 2018; Long et al., 2021). However, the synthetic data is generally relatively similar to the raw data, leading to privacy leakage at some degree. Adding a noise to the shared data is another promising strategy (Chatalic et al., 2022; Cai et al., 2021). Some methods employ the statistics of data (Yoon et al., 2021; Shin et al., 2020) to synthesize for sharing, which still contains some raw data content. Other methods distribute intermediate features (Hao et al., 2021), logits (Chang et al., 2019; Luo et al., 2021), or learn the new embedding (Tan et al., 2022). These tactics will increase the difficulty of privacy protection because some existing methods can reconstruct images based on feature inversion methods (Zhao et al., 2020). Most of the above methods share information without a privacy guarantee or with strong privacy-preserving but poor performance, posing the privacy-performance dilemma.\n\nConcretely, in FD (Jeong et al., 2018) all clients leverages a generative model collaboratively for data generation in a homogeneous distribution. For a better privacy protection, G-PATE (Long et al., 2021) performs discriminators with local aggregation in GAN. Fed-ZDAC(Fed-ZDAS) (Hao et al., 2021), depending on which side to play augmentation, introduce zero-shot data augmentation by gathering intermediate activations and batch normalization(BN) statistics to generate fake data. Inspired by mixup data, MAFL (Yoon et al., 2021) propose FedMIx to share information by averaging local data which also brings about the privacy problem. Cronus (Chang et al., 2019) transmit the logits information while CCVR (Luo et al., 2021) collect statistical inforamtion of logits to sample fake data. FedFTG (Zhang et al., 2022) use a generator to explore input space of local model and transfer local knowledge to global model. FedDF (Lin et al., 2020) utilizes knowledge distillation based on unlabeled data or a generator and then conduct AVGLOGITS. The main difference between FedDF and FedPD is that our method distill the privacy kept locally rather than distilling knowledge. We provide multi steps to protect privacy with drastic performance gain.\n\n17\n\n02505007501000Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova0100200300400Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNova02505007501000Round20406080Test Accuracy [%]FedAvg+FedPDSCAFFOLD+FedPDFedProx+FedPDFedNova+FedPDFedAvgSCAFFOLDFedProxFedNovaUnder review as a conference paper at ICLR 2023\n\nDifferential privacy with federated learning. Recent works on model memorization and gradient leakage confirms that model parameters are seemingly secure. Carlini et.al (Carlini et al., 2019) found that unintended-and-persistent memorization of sensitive data occurs early during training with no relation to data rarity and model size. Training with differential privacy (Zhu et al., 2019)(Nasr et al., 2019) is a feasible solution to avoid serious consequences, albeit at some loss in utility.\n\nDifferential privacy is a framework to quantify to what extent individual privacy in a statistical dataset is preserved while releasing the established model over specific datasets. It has spawned a large set of research topics in data-releasing mechanism and noise-adding mechanism. Particularly, noise-adding mechanism has been widely utilized in various differentially private learning algorithms for protecting whether an individual is in the dataset or not.\n\nIn federated settings, training with differential privacy, i.e., adding noise to the model/data, originally aims to protect local information of each clients. Say, an adversary should not be able to discern whether a client’s data was used for early training. Here, we summarize some works with high citation or from top venue. Yuan et al (Yuan et al., 2019) apply differential privacy to protect medical images by adopting famous AlexNet and Gaussian mechanism. Huang et al (Huang et al., 2020) integrate an approximate augmented Lagrangian function and Gaussian noise mechanism for balancing utility and privacy in FL. Wei etal (Wei et al., 2020) perturb early-trained parameters locally by adding noises before uploading them to a server for aggregation. Both Huang et al and Wei et al are first (to their knowledge) to analyze the relation between convergence and utility in FL. Andrew et al (Thakkar et al., 2019) explore to set an adaptive clipping norm in federated setting rather than using a fixed one. They show that adaptive clipping to gradients can perform as well as any fixed clip chosen by hand.\n\nKim et al (Kim et al., 2021) provide a noise variance bound that guarantees local DP after multiple rounds of parameter aggregations. They introduce a trilemma in privacy, utility, and transmission rate of a federated stochastic gradient decent. Hoeven et al (van der Hoeven, 2019) introduce datadependent bounds and apply symmetric noise in online learning, which allows data provider to pick noise distribution. Triastcyn et al (Triastcyn & Faltings, 2019) adapt the notion of Bayesian differential privacy to federated learning and make necessary analyses on privacy guarantee. Sun et al (Sun et al., 2021) explicitly vary ranges of weights at different layers in a DNN, and shuffle high-dimensional parameters at an aggregation for easing explodes of privacy budgets. All works above start to apply DP and its variants to federated setting for different goals/scenarios, which thus provide underlying security as DP guarantees.\n\nA.6 DIFFERENTIAL PRIVACY\n\nProof of Theorem 3.4 is here.\n\nProof.\n\nDefinition A.1. (Privacy Loss). Let M : D → R be a randomized mechanism with input domain D and range R. Let D, D′ be a pair of adjacent dataset and aux be an auxiliary input. For an outcome o ∈ R, the privacy loss at o is defined by:\n\nL(o)\n\nPri\n\n≜ log\n\nPr[M(aux, D) = o] Pr[M(aux, D′) = o]\n\n(4)\n\nWe need to compute the privacy loss on an outcome o as a random variable when the random mechanism operates on two adjacent database D and D′. Privacy loss is a random variable that accumulates the random noise added to the algorithm/model.\n\nWe aim at an exact analysis on privacy via compositing multiple random mechanisms. For simplification, we start with a particular random mechanism M† and then generalize it. The mechanism M† does not depend on database or the query but relies on hypothesis hp. For hp = 0, the outcome Oi of M† i is independent and identically distributed from a discrete random distribution Ohp=0 ∼ P †,0. P †,0(o) is defined to be: δ for o = 0; (1 − δ)eε/(1 + eε) for o = 1; (1 − δ)/(1 + eε) for o = 2; 0 for o = 3. For hp = 1, the outcome Oi of M† i is Ohp=1 ∼ P †,1. P †,1(o) is defined to be: 0 for o = 0; (1 − δ)/(1 + eε) for o = 1; (1 − δ)eε/(1 + eε) for o = 2; δ for o = 3.\n\nLet R(ε, δ) be privacy region of a single access to M†. Privacy region consists of two rejection regions with errors, i.e., rejecting true null-hypothesis (type-I error) and retaining false null-hypothesis\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nk be M†\n\n(type-II error). Let ε† k, δ† i ’s parameters for defining privacy. R(M, D, D′) of any mechanism M can be regarded as an intersection of {(ε† k)} privacy regions. For an arbitrary mechanism M, we need to compute its privacy region using the (ε† k) pairs. Let D, D′ be neighboring databases and O be the outputting domain. Define (symmetric) P, P ′ to be probability density function of the outputs M(D), M(D′), respectively. Assume a permutation π over O such that P ′(o) = P(π(o)). Let S denote the complement of a rejection region. Since R(M, D, D′) is convex, we have\n\nk, δ†\n\nk, δ†\n\n1 − P(S) ≥ −eε†\n\nk P ′(S) + 1 − δ†\n\nk ⇒ P(S) − eε†\n\nk P ′(S) ≤ δ†\n\nk\n\nDefine Dtε† (P, P ′) = maxS⊆O{P(S) − eε† k ∈ [0, ∞)] s.t. P(o) = eε† ε† k = Dtε† mechanisms M1, . . . , Mi. By accessing M† By algebra on two discrete distributions,\n\nk P ′(o), δ†\n\nk\n\nP ′(S)}. Thus, M’s privacy region is the set: {(ε† k) : (P, P ′)}. Next, we consider composition on random j=1P †,hp(oj). i , P(O1,hp = o1, . . . , Oi,hp = oi) = Πi\n\nk, δ†\n\nDt(i−2j)ε(P i, (P ′)i) = 1 − (1 − δ)i + (1 − δ)i\n\nj−1 (cid:88)\n\nl=0\n\n(cid:16) i\n\nl (eε(i−l) − eε(i−2j+l))\n\n/(1 + eε)k\n\n(cid:17)\n\nHence, privacy region is an interaction of i regions, parameterized by 1 − (1 − ˆδ)Πi(1 − δi).\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper tries to tackle the problem of data heterogeneity in federated learning with differential privacy by attempting to divide the features into private and generalizable features. They ask the question of what is necessary to share to learn global models and can the remaining data stay on the client as local models. They provide empirical evaluation of their claims.\n\n# Strength And Weaknesses\n\nStrengths:\n1. The paper proposes an interesting idea to try and improve federated learning training with differential privacy.\nWeaknesses:\n1. Theorem 3.4 is the standard adaptive composition result of differential privacy and it is not clearly stated that this is not one of the papers contributions.\n2. Theorem 3.5 is an informal statement at best, and in no way shows why the necessary noise to be added is lesser to just the generalizable features. Is the L2 sensitivity lowered in some way?\n3. In the experimental evaluations, the high near central training accuracies make it seem like there is some personalization done using the private data on each client $x_s$. Is this the case? If it is the case, the baseline of no local personalization is quite weak. Unfortunately, I couldn't verify this since the code wasn't attached.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is broadly well written, but lacks rigor where it is required.\nThe idea is novel, but the theoretical results (as claimed) are non-existent and empirical improvements are not clear.\n\n# Summary Of The Review\n\nIn light of the strengths and weaknesses mentioned above, the weaknesses outweigh the strengths and I recommend the paper for rejection.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "MULTI-TASK OPTION LEARNING AND DISCOVERY FOR STOCHASTIC PATH PLANNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThis paper addresses the problem of reliably and efficiently solving broad classes of long-horizon stochastic path planning problems. Starting with a vanilla RL formulation with a stochastic dynamics simulator and an occupancy matrix of the environment, our approach computes useful options with policies as well as high-level paths that compose the discovered options. Our main contributions are (1) data-driven methods for creating abstract states that serve as end points for helpful options, (2) methods for computing option policies using auto-generated option guides in the form of dense pseudo-reward functions, and (3) an overarching algorithm for composing the computed options. We show that this approach yields strong guarantees of executability and solvability: under fairly general conditions, the computed option guides lead to composable option policies and consequently ensure downward refinability. Empirical evaluation on a range of robots, environments, and tasks shows that this approach effectively transfers knowledge across related tasks and that it outperforms existing approaches by a significant margin.\n\n1\n\nINTRODUCTION\n\nAutonomous robots must compute long-horizon motion plans (or path plans) to accomplish their tasks. Robots use controllers to execute these motion plans by reaching each point in the motion plan. However, the physical dynamics can be noisy and controllers are not always able to achieve precise trajectory targets. This prevents robots from deterministically reaching a goal while executing the computed motion plan and increases the complexity of the motion planning problem. Several approaches (Schaul et al., 2015; Pong et al., 2018) have used reinforcement learning (RL) to solve multi-goal stochastic path planning problems by learning goal-conditioned reactive policies. However, these approaches work only for short-horizon problems (Eysenbach et al., 2019). On the other hand, multiple approaches have been designed for handling stochasticity in motion planning (Alterovitz et al., 2007; Sun et al., 2016), but they require discrete actions for the robot. This paper addresses the following question: Can we develop effective approaches that can efficiently compute plans for long-horizon continuous stochastic path planning problems? In this paper, we show that we can develop such an approach by learning abstract states and then learning options that serve as actions between these abstract states.\n\nAbstractions play an important role in long-horizon planning. Temporally abstracted high-level actions reduce the horizon of the problem in order to reduce the complexity of the overall decisionmaking problem. E.g., a task of reaching a location in a building can be solved using abstract actions such as “go from room A to corridor A”, “reach elevator from corridor A”, etc., if one can automatically identify these regions of saliency. Each of these actions is a temporally abstracted action. Not only do these actions reduce the complexity of the problem, but they also allow the transfer of knowledge across multiple tasks. E.g, if we learn how to reach room B from room A for a task, we can reuse the same solution when this abstract action is required to solve some other task.\n\nReinforcement learning allows learning policies that account for the stochasticity of the environment. Recent work (Lyu et al., 2019; Yang et al., 2018; Kokel et al., 2021) has shown that combining RL with abstractions and symbolic planning has enabled robots to solve long-horizon problems that require complex reasoning. However, these approaches require hand-coded abstractions. In this paper,\n\n1\n\nwe show that the abstractions automatically learned (Shah & Srivastava, 2022) can be efficiently combined with deep reinforcement learning approaches.\n\nThe main contributions to this paper are: (1) A formal foundation for constructing a library of two different types of options that are task-independent and transferable, (2) A novel approach for auto-generating dense pseudo-reward function in the form of option guides that can be used to learn policies for synthesized options, and (3) An overall hierarchical algorithm approach that uses combines these automatically synthesized abstract actions with reinforcement learning and uses them for multi-task long-horizon continuous stochastic path planning problems. We also show that these options are composable and can be used as abstract actions with high-level search algorithms.\n\nOur formal approach provides theoretical guarantees about the composability of the options and their executability using an option guide. We present an extensive evaluation of our approach using two separates sets of automatically synthesized options in a total of 14 settings to answer three critical questions: (1) Does this approach learn useful high-level planning representations? (2) Do these learned representations support transferring learning to new tasks?\n\nThe rest of the paper is organized as follows: Sec. 2 some of the existing approaches that are closely related to our approach; Sec. 3 introduces a few existing ideas used by our approach; Sec. 4 presents our algorithm; Sec. 5 presents an extensive empirical evaluation of our approach.\n\n2 RELATED WORK\n\nTo the best of our knowledge, this is the first approach that uses a data-driven approach for synthesizing transferable and composable options and leverages these options with a hierarchical algorithm to compute solutions for stochastic path planning problems It builds upon the concepts of abstraction, stochastic motion planning, option discovery, and hierarchical reinforcement learning and combines reinforcement learning with planning. Here, we discuss related work from each of these areas.\n\nMotion planning is a well-researched area. Numerous approaches( (Kavraki et al., 1996; LaValle, 1998; Kuffner & LaValle, 2000; Pivtoraiko et al., 2009; Saxena et al., 2022)) have been developed for motion planning in deterministic environments. Kavraki et al. (1996); LaValle (1998); Kuffner & LaValle (2000) develop sampling-based techniques that randomly sample configurations in the environment and connect them for computing a motion plan from the initial and goal configurations. Holte et al. (1996); Pivtoraiko et al. (2009); Saxena et al. (2022) discretize the configuration space and use search techniques such as A⇤ search to compute motion plans in the discrete space.\n\nMultiple approaches (Du et al., 2010; Kurniawati et al., 2012; Vitus et al., 2012; Berg et al., 2017; Hibbard et al., 2022) have been developed for performing motion planning with stochastic dynamics. Alterovitz et al. (2007) build a weighted graph called stochastic motion roadmap (SMR) inspired from the probabilistic roadmaps (PRM) (Kavraki et al., 1996) where the weights capture the probability of the robot making the corresponding transition. Sun et al. (2016) use linear quadratic regulator -- a linear controller that does not explicitly avoid collisions -- along with value iteration to compute a trajectory that maximizes the expected reward. However, these approaches require an analytical model of the transition probability of the robot’s dynamics. Tamar et al. (2016) develop a fully differentiable neural module that approximates the value iteration and can be used for computing solutions for stochastic path planning problems. However, these approaches (Alterovitz et al., 2007; Sun et al., 2016; Tamar et al., 2016) require discretized actions. Du et al. (2010); Van Den Berg et al. (2012) formulate a stochastic motion planning problem as a POMDP to capture the uncertainty in robot sensing and movements. Multiple approaches (Jurgenson & Tamar, 2019; Eysenbach et al., 2019; Jurgenson et al., 2020) design end-to-end reinforcement learning approaches for solving stochastic motion planning problems. These approaches only learn policies to solve one path planning problem at a time and do not transfer knowledge across multiple problems. In contrast, our approach does not require discrete actions and learn options that are transferrable to different problems.\n\nSeveral approaches have considered the problem of learning task-specific subgoals. Kulkarni et al. (2016); Bacon et al. (2017); Nachum et al. (2018; 2019); Czechowski et al. (2021) use intrinsic reward functions to learn a two-level hierarchical policy. The high-level policy predicts a subgoal that the low-level goal-conditioned policy should achieve. The high-level and low-level policies are then trained simultaneously using simulations in the environment. Paul et al. (2019) combine imitation learning with reinforcement learning for identifying subgoals from expert trajectories and\n\n2\n\nbootstrap reinforcement learning. Levy et al. (2019) learn a multi-level policy where each level learns subgoals for the policy at the lower level using Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) for control problems rather than long-horizon motion planning problems in deterministic settings. Kim et al. (2021) randomly sample subgoals in the environment and use a path planning algorithm to select the closest subgoal and learn a policy that achieves this subgoal.\n\nNumerous approaches (Stolle & Precup, 2002; ̧Sim ̧sek et al., 2005; Brunskill & Li, 2014; Kurutach et al., 2018; Eysenbach et al., 2019; Bagaria & Konidaris, 2020; Bagaria et al., 2021) perform hierarchical learning by identifying task-specific options through experience collected in the test environment and then use these options (Sutton et al., 1999) along with low-level primitive actions. Stolle & Precup (2002); ̧Sim ̧sek et al. (2005) lay foundation for discovering options in discrete settings. They collect trajectories in the environment and use them to identify high-frequency states in the environment. These states are used as termination sets of the options and initiation sets are derived by selecting states that lead to these high-frequency states. Once options are identified, they use Q-learning to learn policies for these options independently to formulate Semi-MDPs (Sutton et al., 1999). Bagaria & Konidaris (2020) learn options in a reverse fashion. They compute trajectories in the environment that reaches the goal state. In these trajectories, they use the last K points to define an option. They use these points to define the initiation set of the option and the goal state is used as the termination set. They continue to partition rest of collected trajectories similarly and generate a fixed number (a hyperparameter) of options.\n\nApproaches for combining symbolic planning with reinforcement learning (Silver & Ciosek, 2012; Yang et al., 2018; Jinnai et al., 2019; Lyu et al., 2019; Kokel et al., 2021) use pre-defined abstract models to combine symbolic planning with reinforcement learning. In contrast, our approach learns such options (including initiation and termination sets) as well as their policies and uses them to compute solutions for stochastic path planning problems with continuous state and action spaces. Now, we discuss some of the existing concepts required to understand our approach.\n\n3 BACKGROUND\n\nX\n\n=\n\nXfree [X obs be the configuration space of a robot R and let O be the Motion planning Let set of obstacles in a given environment. Given a collision function f : Xfree represents the set of configurations that are not in collision with any obstacle o O such that f (x) = 0 and Xobs represents the set of configurations in collision such that f (x) = 1. Let xi 2X free be the initial configuration of the robot and xg 2X free be the goal configuration of the robot. The motion planning\n\nX!{ 2\n\n0, 1\n\n, }\n\nproblem can be defined as:\n\nDefinition 1. A motion planning problem Xfree [X obs is the configuration space, f is the collision function, xi is an initial configuration, and xg is a goal configuration.\n\nis defined as a 4-tuple\n\n, f, xi, xgi\n\n, where\n\nM\n\nhX\n\n=\n\nX\n\nh\n\nx0, . . . , xni\n\nsuch that x0 = xi, xn = xg, and\n\nA solution to a motion planning problem is a motion plan ⌧ . A motion plan is a sequence of ⌧, f (x) = 0. Robots use configurations controller that accepts sequenced configurations from the motion plan and generates controls that take the robot from one configuration to the next configuration. In practice, these environment dynamics are noisy, which introduces stochasticity in the overall motion planning problem. This stochasticity can be handled by computing a motion policy ⇡ : that takes the current configuration of the robot and generates the next waypoint for the robot.\n\nX!X\n\n2\n\nx\n\n8\n\nIn this work, we define the stochastic path planning problem as a Markov decision process continuous stochastic shortest path (SSP) problem (Bertsekas & Tsitsiklis, 1991). A continuous stochastic shortest path problem is defined as a 5-tuple is a continuous [0, 1] state space (configuration space of the robot), is a transition function, C : is a set of goal states. Discount factor  is set to 1 for this work. A solution to an SSP is a policy ⇡ : that maps states to actions that take the robot to the goal states and minimizes the cumulative cost. Dynamic programming methods such as value iteration (VI) or policy iteration (PI) can be used to compute such policies when every component of the MDP is known. When one or more SSP components are unknown various reinforcement learning (RL) approaches (Watkins, 1989; Mnih et al., 2015; Lillicrap et al., 2016; Haarnoja et al., 2018) can be used to learn policies.\n\nR is a cost function, s0 is an initial state, and G\n\nis a set of continuous actions, T :\n\nX⇥A⇥X ! ✓S\n\n, T, C, s0, G i\n\nwhere\n\nX!A\n\nX!\n\nhX\n\nA\n\nA\n\nX\n\n,\n\n3\n\nis a set of options.\n\nOptions framework Options framework (Sutton et al., 1999) models options as temporal abstracbe the state-space of an MDP. An option o is defined as tion over primitive actions in an MDP. Let defines the initiation set of an option o. An option o can be applied in a 3-tuple hIo, o,⇡ oi Io ✓S state s iff s 2I o. o is the termination set of an option o. Execution of an option o terminates when defines policy for the option o. Sutton et al. (1999) the agent reaches a state s define Semi-Markov Decision Process (SMDP) by adding available options to the set of primitive actions available in the MDP. Solution for an SMDP is a policy ⇡ : and\n\no. ⇡o :\n\nS!A\n\n=\n\n2\n\nS\n\n.\n\n ̄ A\n\nwhere ̄ A\n\nS!\n\nA[O\n\nO Shah & Srivastava (2022) use region-based Voronoi diAutomated synthesis of abstractions agrams (RBVDs) to define abstractions for motion planning problems. To construct effective abstractions, they use critical regions (Molina et al., 2020) to construct the RBVD. Intuitively, critical regions are defined as regions that have a high density of solutions for a given class of problems but low sampling probability under uniform sampling distribution. Shah & Srivastava (2022) use critical regions to define RBVDs as follows: Definition 2. Given a configuration space x\nand a region  is a partitioning of that forall x\n\n, let dc define minimum distance between a configuration . Given a set of regions  and a robot R, a region-based Voronoi diagram  such\n\nsuch that for every Voronoi cell i 2\n\n= i, dc(x, i) < dc(x, j) and each i is connected.\n\n there exists a region i 2\n\n✓X X\n\n2X\n\nX\n\n and forall j 6\n\n2\n\nIn this framework, abstract states are defined using a bijective function ` : Voronoi cell to an abstract state. The abstraction function ↵ : if and only if there exists a Voronoi cell such that x said to be in the high-level abstract state s RBVDs along with critical regions to construct abstractions and synthesize options.\n\nthat maps each is defined such that ↵(x) = s is s) if ↵(x) = s. In this work, we use\n\n and `( ) = s. A configuration x\n\n(denoted by x\n\nX!S\n\n!S\n\n2X\n\n2S\n\n2\n\n2\n\n4 OUR APPROACH\n\n,\n\n6\n\n7\n\nX\n\nO \n\n4 if abstraction is not constructed then 5\n\n1 ⇥ 2 if ⇥ is not trained then train ⇥ using Etrain 3\n\nget_abstract_states(xi,xg); ,si,sg);\n\nOutput: A policy ⇧ composed of options get_critical_regions_predicter();\n\ninitial configuration xi, goal configuration xg\n\npredict_critical_regions(etest,⇥); construct_RBVD(etest,); synthesize_option_endpoints(, );\n\nAlgorithm 1: Stochastic Hierarchical Abstractionguided Robot Planner (SHARP) Input: Training environments Etrain, c-space\n\nAlg. 1 outlines our approach -- stochastic hierarchical abstraction-guided robot planner (SHARP) -- for computing a policy for the given stochastic path planning problem. Our approach requires a robot simulator to compute path plans. It takes the occupancy matrix of the environment along with the initial and goal configuration of the robot as the input and produces a policy composed of options as the output. Learning the critical regions The first step in our approach is to identify critical regions in the given environment. We use a deep neural network to identify these critical regions. We train (lines 1-3) this neural network with the kinematic model of the robot and a set of training environments Etrain that do not include the test environments. We discuss this in detail in Sec. 4.1. Once the network is trained, the same network is used to identify critical regions (line 5) in all test environments for the same robot. Synthesizing option endpoints Alg. 1 uses these identified critical regions to construct RBVD (line 6) and generate a set of abstract states (line 7). We then use this RBVD to construct a collection of options and synthesize option endpoints for the given environment (line 7). Sec. 4.2 explains this step in detail. These option are synthesized only once per the robot and the environment. They are reused for each subsequent pair of initial and goal pairs. We refer to option endpoints as options for brevity. Options as abstract actions Let be the set of synthesized options. Our approach uses these options as high-level abstract actions. It considers the initiation sets of these options as preconditions\n\nG train o.policy; adjust the cost of the option o;\n\n8 si, sg 9 p A*_search( 10 ⇧ = empty_list; 11 ⇡0 12 ⇧.add(⇡0); 13 foreach o 14\n\n19 ⇡n+1 20 ⇧.add(⇡n+1); 21 return ⇧;\n\nif o.policy does not exist then\n\nlearn_policy(on ,xg);\n\ncompute_option_guide(\n\n⇧.add(o.policy);\n\nlearn_policy(xi,\n\n15\n\n16\n\n17\n\n18\n\no,o);\n\np do\n\no1 );\n\nO\n\nO\n\n2\n\nI\n\nI\n\n4\n\n \n \n \n \n for the abstract actions and the termination sets as their effects. We identify the abstract initial and goal state for the robot’s initial and goal configuration (line 8) and use the A⇤ search to compute a sequence of identified abstract actions that takes the robot from the initial abstract state to the goal abstract state (line 9). We use the Euclidean distance between the termination set of the corresponding option and the goal configuration as the heuristic for the A⇤ search. The A⇤ search also requires the costs for the options which are initially unknown. Therefore, we use the Euclidean distance between the initiation and termination sets of an option as an approximation to its cost.\n\nh\n\no1, . . . , oni\n\nComputing policies for options Let p = be the sequence of options that takes the robot from its initial abstract state s0 to its goal abstract state sg. Alg. 1 then computes policies for each of the options in this sequence of options. It first starts by computing a policy ⇡0 that takes the robot from its initial configuration xi to some point in the initiation set of the first option o1 (line 11). Alg. 1 then starts computing the policy for every option in p. First, we check if a policy for an option is already computed from the previous call of Alg. 1 or not (line 14). If it is already computed, we use the same policy. Otherwise, our approach computes an option guide for the given option using its endpoints, and it then uses this option guide to compute a policy for the option (lines 15-16). Sec. 4.3 presents this in details. Lastly, Alg. 1 computes a policy that takes the robot from the termination set of the last option in p to the goal configuration xg (line 19).\n\nUpdating the option costs To efficiently transfer the learned option policies, our approach needs to update the option costs that A⇤ search uses to compute the sequence of options. We update this cost (line 17) by collecting rollouts of the learned policy and using the average number of steps from the initiation set to the termination set as an approximation of the cost of the option. Now, we explain each step of our approach in detail.\n\n4.1 LEARNING CRITICAL REGIONS\n\nOur approach first needs to identify critical regions (Sec. 3) to synthesize options in the given environment. Recall that critical regions are regions in the environment that have a high density of solutions for the given class of problems but at the same time, they are hard to sample under uniform distribution over the configuration space. The training data is generated by solving randomly sampled motion planning problems. Input into the network is an occupancy matrix of the environment that represents the free space and obstacles in the given environment. Labels represent critical regions in the given environment.\n\nThese critical region predictors are environment independent and they are also generalizable across robots to a large extent. Furthermore, the approach presented here directly used the open-source critical regions predictors made available by Shah & Srivastava (2022). These predictors are environment independent and need to be trained only once per kinematic characteristics of a robot. E.g., the non-holonomic robots used to evaluate our approach (details in Sec. 5) are different from those used by Shah & Srivastava (2022) but we used the critical regions predictor developed by them for a rectangular holonomic robot.\n\nShah & Srivastava (2022) use 20 training environments (Etrain) to generate the training data. For Etrain, they randomly sample 100 goal configurations. Shah & each training environment etrain 2 Srivastava (2022) randomly sample 50 initial configurations for each goal configuration and compute motion plans for them using an off-the-shelf motion planner and a kinematic model of the robot. They use UNet (Ronneberger et al., 2015) with Tanh activation function for intermediate layers and Sigmoid activation for the last layer. They use the weighted logarithmic loss as the loss function. 4 to minimize the loss Lastly, they use ADAM optimizer (Kingma & Ba, 2014) with learning rate 10 function for 50, 000 epochs.\n\n4.2 SYNTHESIZING OPTION ENDPOINTS\n\nThe central idea of this paper is to synthesize transferable options using the set of critical regions  and the RBVD . Our approach uses these critical regions and RBVD for identifying option endpoints, i.e., initiation and termination sets. In this work, we define two types of options: 1. Options between centroids of two critical regions -- centroid options ( of two pairs of abstract states -- interface options (\n\nOc) and 2. Options between interfaces\n\nOi). Now we define each of these options.\n\n5\n\nS be a set of abstract states defined using the RBVD .\n\nCentroid options Centroid options are defined using centroids of two neighboring critical regions. Intuitively, these options define abstract actions that transition between a pair of critical regions. Formally, let  be the set of critical regions and be the RBVD constructed using the critical regions . Let be an abstract state in the RBVD with the corresponding critical region Definition 3. Let si 2S . Let d be the Euclidean distance measure and let t define a threshold distance. Let ci be the i 2 centroid of the critical region ri. A centroid region of the critical region ri with the centroid ci is si ^ defined as a set of configuration points\n\nd(x, ci) < t\n\n2\n\nx\n\nx\n\n}\n\n{\n\n|\n\n.\n\nWe use this definition to define the endpoints for the centroid options as follows:\n\nbe a pair of neighboring states in an RBVD constructed using the set  be the critical regions for the abstract states si and sj and let ci hIij, iji\n\nDefinition 4. Let si, sj 2S of critical regions . Let i, j 2 and cj be their centroids regions. The endpoints for a centroid option are defined as a pair Iij = ci and ij = cj. such that Interface options are defined using intersections of high-level states. Intuitively, Interface options these options define high-level temporally abstracted actions between intersections of pairs of abstract states and take the robot across the high-level states. To construct interface options, we first need to be a set of abstract identify interface regions between a pair of neighboring abstract states. Let states defined using the RBVD . We define interface region as follows: Definition 5. Let si, sj 2S critical regions. Let dc(x, ) define the minimum euclidean distance between configuration x 2X . Let p be a configuration such that dc(p, i) = dc(p, j) that is, p and some point in a region  is on the border of the Voronoi cells that define si and sj. Given the Euclidean distance measure d and a threshold distance t, an interface region for a pair of neighboring states (si, sj) is defined as a set\n\nbe a pair of neighboring states and i and j be their corresponding\n\nd(x, p) < t\n\n⇢X\n\n(x\n\nS\n\nx\n\nx\n\nsj)\n\n{\n\n|\n\nsi _\n\n2\n\n2\n\n^\n\n. }\n\nWe use this definition of interface regions to define endpoints for the interface options as follows:\n\nDefinition 6. Let si, sj, sk 2S be abstract states in the RBVD such that si and sj are neighbors and sj and sk are neighbors. Let ˆij and ˆjk be the interface regions for pairs of high-level states (si, sj) and (sj, sk). The endpoints for an interface option are defined as a pair such that\n\nhIoijk , oijk i\n\nIoijk = ˆij and oijk = ˆjk.\n\nWe construct a collection of options for the given environment using these definitions for endpoints of centroid and interface options. Let define a neighborhood function such that\n\nS⇥S !{\n\nare neighbors. We define the set of centroid options as with endpoints computed as in Def. 4 . Similarly, we define the set of with endpoints computed\n\n(sj, sk) = 1\n\n(si, sj) = 1\n\n(si, sj) = 1 iff si, sj 2S (si, sj) = 1 si, sj V oijk|8 Oi =\n\n{ interface options as as in Def. 6.\n\nsi, sj, sk V\n\nOc =\n\noij|8\n\n0, 1\n\n^V\n\n} {\n\nV\n\nV\n\n}\n\n}\n\n:\n\n...\n\nq\"\n\nq!\n\nq$\n\nq#\n\nq#&\"\n\nq#&$\n\n# = #'\n\n# ≠ #'\n\n# ∈ I%\"\n\n# ∈ '%$\n\n # ∈ I%!\n\n # ∉ I%\"\n\n # ∉ I%!\n\n # ∉ I%#\n\n # ∉ '%$\n\nFigure 1: A policy composed of a collection of options o1, . . . , on for a sequence of distinct adjacent abstract states s1, . . . , sn.\n\nOur approach uses this collection of options to compute a composed policy. Let s1, . . . , sn be a set of distinct adja- (si, si+1) = 1. cent states such that o1, . . . , on} Let be the set of centroid or interface options for these sequence of adjacent states. The composed policy ⇧O is a finite state automaton as shown in Fig. 1. Recall that for a sequence of options o1, . . . , on, Alg. 1 computes ⇡0 and ⇡n+1 as special cases. For a controller state qi, ⇧(x) = ⇡i(x) where ⇡i represents the policy for option oi 2O . The controller makes a transition qi ! existence of a composed policy for our approach. Recall that for a configuration space critical regions , the RBVD induces a set of abstract state\n\nqi+1 when the robot reaches a configuration x\n\n2I oi+1 . Now, we prove the\n\nand an abstraction function ↵.\n\nand a set of\n\nO\n\n=\n\nX\n\nV\n\n{\n\nTheorem 4.1. Let x1 and xn be the initial and goal configurations of the robot such that s1 = ↵(x1) and sn = ↵(xn). Let be the set of centroid or interface options. If there exists a sequence of distinct abstract states s1, . . . , sn (si, si+1) = 1 then there exists a composed policy ⇧ such that the resultant configuration such that after the termination of every option in ⇧ would be the goal configuration xn.\n\nbe the neighborhood function as defined above. Let\n\nS⇥S!{\n\n0, 1\n\nO\n\nV\n\nV\n\n}\n\n:\n\nS\n\n6\n\nProof. (Sketch) The proof directly derives from the definition of the endpoints for the centroid and interface options. Given a sequence of adjacent abstract states s1, . . . , sn, Def. 4 and 6 ensures a sequence of options o1, . . . , on such that i = Ii+1. This implies that an option can be executed once the previous option is terminated. Given this sequence of options o1, . . . , on, according to the definition of the compound policy, there exists a compound policy ⇧ such that for every pair Ioj = oi . Thus, we can say that if every option in ⇧ terminates, then the of options oi, oj 2\n\nresulting configuration would be the goal configuration.\n\n⇧,\n\n4.3 SYNTHESIZING AND USING OPTION GUIDES\n\nOnce we identify the endpoints -- initiation and termination sets -- for the option, we use these sets to compute option guides for these options. We use option guides to formulate a reward function that is used to train policies for them. The option guides contain two components: a guide path and a pseudo reward function. We formally define an option guide as follows:\n\nDefinition 7. Given an option oi 2O as 4-tuple Ri is the pseudo reward function.\n\nhIi, i,\n\nGi, Rii\n\nwhere\n\nwith option endpoints\n\nIi and i are endpoints of the option oi,\n\nhIi, ii\n\n, an option guide Goi is defined Gi is the guide path, and\n\nWe now define both components of the option guide. A guide path is a motion plan from the initiation set to the termination set of an option. Recall that ↵ is an abstraction function defined using the RBVD . We defune a guide path as follows: be the configuration space of the robot R. Let d be the Euclidean distance Definition 8. Let Ii and ci define centroids for the initiation measure. Given an option oi with endpoints and termination sets respectively. Given a threshold distance t and the Euclidean distance measure d, a guide path such that p1 = c every point pi 2G i, pi 2\n\nGi for the option oi is a sequence of points [p1, . . . , pn] in the configurations space\n\nIi , pn = ci , for each pair of consecutive points pi, pj 2G i , d(pi, pj) < t, and for\n\nIi) or pi 2\n\nhIi, ii\n\n↵(i).\n\n, let c\n\n↵(\n\nX\n\nX\n\nHere, we abuse the notation and use the abstraction function with a set of low-level configurations rather than a single configuration. Intuitively, ↵( Ii) = . It can be similarly computed for i. We generate this guide path using a sampling-based motion planner HARP (Shah & Srivastava, 2022) and the kinematic model of the robot.\n\n2I i}\n\n↵(x)\n\n|8\n\nx\n\n{\n\nThe guide path is used to define a pseudo reward function R for the option. It is a dense reward function that provides a reward to the robot according to the distance of the robot from the nearest point in the guide path and the distance from the last point of the guide path. For an option oi, we define the dense pseudo reward function Ri : Definition 9. Let oi be an option with endpoints Given a configuration x Euclidean distance measure. The pseudo reward function Ri(x) is defined as:\n\nGi = [p1, . . . , pn] be the guide path. , let n(x) = pi define the closest point in the guide path. Let d be the\n\nR as follows:\n\nhIi, ii\n\nand let\n\nX!\n\n2X\n\nRi(x) =\n\n8 <\n\nrt rp\n\n(d(x, n(x)) + d(n(x), pn))\n\ni if x 2\nif x /\n{ 2S otherwise\n\n↵(\n\nIi),↵ (i)\n\n}\n\n:\n\nHere rt is a large positive reward and rp is a large negative reward that can be tuned as hyperparameters. Intuitively, we wish to automatically generate a dense pseudo-reward function based on the information available form the environment. Thus rather than providing only a sparse reward function (e.g. “+1 when you reach the termination set of the option”), we develop the pseudo reward function that captures this condition (first case), a penalty for straying away from the source and target abstract states (second case), and a reward for covering more of the option guide (third case).\n\nGiven an option oi with endpoints , our approach can use arbitrary reinforcement learning algorithm to learn the policy for the option. For our empirical evaluation (Sec. 5), we use Soft Actor Critic (Haarnoja et al., 2018).\n\nand an option guide Gi =\n\nhIi, ii\n\nhIi, i,\n\nGi, Rii\n\n5 EMPIRICAL EVALUATION\n\nWe conduct an extensive evaluation of our approach in a total of four different environments with two different robots. All experiments were conducted on a system running Ubuntu 18.04 with AMD\n\n7\n\nFigure 2: The figure shows the time taken by our approach and baselines to compute path plans in the test environment. Results for an additional environment is in the Appendix B. The x-axis shows the problem instance and the y-axis shows the time in seconds. The reported time for our approach includes time to predict critical regions, construct abstractions, and learn policies for all the options. Each subsequent problem instance uses trained policies for options from the previous problems if there exists one. Timeout was set to 2400 seconds. The numbers are averaged over 5 independent trails. The transparent bars for SAC show that training was stopped as it reached the timeout.\n\nRyzen Threadripper 3960X 24-core processor and two NVidia 3080 GPUs. We implement our approach using PyBullet (Coumans & Bai, 2016) robotics simulator. PyBullet does not explicitly model stochasticity in the movement of the robot. Therefore, we use random perturbations in the actions to introduce stochasticity in the environment while training and use default controllers to evaluate the learned policies.\n\nWe implement our approach using PyTorch and Stable Baselines. We use a 2 layered neural network with each layer containing 256 neurons in every layer to learn policy for each option. The input to the network is the current configuration of the robot and a vector to the nearest point on the guide path for the current option. The network outputs the next point for the robot to reach. We use rt = 1000 and rp = 100 for the pseudo reward function with maximum learning steps set to 150k to learn policy for each option. We evaluate our model for 20 episode every 10k steps and stop the training if we achieve an average reward of 500. Our code and data are available in the supplementary material.\n\nEvaluating our approach We evaluate our approach using a total of 7 environments. They are inspired by indoor household and office environments. Appendix A shows our test environments. Dimensions of the first four environments (App. A.1) are 15m 15m. The rest of the environments (App. A.2) are of the size 75m 75m . These environments were not included in the set of training environments used to train the network that identifies critical regions. We generate 5 motion planning tasks (P1-P5) for each environment to evaluate our approach. We use two non-holonomic robots to evaluate our approach in these four environments: the ClearPath Husky robot (H) and the AgileX Limo (L). The Husky is a 4-wheeled differential drive robot that can move in one direction and rotate in place. The Limo is also a 4-wheeled robot that can move in one direction and has a steering.\n\n⇥\n\n⇥\n\nWe considered a set of recent approaches (Kulkarni et al., 2016; Lillicrap et al., 2016; Levy et al., 2019; Bagaria & Konidaris, 2020) before selecting two approaches to evaluate against our approach. First, we compare our approach against vanilla soft actor-critic (SAC) (Haarnoja et al., 2018). SAC is an off-policy deep reinforcement learning approach and learns a single policy for taking the robot from its initial configuration to the goal configuration. We use the same network architecture as ours for the SAC neural network. We use the terminal reward of +1000 and a step reward of 1. We also evaluate our approach against a sampling-based motion planner RRT (LaValle, 1998) with replanning. We compute a path plan with RRT and execute it. If the robot does not reach the goal after executing the entire path plan, then we compute a new path plan from the configuration where the robot ended. We continue this loop until either the robot reaches the goal configuration or we reach the timeout.\n\nAnalysis of the results We thoroughly evaluated our approach to answer three critical questions: (1) Does this approach learn useful high-level planning representations? (2) Do these learned representations support transferring learning to new tasks?\n\n8\n\nFigure 3: The figure shows the success rate of our approach and baselines in the test environments. Results for an additional environment is in the Appendix B. The x-axis shows the problem instance and the y-axis shows the fraction of successful executions of the model out of 20 test executions. We used the final policy for our approach and SAC. RRT computed a new plan for each execution with a timeout set to 2400 seconds. The numbers are averaged over 5 independent trails.\n\nFig. 2 shows the time taken by our approach for learning policies compared to the baselines SAC and RRT. Fig. 2 shows that our approach was able to learn policies significantly faster than the baselines. In most cases, our approach was able to compute solutions in half of the time taken by the baselines. Vanilla SAC was not able to reach the threshold average reward of +500 in any environment before the timeout of 2400 seconds. This shows that our approach was able to learn useful high-level planning representations that improve the efficiency of the overall stochastic motion planning.\n\nFig. 3 shows the success rate for our approach and the baselines. Our approach achieves the goal a significantly higher number of times compared to replanning-based deterministic RRT. Our approach was able to consistently reach the goal configuration more than 80% of times. On the other hand, RRT was able to reach the goal only 50% of times in the given timeout of 2400 seconds. For larger environments (E-F), SAC was not able to solve a single problem. We also conducted a thorough analysis of options being used across problems P1-P5 in each environment. Appendix C shows the fraction of options reused by our approach across P1-P5. These reuse rates combined with the success rates show that our approaches is able to successfully transfer learned options across new tasks.\n\n6 CONCLUSION\n\nIn this paper, we presented the first approach that uses a data-driven process to automatically identify transferable options. Our approach synthesizes two different types of options: centroid options and interface options. It uses these options with our hierarchical algorithm for solving stochastic path planning problems. We also provide a way to automatically generate pseudo reward functions for synthesized options to efficiently learn their policies. We show that learned options can be transferred to different problems and used to solve new problems. Our formal framework also provides guarantees on the compossibility of the generated options.\n\nOur empirical evaluation on a large variety of problem settings shows that our approach significantly outperforms other approaches. Through our empirical evaluation, we show that by combining reinforcement learning with abstractions and high-level planning we can improve its efficiency and compute solutions faster. We also show that using reinforcement learning to solve path planning problems in stochastic environments yields robust plans that perform better than deterministic plans computed using replanning-based motion planners.\n\nREFERENCES\n\nRon Alterovitz, Thierry Siméon, and Ken Goldberg. The stochastic motion roadmap: A sampling framework for planning with markov motion uncertainty. In Robotics: Science and systems, 2007.\n\n9\n\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Proc. NeurIPS, 2017.\n\nPierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proc. AAAI, 2017.\n\nAkhil Bagaria and George Konidaris. Option discovery using deep skill chaining. In Proc. ICLR,\n\n2020.\n\nAkhil Bagaria, Jason K Senthil, and George Konidaris. Skill discovery for exploration and planning\n\nusing deep skill graphs. In Proc. ICML, 2021.\n\nJur van den Berg, Sachin Patil, and Ron Alterovitz. Motion planning under uncertainty using differential dynamic programming in belief space. In Robotics Research, pp. 473–490. Springer, 2017.\n\nDimitri P Bertsekas and John N Tsitsiklis. An analysis of stochastic shortest path problems. Mathe-\n\nmatics of Operations Research, 16(3):580–595, 1991.\n\nEmma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning. In\n\nProc. ICML, 2014.\n\nErwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics\n\nand machine learning. http://pybullet.org, 2016.\n\nKonrad Czechowski, Tomasz Odrzygó ́zd ́z, Marek Zbysi ́nski, Michał Zawalski, Krzysztof Olejnik, Yuhuai Wu, Łukasz Kuci ́nski, and Piotr Miło ́s. Subgoal search for complex reasoning tasks. In Proc. NeurIPS, 2021.\n\nYanzhu Du, David Hsu, Hanna Kurniawati, Wee Sun, Lee Sylvie, CW Ong, and Shao Wei Png. A pomdp approach to robot motion planning under uncertainty. In In International Conference on Automated Planning & Scheduling, Workshop on Solving Real-World POMDP Problems. Citeseer, 2010.\n\nBen Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging\n\nplanning and reinforcement learning. In Proc. NeurIPS, 2019.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proc. ICML, 2018.\n\nMichael Hibbard, Abraham P Vinod, Jesse Quattrociocchi, and Ufuk Topcu. Safely: Safe stochastic motion planning under constrained sensing via duality. arXiv preprint arXiv:2203.02816, 2022.\n\nRobert C Holte, Maria B Perez, Robert M Zimmer, and Alan J MacDonald. Hierarchical a*: Searching\n\nabstraction hierarchies efficiently. In AAAI/IAAI, Vol. 1, pp. 530–535, 1996.\n\nYuu Jinnai, David Abel, David Hershkowitz, Michael Littman, and George Konidaris. Finding\n\noptions that minimize planning time. In Proc. ICML, 2019.\n\nTom Jurgenson and Aviv Tamar. Harnessing reinforcement learning for neural motion planning. In\n\nProc. RSS, 2019.\n\nTom Jurgenson, Or Avner, Edward Groshev, and Aviv Tamar. Sub-goal trees a framework for goalbased reinforcement learning. In International Conference on Machine Learning, pp. 5020–5030. PMLR, 2020.\n\nLydia E Kavraki, Petr Svestka, J-C Latombe, and Mark H Overmars. Probabilistic roadmaps for path planning in high-dimensional configuration spaces. IEEE transactions on Robotics and Automation, 12(4):566–580, 1996.\n\nJunsu Kim, Younggyo Seo, and Jinwoo Shin. Landmark-guided subgoal generation in hierarchical\n\nreinforcement learning. In Proc. NeruIPS, 2021.\n\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proc. ICLR, 2014.\n\n10\n\nHarsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, and Prasad Tadepalli. Reprel: Integrating relational planning and reinforcement learning for effective abstraction. In Proc. ICAPS, 2021.\n\nJames J Kuffner and Steven M LaValle. Rrt-connect: An efficient approach to single-query path\n\nplanning. In Proc. ICRA, 2000.\n\nTejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Proc. NeurIPS, 2016.\n\nHanna Kurniawati, Tirthankar Bandyopadhyay, and Nicholas M Patrikalakis. Global motion planning under uncertain motion, sensing, and environment map. Autonomous Robots, 33(3):255–272, 2012.\n\nThanard Kurutach, Aviv Tamar, Ge Yang, Stuart J Russell, and Pieter Abbeel. Learning plannable\n\nrepresentations with causal infogan. In Proc. NeruIPS, 2018.\n\nSteven M LaValle. Rapidly-exploring random trees: A new tool for path planning. 1998.\n\nAndrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierarchies\n\nwith hindsight. In Proc. ICLR, 2019.\n\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proc. ICLR, 2016.\n\nDaoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson. Sdrl: interpretable and data-efficient\n\ndeep reinforcement learning leveraging symbolic planning. In Proc. AAAI, 2019.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n\nDaniel Molina, Kislay Kumar, and Siddharth Srivastava. Learn and link: Learning critical regions for\n\nefficient planning. In Proc. ICRA, 2020.\n\nOfir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical\n\nreinforcement learning. In Proc. NeurIPS, 2018.\n\nOfir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation learning\n\nfor hierarchical reinforcement learning. In Proc. ICLR, 2019.\n\nSujoy Paul, Jeroen Vanbaar, and Amit Roy-Chowdhury. Learning from trajectories via subgoal\n\ndiscovery. In Proc. NeurIPS, 2019.\n\nMihail Pivtoraiko, Ross A Knepper, and Alonzo Kelly. Differentially constrained mobile robot\n\nmotion planning in state lattices. Journal of Field Robotics, 26(3):308–333, 2009.\n\nVitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-\n\nfree deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\n\nimage segmentation. In Proc. MICCAI, 2015.\n\nDhruv Mauria Saxena, Tushar Kusnur, and Maxim Likhachev. Amra*: Anytime multi-resolution\n\nmulti-heuristic a. In Proc. ICRA. IEEE, 2022.\n\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.\n\nIn Proc. ICML, 2015.\n\nNaman Shah and Siddharth Srivastava. Using deep learning to bootstrap abstractions for hierarchical\n\nrobot planning. In Proc. AAMAS, 2022.\n\n11\n\nDavid Silver and Kamil Ciosek. Compositional planning using optimal option models. In Proc.\n\nICML, 2012.\n\nÖzgür ̧Sim ̧sek, Alicia P Wolfe, and Andrew G Barto. Identifying useful subgoals in reinforcement\n\nlearning by local graph partitioning. In Proc. ICML, 2005.\n\nMartin Stolle and Doina Precup. Learning options in reinforcement learning.\n\nIn International\n\nSymposium on abstraction, reformulation, and approximation, pp. 212–223. Springer, 2002.\n\nWen Sun, Jur van den Berg, and Ron Alterovitz. Stochastic extended lqr for optimization-based motion planning under uncertainty. IEEE Transactions on Automation Science and Engineering, 13(2):437–447, 2016.\n\nRichard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211, 1999.\n\nAviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks.\n\nAdvances in neural information processing systems, 29, 2016.\n\nJur Van Den Berg, Sachin Patil, and Ron Alterovitz. Motion planning under uncertainty using iterative local optimization in belief space. The International Journal of Robotics Research, 31 (11):1263–1278, 2012.\n\nMichael P Vitus, Wei Zhang, and Claire J Tomlin. A hierarchical method for stochastic motion\n\nplanning in uncertain environments. In Proc. IROS. IEEE, 2012.\n\nChristopher JCH Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, London,\n\n1989.\n\nFangkai Yang, Daoming Lyu, Bo Liu, and Steven Gustafson. Peorl: Integrating symbolic planning\n\nand hierarchical reinforcement learning for robust decision-making. In Proc. IJCAI, 2018.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe authors propose to, for a specific robot, learn to recognize possible intermediary goals for motion tasks which generalize to new previously unseen maps with static obstacles. These possible intermediary motion goals (options) are either centroids (one per region) and interfaces (one per pair of connected regions). Abstract actions are formed by the transition between two options. RL policys are learned for motion of abstract actions, and shown in the evaluation to generalize to new maps. Motion planning takes place first at the abstract action level, and the plan is realized by following the policy of each abstract action, in sequence, until the robot reach the goal. The approach is compared to, and shown to outperform, SAC and RRT in both computation time and motion goal success rate (within a timelimit of 1200 seconds).\n\n# Strength And Weaknesses\n\nStrength:\n* Abstract actions address one of the brittleness-issues of contemporary DRL. Generalization to new maps is demonstrated, which is a major contribution of the paper.\n* Abstract actions makes the approach more transparent and interpretable - important properties from the perspective of Trustworthy AI. An abstract plan is much easier to understand, explain and monitor than a DRL policy.\n* The approach present an integrated solution, from critical region learning & detection to the hierarchical algorithm that use abstract actions to fine motion plans. It necessitates that the individual parts works in a real way, such that the integrated whole works.\n* Connections to graph search and symbolic action/plan representations (reminds me a bit about landmarks or similar from the automatic planning field) allow the method to use and combine the strengths of different fields (A* and RL, symbols and state-space etc.)\n\nWeaknesses:\n* \\mathcal{S} is used for both state space (configuration space) and for abstract states? Since the state space \\mathcal{S} is equal to \\mathcal{X}, why not use \\mathcal{X} instead?\n* \"This shows that using RL for stochastic motion planning produces robust solutions.\": It is shown to be more robust than RRT (in these environments), but 90% success rate is not *robust*.\n* Not necessarily something that has to change, but there are possibly more suitable baselines to compare with from the path/motion planning literature. Large-scale path planning has and still is often approached using hierarchical planning, e.g. by A* over an \"abstract\" graph, then A* in between the nodes of the graph [1]. Modern non-learning based motion planning approaches (e.g. lattice-based motion planning [2]) use translation invariant motion primitives (generated offline using optimal control) as *actions* in e.g. A*-search to generate physically feasible trajectories in large and complex environments in real-time, with both static and dynamic obstacles. Such methods would provide probably provide a more suitable baseline than RRT (with or without hierarchical comparisons.)\n* Why call it \"Multi-Task\" when it is only path planning? I get that it is for many possible different path goals.\n\n[1] Holte, Robert C., et al. \"Hierarchical A*: Searching abstraction hierarchies efficiently.\" AAAI/IAAI, Vol. 1. 1996.\n\n[2] Pivtoraiko, Mihail, Ross A. Knepper, and Alonzo Kelly. \"Differentially constrained mobile robot motion planning in state lattices.\" Journal of Field Robotics 26.3 (2009): 308-333.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear. I really like Algorithm 1, for clarity. With source code in the appendix (later a link on github?) then the work should be reproducible, but I didn't test the code. From an RL perspective, the ideas and the approach is novel.\n\n# Summary Of The Review\n\nThe proposed approach is interesting, well-integrated and show benefits over RL without abstract actions for motion planning.\nIdeas presented are in part explored in the literature in related fields. Since these are important for the current work it might be good to highlight the connections.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nINCOMPLETE TO COMPLETE MULTIPHYSICS FORECASTING - A HYBRID APPROACH FOR LEARNING UNKNOWN PHENOMENA\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nModeling complex dynamical systems where only partial knowledge of their physical mechanisms is available is a crucial problem across all scientific and engineering disciplines. Purely data-driven approaches, which only make use of an artificial neural network and data, often fail to accurately simulate the evolution of the system dynamics over a sufficiently long time and in a physically consistent manner. Therefore, we propose a hybrid approach that uses a neural network model in combination with an incomplete PDE solver that provides known, but incomplete physical information. In this study, we demonstrate that the results obtained from the incomplete PDEs can be efficiently corrected at every time step by the proposed hybrid neural network – PDE solver model, so that the effect of the unknown physics present in the system is correctly accounted for. For validation purposes, the obtained simulations of the hybrid model are successfully compared against results coming from the complete set of PDEs describing the full physics of the considered system. We demonstrate the validity of the proposed approach on a reactive flow, an archetypal multi-physics system that combines fluid mechanics and chemistry, the latter being the physics considered unknown. Experiments are made on planar and Bunsen-type flames at various operating conditions. The hybrid neural network - PDE approach correctly models the flame evolution of the cases under study for significantly long time windows, yields improved generalization, and allows for larger simulation time steps.\n\n1\n\nINTRODUCTION\n\nModeling and forecasting of complex physical systems described by nonlinear partial differential equations (PDEs) are central to various domains with applications ranging from weather forecasting (Kalnay, 2003), design of airplane wings (Rhie & Chow, 1983), to material science (Wheeler et al., 1992). Typically, a chosen set of PDEs are solved iteratively until convergence of the solution. Modeling complex physical dynamics requires a good understanding of the underlying physical phenomena. For cases where the complete physics information is missing, deep learning models can be employed to complete the physical description when additional data of the system is available. Deep learning methods have shown promises to account for these unknown components of the system (Yin et al., 2021). We consider a set of partial differential equations with partially unknown physics represented. The corresponding PDE model for a general state φ is given by\n\n∂φ ∂x where Pi models the known but incomplete PDE description, Sφ represents the unknown physics.\n\n∂2φ ∂x2 , ...) + Sφ,\n\n= Pi(φ,\n\n∂φ ∂t\n\n,\n\n(1)\n\nWithin the scope of this model, the influence of Sφ can lead to fundamentally different scenarios. A commonly targeted case is when the governing equations of the complete PDE description are computationally too expensive to solve, turbulence modeling in computational fluid dynamics (CFD) being a good example. In CFD, a spacial filtering is performed on the original governing PDEs. This step introduces unclosed terms in the model equations that correspond to unrepresented physics in equation 1, due to the effects of the filtered scales. This is a widely studied problem, where the use of deep learning models is currently being explored (Lapeyre et al., 2019; Kochkov et al., 2021; List\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\net al., 2022; Stachenfeld et al., 2022). In the following, we are targeting a more challenging problem, where increasing spacial resolution and/or reducing time-scales of the incomplete PDE solver does not lead to a converged full solution. Rather, the incomplete and complete PDEs produce drastically different solutions due to the unknown physics. The central learning objective is to correct this behavior and retrieve the evolution that would be obtained with the complete PDE description.\n\nOur work expands on the combination of incomplete PDE solvers and neural networks (NNs) (Yin et al., 2021; Takeishi & Kalousis, 2021) to account for the effects of an incomplete physics model. The NN aims to complete the PDE description, where the differences in complete and incomplete PDE solutions are beyond the effects of spacial and temporal scales. We showcase that combining the trained NN model with a differentiable solver for the incomplete PDE can accurately reproduce the physical solutions of the complete, multi-physics PDE solver with stable long-term rollouts.\n\nWe demonstrate the capabilities of this approach for an archetypal multi-physics system, namely a complex reactive flow which combines fluid mechanics and chemistry, where the latter is considered unknown. Reactive flow modeling has applications in numerous domains such as combustion processes in gas turbines (Lieuwen, 2012), climate modeling (Jacobson, 1999; Rolnick et al., 2022) and astrophysics simulations (Gamezo et al., 2003). Resolving the NavierStokes equations lies at the core of these problems, where additionally the transport of different species of relevance must also be accounted for, together with their production or consumption often following complex reaction mechanisms (Poinsot & Veynante, 2005). For chemically reacting flows, generation or consumption of multiple species via some chemical reaction are modeled using a net source term. It is a well-known fact that the incorporation of a detailed chemical kinetic mechanism in a reacting flow model can result in a stiff system of governing equations (Wanner & Hairer, 1996; Najm et al., 1998; Knio et al., 1999).\n\nComplete PDE\n\nIncomplete PDE\n\nWe consider the reactive flow simulation to be the complete PDE description, while the non-reactive flow simulation represents the incomplete PDE basis, where the chemical kinetic mechanisms are collected in the unknown physics term of equation 1. Figure 1 shows a visual example of the fundamental differences in system dynamics that can be caused by unknown reaction terms. We showcase the effectiveness of our approach for different cases of planar 2D premixed methane-air flames, and the varying transient evolution of Bunsen-type flames. We show that the proposed approach can handle large domains with highly resolved flames, which is closer to the practical flame domains used in many industrial applications. Specifically we concentrate on training a NN model to correct the spatio-temporal effects of energy and species transport source terms. We show that in addition to recovering the desired solutions, this approach overcomes inherent problems of temporal stiffness due to the complex reaction mechanism. Lastly, we demonstrate the applicability of neural network model to control the flow inlet velocities to arrive at desired flame shapes, when combined with a differentiable flow solver.\n\nFigure 1: The incomplete/non-reactive (top) and complete/reactive (bottom) PDE solvers we consider can yield fundamentally different evolutions, as shown here for a sample temperature field over time.\n\nt = 300\n\nt = 50\n\nt = 0\n\n2 RELATED WORK\n\nDeep learning methods have been widely used to model the solutions of partial differential equations (Lagaris et al., 1998; Long et al., 2018; Han et al., 2018; Bar-Sinai et al., 2019) and in particular, the Navier-Stokes equations (Thuerey et al., 2020; Fukami et al., 2019). These models can be very fast and do not suffer from the time-step stability issues associated with traditional numerical solvers. Nevertheless, as these purely data-driven approaches lack the physical understanding of the system being modeled, they generally fail in generalizing to other operating conditions (Kim et al., 2019; Lapeyre et al., 2019). To leverage the potential of deep learning in physical simulations, it is therefore necessary to incorporate some physical information within the deep learning framework.\n\n2\n\n8001000120014001600180020002200Temperature [k]Under review as a conference paper at ICLR 2023\n\nDeep learning models can enforce physical constraints partially through the loss function (Bar-Sinai et al., 2019; Raissi et al., 2019) or changes in neural network architecture (Greydanus et al., 2019; Lu et al., 2021). However, these approaches struggle to enforce physical constraints such as boundary conditions or predict the strong unsteadiness and chaotic nature of flows. Neural operators (Li et al., 2020b; Bhattacharya et al., 2021; Patel et al., 2021), specifically, the Fourier neural operator (FNO) of Li et al. (2020a) introduce an interesting line of work by learning mesh-free, infinite-dimensional operators with neural networks, but do not necessarily offer advantages for longer term predictions.\n\nHybrid approaches that combine machine learning techniques with numerical PDE solvers (Wang et al., 2020), have attracted a significant amount of interest due to their capabilities for generalization (Chen et al., 2018). In this context, neural networks are typically used to model or replace a part of the conventional PDE solver to improve aspects of the solving process. For example, Tompson et al. (2017), ̈Ozbay et al. (2021) and Ajuria Illarramendi et al. (2020) proposed a convolutional neural network based approach to solve the Poisson equation in CFD simulation. In recent years, a number of deep learning based models have been introduced to accurately model turbulent flows (Pathak et al., 2020; Stachenfeld et al., 2022; Dresdner et al., 2022). Um et al. (2020) and BelbutePeres et al. (2020) showed the advantages of training neural networks with differentiable physics to correct the numerical errors that arise in the discretization of PDEs. Sirignano et al. (2020) and Kochkov et al. (2021) additionally correct for closure error by integrating neural network with a differentiable CFD simulator. These approaches demonstrate the capabilities of neural networks to correct errors in a fast, under-resolved simulation.\n\nSimilar to the goals of our work, Yin et al. (2021) and Takeishi & Kalousis (2021) have introduced frameworks of augmenting incomplete physical dynamics with neural network models. These approaches demonstrate the applicability on ODE/PDE systems, which are weakly nonlinear and the unknown dynamics are linear combinations of the underlying flow fields. We expand on these works to explore the significantly more challenging scenario of reactive flows. These are characterised by a multi-physics system with nonlinear advective terms and strongly nonlinear dynamics, described by exponential source terms that exhibit nonlinear combinations of the flow fields.\n\n3 METHODOLOGY\n\nWe consider two different sets of PDEs with their associated numerical solver, which we denote as the incomplete PDE Pi and the complete PDE Pc. By evaluating Pc on an input state φt at time t, we can compute the points of the phase space sequences; φt+∆t = Pc(φt). Without loss of generality, we assume a fixed time-step ∆t and denote a state φt+∆t at next time instance as φt+1. The main goal of this study is to learn a correction function that models the effects of the unknown physical system on the incomplete PDE solver to obtain the complete PDE solutions. We train a correction model C(φ; θ) : X → X that maps Pi(φt) to φt+1, defined over a finite time interval [0, T ] and a spacial domain Ω ⊂ R2. The learning objective is to find the best possible correction function C(φ; θ) given the weights θ and an input flow state, φ. The model parameters θ are estimated from the complete PDE solution trajectories (φ0, φ1, .., φT ). The learned predictions obtained after repeatedly applying the corrector C and invoking Pi are denoted by ( ̃φ0, ̃φ1, .., ̃φT ).\n\n3.1 PARTIAL DIFFERENTIAL EQUATIONS FOR REACTIVE FLOWS\n\nThe physical system we investigate is a laminar premixed methane-air flame with a single step chemistry. It is governed by the following Navier-Stokes equations (Poinsot & Veynante, 2005)\n\n+\n\n∂ρ ∂t ∂ρuj ∂t\n\n∂ρui ∂xi\n\n= 0\n\nρCp(\n\n∂T ∂t\n\n+\n\n∂uiT ∂xi\n\n) = ̇ω′\n\nT +\n\n∂ ∂xi\n\n(λ\n\n+\n\n∂ρuiuj ∂xi\n\n= −\n\n∂p ∂xj\n\n+\n\n∂τij ∂xi\n\n∂ρYk ∂t\n\n+\n\n∂ρuiYk ∂xi\n\n= ̇ωk +\n\n∂ ∂xi\n\n(ρDk\n\n∂T ∂xi ∂Yk ∂xi\n\n)\n\n)\n\n(2)\n\nwhere ρ, ui, p, T , Yk denote the density, velocity, pressure, temperature, and species mass fractions of species k, respectively. τ , Dk, λ are the strain rate tensor, the diffusion coefficient of species k and mixture thermal conductivity. In addition, Cp denotes the mixture specific heat capacity. ̇ωk and ̇ω′\n\nT are the species reaction rate and heat release rate, respectively.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: (A) Multi-step training framework helps to learn the dynamics of complete PDE solver over longer rollouts. (B1) Details of the input flow state and predictor block used in a purely datadriven approach and (B2) the hybrid NN-PDE approach, where S denotes the concatenation of different fields to obtain the complete flow state ̃φ at next time step.\n\nF and υ′ We consider a one-step chemical system of the type υ′ O\nare the stoichiometric coefficients corresponding to fuel and oxidizer. The simplifications proposed by Williams (1985) and Mitani (1980) are used to model the reaction rates ̇ωk for each species. The fuel consumption rate ̇ωF is assumed to have the Arrhenius form.\n\nOO → products, where υ′\n\nF F + υ′\n\n ̇ωF = υ′\n\nF WF B1T β1(\n\nρYO WO T and the fuel source term ̇ωF are linked by ̇ω′\n\n)nO exp(−\n\nρYF WF\n\nEa RT\n\n)nF (\n\n).\n\nThe heat release source term ̇ω′ T = −Q ̇ωF , where Q is the heat of reaction per unit mass. Following Poinsot & Veynante (2005), parameters corresponding to a real-world methane-air flame are chosen as: B1 = 1.08107 uSI; β1 = 0; Ea = 83600 J/mole; nF = 1; nO = 0.5; Q = 50100 kJ/kg; Cp = 1450 J/(kgK). Taken together, the system of equations above is a challenging scenario even for classical solvers, and due to its practical relevance likewise a highly interesting environment for deep learning methods.\n\n(3)\n\n3.2 PROBLEM FORMULATION\n\nThe incomplete PDE solver solves the set of equations 2 without the source terms and reaction rates ̇ωk and ̇ω′ T , while the complete PDE solver solves the full set from equation 2. The neural network model, denoted by C(Pi(φ)|θ), corrects the incomplete/non-reacting flow states Pi(φ) to obtain the complete/reacting states Pc(φ) as shown in figure 2 (B2). The neural network is trained to model the effects of the unknown chemistry using parameters θ given an input flow state, φ = [ui, p, T, Yf , Yo]. As seen from equation 2, and equation 3, the temperature and species mass fractions fields are strongly coupled which significantly increases the prediction problem complexity. A slight error in one of the fields will quickly propagate into the other fields, making the predictions diverge. In the following, a subscript Cs(◦) will denote that the neural network C generates the field s, e.g., CT generating the temperature field. ̃φt+1 = [ui,t+1, pt+1, CT (Pi( ̃φt)|θ), CYf (Pi( ̃φt)|θ), CYo(Pi( ̃φt)|θ)] where ̃· indicates a corrected state. ui,t+1 and pt+1 are the time-advanced velocity and pressure field, respectively, predicted using the incomplete PDE solver.\n\n3.3 TRAINING METHODOLOGY\n\nWe employ a hybrid NN-PDE approach that augments a neural network model with a PDE solver (Um et al., 2020; Kochkov et al., 2021). In contrast to previous work, we use the incomplete PDE solver as a basis, and hence the solver does not converge to the desired solutions under refinement, as explained in 3.1. The neural network is integrated and trained in a loop with the incomplete PDE solver using stochastic gradient descent for m time steps, as shown in figure 2 (A,B2). Here, the number of temporal look-ahead steps, m, is an important hyper-parameter of the training process. Higher m provides the network with longer-term feedback at training time through the gradient rollouts. This gives the model improved feedback on how the time dynamics of the incomplete PDE solver affect the input states, and hence which corrections need to be inferred by the model.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nA central component of the hybrid NN-PDE model is the differentiable solver, which allows us to embed the solver for the incomplete PDE system in the training of a neural network. The differentiable solver acts as additional non trainable layers in the network. Instead, they provide derivatives of the outputs of the simulation with respect to its inputs and parameters. Here, we use the differentiable PDE solver from the phiflow framework of Holl et al. (2019) in combination with Tensorflow to obtain the non-reactive flow solver and reactive flow solver solutions. The marker-and-cell method (Harlow & Welch, 1965) is adopted to represent temperature, pressure, density, species mass fraction fields in a centered grid, and velocities in a staggered grid.\n\nA purely data-driven model (PDD) is used as a baseline. It employs a neural network model to learn the complete flow states Pc(φ) given an input flow state φS where φS = [T, Yf , Yo], as shown in figure 2 (B1). The new state predicted by the trained neural network model is ̃φS t |θ). For the hybrid NN-PDE as well as PDD, the neural network part of the predictor block in figure 2 consists of a fully convolutional neural network model. For both, we experimented with both the ResNet (He et al., 2016) and UNet (Ronneberger et al., 2015) architectures. We found that the ResNet performed the best for the PDD setting, while for the hybrid NN-PDE approach the UNet performed consistently better. Hence, the following results will use a ResNet for PDD models, and a UNet for the NN-PDE hybrids. Additional details of the neural network architectures are provided in the appendix. The network receives the input states as shown in figure 2. The output of the neural network model is used to obtain the corrected state ̃φt+1 as specified above. We constrain the mass fraction fields Yk to contain physical values in the range Yf ∈ [0, 0.05] and Yo ∈ [0, 1]. All models are trained for 100 epochs with a batch size of 3 and a learning rate of 0.0001.\n\nt+1 = C( ̃φS\n\nWe additionally compare against the Fourier neural operator (FNO) of Li et al. (2020a) as an example of a state-of-the-art neural operator method. The new state predicted by the trained t |θ) where ̃φS = [T, Yf , Yo] and C(◦) represents the Fourier model is given by ̃φS neural operator. All three approaches use an L2 based loss that is evaluated for m steps as L(θ) = (cid:80)t+m\n\nt+1 = C( ̃φS\n\nφ={T,Yf ,Yo} || ̃φn, Pc(φn)||2 .\n\nn=t+1\n\n(cid:80)\n\n4 NUMERICAL EXPERIMENTS\n\nWe consider a case of planar 2D premixed methane-air flame propagating in a quiescent mixture (Planar-v0) and two cases of the transient evolution of initially planar laminar premixed flame into a Bunsen-type flame under different inlet velocity conditions (uniform-Bunsen, nonUniformBunsen). We obtain the target data by considering the source terms as defined by equation 3 with the parameters mentioned in section 3.1.\n\nPlanar-v0 For the most basic case, the planar 2D flame model setup, we consider the reacting Navier-Stokes equations described in section 3 with zero inlet velocity i.e. ux = 0, uy = 0. We consider a square domain of size 0.05 m × 0.05 m with 32 × 32 resolution and closed boundary conditions. The simulation is initialized using a steep transition between a premixed methane-air mixture and burnt gases. Our training data consists of 6 simulations of 300 steps created by varying the equivalence ratio E. It represents the stoichiometric mixture (φ) of fuel Yf and oxidizer Yo mass fractions, i.e., E = φ Yf and thus fundamentally influences the dynamics of the chemical reaction. For the training data we use Etrain = {1.0, 0.9, 0.8, 0.7, 0.6}, while the test dataset contains Etest = {0.95, 0.85, 0.75, 0.65}.\n\nYo\n\nuniform-Bunsen In contrast to the planar case, the premixed methane-air mixture is now fed with a constant inlet velocity. The boundary conditions upstream, at y = 0, are (ux, uy)x,y=0 = (0, κ) where κ is a n-dimensional vector with constant amplitude. The target simulation contains a heat release rate term, and in this case the initial temperature field evolves into different Λ-shaped flames at the end of the 300th time step. Training and testing datasets are created by varying the equivalence ratio and inlet velocity amplitude U : Etrain = {1.0, 0.9, 0.8} and Utrain = {0.45, 0.4, 0.3}. The test dataset uses Etest = {0.95, 0.85} Utest = {0.43, 0.375, 0.325}. The length and temperature of the flame significantly vary depending on the inlet velocity and equivalence ratio provided. Further details on the boundary conditions and training data visualizations can be found in the appendix.\n\nnonUniform-Bunsen As a third case we consider the transient evolution of a premixed methaneair flame with non-uniform inlet velocity. The boundary conditions upstream, at y = 0, are\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: 1D cut of the planar flame simulation over 300 steps. The initial state is plotted in red, target state in green. Hybrid NN-PDE approach predicts physically accurate results over longer rollouts with correct flame temperature and relative displacement of flame front across different equivalence ratios.\n\nTable 1: Mean and standard deviation of errors over all time steps of all testsets. The Hybrid NNPDE approach outperforms all other baselines considered.\n\nE P\nA M\n\nE S\nM\n\nPlanar-v0 uniform-Bunsen nonUniform-Bunsen32 nonUniform-Bunsen100 Planar-v0 uniform-Bunsen nonUniform-Bunsen32 nonUniform-Bunsen100\n\nBaseline FNO 8.27 ± 5.51% 15.57 ± 8.72% 12.30 ± 7.98% -\n88316 ± 124394 220817 ± 167024 184860 ± 185694 -\n\nBaseline NN 6.33 ± 3.05% 7.58 ± 3.73% 12.48 ± 11.31% -\n34491 ± 43966 72563 ± 56919 130219 ± 142094 -\n\nHybrid NN-PDE 1.40 ± 0.65% 0.72 ± 0.37% 2.04 ± 1.39% 3.23 ± 3.76% 1122 ± 1300 721 ± 1007 9647 ± 13903 23293 ± 37451\n\nHybrid NN-PDE-dt 1.21 ± 0.39% 1.11 ± 0.47% 2.46 ± 1.61% 4.14 ± 4.99% 292 ± 296 1267 ± 1622 17569 ± 24392 26862 ± 47639\n\n(ux, uy)x,y=0 = (0, κ) where κ is a n-dimensional vector whose elements are each sampled from a uniform distribution from [0.2, 0.65]. We experiment with two different domain sizes, 32 × 32 (nonUniform-Bunsen32) and 100 × 100 (nonUniform-Bunsen100), as shown in figure 4. The larger domain size used is closer to the practical reactive flow domain used in CFD applications (Jaensch et al., 2017) with a highly resolved flame. These inlet velocity conditions generate complex flame shapes which increase the difficulty of the prediction problem. We consider simulation sequences with 500 steps as it takes longer time for the flame to reach the steady-state solution.\n\n5 RESULTS\n\nWe demonstrate the capabilities of the proposed learning approach to represent the complete PDE description with the aforementioned cases of increasing difficulty. We also study its ability to generalize to unseen operating conditions such as equivalence ratios, simultaneous variations in constant inlet velocity and equivalence ratio, and non-uniform inlet velocity profiles. The source code of our work will be published upon acceptance. As baselines, we compare against a purely data-driven approach; a neural network model with exactly the same look-ahead steps, and the Fourier neural operator of Li et al. (2020a) that likewise includes m look-ahead steps as discussed in section 3.3.\n\n5.1 QUALITATIVE AND QUANTITATIVE ANALYSIS\n\nTable 1 compares the mean absolute percentage errors (MAPE) and mean squared errors (MSE) of temperature field for all the cases discussed in section 4. For Planar-v0, the FNO and PDD approaches yield large errors with a MAPE of 8.27% and 6.33%, respectively. On the other hand, the hybrid NN-PDE model trained with 32 look-ahead steps reduces the error to 1.4%, and thus performs significantly better than the two baselines. This behavior is visualized in figure 3 with a 1D transverse cut of the simulation domain over 300 time-steps. The hybrid NN-PDE approach successfully captures the propagation of the flame.\n\nIn figure 3 (E), we also compare two important physical quantities, the flame temperature and relative displacement of the flame front, across different equivalence ratios. It can be seen that the hybrid NN-PDE model (blue circles) accurately predicts the flame temperature for different test cases (solid\n\n6\n\n0.000.020.04Domain length L [m]500750100012501500175020002250Temperature [k](A): FNO0.000.020.04L [m](B): PDD0.000.020.04L [m](C): Hybrid NN-PDE0.000.020.04L [m](D): Ground Truth0.950.850.750.65Equivalence ratio0.20.40.60.81.0Relative displacement of flame(E)170018001900200021002200Flame temperature [k]GTHybridPDDFNOUnder review as a conference paper at ICLR 2023\n\nFigure 4: Left : Inlet velocity profile used. (a-d) Temperature field prediction by hybrid NN-PDE approach over different steps given the inlet velocity profile. (e) Ground truth data. Right : Difference between ground truth data and hybrid NN-PDE output at last snapshot. Top to bottom: 32 × 32 resolution cases of (A) uniform-Bunsen, (B) nonUniform-Bunsen32, and (C) 100 × 100 testcase nonUniform-Bunsen100. ε represents the instantaneous MAPE.\n\nblue line). The relative displacement of the flame front is computed as | ̃xt− ̃x0|/|xt−x0|, where xt is the position along the flame normal at time t on the 1200K isotherm of the ground truth simulation, and ̃x denotes the predicted position. For all test cases, the hybrid approach accurately predicts the flame displacement over t = 300 steps, while the other approaches yield significant errors.\n\nFor uniform-Bunsen case, amongst the baseline models, PDD performs better with an error of 7.58% compared to FNO model which yields an error of 15.57%. A neural network model combined with an incomplete PDE solver for 32 look-ahead steps yields a significantly lower error of 0.72% as shown in table 1. This means, the hybrid approach reduces the errors by a factor of 10 over the baselines considered. Figure 4(A) shows the temporal evolution of the hybrid NN-PDE approach predicted over 300 time steps for the uniform-Bunsen case. It shows the results for a test case with U = 0.4375 and E = 0.95. It predicts a symmetric flame with accurate flame height and achieves very low instantaneous MAPE of 0.73% at t=300, when compared with the ground truth data.\n\nNext, we study a complex scenario of nonUniform-Bunsen flame with 32 × 32 resolution. The hybrid NN-PDE approach outperforms the PDD approach and FNO with an improvement of ∼ 80%. The MSE values show this trend even more clearly. The large standard deviations of the MSE numbers indicate that the predictions made by the baseline approaches contain substantial deviations from target values. Important to highlight is that, despite using the same training data and similar neural network architecture, with same look-ahead steps, the hybrid approach outperforms the PDD approach thanks to its learned collaboration with the incomplete PDE solver. It accurately reproduces the complete PDE behavior. Figure 4(B) highlights this with a visualization of the hybrid NN-PDE model predictions for nonUniform-Bunsen32 case. The trained model accurately predicts the flame simulation over long roll-outs of 500 steps and achieves the complex flame shape with a low, instantaneous MAPE of 2.38%. Finally, we showcase the ability of hybrid approach to predict the temporal evolution of highly resolved flames with the uniform-Bunsen100 scenario. Despite the increased complexity of the larger resolution, it achieves a very good overall MAPE of 3.23% over 12 test cases of 500 simulation steps. Figure 4(C) shows an example of physically accurate predictions made by the hybrid NN-PDE model. We omit the evaluation of baselines for high resolution cases as they do not succeed to model the flame dynamics for low resolution cases.\n\nEffect of training dataset size Figure 5 (A) shows that increasing the number of training sets has little or no effect on the prediction capabilities of the PDD models for nonUniform-Bunsen32 case. The hybrid model with 12 training sets achieves a MAPE of 12.49 ± 4.17%, an improvement of 38% over the PDD model with 32 training sets. This result strengthens the hypothesis that integrating the incomplete PDE solver into the neural network training yields a learning signal that fundamentally differs from that produced by training with precomputed data. The purely data-driven models cannot achieve the same level of accuracy even in the presence of large amounts of data.\n\nGeneralization to incorrect PDE parameters The proposed hybrid NN-PDE model is capable of completing the PDE description even if the underlying incomplete PDE solver does not have access\n\n7\n\n015300.420.440.46 uniform- Bunsen(A)Inlet Condition(a) t = 50(b) t = 100(c) t = 200(d) t = 300Hybrid NN-PDE(e) t = 300Ground Truthε=0.73Difference015300.30.40.5nonUniform-Bunsen32(B)(a) t = 200(b) t = 300(c) t = 400(d) t = 500(e) t = 500ε=2.380501000.30.4nonUniform-Bunsen100(C)(a) t = 200(b) t = 300(c) t = 400(d) t = 500(e) t = 500ε=3.57−300−200−1000100200300Under review as a conference paper at ICLR 2023\n\nFigure 5: (A) Effect of increasing training dataset size for PDD approach, over fixed testsets, is compared with equivalent (trained with same look-ahead steps m = 2) hybrid NN-PDE model. (B) Effect of temporal coarsening on PDD models trained with m = 32 look-ahead steps.\n\nFigure 6: Left : Constant inlet velocity profile used. (a-d) uniform-Bunsen case trained and inferred at double time-step 2 ∆tc, using hybrid NN-PDE approach. It achieves same flame shape at t = 150 for which the complete PDE solver requires 300 time-steps.\n\nto the correct parameters, referred to as ‘incomplete, incorrect PDE’. For this test we assume that the values of τ , Dk, λ in equation 2 are unknown and set to incorrect values. For the nonUniformBunsen32 case, the hybrid model with the incomplete, incorrect solver achieves an overall MAPE of 2.48 ± 1.20 %, a slight increase over the MAPE of 2.04 ± 1.39 % for the hybrid model with the incomplete, correct PDE. Additional details can be seen in appendix C.\n\n5.2 RELAXING TEMPORAL STIFFNESS IN THE PDE SOLVER\n\nTraditionally, the source terms and reaction rate terms involved in modeling the fast chemistry of reacting flows require the use of very small time-steps in simulations due to the stiffness of the chemical mechanisms. The incomplete PDE solver used in the hybrid NN-PDE approach, does not contain the source terms and reaction rate terms. Therefore, the time scales associated with the chemical reactions play a less important role in maintaining numerical stability, and it becomes possible for the solver to employ larger time steps. To illustrate this advantage, we train the neural network model with a time-step that is twice as large as the largest time-step ∆tc required for the complete PDE solver to yield a stable result.\n\nWe mimic the setup described in section 4, but now the incomplete PDE solver uses a time-step of 2∆tc, which is too large for the complete PDE solver by itself to converge to a solution. Figure 6, shown as a first example, indicates that the results obtained from the models trained with a larger time-step at t = {25, 50, 100, 150} are in good agreement with the target data for the difficult uniform-Bunsen case. The flame dynamics are predicted accurately while using less simulation steps. For the uniform-Bunsen case, the hybrid approach takes 8.23 ± 0.011s to infer one simulation run, whereas the complete PDE solver requires 15.21 ± 0.003s. Similar performance gains are observed across the other cases studied. Note that for the previous cases, the trained model incurs a negligible runtime overhead. In these cases, even for costly runs with high resolutions the incomplete PDE solver would not converge to the solutions of the complete description.\n\nThe last column of table 1 (Hybrid NN-PDE-dt) summarizes the errors of the large time-step approach for all four reactive cases considered. Despite an effectively doubled computational performance, this model achieves similar errors to the hybrid NN-PDE approach. This highlights the capabilities of learned, hybrid PDE solvers, which can produce these solutions without the stability problems exhibited by the complete PDE solver, while at the same time being more accurate than pure data-driven predictions.\n\nThe PDD approach is not restricted by the time-step of the PDE solver. Therefore, we investigate the performance of the PDD approach on the nonUniform-Bunsen32 case with larger time-steps as\n\n8\n\n4812162432Hybrid-12(A) N . f simulati ns in training set051015202530MAPEn nUnif rm-Bunsen32PDDΔtc2 Δtc4 Δtc8 Δtc(B) Data timestep051015202530MAPEn nUnif rm-Bunsen32PDDHybrid015300.360.370.380.39 uniform- BunsenInlet Condition(a) t = 25(b) t = 50(c) t = 100(d) t = 150Hybrid NN-PDE(e) t = 300Ground TruthDifference−1000100200300400500Under review as a conference paper at ICLR 2023\n\nFigure 7: Temperature field predictions achieved by the neural network model ̃T N N initial flame shape T0 to achieve target flame shape ̃T∗ by controlling flow velocity.\n\n∗\n\nfrom given\n\nshown in the figure 5 (B). PDD achieves higher MAPE of 22.44 % for 8∆tc setup as compared to the MAPE of 12.48 % for ∆tc setup. The PDD approach does not predict the dynamics accurately for any of the larger time-steps considered. Additional visualizations can be found in appendix C.\n\n5.3 FLAME SHAPE CONTROL\n\nWe lastly demonstrate the potential of our approach with a problem from multi-physics system control: we test the joint applicability of a neural network with reactive flow solver to obtain a desired flame shape. We use a predictor-corrector approach (Holl et al., 2019) with a differentiable physics loss function and control sequence refinement for nonUniform-Bunsen case with 32 × 32 resolution. The learning objective is to arrive at a target temperature field with the desired flame shape from an arbitrary, given initial configuration by controlling the velocity field. We assume that the temperature and mass-fraction fields are observable and only the velocity field is controllable. Figure 7 (a-e) shows the transition achieved and compares the target flame shape obtained using the neural network model with the target flame shape (T∗) for a test case. Figure 7 (f,g) compares the flame shape obtained for additional test cases. The neural network model is able to control the velocity of fuel-air mixture flow to obtain the desired flame shape. To quantify these results, the learned model achieves MAPE of 1.87 ± 0.73% over 50 test cases considered. It achieves an improvement of 52% compared to a baseline model without a predictor network for the long term prediction of temperature fields. This baseline model has a MAPE of 3.91 ± 2.19%. This case highlights the capabilities of differentiable multi-physics solvers, and that our full neural network model is able to reliably control the complex physics of the reactive flow over long time spans.\n\n6 CONCLUSIONS\n\nThese results demonstrate that neural networks can be employed for learning the completion of partial PDE solvers in the context of a multi-physics system. The trained neural network model works alongside a partial solver to accurately account for and predict the effects of unknown physics. The ability of the presented hybrid NN-PDE approach to predict the long-term temporal evolution of multi-physics systems was demonstrated on the specific case of a reacting flow in different configurations. When compared with a purely data-driven approach and state-of-the-art FNO approach, the qualitative and quantitative performance of the hybrid NN-PDE model is found to be clearly superior. The incomplete PDE description helps the neural network model recover the target simulation with a significantly improved accuracy. This hybrid NN-PDE model is able to predict the correct evolution of the important features of the multi-physics system (in our case, the flame interface and flame shape) for longer simulation steps in all scenarios and is able to generalize to other (initial and boundary) conditions of the system. This is demonstrated by variations in the equivalence ratio and inlet velocity forcing. We also show that such hybrid approach may have the added benefits of allowing to relax numerical constraints linked to the potential stiffness of the unknown physics.\n\nOur work represents a stepping stone for numerous avenues of future work, e.g., the proposed hybrid NN-PDE solver could be further utilized to predict and control the dynamics of other tightly coupled systems (Levy, 1999; Dowell & Hall, 2001). Here, the inclusion of transfer learning techniques would pose an interesting avenue for future work. We note that the proposed approach does not seek to replace the classical physical simulation approaches. However, it is an important step in the direction of harnessing the capabilities of neural network models using the knowledge of partial differential equations for complex multi-physical systems.\n\n9\n\n(a) t = 0Initial Sha e T0(b) t = 40(c) t = 50(d) t = 64NN Prediction ̃TNÑ(e) t = 64Target Sha e T̃̃TNÑ(f) t = 64 T̃(g) t = 648001200160020002400 T [k]Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nEkhi Ajuria Illarramendi, Antonio Alguacil, Micha ̈el Bauerheim, Antony Misdariis, Benedicte Cuenot, and Emmanuel Benazera. Towards an hybrid computational strategy based on deep learning for incompressible flows. In AIAA AVIATION 2020 FORUM, pp. 3058, 2020.\n\nYohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P Brenner. Learning data-driven discretizations for partial differential equations. Proceedings of the National Academy of Sciences, 116(31):15344–15349, 2019.\n\nFilipe de Avila Belbute-Peres, Thomas Economon, and Zico Kolter. Combining differentiable pde solvers and graph neural networks for fluid flow prediction. In International Conference on Machine Learning, pp. 2402–2411. PMLR, 2020.\n\nKaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and neural networks for parametric pdes. The SMAI journal of computational mathematics, 7:121–157, 2021.\n\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572–6583, 2018.\n\nEarl H Dowell and Kenneth C Hall. Modeling of fluid-structure interaction. Annual review of fluid\n\nmechanics, 33:445, 2001.\n\nGideon Dresdner, Dmitrii Kochkov, Peter Norgaard, Leonardo Zepeda-N ́u ̃nez, Jamie A Smith, Michael P Brenner, and Stephan Hoyer. Learning to correct spectral methods for simulating turbulent flows. arXiv preprint arXiv:2207.00556, 2022.\n\nKai Fukami, Koji Fukagata, and Kunihiko Taira. Super-resolution reconstruction of turbulent flows\n\nwith machine learning. Journal of Fluid Mechanics, 870:106–120, 2019.\n\nVadim N Gamezo, Alexei M Khokhlov, Elaine S Oran, Almadena Y Chtchelkanova, and Robert O Rosenberg. Thermonuclear supernovae: Simulations of the deflagration stage and their implications. Science, 299(5603):77–81, 2003.\n\nSamuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. Advances\n\nin Neural Information Processing Systems, 32:15379–15389, 2019.\n\nJiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.\n\nFrancis H Harlow and J Eddie Welch. Numerical calculation of time-dependent viscous incompress-\n\nible flow of fluid with free surface. The physics of fluids, 8(12):2182–2189, 1965.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nPhilipp Holl, Vladlen Koltun, and Nils Thuerey. Learning to control pdes with differentiable physics.\n\nIn ICLR, 2019.\n\nMark Z Jacobson. Fundamentals of atmospheric modeling. Cambridge university press, 1999.\n\nS Jaensch, M Merk, EA Gopalakrishnan, S Bomberg, T Emmert, RI Sujith, and W Polifke. Hybrid CFD/low-order modeling of nonlinear thermoacoustic oscillations. Proceedings of the Combustion Institute, 36(3):3827–3834, 2017.\n\nEugenia Kalnay. Atmospheric modeling, data assimilation and predictability. Cambridge university\n\npress, 2003.\n\nByungsoo Kim, Vinicius C Azevedo, Nils Thuerey, Theodore Kim, Markus Gross, and Barbara Solenthaler. Deep fluids: A generative network for parameterized fluid simulations. In Computer Graphics Forum, pp. 59–70. Wiley Online Library, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nOmar M Knio, Habib N Najm, and Peter S Wyckoff. A semi-implicit numerical scheme for reacting flow: Ii. stiff, operator-split formulation. Journal of Computational Physics, 154(2):428–467, 1999.\n\nDmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machine learning accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21), 2021.\n\nIsaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987–1000, 1998.\n\nCorentin J Lapeyre, Antony Misdariis, Nicolas Cazard, Denis Veynante, and Thierry Poinsot. Training convolutional neural networks to estimate turbulent sub-grid scale reaction rates. Combustion and Flame, 203:255–264, 2019.\n\nSalomon Levy. Two-phase flow in complex systems. John Wiley & Sons, 1999.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020a.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485, 2020b.\n\nTim C Lieuwen. Unsteady Combustor Physics. Cambridge University Press, 2012.\n\nBj ̈orn List, Li-Wei Chen, and Nils Thuerey. Learned turbulence modelling with differentiable fluid\n\nsolvers. arXiv preprint arXiv:2202.06988, 2022.\n\nZichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data.\n\nIn\n\nInternational Conference on Machine Learning, pp. 3208–3216. PMLR, 2018.\n\nLu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3):218–229, 2021.\n\nTohru Mitani. Propagation velocities of two-reactant flames. Combustion Science and Technology,\n\n21(3-4):175–177, 1980.\n\nHabib N Najm, Peter S Wyckoff, and Omar M Knio. A semi-implicit numerical scheme for reacting\n\nflow: I. stiff chemistry. Journal of Computational Physics, 143(2):381–402, 1998.\n\nRavi G Patel, Nathaniel A Trask, Mitchell A Wood, and Eric C Cyr. A physics-informed operator regression framework for extracting data-driven continuum models. Computer Methods in Applied Mechanics and Engineering, 373:113500, 2021.\n\nJaideep Pathak, Mustafa Mustafa, Karthik Kashinath, Emmanuel Motheau, Thorsten Kurth, and Marcus Day. Using machine learning to augment coarse-grid computational fluid dynamics simulations. arXiv preprint arXiv:2010.00072, 2020.\n\nThierry Poinsot and Denis Veynante. Theoretical and numerical combustion. RT Edwards, Inc.,\n\n2005.\n\nMaziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707, 2019.\n\nChae M Rhie and Wei-Liang Chow. Numerical study of the turbulent flow past an airfoil with trailing\n\nedge separation. AIAA journal, 21(11):1525–1532, 1983.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. Tackling climate change with machine learning. ACM Computing Surveys (CSUR), 55(2):1–96, 2022.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nJustin Sirignano, Jonathan F MacArt, and Jonathan B Freund. Dpm: A deep learning pde augmentation method with application to large-eddy simulation. Journal of Computational Physics, 423: 109811, 2020.\n\nKim Stachenfeld, Drummond Buschman Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned simulators for turbulence. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=msRBojTz-Nh.\n\nNaoya Takeishi and Alexandros Kalousis. Physics-integrated variational autoencoders for robust and interpretable generative modeling. Advances in Neural Information Processing Systems, 34: 14809–14821, 2021.\n\nNils Thuerey, Konstantin Weißenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods for reynolds-averaged navier–stokes simulations of airfoil flows. AIAA Journal, 58(1):25–36, 2020.\n\nJonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. Accelerating eulerian fluid simulation with convolutional networks. In International Conference on Machine Learning, pp. 3424–3433. PMLR, 2017.\n\nKiwon Um, Robert Brand, Fei Yun, Philipp Holl, and Nils Thuerey. Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers. In Neural Information Processing Systems (NeurIPS), volume 33, pp. 6111–6122, 2020.\n\nRui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physicsinformed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1457–1466, 2020.\n\nGerhard Wanner and Ernst Hairer. Solving ordinary differential equations II, volume 375. Springer\n\nBerlin Heidelberg New York, 1996.\n\nAdam A Wheeler, William J Boettinger, and Geoffrey B McFadden. Phase-field model for isother-\n\nmal phase transitions in binary alloys. Physical Review A, 45(10):7424, 1992.\n\nForman A Williams. Combustion theory, (1985). Cummings Publ. Co, 31, 1985.\n\nYuan Yin, Vincent LE GUEN, J ́er ́emie DONA, Emmanuel de Bezenac, Ibrahim Ayed, Nicolas THOME, and patrick gallinari. Augmenting physical models with deep networks for complex dynamics forecasting. In International Conference on Learning Representations, 2021.\n\nAli Girayhan ̈Ozbay, Arash Hamzehloo, Sylvain Laizet, Panagiotis Tzirakis, Georgios Rizos, and Bj ̈orn Schuller. Poisson cnn: Convolutional neural networks for the solution of the poisson equation on a cartesian mesh. Data-Centric Engineering, 2:e6, 2021. doi: 10.1017/dce.2021.7.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA NEURAL NETWORK ARCHITECTURE\n\nAll results provided in the main paper use a fully convolutional neural network. Different input flow field quantities have different value ranges of the data e.g. T ∈ [400, 2500], Yf ∈ [0, 0.05]. Therefore, the input data provided to the neural network model is normalized using mean and standard deviation. The output of the neural network model is then transformed back using the same mean and standard deviation. When predicting correction of Yk fields, the output of the neural network is constrained to provide values in the range Yf ∈ [0, 0.05], Yo ∈ [0, 1]. In the hybrid NN-PDE approach, the output of the neural network model is provided to the incomplete PDE solver as an input, as shown in figure 2. Each network is trained for 100 epochs with learning rate of 0.0001. An L2 based loss function is used as defined in section 3.3.\n\nWe additionally experimented with ResNet and UNet architectures for the neural network part of the purely data driven and hybrid NN-PDE approach discussed in the paper. Table 2 provides details of the convolutional blocks, layers of the network, and activation function. The UNet32 architecture with 2 layers is used for 32 × 32 cases and the UNet100 architecture with 3 layers is used for 100 × 100 case discussed in the paper. Table 3 shows the comparison between MAPE achieved by ResNet vs. UNet architecture for the purely data-driven and the hybrid NN-PDE approach for different cases considered in the paper. For the hybrid NN-PDE approach with 32 × 32 resolution cases, ResNet and UNet achieve comparable performance with the UNet yielding the lowest error. The main difference arises in the 100 × 100 resolution case. The ResNet fails to converge during training, leading to high test error. On the contrary, the UNet achieves low training and testing errors. The main advantage of UNet comes from transforming high resolution input to low resolution representation and reconstructing it back to the high resolution output. Therefore, we choose the UNet architecture for the hybrid NN-PDE approach. For the purely data-driven approach ResNet was found to be better for 2 out of 3 cases. Specifically for the nonUniform-Bunsen32 case, UNet performs very poorly for the PDD approach. Therefore, we report ResNet errors for the PDD approach throughout the paper. Figure 2 shows the details of the predictor block for PDD and hybrid NN-PDE approach.\n\nTable 2: Details of various neural network architectures used in this paper.\n\nHyper-parameters Kernel size Latent size Activation Loss # ResBlocks # UNet layers CNN stack depth Base latent size Spacial down-sample by layer latent sizes # trainable parameters\n\nResNet 5\n32\n\nUNet32 5\n\nUNet100 5\n\nLeakyReLu LeakyReLu\n\nMSE 5\n\n2\n\n261,953\n\nMSE\n\n2 3\n16 2\n(32) 136,227\n\nReLu MSE\n\n3 2\n16 2\n(32,64) 362,371\n\nTable 3: Mean and standard deviation of errors over different architectures for purely data-driven and hybrid NN-PDE approach.\n\nE P\nA M\n\nPlanar-v0 uniform-Bunsen nonUniform-Bunsen32 nonUniform-Bunsen100\n\nPDD ResNet 6.33 ± 3.05% 7.58 ± 3.73%\n\nPDD UNet 7.09 ± 4.40% 2.87 ± 1.53%\n\n12.48 ± 11.31% 19.19 ± 14.92%\n\n-\n\n-\n\nHybrid NN-PDE Hybrid NN-PDE\n\nResNet 1.62 ± 0.44% 2.01 ± 0.99% 3.25 ± 2.35% 15.68 ± 10.44%\n\nUNet 1.40 ± 0.65% 0.72 ± 0.37% 2.04 ± 1.39% 3.23 ± 3.76%\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Details of the boundary conditions used for the planar flame case and various cases of Bunsen-type flames. The uniform-Bunsen case is obtained with κ = constant and nonUniformBunsen case is obtained with κ ∈ Rn as discussed in section 4\n\nPlanar flame\n\nfield T\nuy ux P\nYf\n\ninlet 400 K 0\n0 101325 1\n\n1+ φ\n\nE (1+3.76\n\nWN2 WO2\n\n)\n\nleft and right wall Neumann BC -\n- Neumann BC slip\n\nfield T\nuy ux P\nYf\n\nBunsen-type flame inlet 800 K κ\n0 101325 1\n\nleft and right wall Neumann BC no-slip slip Neumann BC slip\n\n1+ φ\n\nE (1+3.76\n\nWN2 WO2\n\n)\n\nFigure 8: Instances of (A): incomplete PDE solver; (B): complete PDE solver for different operating conditions. (C): Snapshot of complete PDE solver at t = 300 for all training and testing datasets of uniform-Bunsen case. (D): Snapshot of complete PDE solver at t = 500 for all training and testing datasets of nonUniform-Bunsen32 case.\n\nB DATA GENERATION\n\nTo generate input and target data for training, we simulate the temperature T , mass fractions Yf , Yo and velocity ux, uy fields of all flames under study. Table 4 summarizes the boundary conditions applied for the planar and Bunsen-type flame cases discussed in the paper. No-slip boundary conditions are used for the velocity field uy to obtain the Λ-shaped flames.\n\nWe consider variations in equivalence ratio E and inlet velocity amplitude uy to obtain training and test datasets. Figure 8 (A) shows the temperature field evolution of the incomplete PDE solver at different time instances for 3 different operating conditions for the uniform-Bunsen case. Figure 8 (B) shows the corresponding target training data for the uniform-Bunsen case. All the simulations are run for 300 steps. It shows the difference between non-reactive and reactive flow solver simulations for different operating conditions over 300 time-steps. Figure 8 (C) shows the last snapshot (t = 300) of 9 training simulations and 6 testing datasets with operating parameters interpolated between the training data parameters for uniform-Bunsen case. These datasets are obtained by varying equivalence ratio and constant inlet velocity as discussed in section 4. Flame temperature depends on the equivalence ratio used and flame height depends on the inlet velocity amplitude. Figure 8 (D) showcases the snapshots of training and testing dataset at t = 500 for nonUniform-Bunsen32 case. Non-uniform variations in inlet velocity profile leads to different complex flame shapes. We use 12 datasets with 500 simulation steps to train the models and test it on 12 test cases shown in figure 8 (D).\n\n14\n\nPi(Tt) t = 1Pi(Tt) t = 25Pi(Tt) t = 50Pi(Tt) t = 100Pi(Tt) t = 150E = 1.0U = 0.45Pi(Tt) t = 300E = 0.9U = 0.4E = 0.8U = 0.3(A)Pc(Tt) t = 1Pc(Tt) t = 25Pc(Tt) t = 50Pc(Tt) t = 100Pc(Tt) t = 150E = 1.0U = 0.45Pc(Tt) t = 300E = 0.9U = 0.4E = 0.8U = 0.3(B)TrainingTesting(C)TrainingTesting(D)80010001200140016001800200022002400Under review as a conference paper at ICLR 2023\n\nFigure 9: Generalization to incorrect PDE parameters (A) visualization of differences in temperature fields due to incorrect parameters in incomplete PDE description. (B) Modified hybrid NN-PDE model, which combines the incomplete, incorrect PDE solver with neural network model is able to recover solutions of complete, correct PDE.\n\nFigure 10: Effect of longer look-ahead steps. MAPE of temperature field predictions by hybrid NN-PDE model, over all testsets. Models trained with higher look-ahead steps m accurately predict the temporal evolution of dynamics for longer duration across all cases considered.\n\nC ABLATION STUDY\n\nEffect of training dataset size We study the effect of training dataset size on the accuracy of PDD predictions. We train PDD models with different numbers of simulations in the training dataset. Each simulation contains 500 time-steps. Figure 5 (A) shows the MAPE of PDD models trained with {4, 8, 12, 16, 24, 32} training datasets, over a fixed testset. We compare the performance of these PDD models with an equivalent (trained with same look-ahead steps m = 2) hybrid NN-PDE model, trained with 12 simulation sets. Figure 5 (A) shows increasing the number of training sets has little or no effect on the prediction capabilities of the PDD models for nonUniform-Bunsen32 case. The hybrid model with 12 training sets achieves a MAPE of 12.49 ± 4.17%, an improvement of 38% over the PDD model with 32 training sets. The purely data-driven models cannot achieve the same level of accuracy even in the presence of large amounts of data.\n\nGeneralization to incorrect PDE parameters The proposed hybrid NN-PDE model is capable of completing the PDE description even if the underlying incomplete PDE solver has incorrect parameters. We refer to the incomplete solver with incorrect parameters as ‘incomplete, incorrect PDE’. We experiment with a modified hybrid NN-PDE approach wherein we combine an incomplete, incorrect PDE solver with a neural network model. We assume that the known values of the incomplete PDE parameters in equation 2, strain rate tensor (τ ), the diffusion coefficient of species k (Dk) and mixture thermal conductivity (λ), are incorrect. Figure 9 (A) shows the difference between temperature field evolution of the incomplete PDE solver with correct parameters ([τ, Dk, λ] = [0.1, 0.1, 0.1]) and incorrect parameters ([τ, Dk, λ] = [0.05, 0.05, 0.05]) at different time-steps. The hybrid NNPDE combines this incomplete, incorrect PDE solver with the neural network model to obtain the solutions of the complete PDE solver with correct parameters. Figure 9 (B) compares the flame dynamics predicted by this hybrid NN-PDE model with that of the complete, correct PDE solver. A good match is observed over various test cases. The hybrid model with incomplete, incorrect solver achieves an overall MAPE of 2.48 ± 1.20 %, compared to the MAPE of 2.04 ± 1.39 % for hybrid model with incomplete, correct PDE.\n\n15\n\nIncomplete PDEIncomplete,Incorrect PDEDifference(A)400800120016002000Complete PDEHybrid NN-PDE w. incomplete, incorrect PDEError(B)80012001500180022000100200300Simulation steps05101520MAPE(A) Planar-v0m=2m=4m=8m=16m=320100200300Simulation steps05101520(B) uniform-Bunsen0200400Simulation steps05101520(C) nonUniform-Bunsen320200400Simulation steps05101520(D) nonUniform-Bunsen100Under review as a conference paper at ICLR 2023\n\nFigure 11: Bar plot of MAPE of temperature field predictions by FNO, PDD and hybrid NN-PDE model at time-step ∆tc, for different testcases.\n\nEffect of longer look-ahead steps We evaluate the effect of varying look-ahead steps on the performance of the hybrid model. We show the comparison of models trained with m = {2, 4, 8, 16, 32} for different cases in figure 10. Models with smaller m (2) do not learn to accurately correct the fields over long time and quickly diverge from the target simulation. Using larger m improves the quality of prediction drastically as the model learns the correction via the gradients over longer simulation steps. For Planar-v0 and uniform-Bunsen cases, iterating the NN and PDE solver for 32 timesteps improves the accuracy by 81.7% and 94% respectively compared to the m = 2 model. For nonUniform-Bunsen32 and nonUniform-Bunsen100, the model performance improves by 84.2% and 70.9% respectively, by using m = 32 instead of m = 2 model.\n\nEffect of temporal coarsening Figure 11 and table 1 compare the MAPE of temperature field predictions at time-step ∆tc, for all three scenarios. As seen from the figure 11, the hybrid NN-PDE approach consistently performs better than both the baselines for all three scenarios considered. Higher standard deviations in Table 1 indicate the large differences in the prediction of flame temperatures for baseline approaches. The multi-physics systems we study provide a substantially more difficult environment than regular fluids: the chemical reactions are numerically very stiff, and the resulting dynamic interactions are difficult to capture by numerical solvers. To further illustrate the temporal behavior of the simulations, we perform additional experiments with the baseline approaches using twice the time-step size of the complete solver (2∆tc). As shown in figure 12, for the Planar-v0 and uniform-Bunsen case, PDD catches up with the hybrid approach whereas the PDD approach completely fails to predict the dynamics of the nonUniform-Bunsen32 case. We further investigate the nonUniform-Bunsen32 case with larger time-steps (2 times, 4 times and 8 times) as shown in figure 5 (B). It shows MAPE of the predictions over all the testsets and simulation steps by the PDD model. The PDD approach does not predict the dynamics accurately for any of the larger time-steps considered. Figure 13 visualize these results over different time-steps for one of the test cases. The predictions made by FNO, PDD and hybrid NN-PDE model for nonUniform-Bunsen32 case at twice the time-step of 2∆tc, are compared with the ground truth solutions coming from the complete PDE solver at time-step of ∆tc. FNO and PDD (upper two rows) fail to recover the correct flame shapes over 250 simulation steps. The hybrid NN-PDE model (second row from below) predicts the flame shape accurately, thus relaxing the temporal stiffness of the complete PDE solver.\n\nD ADDITIONAL RESULTS\n\nIn this section, we present additional visualizations of all reactive flow scenarios considered in section 4 for different test cases. Additionally, we discuss the results of the mass fraction fields we obtained. Table 5 summarizes MSE values of fuel (Yf ) and oxidizer (Yo) mass fraction fields for\n\n16\n\n12340246810MAPE(A) Planar-v0FNOPDDHybrid1234560246810MAPE(B) uniform-Bunsen123456789101112Testset number0246810MAPE(C) nonUniform-Bunsen32Under review as a conference paper at ICLR 2023\n\nFigure 12: Bar plot of MAPE of temperature field predictions by FNO, PDD and hybrid NN-PDE model at time-step 2∆tc, for different testcases.\n\nFigure 13: Comparison between different approaches at time-step 2∆tc - from top to bottom - FNO, PDD, hybrid NN-PDE for nonUniform-Bunsen32 test case. Hybrid approach predictions accurately match with the ground truth data over long roll-outs.\n\ndifferent baselines and the hybrid NN-PDE approach. We observe similar performance to the temperature field predictions. The hybrid NN-PDE model consistently outperforms the FNO and PDD approaches for all the scenarios considered. The hybrid NN-PDE model is able to simultaneously correct temperature and mass fraction fields, thus successfully modeling the effects of unknown chemical reactions on all the fields of the flow state.\n\nD.1 PLANAR-V0\n\nFigure 14(A) shows the 2D visualization of the temperature field predictions for Planar-v0 case. As seen from the ground truth images, methane-air mixture (black color) converts into the burned products (yellow color) due the chemical reaction at the flat flame surface (red color). The dotted, horizontal red line helps to compare the transition of the flame interface, i.e. the displacement of the flame front. Due to the chemical reaction, fuel-air mixture is consumed and turns into burned product as the simulation progresses. The FNO approach completely fails to predict the propagation of the flame for the given test case. Its output does not show any evolution from the initial temperature profile for the given operating condition. The purely data-driven approach fails to capture the flame front displacement correctly, thus leading to an inaccurate prediction with large errors. The\n\n17\n\n12340246810MAPE(A) Planar-v0HybridPDDFNO1234560246810MAPE(B) uniform-Bunsen123456789101112Testset number0246810MAPE(C) nonUniform-Bunsen32FNOPDDt = 10HybridNN-PDEt = 20GroundTrutht = 25t = 50t = 50t = 100t = 75t = 150t = 100t = 200t = 150t = 300t = 175t = 350t = 200t = 400t = 225t = 450t = 250t = 5008001000120014001600180020002200Temperature [k]Under review as a conference paper at ICLR 2023\n\nFigure 14: Temperature field predictions for (A): Planar-v0 flame case with E = 0.95; (B) uniformBunsen case with constant inlet velocity U = 0.375 and E = 0.95 for different approaches. Hybrid NN-PDE model predicts physically accurate evolution of the flame cases under study.\n\nFigure 15: Absolute error between target field (Pc(Tt)) and the output predicted - from top to bottom top - by: FNO ( ̃T f no ), purely data-driven approach ( ̃T pdd ) model, for the instances shown in figure 14. The numbers represent instantaneous MAPE.\n\n) and hybrid NN-PDE ( ̃T hyb\n\nt\n\nt\n\nt\n\n18\n\nFNOPDDHybridNN-PDEt = 50GroundTrutht = 100t = 200t = 300(A)400800120016002000 T [k]t = 20t = 50t = 172t = 225(B)8001200150018002200 T [k]ε=4.84|Pc(Tt)−̃Tfnot|FNO Errorε=6.19ε=14.31ε=22.48ε=3.05|Pc(Tt)− Tpddt|PDD Errorε=4.69ε=9.08ε=13.43ε=0.9̃|Pc(Tt)− Thybt|Hybrid Errorε=1.65ε=2.15ε=2.̃6(A)0300̃001100ε=0.14ε=4.82ε=22.̃5ε=26.̃6ε=4.06ε=11.25ε=6.5̃ε=6.51ε=0.50ε=0.56ε=0.95ε=1.26(B)0300̃001100Under review as a conference paper at ICLR 2023\n\nTable 5: Mean and standard deviation of squared errors of fuel (Yf ) and oxidizer (Yo) mass fraction fields over all time steps of all testsets. The Hybrid NN-PDE approach outperforms all other baselines considered.\n\nYf\n\nYo\n\nPlanar-v0 uniform-Bunsen nonUniform-Bunsen32 nonUniform-Bunsen100 Planar-v0 uniform-Bunsen nonUniform-Bunsen32 nonUniform-Bunsen100\n\nBaseline FNO 1.3e-4 ± 7.4e-5 6.4e-4 ± 2.4e-4 2.2e-4 ± 1.9e-4 -\n1.0e-2 ± 6.8e-3 4.7e-1 ± 9.6e-2 1.4e0 ± 1.8e0 -\n\nBaseline NN 2.9e-5 ± 3.8e-5 5.1e-5 ± 4.1e-5 9.6e-5 ± 1.0e-4 -\n1.3e-3 ± 1.2e-3 2.1e-2 ± 2.7e-2 6.1e-3 ± 7.9e-3 -\n\nHybrid NN-PDE 8.9e-7 ± 1.0e-6 5.6e-7 ± 7.4e-7 8.0e-6 ± 1.1e-5 1.8e-5 ± 2.8e-5 3.7e-5 ± 2.9e-5 3.6e-5 ± 3.0e-5 1.6e-4 ± 1.6e-4 3.3e-4 ± 4.2e-4\n\nHybrid NN-PDE-dt 1.6e-7 ± 1.5e-7 9.8e-7 ± 1.3e-6 1.4e-5 ± 1.9e-5 1.8e-5 ± 2.9e-5 4.1e-5 ± 4.4e-5 1.4e-4 ± 1.2e-4 2.8e-4 ± 3.0e-4 5.0e-4 ± 5.8e-4\n\nhybrid NN-PDE model accurately captures this evolution of planar methane-air flame in a quiescent mixture. Figure 15 shows the instantaneous MAPE w.r.t. ground truth data for predictions shown in figure 14. The absolute error shown in figure 15(A) exceeds 1100 K for the FNO and purely datadriven approaches as these do not predict the flame temperature correctly. We use the upper limit of 1100 K for colorbar to highlight the errors in the hybrid NN-PDE approach more clearly. Large errors in FNO and PDD results stem from their inability to reliably predict the flame temperature and flame front displacement.\n\nD.2 UNIFORM-BUNSEN\n\nFigures 14(B) and 15(B) show an example of uniform-Bunsen case; a test case with uy = 0.375 and E = 0.95. The constant inlet velocity results in a symmetric, Λ-shaped flame. Vertical, dotted, red line in figure 14(B) helps to assess the symmetric nature of the flame. The purely data-driven model predicts a thicker flame (t = 50) or a flame with a spurious tip (t = 172) or an asymmetric flame (t = 225). Snapshots of the hybrid NN-PDE model predictions in figure 14(B) show that it adapts to this scenario very well and succeeds in obtaining the correct results for long term forecasts of the temperature field. Furthermore, the flame shape and height are also better predicted, as shown in figure 14(B). Very low error levels in figure 15(B), such as ε = 1.26 at t = 300, indicate that the hybrid model recovers the temperature field satisfactorily. Figure 16 compares the performance of the hybrid NN-PDE model for the Yf and Yo fields for two different test cases. We again observe that the hybrid NN-PDE model predicts all the fields accurately with very low MSE.\n\nD.3 NONUNIFORM-BUNSEN\n\nFigure 4 compares the temporal predictions made by FNO, PDD and the hybrid NN-PDE model with ground truth data. Compared to Planar-v0 and uniform-Bunsen case, FNO predicts qualitatively better results for this case. Although highly still accurate, it predicts shapes that come closer to the target flame shapes for the later part of the simulation (for t > 300). This improvement might be due to the larger training dataset used for this scenario compared to the previous two scenarios. However, it fails to predict accurate temporal predictions over the entire simulation. The hybrid NN-PDE approach satisfactorily predicts the accurate flame evolution. Additionally, we showcase the performance of the hybrid NN-PDE approach on two different test cases with complex flame shapes in figure 18. The neural network model along with incomplete PDE solver reconstructs these complex flame shapes in an accurate manner.\n\nFinally we showcase the performance of the hybrid NN-PDE approach on different test cases for highly resolved flame case of nonUniform-Bunsen100. Figure 19 compares the predictions made by hybrid NN-PDE over {0, 100, 200, 250, 300, 325, 350, 400, 450, 500} simulation steps with the ground truth data. It also shows the absolute difference between them. As the simulation progresses, higher errors are observed around the flame front. However, the hybrid NN-PDE approach captures the flame shape very accurately for longer roll-outs of 500 simulation steps. Currently this approach uses training dataset similar to nonUniform-Bunsen32. Further improvements in accuracy can be\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 16: Comparison between ground truth (or Target) data and the hybrid NN-PDE approach for the Uniform-Bunsen case for fuel (top) and oxidizer (bottom) mass fraction fields for different operating conditions. Error field shows the absolute difference between target state and hybrid prediction over {0, 50, 100, 150, 200, 225, 250, 300} steps. ε indicates the instantaneous MSE.\n\n20\n\nPc(Yf,t)Target̃Yf,tNN-PDEε=0.0ẽ00|Pc(Yf,t)−̃Yf,t|Errorε=6.8e-08ε=4.4e-07ε=1.4e-06ε=1.2e-06ε=1.2e-06ε=1.5e-06ε=4.7e-060.000.010.020.030.040.050.0000.0020.0040.0060.0080.010Pc(Yo,t)Target̃Yo,tNN-PDEε=0.0ẽ00|Pc(Yo,t)−̃Yo,t|Errorε=1.3e-05ε=6.9e-05ε=1.8e-05ε=9.8e-06ε=1.1e-05ε=1.2e-05ε=2.7e-050.00.20.40.60.81.00.0000.0050.0100.0150.0200.0250.030Under review as a conference paper at ICLR 2023\n\nFigure 17: Comparison between different approaches - from top to bottom - FNO, PDD, hybrid NN-PDE for nonUniform-Bunsen32 test case. Hybrid approach predictions accurately match with the ground truth data over long-rollouts of 500 time steps.\n\nFigure 18: Additional visualization of reactive flow trajectories predicted by the hybrid approach for different test cases of nonUniform-Bunsen32 scenario.\n\nachieved by increasing the training dataset size or training the hybrid model with longer look-ahead steps.\n\n21\n\nFNOPDDHybridNN-PDEt = 25GroundTrutht = 50t = 100t = 150t = 200t = 250t = 300t = 350t = 400t = 5008001000120014001600180020002200Temperature [k]Pc(Tt)Target̃TtNÑPDEε=0.00|Pc(Tt)−̃Tt|Errorε=0.59ε=1.04ε=1.89ε=2.74ε=2.61ε=1.68ε=1.78ε=1.83ε=1.538001200160020000250500Pc̃Tt)Target̃TtNN-PDEε=0.00|Pc(Tt)−̃Tt|Errorε=0.95ε=1.02ε=1.34ε=2.08ε=1.93ε=1.82ε=2.27ε=2.29ε=3.738001200160020000250500Under review as a conference paper at ICLR 2023\n\nFigure 19: Comparison between ground truth data (Pc(Tt)) and hybrid NN-PDE approach predictions ( ̃Tt) for various test cases of complex, high resolution scenario of nonUniform-Bunsen100\n\n22\n\nPc(Tt)Target̃TtNÑPDEε=0.00|Pc(Tt)−̃Tt|Errorε=0.54ε=1.42ε=1.75ε=2.06ε=2.29ε=2.38ε=2.87ε=3.30ε=3.468001200160020000250500Pc̃Tt)Target̃TtNN-PDEε=0.00|Pc(Tt)−̃Tt|Errorε=0.82ε=1.47ε=1.96ε=2.90ε=3.43ε=3.97ε=4.63ε=5.21ε=4.968001200160020000250500Pc̃Tt)Target̃TtNN-PDEε=0.00|Pc(Tt)−̃Tt|Errorε=1.05ε=1.59ε=2.40ε=3.48ε=3.99ε=4.62ε=5.81ε=6.40ε=6.658001200160020000250500Under review as a conference paper at ICLR 2023\n\nFigure 20: Predictions made by hybrid NN-PDE model ( ̃Tt) with an incomplete PDE solver at twice the time-step of 2∆tc, are compared with the ground truth solutions coming from the complete PDE solver at time-step of ∆tc. We showcase the effectiveness of hybrid approach in relaxing temporal stiffness of the complete PDE solver on reactive flow cases of - from top to bottom - Planar-v0, uniform-Bunsen, nonUniform-Bunsen32 and nonUniform-Bunsen100 for different test cases.\n\n23\n\nt = 0Pc(Tt)Targett = 50t = 100t = 150t = 200t = 226t = 250t = 300t = 0̃TtNÑPDEt = 25t = 50t = 75t = 100t = 113t = 125t = 150ε=0.00|Pc(Tt)−̃Tt|Errorε=1.04ε=1.01ε=1.02ε=1.19ε=1.42ε=1.99ε=2.328001200160020000250500t̃=̃0Pc(Tt)Targett̃=̃50t̃=̃100t̃=̃150t̃=̃200t̃=̃226t̃=̃250t̃=̃300t̃=̃0̃TtNN-PDEt = 25t = 50t = 75t = 100t = 113t = 125t = 150ε=0.00|Pc(Tt)−̃Tt|Errorε=0.58ε=0.90ε=0.83ε=0.98ε=1.17ε=1.23ε=1.218001200160020000250500t̃=̃0Pc(Tt)Targett̃=̃100t̃=̃200t̃=̃300t̃=̃350t̃=̃400t̃=̃450t̃=̃500t̃=̃0̃TtNN-PDEt = 50t = 100t = 150t = 175t = 200t = 225t = 250ε=0.00|Pc(Tt)−̃Tt|Errorε=0.84ε=1.64ε=2.16ε=2.21ε=2.60ε=3.19ε=3.838001200160020000250500t̃=̃0Pc(Tt)Targett̃=̃100t̃=̃200t̃=̃300t̃=̃350t̃=̃400t̃=̃450t̃=̃500t̃=̃0̃TtNN-PDEt = 50t = 100t = 150t = 175t = 200t = 225t = 250ε=0.00|Pc(Tt)−̃Tt|Errorε=0.93ε=1.34ε=3.06ε=3.90ε=4.91ε=5.82ε=6.378001200160020000250500",
    "reference": "# Summary Of The Paper\n\nThe paper argues for the combination of a neural network with an incomplete description of a system (a hybrid model), in their case, an incomplete PDE, rather than a fully data-driven approach that only relies on a neural network. They compare the two approaches on a set of systems described by thermodynamics and fluid dynamics. \n\nThe authors consider a supervised setting where the complete PDE, or solutions for it, are given and used for training the neural networks with an MSE loss. In this setting, their experiments suggest that the hybrid models outperform the data-driven approach by a significant margin. In addition, they argue that hybrid models can lead to faster solving of the PDEs than relying on the complete description when it is available.\n\n# Strength And Weaknesses\n\n# Strength\nThe idea of using the physical understanding of the studied phenomenon, rather than an entirely data-driven one, makes a lot of sense to me. In addition, the systems studied in this paper seem novel to the hybrid learning literature and constitute solid test scenarios for assessing future developments in hybrid learning. \n\n# Weaknesses\nOverall I find the contribution very limited. Combining a neural network with equations from physics is a long-standing idea in the community. This paper does not introduce anything subtle regarding a good strategy to combine these equations with neural networks. \n\nThe experimental validation of their approach also seems limited in terms of the train/test scenarios considered. In addition, it is unclear how the authors selected the best models and hyperparameters, as no cross-validation or validation set is discussed. This also reduces the solidity of the presented results. For instance, I am pretty sure that at some point, the data-driven approach could catch up with the hybrid approach if the number of train scenarios increases. An analysis of when this (might) happen would have been valuable to motivate the hybrid approach for these problems.\n\nIt is unclear to me whether the parameters of the PDE are also learned at training time. If not, this makes the proposed approach's applicability very limited as, in most cases, if we can observe a system and have little understanding of the physics behind this also means that we do not know the corresponding parameters of the incomplete physical description. I have probably misunderstood something there, but I would encourage the authors to clarify this in the paper. Overall, this paper would benefit from a real-world demonstration of the proposed approach. Indeed, the contribution is not methodological. Thus I would argue that this should contain a more solid empirical evaluation.\n\nAs a remark, I also feel that some confusion is made between the \"model\" and the \"inference\" (in the case of PDEs, the solver). The neural network aims to complete the model, create a more accurate description of the real world, and create a better model. The solver is related to how we make inferences given a model; the terms complete and incomplete solvers do not make sense to me.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n# Clarity\nThe paper's clarity is ok but should be improved to be accepted for publication, in my opinion. For instance, see my confusion regarding some of the points mentioned in the weaknesses of the paper.\n\n# Quality\nI did not find the experimental validation solid enough to trust the empirical conclusion made by the authors entirely. For example, the authors do not explain how they selected the different architectures and training hyperparameters. \n\n# Novelty\nThe novelty is limited. The main contribution is to showcase a simple hybrid approach to original test scenarios.\n\n# Typos and minor remarks\n- What is u in eq (1)?\n- The second paragraph before section 3: don't -> do not\n- The paragraph before section 3: Why don't you discuss Takeishi and Kalousis 2021 (https://scholar.google.com/citations?view_op=view_citation&hl=fr&user=rqF9bAsAAAAJ&citation_for_view=rqF9bAsAAAAJ:hC7cP41nSMkC)? I am not an author of this paper, it is a genuine remark. I also wonder why you do not compare to an approach similar to APHYNITY (https://scholar.google.com/citations?view_op=view_citation&hl=fr&user=rFaxB20AAAAJ&sortby=pubdate&citation_for_view=rFaxB20AAAAJ:IZKZNMMMWs0C).\n- Eq (3): a point at the end of the equation is missing.\n- Eq (5): the upper limit should be t + m.\n- unfiform-Busen -> Uniform-Busen.\n\n# Summary Of The Review\n\nOverall I recommend rejection of this paper for the aforementioned reasons. In particular, the lack of novelty of the approach and the limited reliability of the experiments.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nEFFICIENT SURROGATE GRADIENTS FOR TRAINING SPIKING NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSpiking Neural Network (SNN) is widely regarded as one of the next-generation neural network infrastructures, yet it suffers from an inherent non-differentiable problem that makes the traditional backpropagation (BP) method infeasible. Surrogate gradients (SG), which are an approximation to the shape of the Dirac’s δ-function, can help alleviate this issue to some extent. To our knowledge, the majority of research, however, keep a fixed surrogate gradient for all layers, ignorant of the fact that there exists a trade-off between the approximation to the delta function and the effective domain of gradients under the given dataset, hence limiting the efficiency of surrogate gradients and impairing the overall model performance. To guide the shape optimization in applying surrogate gradients for training SNN, we propose an indicator χ, which represents the proportion of parameters with non-zero gradients in backpropagation. Further we present a novel χ-based training pipeline that adaptively makes trade-offs between the surrogate gradients’ shapes and its effective domain, followed by a series of ablation experiments for verification. Our algorithm achieves 69.09% accuracy on the ImageNet dataset using SEW-ResNet34 - a 2.05% absolute improvement from baseline. Moreover, our method only requires extremely low external cost and can be simply integrated into the existing training procedure.\n\n1\n\nINTRODUCTION\n\nSpike Neural Networks (SNN) have gained increasing attention in recent years due to their biological rationale and potential energy efficiency as compared to the common real-value based Artificial Neural Networks (ANN). SNN communicates across layers by the addition of spiking signals. On the one hand, this spiking mechanism turns multiplicative operations to additive operations, increasing the inference procedure’s efficiency. On the other hand, it introduces an intrinsic issue of differentiability, which makes training SNNs more challenging. At present, the method for obtaining practical SNNs can be roughly divided into three categories: converting a pretrained ANN to SNN (Sengupta et al., 2019; Deng & Gu, 2020; Li et al., 2021a; Bu et al., 2021), training with biological heuristics methods (Hao et al., 2020; Shrestha et al., 2017; Lee et al., 2018), and training with BP-like methods (Wu et al., 2018; Zheng et al., 2020; Li et al., 2021b; Yang et al., 2021). The converting method may not promote increased inference efficiency in practice since it requires a lengthy simulation period (high inference latency) to catch up to the accuracy of the source ANN (Sengupta et al., 2019; Rueckauer et al., 2017). Although the biological heuristics technique requires just local information to change network parameters, it is confined to small datasets due to its limitation in representing global information (Wu et al., 2018; Shrestha et al., 2017). Compared to these two approaches, direct training with BP-like method is capable of handling complex models with a very short simulation duration to attain adequate model performance (Zheng et al., 2020; Fang et al., 2021; Li et al., 2021b).\n\nWith the help of surrogate gradient, the SNN can be directly trained through the BPTT algorithm on an ANN-based platform (Wu et al., 2018). Nonetheless, there is a non-negligible performance disparity between directly trained SNN and ANN, particularly on large and complicated datasets (Deng et al., 2020; Jin et al., 2018). This is because training SNN with surrogate gradient can only obtain approximate gradients, and the final performance is highly affected by the surrogate gradient shape. A more suitable surrogate gradient shape usually results in a better performing SNN (Neftci et al., 2019). However, an appropriate surrogate gradient must strike a compromise between the approximation shape and the effective domain of gradients. So just altering the shape of the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nsurrogate gradient to be more similar to the δ-function may result in the training failing due to gradient disappearance, as the gradients of most membrane potentials are extremely small. Additionally, the optimal surrogate gradient shapes for various layers may different and may change throughout the training process (Li et al., 2021b). As a result, using a fixed initial surrogate gradient shape (adequate effective domain) during the whole training phase will always have a substantial gradient error, which affects the final training result.\n\nThe purpose of this work is to optimize the SNN training pipeline by adaptively altering the shape of surrogate gradient in order to control the effective domain for the surrogate gradients. We suggest an index χ to denote the proportion of membrane potential with non-zero gradients in backpropagation and present a technique to control the proportion of non-zero gradients (CPNG) in the network. The CPNG technique modifies the shape of surrogate gradients during network training, progressively approaching the δ-function while maintaining the index χ steady within an effective range to ensure training stability. Finally, each layer succeeds in finding a surrogate gradient shape that makes a better balance between the approximation error to the δ-function with the size of effective domain than the fixed-shape surrogate gradients. It’s worth mentioning that our strategy only incurs minor additional costs during the training phase and has no effect on the inference phase. We verify the compatibility of CPNG to the existing mainstream SNN infrastructures such as VGG (Simonyan & Zisserman, 2014), ResNet (He et al., 2016), and Sew-ResNet (Fang et al., 2021). In all reported comparative experiments, training with CPNG gives more accurate models than training with vanilla surrogate gradients.\n\nOur main contributions can be summarized as follows:\n\n• We identify and investigate the impact of the shape of surrogate gradients on SNN training. Our finding characterizes a special representative power for SNN that can be utilized to improve its performance.\n\n• We propose a statistical indicator χ for the domain efficiency of surrogate gradients and a χ-based training method CPNG that adjusts the shape of surrogate gradients through the training process, driving the surrogate gradients close to the theoretical δ-function with ensured trainability on sufficiently large domains.\n\n• Our CPNG method improves classification accuracy on both static image datasets including CIFAR10, CIFAR100 and ImageNet, as well as event-based image datasets such as CIFAR10-DVS. We achieve an accuracy of 69.09% in the experiment that trains ImageNet on Sew-ResNet34.\n\n2 RELATED WORK\n\nThere are two primary branches of training a high-performing deep spiking neural network, converting a pretrained artificial neural network to its corresponding spiking neural network, and directly training a spiking neural network through BP-like method.\n\nANN-SNN Conversion ANN-SNN conversion takes advantage of the high performance of ANN and converts the source ANN to the target SNN through weight-normalization (Diehl et al., 2015; 2016) or threshold balancing (Sengupta et al., 2019). However, SNN forming this method requires a huge simulation length to catch up with the source ANN’s performance. Numerous strategies have been proposed to shorten the simulation time, including robust threshold (Rueckauer et al., 2016), SPIKE-NORM (Sengupta et al., 2019), and RMP (Han et al., 2020). A work (Deng & Gu, 2020) examines the conversion error theoretically, decomposes it layer by layer, and offers threshold ReLU and shift bias procedures to decrease the error. Based on it, Li et al. (Li et al., 2021a) divide the conversion error into clip error and floor error and design adaptive threshold, bias correction, potential correction, and weight calibration to dramatically decrease the required simulation length. A recent work (Bu et al., 2021) further proposes unevenness error, trains ANN with a novel activation function and reduce simulation length.\n\nBP-like Method HM2-BP (Jin et al., 2018) enables SNN to adjust the spike sequence rather than just the spike at a certain moment. TSSL-BP (Zhang & Li, 2020) decomposes the backpropagation error into inter and intra interactions, calculating the derivatives only at the spiking moment. NA algorithm (Yang et al., 2021), which calculates the gradient of the non-differentiable part through\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nfinite difference. The surrogate gradient based BP algorithm uses differentiable functions instead of the Dirac’s δ-function to mitigate the non-differentiation problem in the process of SNN training. Different from ANNs, SNNs naturally have time attribute. Therefore, the existing studies consider both temporal and spatial information in BP procedure (Wu et al., 2018). Normalization methods are crucial in SNN training, which help speed up the network’s convergence and prevent gradient disappearance or explosion. For this purpose, SNN-friendly normalization algorithms such as NeuNorm (Wu et al., 2019) and threshold-dependent batch normalization (tdBN) (Zheng et al., 2020) have been developed. The majority of studies have employed fixed-shape surrogate gradients, and some work has preliminarily explored the performance of surrogate gradients with different shapes (Wu et al., 2018; Neftci et al., 2019; Bellec et al., 2018). A recent work looked into the shape-changing surrogate gradient (Li et al., 2021b), proposed Dspike as surrogate gradient, and used the finite difference to guide the change of Dspike’s shape, significantly improving the performance of SNN. In order to train a very deeper SNN, Sew-ResNet (Fang et al., 2021) structure is proposed, which enables SNN training even on a 152-layer network.\n\n3 PRELIMINARY\n\nThrough out the paper, we use bold letters to denote matrices and vectors, superscripts to identify specific layers, subscripts to denote specific neurons, and indexes to identify specific moments.\n\nLeaky Integrate-and-Fire Model. We use the Leaky Integrate-and-Fire (LIF) module for spiking neurons. Formally, given the pre-synaptic input (denoted by c(l) [t + 1]) of the ith neuron in the lth layer at time stept + 1, we can model the iterative process in LIF as\n\ni\n\nc(l)\n\ni\n\n[t + 1] =\n\nN (l−1) (cid:88)\n\nj\n\nw(l)\n\nij s(l−1)\n\nj\n\n[t + 1],\n\nu(l)\n\ni\n\n[t + 1] = τ u(l)\n\ni\n\n[t](1 − s(l)\n\ni\n\n[t]) + c(l)\n\ni\n\n[t + 1],\n\ns(l)\n\ni\n\n[t + 1] = H(u(l)\n\ni\n\n[t + 1] − ν).\n\n(1)\n\n(2)\n\nHere, N (l−1) is the number of neurons in the (l − 1)th layer, s(l−1) [t + 1] is the output spike of the jth neuron in the (l − 1)th layer at time t + 1, w(l) ij is the weight between jth neuron in (l − 1)th layer and ith neuron in lth layer, u(l) [t] is the membrane potential of the ith neuron in the lth layer at time t, τ is the membrane potential attenuation factor, H(·) is the step function, and ν is the activation threshold. When the membrane potential of a neuron exceeds the activation threshold, a spike is released and the membrane potential of the current neuron is set to zero.\n\nj\n\ni\n\nSurrogate Gradient Function. There are various surrogate gradient shapes adopted by previous work (Wu et al., 2018; Neftci et al., 2019). In this work, we used triangle-like function, rectangularlike function and arctan-like function to verify the effectiveness of CPNG. These functions are described below:\n\nftriangle(x) =\n\n(cid:26) β(1 − β|x − ν|)\n\nif\n\n0\n\n|x − ν| < 1/β otherwise\n\n,\n\nfrectangular(x) =\n\n(cid:26) β if 0\n\n|x − ν| < 1/(2β)\n\notherwise\n\n,\n\nfarctan(x) =\n\nβ\n\n1 + (πβ(x − ν))2 ,\n\n(3)\n\n(4)\n\nwhere β represents the maximum gradient value of current surrogate gradient function. Notably, the surrogate gradient satisfy (cid:82) +∞\n\n−∞ f (x) = 1, which is also the property of the δ-function.\n\nLoss Function. formula is given as\n\nIn our experiments, we use cross-entropy-loss (LCE) as the loss function. The\n\nout =\n\n1 T\n\nT (cid:88)\n\nt=1\n\nW(L) · S(L−1)[t], Loss = LCE(out, label),\n\n(5)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nwhere T represents the simulation time, L represents the last layer of the SNN, W(L) represents the weight matrix of the Lth layer, and S(L−1) represents the output spike vector of the (L − 1)th layer. Consistent with (Wu et al., 2019; He et al., 2016), our final output layer is a voting layer devoid of any LIF model.\n\n4 METHOD\n\n4.1 SHAPE PARAMETERS AND EFFECTIVE DOMAIN INDICATOR\n\nShape Parameters. Intuitively, increasing the shape parameter β of surrogate gradients would drive it closer to the δ-function (Fig. 1(right)). One might expect to adopt a very high β to obtain SNN for a good performance. We first examine whether this intuitive approach is possible. We trained VGG16-structured SNN (replace ReLU with LIF, and use average pooling.) on CIFAR100 using triangle-like surrogate gradient with β set from 0.25 to 2 respectively. As shown in Fig. 1(left), the test accuracy increases when β varies from 0.25 to 1.0 but remains at 1.0% when β is set to 1.5 and 2.0, indicating that properly increasing β may benefit the training but arbitrarily increasing β will drive the training collapse. According to another viewpoint (Zenke & Vogels, 2021; Herranz-Celotti & Rouat, 2022), when the beta is greater than 1, the gradient explosion on the deep SNN will occur, resulting in training failure; thus, some of the most recent surrogate gradients fix the maximum value of surrogate gradients at 1 (Suetake et al., 2022; Zenke & Ganguli, 2018). But the narrow effective interval of the surrogate gradients is the primary culprit in our experiments. We detected the maximum gradient absolute values for SNN training at the first 100 batches when β = 1(1.0001) and β = 1.5(1.000), respectively. These results illustrate that when β is 1.5, the gradient explosion does not always occur in the network.\n\nFigure 1: Left: Test accuracy of different β when threshold is 1.0. Right: δ-function and triangle-like surrogate gradient when the threshold is 1.0.\n\nIn fact, these results unveil that efficient training of SNNs requires not only the approximation to the δ-function but also the insurance for the surrogate gradients to work. Thus it is necessary to employ a dynamic shape-changing strategy rather than using a fixed-shape surrogate gradient. This issue is also covered by (Li et al., 2021b) as well and was owed to the lack of adaption to the dataset variation.\n\nEffective Domain Indicator. As aforementioned, if we unrestrictedly increase the β in order to get the surrogate gradient closer to the δ-function, the training curve will remain flat without any update. This is caused by the fact that the majority of the membrane potential remains outside the effective region. On the other hand, if β is too small, it will also lead to a suboptimal training outcome due to the presence of a substantial gradient error (Fig. 1(left)) between the adopted surrogate gradients and the δ-function. Thus a proper surrogate gradients should maintain an optimal balance between the domain effectiveness and the δ-function approximation.\n\nTo quantitatively guide the choice of β, we need a statistical indicator to denote the percentage of membrane potentials that fall into the domain of surrogate gradients. As illustrated in Fig. 2(left), the distribution of membrane potentials on each layer takes the normal shape. Thus, for the simplicity of calculation, we regard the membrane potential distribution of all neurons within the same layer as a Gaussian one. By calculating the mean μ and the standard deviation σ of the membrane potential before this layer releases spikes, we can obtain the proportion of the neuron with a non-zero gradient\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nduring a certain iteration (the area between the red lines in Fig. 2(right)). For a given β, the effective gradient domain of triangle-like surrogate gradient is [ν − 1/β, ν + 1/β], we can obtain the definite integral of the current normal distribution in this effective gradient domain, which is the Effective Domain Indicator χ:\n\nχ =\n\n(cid:90) ν+1/β\n\nν−1/β\n\n√\n\n1\n\n2πσ\n\ne− (x−μ)2\n\n2σ2 dx.\n\n(6)\n\nFor each layer, we record the membrane potential of all neurons in every time step (a tensor shaped like batchsize-by-timestep-by-channels-by-H-by-W) and calculate the mean and variance. Based on this indicator χ, we can then effectively determine to what extent we can tune the β while ensuring that there are enough membrane potentials located within the effective range of surrogate gradients to make the training progress.\n\nFigure 2: Left: Membrane distribution of each layer in experiment training ResNet19 on CIFAR10. Right: When the threshold is 1.0 and β is 1.0, the proportion of neurons with non-zero gradient.\n\n4.2 CPNG METHOD\n\nIn this section, we will cover how to combiningly optimize β and χ to maximize the effectiveness of surrogate gradients. To train the network successfully, there must be sufficient membrane potential values in the effective domain of the surrogate gradient, i.e., χ must be large enough. The most extreme case is β → 0, which gives χ → 1. Obviously, this is not an optimal solution as it introduces substantial error for the gradients. In order to determine the choice of β for a given χ, we need to investigate the gradient form ∂LCE\n\n∂S(l)[t] to see how the β affects the error back-propagation.\n\nWhen l = L − 1, we can directively get\n\n∂L(1) ∂S(L−1)[t]\n\nCE\n\n, t = 1, 2, · · · , T according to Eqn. 5.\n\nWhen l = 0, 1, · · · , L − 2, we can first derive ∂u(l) from Eqn. 2, and further have\n\ni\n\ni\n\n∂s(l)\n\n[t+1]\n\n[t]\n\n= −τ u(l)\n\ni\n\n[t] and ∂u(l)\n\ni\n\n[t+1]\n\n∂u(l)\n\ni\n\n[t]\n\n= τ (1 − s(l)\n\ni\n\n[t])\n\n∂LCE ∂u(l)[t]\n\n=\n\n∂LCE ∂s(l)[t]\n\n=\n\n \n\n \n\n\n\n\n∂LCE ∂s(l)[t] ∂LCE ∂s(l)[t]\n\n∂s(l)[t] ∂u(l)[t] ∂s(l)[t]\n\n∂u(l)[t] + ∂LCE\n\n∂u(l)[t+1] · τ (1 − sl[t])\n\nt = T\n\nt = 1, 2, · · · , T − 1\n\n,\n\n(7)\n\n∂L(1) CE ∂s(l)[t] ∂L(1) ∂s(l)[t] + ∂LCE\n\nCE\n\n∂u(l)[t+1] · (−τ ul[t])\n\nt = T\n\nt = 1, 2, · · · , T − 1\n\n.\n\n(8)\n\nby Eqn. 2. Here, u(l)[t] represents the membrane potentials vector of the lth layer, ∂L(1) ∂s(l)[t] is the gradient directly obtained from previous layer. The complete ∂LCE ∂s(l)[t] needs to consider the dependence between the current moment spike and the membrane potential at the next moment (Eqn. 8). The term ∂LCE ∂u(l)[t] is ∂u(l)[t] , while ∂u(l)[t] , which in turn affects the calculation\n\ngiven by surrogate gradients. We can conclude that each item of the ∂LCE effective interval of surrogate gradient (χ) determines ∂s(l)[t]\n\n∂u(l)[t+1] can be obtained iteratively from the gradients from its succeeding layer, and ∂s(l)[t]\n\n∂u(l)[t] contains ∂s(l)[t]\n\nCE\n\n5\n\nVthVth - 1/betaVth + 1/betaNeuron NumMembrane PotantialUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Control the Proportion of Non-zero Gradient\n\nInput: SNN model with L layer, current iterator epoch e, χlimit, and vector χrecorder: store each layer’s smallest χ Output: Each layer’s surrogate gradient parameter β if e == 0 then\n\nfor l = 1, 2, · · · L do\n\ncalculate current χ by Eqn.6 for layer-l and store at χrecorder[l]\n\nend for\n\nelse\n\nfor l = 1, 2, · · · L do\n\ncalculate current χcur by Eqn.6 for layer-l if χrecorder[l] <χlimit then\n\nχrecorder[l] = χlimit\n\nelse if χcur <χrecorder[l] then\n\nχrecorder[l] = χcur\n\nend if χmin = χrecorder[l] if χmin ̸= χcur then\n\nuse χmin to update β using binary search method.\n\nend if end for\n\nend if return β for each layer\n\n∂W(l) and ∂L(1)\n\nCE\n\nof ∂LCE ∂s(l−1)[t] . However, the surrogate gradient is only an approximation of δ-function, and an overly relaxed surrogate is harmful to the network’s final performance. If we reasonably restrict the effective interval of the surrogate gradients, it is possible to drive the SNN to select those more essential membrane potentials for backpropagation.\n\nWe also need to ensure that the new χ does not make the network difficult to train, for this, CPNG sets the target χ of each layer to the smallest χ that has occurred in the current layer during the training iteration, rather than an artificial goal. If the network can be trained when the smallest χ appears, then the network should still be trained after we adjust the β and maintain the smallest χ. When using CPNG, we expect that the network parameters are appropriate, that is, the network has traversed the whole dataset to prevent the misleading of network parameters by data randomness. For example, if we use CPNG once per batch, the network parameters are mostly affected by the first few batches in the early stages of network training, and the statistical indicator χ obtained by using such network parameters will have a lot of randomnesses.\n\nCPNG computes the smallest χ of each layer during the iteration process as χmin and records it. If the χ value of a certain layer rises after an epoch, CPNG adjusts the χ value of the current layer to χmin by increasing the β, otherwise, keep the current β fixed and update χmin. Since the χmin of each layer of neurons may be different, different layers may have different surrogate gradient shapes. In addition, we set a safe lower bound χlimit. When χ falls below the lower bound, the β may decrease to force χ back above χlimit in order to guarantee that sufficient membrane potential values are covered in the effective domain of surrogate gradient for successful SNN training. The CPNG algorithm is detailed in Algo.1.\n\n4.3 THE COST OF CPNG METHOD\n\nThe extra cost of CPNG occurs in two steps: (1) collecting the mean and variance of the membrane potential before releasing the spikes of each layer; (2) altering the β using the indicator χ. In our experiment, we only use the mean and variance of the last batch to calculate the indicator χ, which makes the cost of the first step in the same order of magnitude as the batch normalization (Ioffe & Szegedy, 2015) operation. For the latter step, we provide a binary search method that solves the problem very fast, and further optimization algorithms can further improve the solution speed. The above analysis is the cost of using CPNG once, the overall cost takes into account the frequency of using CPNG. In our experiments, we employ CPNG just once every epoch, which is quite economical\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: The impact of using time and batch size on Vgg16+CIFAR100 experiment.\n\n256(w/o CPNG)\n\n256(B)\n\n256(E)\n\n512(w/o CPNG)\n\n512(B)\n\n512(E)\n\nAcc Time\n\n69.08% 705.59m\n\n71.05% 71.54% 799.41m 716.67m\n\n68.44% 377.76m\n\n69.59% 70.20% 424.49m 381.79m\n\nB: Use CPNG per batch.\n\nE: Use CPNG per epoch (last batch)\n\nwhen compared to the network training time. Quantitatively, in the VGG16+CIFAR100 experiment, it takes an average of 1.7GFLOPs to obtain the output corresponding to an input without CPNG, and the first step of CPNG will only add 5.59 × 10−3 additional GFLOPs. Using CPNG once per epoch takes an average of 3.3 seconds of overhead (1.57% of total training time).\n\n5 EXPERIMENT\n\nTo verify the effectiveness of the CPNG method, we provide groups of comparative experiments (Sec. 5.2) on both static and neuromorphic datasets. We also compare CPNG with existing works in Sec. 5.3 and show the final β of SNN after training with CPNG in Sec. 5.4.\n\n5.1\n\nIMPLEMENTATION DETAILS\n\nAll the SNN architectures include the tdBN layer (Zheng et al., 2020) with the average-pooling layer, and compared to their ANN versions, we replace the activation function ReLU with LIF. Our experiment settings, such as optimizer, learning rate, are detailed in Appendix. A.2. Except for applying the CPNG method at the end of each epoch, all other conditions, such as learning rate, batch size, etc., are consistent. The data preprocessing for each dataset included in the experiments are as follows:\n\nCIFAR and ImageNet. We use standard processing methods for these datasets, see appendix. A.1 for more details.\n\nCIFAR10-DVS. CIFAR10-DVS is an event-based image datasets. Following the previous work (Li et al., 2021b), we re-sample 10 simulation length, divide the dataset into 9k training images and 1k test images, reduce the spatial resolution to 48×48, and apply data augmentation techniques.\n\nTable 2: χlimit Experiments on ResNet18+CIFAR-DVS. 0.5\n\n0.05\n\n0.1\n\n0.2\n\n0.4\n\n0.3\n\n0.6\n\nχlimit\n\nAccuracy\n\n76.6% 76.6% 76.37% 76.8% 76.0% 76.1% 74.1%\n\n5.2 PERFORMANCE IMPROVEMENT WITH CPNG\n\nAlthough χ is employed as a statistical indicator, table1 demonstrates that increasing the batch size does not always result in better performance of CPNG. This is because there is no strict positive correlation between neural network performance and batch size. We don’t need to increase batch size excessively to increase the sample size of χ. Besides, as we mentioned in 4.2, using CPNG once per batch does not guarantee performance improvement, it may bring more overhead and worse performance. Then we compared the impact of different χlimit on CPNG, table2 shows that we may don’t need to focus too much on χlimit, 0.05 − 0.2 might be suitable. In our experiments, we set χlimit=0.2 uniformly.\n\nAdditionally, we tested the applicability of CPNG to various surrogate gradient functions. For surrogate gradients with non-zero gradient everywhere, such as arctan, we directly use [ν − 1/β, ν + 1/β] as the integration interval to calculate the indicator χ, then use Algo.1 to solve new β. Considering the case of using triangle-like surrogate gradient, we control the percentage of membrane potentials with non-zero gradients, which can also be explained as: under the current surrogate gradient, find a representative interval related to β, and make the calculated χ in this interval equal to the χtarget. In the\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Examine CPNG on various surrogate gradients.\n\nDataset\n\nMethod\n\nArchitecture Time Step\n\nAccuracy\n\nCIFAR10-DVS\n\nResNet18 Triangular ResNet18 Triangular+CPNG ResNet18 Rectangular ResNet18 Rectangular+CPNG ResNet18 ArcTan ResNet18 ArcTan+CPNG Triangular+TET ResNet18 Triangular+TET+CPNG ResNet18\n\n10 10 10 10 10 10 10 10\n\n75.6% 76.37±1.02 % 74.7% 75.3% 67.2% 67.5% 79.9% 82.3%\n\nDataset\n\nMethod\n\nTable 4: Result of training spiking neural network. Time Step Architecture\n\nAccuracy\n\nCIFAR10\n\nCIFAR100\n\nCIFAR10-DVS\n\nImageNet\n\nSTBP-tdBN (Zheng et al., 2020) ResNet19 ResNet20 ANN-to-SNN (Li et al., 2021a) ResNet20 ANN-to-SNN (Bu et al., 2021) ResNet19 CPNG\n\nDiet-SNN (Rathi & Roy, 2020) ANN-to-SNN (Li et al., 2021a) ANN-to-SNN (Bu et al., 2021) CPNG\n\nDspike (Li et al., 2021b) TET(Deng et al., 2022) CPNG CPNG + TET\n\nVGG16 VGG16 VGG16 VGG16\n\nResNet18 VGG11 ResNet18 VGG11\n\n6 32 64 128 8 16 32 6\n\n5 32 64 128 8 16 32 5\n\n93.16% 94.78% 95.30% 95.42% 89.55% 91.62% 92.24% 94.02±0.05%\n\n69.67% 73.55% 76.64% 77.40% 73.96% 76.24% 77.01% 71.32±0.20%\n\n10 10 10 10\n\n75.40% 83.17% 76.37%±0.26 83.83±0.02%\n\nANN-to-SNN (Li et al., 2021a) ANN-to-SNN (Bu et al., 2021) Sew-ResNet (Fang et al., 2021) TET (Deng et al., 2022) TET (Deng et al., 2022) CPNG\n\nResNet34 ResNet34 Sew-ResNet34 Spiking-Sew-ResNet34 Sew-ResNet34 Sew-ResNet34\n\n32 64 128 16 32 64 4\n6 4\n4\n\n64.54% 71.12% 73.45% 59.35% 69.37% 72.35% 67.04% 64.79% 68.00% 69.09%\n\ncase of using surrogate gradients with infinite non-zero gradient interval, the representative interval for calculating the indicator is [ν − 1/β, ν + 1/β], if we need a smaller interval [ν − r1, ν + r1] with the current mean and standard deviation, just set the new β to 1/r1. The experimental results in table3 also demonstrate the effectiveness of our approach (More results are shown in Appendix A.6).\n\nFinally, we verify the compatibility of CPNG with existing direct training methods TET (Deng et al., 2022)(table3), we got a 2.4% improvement on the ResNet18+CIFAR-DVS experiment.\n\n5.3 COMPARISON TO EXISTING WORKS\n\nIn this section, the experimental results we report all use triangle-like surrogate gradient. On some datasets, the current SOTA conversion method performs better than direct training, but they need lengthy simulation time steps, especially on the ImageNet dataset. In addition, the conversion-based method cannot be applied to neuromorphic datasets such as CIFAR10-DVS. Hence, it’s also necessary to design a surrogate gradient search method to optimize the direct training.\n\nVGG16 + CIFAR100. We train the SNN for 300 epochs on 1 TITAN, with the batch size of 256 and the time step of 5. As shown in Table .6, CPNG manages to improve 2.24% accuracy. To show that CPNG effectively controls the rise of χ, we report the χ during training in Fig. 3. The result of ResNet20+CIFAR100 is also shown in Table .6.\n\nResNet19 + CIFAR10. We use ResNet19 (Zheng et al., 2020) in accordance with tdBN layers, the batch size of 256 and the time step of 6. Then we train the SNN for 200 epochs on 4 TITAN, CPNG achieves an increase in test accuracy to 94.02%, while its counterpart is 93.26% (Table .6).\n\nResNet18 + CIFAR10-DVS. We train the model for 200 epochs on 2 TITAN Xp cards, with the time step of 10 and the batch size of 72. The CPNG obtains a higher test accuracy of 76.37% while\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: In the experiment of VGG16, the proportion of non-zero gradient membrane potentials of neurons in different layers without CPNG (left) and with CPNG (right).\n\nFigure 4: Left: Final β of each layer in VGG16 experiment, the initial β is 1.0. Right: Final β of each layer in Sew-ResNet34 experiment, the initial β is 1.0..\n\nconsuming significantly less overhead than Dspike (Li et al., 2021b). In addition, the combination of CPNG and TET can achieve an average test accuracy of 83.83%, which is a new SOTA.\n\nSew-ResNet34 + ImageNet. We use the same structure and membrane decay rate, etc. as SewResNet (Fang et al., 2021). We train the model for 160 epochs on 8 GTX 3090 cards with a time step of 4 and the batch size of 544. Using only CPNG can achieve an accuracy of 69.04%, surpassing the current direct training SOTA.\n\n5.4 SURROGATE GRADIENT SHAPES OF DIFFERENT LAYERS\n\nWe show the final β of some experiments in Fig.4, and all experimental results are presented in the appendix A.3. Various layers’ β are different, which demonstrates that various layers match distinct surrogate gradient shapes as a result of their varying membrane potential distributions.\n\nCPNG eventually increases the β of most layers. Compare to CPNG, randomly increasing the β can make the network difficult to train (Fig. 1(left)). Even with β set to 1.5, which most of the neuron layers shown in Fig. 4(left) can approach or reach, the network is still difficult to train. This demonstrates that it is safe to increase β using CPNG, while it is unsafe to increase β arbitrarily.\n\n6 CONCLUSION\n\nThis work proposes a new perspective for directing the shape change of the surrogate gradient, we propose a statistical indicator χ that guides the shape change of the surrogate gradient, and propose the CPNG method for modifying the shape of the surrogate gradient during training while guaranteeing the proportion of membrane potential with non-zero gradients. It’s possible that the failure to produce satisfactory results when pulling surrogate gradient directly to δ-function is due to a failure to meet the premise that the network can be trained normally. In other words, there may exists a trade-off between the approximation to the δ-function and the effective domain of gradients under the given dataset, and CPNG helps us approach the equilibrium point.\n\n9\n\nχUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nGuillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. Long short-term memory and learning-to-learn in networks of spiking neurons. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ c203d8a151612acf12457e4d67635a95-Paper.pdf.\n\nTong Bu, Wei Fang, Jianhao Ding, PengLin Dai, Zhaofei Yu, and Tiejun Huang. Optimal ann-snn conversion for high-accuracy and ultra-low-latency spiking neural networks. In International Conference on Learning Representations, 2021.\n\nLei Deng, Yujie Wu, Xing Hu, Ling Liang, Yufei Ding, Guoqi Li, Guangshe Zhao, Peng Li, and Yuan Xie. Rethinking the performance comparison between snns and anns. Neural Networks, 121: 294–307, 2020.\n\nShikuang Deng and Shi Gu. Optimal conversion of conventional artificial neural networks to spiking\n\nneural networks. In International Conference on Learning Representations, 2020.\n\nShikuang Deng, Yuhang Li, Shanghang Zhang, and Shi Gu. Temporal efficient training of spiking\n\nneural network via gradient re-weighting. arXiv preprint arXiv:2202.11946, 2022.\n\nPeter U. Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, and Shih Chii Liu. Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing. In Neural Networks (IJCNN), 2015 International Joint Conference on, 2015.\n\nPeter U Diehl, Guido Zarrella, Andrew Cassidy, Bruno U Pedroni, and Emre Neftci. Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware. In 2016 IEEE International Conference on Rebooting Computing (ICRC), pp. 1–8. IEEE, 2016.\n\nWei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timoth ́ee Masquelier, and Yonghong Tian. Deep\n\nresidual learning in spiking neural networks. arXiv preprint arXiv:2102.04159, 2021.\n\nBing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13558–13567, 2020.\n\nYunzhe Hao, Xuhui Huang, Meng Dong, and Bo Xu. A biologically plausible supervised learning method for spiking neural networks using the symmetric stdp rule. Neural Networks, 121:387–395, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\n\nnetworks. In European conference on computer vision, pp. 630–645. Springer, 2016.\n\nLuca Herranz-Celotti and Jean Rouat. Surrogate gradients design. arXiv preprint arXiv:2202.00282,\n\n2022.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nYingyezhe Jin, Wenrui Zhang, and Peng Li. Hybrid macro/micro level backpropagation for training\n\ndeep spiking neural networks. arXiv preprint arXiv:1805.07866, 2018.\n\nChankyu Lee, Priyadarshini Panda, Gopalakrishnan Srinivasan, and Kaushik Roy. Training deep spiking convolutional neural networks with stdp-based unsupervised pre-training followed by supervised fine-tuning. Frontiers in neuroscience, 12:435, 2018.\n\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape\n\nof neural nets. arXiv preprint arXiv:1712.09913, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu. A free lunch from ann: Towards efficient, accurate spiking neural networks calibration. arXiv preprint arXiv:2106.06984, 2021a.\n\nYuhang Li, Yufei Guo, Shanghang Zhang, Shikuang Deng, Yongqing Hai, and Shi Gu. Differentiable spike: Rethinking gradient-descent for training spiking neural networks. Advances in Neural Information Processing Systems, 34, 2021b.\n\nEmre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019.\n\nNitin Rathi and Kaushik Roy. Diet-snn: Direct input encoding with leakage and threshold optimization\n\nin deep spiking neural networks. arXiv preprint arXiv:2008.03658, 2020.\n\nBodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, and Michael Pfeiffer. Theory and tools for the conversion of analog to spiking convolutional neural networks. arXiv preprint arXiv:1612.04052, 2016.\n\nBodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. Frontiers in neuroscience, 11:682, 2017.\n\nAbhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going deeper in spiking\n\nneural networks: Vgg and residual architectures. Frontiers in neuroscience, 13:95, 2019.\n\nAmar Shrestha, Khadeer Ahmed, Yanzhi Wang, and Qinru Qiu. Stable spike-timing dependent plasticity rule for multilayer unsupervised and supervised learning. In 2017 international joint conference on neural networks (IJCNN), pp. 1999–2006. IEEE, 2017.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi ́egas, and Martin Wattenberg. Smoothgrad:\n\nremoving noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n\nKazuma Suetake, Shin-ichi Ikegawa, Ryuji Saiin, and Yoshihide Sawada. Sˆ2 nn: Time step reduction of spiking surrogate gradients for training energy efficient single-step neural networks. arXiv preprint arXiv:2201.10879, 2022.\n\nYujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in neuroscience, 12:331, 2018.\n\nYujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. Direct training for spiking neural networks: Faster, larger, better. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 1311–1318, 2019.\n\nYukun Yang, Wenrui Zhang, and Peng Li. Backpropagated neighborhood aggregation for accurate In International Conference on Machine Learning, pp.\n\ntraining of spiking neural networks. 11852–11862. PMLR, 2021.\n\nFriedemann Zenke and Surya Ganguli. Superspike: Supervised learning in multilayer spiking neural\n\nnetworks. Neural computation, 30(6):1514–1541, 2018.\n\nFriedemann Zenke and Tim P Vogels. The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks. Neural computation, 33(4):899–925, 2021.\n\nWenrui Zhang and Peng Li. Temporal spike sequence learning via backpropagation for deep spiking\n\nneural networks. arXiv preprint arXiv:2002.10085, 2020.\n\nHanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger\n\nspiking neural networks. arXiv preprint arXiv:2011.05280, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 DATA PROCESSING\n\nCIFAR. For the training set, we randomly crop the image to (32,32) and apply a random horizontal flip. Then all the images will be normalized to a standard normal distribution.\n\nImageNet. For the training set, we randomly crop the image to (224,224) and apply a random horizontal flip. Then for the test set, we resize the image to (256,256) and center crop the image to (224,224). Finally, all the images are normalized to a standard normal distribution.\n\nA.2 MORE EXPERIMENT DETAILS\n\nIn this section, we provide more experimental details. We use 1/20 of the training epochs for warm-up, linearly increase learning rate from 0.1lr to lr, then use cosine decay to reduce learning rate to 0 in the remaining epochs. In the last batch, the mean and variance of the membrane potential are additionally stored in the forward process, and after the backward process, the stored mean and variance are used to obtain a new β according to Algorithm 1. Hyperparameters are shown in the table 5.\n\nTable 5: Experiment Setting\n\nExperiment\n\nCIFAR100 CIFAR10 CIFAR-DVS\n\nImageNet CPNG + TET\n\nlearning rate weight decay momentum optimizer warm-up\n\n0.1 1e-4 0.9 sgd True\n\n0.1 1e-4 0.9 sgd True\n\n0.01 4e-5 0.9 sgd False\n\n0.01 4e-5 0.9 sgd False\n\n0.001 4e-5 —- adam False\n\nA.3 EXPERIMENT RESULT\n\nIn this section, we display the change of indicator χ(Fig. 5, Fig. 6), the final β and the test accuracy of each experiment. Our codes can be found in the supplemental. As shown by Fig. 7, Fig. 8, Fig. 9 and Fig. 10, except for the first layer of the Sew-ResNet34+ImageNet experiment, almost all neuron layers have obtained a steeper surrogate gradient (a larger β), which illustrates that surrogate gradient has further optimization space in the SNN training process. However, as we mentioned in Sec. 4.1, choosing a surrogate gradient closer to the δ−function at beginning will make training the network difficult, therefore we’ll need tools (such as CPNG) to assist us in finding a better surrogate gradient during training. In the Sew-ResNet34+ImageNet experiment, CPNG finds a smaller β for the first layer to ensure the proportion of membrane potential with non-zero gradients. This suggests that when backpropagating with surrogate gradient in deeper networks, the initial few layers may only have a small proportion of their parameters updated, and we should account for this more.\n\nFigure 5: In the experiment of ResNet19, the proportion of non-zero gradient membrane potentials of neurons in different layers without CPNG (left) and with CPNG (right).\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: The proportion of non-zero gradient membrane potentials of neurons in different layers with CPNG in the experiment of ResNet18 (left) and Sew-ResNet34 (right).\n\nFigure 7: Left: Test accuracy of triplicate VGG16+CIFAR100 experiments using CPNG. Right: Final β of each layer in CIFAR100 experiment, the initial β is 1.0\n\nA.4 USING BETA OBTAINED FROM CPNG TO TRAIN FROM SCRATCH\n\nWe fix each layer’s β the same as it in Figure 4 (left), the final accuracy is 71.02%, which is 0.3% lower than using CPNG directly and 1.94% higher than the baseline. We keep β = 1 during the first 1/10 epochs for warm-up, otherwise the accuracy will stay at 1%. This demonstrates that the optimal SG shape discovered by CPNG for all layers is valid, but the result under the case that directly changes the β to the optimal value is not as excellent as the case using the CPNG method due to non-stationary transitions.\n\nA.5 MEMBRANE POTENTIAL DISTRIBUTION\n\nIn this section, we show the membrane potential distribution of each layer of neurons in all experiments( Fig. 11, Fig. 12, Fig. 13). The layer after BN is more consistent with the normal distribution than the layer after FC. But for the convenience of calculation, we approximate the membrane potential distribution of all neurons to the normal distribution.\n\nA.6 ADJUST OTHER SURROGATE GRADIENTS\n\nThe results we report are based on triangle-like surrogate gradient, a surrogate gradient with finite non-zero gradient interval, but this does not mean that our method can only be used in this case. All results are shown in Tab. 6.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Left: Test accuracy of triplicate ResNet19+CIFAR10 experiments using CPNG. Right: Final β of each layer in CIFAR10 experiment, the initial β is 1.0\n\nFigure 9: Left: Test accuracy of triplicate ResNet18+CIFAR10-DVS experiments using CPNG. Right: Final β of each layer in CIFAR10-DVS experiment, the initial β is 1.0\n\nA.7 SALIENCY MAP\n\nThe gradient noise generated by surrogate gradient may affect the network’s attention location, resulting in a more blurred saliency map. And there’s no doubt that a more appropriate surrogate gradient will produce fewer gradient noises. Thus, the saliency map’s clarity may be utilized to determine the fitness of surrogate gradient. We used SmoothGrad (Smilkov et al., 2017), which reduces the effect of visually noise. The results of CIFAR100+VGG16 experiments are shown in Fig. 14. We discovered that CPNG can assist the model to identify clearer contour information. For example, in the boys category, the saliency map obtained by using CPNG is more accurately positioned on the person’s face, whereas traditional surrogate gradient also pays much attention to the surrounding background; in the elephants category, the use of CPNG can clearly see the trunk as well as the elephant’s outline, whereas traditional surrogate gradient can only obtain blurred borders.\n\nA.8 LOSS LANDSCAPE\n\nIn order to illustrate the trainability of the model after using CPNG, we use the loss landscape to show the change of loss after changing the model parameters. If the loss landscape does not reveal more non-convex regions than the traditional surrogate gradient, it means that using CPNG will not make convergence more difficult. If the same weight offset is applied for various networks when displaying the loss landscape, the networks with bigger weight will exhibit more stationarity. However, due to the existence of batch normalization, the weight scaling of the network has no effect on the inference results, so the sharpness of loss landscape of different networks may only be due to the weight scaling. To explain the model performance between the traditional surrogate gradient and CPNG, we use\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Left: Test accuracy of Sew-ResNet34+ImageNet experiments using CPNG. Right: Final β of each layer in ImageNet experiment, the initial β is 1.0\n\nFigure 11: Membrane distribution of each layer in experiment training VGG16 on CIFAR100\n\nthe loss landscape demonstration with filter-wise normalization (Li et al., 2017) that mitigates the effect of weight scaling and correlates the model’s generalization ability to the flatness of the loss landscape. As shown in Fig. 15, in the experiment of ResNet19+CIFAR10, utilizing CPNG can obtain a smoother minimum and a wider locally convex region, implying that CPNG has no negative effect on network’s convergence difficulty.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Membrane distribution of each layer in experiment training ResNet19 on CIFAR10\n\nFigure 13: Membrane distribution of each layer in experiment training ResNet18 on CIFAR-DVS\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Examine CPNG on various surrogate gradients.\n\nDataset\n\nMethod\n\nArchitecture Time Step\n\nAccuracy\n\nCIFAR10\n\nCIFAR100\n\nTriangular Triangular+CPNG Rectangular Rectangular+CPNG ArcTan ArcTan+CPNG\n\nTriangular Triangular+CPNG Rectangular Rectangular+CPNG ArcTan ArcTan+CPNG Triangular+CPNG\n\nResNet19 ResNet19 ResNet19 ResNet19 ResNet19 ResNet19\n\nVGG16 VGG16 VGG16 VGG16 VGG16 VGG16 ResNet20\n\nCIFAR10-DVS\n\nResNet18 ArcTan ResNet18 ArcTan+CPNG Triangular+TET ResNet18 Triangular+TET+CPNG ResNet18 ResNet18 Triangular ResNet18 Triangular+CPNG\n\n6 6\n6 6\n6 6\n\n5 5\n5 5\n5 5\n6\n\n10 10 10 10 10 10\n\n93.26% 94.02±0.05% 90.5% 93.07±0.04% 93.24% 93.95%\n\n69.08% 71.32±0.20% 65.54% 67.74±1.02% 68.15% 69.49±0.20% 76.09%\n\n67.2% 67.5% 79.9% 82.3% 75.6% 76.37±1.02 %\n\nFigure 14: Saliency map. Three photos are a group, the top is the original image, the middle is the salilency map given by the model trained by traditional surrogate gradient, and the bottom is the saliency map given by the model trained with CPNG.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 15: Loss landscape of traditional surrogate gradient and CPNG.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a method to improve the training of spiking neural networks (SNNs) via the surrogate gradient method. Their technique called CPNG controls the proportion of non-zero gradients during network backpropagation. Surrogate gradient functions are only non-zero when the membrane potential is within a certain region about the threshold value. The width of that region can be defined using a single parameter, which has so far remained fixed during training. In order to allow this parameter to vary optimally during training, the authors compute the proportion of neurons that have their membrane potential within that region of non-zero gradients, and adapt the region accordingly. If the proportion is high enough, the width decreases, and inversely, if the proportion is too small, the width increases, so that the overall activity of the network neither explodes nor vanishes. Their method shows performance improvements on all four tasks (CIFAR10, CIFAR100, CIFAR10-DVS and ImageNet) compared to the fixed width surrogate gradient.\n\n# Strength And Weaknesses\n\nStrengths\n\nThe paper contributes to improving the surrogate gradient training of SNNs, which should be practical and profitable for the related field of research. The method is relatively simple and well explained to be reproduced.\nThe computational costs of the method are properly indicated and remain limited.\n\nWeaknesses\n\nWhen comparing with the baseline (i.e. without CPNG), it may not be clear that the value of the fixed width is optimised. For a fairer comparison, a hyperparameter search could be done, so that CPNG is compared to the best version of the fixed surrogate gradient function.\nIn relation to that, in table 3 and 6, confidence intervals are only given for CPNG entries. It would be coherent to also include such intervals for the non-CPNG baseline.\nIt appears that the choice of value for the effective domain indicator is only based on a single ad-hoc experiment with two values considered, namely 0.05 and 0.2. Here It would also be more meaningful to make a more advanced hyperparameter search. \nThe grammar could be improved.\n\nRemarks\n\nGiven that membrane potentials evolve over time, it is not exactly clear to me how the effective domain indicator is calculated. I can guess that the mean “mu” is computed over all time steps and all neurons, but it may be clearer to indicate it more explicitly inside the text.\nIt could be interesting to relate the effective domain indicator to the resulting firing rates. Does increasing the former have an impact on the latter?\nThe method is also only tested using a relatively small number of time steps (5-10). It would be good either to increase the number of steps on the same tasks, or to directly apply it to tasks that involve longer sequences such as speech for instance.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is clear enough and appears original.\n\n# Summary Of The Review\n\nThe authors have the right approach; the method makes sense and is helpful.  However, the evaluation could be improved.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDEJA VU: CONTINUAL MODEL GENERALIZATION FOR UNSEEN DOMAINS\n\nChenxi Liu1∗, Lixu Wang1∗ ‡ †, Lingjuan Lyu2‡, Chen Sun2, Xiao Wang1, Qi Zhu1 1Northwestern University, 2Sony AI {chenxiliu2020,lixuwang2025}@u.northwestern.edu, {Lingjuan.Lv,chen.sun}@sony.com, {wangxiao,qzhu}@northwestern.edu\n\nABSTRACT\n\nIn real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation – in what we call the “Unfamiliar Period”, especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models’ target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a trainingfree data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA capabilities.\n\n1\n\nINTRODUCTION\n\nA major concern in applying deep learning models to real-world applications is whether they are able to deal with environmental changes over time, which present significant challenges with data distribution shifts. When the shift is small, deep learning models may be able to handle it because their robustness is often evaluated and improved before deployment. However, when the data distribution shifts significantly for a period of time, in what we call the “Unfamiliar Period”, model performance on new scenarios could deteriorate to a much lower level. For example, surveillance cameras used for environmental monitoring can work normally with excellent performance on clear days, but have inferior performance or even become “blind” when the weather turns bad or the lighting conditions become poor (Bak et al., 2018). As another example, consider conducting lung imaging analysis for corona-viruses, deep learning models may present excellent performance after being trained on a large number of samples for certain variant (e.g., the Alpha variant of COVID19), but are difficult to provide accurate and timely analysis for later variants (e.g., the Delta or Omicron variant) and future types of corona-viruses (Singh et al., 2020) when they just appear. In the following, we will first discuss related works, highlight their limitations in addressing the poor model performance during the Unfamiliar Period, and then introduce our approach.\n\n*Equal contributions (ordered alphabetically); ‡Corresponding authors. †Part of the work was done during an internship at Sony AI.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nDomain adaptation (DA) methods have been proposed to tackle continual data drifts in dynamic environments in either online or offline mode. For example, Continual DA (Liu et al., 2020; Rostami, 2021) starts from a labeled source domain and continually adapts the model to various target domains, while keeping the model performance from degrading significantly on seen domains. However, existing Continual DA works often assume that the source domain can be accessed all the time, which may be difficult to guarantee in practical scenarios, especially considering the possible limitation on memory storage and regulations on privacy or intellectual property. Source-Free DA (Yang et al., 2021; Qu et al., 2022) can overcome this issue and achieve target adaptation without the source domain data. In addition, Test-Time or Online DA (Wang et al., 2022; Iwasawa & Matsuo, 2021; Panagiotakopoulos et al., 2022) can improve the target model performance with a small training cost; however, the target domain data is only learned once by the model and the performance improvement is limited (higher improvement would require a large amount of data). With these DA methods, although the model may perform better on the new target domain after sufficient adaptation, its performance on the target domain before and during the adaptation process, i.e., in the Unfamiliar Period, is often poor. In cases where the domain shift is sudden and the duration of seeing a new target domain is short, this problem becomes even more severe. In this work, we believe that for many applications, it is very important to ensure that the model can also perform reasonably well in the Unfamiliar Period, i.e., before seeing a lot of target domain data. For instance in environmental surveillance, having poor performance under uncommon/unfamiliar weather or lighting conditions may cause significant security and safety risks. In the example of lung imaging analysis for corona-viruses, being able to quickly provide good performance for detecting new variants is critical for the early containment and treatment of the disease.\n\nDomain generalization (DG) methods also solve the learning problem on multiple data domains, especially for cases where the target domain is unavailable or unknown during training. However, existing DG works are typically based on accurate supervision knowledge of the source domain data, whether it is drawn from a single domain (Wang et al., 2021c; Li et al., 2021) or multiple domains (Yao et al., 2022; Zhang et al., 2022), which may not be achievable in continually changing scenarios. Moreover, when DG is applied in scenarios with continual domain shifts, as it focuses more on the target domain, there could be severe catastrophic forgetting of domains that have been learned. There are also some works unifying DA and DG (Ghifary et al., 2016; Motiian et al., 2017; Jin et al., 2021); however they can only be used in standard DA or DG individually, thus still suffering their limitations. Bai et al. (2022) and Nasery et al. (2021) study the smooth temporal shifts of data distribution, but they cannot handle large domain shifts over time.\n\nOur Approach and Contribution. In this work, we focus on the study of Continual Domain Shift Learning (CDSL) problem, in which the learning model is first trained on a labeled source domain and then faces a series of unlabeled target domains that appear continually. Our goal, in particular, is to improve model performance before and during the training stage of each previously-unseen target domain (i.e., in the Unfamiliar Period), while also maintaining good performance in the time periods after the training. Thus, we propose a framework called RaTP that optimizes three objectives: (1) to improve the model generalization performance on a new target domain before and during its training, namely the target domain generalization (TDG) performance, (2) to provide good model performance on a target domain right after its training, namely the target domain adaptation (TDA) performance, and (3) to maintain good performance on a trained domain after the model is trained with other domains, namely the forgetting alleviation (FA) performance. For improving TDG, RaTP includes a training-free data augmentation module that is based on Random Mixup, and this module can generate data outside of the current training domain. For TDA, RaTP includes a Top2 Pseudo Labeling mechanism that lays more emphasis on samples with a higher possibility of correct classification, which can produce more accurate pseudo labels. Finally, for optimizing the model towards TDG, TDA, and FA at the same time, RaTP includes a Prototype Contrastive Alignment algorithm. Comprehensive experiments and ablation studies on Digits, PACS, and DomainNet demonstrate that RaTP can significantly outperform state-of-the-art works in TDG, including Continual DA, SourceFree DA, Test-Time/Online DA, Single DG, Multiple DG, and Unified DA&DG. RaTP can also produce comparable performance in TDA and FA as these baselines. In summary:\n\n• We tackle an important problem in practical scenarios with continual domain shifts, i.e., to improve model performance before and during training on a new target domain, in what we call the Unfamiliar Period. And we also try to achieve good model performance after training, providing the model with capabilities of target domain adaptation and forgetting alleviation.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Overview of applying our framework RaTP for Continual Domain Shift Learning (CDSL). RaTP first starts with a labeled source domain, applies RandMix on the full set of source data to generate augmentation data, and uses a simplified version L′ of PCA for model optimization. Then, for continually arriving target domains, RaTP uses T2PL to generate pseudo labels for all unlabeled samples, applies RandMix on a top subset of these samples based on their softmax confidence, and optimizes the model by PCA.\n\n• We propose a novel framework RaTP to achieve our goal. The framework includes a trainingfree data augmentation module that generates more data for improving model’s generalization ability, a new pseudo-labeling mechanism to provide more accurate labels for domain adaptation, and a prototype contrastive alignment algorithm that effectively aligns domains to simultaneously improve the generalization ability and achieve target domain adaptation and forgetting alleviation.\n\n• We conducted extensive experiments and comprehensive ablation studies that demonstrate the\n\nadvantages of our RaTP framework over a number of state-of-the-art baseline methods.\n\n2 METHODOLOGY\n\nWe first formulate the problem of Continual Domain Shift Learning (CDSL) in Section 2.1, and then introduce the three major modules of our framework RaTP. Specifically, Section 2.2 presents a Random Mixup data augmentation module RandMix that generates data for improving the target domain generalization (TDG) on unadapted domains, which is a crucial technique for improving model generalization in the Unfamiliar Period. Section 2.3 presents a Top2 Pseudo Labeling approach T2PL that provides accurate labels to achieve target domain adaptation (TDA) for continually arriving target domains. Finally, Section 2.4 presents a Prototype Contrastive Alignment algorithm PCA that optimizes the model to achieve TDG, TDA, and forgetting alleviation (FA) on seen domains. Figure 1 shows the overall pipeline of RaTP.\n\n2.1 PROBLEM FORMULATION OF CONTINUAL DOMAIN SHIFT LEARNING\n\nY }NS\n\nX }NT t\n\nX , yi ∼ P S\n\nWe assume that there is a labeled source domain dataset S = {(xi, yi)∥xi ∼ P S i=1 at the beginning of CDSL. Here PX and PY are the input and label distributions, respectively, while NS is the sample quantity. This paper chooses visual recognition as the learning task, in which the number of data classes is K. Then, we consider a sequence of continually arriving target domains T = {T t}T t=1, where T denotes the total number of domains. The t-th domain contains NT t data samples x but no labels y, i.e., T t = {xi∥xi ∼ P T t i=1 , and all the target domains share the same label space with the source domain. Note that the domain order is randomly determined (including the source domain), which means that the superscript t does not always indicate the same domain. The generalization performance for the t-th domain T t depends on both the previously seen t−1 target domains and the original source domain, i.e., S = {S, ∪t−1 j=1T j}. Considering the possible limitation on memory storage and regulations on privacy (Guo et al., 2021) or intellectual property (Wang et al., 2021b), like regular continual learning (Dong et al., 2022a), we also set an exemplar memory M to store |M| exemplars that are closest to the class centroids for each domain in S, where |M| t ≪ NS . Moreover, the memory size is set to be fixed during CDSL. We assume that the model is a deep neural network, and without loss of generality, the neural network consists of a feature extractor fθ at the bottom and a classifier gω at the top. For each target domain θ ◦ gt dataset Tt, in addition to the current model f t ω from the last\n\nω, its inherited version f t−1\n\nt ≪ NT , |M|\n\n◦ gt−1\n\nt\n\nθ\n\n3\n\nPerformanceTimeUnfamiliar PeriodSourceDomainLabeledSamplesAfterAdaptationFeatureExtractorClassifier1st TargetDomainUnlabeledSamplesPseudo-labeledSamplesAfter SplittingAugmentationFeatureExtractorClassifierTop SetExisting MethodProposed RaTPSource Domain1st Target DomainPublished as a conference paper at ICLR 2023\n\ndomain is also stored for later usage. We aim to improve the generalization performance on a new target domain (TDG) before and during its training, i.e., in its Unfamiliar Period of CDSL, while also achieving effective target domain adaptation (TDA) and forgetting alleviation (FA).\n\n2.2 RANDOM MIXUP AUGMENTATION\n\nAt the beginning of CDSL, only a labeled source domain is available. Thus we face a pure singedomain generalization problem where the model is trained on a single domain S and tested on the other unseen domains T (each domain in T is possible as the domain order is random). We apply data augmentation and design a Random Mixup (RandMix) method to solve this problem. RandMix is not only helpful to improve cross-domain transferability from S to T 1, but also beneficial for remaining model’s order agnostic generalization when it encounters low-quality domains. RandMix relies on Naug simple autoencoders R = {Ri}Naug i=1 , where each autoencoder consists of an encoder eξ and a decoder dζ. We want RandMix to be as simple as possible, preferably to work without training. Inspired by (Wang et al., 2021c), the encoder eξ and the decoder dζ are implemented as a convolutional layer and a transposed convolutional layer. With such implementation, even if the parameters ξ and ζ are randomly initialized from a normalized distribution, the autoencoder can still generate reasonable augmentation data. In order to introduce more randomness, we apply AdaIN (Karras et al., 2019) to inject noise into the autoencoder. Specifically, the used AdaIN contains two linear layers lφ1 , lφ2, and when lφ1 and lφ2 are fed with a certain noisy input drawn from a normalized distribution (n ∼ PN(0,1)), they can produce two corresponding noisy outputs with smaller variances. As observed in our experiments (Section 3.1), blindly pushing augmentation data away from the original (Li et al., 2021; Wang et al., 2021c) possibly hurts the model generalization ability, and injecting randomness with a smaller variance is a better solution. The two noisy outputs are injected to the representations of the autoencoder as multiplicative and additive noises:\n\nR(x) = dζ (lφ1 (n) × lIN(eξ(x)) + lφ2 (n)) , where lIN(·) represents an element-wise normalization layer. RandMix works as feeding training data to all autoencoders and mixing the outputs with random weights drawn from a normalized distribution (w = {wi∥wi ∼ PN(0,1)}Naug i=0 ). Finally, the mixture is scaled by a sigmoid function σ(x) = 1/(1 + e−x):\n\n(1)\n\nR(x) = σ\n\n\n\n\n\n1\n\n(cid:80)Naug\n\ni=0 wi\n\n\n\nw0x +\n\nNaug (cid:88)\n\ni=1\n\n\n\n\n\n(wiRi(x))\n\n\n\n .\n\n(2)\n\nIn mini-batch training, every time there is a new data batch, parameters of autoencoders (ξ, ζ), AdaIN-s (φ1, φ2), AdaIN noisy inputs (n) and mixup weights (w) are all initialized again. With RandMix, we can generate augmentation data with the same labels corresponding to all labeled samples from the source domain. Then, these labeled augmentation samples will work together with the original source data and be fed into the model for training. However, for the following continually arriving target domains, conducting RandMix on all target data is unreasonable. In CDSL, all target domains T = {T t}T t=1 are unlabeled. While we can apply approaches to produce pseudo labels, the supervision is likely unreliable and inaccurate. Therefore, to avoid error propagation and accumulation, we augment a subset of the target data rather than the full set. Specifically, we determine whether a target sample is supposed to be augmented based on its prediction confidence:\n\n ̃x =\n\n(cid:26)R(x), ∅,\n\nif max [gω(fθ(x))]K ≥ rcon otherwise\n\n, x ∼ P T\n\nX ,\n\n(3)\n\nwhere max[·]K denotes the maximum of a vector with K dimensions, and rcon is a confidence threshold that is set to 0.8 (sensitivity analysis of rcon is provided in Appendix).\n\n2.3 TOP\n\n2 PSEUDO LABELING\n\nIn CDSL, all target domains arrive with only the data but no label. Thus we need a pseudo labeling mechanism to provide reasonably accurate supervision for subsequent optimizations. However, existing pseudo labeling approaches have various limitations. For example, softmax-based pseudo labeling (Lee et al., 2013) produces hard labels for unlabeled samples, but training with such hard labels sacrifices inter-class distinguishability within the softmax predictions. To preserve inter-class distinguishability, SHOT (Liang et al., 2020) proposes a clustering-based approach to align unlabeled samples with cluster centroids. However, when the model is applied in a new domain, it\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nis difficult to distinguish different classes since they are nearly overlapped together and the class boundary among them is extremely vague. Besides, such clustering-based methods usually use all training samples to construct centroids for each class. During clustering, for a particular cluster, samples that exactly belong to the same class are often not used well and those misclassified pose a negative impact on centroid construction. In this case, such negative impact hurts the pseudo labeling accuracy by resulting in low-quality centroids. To address these issues, we propose a novel mechanism called Top2 Pseudo Labeling (T2PL). Specifically, we use the softmax confidence to measure the possibility of correct classification for data samples, and select the top 50% set to construct class centroids:\n\n(cid:26)\n\nN\n\nT t rtop ·K\n\nIk = ∪\n\narg max x∈T t\n\n[gω,k(fθ(x))]\n\n(cid:27)\n\n, F = ∪K\n\nk=1 {∪i∈Ik,x∈T t{xi}} ,\n\n(4)\n\nwhere gω,k(·) denotes the k-th element of the classifier outputs. rtop controls the size of the selected top set, and if the data is class balanced, top 50% corresponds to rtop = 2. Then the top set F is used to construct class centroids by a prediction-weighted aggregation on representations:\n\npk =\n\n(cid:80)\n\nxi∈F gω,k (fθ(xi)) · fθ(xi) xi∈F gω,k (fθ(xi))\n\n(cid:80)\n\n.\n\n(5)\n\nSubsequently, we can compute the cosine similarity between these centroids and representations of all current unlabeled data, and also select the top half set of samples that have much higher similarity to the centroids than the rest for each class:\n\n(cid:26)\n\nN\n\nT t rtop·K\n\nI ′\n\nk = ∪\n\narg max x∈T t\n\n(cid:20) fθ(x) · p⊤\n\nk\n\n(cid:21)(cid:27)\n\n∥fθ(x)∥∥pk∥\n\n, F ′ = ∪K\n\nk=1\n\n(cid:110)\n\n∪i∈I′\n\nk,x∈T t{(xi, k)}\n\n.\n\n(6)\n\n(cid:111)\n\nThen we use F ′ to fit a k-Nearest Neighbor (kNN) classifier and assign the pseudo label as below. Note that kNN can decentralize the risk of misclassification in assigning pseudo labels by a single comparison between class centroids and data samples, and is more suitable for clustering samples that are highly overlapped than center-based approaches:\n\nˆy = kNN (x, F ′)\n\nN\n\nT t top ·K\n\nr′\n\nEuclidean ,\n\n(7)\n\nr′\n\nwhere the superscript NT t top·K and subscript Euclidean mean that the kNN works to find NT t top·K closest neighbors from F ′ with the measurement of Euclidean distance. We select the top 5% for each class (r′ top are provided in the Appendix). With this kNN, T2PL makes better use of unlabeled samples with a relatively high possibility of correct classification than SHOT, and provides more accurate pseudo labels.\n\ntop = 20) in our implementation (sensitivity analyses of rtop and r′\n\nr′\n\n2.4 PROTOTYPE CONTRASTIVE ALIGNMENT\n\nPrototype learning (PL) (Pan et al., 2019; Kang et al., 2019; Tanwisuth et al., 2021; Dubey et al., 2021) has been demonstrated to be effective for solving cross-domain sample variability and sample insufficiency (Wang et al., 2021a). These two issues are reflected in our problem as the random uncertainty of augmentation data and limited data quantity of seen domains in the exemplar memory M. As a result, in this work, we adopt the idea of PL and propose a new Prototype Contrastive Alignment (PCA) algorithm to align domains together for improving model generalization ability.\n\nk}K\n\nFirst, we need to point out that regular PL is unsuitable to our problem. In regular PL, all samples are fed into the model to extract representations, and these representations are aggregated class-byclass for constructing prototypes pt = {pt k=1 of a certain domain T t. To achieve cross-domain alignment, prototypes of different domains for the same classes are aggregated and averaged. Then data samples from different domains are optimized to approach these domain-average prototypes. However, such domain alignment has a problem of adaptivity gap (Dubey et al., 2021). As shown in Figure 2, we assume that there is an optimal prototype location p∗ k of class k for all domains (S ∪T), and the objective is to move the source prototype ps k to the optimal by training on target domains (T) one-by-one. Meanwhile, we also regard that there are optimal prototypes pt k for each domain itself, and suppose that the regular PL is applied to align prototypes of all these domains. To illustrate the problem of adaptivity gap, we take the alignment between the source S and the first target domain T 1 as an example, and assume that the distance between p1 k is larger than the distance between ps k. In this case, after the domain alignment, the location of the current prototype\n\nk and p∗\n\nk and p∗\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b) Figure 2: (a): When the model encounters a domain whose prototypes (p1 k) are far from the optimal ones (p∗ k), domain alignment with regular prototype learning will enlarge the adaptivity gap. (b): RandMix may be helpful to reduce the gap in many cases of ( ̃ps (c): However, in some cases RandMix may produce low-quality data augmentation ( ̃ps′ k ) that negatively affects the regular prototype learning and enlarges the gap. Here, we have Adaptivity Gap 3 > Adaptivity Gap 1 > Adaptivity Gap 2.\n\nk , ̃p1′\n\nk, ̃p1\n\nk).\n\n(c)\n\nFigure 3: Accuracy Matrix. For each domain, TDG: mean of green elements; TDA: red elements; FA: mean of blue elements.\n\nis worse than the source prototype, which means that the adaptivity gap is enlarged. Although we can use RandMix to generate more data ( ̃ps k) that might be helpful for reducing the adaptivity gap, there is randomness for each use of RandMix and we cannot guarantee such reduction every time (e.g., when the augmentation data is as ̃ps′ k in Figure 2). Therefore, a better prototype construction strategy is needed to provide more representative prototypes.\n\nk and ̃p1′\n\nk, ̃p1\n\nInspired by a semi-supervised learning work (Saito et al., 2019), we view neuron weights of the classifier ω as the prototypes. The classifier gω consists of a single linear layer and is placed exactly behind the feature extractor fθ. Thus the dimension of its neuron weights is the same as the hidden dimension of extracted representations. Prototypes pt\n\nk-s are obtained with a CrossEntropy Loss:\n\nLCE = E\n\nxi∼P T t\n\nX\n\n(cid:34) K\n\n(cid:88)\n\nk=1\n\n− log\n\nI ˆyi=k exp (cid:0)pt⊤ (cid:80)K c=1 exp (pt⊤\n\nk fθ(xi) + bk c fθ(xi) + bc)\n\n(cid:1)\n\n(cid:35)\n\n,\n\n(8)\n\nwhere bk, bc are biases of the linear layer, and we set them as zeros during training, and I(·) is an indicator function that if the subscript condition is true, ITrue = 1, otherwise, IFalse = 0.. Compared with regular prototypes, such linear layer prototypes are built on a lot more data because RandMix is initialized and used for every mini-batch during model training. Moreover, RandMix has general effect on improving generalization ability, as shown in our experiments later in Section 3.2. Thus we believe that linear layer prototypes have smaller adaptivity gaps than regular ones.\n\nWith linear layer prototypes, we can align domains in a better way. Unlike the alignment on regular prototypes, we do not sum up and average prototypes of different domains. Instead, we simultaneously maximize the similarities of data samples to prototypes of different domains, although in practice we may have prototypes of just two domains since there is only one old model being stored (Section 2.1). We also introduce contrastive comparison into the domain alignment process that can enhance the distinguishability of different classes and be beneficial for our pseudo labeling. The contrastive comparison includes negative pairs of a particular sample and samples from other classes. In this case, our prototype contrastive alignment is formulated as follows:\n\nLPCA = E\n\nxi∼P T t\n\nX\n\n(cid:40) K\n\n(cid:88)\n\nk=1\n\nI ˆyi=k\n\n− log\n\n(cid:2)exp (cid:0)pt⊤\n\nk fθ(xi)(cid:1) + exp (cid:0)pt−1⊤\n\nk\n\n∆\n\nfθ(xi)(cid:1)(cid:3)\n\n(cid:41)\n\n, where\n\n∆ =\n\nK (cid:88)\n\nc=1\n\nexp (cid:0)pt⊤\n\nc fθ(xi)(cid:1)+\n\nK (cid:88)\n\nc=1\n\nexp (cid:0)pt−1⊤\n\nc\n\nfθ(xi)(cid:1)+\n\n(cid:88)\n\nI ˆyi̸= ˆyj exp (cid:0)fθ(xi)⊤fθ(xj)(cid:1) .\n\nxj ∈T t,j̸=i\n\n(9)\n\nFurthermore, we also adopt knowledge distillation from the stored old model to the current model for forgetting compensation. In this case, our final optimization objective is shown as follows:\n\nL = LCE + LPCA + LDIS, where LDIS = DKL\n\n(cid:2)gt−1\n\nω\n\n(cid:0)f t−1\n\nθ\n\n(x)(cid:1) ∥ gt\n\nω\n\n(cid:0)f t\n\nθ(x)(cid:1)(cid:3)\n\nx∼P T t\n\nX\n\n.\n\n(10)\n\nNote that this optimization objective is used in all target domains. As for the source domain, there is no distillation LDIS and the nominator of LPCA only contains the first term, because there is no stored old model in the stage of source training. We denote this simplified optimization loss as L′.\n\n6\n\nAdaptivityGap 1AdaptivityGap 2AdaptivityGap 3Stage/Domain1...T1...TPublished as a conference paper at ICLR 2023\n\n3 EXPERIMENTS\n\nOur code is available at https://github.com/SonyAI/RaTP. The datasets, experiment settings, and comparison baselines are introduced below, and more details are in the appendix.\n\nDatasets. Digits consists of 5 different domains with 10 classes, including MNIST (MT), SVHN (SN), MNIST-M (MM), SYN-D (SD) and USPS (US). PACS contains 4 domains with 7 classes, including Photo (P), Art painting (A), Cartoon (C), and Sketch (S). DomainNet is the most challenging cross-domain dataset, including Quickdraw (Qu), Clipart (Cl), Painting (Pa), Infograph (In), Sketch (Sk) and Real (Re). Considering label noise and class imbalance, we follow Xie et al. (2022) to split a subset of DomainNet for our experiments.\n\nExperiment Settings. We apply ResNet-50 as the feature extractor for both PACS and DomainNet, and apply DTN as the feature extractor for Digits (Liang et al., 2020). In the source domain, we randomly split 80% data as the training set and the rest 20% as the testing set. In target domains, all data are used for training and testing. The SGD optimizer with an initial learning rate of 0.01 is used for Digits, and 0.005 for PACS and DomainNet. The exemplar memory size is set as 200 for all datasets, and the batch size is 64. For all experiments, we conduct multiple runs with three seeds (2022, 2023, 2024), and report the average performance.\n\nComparison Baselines. RaTP is compared with a comprehensive set of state-of-the-art works from Continual DA [CoTTA (Wang et al., 2022), AuCID (Rostami, 2021)], Source-Free DA [SHOT (Liang et al., 2020), GSFDA (Yang et al., 2021), BMD (Qu et al., 2022)], Test-Time/Online DA [Tent (Wang et al., 2020), T3A (Iwasawa & Matsuo, 2021)], Single DG [L2D (Wang et al., 2021c), PDEN (Li et al., 2021)], Unified DA&DG [SNR (Jin et al., 2021)], and Multiple DG [PCL (Yao et al., 2022), EFDM (Zhang et al., 2022)].\n\nTable 1: Performance comparisons between ours and other methods on Digits in TDG, TDA, and FA under two domain orders (shown with ↓). We blue and bold the best, and bold the second best.\n\nOrder\n\nMetric\n\nTA CoT\n\nAuCID\n\nOT SH\n\nA\n\nGSFD\n\nD M\nB\n\nTent\n\nT3A\n\nL2D\n\nN E\nPD\n\nR SN\n\nL PC\n\nM\n\nEFD\n\nOurs\n\nMT ↓\nMM ↓\nSN ↓\nSD ↓\nUS\n\nUS ↓\nSD ↓\nSN ↓\nMM ↓\nMT\n\nG D\nT\n\nA D\nT\n\nFA\n\nG D\nT\n\nA D\nT\n\nFA\n\nMM 53.3 42.8 55.6 54.8 55.3 53.1 51.3 73.6 68.4 68.9 51.7 48.0 23.6 16.1 25.9 28.2 24.9 29.7 22.9 35.4 35.6 35.4 22.8 12.6 SN 33.8 31.4 44.1 29.3 42.6 38.0 33.9 52.6 54.1 55.8 35.0 15.1 SD 88.9 81.6 69.0 51.6 73.9 87.8 77.4 87.7 83.5 80.5 71.9 35.5 US 49.9 43.0 48.7 41.0 49.2 52.2 46.4 62.3 60.4 60.2 45.4 27.8 Avg. 99.0 99.1 99.4 99.4 99.4 99.0 99.0 99.3 99.1 99.2 99.2 99.2 MT MM 40.7 42.3 85.3 54.5 80.7 52.8 50.2 86.1 84.7 84.1 53.0 12.9 18.6 18.2 33.4 13.1 34.7 25.5 23.2 53.8 50.8 53.3 22.6 SN 8.0 25.4 30.0 41.6 13.1 45.4 37.2 31.2 58.7 51.6 43.7 28.2 10.5 SD 89.5 79.6 75.7 14.7 81.2 85.1 83.5 90.3 90.2 90.2 58.0 15.4 US 54.6 53.8 67.1 39.0 68.3 59.9 57.4 77.6 75.3 74.1 52.2 29.2 Avg. 93.6 93.6 73.6 33.9 78.8 93.2 88.4 92.2 91.2 91.9 83.0 31.6 MT MM 32.9 49.2 39.0 10.0 44.2 50.1 46.4 70.8 69.7 70.9 36.3 15.4 17.3 24.7 16.0 56.0 50.4 54.2 20.6 10.3 16.5 22.5 16.0 SN 28.4 30.0 41.1 10.8 45.7 35.3 26.1 70.1 68.8 71.1 27.2 15.7 SD 42.9 48.8 42.4 15.8 46.5 50.8 44.2 72.3 70.0 72.0 41.8 18.3 Avg.\n\n8.5\n\n36.6 35.8 37.7 37.2 37.4 37.5 36.4 61.3 60.2 60.7 35.3 37.0 SD 17.6 19.2 18.4 22.0 18.3 27.2 20.6 52.7 51.5 52.2 21.4 13.4 SN MM 29.7 36.2 25.3 33.4 29.4 42.1 34.0 64.5 60.8 60.2 33.5 17.7 52.8 63.4 37.4 55.6 56.5 76.7 61.0 83.4 82.7 80.7 62.0 29.0 MT 34.2 38.7 29.7 37.1 35.4 45.9 38.0 65.5 63.8 63.5 38.1 24.3 Avg. 98.5 98.6 98.6 98.3 98.6 98.5 98.5 99.0 98.8 98.9 98.6 98.3 US 23.2 24.3 32.9 40.2 30.9 39.8 25.7 74.8 71.8 71.8 37.1 SD 9.6 13.2 17.6 21.6 17.9 23.4 29.5 22.0 72.1 67.4 64.7 19.5 10.1 SN MM 25.7 30.4 19.4 20.0 29.6 39.7 36.1 78.9 76.7 74.4 29.4 10.8 42.8 53.4 30.3 19.9 53.8 78.8 64.3 89.6 86.8 83.2 54.8 19.8 MT 40.7 44.9 40.6 39.3 47.3 57.3 49.3 82.9 80.3 78.6 47.9 29.7 Avg. 95.2 90.3 44.2 49.9 57.2 98.5 80.4 93.4 90.5 91.3 81.8 10.7 US 20.1 25.7 22.3 22.4 26.1 37.7 25.9 71.9 70.7 68.8 26.6 11.0 SD 14.1 28.6 13.8 56.8 55.6 55.0 17.7 14.3 SN 11.8 19.1 13.5 MM 26.1 29.1 14.5 10.1 22.7 41.2 14.9 74.4 71.3 70.9 29.1 27.2 38.3 41.1 23.6 22.8 30.0 51.5 33.8 74.1 72.0 71.5 38.8 15.8 Avg.\n\n8.6\n\n86.8 43.7 67.5 90.1 72.0 99.4 90.2 69.1 81.8 93.4 86.8 95.4 82.1 70.5 81.3 82.3\n\n68.6 62.0 68.7 87.3 71.7 98.9 82.6 78.7 81.8 93.3 87.1 96.2 82.9 68.4 83.5 82.8\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n3.1 EFFECTIVENESS OF THE RATP FRAMEWORK\n\nWe assume that each domain corresponds to a single training stage. After the training of every stage, we test the model performance on all domains. In this case, we can obtain an accuracy matrix, as shown in Figure 3. Target domain generalization (TDG) is measured by the mean model performance on a domain before its training stage (mean of the green elements in Figure 3), target domain adaptation (TDA) by the performance on a domain right after finishing its training stage (the red element), and forgetting alleviation (FA) by the mean performance on a domain after the model has been updated with other domains (mean of the blue elements). All these metrics are the higher, the better. Due to space limitation, we report experiment results for Digits (Tables 1) and PACS (Table 2) in two domain orders, and for DomainNet (Table 3) in one domain order. Results for more domain orders can be found in the Appendix. We can observe that RaTP achieves significantly higher performance in TDG than all baselines in all cases, and comparable (and in many cases higher) performance in TDA and FA. Specifically, RaTP significantly outperforms the secondbest baseline on TDG in all cases and by 3.1∼10.3% in average, and the performance improvement is particularly large over DA methods. For TDA and FA, RaTP is also the best in most cases (and clearly outperforms the second best in many cases); and if not, very close to the best. These results demonstrate that our method can effectively improve the model performance during the Unfamiliar Period, adapt the model to different domains, and alleviate catastrophic forgetting on seen domains.\n\nTable 2: Performance comparisons between ours and other methods on PACS in TDG, TDA, and FA under two domain orders (shown with ↓). We blue and bold the best, and bold the second best.\n\nOrder\n\nMetric\n\nTA CoT\n\nAuCID\n\nOT SH\n\nA\n\nGSFD\n\nD M\nB\n\nTent\n\nT3A\n\nL2D\n\nN E\nPD\n\nR SN\n\nL PC\n\nM\n\nEFD\n\nOurs\n\nP ↓\nA ↓\nC ↓\nS\n\nS ↓\nC ↓\nA ↓\nP\n\nG D\nT\n\nA D\nT\n\nFA\n\nG D\nT\n\nA D\nT\n\nFA\n\nA C\nS Avg. P\nA C\nS Avg. P\nA C\nAvg.\n\nC A\nP Avg. S\nC A\nP Avg. S\nC A\nAvg.\n\n71.1 67.0 68.2 75.0 68.4 71.6 71.1 67.5 67.0 67.4 66.4 66.5 47.0 29.9 32.0 44.9 34.9 48.5 35.9 49.2 47.8 42.7 42.5 37.7 46.3 40.8 29.0 46.4 31.3 48.6 38.1 52.7 52.5 45.3 41.4 44.0 54.8 45.9 43.1 55.4 44.9 56.2 48.4 56.5 55.8 51.8 50.1 49.4 99.4 99.4 97.0 98.8 97.0 99.4 99.4 99.1 99.2 99.0 98.8 99.1 77.0 64.6 85.0 82.1 86.9 76.6 74.2 72.8 70.2 71.8 77.4 72.4 64.3 58.0 70.4 81.6 75.0 68.2 66.7 61.4 60.9 61.2 61.5 37.6 54.4 54.3 62.9 81.5 61.3 58.7 57.1 68.6 65.8 65.4 65.6 30.0 73.8 69.1 78.8 86.0 80.1 75.7 74.4 75.5 74.0 74.4 75.8 59.8 99.4 96.6 94.9 71.0 94.3 99.4 95.9 91.1 93.2 96.9 77.1 79.3 77.1 74.7 79.1 42.0 79.8 76.2 61.9 64.4 64.1 64.6 60.2 56.8 64.5 64.6 66.3 23.1 60.9 68.7 55.7 57.6 55.5 60.2 40.1 46.1 80.3 78.6 80.1 45.4 78.3 81.4 71.2 71.0 70.9 73.9 59.1 60.7\n\n58.4 50.5 46.4 70.1 46.5 58.3 58.4 63.7 63.1 63.1 50.1 64.2 49.8 40.8 42.0 46.7 43.7 50.4 47.8 52.0 50.8 51.5 36.4 33.6 54.7 50.8 71.3 60.0 70.9 54.2 63.7 70.2 68.9 70.0 63.3 34.9 54.3 47.4 53.2 58.9 53.7 54.3 56.6 62.0 60.9 61.5 49.9 44.2 95.4 95.5 94.7 96.2 94.7 95.4 95.4 96.3 96.0 96.1 96.1 95.9 68.3 65.3 86.6 79.4 87.9 67.1 75.3 79.5 79.7 80.0 73.9 70.1 58.0 67.9 88.1 83.4 88.9 57.3 72.9 69.7 68.3 69.8 55.7 14.5 55.2 75.1 95.0 69.0 95.8 58.5 77.9 86.6 85.9 87.0 89.6 9.8 69.2 76.0 91.1 82.0 91.8 69.6 80.4 83.0 82.5 83.2 78.8 47.6 96.1 72.4 88.5 89.6 87.6 96.1 86.2 87.1 88.6 88.1 90.7 83.4 68.5 69.9 84.7 67.7 86.6 67.8 64.9 76.7 75.3 76.6 72.0 53.5 58.1 51.7 84.6 76.4 84.7 58.3 61.6 70.3 70.9 69.1 61.2 20.6 74.2 64.7 85.9 77.9 86.3 74.1 70.9 78.0 78.3 77.9 74.7 52.5\n\n73.9 53.1 60.7 62.6 99.5 87.5 75.1 67.1 82.3 97.9 87.8 73.0 86.2\n\n69.6 68.7 83.0 73.8 96.5 77.1 87.7 97.3 89.7 94.0 80.8 87.7 87.5\n\n3.2 ABLATION STUDIES\n\nTo demonstrate the effectiveness of each module in RaTP, we conduct comprehensive ablation studies for RandMix, T2PL, and PCA on Digits with the domain order US→SD→SN→MM→MT.\n\nEffectiveness of RandMix. We first remove RandMix from RaTP and see how the model performance will change. We also try to apply RandMix on a few other baselines, including SHOT (Liang et al., 2020), GSFDA (Yang et al., 2021), and PCL (Yao et al., 2022). The experiment results in Figure 4(a) demonstrate that, removing RandMix from RaTP hurts the performance significantly, and attaching RandMix to other baseline methods clearly improves the model performance. Such observations prove the effectiveness of RandMix.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Performance comparisons between ours and other methods on DomainNet in TDG, TDA, and FA (domain order is shown left with ↓). We blue and bold the best, and bold the second best.\n\nOrder\n\nMetric\n\nTA CoT\n\nAuCID\n\nOT SH\n\nA\n\nGSFD\n\nD M\nB\n\nTent\n\nT3A\n\nL2D\n\nN E\nPD\n\nR SN\n\nL PC\n\nM\n\nEFD\n\nQu ↓\nSk ↓\nCl ↓\nIn ↓\nPa ↓\nRe\n\nG D\nT\n\nA D\nT\n\nFA\n\nSk Cl In Pa Re Avg. Qu Sk Cl In Pa Re Avg. Qu Sk Cl In Pa Avg.\n\n9.7\n\n29.4 27.9 22.7 29.1 23.1 29.4 29.4 31.6 29.6 30.6 27.1 31.4 47.6 42.4 22.2 52.0 25.0 50.3 45.4 52.3 49.4 51.4 41.1 27.3 16.0 16.7 13.0 14.0 11.8 16.6 14.8 13.2 12.2 12.7 12.6 28.4 23.9 15.0 26.4 16.5 27.3 23.0 24.2 22.6 26.2 13.3 14.0 52.8 41.2 30.0 41.1 33.9 53.2 47.7 39.8 37.3 35.8 35.9 20.4 34.8 30.4 20.6 32.5 22.1 35.4 32.1 32.2 30.2 30.7 26.0 21.1 92.7 92.7 92.0 94.8 92.0 92.0 92.1 91.8 92.0 91.8 91.5 90.9 37.9 31.1 34.9 46.5 37.2 38.2 36.9 37.1 34.2 35.0 34.1 12.2 56.7 54.0 34.9 59.0 35.7 57.2 56.2 55.2 52.3 56.2 48.0 8.5 17.5 19.3 17.2 22.5 12.9 18.9 18.2 13.2 12.1 12.6 17.5 11.5 34.8 30.6 19.6 26.9 23.4 33.6 33.2 16.3 12.4 20.7 10.3 13.5 58.2 46.5 34.5 42.9 37.3 60.2 57.2 42.3 39.6 41.8 33.9 4.5 49.6 45.7 38.9 48.8 39.8 50.0 49.0 42.7 40.4 43.0 39.2 23.5 91.9 85.7 42.1 69.7 33.5 91.3 73.6 79.8 76.5 81.9 80.6 64.0 38.6 34.8 29.0 33.8 29.2 39.3 33.1 32.7 31.4 33.0 29.0 13.8 56.6 53.0 29.2 36.8 32.2 58.3 48.6 44.5 43.2 42.9 41.8 21.7 17.9 17.5 15.4 12.3 12.8 19.3 17.6 13.1 11.8 10.9 14.0 12.5 35.1 28.9 21.5 33.3 22.3 34.4 33.7 15.8 16.9 23.7 5.0 48.0 44.0 27.4 37.2 26.0 48.5 41.3 37.2 36.0 38.5 34.2 23.4\n\n5.6\n\nOurs\n\n33.4 54.6 16.2 41.0 62.2 41.5 92.0 49.8 64.8 19.0 49.1 66.5 56.8 86.1 51.3 63.8 19.6 47.8 53.7\n\n(a) RandMix (R.M.)\n\n(b) T2PL Figure 4: Ablation studies of RandMix, T2PL, and PCA on Digits. The average performance of all domains for three metrics (TDG, TDA, FA) is shown. ‘Ours’ here denotes the full framework of RaTP, while ‘Ours-’ represents to remove a corresponding module (a, b or c) from RaTP, ‘Oursw/’ means replacing the corresponding module with a new one.\n\n(c) PCA\n\nEffectiveness of T2PL. We replace T2PL in RaTP with other pseudo labeling approaches to investigate the gain from T2PL, including Softmax (Lee et al., 2013), SHOT (Liang et al., 2020), and BMD (Qu et al., 2022). Figure 4(b) shows that RaTP performs the best when it has T2PL.\n\nEffectiveness of PCA. We first remove LPCA from the optimization objective, leaving only CrossEntropy Loss and Logits Distillation. We then try to replace PCA with regular forwarding prototype learning (PL), supervised contrastive learning (SCL), and PCL (Yao et al., 2022). Figure 4(c) shows that the performance of RaTP degrades significantly without LPCA, and when we use other methods instead, the performance is still worse. This validates the effectiveness of PCA.\n\n4 CONCLUSION\n\nThis paper focuses on improving the model performance before and during the training of continually arriving domains, in what we call the Unfamiliar Period. To solve this practical and important problem, we propose a novel framework RaTP that includes a training-free data augmentation module RandMix, a pseudo labeling mechanism T2PL, and a prototype contrastive alignment training algorithm PCA. Extensive experiments demonstrate that RaTP can significantly improve the model performance in the Unfamiliar Period, while also achieving good performance in target domain adaptation and forgetting alleviation, in comparison with state-of-the-art methods from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG, and Unified DA&DG.\n\n9\n\nTDGTDAFA102030405060708090100Performance (%)OursOurs-SHOT w/R.M.SHOTPCL w/R.M.PCLGSFDA w/R.M.GSFDATDGTDAFA30405060708090Performance (%)OursOurs- w/SHOTOurs- w/BMDOurs- w/SoftmaxTDGTDAFA30405060708090Performance (%)OursOurs-Ours- w/PLOurs- w/SCLOurs- w/PCLPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nWe gratefully acknowledge the support by Sony AI, National Science Foundation awards #2016240, #1834701, #1724341, #2038853, Office of Naval Research grant N00014-19-1-2496, and research awards from Meta and Google.\n\nETHICS STATEMENT\n\nIn this paper, our studies are not related to human subjects, practices to data set releases, discrimination/bias/fairness concerns, and also do not have legal compliance or research integrity issues. Our work is proposed to address continual domain shifts when applying deep learning models in real-world applications. In this case, if the trained models are used responsibly for good purposes, we believe that our proposed methods will not cause ethical issues or pose negative societal impacts.\n\nREPRODUCIBILITY STATEMENT\n\nThe source code is provided at https://github.com/SonyAI/RaTP. All datasets we use are public. In addition, we also provide detailed experiment parameters and random seeds in the Appendix.\n\nREFERENCES\n\nGuangji Bai, Ling Chen, and Liang Zhao. Temporal domain generalization with drift-aware dynamic\n\nneural network. arXiv preprint arXiv:2205.10664, 2022.\n\nSlawomir Bak, Peter Carr, and Jean-Francois Lalonde. Domain adaptation through synthesis for unsupervised person re-identification. In Proceedings of the European conference on computer vision (ECCV), pp. 189–205, 2018.\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.\n\nDian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022.\n\nJiahua Dong, Yang Cong, Gan Sun, Zhen Fang, and Zhengming Ding. Where and how to transfer: knowledge aggregation-induced transferability perception for unsupervised domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nJiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi Zhu. Federated class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10164–10173, 2022a.\n\nXin Dong, Junfeng Guo, Ang Li, Wei-Te Ting, Cong Liu, and HT Kung. Neural mean discrepancy for efficient out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19217–19227, 2022b.\n\nAbhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14340–14349, 2021.\n\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In\n\nInternational conference on machine learning, pp. 1180–1189. PMLR, 2015.\n\nMuhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE transactions on pattern analysis and machine intelligence, 39(7):1414–1430, 2016.\n\nJunfeng Guo, Ang Li, and Cong Liu. Aeva: Black-box backdoor detection using adversarial extreme\n\nvalue analysis. In International Conference on Learning Representations, 2021.\n\nJunyuan Hong, Zhuangdi Zhu, Shuyang Yu, Zhangyang Wang, Hiroko H Dodge, and Jiayu Zhou. Federated adversarial debiasing for fair and transferable representations. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 617–627, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJunyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh.\n\nYusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021.\n\nXin Jin, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Style normalization and restitution for domain\n\ngeneralization and adaptation. IEEE Transactions on Multimedia, 2021.\n\nGuoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4893–4902, 2019.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\n\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013.\n\nLei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang Xia. Progressive domain expansion network for single domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 224– 233, 2021.\n\nJian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020.\n\nJian Liang, Dapeng Hu, Yunbo Wang, Ran He, and Jiashi Feng. Source data-absent unsupervised domain adaptation through hypothesis transfer and labeling transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nHong Liu, Mingsheng Long, Jianmin Wang, and Yu Wang. Learning to adapt to evolving domains.\n\nAdvances in Neural Information Processing Systems, 33:22338–22348, 2020.\n\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.\n\nAdvances in neural information processing systems, 30, 2017.\n\nSaeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In Proceedings of the IEEE international conference on computer vision, pp. 5715–5725, 2017.\n\nAnshul Nasery, Soumyadeep Thakur, Vihari Piratla, Abir De, and Sunita Sarawagi. Training for the future: A simple gradient interpolation loss to generalize along time. Advances in Neural Information Processing Systems, 34:19198–19209, 2021.\n\nShuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. arXiv preprint arXiv:2204.02610, 2022.\n\nYingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, and Tao Mei. Transferrable prototypical networks for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2239–2247, 2019.\n\nTheodoros Panagiotakopoulos, Pier Luigi Dovesi, Linus H ̈arenstam-Nielsen, and Matteo Poggi. Online domain adaptation for semantic segmentation in ever-changing conditions. arXiv preprint arXiv:2207.10667, 2022.\n\nSanqing Qu, Guang Chen, Jing Zhang, Zhijun Li, Wei He, and Dacheng Tao. Bmd: A general class-balanced multicentric dynamic prototype strategy for source-free domain adaptation. arXiv preprint arXiv:2204.02811, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\nicarl: In Proceedings of the IEEE conference on\n\nIncremental classifier and representation learning. Computer Vision and Pattern Recognition, pp. 2001–2010, 2017.\n\nMohammad Rostami. Lifelong domain adaptation via consolidated internal distribution. Advances\n\nin Neural Information Processing Systems, 34:11172–11183, 2021.\n\nKuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8050–8058, 2019.\n\nDilbag Singh, Vijay Kumar, Manjit Kaur, et al. Classification of covid-19 patients from chest ct images using multi-objective differential evolution–based convolutional neural networks. European Journal of Clinical Microbiology & Infectious Diseases, 39(7):1379–1389, 2020.\n\nNima Tajbakhsh, Jae Y Shin, Suryakanth R Gurudu, R Todd Hurst, Christopher B Kendall, Michael B Gotway, and Jianming Liang. Convolutional neural networks for medical image analIEEE transactions on medical imaging, 35(5):1299–1312, ysis: Full training or fine tuning? 2016.\n\nShixiang Tang, Peng Su, Dapeng Chen, and Wanli Ouyang. Gradient regularized contrastive learning for continual domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 2665–2673, 2021.\n\nKorawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. Advances in Neural Information Processing Systems, 34:17194–17208, 2021.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2020.\n\nJindong Wang and Wang Lu. Deepdg: Deep domain generalization toolkit. https://github.\n\ncom/jindongwang/transferlearning/tree/master/code/DeepDG.\n\nLixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10165–10173, 2021a.\n\nLixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi Zhu. Non-transferable learning: A new approach for model ownership verification and applicability authorization. In International Conference on Learning Representations, 2021b.\n\nQin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022.\n\nZijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and Mahsa Baktashmotlagh. Learning to diversify for single domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 834–843, 2021c.\n\nJiangwei Xie, Shipeng Yan, and Xuming He. General incremental learning with domain-aware In Proceedings of the IEEE/CVF Conference on Computer Vision\n\ncategorical representations. and Pattern Recognition, pp. 14351–14360, 2022.\n\nShichao Xu, Lixu Wang, Yixuan Wang, and Qi Zhu. Weak adaptation learning: Addressing crossdomain data insufficiency with weak annotator. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8917–8926, 2021.\n\nShiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized In Proceedings of the IEEE/CVF International Conference on\n\nsource-free domain adaptation. Computer Vision, pp. 8978–8987, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nXufeng Yao, Yang Bai, Xinyun Zhang, Yuechen Zhang, Qi Sun, Ran Chen, Ruiyu Li, and Bei Yu. Pcl: Proxy-based contrastive learning for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7097–7107, 2022.\n\nYabin Zhang, Minghan Li, Ruihuang Li, Kui Jia, and Lei Zhang. Exact feature distribution matching for arbitrary style transfer and domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8035–8045, 2022.\n\nRonghang Zhu and Sheng Li. Crossmatch: Cross-classifier consistency regularization for open-set single domain generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=48RBsJwGkJf.\n\nRonghang Zhu, Xiaodong Jiang, Jiasen Lu, and Sheng Li. Cross-domain graph convolutions for adversarial unsupervised domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nSUMMARY OF THE APPENDIX\n\nThis Appendix includes additional details for the ICLR 2023 paper“Deja Vu: Continual Model Generalization for Unseen Domains”, including related work comparison, more implementation details of main experiments, additional experiment results, and sensitivity analysis on parameters used in the framework. The Appendix is organized as follows:\n\n• Section A discusses the difference between RaTP and related works.\n\n• Section B provides more implementation details, including detailed experiment settings, data split-\n\nting strategy, implementation details of the baseline methods, and network structures.\n\n• Section C presents experiment results of additional domain orders on three datasets.\n\n• Section D provides detailed sensitivity analysis of three parameters used in RaTP.\n\n• Section E shows additional evaluations of RaTP in stationary domain adaptation.\n\nA RELATED WORK COMPARISON\n\nOur work is relevant to a number of different topics as we focus on addressing important and practical problems. We show a detailed comparison among these relevant topics in Table 4. Specifically, Fine-Tuning (Tajbakhsh et al., 2016) is usually conducted on a trained model with labeled target domain data. It can achieve certain domain adaptation capabilities after sufficient training, but the forgetting of previously learned knowledge is often severe. Continual Learning (Lopez-Paz & Ranzato, 2017; Rebuffi et al., 2017), which shares the same settings as Fine-Tuning, can effectively achieve alleviation of catastrophic forgetting. To relax the demand for target labels, a series of domain adaptation (DA) methods are proposed. Unsupervised DA (Ganin & Lempitsky, 2015; Dong et al., 2021; Zhu et al., 2021; Xu et al., 2021) can work effectively when the target data is unlabeled, but suffers from poor forgetting alleviation capability in continually changing cases. Continual DA (Wang et al., 2022; Rostami, 2021; Tang et al., 2021) is able to compensate for the forgetting of learned knowledge, while achieving good domain adaptation, but it requires access to source data. Source-Free DA (Yang et al., 2021; Liang et al., 2020; 2021; Qu et al., 2022) is proposed to adapt the model to unlabeled target domains without the need for source data, but the model forgets a lot of learned knowledge during adaptation. Test-Time DA (Wang et al., 2020; Iwasawa & Matsuo, 2021; Chen et al., 2022; Niu et al., 2022; Hong et al., 2023) does not need source data and target labels, and can achieve effective domain adaptation without heavy catastrophic forgetting on old knowledge. However, all these DA methods cannot achieve effective target domain generalization on new domains before and during the training on them, which is the main focus of our work here.\n\nDifferent from DA, Domain Generalization (DG), as another cross-domain learning paradigm, can improve model out-of-distribution generalization (Dong et al., 2022b; Hong et al., 2021) performance even without seeing target domains. According to the composition of the given source domain, DG can be divided into two types, Single Source (Wang et al., 2021c; Li et al., 2021; Zhu & Li, 2022) and Multi-Source (Yao et al., 2022; Zhang et al., 2022). Both these two DG-s require labeled source domain data, and can address target domain generalization before any adaptation on target domains. However, they cannot achieve effective target domain adaptation and forgetting alleviation, which are important in continually changing scenarios. Recently, there are some works (Jin et al., 2021) that try to unify DA and DG, but they require significant resources such as sourcelabeled data and target-labeled data. Moreover, such Unified DA & DG also suffers from forgetting when being applied in continual learning settings. Compared with these studies, our work focuses on improving model performance on new target domains before and during the training on them, in what we call the Unfamiliar Period in the main text, while also achieving effective target domain adaptation and forgetting alleviation. To ensure the application practicability, we assume that there is only unlabeled data from target domains. As shown in the main text, the proposed RaTP framework can effectively solve the problem we target under such assumptions.\n\nB IMPLEMENTATION DETAILS\n\nExperiment Settings. To classify the datasets, we apply ResNet-50 as the feature extractor for both PACS and DomainNet. For Digits, we follow Liang et al. (2020) to use DTN as the feature extractor. In source domain, we follow Wang & Lu to split 80% data as the training set and the rest 20% as the testing set. In all target domains, all data are used for training and testing. For each dataset,\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Characteristics of various problem settings that adapt a model to potentially shifted target domains, especially assuming the domain shift is continually changing.\n\nTopics\n\nSource Data Target Data Target Label\n\nFine-Tuning Continual Learning Unsupervised DA Continual DA Source-Free DA Test-Time DA Single Source DG Multi-Source DG Unified DA & DG RaTP\n\n(cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:37)\n\n(cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33)\n\n(cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37)\n\nTarget Domain Generalization (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33)\n\nTarget Domain Adaptation (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33)\n\nForgetting Alleviation (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33)\n\nwe keep the training steps per epoch the same for all domains in all different domain orders. We use 800 steps for Digits, 50 steps for PACS and 70 steps for DomainNet. Training epoch is 30 for all datasets and domains. The SGD optimizer with initial learning rate 0.01 is used for Digits and 0.005 for PACS and DomainNet. Moreover, we use momentum of 0.9 and weight decay of 0.0005 to schedule the SGD. The exemplar memory is set as 200 for all datasets, and we set the batch size of mini-batch training as 64. Only ResNet-50 is initialized as the pre-trained version of ImageNet.\n\nSubset of DomainNet. The original DomainNet is class imbalanced, with certain classes in some domains containing very few images (∼10). This makes it hard to assign pseudo labels for these classes. Thus, we refer to Xie et al. (2022) to select the top 10 classes with most images in all domains, which contain 26,013 samples in total. This dataset is still class imbalanced, with the smallest sample number for a class is 32, while the largest is 901. Therefore, this subset is still quite challenging.\n\nBaseline Implementations. For fair comparison, we try our best to extend all baseline methods in our problem settings. AuCID (Rostami, 2021) shares the most similar setting with us. But their exemplar memory size is not limited and can be enlarged as more domains appear. And they transform Digits to gray scale images while we treat them as RGB images. For Test-Time/Online DA [CoTTA (Wang et al., 2022), Tent (Wang et al., 2020), T3A Iwasawa & Matsuo (2021), AdaCon (Chen et al., 2022), EATA (Niu et al., 2022)], we keep the setting that adapts the model on each target domain only once. For Single DG [L2D (Wang et al., 2021c), PDEN (Li et al., 2021)] and Multiple DG [PCL (Yao et al., 2022), EFDM (Zhang et al., 2022)], we use SHOT (Liang et al., 2020) to assign pseudo labels for the optimization on target domains. In EFDM, we use samples from current domain as the content images and randomly select images in the replay memory as the style images. SNR (Jin et al., 2021) directly modifies the model structures to match statistics of different domains, thus it can be viewed as a unified approach for both DA and DG. In our implementation, we use SNR to modify the convolution filters of the feature extractor and optimize the model with SHOT. We also consider a more advanced version of SHOT for further comparison, i.e., SHOT++ (Liang et al., 2021), which equips an additional semi-supervised training step using MixMatch (Berthelot et al., 2019). This may not be fair for other methods because all methods can in principle equip with this additional training step. Thus, we also run SHOT++ without the MixMatch and denote it as SHOT+. All baseline methods are equipped with the same exemplar memory and the same feature extractor.\n\nNetwork Structures. Our RandMix augmentation network includes four autoencoders that consist of a convolutional layer and a transposed convolutional layer. All these layers have 3 channels and kernel sizes of 5, 9, 13, 17, respectively. The classification model contains a feature extractor, a bottleneck module and a classifier. Specifically, pre-trained ResNet-50 is used as the feature extractor for both PACS and DomainNet. Following Liang et al. (2020), DTN, a variant of LeNet is used as the feature extractor for Digits. We also introduce a bottleneck module between the feature extractor and the classifier. The bottleneck consists of a fully-connected layer (256 units for Digits and 512 units for both PACS and DomainNet), a Batch Normalization layer, a ReLU layer and another fully-connected layer (128 units for Digits, 256 units for PACS, and 512 units\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nfor DomainNet). The classifier is a fully-connected layer without the parameters of bias. Both contrastive loss and distillation loss are applied to the representations after the bottleneck.\n\nC ADDITIONAL EXPERIMENT RESULTS\n\nFollowing existing studies of continual domain shifts (Rostami, 2021; Wang et al., 2022; Tang et al., 2021), we have presented experiment results under the most popular domain orders in the main text. To further explore the impact of domain order, which is rarely studied by existing works, we conduct more experiments with additional domain orders for the three datasets. Specifically, for each dataset, we carry out experiments under 10 different domain orders that are randomly shuffled, shown in Table 5. We present the average performance for all orders in Table 6 (Digits), Table 7 (PACS), Table 8 (DomainNet). Note that for the average performance in Tables 6, 7 and 8, we select a subset of the baseline methods that perform relatively better than the rest, including a few additional baselines (i.e., SHOT+ (Liang et al., 2021), SHOT++ (Liang et al., 2021), AdaCon (Chen et al., 2022), EATA (Niu et al., 2022), L2D (Wang et al., 2021c), and PDEN (Li et al., 2021)), to carry out experiments for saving time and space. Under these additional domain orders, RaTP still achieves the best average TDG on all three datasets, and comparable (and in many cases better) performance in TDA and FA. Combining these results with the ones shown in the main text, we can clearly see that RaTP provides significant improvement on TDG over previous state-of-the-art methods, while achieving comparable performance in TDA and FA.\n\nTable 5: Different domain orders used to assess their impact on various approaches’ performance. The results are reported in Tables 6, 7, and 8 for the average performance of all orders.\n\nOrder Datasets Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10\n\nPACS\n\nDigits\n\nDomainNet SN→MT→MM→SD→US A→C→P→S Re→Pa→In→Cl→Sk→Qu SN→SD→MT→US→MM A→C→S→P Cl→In→Pa→Qu→Re→Sk MM→US→MT→SD→SN A→P→C→S Cl→Re→In→Qu→Sk→Pa In→Qu→Cl→Pa→Re→Sk C→A→S→P MT→MM→SN→SD→US MT→MM→US→SN→SD C→S→P→A Pa→Sk→Qu→In→Re→Cl P→A→C→S Qu→Re→Cl→Pa→In→Sk SD→MM→SN→MT→US P→S→A→C Qu→Sk→Cl→In→Pa→Re SD→SN→US→MM→MT P→S→C→A Sk→In→Pa→Cl→Re→Qu SD→US→MM→SN→MT Sk→Re→Pa→Cl→Qu→In US→MT→SN→MM→SD S→C→A→P S→P→C→A Sk→Re→Qu→Pa→In→Cl US→SD→SN→MM→MT\n\nD SENSITIVITY ANALYSIS\n\nWe conduct sensitive analysis of the confidence threshold rcon in splitting RandMix, and rtop and r′ top in T2PL. All sensitive analysis experiments are conducted on Digits with the domain order of US→SD→SN→MM→MT. For splitting RandMix, rcon controls the proportion of unlabeled data that needs to be augmented. As shown in Figure 5(a), different rcon-s correspond to different model performance, and we can obtain the best performance when rcon = 0.8, which is adopted for Digits and DomainNet. And rcon is set as 0.5 for PACS. As for the sensitivity analysis of the other two parameters, according to the results shown in Figures 5(b) and 5(c), we can observe that a larger rtop that corresponds to a smaller amount of fitting samples is detrimental to the model performance, while a larger r′ top that corresponds to fewer nearest samples for the kNN classification leads to slightly better performance. We choose rtop = 2 and r′ top = 20 for Digits and PACS, rtop = 1 and r′ top = 10 for DomainNet.\n\nE EVALUATIONS ON STATIONARY DOMAIN ADAPTATION\n\nWhile our work primarily focuses on continual domain shift, here we provide results of stationary domain adaptation for further comparison. In this setting, we consider the combination of one single source domain and one single target domain, and consider the generalization performance on the target domain accuracy after adaptation. In such situation, we remove the replay memory, the distillation loss term (Eq. 10) and the second term in the nominator of LPCA (Eq. 9), as they force the model to achieve forgetting alleviation for previous domains and may impede the adaptation to the target domain. We conduct our stationary domain adaptation in PACS, and the results after adaptation are shown in Tables 9 Compared with other baselines, RaTP performs the best in most cases and achieves 4.4% higher average accuracy than the second best.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Performance comparisons between ours and other baseline methods on Digits in average TDG, TDA, and FA under 10 different domain orders. We blue and bold the best performance, and bold the second best.\n\nTDG\n\nMetric & Orders Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg. Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg. Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg.\n\nFA\n\nTDA\n\nSHOT+ 66.2 78.0 68.3 49.1 54.0 72.3 74.8 73.9 35.1 38.6 61.0 84.0 91.6 81.2 73.8 79.7 87.0 89.9 89.0 48.4 61.2 78.6 60.0 73.9 70.7 56.5 77.0 59.3 62.2 57.2 25.1 39.7 58.2\n\nSHOT++ Tent AdaCon EATA L2D PDEN\n\n68.3 78.2 65.8 52.0 54.1 75.2 76.0 72.6 39.0 41.7 62.3 87.5 94.8 79.9 79.6 84.9 92.1 91.2 91.5 48.8 62.9 81.3 67.1 75.5 71.2 65.3 79.1 67.4 71.0 66.0 30.0 52.5 64.5\n\n71.1 72.9 70.7 52.2 53.1 76.9 76.9 79.3 41.3 45.9 64.0 71.5 77.5 70.7 59.9 59.5 80.2 80.9 80.5 48.7 57.3 68.7 67.8 82.2 72.9 50.8 61.4 81.2 79.8 80.0 33.1 51.5 66.1\n\n72.6 75.8 67.0 53.2 51.1 75.8 73.0 76.9 41.3 46.3 63.3 77.4 76.0 75.8 64.9 65.3 80.5 82.1 80.2 55.7 58.3 71.6 75.2 82.7 80.4 59.0 71.7 80.4 82.1 81.9 56.8 52.0 72.2\n\n71.3 71.5 69.6 53.7 53.6 77.8 76.1 77.9 44.1 44.2 64.0 76.8 76.9 76.4 65.0 65.8 81.1 83.2 82.2 55.4 57.1 72.0 76.2 83.6 85.5 58.8 71.2 79.7 80.9 79.4 61.8 52.4 73.0\n\n72.3 78.1 71.7 62.3 62.7 78.2 78.1 78.0 61.7 65.5 70.9 85.9 91.3 85.9 77.6 79.3 89.7 87.6 88.6 74.2 82.9 84.3 75.2 81.1 85.1 72.3 74.9 76.8 77.6 75.0 72.5 74.1 76.5\n\n69.4 78.4 70.5 60.4 61.4 76.8 76.8 77.3 61.7 63.8 69.7 81.9 89.5 86.2 75.3 78.3 89.0 85.2 85.9 70.9 80.3 82.3 71.4 79.6 81.9 70.0 73.9 74.1 76.1 72.6 68.5 72.0 74.0\n\nOurs 77.0 79.5 77.0 72.0 72.9 81.0 81.9 81.3 73.2 71.7 76.8 89.7 90.7 87.8 86.8 87.5 90.0 91.6 89.7 85.9 87.1 88.7 83.8 87.4 90.1 82.3 85.2 84.9 84.7 83.3 85.1 82.8 85.0\n\n(a) rcon\n\n(b) rtop\n\n(c) r′\n\ntop\n\nFigure 5: Sensitivity analysis of confidence threshold rcon in RandMix; rtop and r′\n\ntop in T2PL.\n\n17\n\n0.00.30.50.81.065707580859095Performance (%)TDGTDAFA235102050556065707580859095Performance (%)TDGTDAFA235102065707580859095Performance (%)TDGTDAFAPublished as a conference paper at ICLR 2023\n\nTable 7: Performance comparisons between ours and other baseline methods on PACS in average TDG, TDA, and FA under 10 different domain orders. We blue and bold the best performance, and bold the second best.\n\nTDG\n\nMetric & Orders Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg. Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg. Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg.\n\nFA\n\nTDA\n\nSHOT+ 69.4 67.0 67.8 69.5 61.1 48.5 36.6 37.2 53.1 39.1 54.9 86.7 87.8 88.7 89.2 85.2 83.1 66.9 64.0 91.5 75.9 81.9 73.0 72.4 81.8 76.9 82.9 79.6 65.3 58.3 86.5 72.0 74.9\n\nOurs 76.6 75.8 76.6 79.8 78.1 62.6 56.8 54.8 73.8 70.7 70.5 83.5 84.5 84.2 86.8 85.5 82.3 76.6 74.9 89.7 90.1 83.8 87.8 77.9 87.5 82.5 84.2 86.2 79.7 73.4 87.5 89.6 83.6\n\nSHOT++ Tent AdaCon EATA L2D PDEN\n\n75.2 74.6 75.9 77.1 74.6 57.2 55.4 52.1 57.3 52.3 65.2 85.8 81.6 82.8 88.7 86.4 78.6 74.0 73.9 77.8 69.7 79.9 90.7 77.7 89.5 84.5 88.0 80.7 78.0 74.0 79.0 73.7 81.6\n\n75.1 72.5 76.1 77.4 78.3 57.4 54.3 51.8 48.2 50.0 64.1 86.7 85.3 85.6 89.2 88.2 79.2 73.0 72.3 73.0 70.4 80.3 91.4 83.7 90.5 87.7 90.8 83.5 74.4 73.3 76.6 74.3 82.6\n\n74.0 76.0 72.8 78.1 74.6 56.5 54.9 52.8 62.0 56.7 65.8 84.5 83.6 82.9 84.6 80.1 75.5 71.3 68.5 83.0 74.4 78.8 85.6 80.6 84.8 77.5 76.7 71.0 75.7 72.6 78.0 73.4 77.6\n\n73.7 71.6 73.5 77.2 73.5 55.8 52.0 51.5 60.9 54.6 64.4 83.7 83.3 79.9 83.0 78.2 74.0 71.6 69.8 82.5 72.1 77.8 85.2 77.4 78.7 77.1 76.5 70.9 75.4 72.1 78.3 71.4 76.3\n\n70.4 68.7 63.3 66.1 62.2 50.0 43.2 39.0 52.7 44.6 56.0 89.4 89.4 88.8 91.2 85.4 85.3 69.9 68.8 92.2 83.2 84.4 78.6 82.3 78.9 77.6 86.1 84.3 80.5 83.5 88.8 89.5 83.0\n\n75.5 73.1 75.6 78.5 81.6 56.2 52.5 50.6 54.3 60.2 65.8 84.0 82.0 82.5 88.2 88.6 75.7 74.4 72.5 69.6 69.8 78.7 89.5 79.5 88.5 83.3 89.0 81.4 78.0 73.3 74.1 73.1 81.0\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Performance comparisons between ours and other baseline methods on DomainNet in average TDG, TDA, and FA under 10 different domain orders. We blue and bold the best performance, and bold the second best.\n\nTDG\n\nMetric & Orders Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg. Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg. Order 1 Order 2 Order 3 Order 4 Order 5 Order 6 Order 7 Order 8 Order 9 Order 10 Avg.\n\nFA\n\nTDA\n\nSHOT+ 46.9 52.2 53.6 40.8 48.4 34.0 23.2 59.2 58.7 56.2 47.3 68.4 69.7 72.6 51.3 68.5 63.1 47.7 72.8 74.2 71.9 66.0 61.4 64.5 62.9 42.1 60.9 61.1 42.8 61.6 67.4 60.4 58.5\n\nSHOT++ Tent AdaCon EATA L2D PDEN\n\n45.5 50.0 53.3 41.9 49.6 35.3 25.7 59.9 59.3 60.1 48.1 70.5 66.2 73.4 53.5 70.9 65.3 48.1 73.2 75.9 72.1 66.9 66.5 68.9 67.7 65.4 68.5 66.3 51.7 67.5 77.3 69.6 66.9\n\n52.1 31.0 58.4 50.6 52.8 33.1 35.4 61.0 61.3 41.2 47.7 59.0 28.9 65.6 54.6 60.3 51.8 50.0 67.6 67.6 31.0 53.6 67.4 34.1 65.6 56.4 58.0 52.4 48.5 71.6 76.8 30.4 56.1\n\n51.3 52.7 53.7 51.3 53.0 32.9 32.9 62.0 61.6 61.3 51.3 60.4 66.2 68.6 52.2 60.3 52.4 50.8 71.6 71.3 67.9 62.2 67.0 62.6 66.3 53.3 56.6 49.4 48.5 72.8 76.1 65.5 61.8\n\n51.2 55.2 58.7 51.1 52.8 33.6 34.4 62.0 63.3 59.0 52.1 60.0 65.8 69.4 52.9 58.7 55.0 51.7 70.7 69.3 71.2 62.5 64.3 65.8 69.2 52.7 57.4 54.8 47.7 73.5 76.0 66.3 62.8\n\n49.6 55.6 52.8 48.2 53.1 36.5 32.2 62.1 59.4 57.4 50.7 59.9 56.5 54.8 57.3 56.9 49.3 41.7 64.0 61.9 59.9 56.2 63.7 48.9 45.2 41.5 51.2 48.0 37.2 58.8 66.5 61.4 52.2\n\n48.0 51.2 51.1 47.0 51.0 37.2 30.2 61.4 59.9 56.4 49.3 60.8 54.2 52.7 55.2 55.7 50.2 40.4 63.4 62.2 61.0 55.6 61.1 46.3 43.1 39.5 48.6 46.0 36.0 55.1 65.6 60.9 50.2\n\nOurs 51.9 57.4 59.4 54.7 56.0 46.7 41.5 61.8 63.7 62.3 55.5 68.1 69.6 70.3 60.3 68.2 65.1 56.8 75.5 74.1 67.8 67.6 67.6 67.7 68.1 56.5 65.4 62.8 53.7 72.0 79.6 62.9 65.6\n\nTable 9: Comparison of domain adaptation for stationary source-target pairs on PACS. We blue and bold the best average performance, and bold the second best.\n\nMethod P→A P→C P→S A→P A→C A→S C→P C→A C→S S→P S→A S→C Avg. 87.2 76.8 SHOT 77.7 SHOT+ 90.4 SHOT++ 93.1 78.4 72.4 77.0 80.6 88.2 75.4 79.2 85.0 92.5\n\nTent AdaCon EATA Ours\n\n99.0 98.9 99.1 95.4 97.7 95.7 98.9\n\n44.4 41.9 41.3 63.7 67.7 72.7 59.0\n\n91.9 92.6 93.6 82.8 91.5 84.8 92.6\n\n53.5 52.2 53.1 55.3 61.3 55.3 99.2\n\n84.9 84.3 87.2 59.1 82.7 60.0 92.8\n\n89.8 89.5 91.7 68.0 75.3 68.7 86.7\n\n85.4 87.9 89.1 64.9 85.3 72.7 83.2\n\n99.3 99.5 99.6 97.4 99.5 97.2 99.5\n\n33.8 38.8 38.8 55.5 56.6 59.0 62.4\n\n87.8 88.8 90.1 76.7 86.8 80.0 88.0\n\n64.9 67.2 68.2 73.5 75.0 79.7 64.8\n\n19",
    "reference": "# Summary Of The Paper\n\nThis work focuses on a practical challenge that deep learning models cannot remain stable performance when being deployed in real-world environments. For this problem, the author(s) defines three objectives, i.e., target domain generalization, target domain adaptation and forgetting alleviation. To achieve these objectives, a framework called Twofer is proposed to better align domains over time. According to extensive evaluation experiments and comprehensive baseline comparison, the effectiveness of Twofer can be demonstrated.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. The objective of target domain generalization is appealing and significant, and I particularly like the definition of ‘unfamiliar period’ in the paper. Although a number of domain adaptation studies have been proposed, including continual and source-free, most of them neglect the generalization before adaptation. This paper bridges this gap by allowing the deployed models to evolve themselves and remain stable performance all the time. \n\n2. The proposed Twofer is composed of three major modules, which work together for achieving better domain alignment. These three modules are well-motivated.\n\n3. The author(s) compare twofer with a number of SOTA baselines. The experiments are extensive, and the results can demonstrate the effectiveness of proposed methods.\n\nWeakness:\n\n1. If I understand correctly, twofer needs to store a small number of samples at each stage for later usage, is it possible to relax this requirement? More should be discussed. \n\n2. I notice that there is no section of related work, which is important to this work. Therefore, I believe the author(s) needs a section or a table to present the difference among different DA and DG topics.\n\nQuestions:\n\n1.\tThe author(s) mentions that blindly pushing augmentation data away from the original possibly hurts the generalization performance. Could the author(s) give more explanation here? \n\n2.\tBesides, is it possible to extract useful information from seen domains for guiding the data augmentation in the future? Because the used augmentation module is simple in terms of network structure, the training cost is acceptable if learnable augmentation can bring noticeable performance gain.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-organized and clearly written, and it focuses on practical issues in deep learning model deployment. This work should be reproducible. The implementation details and code are provided.\n\n# Summary Of The Review\n\nThis paper is clear and technically solid. The experiments are thorough with impressive results, and the analysis is extensive.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDICHOTOMY OF CONTROL: SEPARATING WHAT YOU CAN CONTROL FROM WHAT YOU CANNOT\n\nMengjiao Yang University of California, Berkeley Google Research, Brain Team sherryy@berkeley.edu\n\nDale Schuurmans University of Alberta Google Research, Brain Team\n\nPieter Abbeel University of California, Berkeley\n\nOfir Nachum Google Research, Brain Team\n\nABSTRACT\n\nFuture- or return-conditioned supervised learning is an emerging paradigm for offline reinforcement learning (RL), where the future outcome (i.e., return) associated with an observed action sequence is used as input to a policy trained to imitate those same actions. While return-conditioning is at the heart of popular algorithms such as decision transformer (DT), these methods tend to perform poorly in highly stochastic environments, where an occasional high return can arise from randomness in the environment rather than the actions themselves. Such situations can lead to a learned policy that is inconsistent with its conditioning inputs; i.e., using the policy to act in the environment, when conditioning on a specific desired return, leads to a distribution of real returns that is wildly different than desired. In this work, we propose the dichotomy of control (DoC), a future-conditioned supervised learning framework that separates mechanisms within a policy’s control (actions) from those beyond a policy’s control (environment stochasticity). We achieve this separation by conditioning the policy on a latent variable representation of the future, and designing a mutual information constraint that removes any information from the latent variable associated with randomness in the environment. Theoretically, we show that DoC yields policies that are consistent with their conditioning inputs, ensuring that conditioning a learned policy on a desired high-return future outcome will correctly induce high-return behavior. Empirically, we show that DoC is able to achieve significantly better performance than DT on environments that have highly stochastic rewards and transitions.1\n\n1\n\nINTRODUCTION\n\nOffline reinforcement learning (RL) aims to extract an optimal policy solely from an existing dataset of previous interactions (Fujimoto et al., 2019; Wu et al., 2019; Kumar et al., 2020). As researchers begin to scale offline RL to large image, text, and video datasets (Agarwal et al., 2020; Fan et al., 2022; Baker et al., 2022; Reed et al., 2022; Reid et al., 2022), a family of methods known as returnconditioned supervised learning (RCSL), including Decision Transformer (DT) (Chen et al., 2021; Lee et al., 2022) and RL via Supervised Learning (RvS) (Emmons et al., 2021), have gained popularity due to their algorithmic simplicity and ease of scaling. At the heart of RCSL is the idea of conditioning a policy on a specific future outcome, often a return (Srivastava et al., 2019; Kumar et al., 2019; Chen et al., 2021) but also sometimes a goal state or generic future event (Codevilla et al., 2018; Ghosh et al., 2019; Lynch et al., 2020). RCSL trains a policy to imitate actions associated with a conditioning input via supervised learning. During inference (i.e., at evaluation), the policy is conditioned on a desirable high-return or future outcome, with the hope of inducing behavior that can achieve this desirable outcome.\n\n1Code available at https://github.com/google-research/google-research/tree/\n\nmaster/dichotomy_of_control.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustration of DT (RCSL) and DoC. Circles and squares denote states and actions. Solid arrows denote policy decisions. Dotted arrows denote (stochastic) environment transitions. All arrows and nodes are present in the dataset, i.e., there are 4 trajectories, 2 of which achieve 0 reward. DT maximizes returns across an entire trajectory, leading to suboptimal policies when a large return (r = 100) is achieved only due to very low-probability environment transitions (T = 0.01). DoC separates policy stochasticity from that of the environment and only tries to control action decisions (solid arrows), achieving optimal control through maximizing expected returns at each timestep.\n\nDespite the empirical advantages that come with supervised training (Emmons et al., 2021; Kumar et al., 2021), RCSL can be highly suboptimal in stochastic environments (Paster et al., 2022; Brandfonbrener et al., 2022), where the future an RCSL policy conditions on (e.g., return) can be primarily determined by randomness in the environment rather than the data collecting policy itself. Figure 1 (left) illustrates an example, where conditioning an RCSL policy on the highest return observed in the dataset (r = 100) leads to a policy (a1) that relies on a stochastic transition of very low probability (T = 0.01) to achieve the desired return of r = 100; by comparison the choice of a2 is much better in terms of average return, as it surely achieves r = 10. The crux of the issue is that the RCSL policy is inconsistent with its conditioning input. Conditioning the policy on a desired return (i.e., 100) to act in the environment leads to a distribution of real returns (i.e., 0.01 ∗ 100) that is wildly different from the return value being conditioned on. This issue would not have occurred if the policy could also maximize the transition probability that led to the high-return state, but this is not possible as transition probabilities are a part of the environment and not subject to the policy’s control.\n\nA number of works propose a generalization of RCSL, known as future-conditioned supervised learning methods. These techniques have been shown to be effective in imitation learning (Singh et al., 2020; Pertsch et al., 2020), offline Q-learning (Ajay et al., 2020), and online policy gradient (Venuto et al., 2021). It is common in future-conditioned supervised learning to apply a KL divergence regularizer on the latent variable – inspired by variational auto-encoders (VAE) (Kingma & Welling, 2013) and measured with respect to a learned prior conditioned only on past information – to limit the amount of future information captured in the latent variable. It is natural to ask whether this regularizer could remedy the insconsistency of RCSL. Unfortunately, as the KL regularizer makes no distinction between future information that is controllable versus that which is not, such an approach will still exhibit inconsistency, in the sense that the latent variable representation may contain information about the future that is due only to environment stochasticity.\n\nIt is clear that the major issue with both RCSL and na ̈ıve variational methods is that they make no distinction between stochasticity of the policy (controllable) and stochasticity of the environment (uncontrollable) (Paster et al., 2020; ˇStrupl et al., 2022). An optimal policy should maximize over the controllable (actions) and take expectations over uncontrollable (e.g., transitions) as shown in Figure 1 (right). This implies that, under a variational approach, the latent variable representation that a policy conditions on should not incorporate any information that is solely due to randomness in the environment. In other words, while the latent representation can and should include information about future behavior (i.e., actions), it should not reveal any information about the rewards or transitions associated with this behavior.\n\nTo this end, we propose a future-conditioned supervised learning framework termed dichotomy of control (DoC), which, in Stoic terms (Shapiro, 2014), has “the serenity to accept the things it cannot\n\n2\n\nSS’S’S’a1a2aaaESS’S’S’a1a2aaaDichotomy of ControlRCSL / Decision TransformerT=0.01r=100r=10T=1r=100r=10T=1T=0.01Published as a conference paper at ICLR 2023\n\nchange, courage to change the things it can, and wisdom to know the difference.” DoC separates mechanisms within a policy’s control (actions) from those beyond a policy’s control (environment stochasticity). To achieve this separation, we condition the policy on a latent variable representation of the future while minimizing the mutual information between the latent variable and future stochastic rewards and transitions in the environment. DoC only captures information from the controllable actions and avoids capturing information from the uncontrollable environment transitions in the latent variable so that maximization only happens with respect to the controllable actions. Theoretically, we show that DoC policies are consistent with their conditioning inputs, ensuring that conditioning on a high-return future will correctly induce high-return behavior. Empirically, we show that DoC can outperform both RCSL and na ̈ıve variational methods on highly stochastic environments.\n\n2 RELATED WORK\n\nReturn-Conditioned Supervised Learning. Since offline RL algorithms (Fujimoto et al., 2019; Wu et al., 2019; Kumar et al., 2020) can be sensitive to hyper-parameters and difficult to apply in practice (Emmons et al., 2021; Kumar et al., 2021), return-conditioned supervised learning (RCSL) has become a popular alternative, particularly when the environment is deterministic and near-expert demonstrations are available (Brandfonbrener et al., 2022). RCSL learns to predict behaviors (actions) by conditioning on desired returns (Schmidhuber, 2019; Kumar et al., 2019) using an MLP policy (Emmons et al., 2021) or a transformer-based policy that encapsulates history (Chen et al., 2021). Richer information other than returns, such as goals (Codevilla et al., 2018; Ghosh et al., 2019) or trajectory-level aggregates (Furuta et al., 2021), have also been used as inputs to a conditional policy in practice. Our work also conditions policies on richer trajectory-level information in the form of a latent variable representation of the future, with additional theoretical justifications of such conditioning in stochastic environments.\n\nRCSL Failures in Stochastic Environments. Despite the empirical success of RCSL achieved by DT and RvS, recent work has noted the failure modes in stochastic environments. Paster et al. (2020) and ˇStrupl et al. (2022) presented counter-examples where online RvS can diverge in stochastic environments. Paster et al. (2022) first identified the failure of return-conditioned supervised learning with stochastic transitions and proposed to cluster offline trajectories and condition the policy on the average cluster returns. While conditioning on expected as opposed to maximum returns is more reasonable, this approach has technical limitations (see Appendix C), and can lead to undesirable policy-averaging, i.e., a single policy covering two very different behaviors (clusters) that happen to have the same return. Brandfonbrener et al. (2022) also identified near-determinism as a necessary condition for RCSL to achieve optimality guarantees similar to other offline RL algorithms but did not propose a solution for RCSL in stochastic settings. Villaflor et al. (2022) also identifies overly optimistic behavior of DT and proposes to use discrete β-VAE to induce diverse future predictions a policy can condition on. This approach only differs the issue with stochastic environments to stochastic latent variables, i.e., the latent variables will still contain stochastic environment information that the policy cannot reliably reproduce.\n\nLearning Latent Variables from Offline Data. Various works have explored learning a latent variable representation of the future (or past) transitions in offline data via maximum likelihood and use the latent variable to assist planning (Lynch et al., 2020), imitation learning (Kipf et al., 2019; Ajay et al., 2020; Hakhamaneshi et al., 2021), offline RL (Ajay et al., 2020; Zhou et al., 2020), or online RL (Fox et al., 2017; Krishnan et al., 2017; Goyal et al., 2019; Shankar & Gupta, 2020; Singh et al., 2020; Wang et al., 2021; Venuto et al., 2021). These works generally focus on the benefit of increased temporal abstraction afforded by using latent variables as higher-level actions in a hierarchical policy. Villaflor et al. (2022) has introduced latent variable models into RCSL, which is one of the essential tools that enables our method, but they did not incoporate the appropriate constraints which can allow RCSL to effectively combat environment stochasticity, as we will see in our work. In general, existing work in latent variable models for future-conditioned supervised learning provides no theoretical guarantees in the literature, whereas our approach provides consistency guarantees. Lastly, Dietterich et al. (2018) used mutual information constraints to separate controllable from uncontrollable aspects of an MDP with the goal of accelerating reinforcement learning, whereas we study the dichotomy of control under the context of return-and-future conditioned supervised learning.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 PRELIMINARIES\n\nEnvironment Notation We consider the problem of learning a decision-making agent to interact with a sequential, finite-horizon environment. At time t = 0, the agent observes an initial state s0 determined by the environment. After observing st at a timestep 0 ≤ t ≤ H, the agent chooses an action at. After the action is applied the environment yields an immediate scalar reward rt and, if t < H, a next state st+1. We use τ := (st, at, rt)H t=0 to denote a generic episode generated from interactions with the environment, and use τi:j := (st, at, rt)j t=i to denote a generic sub-episode, with the understanding that τ0:−1 refers to an empty sub-episode. The return associated with an episode τ is defined as R(τ ) := (cid:80)H\n\nt=0 rt.\n\nWe will use M to denote the environment. We assume that M is determined by a stochastic reward function R, stochastic transition function T , and unique initial state s0, so that rt ∼ R(τ0:t−1, st, at) and st+1 ∼ T (τ0:t−1, st, at) during interactions with the environment. Note that these definitions specify a history-dependent environment, as opposed to a less general Markovian environment.\n\nLearning a Policy in RCSL In future- or return-conditioned supervised learning, one uses a fixed training data distribution D of episodes τ (collected by unknown and potentially multiple agents) to learn a policy π, where π is trained to predict at conditioned on the history τ0:t−1, the observation st, and an additional conditioning variable z that may depend on both the past and future of the episode. For example, in return-conditioned supervised learning, policy training minimizes the following objective over π:\n\nLRCSL(π) := Eτ ∼D\n\n(cid:34) H\n\n(cid:88)\n\nt=0\n\n− log π(at|τ0:t−1, st, z(τ ))\n\n,\n\n(1)\n\n(cid:35)\n\nwhere z(τ ) is the return R(τ ).\n\nInconsistency of RCSL To apply an RCSL-trained policy π during inference — i.e., interacting online with the environment — one must first choose a specific z.2 For example, one might set z to be the maximal return observed in the dataset, in the hopes of inducing a behavior policy which achieves this high return. Using πz as a shorthand to denote the policy π conditioned on a specific z, we define the expected return VM(πz) of πz in M as,\n\nVM(πz) := Eτ ∼Pr[·|πz,M] [R(τ )] .\n\n(2)\n\nIdeally the expected return induced by πz is close to z, i.e., z ≈ VM(πz), so that acting according to π conditioned on a high return induces behavior which actually achieves a high return. However, RCSL training according to Equation 1 will generally yield policies that are highly inconsistent in stochastic environments, meaning that the achieved returns may be significantly different than z (i.e., VM(πz) ̸= z). This has been highlighted in various previous works (Brandfonbrener et al., 2022; Paster et al., 2022; ˇStrupl et al., 2022; Eysenbach et al., 2022; Villaflor et al., 2022), and we provided our own example in Figure 1.\n\nApproaches to Mitigating Inconsistency A number of future-conditioned supervised learning approaches propose to learn a stochastic latent variable embedding of the future, q(z|τ ), while regularizing q with a KL-divergence from a learnable prior conditioned only on the past p(z|s0) (Ajay et al., 2020; Venuto et al., 2021; Lynch et al., 2020), thereby minimizing:\n\nLVAE(π, q, p) := Eτ ∼D,z∼q(z|τ )\n\n− log π(at|τ0:t−1, st, z)\n\n+ β · Eτ ∼D [DKL(q(z|τ )∥p(z|s0))] .\n\n(cid:35)\n\n(cid:34) H\n\n(cid:88)\n\nt=0\n\n(3)\n\nOne could consider adopting such a future-conditioned objective in RCSL. However, since the KL regularizer makes no distinction between observations the agent can control (actions) from those it cannot (environment stochasticity), the choice of coefficient β applied to the regularizer introduces\n\n2For simplicitly, we assume z is chosen at timestep t = 0 and held constant throughout an entire episode. As noted in Brandfonbrener et al. (2022), this protocol also encompasses instances like DT (Chen et al., 2021) in which z at timestep t is the (desired) return summed starting at t.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\na ‘lose-lose’ trade-off. Namely, as noted in Ajay et al. (2020), if the regularization coefficient is too large (β ≥ 1), the policy will not learn diverse behavior (since the KL limits how much information of the future actions is contained in z); while if the coefficient is too small (β < 1), the policy’s learned behavior will be inconsistent (in the sense that z will contain information of environment stochasticity that the policy cannot reliably reproduce). The discrete β-VAE incoporated by Villaflor et al. (2022) with β < 1 corresponds to this second failure mode.\n\n4 DICHOTOMY OF CONTROL\n\nIn this section, we first propose the DoC objective for learning future-conditioned policies that are guaranteed to be consistent. We then present a practical framework for optimizing DoC’s constrained objective in practice and an inference scheme to enable better-than-dataset behavior via a learned value function and prior.\n\n4.1 DICHOTOMY OF CONTROL VIA MUTUAL INFORMATION MINIMIZATION\n\nAs elaborated in the prevous section, whether z(τ ) is the return R(τ ) or more generally a stochastic latent variable with distribution q(z|τ ), existing RCSL methods fail to satisfy consistency because they insufficiently enforce the type of future information z can contain. A key observation is that z should not include any information due to environment stochasticity, i.e., any information about a future rt, st+1 that is not already known given the previous history up to that point τ0:t−1, st, at. (A similar observation was made in (Paster et al., 2022) under the more restrictive assumption that z is a cluster index, which we do not require here.) To address the independence requirement in a general and sound way, we modify the RCSL objective from Equation 1 to incorporate a conditional mutual information constraint between z and each pair rt, st+1 in the future:\n\nLDoC(π, q) := Eτ ∼D,z∼q(z|τ )\n\n− log π(at|τ0:t−1, st, z)\n\n(cid:35)\n\n(cid:34) H\n\n(cid:88)\n\nt=0\n\ns.t. MI(rt; z | τ0:t−1, st, at) = 0, MI(st+1; z | τ0:t−1, st, at) = 0,\n\n∀ τ0:t−1, st, at and 0 ≤ t ≤ H,\n\n(4)\n\n(5)\n\nwhere MI(rt; z|τ0:t−1, st, at) denotes the mutual information between rt and z given τ0:t−1, st, at when measured under samples of rt, z from D, q; and analogously for MI(st+1; z|τ0:t−1, st, at).\n\nThe first part of the DoC objective conditions the policy on a latent variable representation of the future, similar to the first part of the future-conditioned VAE objective in Equation 3. However, unlike Equation 3, the DoC objective enforces a much more precise constraint on q, given by the MI constraints in Equation 4.\n\n4.2 DICHOTOMY OF CONTROL IN PRACTICE\n\nContrastive Learning of DoC Constraints. To satisfy the mutual information constraints in Equation 4 we transform the MI to a contrastive learning objective. Specifically, for the constraint on r and z (and similarly on st+1 and z) one can derive,\n\nMI(rt; z|τ0:t−1, st, at)\n\n= DKL (Pr[rt, z|τ0:t−1, st, at]∥Pr[rt|τ0:t−1, st, at]Pr[z|τ0:t−1, st, at])\n\n= EPr[rt,z|τ0:t−1,st,at]\n\n(cid:20)\n\nlog\n\n(cid:18) Pr[rt|z, τ0:t−1, st, at] Pr[rt|τ0:t−1, st, at]\n\n(cid:19)(cid:21)\n\n= EPr[rt,z|τ0:t−1,st,at] log Pr[rt|z, τ0:t−1, st, at] − EPr[rt|τ0:t−1,st,at] log Pr[rt|τ0:t−1, st, at]. (6)\n\nThe second expectation above is a constant with respect to z and so can be ignored during learning. We further introduce a conditional distribution ω(rt|τ0:t−1, st, at) parametrized by an energy-based function f : Ω (cid:55)→ R:\n\nω(rt|z, τ0:t−1, st, at) ∝ ρ(rt) exp {f (rt, z, τ0:t−1, st, at)},\n\n(7)\n\nwhere ρ is some fixed sampling distribution of rewards. In practice, we set ρ to be the marginal distribution of rewards in the dataset. Hence we express the first term of Equation 6 via an optimization\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nover ω, i.e.,\n\nmax ω\n\n= max\n\nf\n\nEPr[rt,z|τ0:t−1,st,at] [log ω(rt|τ0:t−1, st, at)]\n\nEPr[rt,z|τ0:t−1,st,at]\n\n(cid:2)f (rt, z, τ0:t−1, st, at) − log Eρ( ̃r) [exp{f ( ̃r, z, τ0:t−1, st, at)}](cid:3) .\n\nCombining this (together with the analogous derivation for MI(st+1; z|τ0:t−1, st, at)) with Equation 4 via the Lagrangian, we can learn π and q(z|τ ) by minimizing the final DoC objective:\n\nLDoC(π, q) = max\n\nf\n\nEτ ∼D,z∼q(z|τ )\n\n− log π(at|τ0:t−1, st, z)\n\n(cid:35)\n\n(cid:34) H\n\n(cid:88)\n\nt=0\n\n+β ·\n\nH (cid:88)\n\nt=0\n\nEτ ∼D,z∼q(z|τ )\n\n(cid:2)f (rt, st+1, z, τ0:t−1, st, at) − log Eρ( ̃r, ̃s′) [exp{f ( ̃r, ̃s′, z, τ0:t−1, st, at)}](cid:3) .\n\n(8)\n\nAlgorithm 1 Inference with Dichotomy of Control\n\nInputs Policy π(·|·, ·, ·), prior p(·), value function V (·), initial state s0, number of samples hyperparameter K. Initialize z∗; V ∗ for k = 1 to K do\n\n▷ Track the best latent and its value.\n\n▷ Sample a latent from the learned prior.\n\nSample zk ∼ p(z|s0) if V (zk) > V ∗ then z∗ = zk; V ∗ = V\n\nreturn π(·|·, ·, z∗)\n\n▷ Set best latent to the one with the highest value. ▷ Policy conditioned on the best z∗.\n\nDoC Inference. As is standard in RCSL approaches, the policy learned by DoC requires an appropriate conditioning input z to be chosen during inference. To choose a desirable z associated with high return, we propose to (1) enumerate or sample a large number of potential values of z, (2) estimate the expected return for each of these values of z, (3) choose the z with the highest associated expected return to feed into the policy. To enable such an inference-time procedure, we need to add two more components to the method formulation: First, a prior distribution p(z|s0) from which we will sample a large number of values of z; second, a value function V (z) with which we will rank the potential values of z. These components are learned by minimizing the following objective:\n\nLaux(V, p) = Eτ ∼D,z∼q(z|τ )\n\n(cid:20)\n\n(V (z) − R(τ ))2 + DKL(stopgrad(q(z|τ ))∥p(z|s0))\n\n(cid:21) .\n\n(9)\n\nNote that we apply a stop-gradient to q(z|τ ) when learning p so as to avoid regularizing q with the prior. This is unlike the VAE approach, which by contrast advocates for regularizing q via the prior. See Algorithm 1 for inference pseudocode (and Appendix D for training pseudocode). Since DoC requires sampling multiple latents, it incurs additional computational overhead compared to vanilla DT. We differ improving the computational efficiency of DoC to future work.\n\n5 CONSISTENCY GUARANTEES FOR DICHOTOMY OF CONTROL\n\nWe provide a theoretical justification of the proposed learning objectives LDoC and Laux, showing that, if they are minimized, the resulting inference-time procedure will be sound, in the sense that DoC will learn a V and π such that the true value of πz in the environment M is equal to V (z). More specifically we define the following notion of consistency:\n\nDefinition 1 (Consistency). A future-conditioned policy π and value function V are consistent for a specific conditioning input z if the expected return of z predicted by V is equal to the true expected return of πz in the environment: V (z) = VM(πz).\n\nTo guarantee consistency of π, V , we will make the following two assumptions:\n\nAssumption 2 (Data and environment agreement). The per-step reward and next-state transitions observed in the data distribution are the same as those of the environment. In other words,\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nfor any τ0:t−1, st, at with Pr[τ0:t−1, st, at|D] > 0, we have Pr[ˆrt = rt|τ0:t−1, st, at, D] = R(ˆrt|τ0:t−1, st, at) and Pr[ˆst+1 = st+1|τ0:t−1, st, at, D] = T (ˆst+1|τ0:t−1, st, at) for all ˆrt, ˆst+1. Assumption 3 (No optimization or approximation errors). DoC yields policy π and value function In other words, V that are Bayes-optimal with respect to the training data distribution and q. V (z) = Eτ ∼Pr[·|z,D] [R(τ )] and π(ˆa|τ0:t−1, st, z) = Pr [ˆa = at|τ0:t−1, st, z, D].\n\nGiven these two assumptions, we can then establish the following consistency guarantee for DoC.\n\nTheorem 4. Suppose DoC yields π, V, q with q satisfying the MI constraints:\n\nMI(rt; z|τ0:t−1, st, at) = MI(st+1; z|τ0:t−1, st, at) = 0,\n\n(10)\n\nfor all τ0:t−1, st, at with Pr[τ0:t−1, st, at|D] > 0. Then under Assumptions 2 and 3, V and π are consistent for any z with Pr[z|q, D] > 0.\n\nFor proof, see Appendix A.\n\nConsistency in Markovian environments. While the results above are focused on environments and policies that are non-Markovian, one can extend Theorem 4 to Markovian environments and policies. This result is somewhat surprising, as the assignments of z to episodes τ induced by q are necessarily history-dependent, and projecting the actions appearing in these clusters to a nonhistory-dependent policy would seemingly lose important information. However, a Markovian assumption on the rewards and transitions of the environment is sufficient to ensure that no ‘important’ information will be lost, at least in terms of the satisfying requirements for consistency in Definition 1. Alternative notions of consistency are not as generally applicable; see Appendix C.\n\nWe begin by stating our assumptions.\n\nAssumption 5 (Markov environment). The rewards and transitions of M are Markovian; i.e., R(τ0:t−1, st, at) = R( ̃τ0:t−1, st, at) and T (τ0:t−1, st, at) = T ( ̃τ0:t−1, st, at) for all τ, ̃τ , st, at. We use the shorthand R(st, at), T (st, at) for these history-independent functions. Assumption 6 (Markov policy, without optimization or approximation errors). The policy learned by DoC is Markov. This policy π as well as its corresponding learned value function V are In other words, V (z) = Bayes-optimal with respect to the training data distribution and q. Eτ ∼Pr[·|z,D] [R(τ )] and π(ˆa|st, z) = Pr [ˆa = at|st, z, D].\n\nWith these two assumptions, we can then establish the analogue to Theorem 4, which relaxes the dependency on history for both the policy π and the MI constraints:\n\nTheorem 7. Suppose DoC yields π, V, q with q satisfying the MI constraints:\n\nMI(rt; z|st, at) = MI(st+1; z|st, at) = 0,\n\n(11)\n\nfor all st, at with Pr[st, at|D] > 0. Then under Assumptions 2, 5, and 6, V and π are consistent for any z with Pr[z|q, D] > 0.\n\nFor proof, see Appendix B.\n\n6 EXPERIMENTS\n\nWe conducted an empirical evaluation to ascertain the effectiveness of DoC. For this evaluation, we considered three settings: (1) a Bernoulli bandit problem with stochastic rewards, based on a canonical ‘worst-case scenario’ for RCSL (Brandfonbrener et al., 2022); (2) the FrozenLake domain from (Brockman et al., 2016), where the future VAE approach proves ineffective; and finally (3) a modified set of OpenAI Gym (Brockman et al., 2016) environments where we introduced environment stochasticity. In these studies, we found that DoC exhibits a significant advantage over RCSL/DT, and outperforms future VAE when the analogous to “one-step” RL is insufficient. We consider vanilla DT (Chen et al., 2021), which trains a ”returns-to-go, state, action” transformer. For DoC and the VAE baseline, we adopt similar inference scheme as (Lee et al., 2022), where multiple latent is sampled to determine the best next action. For DT, we use the same implementation and hyperparameters as Chen et al. (2021). Both VAE and DoC are built upon the DT implementation and additionally learn a Gaussian latent variable over succeeding 20 future steps. See experiment details in Appendix E and additional results in Appendix F.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n6.1 EVALUATING STOCHASTIC REWARDS IN BERNOULLI BANDIT\n\nBernoulli Bandit. Consider a two-armed bandit as shown in Figure 2 (left). The two arms, a1, a2, have stochastic rewards drawn from Bernoulli distributions of Bern(1 − p) and Bern(p), respectively. In the offline dataset, the a1 arm with reward Bern(1 − p) is pulled with probability πD(a1) = p. When p is small, this corresponds to the better arm only being pulled occasionally. Under this setup, πRCSL(a1|r = 1) = πRCSL(a2|r = 1) = 0.5, which is highly suboptimal compared to always pulling the optimal arm a1 with reward Bern(1 − p) for p < 0.5.\n\nResults. We train tabular DoC and baselines on 1000 samples where the superior arm with r ∼ Bern(1 − p) is pulled with probability p for p ∈ {0.1, ..., 0.5}. Figure 2 (right) shows that RCSL and percentage BC (filtered by r = 1) always result in policies that are indifferent in the arms, whereas DoC is able to recover the Bayesoptimal performance (dotted line) for all p values considered. Future VAE performs similarly to DoC for small p values, but is sensitive to the KL regularization coefficient when p is close to 0.5.\n\n6.2 EVALUATING STOCHASTIC TRANSITIONS IN FROZENLAKE\n\nFigure 2: [Left] Bernoulli bandit where the better arm a1 with reward Bern(1 − p) for p < 0.5 is pulled with probability πD(a1) = p in the offline data. [Right] Average rewards achieved by DoC and baselines across 5 environment seeds. RCSL is highly suboptimal when p is small, whereas DoC achieves close to Bayes-optimal performance (dotted line) for all values of p.\n\nFrozenLake. Next, we consider the FrozenLake environment with stochastic transitions where the agent taking an action has probability p of moving in the intended direction, and probability 0.5 · (1 − p) of slipping to either of the two sides of the intended direction. We collect 100 trajectories of length 100 using a DQN policy trained in the original environment (p = 1 3 ) which achieves an average return of 0.7, and vary p during data collection and evaluation to test different levels of stochasticity. We also include uniform actions with probability ε to lower the performance of the offline data so that BC is highly suboptimal.\n\nResults. Figure 3 presents the visualization (left) and results (right) for this task. When the offline data is closer to being expert (ε = 0.3), DT, future VAE, and DoC perform similarly with better performance in more deterministic environments. As the offline dataset becomes more suboptimal (ε = 0.5), DoC starts to dominate across all levels of transition stochasticity. When the offline data is highly suboptimal (ε = 0.7), DT and future VAE has little advantage over BC, whereas DoC continues to learn policies with reasonable performance.\n\nFigure 3: [Left] Visualization of the stochastic FrozenLake task. The agent has a probability p of moving in the intended direction and 1 − p of slipping to either sides. [Right] Average performance (across 5 seeds) of DoC and baselines on FrozenLake with different levels of stochasticity (p) and offline dataset quality (ε). DoC outperforms DT and future VAE, where the gain is more salient when the offline data is less optimal (ε = 0.5 and ε = 0.7).\n\n8\n\nr∼Bern(1−p)a1a2πD(a2)=1−pπD(a1)=pr∼Bern(p)UDLRT(s′ |s,a)=p random in = 0.3εD random in = 0.5εD random in = 0.7εDPublished as a conference paper at ICLR 2023\n\nFigure 4: Average performance (across 5 seeds) of DoC and baselines on modified stochastic Gym MuJoCo and AntMaze tasks. DoC and future VAE generally provide benefits over DT, where DoC provide more benefits on harder tasks such as Humanoid. Future VAE can be sensitive to the KL coefficient β, which can result in the failure mode shown in Reacher-v2 if not tuned properly.\n\n6.3 EVALUATING STOCHASTIC GYM MUJOCO\n\nEnvironments. We now consider a set of Gym MuJoCo environments including Reacher, Hopper, HalfCheetah, and Humanoid. We additionally consider AntMaze from D4RL (Fu et al., 2020). These environments are deterministic by default, which we modify by introducing time-correlated Gaussian noise to the actions before inputing the action into the physics simulator during data collection and evaluation for all but AntMaze environments. Specifically, the Gaussian noise we introduce to the actions has 0 mean and standard deviation of the form (1 − e−0.01·t) · sin(t) · σ where t is the step number and σ ∈ [0, 1]. For AntMaze where the dataset has already been collected in the deterministic environment by D4RL, we add gaussian noise with 0.1 standard deviation to the reward uniformly with probability 0.1 (both to the dataset and during evaluation).\n\nResults. Figure 4 shows the average performance (across 5 seeds) of DT, future VAE, and DoC on these stochastic environments. Both future VAE and DoC generally provide benefits over DT, where the benefit of DoC is more salient in harder environments such as HalfCheetah and Humanoid. We found future VAE to be sensitive to the β hyperparameter, and simply using β = 1 can result in the falure case as shown in Reacher-v2.\n\n7 CONCLUSION\n\nDespite the empirical promise of return- or future-conditioned supervised learning (RCSL) with large transformer architectures, environment stochasticity hampers the application of supervised learning to sequential decision making. To address this issue, we proposed to augment supervised learning with the dichotomy of control principle (DoC), guiding a supervised policy to only control the controllable (actions). Theoretically, DoC learns consistent policies, guaranteeing that they achieve the future or return they are conditioned on. Empirically, DoC outperforms RCSL in highly stochastic environments. While DoC still falls short in addressing other RL challenges such as ‘stitching’ (i.e., composing sub-optimal trajectories), we hope that dichotomy of control serves as a stepping stone in solving sequential decision making with large-scale supervised learning.\n\nACKNOWLEDGMENTS\n\nThanks to George Tucker for reviewing draft versions of this manuscript. Thanks to Marc Bellemare for help with derivations. Thanks to David Brandfonbrener and Keiran Paster for discussions around stochastic environments. We gratefully acknowledges the support of a Canada CIFAR AI Chair, NSERC and Amii, and support from Berkeley BAIR industrial consortion.\n\nREFERENCES\n\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR, 2020.\n\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive discovery for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv:2206.11795, 2022.\n\nDavid Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When arXiv\n\ndoes return-conditioned supervised learning work for offline reinforcement learning? preprint arXiv:2206.01079, 2022.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.\n\nFelipe Codevilla, Matthias M ̈uller, Antonio L ́opez, Vladlen Koltun, and Alexey Dosovitskiy. EndIn 2018 IEEE international conference on\n\nto-end driving via conditional imitation learning. robotics and automation (ICRA), pp. 4693–4700. IEEE, 2018.\n\nThomas Dietterich, George Trimponias, and Zhitang Chen. Discovering and removing exogenous state variables and rewards for reinforcement learning. In International Conference on Machine Learning, pp. 1262–1270. PMLR, 2018.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nBenjamin Eysenbach, Soumith Udatha, Sergey Levine, and Ruslan Salakhutdinov. Imitating past\n\nsuccesses can be very suboptimal. arXiv preprint arXiv:2206.03378, 2022.\n\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.\n\nRoy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options.\n\narXiv preprint arXiv:1703.08294, 2017.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pp. 2052–2062. PMLR, 2019.\n\nHiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline\n\nhindsight information matching. arXiv preprint arXiv:2111.10364, 2021.\n\nDibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019.\n\nAnirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Sch ̈olkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893, 2019.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\nKourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, and Michael Laskin. Hierarchi-\n\ncal few-shot imitation with skill transition models. arXiv preprint arXiv:2107.08981, 2021.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nThomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, and Peter Battaglia. Compile: Compositional imitation learning and execution. In International Conference on Machine Learning, pp. 3418–3428. PMLR, 2019.\n\nSanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg. Ddco: Discovery of deep continuous options for robot learning from demonstrations. In Conference on robot learning, pp. 418–437. PMLR, 2017.\n\nAviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint\n\narXiv:1912.13465, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n\nAviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. Should i run offline reinforcement learning or behavioral cloning? In International Conference on Learning Representations, 2021.\n\nKuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. arXiv preprint arXiv:2205.15241, 2022.\n\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In Conference on Robot Learning, pp. 1113– 1132. PMLR, 2020.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nKeiran Paster, Sheila A McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics\n\nmodels. arXiv preprint arXiv:2012.02419, 2020.\n\nKeiran Paster, Sheila McIlraith, and Jimmy Ba. You can’t count on luck: Why decision transformers\n\nfail in stochastic environments. arXiv preprint arXiv:2205.15967, 2022.\n\nKarl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned\n\nskill priors. arXiv preprint arXiv:2010.11944, 2020.\n\nMartin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John\n\nWiley & Sons, 2014.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n\nMachel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help offline reinforcement\n\nlearning? arXiv preprint arXiv:2201.12122, 2022.\n\nJuergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map them\n\nto actions. arXiv preprint arXiv:1912.02875, 2019.\n\nTanmay Shankar and Abhinav Gupta. Learning robot skills with temporal variational inference. In\n\nInternational Conference on Machine Learning, pp. 8624–8633. PMLR, 2020.\n\nFred R Shapiro. Who wrote the serenity prayer? The Chronicle Review, 28, 2014.\n\nAvi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. arXiv preprint arXiv:2011.10024, 2020.\n\nRupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja ́skowski, and J ̈urgen Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMiroslav ˇStrupl, Francesco Faccio, Dylan R Ashley, J ̈urgen Schmidhuber, and Rupesh Kumar Srivastava. Upside-down reinforcement learning can diverge in stochastic environments with episodic resets. arXiv preprint arXiv:2205.06595, 2022.\n\nDavid Venuto, Elaine Lau, Doina Precup, and Ofir Nachum. Policy gradients incorporating the\n\nfuture. arXiv preprint arXiv:2108.02096, 2021.\n\nAdam R Villaflor, Zhe Huang, Swapnil Pande, John M Dolan, and Jeff Schneider. Addressing optimism bias in sequence modeling for reinforcement learning. In International Conference on Machine Learning, pp. 22270–22283. PMLR, 2022.\n\nXiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter Abbeel, and Michael Laskin. Skill preferences: Learning to extract and execute robotic skills from human feedback. arXiv preprint arXiv:2108.05382, 2021.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nWenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforce-\n\nment learning. arXiv preprint arXiv:2011.07213, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nA PROOF OF THEOREM 4\n\nThe proof relies on the following lemma, showing that the MI constraints ensure that the observed rewards and dynamics conditioned on z in the training data are equal to the rewards and dynamics of the environment.\n\nLemma 8. Suppose DoC yields q satisfying the MI constraints:\n\nMI(rt; z|τ0:t−1, st, at) = MI(st+1; z|τ0:t−1, st, at) = 0,\n\nfor all τ0:t−1, st, at with Pr[τ0:t−1, st, at|D] > 0. Then under Assumption 2,\n\nPr [ˆr = rt | τ0:t−1, st, at, z, D] = R(ˆrt | τ0:t−1, st, at),\n\nPr [ˆst+1 = st+1 | τ0:t−1, st, at, z, D] = T (ˆst+1 | τ0:t−1, st, at),\n\nfor all τ0:t−1, st, at, z and ˆr, ˆst+1, as long as Pr[τ0:t−1, st, at, z|D] > 0.\n\n(12)\n\n(13)\n\n(14)\n\nProof. We show the derivations relevant to reward, with those for next-state being analogous. We start with the definition of mutual information:\n\nMI(rt; z|τ0:t−1, st, at) = E(rt,z)∼Pr[·|τ0:t−1,st,at,D]\n\n(cid:20)\n\nlog\n\nPr [rt|τ0:t−1, st, at, z, D] Pr [rt|τ0:t−1, st, at, D]\n\n(cid:21)\n\n= Ez∼Pr[·|τ0:t−1,st,at,D] [DKL(Pr[r|τ0:t−1, st, at, z, D]∥Pr[r|τ0:t−1, st, at, D])] .\n\n(15)\n\n(16)\n\nThe KL divergence is a nonnegative quantity, and it is zero only when the two input distributions are equal. Thus, the constraint MI(rt; z|τ0:t−1, st, at) = 0 implies,\n\nPr [r|τ0:t−1, st, at, z, D] = Pr[r|τ0:t−1, st, at, D],\n\nfor all τ0:t−1, st, at, z with Pr[z|τ0:t−1, st, at, D] > 0. From Assumption 2 we know\n\nPr[r|τ0:t−1, st, at, D] = R(r|τ0:t−1, st, at),\n\n(17)\n\n(18)\n\nand so we immediately have the desired result.\n\nWe will further employ the following lemma, which takes us most of the way to proving Theorem 4:\n\nLemma 9. Suppose DoC yields π, q with q satisfying the MI constraints:\n\nMI(rt; z|τ0:t−1, st, at) = MI(st+1; z|τ0:t−1, st, at) = 0,\n\nfor all τ0:t−1, st, at with Pr[τ0:t−1, st, at|D] > 0. Then under Assumptions 2 and 3, we have\n\nPr [τ | z, D] = Pr [τ | πz, M] ,\n\n(19)\n\n(20)\n\nfor all τ and all z with Pr[z|q, D] > 0.\n\nProof. We may write the probability Pr [τ | z, D] as,\n\nPr [τ | z, D] =\n\nH (cid:89)\n\nt=0\n\nPr [at | τ0:t−1, st, z, D]\n\n·\n\nH (cid:89)\n\nt=0\n\nPr [rt | τ0:t−1, st, at, z, D]\n\nPr [st+1 | τ0:t−1, st, at, z, D] .\n\n(21)\n\nH−1 (cid:89)\n\n·\n\nt=0\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nCase 1: We begin by considering the case of τ satisfying Pr [τ | z, D] > 0. For such a τ , by Assumption 3 we may write the first probability above as\n\nMoreover, by Lemma 8 we may write the second and third probabilities as\n\nPr [at | τ0:t−1, st, z, D] = πz(at|τ0:t−1, st).\n\nPr [rt | τ0:t−1, st, at, z, D] = R(rt|τ0:t−1, st, at) Pr [st+1 | τ0:t−1, st, at, z, D] = T (st+1|τ0:t−1, st, at).\n\nTherefore, for any τ with Pr [τ | z, D] > 0 we have,\n\nPr [τ | z, D] =\n\nH (cid:89)\n\nt=0\n\nπz(at|τ0:t−1, st) ·\n\nH (cid:89)\n\nt=0\n\n= Pr [τ | πz, M] .\n\nR(rt|τ0:t−1, st, at) ·\n\nH−1 (cid:89)\n\nt=0\n\nT (st+1|τ0:t−1, st, at)\n\n(22)\n\n(23) (24)\n\n(25)\n\nCase 2: To handle the case of Pr [τ | z, D] = 0 we will show that Pr [τ0:t | z, D] = 0 implies Pr [τ0:t | πz, M] = 0 by induction on t. The base case of t = −1 is trivial. For t > −1, we may write,\n\nPr [τ0:t | z, D] = Pr [τ0:t−1 | z, D] · Pr [st | τ0:t−2, st−1, at−1, z, D] · Pr [at | τ0:t−1, st, z, D] ·\n\nPr [rt | τ0:t−1, st, at, z, D] ,\n\n(26)\n\nPr [τ0:t | πz, M] = Pr [τ0:t−1 | πz, M] · T (st|τ0:t−2, st−1, at−1) · πz(at|τ0:t−1, st)·\n\nR(rt|τ0:t−1, st, at).\n\n(27)\n\nSuppose, for the sake of contradiction, that Pr [τ0:t | z, D] = 0 while Pr [τ0:t | πz, M] > 0. By the inductive hypothesis, Pr [τ0:t−1 | z, D] > 0. Thus, by Lemma 8 we must have\n\nPr [st | τ0:t−2, st−1, at−1, z, D] = T (st|τ0:t−2, st−1, at−1),\n\n(28)\n\nand so T (st|τ0:t−2, st−1, at−1) > 0 implies that Pr [st | τ0:t−2, st−1, at−1, z, D] > 0. Thus, by Assumption 3 we must have\n\nPr [at | τ0:t−1, st, z, D] = πz(at|τ0:t−1, st),\n\n(29)\n\nand so πz(at|τ0:t−1, st) > 0 implies that Pr [at | τ0:t−1, st, z, D] > 0. Lastly, by Lemma 8 we must have\n\nPr [rt | τ0:t−1, st, at, z, D] = R(rt|τ0:t−1, st, at), and so R(rt|τ0:t−1, st, at) > 0 implies that Pr [rt | τ0:t−1, st, at, z, D] > 0. Altogether, we find that each of the three terms on the RHS of Equation 26 is strictly positive and so Pr [τ0:t | z, D] > 0; contradiction.\n\n(30)\n\nA.1 THEOREM PROOF\n\nWe are now prepared to prove Theorem 4.\n\nUsing Assumption 3, we can express V (z) as,\n\nBy Lemma 9 we have,\n\nas desired.\n\n(cid:90)\n\nV (z) =\n\nPr [ˆτ = τ | z, D] · R(ˆτ ) dˆτ .\n\n(cid:90)\n\n(cid:90)\n\nV (z) =\n\n=\n\nPr [ˆτ = τ | z, D] · R(ˆτ ) dˆτ\n\nPr [ˆτ = τ | πz, M] · R(ˆτ ) dˆτ\n\n= VM(πz),\n\n14\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\nPublished as a conference paper at ICLR 2023\n\nB PROOF OF THEOREM 7\n\nWe begin by proving a result under stricter conditions, namely, when the MI constraints retain the conditioning on history.\n\nLemma 10. Suppose DoC yields π, V, q with q satisfying the MI constraints:\n\nMI(rt; z|τ0:t−1, st, at) = MI(st+1; z|τ0:t−1, st, at) = 0,\n\n(35)\n\nfor all τ0:t−1, st, at with Pr[τ0:t−1, st, at|D] > 0. Then under Assumptions 2, 5, and 6, V and π are consistent for any z with Pr[z|q, D] > 0.\n\nProof. Let\n\nBy Lemma 9 and Theorem 4 we have\n\nπhist\n\nz\n\n(ˆa | τ0:t−1, st) = Pr [ˆa = at | τ0:t−1, st, z, D] .\n\nfor all τ and\n\nfor all z with Pr[z | q, D] > 0.\n\nPr [τ | z, D] = Pr (cid:2)τ | πhist\n\nz\n\n, M(cid:3) ,\n\nV (z) = VM(πhist\n\nz\n\n),\n\n(36)\n\n(37)\n\n(38)\n\nIt is left to show that VM(πhist ) = VM(πz). To do so, we invoke Theorem 5.5.1 in Puterman (2014), which states that, for any history-dependent policy, there exists a Markov policy such that the state-action visitation occupancies of the two policies are equal (and, accordingly, their values are equal). In other words, there exists a Markov policy ̃πz such that\n\nz\n\nPr (cid:2)ˆs = st, ˆa = at | πhist\n\nz\n\n, M(cid:3) = Pr [ˆs = st, ˆa = at | ̃πz, M] ,\n\nfor all t, ˆs, ˆa, and\n\nVM(πhist\n\nz\n\n) = VM( ̃πz).\n\nTo complete the proof, we show that ̃πz = πz. By Equation 37 we have\n\nPr (cid:2)ˆs = st, ˆa = at | πhist\n\nz\n\n, M(cid:3) = Pr [ˆs = st, ˆa = at | z, D] .\n\nThus, for any t, ˆs, ˆa we have\n\n ̃πz(ˆa = at|ˆs = st) =\n\n=\n\n=\n\nPr [ˆs = st, ˆa = at | ̃πz, M] Pr [ˆs = st | ̃πz, M] Pr (cid:2)ˆs = st, ˆa = at | πhist z\nPr [ˆs = st | πhist , M] Pr [ˆs = st, ˆa = at | z, D] Pr [ˆs = st | z, D]\n\nz\n\n, M(cid:3)\n\n= πz(ˆa = at|ˆs = st),\n\n(39)\n\n(40)\n\n(41)\n\n(42)\n\n(43)\n\n(44)\n\n(45)\n\nwhere the first equality is Bayes’ rule, the second equality is due to Equation 39, the third equality is due to Equation 41, and last equality is by definition of πz (Assumption 6).\n\nBefore continuing to the main proof, we present the following analogue to Lemma 8:\n\nLemma 11. Suppose DoC yields q satisfying the MI constraints:\n\nfor all st, at with Pr[st, at|D] > 0. Then under Assumptions 2 and 5,\n\nMI(rt; z|st, at) = MI(st+1; z|st, at) = 0,\n\nPr [ˆr = rt | st, at, z, D] = R(ˆrt | st, at),\n\nPr [ˆst+1 = st+1 | st, at, z, D] = T (ˆst+1 | st, at),\n\nfor all st, at, z and ˆr, ˆst+1, as long as Pr[st, at, z|D] > 0.\n\nProof. The proof is analogous to the proof of Lemma 8.\n\n15\n\n(46)\n\n(47)\n\n(48)\n\nPublished as a conference paper at ICLR 2023\n\nB.1 THEOREM PROOF\n\nWe can now tackle the proof of Theorem 7. To do so, we start by interpreting the episodes τ in the training data D as coming from a modified Markovian environment M†. Specifically, we define M† as an environment with the same state space as M but with an action space consisting of tuples (a, r, s′), where a is an action from the action space of M, r is a scalar, and s′ is a state from the state space of M. We define the reward and transition functions of M† to be deterministic, so that the reward and next state associated with (a, r, s′) is r and s′, respectively. This way, we may interpret any episode τ = (st, at, rt)H\n\nt=0 in M as an episode\n\nτ † = (st, (at, rt, st+1), rt)H\n\nt=0\n\n(49)\n\nin the modified environment M†. Denoting D† as the training data distribution when interpreted in this way, we note that the MI constraints of Lemma 10 hold, since rewards and transitions are deterministic. Thus, the policy π† defined as\n\nπ†((ˆa, ˆr, ˆs′)|st, z) = Pr[(ˆa, ˆr, ˆs′) = (at, rt, st+1)|st, z, D†]\n\nsatisfies\n\nIt is left to show that VM†(π† any single-step transition in this episode,\n\nV (z) = VM†(π†\n\nz). z) = VM(πz). To do so, consider an episode τ † ∼ Pr[·|π†\n\nz, M†]. For\n\n(50)\n\n(51)\n\n(st, (at, rt, st+1), rt, st+1),\n\nwe have, by definition of π† z,\n\nPr[ˆa = at|st, π†\n\nz] = Pr[ˆa = at|st, z, D†] = πz(ˆa|st).\n\nIn a similar vein, by definition of π†\n\nz and Lemma 11 we have,\n\nPr[ˆr = rt|st, at, π† Pr[ˆst+1 = st+1|st, at, π†\n\nz] = Pr[ˆr = rt|st, at, z, D†] = R(ˆr|st, at), z] = Pr[ˆst+1 = st+1|st, at, z, D†] = T (ˆst+1|st, at).\n\n(52)\n\n(53)\n\n(54)\n\n(55)\n\nThus, any τ † = (st, (at, rt, st+1), rt)H (st, at, rt)H R(τ †) = R(τ ), and so we immediately have\n\nt=0 sampled from π† t=0 in the original environment M, where Pr[τ †|π†\n\nz, M† can be mapped back to a τ = z, M†] = Pr[τ |πz, M]. It is clear that\n\nas desired.\n\nVM†(π†\n\nz) = VM(πz),\n\n(56)\n\nC INVALIDITY OF ALTERNATIVE CONSISTENCY FRAMEWORKS\n\nPaster et al. (2022) propose a similar but distinct notion of consistency compared to ours (i.e., Definition 1), and claim that it can be achieved with stationary policies in Markovian environments. In this section, we show that this is, in fact, false, supporting the benefits of our framework. We begin by rephrasing Theorem 2.1 of Paster et al. (2022) using our own notation:\n\n(Incorrect) Theorem 2.1 of Paster et al. (2022). such that\n\nSuppose M is Markovian and D, q are given\n\nPr[ˆst+1 = st | st, at, z, D] = Pr[ˆst+1 = st | st, at, D],\n\nfor all st, at, z, ˆst+1 with Pr[st, at, z|q, D] > 0 and define a Markov policy π as\n\nThen for any z with Pr[z|q, D] > 0 and any τ ,\n\nπ(ˆa|st, z) = Pr[ˆa = at|st, z, D].\n\nPr[τ | πz, M] > 0 if and only if Pr[τ | z, D] > 0.\n\n(57)\n\n(58)\n\n(59)\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Deterministic environment used in the counter-example described in Appendix C. Circles represent states and squares represent actions; solid arrows represent choice of actions and dashed arrows represent environment dynamics.\n\nCounter-example. A simple counter-example may be constructed by considering the Markovian environment displayed in Figure 5. The environment has three states. The first state gives a choice of two actions (a0 ∈ {0, 1}), and each action deterministically transitions to the same second state. The second state again provides a choice of two actions (a1 ∈ {0, 1}), and each of these again deterministically transitions to the same terminal state. Thus, episodes in this environment are uniquely determined by choice of a0, a1. There are four unique episodes:\n\nτ0 = ⟨a0 = 0, a1 = 0⟩, τ1 = ⟨a0 = 1, a1 = 1⟩, τ2 = ⟨a0 = 0, a1 = 1⟩, τ3 = ⟨a0 = 1, a1 = 0⟩.\n\n(60) (61)\n\n(62)\n\n(63)\n\nWe now construct q as a deterministic function, clustering these four trajectories into two distinct z:\n\nz0 = q(τ0) = q(τ1), z1 = q(τ2) = q(τ3).\n\n(64)\n\n(65)\n\nSuppose D includes τ0, τ1, τ2, τ3 with equal probability. Since the environment is deterministic, the conditions of Theorem 2.1 in Paster et al. (2022) are trivially satisfied. Learning a policy π with respect to z0 yields\n\nπ(·|s0, z0) = [0.5, 0.5], π(·|s1, z0) = [0.5, 0.5].\n\n(66)\n\n(67)\n\nHowever, it is clear that interacting with π(·|·, z0) in the environment will lead to τ2, τ3 with nonzero probability, while τ2, τ3 are never associated with z0 in the data D. Contradiction.\n\nD PSEUDOCODE FOR DOC TRAINING\n\nAlgorithm 2 Training with Dichotomy of Control\n\nt\n\nt\n\nt\n\n, r(m)\n\n, a(m)\n\n0 }M\n\nm=1 where τ (m) = (s(m)\n\nm=1 and initial return-to-go values {R(m)}M\n\n)H Inputs Offline dataset D = {τ (m)}M t=0 with initial states {s(m) m=1, a parametrized distribution qφ(·), a policy πθ1 (·, ·), a value function Vθ2(·), a prior pψ(·), an energy function fw(·), a fixed distribution ρ(r, s′), learning rates η, and training batch size B. while training has not converged do Sample batch {(τ = (st, at, rt)H Sample z from qφ(τ ) with reparametrization. Compute LDoC + Laux according to Equation 8 and Equation 9. Update φ ← φ − η∇φ ˆL, ψ ← ψ − η∇ψstopgrad( ˆL, φ), w ← w + η∇w ˆL, θ1 ← θ1 − η∇θ1 θ2 ← θ2 − η∇θ1\n\nm=1 from D, for m = 1, . . . , B.\n\nt=0)(m)}B\n\nˆL.\n\nˆL,\n\nreturn πθ1 (·, ·), Vθ2 (·), pψ(·)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nE EXPERIMENT DETAILS\n\nE.1 HYPERPARAMETERS.\n\nWe use the same hyperparameters as the publically available Decision Transformer (Chen et al., 2021) implementation. For VAE, we additionally learn a future and a prior both parametrized the same as the policy using transformers with context length 20. All models are trained on NVIDIA GPU P100.\n\nTable 1: Hyperparameters of Decision Transformer, future-conditioned VAE, and Dichotomy of Control.\n\nHyperparameter\n\nValue\n\nNumber of layers Number of attention heads Embedding dimension Latent future dimension Nonlinearity function Batch size Context length K\n\nFuture length Kf Return-to-go conditioning for DT\n\nDropout Learning rate Grad norm clip Weight decay Learning rate decay β coefficient\n\n3 1\n128 128 ReLU 64 20 FrozenLake, HalfCheetah, Hopper, Humanoid, AntMaze 5 Reacher Same as context length K 1 FrozenLake 6000 HalfCheetah 3600 Hopper 5000 Humanoid 50 Reacher 1 AntMaze 0.1 10−4 0.25 10−4 Linear warmup for first 105 training steps 1.0 for DoC, Best of 0.1, 1.0, 10 for VAE\n\nE.2 DETAILS OF THE OFFLINE DATASETS\n\nFrozenLake. We train a DQN (Mnih et al., 2013) policy for 100k steps in the original 4x4 FrozenLake Gym environment with stochasticity level p = 1 3 . We then modify p to simulate environments of different stochasticity levels, while collecting 100 trajectories of maximum length 100 at each level using the trained DQN agent with probability ε of selecting a random action as opposed to the action output by the DQN agent to emulate offline data with different quality.\n\nGym MuJoCo. We train SAC (Haarnoja et al., 2018) policies on the original set of Gym MuJoCo environments for 100M steps. To simulate stochasticity in these environments, we modify the original Gym MuJoCo environments by introducing noise to the actions before inputting the action to the physics simulator to compute rewards and next states. The noise has 0 mean and standard deviation of the form (1 − e−0.01·t) · sin(t) · σ where t is the step number and σ ∈ [0, 1]. We then collect 1000 trajectories of 1000 steps each for all environments except for Reacher (which has 50 steps in each trajectory) in the stochastic version of the environment using the SAC policy to acquire the offline dataset for training.\n\nAntMaze. For the AntMaze task, we use the AntMaze dataset from D4RL (Fu et al., 2020), which contains 1000 trajectories of 1000 steps each. We add gaussian noise with standard deviation 0.1 to the rewards in the dataset uniformly with probability 0.1 to both the offline dataset and during environment evaluation to simulate stochastic rewards from the environment.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nF ADDITIONAL RESULTS\n\nF.1 FROZENLAKE WITH DIFFERENT OFFLINE DATASET QUALITY.\n\nFigure 6: Average performance (across 5 seeds) of DoC and baselines on FrozenLake with different levels of stochasticity (p) and offline dataset quality (ε). DoC outperforms DT and future VAE with bigger gains the offline data is less optimal.\n\nF.2\n\nIMPROVEMENT OF DOC OVER RVS\n\nTo test the effect of applying the MI constraint to other future-conditioned supervised learning baselines, we evaluate RvS parametrized by MLP policies (Emmons et al., 2021) with VAE and DoC modifications. In general, MLP parametrization performs worse than transformer parametrization, but DoC is still able to provide significant benefit over vanilla RvS.\n\nFigure 7: Average performance (across 5 seeds) of DoC and baselines on FrozenLake with different levels of stochasticity (p) and offline dataset quality (ε). DoC outperforms RvS and future VAE with bigger gains the offline data is less optimal.\n\n19\n\n random in = 0.1εD random in = 0.2εD random in = 0.3εD random in = 0.4εD random in = 0.5εD random in = 0.6εD random in = 0.7εD random in = 0.8εD random in = 0.9εD random in = 0.3εD random in = 0.5εD random in = 0.7εDUDLRT(s′ |s,a)=pPublished as a conference paper at ICLR 2023\n\nG ADDITIONAL ABLATIONS\n\nG.1 VAE WITH STOP GRADIENT\n\nOne difference between DoC and VAE is whether there is a stop gradient operation on the posterior q(z|τ ) when minimizing the KL-divergence between q(z|τ ) and the prior p(z|s0). We conduct the ablation below in Figure 8 where we also apply stop gradient to VAE, and observe that VAE’s performance drops significantly.\n\nFigure 8: Average performance (across 5 seeds) of DoC and baselines on FrozenLake with different levels of stochasticity (p) and offline dataset quality (ε). DoC outperforms RvS and future VAE with bigger gains the offline data is less optimal.\n\n20\n\n0.20.30.40.50.6Prob. moving in intended dir0.20.40.6performanceVAEVAE stopgradDoCPublished as a conference paper at ICLR 2023\n\nG.2 DOC WITH DIFFERENT REGULARIZATION STRENGTH (β)\n\nFigure 9: Average performance (across 5 seeds) of DoC with different regularization strength (β). The effect of β is more pronounced when the dataset is highly optimal (e.g., ε random in D = 0.7), for which we found a smaller β (e.g., 0.1) to generally perform better.\n\nG.3 DOC WITH DIFFERENT NUMBER OF FUTURE SAMPLES (K)\n\nFigure 10: Average performance (across 5 seeds) of DoC with different number of samples during inference (K). We found that higher number of samples leads to better performance as we expect, and the gain beyound 100 samples is negligible.\n\n21\n\n0.20.30.40.50.6Prob. moving in intended dir0.10.20.30.4performanceDoC, beta=0.01DoC, beta=0.1DoC, beta=1DoC, beta=100.20.30.40.50.6Prob. moving in intended dir0.30.40.5performanceDoC, K=10DoC, K=50DoC, K=100DoC, K=500",
    "reference": "# Summary Of The Paper\n\nThis paper aims to address the issue of inconsistency in return-conditioned supervised learning (RCSL), such as Decision Transformers. Specifically, when RCSL in highly stochastic environments is conditioned on the highest dataset return, the resulting expected return could be lower as the environment randomness is not under the agent's control. To this end, this paper proposes to capture only the agent's controllable factors and minimize the learning dependence on future rewards and transitions — which are environment characteristics. Under reasonable assumptions, the paper proves that their latent variable approach with Mutual Information constraints leads to policies consistent with their conditioning input reward.\n\n# Strength And Weaknesses\n\n## Strengths\n- The paper identifies an important problem and solution for RCSL with inconsistent conditioning, especially in stochastic environments.\n- The paper is clearly written with all the necessary details and flow to understand everything.\n- A good set of simple environments are chosen, including a didactic Bernoulli Bandit environment and MuJoCo benchmarks.\n- The paper has theoretical results on the consistency of their proposed method.\n\n## Weaknesses\n- **Missing crucial comparisons**\n    + There are two key differences of DoC from prior approaches (i.e., VAE) using latent embedding of future to mitigate inconsistency: (a) Mutual Information (MI) constraint and (b) Inference from a learned prior and value function. The importance of these two components must be ablated on all the environments. Specifically, the following comparisons can be added:\n        * VAE + Inference with learned prior (a separate copy with stopgrad) and value function\n        * DoC w/o MI constraint (= DoC - a)\n        * DoC + conditioning on highest return (= DoC - b)\n- **Comparison against VAE**\n    + Since future-VAE also regularizes the z to the learnable prior conditioned on the past, this ensures that the latent z is incentivized not to use future information. Therefore, DoC's key benefit must come from utilizing the controllable part of the future in z while ignoring the environment transitions. Why is it expected that encoding the controllable information in z will improve the performance? What are example environments with this property?\n    + The difference between the ideology of VAE and DoC being seemingly small is reinforced by the results on the mujoco environments, where the empirical performance of VAE and DoC are pretty similar. Even on reacher, where KL beta is not tuned well, VAE first reachers almost the optimal performance before falling. Therefore, it is not clear that DoC is necessarily better than VAE.\n        * Is more stochasticity in mujoco environments expected to increase the difference in performance between DoC and VAE?\n        * Is any other experiment (quantitative or qualitative) possible to show that the importance of encoding controllable future information in z is helpful?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written with good quality and novelty. However, certain experiments are needed to justify the novelty.\n\n# Summary Of The Review\n\nI like the paper's writing, problem, and proposed approach. However, it is missing some crucial justification as to why it is expected to be better than the best baseline (VAE), which prior work has proposed to mitigate inconsistency. I would happily reconsider my rating if the above issues are addressed, resulting in a clear demonstration of improvement over baselines and ablations.\n\n----  \n[Post-rebuttal]\nThe author response addressed most of my concerns, except \"DoC does not convincingly outperform VAE in control tasks.\" I have increased my score to reflect this.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\n-GNN: INCORPORATING RING PRIORS\n\nO INTO MOLECULAR MODELING∗\n\n1Jinhua Zhu, 1Kehan Wu, 1Bohan Wang, 2Yingce Xia, 3Shufang Xie, 2Qi Meng, 2Lijun Wu, 2Tao Qin, 1Wengang Zhou, 1Houqiang Li, 2Tie-Yan Liu 1University of Science and Technology of China, 2Microsoft Research AI4Science 3Gaoling School of Artificial Intelligence, Renmin University of China 1\n\n@mail.ustc.edu.cn, 1bhwangfy@gmail.com,\n\nteslazhu, wu 2018 zhwg,lihq yingce.xia, meq, lijuwu, taoqin, tyliu\n\n@ustc.edu.cn, 3shufangxie@ruc.edu.cn,\n\n1\n\n2\n\n}\n\n}\n\n@microsoft.com\n\n}\n\n{ {\n{\n\nABSTRACT\n\nO\n\nCyclic compounds that contain at least one ring play an important role in drug design. Despite the recent success of molecular modeling with graph neural networks (GNNs), few models explicitly take rings in compounds into consideration, consequently limiting the expressiveness of the models. In this work, we design a -GNN), that explicitly models rings new variant of GNN, ring-enhanced GNN ( -GNN, each ring is represented in addition to atoms and bonds in compounds. In by a latent vector, which contributes to and is iteratively updated by atom and bond representations. Theoretical analysis shows that -GNN is able to distinguish two isomorphic subgraphs lying on different rings using only one layer while conventional graph convolutional neural networks require multiple layers to distinguish, -GNN demonstrating that shows good performance on 11 public datasets. In particular, it achieves state-ofthe-art validation result on the PCQM4Mv1 benchmark (outperforming the previous KDDCup champion solution) and the drug-drug interaction prediction task on -GNN outperforms strong baselines (without modelDrugBank. Furthermore, ing rings) on the molecular property prediction and retrosynthesis prediction tasks. The code is released at https://github.com/O-GNN/O-GNN.\n\n-GNN is more expressive. Through experiments,\n\nO\n\nO\n\nO\n\nO\n\nO\n\n1\n\nINTRODUCTION\n\nCyclic compounds, which refers to the molecules that have at least one ring in its system, naturally exist in the chemical space. According to our statistics on 109M compounds from PubChem (Kim et al., 2019) which is a widely used chemical library, more than 90% compounds have at least one ring. The rings could be small/simple (e.g., the benzene is a six-member carbon ring, and the pentazole is a five-member nitrogen ring) or large/complex (e.g., the molecule shown in Figure 1). Rings are important in drug discovery, for example: (1) Rings can potentially reduce the flexibility of molecules, reduce the uncertainty when interacting with target proteins, and lock the molecules to their bioactive conformation (Sun et al., 2012). (2) Macrocyclic compounds, which usually have a ring with more than 12 atoms, play important roles in antibotics design (Venugopal & Johnson, 2011) and peptide drug design (Bhardwaj et al., 2022).\n\nRecently, deep neural networks, especially graph neural networks (denoted as GNN) (Kipf & Welling, 2017; Hamilton et al., 2017a), have been widely used in molecular modeling. A GNN takes a graph as input, and messages of different nodes are passed along edges. GNNs have made great success in scientific discovery: (1) Stokes et al. (2020) train a GNN to predict growth inhibition of Escherichia coli and find that Halicin is a broad-spectrum bactericidal antibiotic. (2) Shan et al. (2022) leverage GNN to model the interactions between proteins, and they eventually obtain possible antibodies for SARS-CoV-2. In addition, GNNs are widely used in drug property prediction (Rong et al., 2020), drug-target interaction modeling (Torng & Altman, 2019), retrosynthesis\n\n∗This work was done when Jinhua Zhu, Kekan Wu and Bohan Wang were interns at Microsoft Research\n\nAI4Science. Correspondence to: Yingce Xia.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(Chen & Jung, 2021), etc. However, none of the above work explicitly models the ring information into GNNs. From the application’s perspective, they miss an important feature for their tasks. From the machine learning’s perspective, Loukas (2020) points out that existing message-passing-based GNNs cannot properly capture the ring information when the product of network width and height is not large enough (see the Table 1 in Loukas (2020)). Therefore, with the classic GNNs, the ring information in compounds is not well leveraged.\n\nO\n\nO\n\nTo tackle this issue, in this work, we propose a new model, ring-enhanced GNN (denoted as -GNN), that explicitly models the ring information in a compound. stands for the rings in molecules and is pronounced The as “O”. Generally speaking, -GNN stacks L layers, and each layer sequentially updates edge representations, node representations and ring representations by aggregating their neighbourhood information. We mainly use a self-attention layer for adaptive message passing, and use a feed-forward layer to introduce non-linearity to representations.\n\nO\n\nO\n\n-GNN through We first demonstrate the advantage of O\ntheoretical analysis. -GNN is able to distinguish two isomorphic sub-graphs lying on different rings using only one layer (see Figure 2 for the example). On the contrary, if we remove the ring-modeling components from -GNN, such a distinguishability would require multiO ple layers (see Section 2.3 for detailed analysis). These results demonstrate that -GNN is more expressive than conventional graph convolutional networks in the absence of explicitly modeling rings.\n\nFigure 1: Paclitaxel, a compound with 7 simple rings. Kampan et al. (2015) summarized that the intact taxane ring (i.e., r4, r5, r6) and a four-membered oxetane side ring (i.e., r7) is essential to induce cytotoxic activity.\n\nO\n\nWe then conduct experiments on 11 datasets from three tasks, including molecular property prediction, drug-drug interaction prediction and retrosynthesis:\n\n(1) For molecular property prediction, we first conduct experiments on PCQM4Mv1, which is to predict the HOMO-LUMO gap of molecules. Our method outperforms the champion solution of KDDCup on the validation set (Shi et al., 2022) (note that test set labels are not -GNN on six datasets from available). Next, we verify O\nMoleculeNet (Wu et al., 2018), which is to predict several pharmaceutical related properties of molecules. -GNN outperforms the corresponding GNN baselines without rings. Finally, we conduct experiments on FS-Mol (Stanley et al., 2021), a few-shot property prediction task, and shows that modeling rings can also improve the prediction accuracy. (2) For drug-drug interaction prediction, which is to predict whether two drugs interacts with each -GNN on DrugBank following the previous settings (Nyamabo et al., 2021; Li et al., other, we test 2022), and achieve state-of-the-art results. (3) For retrosynthesis, we apply -GNN to LocalRetro (Chen & Jung, 2021), a strong GNN-based method for retrosynthesis. On USPTO-50k, our method significantly boosts the accuracy.\n\nFigure 2: An illustrative example of theoretical results. The three substructures in the red circles are isomorphic. The second and third substructures lie on different rings (a Cyclooctane and an Azocane). A regular GNN requires multiple layers to distinguish the three -GNN requires substructures while only one layer due to the ring representations.\n\nO\n\nO\n\nO\n\nO\n\n2 METHOD\n\n2.1 NOTATION AND PRELIMINARIES\n\nLet G = (V, E) denote a molecular graph, where V and E are the collections of nodes/atoms and edges/bonds1. Let R denote the collection of rings in G. Define V = and\n\nv1, v2,\n\n{\n\n, v|V |}\n\n· · ·\n\n1When the context is clear, we use nodes/atoms and edges/bonds alternatively in this work.\n\n2\n\n(A)(B)(C)Published as a conference paper at ICLR 2023\n\n{\n\n}\n\n{\n\nN\n\nvj\n\neij\n\neij\n\n(i) =\n\n, where vi is the i-th atom and eij is the bond connecting vi and vj. When the context is E = (i) denote the neighbors clear, we use i to denote atom vi, and use e(vi, vj) to denote edge eij. Let N\nof atom i, i.e., , where each ri is a simple , r|R|} E\nring. A simple ring does not contain any ring structure. For example, for the molecule in Figure 3, it has two simple rings as we marked (r1 and r2). The ring (1, 2, 3, 4, 5, 6, 7, 8, 9, 1) is not a simple ring. Let R(vi) and R(eij) denote the rings that the atom vi or the bond eij lies on, and V (r) and E(r) denote all the atoms and the bonds lying on ring r. For example, in Figure 3, R(v4) = while R(v3) = r2. R(e49) = E(r1) =\n\nwhile R(e78) = r1. V (r1) =\n\ne45, e56, e67, e78, e89, e94\n\nv4, v5, v6, v7, v8, v9\n\n. Define R =\n\nr1, r2,\n\nr1, r2\n\n} and\n\n· · ·\n\n∈\n\n{\n\n{\n\n{\n\n{\n\n}\n\n}\n\n{\n\n}\n\n|\n\nr1, r2 .\n}\n\nA graph neural network (GNN) is usually stacked by several identical GNN layers. Each GNN layer is composed of an Aggregate function and an Update function,\n\nwhere hi is the representation of atom i and h′ different Aggregate functions and Update functions. Details are summarized in Appendix D.\n\ni is its updated representation. Different GNNs have\n\nh′\n\ni = Update (hi, Aggregate(hj\n\n(i))) ,\n\n(1)\n\nj\n\n|\n\n∈ N\n\nFigure 3: The workflow of our method. H (l) tions of bond, atom, ring and the global compound at the l-th layer.\n\nV , H (l)\n\nE , H (l)\n\nR and U (l) denote the representation collec-\n\n2.2 MODEL\n\ni\n\ni\n\n, h(l)\n\nij and h(l)\n\nOur model consists of L identical layers with different parameters. The architecture of each layer is shown in Figure 3. Let h(l) r denote the output representations of atom vi, bond eij and ring r at the l-th layer, respectively. Let U (l) denote the compound representation at the l-th layer. We initialize h(0) via a learnable embedding layer which indicates its atomic type, chirality, degree number, formal charge, hybridization type, and so on. Similarly, we initialize h(0) ij with a learnable embedding wich indicates its bond type, stereoisomerism type and whether the bond is conjugated. Then we initialize h(0) by concatenating the node and edge embedding and then transform it with a non-linear layer. Last, we initialize the compound representation with a learnable embedding. In each layer, we update representations of nodes, bonds, rings and the compound sequentially. We will frequently use MLP( ), a multi-layer perception network with one hidden layer, to build our model. The inputs of MLP are concatenated as a long vector and processed by the network.\n\n· · ·\n\nr\n\n(1) Update bond representations: The representation of a bond is updated via the connected atoms, the rings that the bond belongs to and the compound representation from the last layer:\n\nh(l)\n\nij = h(l−1)\n\nij\n\n h(l−1)\n\ni\n\n+ MLP\n\n, h(l−1)\n\nj\n\n, h(l−1)\n\nij\n\n,\n\n(cid:80)\n\nr∈R(eij ) h(l−1) |R(eij)|\n\nr\n\n\n\n, U (l−1)\n\n .\n\n(2)\n\n(2) Update atom representations: We use an attention model to adaptively aggregate bond representations into the centralized atoms. Mathematically,\n\n ̄h(l)\n\ni =\n\n(cid:88)\n\nj∈N (i)\n\nαjWvconcat(h(l)\n\nij , h(l−1)\n\nj\n\n);\n\nαj ∝ exp(a⊤LeakyReLU(Wqh(l−1)\n\ni\n\nh(l)\n\ni = h(l−1)\n\ni\n\n+ MLP\n\n(cid:16)\n\nh(l−1)\n\ni\n\n, ̄h(l)\n\ni\n\n,\n\n+ Wkconcat(h(l−1) 1\n|R(vi)|\n\nh(l−1)\n\n, h(l) , U (l−1)(cid:17)\n\n(cid:88)\n\nr\n\nj\n\n.\n\nij )));\n\nr∈R(vi)\n\n(3)\n\n3\n\nHV(l−1)HE(l−1)HR(l−1)U(l−1)input:Update bondUpdate atomUpdate ringUpdate compoundHE(l)HV(l)HR(l)U(l)012378945601237894560123789456r1r2r1r2r1r2Published as a conference paper at ICLR 2023\n\nIn Eqn.(3), the W ’s are the parameters to be learned, and concat denotes concatenating the input vectors as a long one.\n\n(3) Update ring representations: The ring representations are updated using MLP networks:\n\nh(l)\n\nr = h(l−1)\n\nr\n\n(cid:16)\n\n+MLP\n\nh(l−1)\n\nr\n\n,\n\n(cid:88)\n\nh(l)\n\ni\n\n,\n\n(cid:88)\n\nij , U (l−1)(cid:17) h(l)\n\nvi∈V (r)\n\neij ∈E(r)\n\n(4) Update the compound representation:\n\nU (l) = U (l−1) + MLP\n\n\n\n\n\n1 |V |\n\n|V | (cid:88)\n\ni=1\n\nh(l)\n\ni\n\n,\n\n1 |E|\n\n(cid:88)\n\ni,j\n\nh(l) ij ,\n\n1 |R|\n\n(cid:88)\n\nr∈R\n\n\n\nh(l)\n\nr , U (l−1)\n\n .\n\n(4)\n\n(5)\n\nAfter stacking L i.e., hG = 1 tion tasks, we can add a classification head to h(L)\n\nO (cid:80)|V | i=1 h(L)\n\n|V |\n\ni\n\n.\n\n-GNN layers, we get the graph representation by a simple average pooling layer, , which could be utilized by graph classification tasks. For node classifica-\n\ni\n\n2.3 THEORETICAL ANALYSIS\n\nO\n\nIn this section, we compare the distinguishability between standard GNN (without ring representations) and -GNN. In addition to the notations defined in Section 2.1, we define the valued version of a graph G = (V, E) as a triplet VALUEf (G) = (V, E, f ), where f is a mapping storing feature information and mapping a node or an edge to its corresponding input feature (e.g., a 256-dimension representation). We call f as a feature mapping on G. Definition 1 (k-neighbourhood node). For a molecular graph G = (V, E) and two nodes u, v V , we say u is a k-neighbourhood of v if there exists a path in G connecting u and v with length no larger than k. More formally, u is a k-neighbourhood of v if and only if there exists a set of nodes ,\n} vi+1\n\nk, v0 = v, vt = u, and for any i\n\nV , such that, t\n\n} ⊂\n\n∈ {\n\n, vt\n\n· · ·\n\n· · ·\n\n, t\n\n≤\n\n−\n\n0,\n\n∈\n\n1\n\nv0, v1, {\nN (vi). ∈\n\n≥\n\n0) of itself.\n\nWe highlight here that v is a 0-neighbourhood node (and thus a k-neighbourhood node with any k\nDefinition 2 (k-neighbourhood sub-graph). For a molecular graph G = (V, E) and a node v in G, we define the k-neighbourhood sub-graph of v as the sub-graph composed of all v’s k-neighbourhood node. More formally, we slightly abuse the notations and denote the kneighbourhood sub-graph of v as G(v, k) ≜ (V (v, k), E(v, k)), where\n\nV (v, k) ≜\n\nu E(v, k) ≜\n\n{\n\n∈\n\nV : u is a k-neighbourhood node of v\n\ne(v1, v2)\n\nE : v1, v2\n\nV (v, k)\n\n, }\n\n. }\n\n∈ Definition 3 (Equivalent valued graph). For two valued graphs VALUEf1 (G1) = (V1, E1, f1) and VALUEf2(G2) = (V2, E2, f2), we say that they are equivalent if (i). G1 and G2 are isomorphic, i.e., there exists a one-to-one mapping also preserves the value of edges and the the value of nodes, i.e.,\n\nV2, such that the edges are preserved; (ii). G1,\n\n: V1\n\nu, v\n\n→\n\nP\n\nP\n\n∈\n\n{\n\nf1(u) = f2(\n\nP\n\ne(u, v)\n\nE1\n\n∈\n\n(u)), f1(v) = f2(\n\n⇔ P\n\n∀ (v)) e( (v)), f1(e(u, v)) = f2(e(\n\n∈ E2,\n\n(u),\n\nP\n\nP\n\n∈\n\n(u),\n\nP\n\n(v))).\n\nP\n\nWith all the preparations above, we are now ready to define the graph feature extractor and its discriminatory ability. Definition 4 (Graph feature extractor and its discriminatory ability). We say a mapping Φ is a graph feature extractor, if it maps a valued graph VALUEf (G) to a new feature mapping ̃f on G. We further allow Φ to be parameterized as Φθ, and call Φθ a parameterized graph feature extractor.\n\nFor a parameterized graph feature extractor Φθ, we say Φθ has the discriminatory ability for kneighbourhood sub-graphs, if for any valued graphs (G, f ) and any two nodes u, v in G, if the valued k-neighbourhood sub-graph of u and v (i.e., (G(u, k), f ) and (G(v, k), f )) are equivalent, there exists θ⋆ such that Φθ⋆ ((G, f ))(u) = Φθ⋆ ((G, f ))(v). In this case, we also say that Φθ⋆ can distinguish u and v.\n\n4\n\n̸ Published as a conference paper at ICLR 2023\n\nWe point out that i } and thus above provides a formal definition of\n\ni,j}\n\n∪ {\n\ni\n\nh(l) {\n\nh(l)\n\ni,j defined by Eqn. (2, 3, 4, 5) is a parameterized feature extractor,\n\n-GNN’s discriminatory ability.\n\nO\n\nThe next proposition shows that without the ring representation, the layer to have the has the discriminatory power for k-neighbourhood sub-graphs. Proposition 1. Without the ring presentation, discriminatory ability for k-neighbourhood sub-graphs.\n\nO\n\nO\n\n-GNN with no more than k layers does not have the\n\n-GNN needs at least k + 1\n\nO\n\nNote that Proposition 1 can be easily extended to the conventional graph convolutional neural networks, which only aggregate information from 1-neighborhood nodes. We then show that with the ring representations,\n\n-GNN with only one layer has the discriminatory power.\n\nProposition 2. If u and v lie on different rings,\n\n-GNN with only one layer can distinguish them.\n\nO\n\nO The proofs are deferred to Appendix B due to space limitation. From Proposition 1 and 2, we can see that -GNN is more expressive than the regular GNN that does not model rings. The regular GNN requires at least k layers to distinguish two isomorphic k-neighborhood sub-graphs on different rings, while -GNN only requires one layer for this purpose (see the example in Figure 2). Comparing -GNN to a regular GNN with the same number of layers, modeling ring presentations constantly increases the percentages of parameters (irrelevant to k). However, a regular GNN may require k layers to achieve the discriminatory power for k-neighborhood sub-graphs. When k is large,\n\n-GNN will be much more parameter efficient. More discussions are in Appendix C.5.\n\nO\n\nO\n\nO\n\n3 EXPERIMENTS\n\nTo validate the effectiveness of our method, we test -GNN on the following three tasks: molecular property prediction, drug-drug interaction prediction and retrosynthesis. The first two tasks are graph classification tasks, and the third one is a node/link prediction task.\n\nO\n\n3.1 APPLICATION TO MOLECULAR PROPERTY PREDICTION\n\nDatasets. We work on three datasets for this application:\n\n(1) The HOMO-LUMO energy gap prediction of the PCQM4Mv1 dataset (Hu et al., 2021). The input is a 2D molecular graph, and the target is its HOMO-LUMO energy gap, which is an essential molecular property in quantum chemistry. PCQM4Mv1 has 3045360 and 380670 training and validation data (test labels are not available). The properties are obtained via density function theory.\n\n(2) Molecular property prediction on MoleculeNet dataset (Wu et al., 2018). This is a dataset about the prediction of pharmaceutical properties of small molecules. We choose six molecular property prediction tasks (including BBBP, Tox21, ClinTox, HIV, BACE and SIDER), and the data ranges from 1.5k to 41k.\n\n(3) Few-shot molecular property prediction of the FS-Mol dataset (Stanley et al., 2021). FS-Mol is made up of 5120 separate assays extracted from ChEMBL27 (https://www.ebi.ac.uk/ chembl/). Each assay has 94 molecular-property pairs on average.\n\nTraining configuration. For PCQM4Mv1, we set the number of layers as 12 and hidden dimension as 256, which is selected by the cross-validation method on the training set. For FS-Mol, the number of layers are 6 and the hidden dimension is 256. The candidate number of layers and hidden dimensions for MoleculeNet are . On FS-Mol and MoleculetNet, }\nthe hyper-parameters are selected according to validation performance. We train all these tasks on one GPU. The optimizer is AdamW (Loshchilov & Hutter, 2019). More detailed parameters are summarized in Table 5 of Appendix A.\n\n4, 6, 8, 12\n\n128, 256\n\nand\n\n}\n\n{\n\n{\n\nResults on PCQM4Mv1 The results of PCQM4Mv1 are reported in Table 1. We compare -\nO GNN with the following baselines: (1) Conventional GCN/GIN with/without virtual node (marked by “vn”). The results are from Hu et al. (2021); (2) ConfDSS (Liu et al., 2021), which predicts quantum properties conditioned on low-cost conformer sets; (3) Two-branch Transformer (Xia et al., 2021), which has a regression head and a classification head that learn from each other; (4) Graphormer (Ying et al., 2021; Shi et al., 2022), the champion solution of PCQM4Mv1. Since\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nGCN GCN + vn GIN GIN + vn ConfDSS Two-branch Transformer Graphormerbase Graphormerlarge -GNN (ours)\n\nO\n\n)\n\nMAE (\n\n↓ 0.1684 0.1510 0.1536 0.1396 0.1278 0.1237 0.1193 0.1231 0.1148\n\nTable 1: Validation MAE on PCQM4Mv1.\n\nFigure 4: MAE w.r.t. numbers of layers.\n\n(a) Number of rings.\n\n(b) Number of atoms lying on rings.\n\n(c) Maximum ring size.\n\nFigure 5: Performance improvement over several ring properties.\n\nO\n\nthe owners of PCQM4Mv1 did not release labels of the test set, we can only compare the results on the validation set. The evaluation metric is the mean absolute error (MAE). From Table 1, we -GNN achieves the best results among the strong baseline models, which shows the can see that effectiveness of our method. In addition, GIN vn, ConfDSS and Graphormer do not explicitly use the ring information, and we will combine\n\n-GNN with the strong methods in the future.\n\nO\n\nO\n\nO\n\n-GNN and “\n\n-GNN by removing the To investigate the significance of the ring information, we study a variant of -GNN w/o ring”. Specifically, -GNN, and denote this variant as “ ring modeling component from it is implemented by removing Eqn.(4) and all the hr’s in Eqn.(2,3,5). We conduct experiments for -GNN w/o ring” from 2 to 14 layers. The results are in Figure 4. We can see that O\nby utilizing ring information, the performance is boosted regardless of the number of layers. In -GNN w/o ring, which addition, we find that a 6-layer O\n-GNN shows the great power of modeling rings in GNN. We also have that It is noteworthy to point out that w/o ring” in terms of the number of parameters (see Figure 10). validation MAE of the 14-layer -GNN. Note that this phenomenon is also observed in Graphormer (Shi et al., 2022) that larger models do not always lead to better validation results. We will explore how to train deeper models in the future.\n\n-GNN slightly drops compared to the 12-layer\n\n-GNN is comparable with the 12-layer\n\n-GNN outperforms “\n\nO\n\nO\n\nO\n\nO\n\nO\n\nO\n\nO\n\nO\n\n-GNN w/o rings” and\n\nOn PCQM4Mv1, we also study the average performance improvement w.r.t. several ring properties. The performance improvement is defined as ε1 ε2, where ε1 and ε2 denote the validation MAE of “\n-GNN. The ring properties include: (i) the number of rings in a molecule; (ii) the number of atoms lying on rings; (iii) the number of atoms in the largest ring. We conduct experiments for the networks with different numbers of layers (L = 2, 6, 12). Results are reported in Figure 5. We can conclude that overall, as the increase of number of rings, maximum ring sizes and the number of atoms lying on rings, -GNN achieves more improvement compared to the variant without modeling rings. More analyses are in Appendix C.4.\n\nO\n\nO\n\n−\n\nResults on MoleculeNet For MoleculeNet, we compare with both pretraining and non-pretraining methods. For non-pretraining methods, we compare with the following baselines: (i) GCN (Kipf & Welling, 2017) with virtual node ; (ii) GIN (Xu et al., 2018) with virtual node; (iii) -GNN -GNN w/o ring”). For pre-training methods, we select without using ring information (denoted as “\n\nO\n\nO\n\n6\n\n2468101214Numberoflayers0.1150.1200.1250.130MAEO-GNNw/oringO-GNN0246Numberofrings0.000.010.020.030.040.05MeanpredictionimprovementL=2L=6L=12[0,3)[3,6)[6,9)[9,12)[12,15)[15,18)Numberofatomslyingonrings0.0000.0050.0100.0150.020MeanpredictionimprovementL=2L=6L=12[0,3)[3,6)[6,9)>=9Numberofatomsinthemaxring0.000.010.020.03MeanpredictionimprovementL=2L=6L=12Published as a conference paper at ICLR 2023\n\nDataset # Molecules\n\n(Hu et al., 2020) G-Contextual (Liu et al., 2022) G-Motif (Liu et al., 2022) GraphMVP (Liu et al., 2022)\n\nGCN + vn GIN + vn O-GNN w/o ring\n\nO-GNN (ours)\n\nBBBP 2039\n\nTox21 7831\n\n71.2 ± 0.9 70.3 ± 1.6 66.4 ± 3.4 72.4 ± 1.6\n\n72.7 ± 1.3 71.7 ± 0.6 74.5 ± 1.4\n\n74.2 ± 0.8 75.2 ± 0.3 73.2 ± 0.8 75.9 ± 0.5\n\n75.0 ± 0.4 74.8 ± 0.6 75.2 ± 0.9\n\nClinTox 1478\n\n73.7 ± 4.0 59.9 ± 8.2 77.8 ± 2.0 79.1 ± 2.8\n\n92.0 ± 1.1 89.4 ± 3.2 90.2 ± 2.1\n\nHIV 41127\n\n75.8 ± 1.1 75.9 ± 0.9 73.8 ± 1.4 77.0 ± 1.2\n\n78.8 ± 1.1 79.3 ± 1.0 80.5 ± 1.0\n\nBACE 1513\n\n78.6 ± 1.4 79.2 ± 0.3 73.4 ± 4.0 81.2 ± 0.9\n\n80.0 ± 0.8 82.0 ± 1.0 84.2 ± 1.5\n\nSIDER 1478\n\n60.4 ± 0.6 58.4 ± 0.6 60.6 ± 1.1 63.9 ± 1.2\n\n62.9 ± 1.3 60.8 ± 0.8 65.5 ± 1.6\n\n76.4 ± 0.4\n\n75.7 ± 0.7\n\n94.3 ± 1.6\n\n81.3 ± 1.2\n\n85.8 ± 1.0\n\n66.2 ± 1.2\n\nTable 2: Test ROC-AUC (%) performance of different methods on 6 binary classification tasks from MoleculeNet benchmark. The training, validation and test sets are provided by DeepChem. Each experiment is independently run for three times. The mean and standard derivation are reported.\n\nContextual, Motif\n\nseveral representative graph-based methods: (i) Hu et al. (2020) proposed to predict the masked attributes on graphs as well as maintaining the consistency between a subgraph and its neighbors; (ii) Gare variants of (Rong et al., 2020), which are provided in Liu et al. (2022). (iii) GraphMVP (Liu et al., 2022), which is a joint pre-training between 2D molecules and its 3D conformation. The results of (Hu et al., 2020), Gand GraphMVP are all extracted from Liu et al. (2022), since Liu et al. (2022) use the same scaffold-based splitting as us.\n\nContextual, Motif\n\n{\n\n}\n\n}\n\n{\n\nThe results are reported in Table 2. We can see that: (i) -GNN outperforms the conventional network architectures like GIN and GCN with virtual nodes, which demonstrate the effectiveness of our new architecture; (ii) When comparing with G- , GraphMVP (Liu et al., 2022) and Hu et al. (2020) which are all pre-training methods, -GNN still outperforms those methods. (More discussion about pre-training methods are left in Table 11 of Appendix C.5) This -GNN and we will combine it with pre-training in the future. (iii) shows the great potential of Comparing -GNN w/o ring, the average improvement overall the six tasks is 1.6. This shows the advantage of using ring information in molecular property prediction.\n\nContextual, Motif\n\n-GNN and\n\nO\n\nO\n\nO\n\nO\n\nO\n\n}\n\n{\n\nResults on FS-Mol Stanley et al. (2021) verify that prototypical networks (PN) performs the best on FS-Mol compared with other methods like MAML (Finn et al., 2017), multi-task learning (MT) and random forest (RF). Stanley et al. (2021) use a Transformer-like residual network for few-shot classifi- -GNN cation. We replace that backbone to our w/o ring”, and the other parts remain unchanged. Following Stanley et al. (2021), the results with different support set sizes (denoted as ) are reported. A support set consists |\nof a few examples with input-label pairs used to train models. The evaluation metric is ∆AUPRC, which is the difference between the AUPRC (area under the precision-recall curve) and the ratio of the active compounds in that query set. A higher ∆AUPRC score indicates better classification performance of the model.\n\n-GNN and “\n\nu,support\n\n|T\n\nO\n\nO\n\nFigure 6: Results on FS-Mol.\n\nThe results are in Figure 6. We report the mean and the standard derivations for different tasks across various support sizes. We have the following observations: (i) By using -GNN as the backbone model for the prototypical network, the results are boosted for different support set sizes. (ii) The improvement is more significant when the support set size is large. When =\n128 and 256, the improvements are 0.014 and 0.016. When reducing the sizes to 16/32/64, their improvements are all around 0.008. We will further improve the results on limited data size in the future.\n\nu,support\n\n|T\n\nO\n\n|\n\n3.2 APPLICATION TO DDI PREDICTION\n\nDrug-drug interaction (DDI) prediction is to predict therapeutic output of taking two drugs together, like increasing the risk of some side effects, or the effect is enhanced to take them together. We\n\n7\n\n163264128256|Tu,support|0.100.150.200.250.30∆AUPRCO-GNNO-GNNw/oringPNGNN-MAMLGNN-MTRFPublished as a conference paper at ICLR 2023\n\nfocus on the classification task, where the inputs are two drug molecules and one interaction (e.g., inhibition), and the output is 0 or 1 to indicate whether the two drugs have this specific interaction. Following Nyamabo et al. (2022) and Li et al. (2022), we work on the inductive setting of the DrugBank dataset (Wishart et al., 2018), which has 1, 706 drugs, 86 interaction types, and 191, 808 triplets. To test the generalization ability of the model, we conduct experiments on two settings w.r.t. the drugs: the S1 setting, where neither of the two drugs on the test set appears in the training set; the S2 setting, where one drug is seen in the training set and the other is not. Noting that the drug pairs in the test set do not appear in the training set. Hence, the DrugBank data is split into training and test sets by the visibility of the drugs, and the negative samples are offline generated. We directly use the data provided by Nyamabo et al. (2021; 2022), where 20% drugs are first hold as unseen drugs for formulating test set and the rest 80% drugs are used to create the training set.\n\nMethod\n\nS2 setting (1 known drug + 1 unknown drug) ACC AUROC\n\nAP\n\nF1\n\nGAT-DDI (Nyamabo et al., 2021) MHCADDI (Deac et al., 2019) MR-GNN (Xu et al., 2019) SSI-DDI (Nyamabo et al., 2021) GMPNN (Nyamabo et al., 2022) MSAN-GCN (Zhu et al., 2022) MSN-DDI (Li et al., 2022) O-GNN w/o ring\n\nO-GNN (ours)\n\n69.83 70.58 74.67 76.38 77.72 77.81 81.92 87.72\n\n88.47\n\n77.29 77.84 83.15 84.23 84.84 85.74 91.01 94.51\n\n95.87\n\n75.79 76.16 83.81 84.94 84.87 –\n91.09 95.28\n\n96.51\n\n73.01 72.74 69.88 73.54 78.29 76.48 80.18 85.91\n\n86.91\n\nS1 setting (2 unknown drugs)\n\nACC AUROC\n\nAP\n\nF1\n\n62.63 65.40 66.50 66.31 68.57 69.17 73.42 75.47\n\n76.81\n\n70.92 73.43 72.53 72.75 74.96 76.12 81.79 83.83\n\n87.64\n\n73.01 75.03 71.06 71.61 75.44 –\n81.82 85.58\n\n45.81 54.12 67.21 68.68 65.32 67.10 70.34 65.59\n\n88.70\n\n70.81\n\nTable 3: Results of drug-drug interaction prediction on DrugBank.\n\nTo predict the interaction between two drugs, we use one 6-layer -GNN to extract features for the two drugs. Specifically, for each drug, we average the node representations output by the last layer as the drug feature. We concatenate the two drug features together, and then multiply the interaction embedding to do the prediction. The detailed parameters are left in Table 6 of Appendix A.\n\nO\n\nThe results are reported in Table 3. -GNN significantly outperforms previous baselines in terms of accuracy (denoted as ACC), the area under the receiver operating characteristic (AUROC), the average precision (AP), and the F1 score. Most of previous works use GCN, GIN or GAT backbones, and they focus on designing comprehensive interaction module (Nyamabo et al., 2021; Li et al., 2022). By using the advanced -GNN backbone, we can significantly improve the results without designing complex interaction modules. This shows the effectiveness of our method.\n\nO\n\nO\n\n3.3 APPLICATION TO RETROSYNTHESIS\n\nO\n\nRetrosynthesis is to predict the reactants of a given product. Various GNNs have been applied to this task. For example, GLN (Dai et al., 2019) use GNNs to predict the distributions of candidate reaction templates and reactants. GraphRetro (Somnath et al., 2021) and G2G (Shi et al., 2020) use GNNs to predict where to break a bond and how to add the fragments to complete the synthons. To demonstrate the ability of our -GNN, we combine our method with LocalRetro (Chen & Jung, 2021), the current best graph-based model for retrosynthesis (without using pre-training). LocalRetro uses GNN to predict the possible templates for each atom and each bond, and sort the predicted templates according to their probabilities. The top templates will be applied to the corresponding atoms or bonds via RDKit (Landrum et al., 2016) to generate the reactants. Chen & Jung (2021) use MPNN (Gilmer et al., 2017a) for prediction, and we replace the MPNN with We conduct experiments on the USPTO-50k dataset (Coley et al., 2017) that contains 50, 016 reactions. Following Chen & Jung (2021), we partition the dataset as 45k training set, 5k validation set and 5k test set. The evaluation metric is the the top-k accuracy, where k = 1, 3, 5, 10, 50. The results are summarized in Table 4. We can observe that -GNN can predict reactions more accurately than the baselines without ring information. Especially, when the reaction type is known, we improve the top-1 accuracy for 1.8 points and the top-3 accuracy for 1.6 points. These results show the importance of modeling ring structure and the effectiveness of our method.\n\n-GNN.\n\nO\n\nO\n\nThe performance for different number of rings. To study the prediction performance of molecules with different number of rings, we group the USPTO-50k test set by the number of rings in the product molecules and compute the top-1 accuracy for each group. More specifically, we divide\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nTop-1 Top-3\n\nTop-5 Top-10 Top-50\n\nReaction type unknown\n\nReaction type known Top-1 Top-3 Top-5 Top-10 Top-50\n\nG2G GLN GraphRetro LocalRetro\n\nO-GNN (ours)\n\n48.9 52.5 53.7 53.4\n\n54.1\n\n67.6 69.0 68.3 77.5\n\n77.7\n\n72.5 75.6 72.2 85.9\n\n86.0\n\n75.5 83.7 75.5 92.4\n\n92.5\n\n– 92.4 –\n97.7\n\n98.2\n\n61.0 64.2 63.9 63.9\n\n65.7\n\n81.3 79.1 81.5 86.1\n\n87.7\n\n86.0 85.2 85.2 92.4\n\n93.4\n\n88.7 90.0 88.1 96.3\n\n96.9\n\n– 93.2 –\n97.9\n\n98.3\n\nTable 4: Results on USPTO-50k datasets with reaction type known/unknown.\n\n(a) Accuracy w.r.t #rings.\n\n(b) Case study on molecule with complex rings.\n\nFigure 7: Study of -GNN on retrosynthesis task. (a) The top-1 accuracy w.r.t. number of rings in product molecules. (b) The one-step retrosynthesis prediction of a product molecule with five rings. The first\n\n-GNN output is the same as the ground truth (marked as green).\n\nO\n\nO\n\nthe test set into four groups with ring numbers [0, 2), [2, 4), [4, 6), [6, 12], and those groups have 808, 2347, 1617, 235 reactions, respectively. The results are in Figure 7(a) where the blue bars represent the LocalRetro baseline and the green bars represent -\nO GNN has better accuracy on all groups, where the improvements are 0.99, 0.85, 1.30 and 5.96. Overall speaking, the improvement is larger when there are more rings in a molecule. Especially, -GNN increases the accuracy for when there are at least 6 rings in a group (i.e., the last column), 5.96 points, demonstrating that our method can better leverage ring structures.\n\n-GNN. The results show that\n\nO\n\nO\n\nCase study. In Figure 7(b), we show an example prediction of a product molecule with 5 rings. The reactions in the left panel are the top-3 predictions from LocalRetro baseline and the ones on the right -GNN. Our method successfully predicts the correct reactants in its first output (marked are from as green), but the baseline fails to give a correct prediction. More importantly, the baseline system even fails to identify the correct bond to change. These results suggest that modeling ring structures is crucial to predict reactions accurately, and\n\n-GNN is an effective algorithm for retrosynthesis.\n\nO\n\nO\n\n4 CONCLUSIONS AND FUTURE WORK\n\nO\n\nIn this work, we propose a new model, ring-enhanced GNN (briefly, -GNN) for molecular modeling. We explicitly incorporate the ring representations into GNN and jointly update them with -GNN and prove that by using atom and bond representations. We provide theoretical analysis to -GNN, the node representations are more distinguishable than the variant without using ring repreO sentations. We conduct experiments on molecular property prediction, drug-drug interaction (DDI) prediction and retrosynthesis. -GNN outperforms strong baselines on these tasks and achieves state-of-the-art results on the validation performance of PCQM4Mv1 and DDI prediction. For fu- -GNN. Second, we need ture work, first, we will combine with pre-training to obtain a stronger to further improve our model when the training data is very limited (e.g., when the support set size is 16 or fewer). Third, how to efficiently identify and incorporate the representations with more complex structures is another interesting direction to explore. Fourth, we will apply our model to more real world scenarios, like the synthesis and generation of natural products with large rings.\n\nO\n\nO\n\nO\n\n9\n\n[0,2)[2,4)[4,6)[6,12]#Rings50.052.555.057.560.062.565.067.570.0Accuracy[%]LocalRetroO-GNNLocalRetro: output 1LocalRetro: output 2LocalRetro: output 3O-GNN: output 1 (ground truth)O-GNN: output 2O-GNN: output 3Published as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was supported in part by NSFC under Contract 61836011, and in part by the Fundamental Research Funds for the Central Universities under contract WK3490000007.\n\nREFERENCES\n\nRavichandra Addanki, Peter Battaglia, David Budden, Andreea Deac, Jonathan Godwin, Thomas Keck, Wai Lok Sibon Li, Alvaro Sanchez-Gonzalez, Jacklynn Stott, Shantanu Thakoor, and Petar Veliˇckovi ́c. Large-scale graph representation learning with very deep gnns and self-supervision. arXiv preprint arXiv:2107.09422, 2021.\n\nGaurav Bhardwaj, Jacob O’Connor, Stephen Rettie, Yen-Hua Huang, Theresa A. Ramelot, Vikram Khipple Mulligan, Gizem Gokce Alpkilic, Jonathan Palmer, Asim K. Bera, Matthew J. Bick, Maddalena Di Piazza, Xinting Li, Parisa Hosseinzadeh, Timothy W. Craven, Roberto Tejero, Anna Lauko, Ryan Choi, Calina Glynn, Linlin Dong, Robert Griffin, Wesley C. van Voorhis, Jose Rodriguez, Lance Stewart, Gaetano T. Montelione, David Craik, and David Baker. Accurate de novo design of membrane-traversing macrocycles. Cell, 185(19):3520– 3532.e26, 2022. ISSN 0092-8674. doi: https://doi.org/10.1016/j.cell.2022.07.019. URL https: //www.sciencedirect.com/science/article/pii/S0092867422009229.\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks?\n\narXiv\n\npreprint arXiv:2105.14491, 2021.\n\nShuan Chen and Yousung Jung. Deep retrosynthetic reaction prediction using local reactivity and\n\nglobal attention. JACS Au, 1(10):1612–1620, 2021. doi: 10.1021/jacsau.1c00246.\n\nConnor W. Coley, Luke Rogers, William H. Green, and Klavs F. Jensen. Computer-Assisted Retrosynthesis Based on Molecular Similarity. ACS Central Science, 3(12):1237–1245, DecemISSN 2374-7943, 2374-7951. doi: 10.1021/acscentsci.7b00355. URL https: ber 2017. //pubs.acs.org/doi/10.1021/acscentsci.7b00355.\n\nWeilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=r-oRRT-ElX.\n\nHanjun Dai, Chengtao Li, Connor Coley, Bo Dai, and Le Song. Retrosynthesis prediction with In Advances in Neural Information Processing Systems, pp.\n\nconditional graph logic network. 8870–8880, 2019.\n\nAndreea Deac, Yu-Hsiang Huang, Petar Veliˇckovi ́c, Pietro Li`o, and Jian Tang. Drug-drug adverse\n\neffect prediction with graph co-attention. arXiv preprint arXiv:1905.00534, 2019.\n\nJ ̈org Degen, Christof Wegscheid-Gerlach, Andrea Zaliani, and Matthias Rarey. On the art of compiling and using’drug-like’chemical fragment spaces. ChemMedChem: Chemistry Enabling Drug Discovery, 3(10):1503–1507, 2008.\n\nXiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127–134, 2022.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1126–1135. PMLR, 06–11 Aug 2017.\n\nJustin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry, 2017a. URL https://arxiv.org/abs/1704. 01212.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017b.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017a. URL https://proceedings.neurips.cc/paper/2017/file/ 5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nAdvances in neural information processing systems, 30, 2017b.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure In International ConferLeskovec. ence on Learning Representations, 2020. URL https://openreview.net/forum?id= HJlWWJSFDH.\n\nStrategies for pre-training graph neural networks.\n\nWeihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.\n\nNirmala Chandralega Kampan, Mutsa Tatenda Madondo, Orla M McNally, Michael Quinn, and Magdalena Plebanski. Paclitaxel and its evolving role in the management of ovarian cancer. Biomed Res. Int., 2015:413076, June 2015.\n\nSunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Leonid Zaslavsky, Jian Zhang, and Evan E Bolton. PubChem 2019 update: improved access to chemical data. Nucleic Acids Res., 47(D1):D1102– D1109, January 2019.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. ICLR, 2017.\n\nGreg Landrum et al. Rdkit: Open-source cheminformatics software, 2016. URL http://www. rdkit.\n\norg/, https://github. com/rdkit/rdkit, 149:150, 2016.\n\nGuohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train\n\ndeeper gcns, 2020.\n\nJunying Li, Deng Cai, and Xiaofei He. Learning graph-level representation for drug discovery. arXiv\n\npreprint arXiv:1709.03741, 2017.\n\nZimeng Li, Shichao Zhu, Bin Shao, Tie-Yan Liu, Xiangxiang Zeng, and Tong Wang. Multi-view substructure learning for drug-drug interaction prediction, 2022. URL https://arxiv.org/ abs/2203.14513.\n\nMeng Liu, Cong Fu, Xuan Zhang, Limei Wang, Yaochen Xie, Hao Yuan, Youzhi Luo, Zhao Xu, Shenglong Xu, and Shuiwang Ji. Fast quantum property prediction via deeper 2d and 3d graph networks, 2021. URL https://ogb.stanford.edu/paper/kddcup2021/pcqm4m_ DIVE.pdf.\n\nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. In International ConferPre-training molecular graph representation with 3d geometry. ence on Learning Representations, 2022. URL https://openreview.net/forum?id= xQUe1pOKPam.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7.\n\nAndreas Loukas. What graph neural networks cannot learn: depth vs width. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id= B1l2bp4YwS.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nArnold K Nyamabo, Hui Yu, and Jian-Yu Shi. Ssi–ddi: substructure–substructure interactions for\n\ndrug–drug interaction prediction. Briefings in Bioinformatics, 2021.\n\nArnold K Nyamabo, Hui Yu, Zun Liu, and Jian-Yu Shi. Drug–drug interaction prediction with\n\nlearnable size-adaptive molecular substructures. Briefings in Bioinformatics, 2022.\n\nTrang Pham, Truyen Tran, Hoa Dam, and Svetha Venkatesh. Graph classification via deep learning\n\nwith virtual nodes. arXiv preprint arXiv:1708.04357, 2017.\n\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33:12559–12571, 2020.\n\nSisi Shan, Shitong Luo, Ziqing Yang, Junxian Hong, Yufeng Su, Fan Ding, Lili Fu, Chenyu Li, Peng Chen, Jianzhu Ma, Xuanling Shi, Qi Zhang, Bonnie Berger, Linqi Zhang, and Jian Peng. Deep learning guided optimization of human antibody against sars-cov-2 variants with broad neutralization. Proceedings of the National Academy of Sciences, 119(11), 2022. doi: 10.1073/ pnas.2122954119.\n\nChence Shi, Minkai Xu, Hongyu Guo, Ming Zhang, and Jian Tang. A graph to graphs framework for retrosynthesis prediction. In Hal Daum ́e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8818–8827. PMLR, 13–18 Jul 2020.\n\nYu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie Luo, Chang Liu, Di He, and Tie-Yan Liu. Benchmarking graphormer on large-scale molecular modeling datasets, 2022. URL https://arxiv.org/abs/2203.04810.\n\nVignesh Ram Somnath, Charlotte Bunne, Connor Coley, Andreas Krause, and Regina Barzilay. Learning graph models for retrosynthesis prediction. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 9405–9415. Curran Associates, Inc., 2021.\n\nMegan Stanley, John F Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, Marwin FS-mol: A few-shot learning dataset Segler, Nadine Schneider, and Marc Brockschmidt. In Thirty-fifth Conference on Neural Information Processing Systems Datasets of molecules. and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id= 701FtuyLlAd.\n\nJonathan M. Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M. Donghia, Craig R. MacNair, Shawn French, Lindsey A. Carfrae, Zohar Bloom-Ackermann, Victoria M. Tran, Anush Chiappino-Pepe, Ahmed H. Badran, Ian W. Andrews, Emma J. Chory, George M. Church, Eric D. Brown, Tommi S. Jaakkola, Regina Barzilay, and James J. Collins. A deep learning approach to antibiotic discovery. Cell, 180(4):688–702.e13, 2020. ISSN 0092-8674. doi: https://doi.org/10.1016/j.cell.2020.01.021. URL https://www.sciencedirect. com/science/article/pii/S0092867420301021.\n\nHongmao Sun, Gregory Tawa, and Anders Wallqvist. Classification of scaffold-hopping approaches.\n\nDrug Discov. Today, 17(7-8):310–324, April 2012.\n\nRuoxi Sun, Hanjun Dai, and Adams Wei Yu. Does GNN pretraining help molecular representation? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=uytgM9N0vlR.\n\nWen Torng and Russ B. Altman. Graph convolutional neural networks for predicting drug-target interactions. Journal of Chemical Information and Modeling, 59(10):4131–4149, 2019. doi: 10.1021/acs.jcim.9b00628.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAnilrudh A. Venugopal and Stuart Johnson. Fidaxomicin: A novel macrocyclic antibiotic approved for treatment of clostridium difficile infection. Clinical Infectious Diseases, 54(4):568–574, 12 2011. ISSN 1058-4838. doi: 10.1093/cid/cir830. URL https://doi.org/10.1093/cid/ cir830.\n\nDavid S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, et al. Drugbank 5.0: a major update to the drugbank database for 2018. Nucleic acids research, 46(D1):D1074–D1082, 2018.\n\nZhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical Science, 9:513–530, 2018. doi: 10.1039/C7SC02664A. URL http: //dx.doi.org/10.1039/C7SC02664A.\n\nYingce Xia, Jinhua Zhu, Lijun Wu, Yang Fan, Shufang Xie, Yutai Hou, and Tao Qin. When transformer meets graph neural networks. 2021. URL https://ogb.stanford.edu/paper/ kddcup2021/pcqm4m_GNNLearner.pdf.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? arXiv preprint arXiv:1810.00826, 2018.\n\nNuo Xu, Pinghui Wang, Long Chen, Jing Tao, and Junzhou Zhao. Mr-gnn: Multi-resolution arXiv preprint\n\nand dual graph neural network for predicting structured entity interactions. arXiv:1905.09558, 2019.\n\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In Advances in Neural Information Processing Systems, volume 34, pp. 28877–28888, 2021.\n\nZaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph selfsupervised learning for molecular property prediction. Advances in Neural Information Processing Systems, 34:15870–15882, 2021.\n\nXinyu Zhu, Yongliang Shen, and Weiming Lu. Molecular substructure-aware network for drug-drug\n\ninteraction prediction. CIKM, 2022.\n\nA DETAILED EXPERIMENT CONFIGURATIONS\n\nThe hyperparameters for the molecular property prediction, drug-drug interaction prediction and retrosynthesis are summarized in Table 5, Table 6 and Table 7 respectively.\n\nPCQM4Mv1\n\nFS-Mol\n\nMoleculeNet\n\nNumber of Layers Hidden dimension Optimizer Dropout Learning rate Training steps Batch size Weight Decay Learning Rate Decay\n\n12 256 AdamW 0.0 0.0003 300 epochs 512 0.1 Cosine\n\n{\n\n{\n\n6 256 AdamW 0.0, 0.1, 0.2 0.0001, 0.0002, 0.0003 10000 iterations 16 0.01, 0.1 Cosine\n\n}\n\n{\n\n}\n\n}\n\n{\n\n{ {\n\n4, 6, 8, 12 128, 256 AdamW 0.0, 0.1, 0.2, 0.3, 0.5 0.00005,0.0001,0.0002,0.0005\n\n} }\n\n}\n\n{\n\n{\n\n50,100\n\nepochs\n\n{\n\n} 32,64 0.01 Linear\n\n}\n\n}\n\nTable 5: Detailed hyper-parameters for molecular property prediction tasks.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nNumber of Layers Hidden dimension Optimizer Dropout Learning rate Training steps Batch size Weight Decay Learning Rate Decay\n\n{\n\n6 512 AdamW 0.2, 0.5 0.0003 100 epochs 128 0.01 Cosine\n\n}\n\nTable 6: Detailed hyper-parameters for drug-drug interaction prediction.\n\nNumber of Layers Hidden dimension Optimizer Dropout Learning rate Training steps Batch size Weight Decay Learning Rate Decay\n\n6 512 AdamW 0.1 0.0003 200 epochs 64 0.1 Cosine\n\nTable 7: Detailed hyper-parameters for retrosynthesis.\n\nB PROOFS OF THE TWO PROPOSITIONS\n\nProof of Proposition 1. We start the proof by explicitly writing down the ring-free variant of GNN.\n\n-\n\nO\n\nSpecifically, the bond representations are given by\n\nij = h(l−1) h(l)\n\nij\n\n+ MLP(h(l−1)\n\ni\n\n, h(l−1)\n\nj\n\n, h(l−1)\n\nij\n\n, U (l−1)).\n\nThe atom representations are given by\n\n ̄h(l)\n\ni =\n\n(cid:88)\n\nj∈N (i)\n\nαjWvconcat(h(l)\n\nij , h(l−1)\n\nj\n\n);\n\n∝\n\nαj i = h(l−1) h(l)\n\ni\n\nexp(a⊤LeakyReLU(Wqh(l−1) (cid:16)\n\ni\n\n+ MLP\n\nh(l−1)\n\n, ̄h(l)\n\ni\n\ni\n\n, U (l−1)(cid:17)\n\n.\n\n+ Wkconcat(h(l−1)\n\nj\n\n, h(l)\n\nij )));\n\nThe compound representations are given by\n\nU (l) = U (l−1) + MLP\n\n\n\n\n\n1 V\n|\n\n|V | (cid:88)\n\ni=1\n\n|\n\nh(l)\n\ni\n\n,\n\n1 E\n|\n\n|\n\n(cid:88)\n\ni,j\n\n\n\nh(l)\n\nij , U (l−1)\n\n .\n\nGiven the above notations, Proposition 1 can then be translated to the following claim:\n\n(6)\n\n(7)\n\nClaim. For any valued graphs (G, f ) and any two nodes va, vb in G, if the valued k-neighbourhood sub-graph of u and v (i.e., (G(va, k), f ) and (G(vb, k), f )) are equivalent, we have that hl b for any l\n\nholds regardless of the parameter.\n\na = hl\n\n, k\n\n1,\n\n∈ {\n\n· · ·\n\n}\n\nWe denote the equivalent mapping between (G(va, k), f ) and (G(vb, k), f ) as abuse the notations by letting\n\n(i) = j if\n\n(vi) = vj.\n\n. We will slightly\n\nP\n\nP\n\nP\n\nWe will prove the above claim by induction. Specifically, we will prove that for any l\n\nP(c1) for any vc1 ∈\n\nV (va, k\n\n−\n\nl), and hl\n\nc1c2 = hl\n\n∈ P(c1)P(c2) for\n\n0, 1,\n\n{\n\n· · ·\n\n}\n\n, k\n\n, we have hl\n\nany vc1, vc2 ∈\n\nV (va, k\n\n−\n\nc1 = hl l).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nBase case: for l = 0, by the definition of f , we have f (vc1) = f ( and f (vc1, vc2 ) = f ( c1 = f (vc1 ), h0 as h0\n\n(vc2)) for every vc1 , vc2 ∈ (vc1)), h0\n\nP c1c2 = f (vc1 , vc2), and h0 P(c1)P(c2) = f (\n\n(vc1)) for every vc1 ∈ V (va, k) V (va, k). The claim immediately follows\n\nP\n\n(vc1), P\nP(c1) = f ( P\n\n(c1),\n\nP\n\nP\n\n(c2)).\n\nInduction step: suppose the claim is true for l = i\n\nthat for every vc1, vc2 ∈\n\nV (va, k\n\nl),\n\n−\n\n0,\n\n∈ {\n\n, k\n\n1\n\n−\n\n· · ·\n\n. Then, for l = i + 1, we have\n\n}\n\nh(l)\n\nc1c2 =h(l−1) = h(l−1)\n\nc1c2 + MLP(h(l−1) P(c1)P(c2) + MLP(h(l−1)\n\n, h(l−1) c2\n\n(⋆)\n\nc1\n\n, h(l−1)\n\nc1c2 , U (l−1))\n\nP(c1), h(l−1)\n\nP(c2), h(l−1)\n\nP(c1)P(c2), U (l−1))\n\n=h(l)\n\nP(c1)P(c2),\n\n(8)\n\nV (va, k\n\nl)\n\n−\n\n⊂\n\nV (va, k\n\n(l\n\n−\n\n−\n\n1)).\n\nwhere Eq.(⋆) is due to the induction hypothesis, as vc1 , vc2 ∈ Similarly, for every vc1 ∈ ̄h(l)\n\nαjWvconcat(h(l)\n\nV (va, k\n\n(cid:88)\n\nl),\n\n−\n\nc1 =\n\nc1j, h(l−1)\n\nj\n\n)\n\nαjWvconcat(h(l)\n\nP(c1)P(j), h(l−1) P(j) )\n\nαP(j)Wvconcat(h(l)\n\nP(c1)P(j), h(l−1) P(j) )\n\nαjWvconcat(h(l)\n\nP(c1)j, h(l−1)\n\nj\n\n)\n\nj∈N (c1)\n\n(cid:88)\n\nj∈N (c1)\n\n(cid:88)\n\nj∈N (c1) (cid:88)\n\n(◦) =\n\n(⋄) =\n\n=\n\nj∈N (c1)\n\n= ̄h(l)\n\nP(c1),\n\nwhere Eq. (\n\n) is due to the induction hypothesis and Eq. (8). Eq. (\n\n) is due to\n\n⋄\n\nexp(a⊤LeakyReLU(Wqh(l−1) ∝\n= exp(a⊤LeakyReLU(Wqh(l−1)\n\nc1 + Wkconcat(h(l−1) P(c1) + Wkconcat(h(l−1)\n\n, h(l) P(j) , h(l)\n\nj\n\nc1j)))\n\nP(c1)P(j)))),\n\n◦\n\nαj\n\nand thus αj = αP(j) for any j\n\nWe then have\n\n(c1).\n\n∈ N\n\nh(l)\n\nc1 =h(l−1)\n\nc1 + MLP\n\n(cid:16)\n\nh(l−1) c1\n\n, ̄h(l)\n\nc1 , U (l−1)(cid:17) P(c1), U (l−1)(cid:17)\n\nP(c1), ̄h(l) h(l−1)\n\n=h(l−1)\n\nP(c1) + MLP\n\n(cid:16)\n\n=h(l)\n\nP(c1).\n\nThus, the claim holds for l = i + 1, and the proof for the induction claim completes. Thus, the claim is true for every l\n\n, k\n\n0,\n\n∈ {\n\n· · · For every l u\n∈ { and the proof is completed.\n\n· · ·\n\n, k\n\n0,\n\n}\n\n∈\n\n. }\nG(va, 0)\n\nG(va, k\n\n−\n\n⊂\n\nl). Therefore, we have h(l)\n\na = h(l)\n\nP(a) = h(l) b ,\n\nProof of Proposition 2. For two equivalent valued sub-graph (G(va, k), f ) and (G(vb, k), f ), if va and vb lie on different rings, we have\n\n1 R(va)\n\n|\n\n(cid:88)\n\nh(0)\n\nr\n\n=\n\n|\n\nr∈R(va)\n\n1 R(vb) |\n|\n\n(cid:88)\n\nh(0) r .\n\nr∈R(vb)\n\n15\n\n̸ Published as a conference paper at ICLR 2023\n\nTherefore, there exists a choice of MLP, such that\n\nh(1)\n\na =h(0)\n\na + MLP\n\n(cid:16)\n\nh(0)\n\na , ̄h(1) a ,\n\n1 R(va)\n\n|\n\n(cid:88)\n\nr , U (0)(cid:17) h(0)\n\n|\n\nr∈R(va)\n\n=h(0)\n\nb + MLP\n\n(cid:16)\n\nh(0)\n\nb\n\n, ̄h(1)\n\nb\n\n,\n\n1 R(vb) |\n\n|\n\n(cid:88)\n\nr , U (0)(cid:17) h(0)\n\nr∈R(vb)\n\n=h(1)\n\nb\n\n.\n\nThe proof is completed.\n\nC MORE ABLATION STUDY\n\nC.1 NODE REPRESENTATION POOLING V.S. COMPOUND REPRESENTATIONS\n\n(cid:80)|V |\n\ni=1 h(L)\n\n|V |\n\nWe explore the difference between using average pooling hG = 1 and the compound representation U (L) for classification. We try two networks with different numbers of layers (L = 6 and 12). We conduct experiments on PCQM4Mv1 dataset. The validation mean absolute errors (MAE) are reported in Table 8. We can see that using average node pooling is better than using compound representation. This is consistent with the discovery of using virtual node in GIN (Hu et al., 2021). A virtual node can be regarded as a compound representation, which connects to all nodes in graph. When using virtual nodes, it is a common practice to use the average or sum pooling of node representations to represent a graph. One can refer to https://github.com/ snap-stanford/ogb/blob/1c875697fdb20ab452b2c11cf8bfa2c0e88b5ad3/ examples/lsc/pcqm4m/gnn.py#L60 for the detailed implementation.\n\ni\n\nL = 6 L = 12\n\nAverage node pooling Compound representation\n\n0.1171 0.1196\n\n0.1149 0.1167\n\nTable 8: Comparison between using average node representations VS compound representations.\n\nC.2 ATTENTIVELY AGGREGATE THE INFORMATION FROM RINGS\n\nIn Eqn.(4), we concatenate the sum pooling of atom representations, the sum pooling of bond representations and compound representations to update ring representations. An alternative solution is to use attention models to aggregate the atom and bond representations. We study a variant which updates the ring representations as follows:\n\nh(l)\n\nr = h(l−1)\n\nr\n\n+ MLP\n\n(cid:16)\n\nh(l−1)\n\nr\n\n,\n\n(cid:88)\n\nα(l)\n\ni h(l)\n\ni\n\n(cid:88)\n\n,\n\nij h(l) β(l)\n\nij , U (l−1)(cid:17)\n\n,\n\n(9)\n\nvi∈V (r)\n\neij ∈E(r)\n\nIn Eqn.(9),\n\n(cid:16)\n\nexp\n\nα(l)\n\ni ∝\n\nWq1h(l−1)\n\nr\n\n+ Wk1h(l)\n\ni\n\n(cid:17)\n\nand β(l)\n\nij ∝\n\n(cid:16)\n\nexp\n\nWq2h(l−1)\n\nr\n\n+ Wk2h(l)\n\nij\n\n(cid:17)\n\n,\n\n(10)\n\nwhere the four W ’s are parameters to be learned. The results are reported in Table 9. We can see that although our method is simple, it can effectively leverage the ring information, and outperform this attention-based variant.\n\nC.3\n\nO\n\n-GNN WITH BRICS\n\nThe ring representation used in our method could be considered as a special motif. One might wonder whether other types of new motifs would be helpful. To see the effect, we use BRICS model (Degen et al., 2008) to decompose molecules into fragments. BRICS designs 16 rules to break bonds that can match a set of chemical reactions. The ring representations in Eqn.(2,3,4,5) are replaced by\n\n16\n\n̸ Published as a conference paper at ICLR 2023\n\n-GNN -GNN with attention models when updating ring representations\n\n0.1171 0.1179\n\n0.1149 0.1160\n\nO O\n\nTable 9: Comparison between our method and using attention models when updating ring representations.\n\nL = 6 L = 12\n\nL = 2\n\nL = 4\n\nL = 6\n\nL = 8 L = 12\n\n-GNN -GNN w/o rings\n\nO O\nBRICS\n\n0.1247 0.1325 0.1294\n\n0.1201 0.1243 0.1239\n\n0.1181 0.1222 0.1219\n\n0.1172 0.1221 0.1208\n\n0.1155 0.1204 0.1193\n\nTable 10: Comparison between using simple rings (i.e., our method) and using BRICS-based fragments.\n\nthese motif representations. The remaining parts remain unchanged. We conduct the experiments on PCQM4Mv1 dataset, and the results are shown in Table 10. Due to time and computation resource limitation, all the models are trained for 200 epochs.\n\nFrom Table 10, we can conclude that: (1) using simple ring representations achieves better results than using BRICS; (2) in general, using BRICS is better than the variant without using any ring or ring-based fragmentation information. We will keep exploring more segmentation methods.\n\nC.4 MORE COMPARISON BETWEEN\n\n-GNN AND\n\nO\n\nO\n\n-GNN W/O RINGS\n\nAs a complementary to the analysis of the MAE w.r.t. the number of rings in molecules in Figure 5, we also report the predicted error (i.e., mean absolute error, MAE) of -\nO GNN w/o rings” in Figure 8. We can observe that when molecules have no rings, the two methods -GNN perform similar. As the number of rings increases from 1 to 6, the MAE increases, and always outperforms the “\n\n-GNN w/o rings” variant.\n\n-GNN and the variant “\n\nO\n\nO\n\nO\n\nC.5 ADDITIONAL DISCUSSIONS\n\nAbout over-smoothing One might be curious that since we build a 12-layer network, whether it suffers from over-smoothing. Actually, Cong et al. (2021) point that “over-smoothing does not necessarily happen in practice, a deeper model is provably expressive, can converge to global optimum with linear convergence rate, and achieve very high training accuracy as long as properly trained.” (The words are from (Cong et al., 2021) for accurate expression). In addition, Li et al. (2020) and Addanki et al. (2021) both successfully trained 50+ layer networks. Our method follows\n\n(a) L = 2\n\n(b) L = 6\n\n(c) L = 12\n\nFigure 8: Predicted MAE categorized by different properties. x-axis denotes the number or rings, and y-axis denoted the mean absolute error (MAE) on the validation set.\n\n17\n\n0246Numberofrings0.00.10.20.3MAEO-GNNw/oringsO-GNN0246Numberofrings0.00.10.20.3MAEO-GNNw/oringsO-GNN0246Numberofrings0.00.10.20.3MAEO-GNNw/oringsO-GNNPublished as a conference paper at ICLR 2023\n\nthe architecture of (Addanki et al., 2021), therefore we do not think that our model suffers from over-smoothing.\n\nModeling k-neighborhood: If we want to explicitly use the k-neighborhood information, we might need additional modules to process them, like\n\nnet1(1 neighbor nodes) + net2(2 neighbor nodes) +\n\n+ netk(k neighbor nodes).\n\n(11)\n\n· · ·\n\nTo ensure expressiveness, we usually do not share parameters. Therefore, the parameters are k times -GNN constantly increases the percentages of parameters (irrelevant to larger than regular GNN. k). When k is large, -GNN will be much more parameter efficient. On the other hand, the optimal k∗ is not easy to determine. For example, in DrugBank, the ring maximum ring sizes range from 3 (e.g., DB00658) to 53 (e.g., DB05034). Which k is the best is hard to determine.\n\nO\n\nO\n\nAbout invariant constraints In -GNN, the features of atoms, bonds and rings are all invariant. Specifically, the features of atoms and bonds are about their types, number of correlated electrons, number of neighbors, etc (please refer to https://github.com/O-GNN/O-GNN/blob/ 5b70a4f9dc9a5f87a0171eea1e9cecde30489eb8/ogb/utils/features.py#L2 for details). The ring representations are obtained via atom and bond representations (please kindly refer to Eqn.(4)), which are also invariant. The variant features (like coordinates) are not encoded.\n\nO\n\nComparison about the convergence speed: The validation MAE curves of PCQM4Mv1 are shown in Figure 9. The results of 6-layer -GNN (with/without rings) are reported. We can see that:\n\n-GNN (with/without rings) 12-layer\n\nO\n\nO\n\n-GNN for 175 epochs, the results are almost the same as training the\n\n(1) by training the 6-layer 12-layer “\n\nO -GNN w/o ring” for 275 epochs;\n\n(2) by training the 12-layer 12-layer “\n\n-GNN w/o ring” for 275 epochs.\n\nO\n\nO\n\nO\n\nThese results demonstrate that\n\n-GNN has better convergence speed.\n\nO\n\n-GNN for 75 epochs, the results are almost the same as training the\n\nFigure 9: Comparison about the convergence speed of denotes the training epoch and y-axis denotes the validation MAE.\n\nO\n\n-GNN and “\n\nO\n\n-GNN w/o ring”. x-axis\n\nComparison between different number of parameters. Figure 4 shows the validation MAE of -\nO GNN and “ -GNN w/o ring” w.r.t. the number of layers. We also visualize the validation MAE w.r.t. the number of parameters in Figure 10. We can observe that when aligned with the number of parameters,\n\n-GNN still outperforms the variant without modeling rings.\n\nO\n\nPre-training baselines on MoleculeNet. We summarize the pre-training baselines on MoleculeNet in Table 11. Sun et al. (2022) have demonstrated different data splitting method could result in significantly different results. We follow the common practice to use scaffold based splitting, and we cite the results of Rong et al. (2020) from Fang et al. (2022). Note that the results of -GNN is not pre-trained on unlabeled molecules. We can see that in terms of the average score, our method\n\nO\n\nO\n\n18\n\n25751251752252750.1200.1250.1300.1350.1400.145O-GNN w/o ring, L=6O-GNN w/o ring, L=12O-GNN, L=6O-GNN, L=12Published as a conference paper at ICLR 2023\n\nFigure 10: Validation MAE of PCQM4Mv1 w.r.t. the number of parameters.\n\nis comparable with those strong baselines, which demonstrate the effectiveness of our method. We will combine our method with pre-training in the future.\n\nDataset # Molecules\n\n(Hu et al., 2020) G-Contextual (Liu et al., 2022) G-Motif (Liu et al., 2022) GraphMVP (Liu et al., 2022) MGSSL (Zhang et al., 2021) GROVERbase (Rong et al., 2020) GROVERlarge (Rong et al., 2020) GEM (Fang et al., 2022)\n\nBBBP 2039\n\nTox21 7831\n\n71.2 ± 0.9 70.3 ± 1.6 66.4 ± 3.4 72.4 ± 1.6 70.5 ± 1.1 70.0 ± 0.1 69.5 ± 0.1 72.4 ± 0.4\n\n74.2 ± 0.8 75.2 ± 0.3 73.2 ± 0.8 75.9 ± 0.5 76.5 ± 0.3 74.3 ± 0.1 73.5 ± 0.1 78.1 ± 0.1\n\nClinTox 1478\n\n73.7 ± 4.0 59.9 ± 8.2 77.8 ± 2.0 79.1 ± 2.8 80.7 ± 2.1 81.2 ± 3.0 76.2 ± 3.7 90.1 ± 1.3\n\nHIV 41127\n\n75.8 ± 1.1 75.9 ± 0.9 73.8 ± 1.4 77.0 ± 1.2 79.5 ± 1.1 62.5 ± 0.9 68.2 ± 1.1 80.6 ± 0.9\n\nBACE 1513\n\n78.6 ± 1.4 79.2 ± 0.3 73.4 ± 4.0 81.2 ± 0.9 79.7 ± 0.8 82.6 ± 0.7 81.0 ± 1.4 85.6 ± 1.1\n\nSIDER 1478\n\n60.4 ± 0.6 58.4 ± 0.6 60.6 ± 1.1 63.9 ± 1.2 61.8 ± 0.8 64.8 ± 0.6 65.4 ± 0.1 67.2 ± 0.4\n\nO-GNN (ours)\n\n76.4 ± 0.4\n\n75.7 ± 0.7\n\n94.3 ± 1.6\n\n81.3 ± 1.2\n\n85.8 ± 1.0\n\n66.2 ± 1.2\n\nAvg\n\n72.3 69.8 70.9 74.9 74.8 72.6 72.3 79.0\n\n80.0\n\nTable 11: Pre-training baselines on MoleculeNet.\n\nD RELATED WORK SUMMARY\n\nGCN (Kipf & Welling, 2017) aggregates its neighbor information according to the adjacency matrix and degree matrix, and then updates the aggregated information with a linear transformation and a non-linear activation layer. GraphSAGE (Hamilton et al., 2017b) aggregates the neighbors information by element-wise average. GAT (Veliˇckovi ́c et al., 2017) introduces the attention mechanism into GNN, by which it can adaptively aggregates the representations of the neighbors. Brody et al. (2021) propose the GATv2 to improve the attention mechanism in a more expressive way. Xu et al. (2018) develop a simple aggregate function which involves an ε parameter and multi-layer perceptrons (MLPs) which is provably as powerful as the Weisfeiler-Lehman graph isomorphism test. Besides, Gilmer et al. (2017b); Li et al. (2017); Pham et al. (2017) propose to augment the graph with a virtual node to capture the global information of the graph. A virtual node connects to all the other nodes in the graph and is jointly updated during training. Its effectiveness is validated in a series of graph classification tasks.\n\nHowever, these work do not explicitly use the ring R in graph neural networks. Complementary to these work, we consider how to incorporate the ring information, which is another important component on top of the node and edge information, into molecular modeling. These advanced Aggregate and Update functions are also applicable to our work.\n\n19\n\n204060#Parameters(M)0.1150.1200.1250.130MAEO-GNNw/oringO-GNN",
    "reference": "# Summary Of The Paper\n\nIn the context of predictive tasks which involve molecules, the paper presents a GNN variant called $\\mathcal{O}$-GNN that explicitly models rings in compounds, besides the usual atom/bond modeling. The paper presents a theoretical analysis that justifies why it is preferable to account for rings in molecules, as well as a series of experiments to show that the model performs well in practical usage. Notably, this model is able to outperform the winner of the KDDCup on the PCQM4Mv1 benchmark (although on the validation set).\n\n# Strength And Weaknesses\n\n**Strengths**\n\n- It makes sense from a chemical point of view to have a separate modeling \"circuit\" for rings in molecules.\n- Really impressive results across different tasks.\n\n**Weaknesses**\n\n- Not really well-written (although well-organized).\n- I am a bit confused by some of the experimental results. The PCQM4Mv1 and MolNet benchmarks are somewhat okay. On FS-Mol, $\\mathcal{O}$-GNN replaces a backbone transformer-based residual network. However, the results using \"$\\mathcal{O}$-GNN without rings\" as a backbone are not shown, so it is not clear if the improvement is due to the use of GNN or because of the ring modeling. A similar pattern happens in the drug-drug interaction prediction task.\n- The theoretical analysis shows that few layers of $\\mathcal{O}$-GNN are at least as expressive as many more layers of a regular-GNN. But this does not necessarily qualify as an advantage in practice, unless **1)** $\\mathcal{O}$-GNN layers uses fewer parameters than a regular GNN with the same expressivity; **2)** $\\mathcal{O}$-GNN is faster to train than a regular GNN with the same expressivity. To put it in a different way, why do I have to use few layers of $\\mathcal{O}$-GNN, when I can get the same expressivity with many regular-GNN layers (and perhaps also using fewer parameters or being computationally faster)? I wonder if the authors can provide evidence that using $\\mathcal{O}$-GNN is really more advantageous than using regular-GNN once the expressivity is similar.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\n\nThe work is well structured. However, I cannot say the same about the writing quality. It seems to be written in a hurry, and it is riddled with typos and generally poor wording. I was tempted to signal the typos, but they were so many I had to desist.\n\n**Quality**\n\nI'd say medium, considering that the clarity is low.\n\n**Novelty**\n\nExplicitly modeling rings in molecular GNNs is the main contribution of this paper, which to my knowledge has not been done up until now. So, this work qualifies as novel (unless I'm mistaken, will dig more deeply into the literature before the rebuttal phase begins).\n\n**Reproducibility**\n\nThe code is provided in an anonymous repo. Though I haven't replicated the experiments, it's nice to see that the authors were kind enough to make their code available.\n\n# Summary Of The Review\n\n**Disclaimer**\n\nI did not check the proofs.\n\n**Overall judgment**\n\nWith the doubts I have exposed above, I consider this paper borderline, and I'm giving a 5 for the moment. I'd really like the author to respond to my points in order to be able to raise my score. Also, I **highly** recommend getting this paper proofread for the rebuttal. It would be a shame if I had to leave my score as-is just because nothing was done to improve the writing.\n\n**Questions/comments to authors**\n\n- I think an image will help the reader understand the \"two isomorphic sub-graphs lying on different rings\" in Section 1. Also, this statement is a bit misleading since two graphs with different rings are not isomorphic. Do you mean isomorphic in structure, but not in the node/bond features that make up the ring? I think this difference is addressed in Section 3.2, however, reading up to this point, the reader is confused.\n- In Eq. 4, why the ring representation at the previous layer $h_r^{(l-1)}$ is inside the $\\texttt{MLP}$, when the other representations (Eq. 2, 4, 5) are outside it?\n- For the graph classification tasks, why do you do an average pooling to get the graph representation, when you have all the compound representations at each layer already available?\n\n**Minors**\nPage 7: define multi-task learning as MT and random forest as RF in the text, so they are readily understandable when looking at Figure 4.\n\nPage 7: although it is clear from context, you should specify that $\\Delta$-AUPRC is a metric where higher values are better. You should also explain what a \"support set\" is.\n\n**EDIT**\n\nAfter 1st rebuttal, I am changing my score to 6 given the impressive effort put by the authors to answer my enquiries.\n\n**EDIT**\n\nAfter a successful rebuttal, I am now convinced this paper deserves acceptance, and I'm therefore raising my score to 8.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nREPRESENTATIVE PROTOTYPE WITH CONSTRASTIVE LEARNING FOR SEMI-SUPENVISED FEW-SHOT CLASSIFICATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFew-shot learning aims to learn novel classes in the dataset with few samples per class, which is a very challenging task. To mitigate this issue, the prior work obtain representative prototypes with semantic embedding based on prototypical networks. While the above methods do not meet the requirement of fewshot learning, which requires abundant labeled samples. Therefore, We propose a new model framework to get representative prototypes with semi-supervised learning. Specifically, we introduces the dataset containing unlabeled samples to assist training the model. More importantly, to fully utilize these unlabeled samples, we adopt conditional variational autoencoder to construct more representative prototypes. Simultaneously, we develop novel contrastive loss to improve the model generalization ability. We evaluate our method on miniImageNet and tieredImageNet benchmarks for both 1-shot and 5-shot settings and achieve better performance over the state-of-the-art semi-supervised few-shot method.\n\n1\n\nINTRODUCTION\n\nIn real life, humans are able to quickly establish awareness of new concepts from just one or a few examples. However, conventional machine learning usually learn with abundant labeled samples to ensure its generalization ability. Actually, obtaining a plentiful of labeled samples is exceedingly hard on account of security and the high cost time and money. Motivated by this, many researchers turn to few-shot learning (FSL). In the field of image classification, FSL means getting better image classification accuracy in a small dataset. Generally, prior knowledge is obtained from the base classes and then applied to the novel classes, which contains a few labeled samples (Fei-Fei et al., 2006) (Wang et al., 2020).\n\nExisting studies on FSL roughly fall into four types. (1) Metric-based method (Koch et al., 2015) (Vinyals et al., 2016) (Zhang et al., 2019b). The type of methods is a space mapping method, which aims to learn a good feature space. In this space, all data is converted into feature vectors, and the feature vectors of similar samples are close, while the feature vectors of dissimilar samples are far, so as to distinguish samples, and the distance usually use Euclidean distance (Snell et al., 2017) or cosine distance Chen et al. (2019a). (2) Optimization-based method. In the meta-learning framework, the method first learns a group of good and potential parameters for the network model with a large number of similar tasks, and then uses this group of parameters as the initial value to train on specific tasks, so as to achieve the convergence effect as long as fine-tuning on the new tasks, such as: (Finn et al., 2017) (Lee et al., 2019) (Fallah et al., 2020). (3) Data augmentationbased method (Alfassy et al., 2019) (Schwartz et al., 2018). The fundamental problem of FSL is that samples is few, so it can be solved by increasing the diversity of samples. For example, (Zhang et al., 2019a) proposed to segment the image into foreground and background, and then combine the foreground and background of different pictures, so as to expand the dataset. (4) Semantics-based method (Chen et al., 2019b) (Xing et al., 2019) (Li et al., 2020) (Zhang et al., 2021) (Xu & Le, 2022). This method is a recent research hotspot, which is mainly inspired by zero-shot learning (ZSL). This series of methods use semantic information as auxiliary information to enhance classification performance. In some cases, visual information is richer, while in some cases, semantic information is richer (Xing et al., 2019), which explains that fusing cross-modal information plays an important role in constructing representative class prototypes.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nGenerally, most of these methods are not used alone but integrated, and almost all are based on the meta-learning framework. However, class prototypes based on the meta-learning framework are not representative enough due to the number of samples in support set is few. Therefore, we propose a new model to construct representative class prototypes. For FSL, prototype-based is the typical method. Simply put, prototype-based is to construct a class prototype for each class using support set, and then keep test samples (from query set) close to the class prototype to which they belong and away from the other class prototypes. Prototypical networks (ProtoNet) firstly to tackle FSL (Snell et al., 2017), the basic idea of which is that samples in each class will be mapped to a feature space through neural network, and calculating the mean features of all samples of each class in this space as class prototype. And then there are a lot of work around ProtoNet.\n\nThe novel extensions of ProtoNet (Ren et al., 2018) exploits unlabeled samples when constructing prototypes, moreover, this paper makes the precise analysis of the distractor in unlabeled samples. Furthermore, a cosine similarity based prototypical network to select neighbor samples to augment support set (Liu et al., 2020) and training the regression model to restore the biased prototype with the Euclidean distance between the biased prototype and the real prototype (Xue & Wang, 2020), etc. The setting of our work is similar to (Ren et al., 2018), the different is that we cluster unlabeled samples first, and then judge the labels of unlabeled samples according to cluster centers and class prototypes. Except that we adopt the generation model to make full use of unlabeled samples.\n\nWe discover that the key of the prototype-based method is how to use a few samples to construct a representative class prototype, which is the challenging task on account of few samples or noise samples. The above approaches mentioned above rely solely on visual features for few-shot classification. Recently, inspired by ZSL, some work combined semantic embedding with prototype-based to enhance the performance. (Xing et al., 2019) utilize cross-modal information (visual features and semantic embedding) to generate visual and semantic prototypes and fuse the two prototypes according to different weights. (Zhang et al., 2021) takes attribute features as prior knowledge to complete the biased prototype. (Xu & Le, 2022) first selects the representative samples in the base classes via assuming that the features of each class follow the Gaussian multivariate distribution. Then, conditional variational autoencoder (CVAE) is used to generate representative features with these representative samples, and constructs representative class prototypes with the generate features and the support features in the novel classes. This paper opens up a ideas for constructing representative class prototypes, the one is data preprocessing, and the other is using generation models to augment data.\n\nMany previous methods require a large number of labeled samples at training stage, which is not consistent with the common scenario in our life, so semi-supervised learning can be used in our work. In order to make full use of unlabeled samples, these samples and semantic information are inputed CVAE to generate more features to construct representative class prototypes. Concurrently, the novel contrastive loss is introduced, which improves model generalization ability. Please note that this contrastive loss is calculated in feature space via feature extractor. Based on the above contents, we propose a novel model framework via meta-learning. Our main contributions of this paper can be summarized as follows:\n\n• We propose a new prototype recovery framework based on meta-learning, which can effec-\n\ntively use unlabeled samples to construct representative class prototypes.\n\n• We develop novel contrastive loss, using class prototypes as the anchor, which allows better\n\ninter-class discriminability to mitigate generalization problem.\n\n• We evaluate our approach on two benchmark datasets for few-shot learning, namely miniImageNet and tieredImageNet. The experimental results show that our method achieves higher performance, outperforming semi-supervised few-shot learning baselines.\n\nWe summarize related works in Section 2. Section 3 provides a rundown of our approach. Section 4 reports the main results obtained with our method. In section 5, we analyzed our methods from different aspects.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\n2.1 SEMI-SUPERVISED LEARNING\n\nMany methods for FSL are supervised learning. However, in many fields, such as medical treatment and aerospace, there will be abundant information that has not been labeled, some of which are useful, but if all of them are manually labeled, it will be very time-consuming and laborious. Therefore, FSL based on semi-supervised learning paradigm is more practical (Ren et al., 2018) (Liu et al., 2019) (Liu et al., 2020). Semi-supervised few-shot learning mainly improves performance with unlabeled samples. Semi-supervised paradigm is used firstly to tackle FSL in (Ren et al., 2018), which augmenting data by tagging unlabeled samples. Simultaneously, they proposed soft K-means, soft K-means + cluster and soft K-means + mask to get more representative prototypes based on ProtoNet. (Liu et al., 2019) puts forward a new idea, which uses graph model and label propagation to predict the labels of query set. Although these methods have achieved good performance, they do not make further use of unlabeled samples, we propose to take these unlabeled samples as the input of CVAE, combined with the corresponding class attributes, to construct more representative prototypes and improve the classification performance.\n\n2.2 CONSTRASTIVE LEARNING\n\nThe core idea of contrastive learning is to shorten the distance between the positive samples and the anchor sample in the vector representation space, and lengthen the distance between the negative samples and the anchor sample. This makes the boundary between positive and negative samples more obvious. The performance of contrastive learning has been demonstrated in the image domain (Kipf et al., 2019) (He et al., 2020) (Chen et al., 2020) (Han et al., 2021). Compared with the generative methods, which need to reconstruct pixel details to learn sample features, contrastive methods only need to learn discrimination in the feature space. Therefore, contrastive methods do not pay too much attention to pixel details, but can focus on abstract semantic information (more general knowledge), so as to improve the generalization ability of models. For sample selection, in traditional contrastive learning, we regard the input samples as anchors, the samples after data augmentation as positive samples, and other samples in the same batch as negative samples. Differently, sample selection in our model is to use the original prototype as the anchor.\n\n2.3 CONDITIONAL VARIATIONAL AUTOENCODER\n\nVAE (Kingma & Welling, 2013) belongs to the generation model family. Using VAE models for generating features conditioned on the corresponding semantic embedding is fairly common in ZSL methods (Mishra et al., 2018) (Schonfeld et al., 2019). (Mishra et al., 2018) is the first to propose to use a conditional VAE for ZSL. (Xu & Le, 2022) is the first FSL method that uses a conditional VAE model to generate visual features, conditioned on the semantic embedding of each class. In (Xu & Le, 2022), focusing on generating more features for the novel classes and constructing representative class prototypes with the support features in the novel classes, while our method focuses on generating more features for the support set to construct the representative class prototypes at two stages: meta-learning training and meta-learning test.\n\n3 SEMI-SUPENVISED FSL WITH REPRESENTATIVE PROTOTYPE\n\nIn this section, we introduce the overall framework of our model. As shown in Figure 1. For N-way K-shot image classification, we have two set: one is support set S = {(xi, yi)}N ×K , where xi is the image, yi is the label of xi , has few labeled samples. The other is query set Q = {(xi)}Q i=1, where Q is the number of images in Q , contains unlabeled samples. The samples in both sets are from Dnovel. Samples in Dnovel are few, so most work introduce an auxiliary dataset Dbase. During Meta-Training stage, we add an additional dataset R = {(xi)}R i=1, contains a large of unlabeled samples. Noted that the classes in Dbase and Dnovel are different, formalized as Cbase ∩ Cnovel = ∅. Our goal is to classify query samples correctly using few labeled samples from the support set.\n\ni=1\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Overall Framework. First, getting features representation of samples. Then labeling samples with GMM, the support set turn to the extended support set with unlabeled samples. Finally, constructing class prototypes with CVAE. Training the model with loss function L = Lcl + Lvae + Ltest, where the loss Lcl according to shrink the distance of the anchor and the positive samples, while enlarging the distance between the anchor and the negative samples.\n\nIn this paper, we exploit a convolutional neural network fθf to extract features from an input xi, where θf indicates a parameter of the network. We adopt ResNet12 as feature extractor for both the support set S and the query set Q.\n\n3.1 LABELING SAMPLES WITH GAUSSIAN MIXTURE MODEL\n\nIn this part, generating label for unlabeled samples by the Gaussian mixture model. Specifically,\n\nFirst, defining a multivariate Gaussian mixture model with K classes to estimate the probability density of the samples. Its defined distribution as Equation (1), where μk, Σk denotes the mean and covariance of the k-th multivariate Gaussian distribution, and αk denotes the probability of the k-th mixture component.\n\nP (x) =\n\nK (cid:88)\n\nk=1\n\nαk · p(x|μk, Σk)\n\n(1)\n\nNext, calculating the parameters Θ = {α1, α2, . . . , αk, μ1, μ2, . . . , μk, Σ1, Σ2, . . . , Σk} with the EM algorithm, which is divided into two steps: E-Step: calculating the posterior probability that the sample xj comes from the k-th multivariate Gaussian distribution, where R denotes the number of R, formalized as Equation (2):\n\nγjk =\n\nαk · p(xj|μk, Σk) k=1 αk · p(xj|μk, Σk)\n\n(cid:80)K\n\n, j = 1, 2, 3, ..., R\n\n(2)\n\nM-step: calculating a new round of model parameter estimates (αk, μk, Σk), formalized as Equation (3) ∼ (5). Repeating the E-step and M-step until the model converges.\n\nαk =\n\n(cid:80)R\n\nj=1 γjk\n\nR\n\nμk =\n\n(cid:80)R\n\nj=1 γjk · xj (cid:80)R j=1 γjk\n\nΣk =\n\n(cid:80)R\n\nj=1 γjk(xj − μk)(xj − μk)T j=1 γjk\n\n(cid:80)R\n\n(3)\n\n(4)\n\n(5)\n\nFinally, utilizing the labeled samples from S to label unlabeled samples. Specifically, after the model convergence is completed, R is clustered into K classes, expressed as C = {c1, c2, . . . , ck}. The model calculates the mean vector as the class prototypes pk in the support set according to Equation (6), so we have K class prototypes, expressed as P = {p1, p2, . . . , pk}. Then the probability of each\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ncluster ci to be class k is estimated based on the proximity between the ci ∈ C and pk ∈ P over a softmax (Bridle, 1990), formalized as Equation (7).\n\npk =\n\n1 |Sk|\n\n(cid:88)\n\nfθf (xi)\n\n(xi,yi)∈Sk\n\nwhere Sk ∈ S is the subset of support belonging to class k.\n\nP (ci = k) =\n\nexp(−d < ci, pk >) k exp(−d < ci, pk >)\n\n(cid:80)\n\nwhere d <, > denotes the Euclidean distance of two vectors.\n\n3.2 CONTRASTIVE LOSS\n\n(6)\n\n(7)\n\nContrastive learning has been widely shown to capable of improving model generalization. Hence, we develop a new loss to further improve model generalization. Given the class prototype pk as the anchor, simultaneously, we’ve possessed the dataset T = {x1, x2, . . . , xt}, having the sample label as pk (in section 3.1), which as the positive samples. Also, having a negative dataset O = {x1, x2, . . . , xo}, which has different label from pk. Here we propose a prototype-based contrastive loss function, which adopts the class prototype the anchor, formalized as Equation 8. Now, our goal is to shrink the distance between xi ∈ T and pk while enlarging the distance between xi ∈ O and pk.\n\nLcl = −log\n\n(cid:80)t\n\ni=1 exp(−d < xi, pk >) i=1 exp(−d < xi, pk >)\n\n(cid:80)t+o\n\n(8)\n\nwhere t denotes the number of positive samples, o denotes the number of negative samples, and d <, > denotes the Euclidean distance of two vectors.\n\n3.3 CONSTRUCTING CLASS PROTOTYPES WITH CVAE\n\nBased on the above work, we already have the labeled samples (defined dataset ̃S) according to the support set S and the corresponding dataset C. To generate samples with semantic embedding, we combine xi ∈ ̃S and the interrelated attributes embedding as the input of VAE. VAE consists of an encoder E(x, a), which encodes a sample x to a latent code z, and a decoder D(x, a), which reconstructs x from z. The objection loss can be defined as:\n\nLvae = KL(q(z|xi, ak)||p(z|ak)) − log p(xi|z, ak)\n\n(9)\n\nwhere ak denotes the semantic embedding of class k. The first term is the regularization term, which aligns the variation term q(z|x, a) to the prior distribution p(z|a) through the Kullback-Leibler divergence. The second term is the reconstruction loss, which aims to make the features z from the encoder generated by the decoder approximate the original input features..\n\nWe acquire a new dataset ˆS whose samples contain semantic information. Combining the previous datasets ̃S, we can reconstruct representative class prototypes as follows:\n\npk =\n\n1 | ̃Sk|\n\n(cid:88)\n\n(fθf (xi)) +\n\n(xi,yi)∈ ̃Sk\n\n1 | ˆSk|\n\n(cid:88)\n\n(fθf (xi))\n\n(xi,yi)∈ ˆSk\n\n(10)\n\nwhere ̃Sk ∈ ̃S is the subset of support belonging to class k. ˆSk ∈ ˆS is the subset of support belonging to class k.\n\n3.4 META-LEARNING WITH REPRESENTATIVE PTOTOTYPE\n\nWe construct a number of N-way K-shot tasks from Dbase following the episodic training manner (Vinyals et al., 2016). Specially, in each task, we sample N classes from the base classes, K images in each class as the support set S, and Q images as the query set Q. Beyond that, we introduce R, having R images. For each task, first, we access more labeled samples from R with GMM, and then calculating contrastive loss with these labeled samples, finally constructing class prototypes via\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nVAE with semantic embedding. Next, calculating the Euclidean distance between the test samples, which from the query set, and the representative class prototypes to predict the labels:\n\nP ( ̃yi = k|xi) =\n\nexp(−d < fθf (xi), pk > k exp(−d < fθf (xi), pk >\n\n(cid:80)\n\n(11)\n\nHere, ̃yi denotes the final predicted label for xi, and d <, > denotes the Euclidean similarity between of two vector. The largest probability value is taken as the predict label. Then the loss function is calculated as:\n\nLtest = ce( ̃y, y)\n\n(12)\n\nHere, ̃yi denotes the final predicted label for xi, y denotes the real label for xi, ce(, ) denotes the cross-entropy loss. Therefore, at the meta-learning training stage, the loss function as follow:\n\nL = Lcl + Lvae + Ltest\n\n(13)\n\nFinally, applying the meta-trained model to the novel class.\n\nTable 1: Few-shot classification accuracies on miniImageNet and teiredImageNet. All results are averaged over 600 test episodes. Top results are highlighted.\n\nMethod\n\nBackbone\n\nminiImageNet\n\nteiredImageNet\n\nProtoNet (Snell et al., 2017)\n\nMAML (Finn et al., 2017)\n\nSoft k-Means (Ren et al., 2018)\n\nTPN-semi(Liu et al., 2019)\n\nConvNet-64\n\nResNet-18\n\nConvNet-64\n\nConvNet-64\n\nPSN, Semi-supervised (Simon et al., 2018) ConvNet-64\n\n5-way 1-shot\n\n5-way 5-shot\n\n5-way 1-shot\n\n5-way 5-shot\n\n49.42 ± 0.78 48.70 ± 1.84 50.09 ± 0.45\n\n52.78\n\n-\n\n68.20 ± 0.66 63.11 ± 0.92 64.59 ± 0.28\n\n66.42 68.12 ± 0.67\n\n-\n\n-\n\n- 51.52 ± 0.36\n\n55.74\n\n-\n\n- 70.25 ± 0.31\n\n71.01 71.15 ± 0.67\n\nOurs (Semi-supervised learning)\n\nResNet-12\n\n53.10 ± 0.03\n\n68.14 ± 0.02\n\n57.14 ± 0.14\n\n74.13 ± 0.02\n\n4 EXPERIMENTS\n\n4.1 BENCHMARKS\n\nminiImageNet. In 2016, Google DeepMind team extracted miniImageNet based on ImageNet, which contains 100 classes, each class has 600 images. DeepMind team first applied miniImagenet to few-shot learning research (Vinyals et al., 2016), since then miniImageNet has became the benchmark for few-shot learning fields. In general, we divided the dataset into 64 classes as training sets, 16 as validation sets, and 20 as test sets.\n\nteiredImageNet. The data set is also a subset of ImageNet, which proposed in (Ren et al., 2018). teiredImageNet is similar to Omniglot, its classification has the concept of hierarchy. The dataset is divided into 34 high-level classes (such as Instruments, tools, vehicles, etc.), each of which contains 10-30 more detailed sub-classes (such as Musical Instruments, including guitars, pianos, etc.). The 34 classes are divided into 20 training classes, 6 validation classes and 8 test classes.\n\n4.2\n\nIMPLEMENTATION DETAILS\n\nDataset Split. For each dataset, we first create an additional split to separate labeled and unlabeled samples. For miniImagenet, 40% of the data for labeled samples and 60% of data for unlabeled samples. For teiredImagenet, 10% of the data for labeled samples and the remaining 90% for unlabeled samples.\n\nTraining Details. We adopt ResNet12 architectures as feature extractor, and the dimension of the feature representation is 512. The dimensions of semantic embedding are set to be 512, and are extracted from CLIP Radford et al. (2021). And then, all parameters are trained jointly with 100 episodes in a epoch. All our models are trained with Adam KingaD (2015) and initial learning rate of 10−3.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 2: Clustering Visualization. (a) To update the parameters of ResNet12. (b) To compare features distribution of the original and the clustered.\n\nEvaluation. We conduct few-shot classification on 600 randomly sampled episodes from the test set and report the mean accuracy together with the 95% confidence interval. In each episode, we randomly sample 15 unlabeled images per class for augmenting the support set, and sample 15 query images per class for evaluation in 5-way 1-shot/5-shot tasks .\n\n4.3 RESULTS\n\nTable 1 presents the 5-way 1-shot and 5-way 5-shot classification results of our methods on miniImageNet and tieredImageNet in comparision with previous FSL methods. We compare our method with (Snell et al., 2017) (Finn et al., 2017) (Ren et al., 2018) (Liu et al., 2019) (Simon et al., 2018). The first is classical meta-learning methods, ProtoNet and MAML. The second is Semi-supervised method, such as Soft k-Means and TPN-semi. Since we use the Semi-supervised method paradigm for training, we choose these two methods for comparison. Our method outperforms existing semisupervised methods, which demonstrates its effectiveness.\n\n5 ANALYSES\n\nIn order to better illustrate the experimental results, we explained and visualized some details of the experiment, found the experimental results under different experimental settings, and found better experimental settings.\n\n5.1 FEATURE DISTRIBUTION ANALYSIS\n\nClustering Visualization. As shown in Figure 2(a), we first extract features of the input samples to obtain 512-dimensional feature vectors. And then in feature space, clustering features of unlabeled samples. Finally, updating the parameters of feature extractor according to the classifier. To sum up, we cluster unlabeled samples in feature space. To further understand the distribution of clusters in the feature space. We use t-SNE (Hinton & van der Maaten, 2008) to map feature representation to 2-d space. Figure 2(b) shows the feature distribution. The left is the original feature distribution, and the right is the clustered distribution. We find that clustering will change the feature distribution of the original data and cluster the features of the same samples. However, since the image is highdimensional data, the label generated by clustering may not be correct, which also causes problems in downstream tasks. Therefore, we will study how to further improve the accuracy of clustering by GMM.\n\nFeatures Visualization on Training. To verify whether feature extractor has learned discriminative features, we randomly select three classes to visualized the feature distribution at different training stages. Figure 3(a) is the original features distribution. Figure3(b) is the features distribution after epoch 1. Figure.3(c) is the features distribution after epoch 64. Figure3(d) is the features distribution after epoch 128. As shown in Figure 3, we find that with the increase of training epochs, the features extracted by feature extractor become more and more discriminative. This shows that the model obtained in this training stage is effective.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 3: Feature Visualization on Training. (a) Epoch 0. (b) Epoch 1. (c) Epoch 64. (d) Epoch 128.\n\n5.2 PERFORMANCE ON DIFFERENT NUMBER OF UNLABELED SAMPLES\n\nFigure 1 shows the classification accuracy with different number of unlabeled samples. The model is carried out under the setting of way = 5. The number of unlabeled samples of each class increases from 0 to 25, and we observe that the classification accuracy of the model also increases. This demonstrates that under the meta-learning framework, even under the semi-supervised paradigm, the model can learn to obtain a better representation.\n\nFigure 4: Model Performance on miniImageNet with different numbers of unlabeled samples at meta-learning test.\n\n6 CONCLUSIONS\n\nA novel framework for few-shot learning to construct representative class prototypes is proposed. First, clustering unlabeled samples with GMM, and predict the labels of unlabeled samples according to the support set with few labeled samples, thus the model conforms to the supervised learning paradigm. Then, in order to learn more distinctive features, the prototype-based contrastive loss is developed. Finally, the corresponding semantic embedding of the extended support set and the extended support set as the input of CVAE to reconstruct more features, so as to calculate more representative class prototypes. The whole model is based on the meta-learning framework, and the whole training loss is composed of three parts. The results show that the performance of our method is better than the existing semi-supervised learning methods. In the future, we will focus on how to get better clustering results in high-dimensional data.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAmit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan Harary, Rogerio Feris, Raja Giryes, and Alex M Bronstein. Laso: Label-set operations networks for multi-label few-shot learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6548–6557, 2019.\n\nJohn S Bridle. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neurocomputing, pp. 227–236. Springer, 1990.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer\n\nlook at few-shot classification. arXiv preprint arXiv:1904.04232, 2019a.\n\nZitian Chen, Yanwei Fu, Yinda Zhang, Yu-Gang Jiang, Xiangyang Xue, and Leonid Sigal. Multilevel semantic feature augmentation for one-shot learning. IEEE Transactions on Image Processing, 28(9):4594–4605, 2019b.\n\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradientbased model-agnostic meta-learning algorithms. In International Conference on Artificial Intelligence and Statistics, pp. 1082–1092. PMLR, 2020.\n\nNanyi Fei, Zhiwu Lu, Tao Xiang, and Songfang Huang. Melr: Meta-learning via modeling episodelevel relationships for few-shot learning. In International Conference on Learning Representations, 2020.\n\nLi Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE trans-\n\nactions on pattern analysis and machine intelligence, 28(4):594–611, 2006.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.\n\nJunlin Han, Mehrdad Shoeiby, Lars Petersson, and Mohammad Ali Armin. Dual contrastive learning In Proceedings of the IEEE/CVF Conference on\n\nfor unsupervised image-to-image translation. Computer Vision and Pattern Recognition, pp. 746–755, 2021.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nG Hinton and L van der Maaten. Visualizing data using t-sne journal of machine learning research.\n\n2008.\n\nA KingaD. A methodforstochasticoptimization. Anon. International Conferenceon Learning Rep-\n\nresentations. SanDego: ICLR, 2015.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nThomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models.\n\narXiv preprint arXiv:1911.12247, 2019.\n\nGregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot\n\nimage recognition. In ICML deep learning workshop, volume 2, pp. 0. Lille, 2015.\n\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with In Proceedings of the IEEE/CVF conference on computer\n\ndifferentiable convex optimization. vision and pattern recognition, pp. 10657–10665, 2019.\n\nAoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, and Liwei Wang. Boosting few-shot In Proceedings of the IEEE/CVF conference on computer\n\nlearning with adaptive margin loss. vision and pattern recognition, pp. 12576–12584, 2020.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nJinlu Liu, Liang Song, and Yongqiang Qin. Prototype rectification for few-shot learning. In Euro-\n\npean Conference on Computer Vision, pp. 741–756. Springer, 2020.\n\nYanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to propagate labels: Transductive propagation network for few-shot learning. In International Conference on Learning Representations, 2019.\n\nAshish Mishra, Shiva Krishna Reddy, Anurag Mittal, and Hema A Murthy. A generative model In Proceedings of the IEEE\n\nfor zero shot learning using conditional variational autoencoders. conference on computer vision and pattern recognition workshops, pp. 2188–2196, 2018.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. In Proceedings of 6th International Conference on Learning Representations ICLR, 2018.\n\nEdgar Schonfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized zero-and few-shot learning via aligned variational autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8247–8255, 2019.\n\nEli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, and Alex Bronstein. Delta-encoder: an effective sample synthesis method for few-shot object recognition. Advances in neural information processing systems, 31, 2018.\n\nChristian Simon, Piotr Koniusz, and Mehrtash Harandi. Projective subspace networks for few-shot\n\nlearning. 2018.\n\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Ad-\n\nvances in neural information processing systems, 30, 2017.\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\n\nshot learning. Advances in neural information processing systems, 29, 2016.\n\nYaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples:\n\nA survey on few-shot learning. ACM computing surveys (csur), 53(3):1–34, 2020.\n\nChen Xing, Negar Rostamzadeh, Boris Oreshkin, and Pedro O O Pinheiro. Adaptive cross-modal\n\nfew-shot learning. Advances in Neural Information Processing Systems, 32, 2019.\n\nJingyi Xu and Hieu Le. Generating representative samples for few-shot classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9003–9013, 2022.\n\nWanqi Xue and Wei Wang. One-shot image classification by learning to restore prototypes.\n\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 6558–6565, 2020.\n\nBaoquan Zhang, Xutao Li, Yunming Ye, Zhichao Huang, and Lisai Zhang. Prototype completion with primitive knowledge for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3754–3762, 2021.\n\nHongguang Zhang, Jing Zhang, and Piotr Koniusz. Few-shot learning via saliency-guided hallucination of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2770–2779, 2019a.\n\nJian Zhang, Chenglong Zhao, Bingbing Ni, Minghao Xu, and Xiaokang Yang. Variational few-shot In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n\nlearning. 1685–1694, 2019b.\n\nManli Zhang, Jianhong Zhang, Zhiwu Lu, Tao Xiang, Mingyu Ding, and Songfang Huang. Iept: Instance-level and episode-level pretext tasks for few-shot learning. In International Conference on Learning Representations, 2020.\n\n10",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new method for few-shot image classification. The method contains a new framework for obtaining representative prototypes, by leveraging an external unlabeled dataset to assist training. The method also includes a conditional variational autoencoder and a contrastive loss. Experimental results are shown for 1-shot and 5-shot settings on two datasets (miniImageNet and tieredImageNet).\n\n# Strength And Weaknesses\n\n*Strengths*\n- The topic of FSL is important finding effective prototypes is important for improving FSL.\n- \n\n*Weaknesses*\n- Many details about the experimental setting are missing. For example how were the dataset splits obtained? Were they sampled at random, or was another protocol used? \n- tieredImageNet is an interesting dataset to use, but the paper does not explore various few-shot settings that are possible in tieredImageNet. For example, splitting the dataset on high-level vs low-level. \n- No ablation studies on the different components on the method (Contrastive Loss, CVAE protypes, Meta Learning, etc.)\n- only one architecture is used. Experiments should be replicated on various architectures (eg. ViT, ResNet, etc.)\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Clarity: the paper needs significant editing -- including a thorough grammar check, spell-check (even the TITLE has a typo \"Supenvised\").  \n- Quality: The results section is lacking in insight -- experiments and analyses should be much more exhaustive than presented.\n- Novelty: Limited.  Many of the components (CVAE, prototype construction and using unlabeled data for training) have been used before for FSL\n- Reproducibility: no code available\n\n# Summary Of The Review\n\nThe paper addresses an important problem but falls short in terms of experimental and analytical insights.  There are several modules in the method, but no ablation study is provided to establish the utility of each module.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDOES CONTINUAL LEARNING EQUALLY FORGET ALL PARAMETERS?\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDistribution shift (e.g., task or domain shift) in continual learning (CL) usually results in catastrophic forgetting of neural networks. Although it can be alleviated by repeatedly replaying buffered data, the every-step replay is time-consuming and the memory to store historical data is usually too small for retraining all parameters. In this paper, we study which modules in neural networks are more prone to forgetting by investigating their training dynamics during CL. Our proposed metrics show that only a few modules are more task-specific and sensitively alters between tasks, while others can be shared across tasks as common knowledge. Hence, we attribute forgetting mainly to the former and find that finetuning them only on a small buffer at the end of any CL method can bring non-trivial improvement. Due to the small number of finetuned parameters, such “Forgetting Prioritized Finetuning (FPF)” is efficient on both the computation and buffer size required. We further propose a more efficient and simpler method that entirely removes the every-step replay and replaces them by only k-times of FPF periodically triggered during CL. Surprisingly, this “k-FPF” performs comparably to FPF and outperforms the SOTA CL methods but significantly reduces their computational overhead and cost. In experiments on several benchmarks of classand domain-incremental CL, FPF consistently improves existing CL methods by a large margin and k-FPF further excels on the efficiency without degrading the accuracy. We also empirically studied the impact of buffer size, epochs per task, and finetuning modules to the cost and accuracy of our methods.\n\n1\n\nINTRODUCTION\n\nEmpowered by advancing deep learning techniques and neural networks, machine learning has achieved unprecedented promising performance on challenging tasks in different fields, mostly under the i.i.d. (independent and identically distributed) offline setting. However, its reliability and performance degenerates drastically in continual learning (CL) where the data distribution or task in training is changing over time, as the model quickly adapts to a new task and overwrites the previously learned weights. This leads to severe bias towards more recent tasks and “catastrophic forgetting” of previously learned knowledge, which is detrimental to a variety of practical applications.\n\nA widely studied strategy to mitigate forgetting is experience replay (ER) (Ratcliff, 1990; Robins, 1995) and its variants (Riemer et al., 2018; Buzzega et al., 2020; Boschini et al., 2022), which store a few data from previous tasks in a limited memory and train the model using both the current and buffered data. However, they only bring marginal improvements when the memory is too small to store sufficient data for recovering previously learned knowledge, which is common due to the complicated distributions of previous tasks. In contrast, multi-task learning (Caruana, 1997) usually adopts a model architecture composed of a task-agnostic backbone network and multiple task-specific adaptors on top of it. While the backbone needs to be pre-trained on large-scale data, the adaptors are usually light-weight and can be achieved using a few data. In CL, however, we cannot explicitly pre-define and separate the task-agnostic parts and task-specific parts. Although previous methods (Schwarz et al., 2018; Zenke et al., 2017) have studied to restrict the change of parameters critical to previous tasks, such extra constraint might degrade the training performance and discourage task-agnostic modules capturing shared knowledge.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn this paper, we study a fundamental but open problem in CL, i.e., are most parameters task-specific and sensitively changing with the distribution shift? Or is the catastrophic forgetting mainly caused by the change on a few task-specific parameters? It naturally relates to the plasticity-stability trade-off in biological neural systems (Mermillod et al., 2013): more task-specific parameters improves the plasticity but may cause severe forgetting, while the stability can be improved by increasing parameters shared across tasks. In addition, how many task-specific parameters suffice to achieve promising performance on new task(s)? Is every-step replay necessary?\n\nTo answer these questions, we investigate the training dynamics of model parameters during the course of CL by measuring their changes over time. For different CL methods training with various choices of buffer size and number of epochs per task on different neural networks, we consistently observe that only a few parameters change more drastically than others between tasks. The results indicate that most parameters can be shared across tasks and we only need to finetune a few task-specific parameters to retain the previous tasks’ performance. Since these parameters only contain a few layers of various network architectures, they can be efficiently and accurately finetuned using a small buffer.\n\nThe empirical studies immediately motivate a simple yet effective method, “forgetting prioritized finetuning (FPF)”, which finetunes the task-specific parameters using buffered data at the end of CL methods. Surprisingly, on multiple datasets, FPF consistently improves several widely-studied CL methods and substantially outperforms a variety of baselines. Moreover, we extend FPF to a more efficient replay-free CL method “k-FPF” that entirely eliminates the cost of every-step replay by replacing such frequent replay with occasional FPF. k-FPF applies FPF only k times during CL. We show that a relatively small k suffices to enable k-FPF achieving comparable performance with that of FPF+SOTA CL methods and meanwhile significantly reduces the computational In addition, we explore different groups of parameters to finetune in FPF and k-FPF by cost. ranking their sensitivity to task shift evaluated in the empirical studies. For FPF, we compare them under different choices for the buffer size, the number of epochs per task, the CL method, and the network architecture. FPF can significantly improve existing CL methods by only finetuning ≤ 0.127% parameters. For k-FPF, we explore different groups of parameters, k, and the finetuning steps per FPF. k-FPF can achieve a promising trade-off between efficiency and performance. Our experiments are conducted on a broad range of benchmarks for class- and domain-incremental CL in practice, e.g., medical image classification and realistic domain shift between image styles.\n\n2 RELATED WORK\n\nContinual Learning and Catastrophic Forgetting A line of methods stores samples of past tasks to combat the forgetting of previous knowledge. ER (Riemer et al., 2018) applies reservoir sampling (Vitter, 1985) to maintain a memory buffer of uniform samples over all tasks. Each mini-batch of ER is randomly sampled from current task and the buffered data. MIR (Aljundi et al., 2019) proposes a new strategy to select memory samples suffering the largest loss increase induced by the incoming mini-batch so those at the forgetting boundary are selected. DER and DER++ (Buzzega et al., 2020) apply knowledge distillation to mitigate forgetting by storing the output logits for buffered data during CL. iCaRL (Rebuffi et al., 2017) selects samples closest to the representation mean of each class and trains a nearest-mean-of-exemplars classifier to preserve the class information of samples. A-GEM (Chaudhry et al., 2018) constrains new task’s updates to not interfere with previous tasks. Our methods are complementary techniques to these memory-based methods. It can further improve their performance by finetuning a small portion of task-specific parameters on buffered data once (FPF) or occasionally (k-FPF).\n\nAnother line of work imposes a regularization on model parameters or isolates task-specific parameters to retain the previous knowledge. oEWC (Schwarz et al., 2018) constrains the update of model parameters important to past tasks by a quadratic penalty. To select task-specific parameters, SI (Zenke et al., 2017) calculates the effect of the parameter change on the loss while MAS (Aljundi et al., 2018) calculates the effect of parameter change on the model outputs when each new task comes. PackNet (Mallya & Lazebnik, 2018) and HAT (Serra et al., 2018) iteratively assign a subset of parameters to consecutive tasks via binary masks. All these works try to identify critical parameters for different tasks during CL and restrict the update of these parameters. But they can also prevent task-agnostic parameters from learning shared knowledge across tasks. From the\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ntraining dynamics of CL, we identify the parameters sensitive to distribution shift. FPF and k-FPF finetune these parameters to mitigate bias without restricting the update of task-agnostic parameters.\n\nDifferent modules in neural networks (Ramasesh et al., 2020) shows that freezing earlier layers after training the first task have little impact on the performance of the second task. This is because their unfrozen part covers the last FC layer and many BN parameters, which are the most sensitive/critical according to our empirical study. Moreover, they did not take into account that the earlier layers have much less parameters and capacity than the top layers. (Pham et al., 2022) only studies the effect of different normalization layers on CL while our method investigates the sensitivity of all parameters in different network architectures. Their continual-norm still suffers from the forgetting to task shift. Our methods directly finetune the task-specific layers on the buffered data to eliminate the bias caused by the task drift. (Zhang et al., 2019) argues different layers play different roles in the representation function. They find that in different architectures, the parameters in the top layers(close to input) are more critical and perturbing them leads to poor performance. Our empirical study is consistent with their findings in that the earlier convolutional layer is sensitive to task drift and the induced biases on them lead to catastrophic forgetting.\n\n3 PROBLEM SETUP\n\nNotations We consider the CL setting, where the model is trained on a sequence of tasks indexed by t ∈ {1, 2, . . . , T }. During each task t, the training samples (x, y) (with label y) are drawn from an i.i.d. distribution Dt. Given a neural network fΘ(·) of L layers with parameter Θ = {θl}l=1:L, θl = {θl,i}i=1:nl denote all parameters in layer-l where θl,i denotes parameter-i. On each task, fΘ(·) is trained for N epochs. We denote all parameters and the layer-l’s parameters at the end of the n-th epoch of task t by Θt\n\nl,n, n ∈ {1, . . . , N }, respectively.\n\nn and θt\n\nt=1\n\nSettings In this paper, we mainly focus on class-incremental learning (class-IL) and domainincremental learning (domain-IL). In class-IL, Dt are drawn from a subset of classes Ct and {Ct}T t=1 for different tasks are assumed to be disjoint. class-IL is a more challenging setting of CL(Van de Ven & Tolias, 2019) than task-incremental learning (task-IL) (Lopez-Paz & Ranzato, 2017). Unlike task-IL, class-IL cannot access to the task label during inference and has to distinguish among all classes from all tasks. In domain-IL, tasks to be learnt remain the same but the input data distribution Dt changes. The model is expected to adapt the domain varies, i.e. to the new domain without forgetting the old ones. The goal of the class-IL and domain-IL is: minΘ L(Θ) ≜ (cid:80)T E(x,y)∼Dt[l(y, fΘ(x))], where l is the objective function. Class-IL datasets We conduct class-IL experiments on Seq-MNIST, Seq-OrganAMNIST, SeqPathMNIST, Seq-CIFAR-10, and Seq-TinyImageNet. Seq-OrganAMNIST and Seq-PathMnist are generated by splitting OrganAMNIST or PathMNIST from MedMNIST(Yang et al., 2021), a medical image classification benchmark. CL on medical images is important in practice but also challenging since medical images always come as a stream with new patients and new deceases. Moreover, medical images of different classes might only have subtle differences that are hard to distinguish. Both Seq-OrganAMNIST and Seq-PathMnist consist of 4 disjoint classification tasks. The number of classes per task in Seq-OrganAMNIST and Seq-PathMnist are [3, 3, 3, 2] and [3, 2, 2, 2] respectively. Seq-MNIST (Seq-CIFAR-10) are generated by splitting the 10 classes in MNISTLeCun et al. (1998) (CIFAR-10Krizhevsky et al. (2009)) into five binary classification tasks. Seq-TinyImageNet partitions the 200 classes of TinyImageNet(Le & Yang, 2015) into 10 disjoint classification tasks with 20 classes per task.\n\nDomain-IL datasets We conduct domain-IL experiments on PACS dataset (Li et al., 2017), which is widely used for domain generalization. It can present more realistic domain-shift challenge than the toy-setting of PermuteMNIST (Kirkpatrick et al., 2017). Images in PACS come from seven classes and belong to four domains: Paintings, Photos, Cartoons, and Sketches. In Seq-PACS for CL, each task only focuses on one domain and the sequence of tasks is Sketches → Cartoons → Paintings → Photos (increasing the level of realism over time) (Volpi et al., 2021).\n\nModels We follow the standard network architectures adopted in most previous CL works. For Seq-MNIST, following Lopez-Paz & Ranzato (2017); Riemer et al. (2018), we employ an MLP, i.e., a fully-connected (FC) network with two hidden layers, each composed of 100 ReLU units. Following (Rebuffi et al., 2017; Li et al., 2020; Derakhshani et al., 2022), we train ResNet-18 (He\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\net al., 2016) on other five datasets. architecture, i.e., VGG-11 (Simonyan & Zisserman, 2014) on Seq-CIFAR-10.\n\nIn addition, we also extend our empirical study to another\n\n4 FORGETTING OF DIFFERENT PARAMETERS: AN EMPIRICAL STUDY\n\nA fundamental and long-lasting question in CL is how the distribution shift impacts different model parameters and why it leads to harmful forgetting. Its answer could unveil the plasticity-stability trade-off in CL, where some parameters are plastic and task-specific and thus have to be finetuned before deploying the model, while the stable ones can be shared with and generalized to new tasks. In order to answer this question, we conduct an comprehensive empirical study that compares the training dynamics of different parameters in three widely studied neural networks. We propose a novel metric measuring the sensitivity of parameters to distribution shifts. On all the studied networks, it helps us distinguish between plastic and stale parameters and allocate the task-specific ones.\n\n4.1 MEASURING FORGETTING VIA TRAINING DYNAMICS\n\nTo measure and compare the forgetting effects of different parameters, we adopt an intuitive metric to compute the change of parameters and investigate their dynamics over CL. In CL, the unstable changes of parameters are mainly caused by the task shift, while the learning within each task usually leads to smooth changes. Hence, the proposed metric focuses on the difference between two consecutive tasks, e.g., the change of parameters between epoch-n of the two tasks, i.e., (1/|θl|)∥θt+1\n\nl,n∥1. Its results on different neural networks are displayed in Fig. 1.\n\nl,n − θt\n\n4.2 FORGETTING OF DIFFERENT PARAMETERS DURING CL\n\nWe first investigate and compare the training dynamics of different parameters in three types of neural networks. To gain insights applicable to all CL methods, we exclude any specific CL techniques but simply apply SGD to train a model on a sequence of tasks, without any countermeasure to forgetting. Then, we extend the experiment to different CL methods, hyper-parameters (e.g., buffer size), and datasets to verify whether the observations still hold.\n\n(a) Dynamics of MLP\n\n(b) Dynamics of VGG-11 Figure 1: The training dynamics of different groups of parameters when applying SGD in CL to train three types of deep neural networks. Note the the y-axis is of logarithmic scale.\n\n(c) Dynamics of ResNet-18\n\nDynamics of MLP We train a three-layer MLP (including the classifier) for Seq-MNIST. Fig. 1(a) reports how the metric introduced in Section 4.1 for each layer changes in CL. It shows that the top FC layer (closest to the output) is the most sensitive one with the greatest changes among all the three layers. This is because tasks in class-IL differ on their predicted classes, which are the outputs of the FC layer. Since task shift mainly changes the top FC layer, finetuning it using all-tasks’ data help reduce the forgetting.\n\nDynamics of VGG Fig. 1 (b) shows the training dynamics of parameters in VGG-11 when trained on Seq-CIFAR10. We partition all parameters into several groups, i.e., the bottom convolutional layer (closest to the input), convolutional layers in different blocks, and three top FC layers. The observations on FC layers are consistent with those on MLP: the last FC layer in VGG is much more sensitive to the task shift than other two FC layers. In contrast, the sensitivity of all convolutional layers increases as the layer becomes closer to the input because they are producing the representations for the input images, whose distribution shift directly impacts the bottom convolutional layer. However, they are still more stable (or less plastic) than the last FC layer. Since task shift mainly changes the bottom convolutional layers (among all convolutional layers), finetuning them can be important to alleviate the forgetting.\n\n4\n\n2345Training epochs102Dynamics of parametersFC layer 1FC layer 2FC layer 3121518212427Training epochs104103102101Dynamics of parametersConv Layer 1Conv Block 1Conv Block 2Conv Block 3Conv Block 4FC layer 1FC layer 2FC layer 3121518212427Training epochs106105104103102101Dynamics of parametersConv Layer 1Conv Block 1Conv Block 2Conv Block 3Conv Block 4BN Mean&Var LayersBN Weight&Bias LayersFC layerUnder review as a conference paper at ICLR 2023\n\nDynamics of ResNet Fig. 1 (c) reports the training dynamics of parameters in ResNet-18 when trained on Seq-CIFAR10. In addition to the groups of VGG-11, ResNet-18 applies batchnormalization (BN) layers, which have two groups of parameters, i.e., (1) their weights and bias, and (2) their mean and variance. Unlike MLP or VGG-11, in ResNet-18, BN layers’ mean and variance become the most changed parameters. This observation makes intuitive sense because the mean and variance of BN layers capture the first and second order moments of the distribution for the latent representations. Except BN mean and variance, the last FC layer and the bottom convolutional layers are still the top-2 sensitive groups among the rest parameters, which are consistent with the observations on MLP and VGG-11. The variance of BN weight and bias is relatively large compared to the rest layers, please refer to Appendix for dynamics of BN weight and bias in different groups.\n\nIn the above section, we observe that for three types of deep neural networks, parameters are not equally sensitive to the distribution shift in CL. Moreover, only a small portion of them are much more sensitive and task-specific than others. This implies that only finetuning these task-specific (or plastic) parameters may suffice to retain the previous tasks. That being said, the empirical study is limited to SGD without applying any other CL techniques and only focuses on class-IL. In the following, we extend the studies to different CL methods, buffer sizes, non-standard datasets, and domain-IL, while fixing the model to be ResNet-18.\n\n(a) Seq-OrganAMNIST\n\n(b) ER(buffer size= 50)\n\n(c) ER(buffer size= 2000)\n\n(d) Seq-PACS\n\nFigure 2: The training dynamics of different groups of parameters in ResNet-18: (a) on a non-standard dataset; (b,c) using a different CL method with different buffer sizes; (d) in domain-IL setting. Note the the y-axis is of logarithmic scale.\n\nDynamics on different scenarios Fig. 2 (a) extends the empirical study to a medical image dataset Seq-OrganAMNIST. Comparing to Seq-CIFAR-10, it differs on the number of tasks, dataset size, image size, and data type. Despite these differences, the sensitive groups of parameters stays the same. We further replace SGD with ER using two replay buffer sizes, whose results are reported in Fig. 2(b)-(c). The ranking order of parameter groups in terms of sensitivity stays consistent under the change of the replay strategy and buffer size.\n\nDynamics on domain-IL In domain-IL, as shown in Fig. 2 (d), the training dynamics of different parameters is in line with our observations in class-IL: only a small portion of parameters are task-specific. However, one difference is worth noting. Since the output classes stay the same across tasks and only the input domain changes, the most sensitive parameters in class-IL, i.e., the last FC layer, becomes equally or less sensitive than the bottom convolutional layer. Hence, the plasticity and stability of parameters are impacted by how close they are to the changed data distributions.\n\n5 FORGETTING PRIORITIZED FINETUNING (FPF) METHODS\n\nThe above empirical study of the training dynamics on parameters immediately motivates a simple but novel method for CL, i.e., “forgetting prioritized finetuning (FPF)”, which can be applied to any existing CL method. In the more efficient k-FPF, we further remove the every-step replay and any other CL techniques but simply applies k-times of FPF in SGD training. In Fig. 3, we provide an illustration that compares SGD, replay-based methods and our methods.\n\nFigure 3: Comparison of SGD, replay-based method, FPF and k-FPF. SGD trains each task sequentially without replay. Replay-based methods train model on buffered data and current data simultaneously. FPF finetunes the most sensitive (plastic) parameters for a few iterations using the buffered data at the end of arbitrary CL methods. k-FPF periodically (regardless of task boundaries) applies FPF for k times over the course of training. 5\n\n1012141618202224Training epochs104103102101Dynamics of parametersConv Layer 1Conv Block 1Conv Block 2Conv Block 3Conv Block 4BN Mean&Var LayersBN Weight&Bias LayersFC layer121518212427Training epochs104103102101Dynamics of parametersConv Layer 1Conv Block 1Conv Block 2Conv Block 3Conv Block 4BN Mean&Var LayersBN Weight&Bias LayersFC layer121518212427Training epochs104103102101Dynamics of parametersConv Layer 1Conv Block 1Conv Block 2Conv Block 3Conv Block 4BN Mean&Var LayersBN Weight&Bias LayersFC layer1012141618202224Training epochs103102101Dynamics of parametersConv Layer 1Conv Block 1Conv Block 2Conv Block 3Conv Block 4BN Mean&Var LayersBN Weight&Bias LayersFC layerTask 1Task 2Task 3Task N...SGD + replay (replay-based method)Task 1Task 2Task 3Task N...SGD + replay + FPFTask 1Task 2Task 3Task N...SGDreplayFPFTask 1Task 2Task 3Task N...SGD + k-FPFUnder review as a conference paper at ICLR 2023\n\nFPF to improve CL performance. FPF applies light-weight finetuning to the most task-specific parameters using the buffered data after the training of arbitrary CL methods. Hence, it is complementary to any existing CL methods as a correction step to remove their biases in the task-specific parameters by finetuning them on the unbiased buffered data. Thereby, it can improve the performance of any existing CL methods without causing notably extra computation.\n\nk-FPF to improve CL efficiency and performance. FPF is a simple technique that brings non-trivial improvement but it is applied after the training of an existing CL method. Unfortunately, many SOTA CL methods require time-consuming replay in every step, which at least doubles the total computation. Since only a few parameters are sensitive during the task shift, can we develop a replay-free and lazy CL that replaces every-step-replay with occasional FPF? We propose k-FPF that applies FPF k times during CL as shown in Fig. 3. Without the costly experience replay, k-FPF can still achieve comparable performance as FPF+SOTA CL methods but only requires nearly half of their computation. We can apply k-FPF with any replay-free method, e.g., SGD, which is usually used as a lower-bound for CL methods. We still maintain a small buffer by reservoir sampling but it is only for FPF so SGD never accesses it. We lazily apply FPF on the buffer after every τ SGD steps (in total k times over kτ SGD steps) without knowing the task boundaries.\n\nk-FPF-CE+SGD We propose two variants of k-FPF, i.e., k-FPF-CE+SGD and k-FPF-KD+SGD. k-FPF-CE+SGD uses the cross-entropy loss to update the sensitive parameters during FPF. In this paper, k-FPF-CE refers to k-FPF-CE+SGD if not specified. The objective of FPF in k-FPF-CE is: minΘ⋆ L(Θ⋆) ≜ E(x,y)∼B[lCE(y, fΘ(x))] where Θ⋆ denotes selected groups of task-specific parameters, B refers to the buffered data and lCE is the cross-entropy loss.\n\nk-FPF-KD+SGD to further improve performance Inspired by DER (Buzzega et al., 2020), we further propose k-FPF-KD that introduces knowledge distillation (KD) (Hinton et al., 2015) to the objective in k-FPF-CE. In this paper, k-FPF-KD refers to k-FPF-KD+SGD if not specified. Same as DER, the pre-softmax responses (i.e. logits) for buffered data at training time are stored in buffer as well. During FPF, the current model is trained to match the buffered logits to retain the knowledge of previous models. The objective of FPF in k-FPF-KD is: minΘ⋆ L(Θ⋆) ≜ E(x,y)∼B[lCE(y, fΘ(x))] + λE(x,z)∼B[lM SE(z, hΘ(x))] where z is the logits of buffered sample x, lM SE refers to the mean-squared loss, hΘ(x) computes the pre-softmax logits and λ is a hyper-parameter balancing the two terms. Compared to the computation of k-FPF-CE, the additional computation by k-FPF-KD is negligible.\n\nSelection of sensitive parameters for FPF and k-FPF\n\nA key challenge in both FPF and k-FPF is to select the task-specific parameters for finetuning. Please refer to A.1 for the detailed metric of selecting sensitive parameters in different neural networks. In the experiments later, under different scenarios and on various benchmarks, we evaluate the performance of FPF and k-FPF when selecting different subsets of task-specific parameters according to the training dynamics studies in empirical study. In a nutshell, finetuning more sensitive parameters achieve more improvement, which is in line with our findings in empirical studies. For FPF, finetuning one or two the most sensitive groups of layers, like the last FC layer, is enough to achieve the best performance among all evaluated combinations. In all scenarios, FPF consistently improves CL’s performance by a large margin. For k-FPF, finetuning slightly more parameters, i.e., the earlier convolutional layers, achieves the best performance, which is comparable with that of FPF+CL. This is a price of removing replay, which halves the computational cost.\n\n6 EXPERIMENTS\n\nIn this section, to compare FPF and k-FPF with SOTA CL methods, we conduct our experiments mainly on ResNet-18. Please refer to the Appendix for the results on other neural networks. We apply FPF and k-FPF to multiple benchmark datasets and compare them with SOTA CL baselines in terms of test accuracy and efficiency. Besides, we also compare the performance of finetuning different parameters in FPF and k-FPF and show that finetuning a small portion of task-specific parameters suffices to improve CL. FPF improves SOTA CL methods by a large margin under all these scenarios while k-FPF achieves comparable performance with FPF but is more efficient.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Test accuracy (%) of CL baselines, FPF and k-FPF. “-” indicates that the algorithm is not applicable to the setting. For FPF and k-FPF, we report the best performance among all combinations of parameters in Fig. 5. k-FPF-KD applies an additional knowledge distillation loss to the finetuning objective of k-FPF-CE. Bold and Bold gray mark the best and second best accuracy.\n\nBUFFER METHODS\n\nSEQ-ORGANAMNIST\n\nSEQ-PATHMNIST\n\nSEQ-CIFAR-10\n\nSEQ-TINY-IMAGENET\n\nCLASS-IL\n\nJOINT SGD OEWC (SCHWARZ ET AL., 2018)\n\nGDUMB (PRABHU ET AL., 2020) k-FPF-CE k-FPF-KD\n\nER (RIEMER ET AL., 2018) FPF+ER\n\n200\n\nAGEM (CHAUDHRY ET AL., 2018) FPF+AGEM\n\nICARL (REBUFFI ET AL., 2017) FPF+ICARL\n\nFDR (BENJAMIN ET AL., 2018) FPF+FDR\n\nDER (BUZZEGA ET AL., 2020) FPF+DER\n\nDER++ (BUZZEGA ET AL., 2020) FPF+DER++\n\nGDUMB (PRABHU ET AL., 2020) k-FPF-CE k-FPF-KD\n\nER (RIEMER ET AL., 2018) FPF+ER\n\n500\n\nAGEM (CHAUDHRY ET AL., 2018) FPF+AGEM\n\nICARL (REBUFFI ET AL., 2017) FPF+ICARL\n\nFDR (BENJAMIN ET AL., 2018) FPF+FDR\n\nDER (BUZZEGA ET AL., 2020) FPF+DER\n\nDER++ (BUZZEGA ET AL., 2020) FPF+DER++\n\n91.92±0.46 24.19±0.15 22.71±0.67\n\n61.78±2.21 75.21±2.03 80.32±1.16\n\n71.69±1.71 77.66±1.93\n\n24.16±0.17 73.76±2.45\n\n79.61±0.56 80.24±0.70\n\n68.29±3.27 76.92±1.38\n\n73.28±1.33 79.63±1.21\n\n78.22±2.05 80.99±0.91\n\n73.29±1.82 81.28±0.71 85.16±0.67\n\n80.45±0.99 84.07±1.26\n\n24.00±0.18 79.86±0.88\n\n82.95±0.47 84.53±0.37\n\n76.62±1.81 82.32±0.91\n\n82.52±0.52 85.24±0.55\n\n84.25±0.47 85.67±0.23\n\n82.47±2.99 23.65±0.07 22.36±1.18\n\n46.31±5.64 72.88±3.22 74.68±4.72\n\n51.66±5.86 67.34±2.68\n\n27.93±4.24 67.04±4.51\n\n54.35±0.94 71.83±1.51\n\n44.27±3.20 70.08±4.06\n\n54.45±5.92 67.29±3.75\n\n62.00±3.79 68.78±2.99\n\n63.55±5.62 76.72±1.94 79.20±3.89\n\n57.54±3.05 69.83±2.87\n\n27.33±3.93 73.32±3.73\n\n57.67±1.13 74.35±4.89\n\n40.08±4.13 75.59±2.64\n\n66.71±3.40 74.80±3.45\n\n71.09±2.60 77.37±1.32\n\n81.05±1.67 19.34±0.06 18.48±0.71\n\n30.36±2.65 57.97±1.53 58.50±1.03\n\n45.71±1.44 57.68±0.76\n\n19.29±0.04 55.40±1.97\n\n59.60±1.06 63.95±0.84\n\n41.77±4.24 52.49±2.97\n\n47.04±3.03 57.25±2.19\n\n59.13±0.81 61.98±1.04\n\n42.18±2.05 64.35±0.87 66.43±0.50\n\n57.64±4.27 65.47±2.64\n\n19.47±0.03 57.84±1.98\n\n62.26±1.09 67.75±0.67\n\n43.52±1.74 63.82±0.69\n\n55.98±3.35 67.52±0.83\n\n67.06±0.31 69.09±0.74\n\n41.57±0.55 7.10±0.14 6.58±0.12\n\n2.43±0.31 13.76±0.72 14.74±0.94\n\n8.15±0.25 13.13±0.63\n\n7.22±0.15 13.24±0.54\n\n12.13±0.20 17.45±0.38\n\n8.81±0.19 12.25±0.77\n\n9.89±0.58 12.62±1.08\n\n12.12±0.69 13.78±0.57\n\n3.67±0.25 19.57±0.37 20.56±0.32\n\n10.09±0.34 18.61±0.70\n\n7.14±0.10 17.35±0.65\n\n14.81±0.37 17.37±0.35\n\n11.33±0.33 17.94±0.56\n\n11.54±0.70 17.60±0.50\n\n17.14±0.66 20.17±0.35\n\nDOMAIN-IL SEQ-PACS\n\n70.85±8.90 31.43±6.39 35.96±4.59\n\n34.16±3.45 60.70 ±2.81 63.15±1.19\n\n51.53±5.10 65.16±1.97\n\n40.54±3.43 57.33±0.76\n\n- -\n\n45.91±3.54 58.38±1.70\n\n46.93±4.94 61.49±1.37\n\n55.75±2.02 65.28±1.02\n\n43.29±2.53 65.90±0.72 66.42±2.21\n\n52.72±4.01 64.27±1.91\n\n35.29±4.94 62.40±1.89\n\n- -\n\n48.50±4.67 65.47±1.13\n\n47.63±3.85 65.69±1.66\n\n57.77±2.54 66.89±1.32\n\nImplementation Details. We follow the settings in (Buzzega et al., 2020) to train various SOTA CL methods on different datasets, except training each task for only 5 epochs, which is more practical than 50 or 100 epochs in (Buzzega et al., 2020) for the streaming setting of CL. Since the epochs are reduced, we re-tune the learning rate and hyper-parameters for different scenarios by performing a grid-search on a validation set of 10% samples drawn from the original training set.\n\nFor both FPF and k-FPF, we use the same optimizer, i.e., SGD with the cosine-annealing learning rate schedule, and finetune the selected parameters with a batchsize of 32 for all scenarios. The finetuning steps for FPF and k-FPF are 300 and 100 respectively. We perform a grid-search on the validation set to tune the learning rate and other hyper-parameters. Please refer to the Appendix for the hyper-parameters we explored.\n\nBaseline methods. We apply FPF to several SOTA memory-based CL methods: ER (Riemer et al., 2018), iCaRL (Rebuffi et al., 2017), A-GEM (Chaudhry et al., 2018), FDR (Benjamin et al., 2018), DER (Buzzega et al., 2020), and DER++(Buzzega et al., 2020). Besides, we also compare our methods with GDUMB (Prabhu et al., 2020) and oEWC (Schwarz et al., 2018). We report the test accuracy of these baseline methods and the best test accuracy of FPF and k-FPF among a few choices of finetuned parameters. We take JOINT as the upper bound for CL, which trains all tasks jointly, and SGD as the lower bound, which trains tasks sequentially without any countermeasure to forgetting. For FPF, k-FPF, and all memory-based methods, the performance with buffer size 200 and 500 are reported. All results reported in Table1 are averaged over five trials with different random seeds.\n\n6.1 MAIN RESULTS\n\nFPF considerably improves the performance of all memory-based CL methods and achieves SOTA performance over all scenarios in class-IL and domain-IL in Table 1. For methods with catastrophic forgetting, like AGEM, the accuracy of FPF increases exponentially. The surge of performance illustrates that FPF can eliminate bias by finetuning task-specific parameters to adapt to all seen tasks.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Seq-PathMNIST\n\n(b) Seq-Tiny-ImageNet\n\n(c) Seq-PACS\n\nFigure 4: Comparison of FLOPs and accuracy between FPF, k-FPF and SOTA CL methods. FPF improves all CL methods by a large margin without notably extra computation. k-FPF consumes much less computation but achieves comparable performance as FPF.\n\nk-FPF-CE replaces the costly every-step replay with efficient occasional FPF. In Table 1, the performance of k-FPF-CE on Seq-PathMNIST, Seq-Tiny-ImageNet and Seq-PACS are better than the best CL methods and its performance on Seq-OrganAMNIST and Seq-Cifar10 are also better than most CL methods, which implies that finetuning the task-specific parameters on a small number of buffer during SGD can help retain the previous knowledge and mitigate forgetting, every-step replay is not necessary. In Fig. 4, the number of training FLOPs and accuracy of different methods are reported. Compared to the training FLOPs of several CL methods, the computation cost of FPF and k-FPF-CE is almost negligible. The overall training FLOPs of k-FPF-CE is still much less than SOTA CL methods while its performance are better, which show the efficiency of k-FPF-CE.\n\nk-FPF-KD further improves the performance of k-FPF-CE to be comparable to FPF. k-FPFCE propose the efficiency of CL methods, but its performance is a bit worse than that of FPF. One of the most difference between k-FPF and FPF is the experience replay during training of CL. Inspired by DER, we propose k-FPF-KD, which uses knowledge distillation to match the outputs of previous models on buffered data, hence retaining the knowledge of previous tasks. The results of k-FPF-KD in Table 1 show that it is comparable to FPF in most scenarios. Fig. 4 shows that the FLOPs of k-FPF-KD is similar to k-FPF-CE but much less than other CL methods and FPF, and in some cases, it outperforms FPF. k-FPF-KD shows SOTA performance in both efficiency and accuracy.\n\n6.2 COMPARISON OF FINETUNING DIFFERENT PARAMETERS IN FPF AND k-FPF\n\nFPF and k-FPF get the best performance when only a small portion of task-specific parameters are finetuned. In Fig. 5, the accuracy, training FLOPs and number of trainable parameters during finetune of applying FPF or k-FPF to different task-specific parameters in ResNet-18 on Seq-PathMNIST are compared. Over all different scenarios, k-FPF only needs about half FLOPs of FPF with better performance (indicated by Red Stars). When finetuning on different task-specific parameters, FPF performs the best when BN+FC layers are finetuned, which is only 0.127% of all parameters (indicated by Orange Stars). This is consistent with our observations in empirical studies where BN and FC layers are the most sensitive parameters to distribution shift. And the results shows that only finetuning a small portion of task-specific parameters can mitigate catastrophic forgetting and generalize the model.\n\nThe phenomenon for k-FPF is a little different. (1) In the bottom plot of Fig. 5, when FC layer is not selected for finetuning in k-FPF, the performance is much worse. This is because in class-IL, the output classes change across tasks so the FC layer is trained to only output the classes for the current task (Hou et al., 2019). In contrast, when applying k-FPF to domain-IL on Seq-PACS, where the output classes keep the same for different tasks, Fig. 11 in Appendix shows that finetuning FC layer performs similarly as finetuning other parameters. Hence, the last FC layer is more sensitive (2) As the red star indicates, in class-IL than in Domain-IL. This is also shown Fig. 2 (a), (d). k-FPF needs to finetune a little more parameters (Block3 of convolutional layers, 18.91% of all parameters) to achieve a comparable accuracy with FPF. Without experience replay during SGD, the model has a larger bias on the current task and thus more task-specific parameters are needed to be finetuned. This also indicates that such bias of task-specific parameters is the main reason for catastrophic forgetting. When Block4 (75.22% of all parameters) is finetuned, since it is the most stable parameters in our empirical study, the performance of k-FPF degrades.\n\n6.3 ANALYSIS OF FPF AND k-FPF IN DIFFERENT SCENARIOS\n\n8\n\nERAGEMICARLFDRDERDER++SGDCL Methods0.00.51.01.52.0FLOPs1e150.00.20.40.60.8AccuracyERAGEMICARLFDRDERDER++SGDCL Methods0.00.20.40.60.81.0FLOPs1e160.000.050.100.150.20AccuracyERAGEMFDRDERDER++SGDCL Methods02468FLOPs1e150.00.10.20.30.40.50.60.7AccuracyFLOPs of CL MethodsFLOPs of FPF(Ours)+CL MethodsFLOPs of k-FPF-CE(Ours)FLOPs of k-FPF-KD(Ours)Accuracy of CL MethodsAccuracy of FPF(Ours)+CL MethodsAccuracy of k-FPF-CE(Ours)Accuracy of k-FPF-KD(Ours)Under review as a conference paper at ICLR 2023\n\nDifferent training FLOPs for k-FPF In Fig. 6(a), we study the trade-off between the training FLOPs and the accuracy of k-FPF on Seq-PathMNIST by changing k and the number τ in the of finetuning steps. legend refers to the interval of two consecutive FPF. Fixing k, k-FPF saturates quickly as the finetuning steps increase. This implies that k-FPF is efficient on FLOPs to achieve the best performance. For experiments with small k, e.g. k=2, though the computation required is very low, performance cannot be further improved. This implies that FPF needs to be applied on buffered samples more frequently to mitigate forgetting. When k is large, e.g., k=41 or 121, the accuracy slightly improves with the price of much more required computation. As the red star in the plot indicates, applying FPF every 1500 training steps can achieve the best computation-accuracy trade-off.\n\nFigure 5: Comparison of FLOPs, number of finetuned parameters, and accuracy for FPF(Top) and k-FPF(Bottom) finetuning different combinations of parameters. All FLOPs are normalized together to (0,1], as well as the number of finetuning parameters. “Basis” in the x-label refers to “BN+FC+CONV1”. Red stars highlight the best accuracy and show both FPF and k-FPF only require to finetune a small portion of taskspecific parameters. k-FPF halves FPF’s FLOPs.\n\n(a) FLOPs-Accuracy in k-FPF\n\n(b) Different buffer sizes and training epochs for FPF\n\nFigure 6: (a) Trade-off between FLOPs and accuracy for k-FPF with different k and τ (the SGD steps between two consecutive FPF). By increasing the finetunine steps per FPF, the accuracy quickly saturates. The best trade-off is highlighted at the top-left corner when k = 9(τ = 1500). (b) Comparison between ER and FPF+ER finetuning different parameters with different buffer sizes and number of epochs per task. In all scenarios, FPF can significantly improve ER by only finetuning BN+FC.\n\nDifferent buffer sizes and training epochs for FPF The buffer size and the training epochs per task are usually crucial in memory-based CL methods. In Fig. 6(b), when the buffer size or number of epochs increases, the performance of ER improves as well. However, increasing the buffer size brings more benefits. When the buffer size or epochs grow too large, the performance of ER seems saturate and increases slowly. For all scenarios, finetuning BN+FC layers are highly effective to alleviate the current task’s bias and promote the performance, which is consistent with our observations from the empirical studies.\n\n7 CONCLUSION\n\nWe study a fundamental problem in CL, i.e., which parts of a neural network are task-specific and more prone to catastrophic forgetting. Extensive empirical studies in diverse settings consistently show that only a small portion of parameters is task-specific and sensitive. This discovery leads to a simple yet effective “forgetting prioritized finetuning (FPF)” that only finetunes a subset of these parameters on the buffered data before model deployment. FPF is complementary to existing CL methods and can consistently improve their performance. We further replace the costly every-step replay with k-times of occasional FPF during CL to improve the efficiency. Such k-FPF achieves comparable performance as FPF+SOTA CL while consumes nearly half of its computation. In future work, we will study how to further reduce the memory size required by FPF.\n\n9\n\n0.00.20.40.60.81.0AccuracyFLOPs of ERFLOPs of FPF+ERAccuracy of ERAccuracy of FPF+ERNumber of trainable parameters during finetuneBNFCCONV1BN+FCBN+CONV1FC+CONV1BasisBasis+Block1Basis+Block2Basis+Block3Basis+Block4Basis+Block1~2Basis+Block1~3FPF Finetuned Parameters0.00.20.40.60.81.0AccuracyFLOPs of SGDFLOPs of k-FPF-CEAccuracy of k-FPF-CENumber of trainable parameters during finetune02468FLOPs1e140.700.720.740.760.780.80Accuracyk=2 (=7000)k=5 (=2500)k=9 (=1500)k=25 (=500)k=41 (=300)k=121 (=100)ERBN+FCBasisBasis+Block3Basis+Block4Basis+Block1~2Basis+Block1~30.30.40.50.60.70.8AccuracyBuffer size = 50Buffer size = 200Buffer size = 500Buffer size = 2000Buffer size = 5120ERBN+FCBasisBasis+Block3Basis+Block4Basis+Block1~2Basis+Block1~30.30.40.50.60.70.8Number of epochs = 1Number of epochs = 5Number of epochs = 10Number of epochs = 20Number of epochs = 50Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 139–154, 2018.\n\nRahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. arXiv preprint arXiv:1908.04742, 2019.\n\nAri S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in\n\nfunction space. arXiv preprint arXiv:1805.08289, 2018.\n\nMatteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone Calderara. Classincremental continual learning into the extended der-verse. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. arXiv preprint arXiv:2004.07211, 2020.\n\nRich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\n\nArslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient\n\nlifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.\n\nMohammad Mahdi Derakhshani,\n\nIvona Najdenkoska, Tom van Sonsbeek, Xiantong Zhen, Dwarikanath Mahapatra, Marcel Worring, and Cees GM Snoek. Lifelonger: A benchmark for continual disease classification. arXiv preprint arXiv:2204.05737, 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2(7), 2015.\n\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 831–839, 2019.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nYa Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain In Proceedings of the IEEE international conference on computer vision, pp.\n\ngeneralization. 5542–5550, 2017.\n\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Sequential learning for domain\n\ngeneralization. In European Conference on Computer Vision, pp. 603–619. Springer, 2020.\n\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.\n\nAdvances in neural information processing systems, 30:6467–6476, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nArun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7765–7773, 2018.\n\nMartial Mermillod, Aur ́elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects. Frontiers in psychology, 4:504, 2013.\n\nQuang Pham, Chenghao Liu, and Steven HOI. Continual normalization: Rethinking batch normalization for online continual learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=vwLLQ-HwqhZ.\n\nAmeya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions In European conference on computer vision, pp. 524–540.\n\nour progress in continual learning. Springer, 2020.\n\nVinay V Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hidden\n\nrepresentations and task semantics. arXiv preprint arXiv:2007.07400, 2020.\n\nRoger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and\n\nforgetting functions. Psychological review, 97(2):285, 1990.\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\nicarl: In Proceedings of the IEEE conference on\n\nIncremental classifier and representation learning. Computer Vision and Pattern Recognition, pp. 2001–2010, 2017.\n\nMatthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018.\n\nAnthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):\n\n123–146, 1995.\n\nJonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International Conference on Machine Learning, pp. 4528–4537. PMLR, 2018.\n\nJoan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In International Conference on Machine Learning, pp. 4548–4557. PMLR, 2018.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nGido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint\n\narXiv:1904.07734, 2019.\n\nJeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software\n\n(TOMS), 11(1):37–57, 1985.\n\nRiccardo Volpi, Diane Larlus, and Gr ́egory Rogez. Continual adaptation of visual representations via domain randomization and meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4443–4453, 2021.\n\nJiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification. arXiv preprint arXiv:2110.14795, 2021.\n\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\n\nIn International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017.\n\nChiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? arXiv preprint\n\narXiv:1902.01996, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 SELECTION OF THE SENSITIVE PART OF DIVERSE TYPES OF NEURAL NETWORKS\n\nWe select sensitive parameters according to their training dynamics in the early epochs. Examples of the training dynamics for different layers are shown in Fig 1-2 and their ranking does not change over epochs. Specifically, we sort layers by their training dynamics values in descent order. Then we greedily add layers one after another to the sensitive group until the sum of all selected layers’ training dynamics exceeds p percent of the sum for all layers, where p is a hyper-parameter. In our experiments, for models with the batch-norm layer like ResNet-18, FPF and k-FPF outperforms all baselines when p = 97, when only 18.90% of parameters in the neural network are regarded as sensitive parameters. For other models like MLP and VGG-11, p = 70 and only 1.12% and 0.32% of parameters are regarded as sensitive parameters.\n\nA.2 COMPARISON BETWEEN FPF AND THE METHOD FINE-TUNING ALL PARAMETERS\n\nIn Tab.2, we compare FPF with FPF-ALL (which finetunes all parameters) when applied to different CL methods for two types of CL, i.e., class-IL and domain-IL. The results shows that FPF consistently achieve comparable or slightly higher accuracy than FPF-ALL by spending significantly less FLOPs. This demonstrates the advantage of FPF on efficiency.\n\nTable 2: Comparison of accuracy and FLOPs between FPF and FPF-ALL(finetuning all parameters).\n\nMethods\n\nSeq-PathMNIST\n\nSeq-PACS\n\nAccuracy\n\nFLOPs(B)\n\nAccuracy\n\nFLOPs(B)\n\n76.72±1.94 k-FPF-CE 75.74±2.91 k-FPF-ALL-CE 69.83±2.87 FPF+ER 70.64±4.00 FPF-ALL+ER 73.32±3.73 FPF+AGEM FPF-ALL+AGEM 74.80±3.12 74.35±4.89 FPF+iCaRL 72.77±4.12 FPF-ALL+iCaRL 75.59±2.64 FPF+FDR 74.24±1.48 FPF-ALL+FDR 74.80±3.45 FPF+DER 74.54±3.19 FPF-ALL+DER 77.37±1.32 FPF+DER++ 77.16±1.45 FPF-ALL+DER++\n\n21.35 43.95 4.68 8.79 7.07 8.79 4.27 8.79 2.94 8.79 2.96 8.79 4.68 8.79\n\n65.90±0.72 64.48±2.23 64.27±1.91 63.81±2.33 62.40±1.89 62.65±1.65 -\n- 65.47±1.13 64.88±2.28 65.69±1.66 66.22±0.87 66.89±1.32 65.19±1.33\n\n148.25 174.60 24.39 34.92 18.47 34.92 -\n- 11.70 34.92 18.47 34.92 24.39 34.92\n\nA.3 EXPERIMENTS ON THE TASK SEQUENCE CONTAINING TOTALLY DIFFERENT DATASETS\n\nWe concatenate the CL tasks from two datasets (i.e., Seq-CIFAR-10 and Seq-PathMNIST) to form a twice-longer task sequence and evaluate the training dynamics of different groups of parameters. The result is shown in Fig.7. The vertical dashed line at epochs = 30 is the boundary between the two datasets. Although the two datasets are from different domains and thus have different distributions, the sensitivity of parameters stays consistent with our observations on tasks from a single dataset. This indicates that the sensitive parameters almost do not change across different tasks/datasets so they can be identified at very early stages.\n\nA.4 A MORE CLEAR VERSION OF FIG. 4 AND FIG.5\n\nIn Fig.8 and Fig.9, to make Fig.4 and Fig.5 more concise and easy to understand, we draw the barplots of different parts separately.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: The training dynamics of different groups of parameters in ResNet-18 when sequentially training Seq-CIFAR-10 and Seq-PathMNIST.\n\nA.5 PERFORMANCE OF VARIOUS METHODS DURING THE TRAINING OF CL\n\nIn Tab. 3 and Tab. 4, the average test accuracy of previous tasks at the end of each task during the training of CL on Seq-PathMNIST and Seq-PACS is reported. The results show that during training, k-FPF can always achieve the best performance among various CL methods. Whenever the training stops, k-FPF can always achieve a model performing well on previous tasks.\n\nTable 3: The average accuracy of previous tasks at the end of each task during the training of CL on SeqPathMNIST.\n\nMethods\n\nTask 1\n\nTask 2\n\nTask 3\n\nTask 4\n\nk-FPF-CE ER AGEM iCaRL FDR DER DER++\n\n99.95±0.04 98.62±1.59 99.71±0.19 99.98±0.02 99.97±0.06 99.98±0.02 99.95±0.06\n\n95.41±1.98 83.06±3.12 46.58±3.13 86.86±5.47 48.06±0.82 91.92±3.42 94.06±6.14\n\n81.92±2.26 74.60±3.18 36.12±3.17 66.62±5.64 55.75±6.55 76.50±5.77 80.35±3.32\n\n76.72±1.94 57.54±3.05 27.33±3.93 57.67±1.13 40.08±4.13 66.71±3.40 71.09±2.60\n\nTable 4: The average accuracy of previous tasks at the end of each task during the training of CL on Seq-PACS.\n\nMethods\n\nTask 1\n\nTask 2\n\nTask 3\n\nTask 4\n\nk-FPF-CE ER AGEM FDR DER DER++\n\n70.94±2.02 56.64±9.04 47.34±7.35 58.59±4.36 48.49±9.40 55.33±7.45\n\n73.75±2.68 54.34±9.44 38.02±5.82 54.00±4.01 45.28±8.88 64.43±6.50\n\n62.37±0.49 46.79±8.48 32.70±7.13 46.38±4.80 34.48±7.81 50.19±7.30\n\n65.90±0.72 52.72±4.01 35.29±4.94 48.50±4.67 47.63±3.85 57.77±2.54\n\nA.6 DETAILED DYNAMICS OF BN WEIGHTS AND BIAS IN DIFFERENT GROUPS\n\nIn Fig. 10, the training dynamics of BN weights and biases in different groups are reported. This provides a fine-grained explanation to the phenomenon in Fig. 1 (c): the bottom BN layer is much more sensitive and task-specific than other BN layers. Consistent with convolutional layers, the deep BN layers are less sensitive to task drift than the shallower ones.\n\nIn a neural network, lower layers are closer to the input. Since the distribution of the inputs changes, the parameters of lower convolutional layers change sensitively to adapt to the distribution shift. The weights and biases of BN, which are the scale and shift of the featuremap, will change along\n\n13\n\n101520253035404550Training epochs105104103102101Dynamics of parametersConv Layer 1Conv Block 1Conv Block 2Conv Block 3Conv Block 4BN Mean&Var LayersBN Weight&Bias LayersFC layerUnder review as a conference paper at ICLR 2023\n\n(a) Seq-PathMNIST\n\n(b) Seq-Tiny-ImageNet\n\n(c) Seq-PACS\n\nFigure 8: Comparison of FLOPs and accuracy between FPF, k-FPF and SOTA CL methods. FPF improves all CL methods by a large margin without notably extra computation. k-FPF consumes much less computation but achieves comparable performance as FPF.\n\n14\n\nERAGEMICARLFDRDERDER++SGDCL Methods0.00.51.01.52.0FLOPs1e15FLOPs of CL MethodsFLOPs of FPF(Ours)+CL MethodsFLOPs of k-FPF-CE(Ours)FLOPs of k-FPF-KD(Ours)ERAGEMICARLFDRDERDER++SGD0.00.20.40.60.8AccuracyAccuracy of CL MethodsAccuracy of FPF(Ours)+CL MethodsAccuracy of k-FPF-CE(Ours)Accuracy of k-FPF-KD(Ours)ERAGEMICARLFDRDERDER++SGDCL Methods0.00.20.40.60.81.0FLOPs1e16FLOPs of CL MethodsFLOPs of FPF(Ours)+CL MethodsFLOPs of k-FPF-CE(Ours)FLOPs of k-FPF-KD(Ours)ERAGEMICARLFDRDERDER++SGD0.000.050.100.150.20AccuracyAccuracy of CL MethodsAccuracy of FPF(Ours)+CL MethodsAccuracy of k-FPF-CE(Ours)Accuracy of k-FPF-KD(Ours)ERAGEMFDRDERDER++SGDCL Methods02468FLOPs1e15FLOPs of CL MethodsFLOPs of FPF(Ours)+CL MethodsFLOPs of k-FPF-CE(Ours)FLOPs of k-FPF-KD(Ours)ERAGEMFDRDERDER++SGD0.00.10.20.30.40.50.60.7AccuracyAccuracy of CL MethodsAccuracy of FPF(Ours)+CL MethodsAccuracy of k-FPF-CE(Ours)Accuracy of k-FPF-KD(Ours)Under review as a conference paper at ICLR 2023\n\nFigure 9: Comparison of FLOPs, number of finetuned parameters, and accuracy for FPF(Top) and kFPF(Bottom) finetuning different combinations of parameters. All FLOPs are normalized together to (0,1], as well as the number of finetuning parameters. “Basis” in the x-label refers to “BN+FC+CONV1”. Red stars highlight the best accuracy and show both FPF and k-FPF only require to finetune a small portion of taskspecific parameters. k-FPF halves FPF’s FLOPs.\n\nwith the convolutional parameters to adjust the distribution of the output featuremap. In the deeper layers, the functionality of each filter is relatively stable, so the distribution of the featuremap need not change drastically.\n\nFigure 10: The training dynamics of different groups of BN weights and biases in ResNet-18.\n\nA.7 RESULTS OF OTHER NEURAL NETWORKS\n\nIn the Tab. 5, the results of various CL benchmarks and FPF on MLP and VGG-11 are reported. Similar to the results in Tab.1, by finetuning the most sensitive parameters in MLP and VGG-11, FPF can further improve the performance of all SOTA CL methods and achieve the best performance. k-FPF-CE also achieves comparable performance as FPF + SOTA methods. Our methods can generalize to various neural networks.\n\n15\n\n0.00.10.20.30.40.50.60.7AccuracyAccuracy of ERAccuracy of FPF+ER5.05.15.25.35.45.55.6FLOPs1e15FLOPs of ERFLOPs of FPF+ERBNFCCONV1BN+FCBN+CONV1FC+CONV1BasisBasis+Block1Basis+Block2Basis+Block3Basis+Block4Basis+Block1~2Basis+Block1~3FPF Finetuned Parameters012345678Number of Parameters1e6Number of trainable parameters during finetune of FPF0.00.10.20.30.40.50.60.70.8Accuracy of k-FPF-CE2.02.22.42.62.83.03.21e15FLOPs of SGDFLOPs of k-FPF-CEBNFCCONV1BN+FCBN+CONV1FC+CONV1BasisBasis+Block1Basis+Block2Basis+Block3Basis+Block4Basis+Block1~2Basis+Block1~3FPF Finetuned Parameters0123456781e6Number of trainable parameters during finetune of k-FPF1012141618202224Training epochs104103Dynamics of parametersBN Layer 1BN Weight&Bias Block 1BN Weight&Bias Block 2Bn Weight&Bias Block 3Bn Weight&Bias Block 4Under review as a conference paper at ICLR 2023\n\nTable 5: Classification results for CL benchmarks and FPF on MLP and VGG-11. Bold and underline indicate the best and second best algorithms in each setting.\n\nBUFFER METHODS\n\nSEQ-MNIST(MLP)\n\nSEQ-CIFAR10(VGG-11)\n\nCLASS-IL\n\nJOINT SGD OEWC\n\n95.58±0.33 19.64±0.07 20.69±1.34\n\n90.60±0.37 GDUMB 90.63±0.57 k-FPF-CE 86.73±1.03 ER 91.15±0.16 FPF+ER 51.03±4.94 AGEM FPF+AGEM 89.26±0.52 58.12±1.94 ICARL 80.83±0.49 FTF+ICARL 83.79±4.15 FDR 89.67±0.37 FPF+FDR 91.17±0.94 DER 91.25±0.89 FPF+DER 91.18±0.74 DER++ 91.22±0.67 FTF+DER++\n\n500\n\n69.50±0.73 18.71±0.33 18.46±0.23\n\n41.65±0.78 55.45±1.16 46.27±1.18 53.48±1.08 19.40±1.09 29.84±1.37 45.63±1.94 48.03±0.65 45.56±2.23 55.59±1.56 51.12±2.47 57,46±1.15 47.60±3.23 54.69±0.73\n\nA.8 PERFORMANCE OF FINETUNING DIFFERENT PARAMETERS FOR FPF AND K-FPF ON\n\nDOMAIN-IL DATASET\n\nIn Figure 11, the performance of finetuning different parameters for FPF and k-FPF on domain-IL dataset Seq-PACS are reported.\n\nA.9 COMPARISON WITH RELATED WORKS (RAMASESH ET AL., 2020)\n\nPaper “Anatomy of catastrophic forgetting: Hidden representations and task sementics” shows that freezing bottom layers had little impact on the performance of the second task. (i) Their setting is different: our study and most CL methods focus on the performance of ALL tasks. And it is unfair in terms of parameter amount to compare freezing effects of multiple layers/blocks (e.g., block 1-3) vs. one layer/block. (ii) Their result is partially consistent with ours since their unfrozen part covers the last layer and many BN parameters, which are the most sensitive/critical part to finetune in our paper. (iii) The rest difference is due to our finer-grained study on parameters and on > 2 tasks but this paper only studies two tasks and focuses on the second. Table 6 shows the class-IL accuracy at the end of each task if freezing different single ResNet block (bottom to top: block-1 to block-4). At the end of task-2, our observation is the same as this paper and freezing bottom blocks showing little reduction of accuracy. However, at the end of task 3-5, their performance drops and freezing block-1 drops most.\n\nA.10 HYPER-PARAMETER SEARCH SPACE\n\nIn the following, we provide a list of all the hyper-parameter combinations that were considered for FPF and k-FPF.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: Comparison of FLOPs, number of finetuned parameters, and accuracy for FPF(Top) and kFPF(Bottom) finetuning different combinations of parameters. All FLOPs are normalized together to (0,1], as well as the number of finetuning parameters. “Basis” in the x-label refers to “BN+FC+CONV1”. Red stars highlight the best accuracy and show both FPF and k-FPF only require to finetune a small portion of task-specific parameters. k-FPF halves FPF’s FLOPs. Different from the results of k-FPF in class-IL, in Seq-PACS, since the output classes for different tasks are always the same, the last FC layer will not have a large bias on particular classes. Only finetuning BN or CONV1 layers for k-FPF can get comparable performance with ER. Similar to class-IL, since experience replay is not allowed during the training of CL method SGD, a little more parameters are required to be finetuned by k-FPF to get comparable performance with FPF (about 24.92% of all parameters).\n\n17\n\n0.00.20.40.60.81.0AccuracyFLOPs of ERFLOPs of FPF+ERAccuracy of ERAccuracy of FPF+ERNumber of trainable parameters during finetuneBNFCCONV1BN+FCBN+CONV1FC+CONV1BasisBasis+Block1Basis+Block2Basis+Block3Basis+Block4Basis+Block1~2Basis+Block1~3FPF Finetuned Parameters0.00.20.40.60.81.0AccuracyFLOPs of SGDFLOPs of k-FPF-CEAccuracy of k-FPF-CENumber of trainable parameters during finetuneUnder review as a conference paper at ICLR 2023\n\nTable 6: class-IL accuracy of ER at the end of each task on Seq-CIFAR-10\n\nTask-1\n\nTask-2\n\nTask-3\n\nTask-4\n\nTask-5\n\n97.52 ± 0.23 80.53 ± 0.80 63.96 ± 0.51 58.05 ± 1.91 57.03 ± 2.29 No Freeze Freeze conv-1 97.52 ± 0.23 79.62 ± 2.75 63.28 ± 2.13 56.11 ± 0.61 55.58 ± 1.31 Freeze block-1 97.52 ± 0.23 78.88 ± 3.01 60.07 ± 0.61 55.49 ± 0.22 52.75 ± 1.90 Freeze block-2 97.52 ± 0.23 78.93 ± 3.34 63.78 ± 2.32 56.23 ± 0.82 56.55 ± 3.17 Freeze block-3 97.52 ± 0.23 80.37 ± 2.35 64.31 ± 2.23 57.21 ± 0.40 56.52 ± 0.76 Freeze block-4 97.52 ± 0.23 80.68 ± 1.53 64.89 ± 1.00 53.78 ± 3.37 54.01 ± 2.07\n\nTable 7: The hyper-parameter search space for FPF on different datasets. For all experiments of FPF, we use the same number of batch size 32 and finetuning steps 300. The hyper-parameter spaces of finetuning different parameters in the models generated by different CL methods are always same for a given dataset. ft-lr refers to the learning rate during finetuning of FPF.\n\nDataset\n\nHyper-parameter Values\n\nSeq-OrganAMNIST Seq-PathMNIST Seq-CIFAR-10 Seq-Tiny-ImageNet Seq-PACS\n\nlr lr lr lr lr\n\n[1, 0.3, 0.1, 0.03, 0.01] [1, 0.75, 0.3, 0.05, 0.03] [1, 0.3, 0.1, 0.03, 0.01] [1, 0.5, 0.3, 0.075, 0.05] [1, 0.5, 0.3, 0.05, 0.03, 0.005, 0.003]\n\nTable 8: The hyper-parameter search space for k-FPF-SGD on different datasets. For all experiments of kFPF-SGD, we use the same number of batch size 32 and finetuning steps 100. The hyper-parameter spaces of finetuning different parameters are always same for a given dataset. lr refers to the learning rate during training of CL method SGD. ft-lr refers to the learning rate during finetuning.\n\nDataset\n\nHyper-parameter Values\n\nSeq-OrganAMNIST\n\nSeq-PathMNIST\n\nSeq-CIFAR-10\n\nSeq-Tiny-ImageNet\n\nSeq-PACS\n\n[0.2, 0.15, 0.1, 0.075] [0.5, 0.2, 0.15, 0.1]\n\n[0.05, 0.03, 0.01] [0.1, 0.075, 0.05, 0.03, 0.01]\n\n[0.05, 0.03, 0.01] [0.075, 0.05, 0.03, 0.01]\n\n[0.075, 0.05, 0.03] [0.1, 0.075, 0.05]\n\n[0.05, 0.03, 0.01] [0.075, 0.05, 0.03, 0.0075]\n\nlr ft-lr\n\nlr lr\n\nlr ft-lr\n\nlr ft-lr\n\nlr ft-lr\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: The hyper-parameter search space for k-FPF-KD on different datasets. For all experiments of kFPF-KD, we use the same number of batch size 32 and finetuning steps 100. The hyper-parameter spaces of finetuning different parameters are always same for a given dataset. lr refers to the learning rate during training of CL method SGD. ft-lr refers to the learning rate during finetuning. λ is the hyper-parameter to balance the two losses.\n\nDataset\n\nHyper-parameter Values\n\nSeq-OrganAMNIST\n\nSeq-PathMNIST\n\nSeq-CIFAR-10\n\nSeq-Tiny-ImageNet\n\nSeq-PACS\n\nlr ft-lr λ\n\nlr lr λ\n\nlr ft-lr λ\n\nlr ft-lr λ\n\nlr ft-lr λ\n\n[0.2, 0.15, 0.1, 0.075] [0.5, 0.2, 0.15, 0.1] [1, 0.5, 0.2, 0.1]\n\n[0.05, 0.03, 0.01] [0.1, 0.075, 0.05, 0.03, 0.01] [1, 0.5, 0.2, 0.1]\n\n[0.05, 0.03, 0.01] [0.075, 0.05, 0.03, 0.01] [0.5, 0.2, 0.1]\n\n[0.075, 0.05, 0.03]] [0.1, 0.075, 0.05] [1, 0.5, 0.2]\n\n[0.05, 0.03, 0.01] [0.075, 0.05, 0.03, 0.0075] [1, 0.5 0.2 0.1]\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a simple technique for preventing the catastrophic forgetting problem in continual learning (CL). By showing that there are sensitive parts in neural network model when learning a novel task, the proposed approach (FPF) just fine-tune the sensitive part of a model after learning sequence of tasks. Furthermore, the additional computation cost for performing FPF the sensitive part is somewhat marginal. In the experiments, authors show that applying FPF can drastically increase the performance of baselines.\n\n# Strength And Weaknesses\n\n**Pros:**\n\nP1. FPF is simple and highly effective, and showing sensitive regions in the parameters of neural network in CL can be a novel finding. \n\nP2. FPF can be easily plugged-in baselines, and it increases the performance significantly. \n\n**Cons:**\n\nC1. In Section 4.1, the notation in the denominator of the metric for the change of parameters is somewhat confusing. Is it $\\theta_{\\ell,n}^{t+1}$? If it is, is it feasible to use this metric for measuring the dynamics of parameters? \n\nC2. Selecting the sensitive part of the neural network is little bit confusing. How can we select the parts when we use much larger architectures (e.g. ResNet-110)? It would be better to specify the standard for selecting the sensitive region.\n\nC3. There is no comparison between FFP and the method that fine-tune all parameters using memory buffer. I think the results are quite different from \"ER\". It would be better to show the comparison between two methods with respect to the FLOPs and the average accuracy. \n\nC4. There is no experiments on the task sequence containing totally different datasets (e.g. CIFAR-100 & MNIST). The trends that Conv & FC layers are sensitive would be different from the experiments in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is somewhat clear, but some notations are confusing. Though the methods are simple and might not be novel, showing the sensitive region of neural network in CL can be novel.\n\n# Summary Of The Review\n\nI vote to marginally accept this paper. Though some experiments are missing, and the proposed solutions are somewhat straightforward, finding out which parts are sensitive to the distribution shift in CL has significant contribution.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWINDOW-BASED DISTRIBUTION SHIFT DETECTION FOR DEEP NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTo deploy and operate deep neural models in production, the quality of their predictions, which might be contaminated benignly or manipulated maliciously by input distributional deviations, must be monitored and assessed. Specifically, we study the case of monitoring the healthy operation of a deep neural network (DNN) receiving a stream of data, with the aim of detecting input distributional deviations over which the quality of the network’s predictions is potentially damaged. Using selective prediction principles, we propose a distribution deviation detection method for DNNs. The proposed method is derived from a tight coverage generalization bound computed over a sample of instances drawn from the true underlying distribution. Based on this bound, our detector continuously monitors the operation of the network over a test window and fires off an alarm whenever a deviation is detected. This novel detection method consistently and significantly outperforms the state of the art with respect to the CIFAR-10 and ImageNet datasets, thus establishing a new performance bar for this task , while being substantially more efficient in time and space complexities.\n\n1\n\nINTRODUCTION\n\nA wide range of artificial intelligence applications and services rely on deep neural models because of their remarkable accuracy. When a trained model is deployed in production, its operation should be monitored for abnormal behavior, and a flag should be raised if such is detected. Corrective measures can be taken if the underlying cause of the abnormal behavior is identified. For example, simple distributional changes may only require retraining with fresh data, while more severe cases may require redesigning the model (e.g., when new classes emerge).\n\nIn this paper we focus on distribution shift detection in the context of deep neural models and consider the following setting. Pretrained model f is given, and we presume it was trained with data sampled from some distribution P . In addition to the dataset used in training f , we are also given an additional sample of data from P , which is used to train a detector D (we refer to this as the detection-training dataset). While f is used in production to process a stream of emerging input data, we continually feed D with the most recent window Wk of k input elements. The detector also has access to the final layers of the model f and should be able to determine whether the data contained in Wk came from a distribution different from P . Detection algorithms based on a window, such as we consider here, have rarely been considered in the context of deep neural networks. To the best of our knowledge window-based deep detection has only been considered by (Rabanser et al., 2019). We emphasize that in this paper we are not considering the problem of identifying single-instance out-of-distribution or outlier instances (Liang et al., 2018; Hendrycks & Gimpel, 2017; Hendrycks et al., 2019; Golan & El-Yaniv, 2018; Ren et al., 2019; Nalisnick et al., 2019; Nado et al., 2021; Fort et al., 2021), but rather the information residing in a population of k instances. Single-instance methods are trivially applicable to a window. However, these methods are not designed to detect population-based changes (see discussion in Section 2). We also note that this paper does not address the issue of characterizing the type of distribution shift, nor correcting it (by “redesigning” the model to make accurate predictions on the shifted distribution).\n\nThe detection of distribution shifts is a fundamental topic in machine learning and statistics, and the standard method for tackling it is by performing a dimensionality reduction over both the detectiontraining (source) and test (target) samples, and then applying a two-sample statistical test over these\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nreduced representations to detect a deviation. This is further discussed in Section 2. Distribution shift detection has been scarcely considered in the context of deep neural networks (DNNs). Deep models can benefit from the semantic representation created by the model itself, which provides meaningful dimensionality reduction that is readily available at the last layers of the model. Using the embedding layer (or softmax) along with statistical two-sample tests was recently proposed by (Lipton et al., 2018) and (Rabanser et al., 2019) who termed solutions of this structure black-box shift detection (BBSD). Using both the univariate Kolmogorov-Smirnov (KS) test and the maximum mean discrepancy (MMD) method, see details below, (Rabanser et al., 2019) achieve impressive detection results when using MNIST and CIFAR-10 as proxies for the distribution P . As we demonstrate here, the KS-BBSD method is also very effective over ImageNet when a stronger model is used (EfficientNet vs ResNet-18). BBSD methods have the disadvantage of being computationally intensive due to the use of two-sample tests between the detection-training set (which can, and are preferred to be the largest possible) and the window W (a complexity analysis is provided in table 1).\n\nWe propose a different approach based on selective prediction (El-Yaniv & Wiener, 2010; Geifman & El-Yaniv, 2017), where a model quantifies its prediction uncertainty and abstains from predicting uncertain instances. First, we develop a method for selective prediction with guaranteed coverage. This method identifies the best abstaining threshold and coverage bound for a given pretrained classifier f , such that the resulting empirical coverage will not violate the bound with a high probability (when abstention is determined using the threshold). The guaranteed coverage method is of independent interest, and it is analogous to selective prediction with guaranteed risk (Geifman & El-Yaniv, 2017). Because the empirical coverage of such a classifier is highly unlikely to violate the bound if the underlying distribution remains the same, a systematic violation indicates a shift in distribution. To be more specific, given a detection-training sample Sm, our coverage-based detection algorithm computes log2 m tight generalization coverage bounds, which are then used to detect a distribution shift in a window W of test data. Due to its aggressive reduction of Sm to O(log m) numbers, the proposed detection algorithm is extremely efficient in its computation requirements, unlike the baseline algorithms mentioned above, which follow the framework depicted in Figure 3 in Appendix 7.1. For example, consider the JFT-3B dataset (Zhai et al., 2021). Previous methods that require the processing of this set for each incoming window are infeasible, while our method allows one to summarize it with only 32 scalars.\n\nIn a comprehensive empirical study, we compared our coverage-based detection algorithm with the best-performing BBSD baselines, including the KS approach of (Rabanser et al., 2019). All methods used the same underlying models (ResNet-18, ResNet-50 and EfficientNet) for a fair comparison. We simulated source distributions using both the CIFAR-10 and ImageNet databases. Distribution shifts were produced using various methods, beginning with simple noise and ending with adversarial examples. Based on these experiments, we can claim that our coverage-based detection method is significantly more powerful than the baselines across a wide range of test window sizes. To summarize, the contributions of this paper are: (1) A theoretically justified algorithm (Algorithm 1), that produces a coverage bound, which is of independent interest, and allows for the creation of selective classifiers with guaranteed coverage. (2) A theoretically motivated “windowed” detection algorithm (Algorithm 2), which detects a distribution shift over a window. (3) A comprehensive empirical study demonstrating significant improvements relative to existing baselines over a variety of datasets and architectures.\n\n2 RELATED WORK\n\nDistribution shift detection methods often comprise the following two steps: dimensionality reduction, and a two-sample test between the detection-training sample and test samples. In most cases, these methods are “lazy” in the sense that for each test sample, they make a detection decision based on a computation over the entire detection-training sample. Their performance will be sub-optimal if only a subset of the train sample is used. Figure 3 in Appendix 7.1 illustrates this general framework.\n\nThe use of dimensionality reduction is optional. It can often improve performance by focusing on a less noisy representation of the data. Dimensionality reduction techniques include no reduction, principal components analysis (Wold et al., 1987), sparse random projection (Bingham & Mannila, 2001), autoencoders (Rumelhart et al., 1985; Pu et al., 2016), domain classifiers, (Rabanser et al., 2019) and more. In this work we focus on black box shift detection (BBSD) methods (Lipton et al., 2018), that rely on deep neural representations of the data generated by a pretrained model. The representation we extract from the model will typically utilize either the softmax outputs\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n(acronymed BBSD-S) or the embeddings (acronymed BBSD-E). Due to the dimensionality of the final representation, multivariate or multiple univariate two-sample tests can be conducted.\n\nBy combining BBSD-S with a Kolmogorov-Smirnov (KS) statistical test (Massey Jr, 1951) and using the Bonferroni correction (Bland & Altman, 1995), (Rabanser et al., 2019) achieved state-of-the-art results in distribution shift detection in the context of image classification (MNIST and CIFAR-10). We acronym their method as KS-BBSD-S. The univariate KS test processes individual dimensions separately; its statistic is calculated by computing the largest difference Z of the cumulative density functions (CDFs) across all dimensions as follows: Z = sup |FP (z) − FQ(z)|, where FQ and FP\n\nare the empirical CDFs of the detection-training and test data (which are sampled from P and Q, respectively; see Section 3). The Bonferroni correction rejects the null hypothesis when the minimal p-value among all tests is less than α d , where α is the significance level of the test, and d is the number of dimensions. Although there have been several less conservative approaches to aggregation (Heard & Rubin-Delanchy, 2018; Loughin, 2004), they usually assume some dependencies among the tests.\n\nz\n\n1, x′\n\nThe maximum mean discrepancy (MMD) method (Gretton et al., 2012) is a kernel-based multivariate test that can be used to distinguish between probability distributions P and Q. Formally, M M D2(F, P, Q) = ||μP − μQ||2 F 2, where μP and μQ are the mean embeddings of P and Q in a reproducing kernel Hilbert space F. Given a kernel K, and samples, {x1, x2, . . . , xm} ∼ P m and {x′ k} ∼ Qk, an unbiased estimator for M M D2 can be found in (Gretton et al., 2012; Serfling, 2009). (Sutherland et al., 2017) and (Gretton et al., 2012) used the RBF kernel K(x, x′) = e− 1 2, where 2σ2 is set to the median of the pairwise Euclidean distances between all samples. By performing a permutation test on the kernel matrix, the p-value is obtained. In our experiments (see Section 5), we thus use four baselines: KS-BBSD-S, KS-BBSD-E, MMD-BBSD-S, and MMD-BBSD-E.\n\n2σ2 ||x−x′||2\n\n2, . . . , x′\n\nAs mentioned in the introduction, our work is complementary to the topic of single-instance outof-distribution (OOD) detection (Liang et al., 2018; Hendrycks & Gimpel, 2017; Hendrycks et al., 2019; Golan & El-Yaniv, 2018; Ren et al., 2019; Nalisnick et al., 2019; Nado et al., 2021; Fort et al., 2021). Obviously, these methods can be applied in a trivial manner over a window, by applying the detector to each instance in the given window. However, these methods typically do not consider the population statistics over the window. Interestingly, we demonstrate in Section 5.1, Figure 2 that near perfect performance can be achieved by (our) window-based detection when the window is sufficiently large. For instance, a detection AUROC score close to 100% can be obtained over a 1K window when considering various distributional changes such as adversarial attacks, Gaussian noise, and more. OOD detection methods rarely achieve such a near-perfect AUROC score when considering such distributional changes.\n\nFinally, we mention (Geifman & El-Yaniv, 2017) who developed a risk generalization bound for selective classifiers (El-Yaniv & Wiener, 2010). The bound presented in that paper is analogous to the coverage generalization bound we present in Theorem 4.2. The risk bound in (Geifman & El-Yaniv, 2017) can also be used for shift-detection. To apply their risk bound to this task, however, labels, which are not available, are required. Our method (Section 4) detects distribution shifts without using any labels.\n\n3 PROBLEM FORMULATION\n\nWe consider the problem of detecting distribution shifts in input streams provided to pretrained deep neural models. Let P ≜ PX denote a probability distribution over an input space X , and assume that a model f has been trained on a set of instances drawn from P. Samples drawn from P are referred to as in-distribution (ID), or detection-training data. Consider a setting where the model f is deployed and while being used in production its input distribution might change or even be attacked by an adversary. Our goal is to detect such events to allow for appropriate action, e.g., retraining the model with respect to the revised distribution.\n\nInspired by (Rabanser et al., 2019), we formulate this problem as follows. We are given a pretrained model f (whose ID training data was sampled from P). Then having f and additional ID detectiontraining data Sm ∼ P m (possibly unlabeled), we would like to train a detection model to be able to detect a distribution shift; namely, discriminate between windows containing ID data, and alternative-\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ndistribution (AD) data. Thus, given an unlabeled test sample window Wk ∼ Qk, where Q is a possibly different distribution, the objective is to determine whether P ̸= Q. We also ask what is the smallest test sample size k required to determine that P ̸= Q. Since typically the training set Sm can be quite large, we further ask whether it is possible to devise an effective detection procedure whose time complexity is o(m).\n\n4 PROPOSED METHOD – COVERAGE-BASED DETECTION\n\nIn this section we present a novel technique for detecting a distribution shift based on selective prediction principles (definitions follow). We develop a tight generalization coverage bound, based on ID training set sampled i.i.d. from the source distribution. This bound should hold with high probability for ID data from the source distribution. For a given window of data at test time we calculate its empirical coverage, and compare it to the theoretical coverage bound. Since the bound should hold with high probability on ID data, a coverage violation indicates w.h.p. a distribution shift from the ID source.\n\n4.1 SELECTION WITH GUARANTEED COVERAGE\n\nWe begin by introducing basic selective prediction terminology and definitions that are required to describe our method. Consider a standard multiclass classification problem, where X is some feature space (e.g., raw image data) and Y is a finite label set, Y = {1, 2, 3, ..., C}, representing C classes. Let P (X, Y ) be a probability distribution over X × Y, and define a classifier as a function f : X → Y. We refer to P as the source distribution. A selective classifier (El-Yaniv & Wiener, 2010) is a pair (f, g), where f is a classifier and g : X → {0, 1} is a selection function (El-Yaniv et al., 2010), which serves as a binary qualifier for f as follows,\n\n(f, g)(x) ≜\n\n(cid:26)f (x),\n\ndon’t know,\n\nif g(x) = 1; if g(x) = 0.\n\nA general approach for constructing a selection function based on a given classifier f is to work in terms of a confidence-rate function (Geifman et al., 2019), κf : X → R+, referred to as CF. The CF κf should quantify confidence in predicting the label of x based on signals extracted from f (Geifman et al., 2019). The most common and well-known CF for a classification model f (with softmax at its last layer) is its softmax response (SR) value (Cordella et al., 1995; De Stefano et al., 2000; Hendrycks & Gimpel, 2017). A related CF is the entropy of the softmax vector (more precisely one minus the entropy, as κf should indicate confidence), denoted here as Soft-Entropy (Gal, 2016). A given CF κf can be straightforwardly used to define a selection function: gθ(x) ≜ gθ(x|κf ) = 1[κf (x) ≥ θ], where θ is a user-defined constant. For any selection function, we define its coverage w.r.t. a distribution P , and its empirical coverage w.r.t. a sample Sk ≜ {x1, x2, . . . xk}, as c(θ, P ) ≜ EP [gθ(x)], and ˆc(θ, Sk) ≜ 1\n\ni=1 gθ(xi), respectively.\n\n(cid:80)k\n\nk\n\nGiven a bound on the expected coverage for a given selection function, we can use it to detect a distribution shift via violations of the bound. We now develop such a bound and show how to use it to detect distribution shifts. For a classifier f , a detection-training sample Sm ∼ P m, a confidence parameter δ > 0, and a desired coverage c∗ > 0, our goal is to use Sm to find a θ value (which implies a selection function gθ) that guarantees the desired coverage. This means that under coverage should occur with probability of at most δ,\n\nPrSm{c(θ, P ) < c∗} < δ.\n\n(1)\n\nA θ that guarantees Equation (1) provides a probabilistic lower bound, guaranteeing that coverage c of ID unseen population (sampled from P ) satisfies c > c∗ with probability of at least 1 − δ. A symmetric upper bound is presented in Appendix 7.5.\n\nWe now describe the selection with guaranteed coverage (SGC) algorithm. The algorithm receives as input a classifier f , a CF κf , a confidence parameter δ, a target coverage c∗, and a detection-training set Sm. The algorithm performs a binary search to find the optimal coverage lower bound with confidence δ, and outputs a coverage bound b∗ and the threshold θ, defining the selection function. A pseudo code of the SGC algorithm appears in Algorithm 1. Our analysis of the SGC algorithm makes\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Selection with guaranteed coverage (SGC)\n\n1 Input: train set: Sm, confidence-rate function: κf , confidence parameter δ, target coverage: c∗. 2 Sort Sm according to κf (xi), xi ∈ Sm (and now assume w.l.o.g. that indices reflect this ordering). 3 zmin = 1, zmax = m 4 for i = 1 to k = ⌈log2 m⌉ do\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\nz = ⌈(zmin + zmax)/2⌉ θi = κf (xz) Calculate ˆci(θi, Sm) Solve for b∗ if b∗\n\ni (m, m · ˆci(θi, Sm), δ zmax = z\n\ni (m, m · ˆci(θi, Sm), δ\n\nk ) {see Lemma 4.1}\n\nk ) ≤ c∗ then\n\nelse\n\nzmin = z\n\nend if\n\n14 end for 15 Output: bound: b∗\n\nk(m, m · ˆck(θk, Sm), δ\n\nk ), threshold: θk.\n\nuse of Lemma 4.1, which gives a tight numerical (generalization) bound on the expected coverage, based on a test over a sample. The proof of Lemma 4.1 is nearly identical to Langford’s proof of Theorem 3.3 in (Langford & Schapire, 2005), p. 278, where instead of the empirical error used in (Langford & Schapire, 2005), we use the empirical coverage, which is also a Bernoulli random variable. To gain a better understanding of Lemma 4.1, please see Appendix 7.8.\n\nLemma 4.1. Let P be any distribution and consider a selection function gθ with a threshold θ whose coverage is c(θ, P ). Let 0 < δ < 1 be given and let ˆc(θ, Sm) be the empirical coverage w.r.t. the set Sm, sampled i.i.d. from P. Let b∗(m, m · ˆc(θ, Sm), δ) be the solution of the following equation:\n\n\n\n\n\nm·ˆc(θ,Sm) (cid:88)\n\nj=0\n\n(cid:19)\n\n(cid:18)m j\n\nbj(1 − b)m−j ≤ 1 − δ\n\n\n\n .\n\narg min b\n\nThen,\n\nPrSm {c(θ, P ) < b∗(m, m · ˆc(θ, Sm), δ)} < δ.\n\n(2)\n\n(3)\n\nThe following is a uniform convergence theorem for the SGC procedure stating that all the calculated bounds are valid simultaneously with a probability of at least 1 − δ. More specifically, we apply Lemma 4.1 ⌈log2 m⌉ times (using a binary search), and the returned value (the last one) depends on all the other values. Since these applications are dependent we must use the union bound.\n\nTheorem 4.2. (SGC – Uniform convergence) Assume Sm is sampled i.i.d. from P , and consider an application of Algorithm 1. For k = ⌈log2 m⌉, let b∗ k ) and θi be the values obtained in the ith iteration of Algorithm 1. Then,\n\ni (m, m · ˆci(θi, Sm), δ\n\nPrSm{∃i : c(θi, P ) < b∗\n\ni (m, m · ˆci(θi, Sm), δ\n\nk )} < δ.\n\nProof (sketch - see full proof in the Appendix 7.2.1). Define, Bθi\n\ni (m, m · ˆci(θi, Sm), δ\n\n≜ c(θi, P ), then,\n\n≜ b∗\n\nk ), Cθi\n\nPrSm{∃i : Cθi < Bθi} =\n\n<\n\nk (cid:88)\n\n(cid:90) 1\n\n0\n\n(cid:90) 1\n\ni=1\n\nk (cid:88)\n\ni=1\n\n0\n\ndθ′PrSm{Cθ′ < Bθ′} · PrSm{θi = θ′}\n\ndθ′ δ\n\nk\n\n· PrSm {θi = θ′} =\n\nk (cid:88)\n\ni=1\n\nδ k\n\n= δ.\n\n4.2 COVERAGE-BASED DETECTION ALGORITHM\n\nOur detection algorithm works by applying SGC ⌊log2 m⌋ times for various target coverage values j , yields a corresponding pair, (b∗ (c∗). Application j of SGC, with c∗ j , θj), of a bound and a threshold,\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nrespectively. All applications of SGC are over the same sample Sm ∼ P m. In our experiments the ⌊log2 m⌋ target coverages are uniformly spread in the interval [0, 1] (excluding its end points 0 and 1). Recalling that k is the size of the window sample, Wk ∼ Qk, define, ⌊log2 m⌋ (cid:88)\n\nb∗ j ,\n\nμ ≜\n\n1 ⌊log2 m⌋\n\nˆμ ≜\n\n1 ⌊log2 m⌋\n\nj=1\n\n⌊log2 m⌋ (cid:88)\n\nj=1\n\nˆcj(θj, Wk) =\n\n1 ⌊log2 m⌋\n\n⌊log2 m⌋ (cid:88)\n\nj=1\n\n1 k\n\nk (cid:88)\n\ni=1\n\ngθj (xi)\n\n=\n\n1 k⌊log2 m⌋\n\n⌊log2 m⌋ (cid:88)\n\nk (cid:88)\n\nj=1\n\ni=1\n\ngθj (xi).\n\nIf Q is identical to P (i.e., no distribution shift), we expect that the bound computed by SGC over Sm will hold over Wk as well; namely, ˆcj(θj, Wk) ≥ b∗\n\nj , for every iteration j of SGC.\n\nAs ˆμ represents the average of k⌊log2 m⌋ values, and as in our experiments, k⌊log2 m⌋ ≫ 30, a well known rule of thumb (James et al. (2013), p. 67, states that ˆμ is nearly normally distributed. Therefore, we can apply a t-test1 to accept or reject the null hypothesis H0 : ˆμ ≥ μ, where the alternative hypothesis is H1 : ˆμ < μ. The null hypothesis is rejected if the p-value is less than the desired threshold (significance level) of the test α (user defined). Thus, when evaluating performance using any tradeoff-based metric (e.g., AUROC) we vary α to obtain the tradeoff. A pseudo code of our coverage-based detection algorithm appears in Algorithm 2.\n\nAlgorithm 2: Coverage-Based Detection\n\n1 // Training 2 Input Training: δ, κf , {c∗ 3 for j = 1 to k = ⌊log2 m⌋ do\n\nj }⌊log2 m⌋\n\nj=1\n\nj , θj = SGC(Sm, δ, c∗ b∗\n\nj , κf )\n\n4 5 end for 6 μ ≜\n\n1 ⌊log2 m⌋\n\n(cid:80)⌊log2 m⌋ j=1\n\n7 Output Training: μ, {(b∗ 8 // Detection model 9 Input Detection: μ, {(b∗\n\nj\n\nb∗ j , θj)}⌊log2 m⌋\n\nj=1\n\nj , θj)}⌊log2 m⌋\n\nj=1\n\n, κf , α, k while True do\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\nReceive windows Wk = {x1, x2, . . . , xk} for j = 1 to k = ⌊log2 m⌋ do\n\nˆcj(θj, Wk) ≜ 1\n\nk\n\n(cid:80)k\n\ni=1 gθj (xi)\n\n1 k⌊log2 m⌋\n\nend for ˆμ ≜ Obtain p-value from t-test, H0 : ˆμ ≥ μ, H1 : ˆμ < μ if pvalue < α then\n\n(cid:80)⌊log2 m⌋ j=1\n\ni=1 gθj (xi)\n\n(cid:80)k\n\nShift_detected ← True Output Detection: Shift_detected, pvalue\n\nend if 19 20 end while\n\nWe train only once using SGC (Algorithm 1) on the detection-training data Sm for ⌊log2 m⌋ times, in order to construct the pairs {b∗ . Our detection model utilizes these pairs to monitor a given model, receiving at each time instant a test sample window of size k (user defined), Wk = {x1, x2, . . . , xk}, which is checked to see if its content is distributionally shifted from the underlying distribution reflected by the detection-training data Sm. A schematic diagram of our procedure on window data, Wk, appears in Figure 1.\n\nj , θj}⌊log2 m⌋\n\nj=1\n\nIn comparison to the previous baselines, our proposed method is extremely more efficient, as shown in table 1. A derivation of the complexity bounds can be found in Appendix 7.3.\n\n1In our experiments we apply SciPy’s stats.ttest_1samp t-test implementation (Virtanen et al., 2020).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Our detection procedure comprising two stages, training and detecting. The detection stage requires parameters {b∗ and {θi}⌊log2 m⌋ (computed at the training stage), i.e., only O(log2 m) parameters relative to the training data size (m).\n\ni }⌊log2 m⌋\n\ni=1\n\ni=1\n\nDetection Method\n\nSpace\n\nTime\n\nCoverage-Based Detection (Ours) MMD KS\n\nO(k + log m) O(m2 + k2 + mk) O (d(m + k))\n\nO(k + log m) O (cid:0)d(m2 + k2 + mk)(cid:1) O (d(m log m + k log k))\n\nTable 1: Complexity comparison. Orange bolds entries indicate the best detection complexity. m, k refers to the detection-training size, and window size, respectively. d refers to the number of dimensions after dimensionality reduction. A derivation of the complexity bounds can be found in Appendix 7.3.\n\n5 EMPIRICAL STUDY\n\nIn this section we evaluate the performance of our coverage-based detection algorithm, as well as the baselines defined in Section 2. Three experiments are conducted. The first experiment, is carried out using synthetic data, and its purpose is to provide some insight into the operation of our algorithm. Specifically, we use synthetic data in order to demonstrate that our proposed bound tightly holds, and when a distribution shift occurs, it is violated as expected. Unfortunately, due to lack of space, this experiment is described in Appendix 7.4.1. The purpose of the other two experiments is to examine the performance of our detection algorithm, with respect to the four baselines (KS-BBSD-S, KS-BBSD-E, MMD-BBSD-S, and MMD-BBSD-E; see Section 2). These experiments are carried out on the CIFAR-10 (Krizhevsky et al., 2009) and the ImageNet (Deng et al., 2009) datasets, which are used as our detection-training (ID) data. As our test data we use a number of datasets and corruptions of the detection-training data to represent a variety of distribution shifts (see details below).\n\nWe now define the metrics we use to evaluate detection performance. Following (Liang et al., 2018), we consider a soft binary classification setting (is P = Q or P ̸= Q, see Section 3), where the answer is determined based on a decision threshold. We thus use the Area Under an Operating Characteristic curve (AUROC) and the Area under the Precision-Recall curve (AUPR).\n\nAUROC is a threshold-independent metric (Davis & Goadrich, 2006). The ROC curve depicts the relationship between the true positive rate and the false positive rate. The AUROC can be interpreted as the probability that a positive labeled window will have a higher detection score than a negative one (Fawcett, 2006). An AUROC score of 100% corresponds to a perfect detector. AUPR is also a threshold-independent metric. The precision-recall (PR) curve is a function giving the relation between the precision = TP/(TP+FP) and the recall = TP/(TP+FN) for all threshold values. Following (Liang et al., 2018), we separate this metric into AUPR-Tr and AUPR-Te, which measure the area under the PR curve at which positive test windows are containing ID (detection-training) data (AUPR-Tr) and AD (test) data (AUPR-Te), respectively. It should be noted that AUPR-Te is almost saturated in most of our experiments (due to data imbalance), however, we decided to include the results of AUPR-Te nonetheless for completeness.\n\n5.1 CIFAR-10 AND IMAGENET\n\nWe now present our primary empirical study using the CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009) datasets. In both cases the customary validation/test sets (10,000 instances in CIFAR-10 and 50,000 in ImageNet) are split randomly (and uniformly) 30 times into two groups to form 30 detection-training sets (Sm, see Section 3) and tests pairs, that are used to train the detection model and evaluate its performance, respectively. The size of the test set is 1000 in all cases. Thus, the sizes of detection-training sets and test sets are (9000, 1000) for CIFAR-10 and (49,000, 1000) for ImageNet. Detection models are challenged by forming windows Wk, for various sizes of 0 < k ≤ 1000, where in each case Wk comprises simulated distribution shifts that represent various\n\n7\n\nWkδTraining(SGC)ci∗i$1⌊log2m⌋κfbi∗i$1⌊log2m⌋,θi∗i$1⌊log2m⌋DetectionModelShift orNo-ShiftαSmUnder review as a conference paper at ICLR 2023\n\ndistributions Q, including the no-shift case (where Q = P ), to check for false-alarms (see details below). To aggregate a metric score such as AUROC, over all window sizes to a single number, we define the metrics Agg-AUROC (see Figure 2-left), Agg-AUPR-Tr, and Agg-AUPR-Te, which are used in Tables 2, 3. These metrics denote the area under the curve of the metric as a function of k. We also display the scores individually for each window size, see details later.\n\nFigure 2: ImageNet Experiments. AUROC as a function of the window size k (left), and the margin between our best model (OursEnt), and the best baseline, KSBBSD-S (right). The margin is the difference between the AUROC scores of Ours-Ent and KS-BBSDS. One-σ error-bars are shadowed. All figures show metric values with a one-σ error-bar obtained via bootstrapping. Throughout all experiments, we use the coverage-based detection algorithm (Algorithm 2) with δ = 0.001, and in the applications described below, we instantiate CF (κf ) with both SR and Soft-Entropy. These two applications of our method are referred to as Ours-SR and Ours-Ent.\n\n5.1.1 CIFAR-10 EXPERIMENTS\n\nTo evaluate performance, we used two pretrained ResNet-18 and ResNet-50 (He et al., 2016). Following (Rabanser et al., 2019) we simulated the following shifts: Rotations: with angles θ ∈ {10, 40, 90}, Gaussian noise: with STD σ ∈ { 40 255 }, the CIFAR-100 dataset (Krizhevsky et al., 2009), the SVHN dataset (Netzer et al., 2011), and Adversarial: fast gradient sign method (Goodfellow et al., 2015), projected gradient descent (Madry et al., 2018), Carlini Wagner (Carlini & Wagner, 2017), referred to as FGSM, PGD and CW, respectively. The aggregated results for all these shifts are summarized in Table 2.\n\n255 , 80\n\n255 , 60\n\nMethod\n\nOurs\n\nKS\n\nMMD\n\nEnt SR BBSD-S BBSD-E BBSD-S BBSD-E\n\nResNet-18\n\nResNet-50\n\nAgg-AUROC Agg-AUPR-Te/Tr Agg-AUROC Agg-AUPR-Te/Tr\n\n98.57 98.56 97.27 96.76 84.3 87.25\n\n99.85 / 92.18 99.85 / 92.11 99.73 / 82.58 99.69 / 77.09 98.38 / 27.75 98.73 / 35.1\n\n98.72 98.7 97.61 97.47 83.13 85.7\n\n99.87 / 92.08 99.87 / 91.84 99.77 / 83.71 99.75 / 82.94 98.19 / 26.98 98.47 / 32.09\n\nTable 2: CIFAR-10 Experiments. An underlined entry indicates the best baseline among the proposed baselines (KS-BBSD-S, KS-BBSD-E, MMD-BBSD-S, and MMD-BBSD-E, see Section 2), and bold orange indicates the best detection score for a given model and metric.\n\nOrange bold entries indicate the best evaluation score, while underlined entries indicate the best evaluation score vis-a-vis the baselines. It is evident that Ours-Ent dominates all methods w.r.t. all performance metrics, and when applied with both architectures. Interestingly, both our detection variants excel in the Agg-AUPR-Tr metric, compared to the baselines. This reflects the fact that our detection methods have significantly fewer false-alarms compared to the baselines; i.e., the baselines are more likely to misidentify ID windows as AD windows than we are. This is crucial when considering distribution shift detection, since most of the time the model is processing ID data. Among the baselines, KS-BBSD-S dominates the three other baselines, in line with the findings of (Rabanser et al., 2019), who also experimented over CIFAR-10. For individual results regarding each window size, see Appendix 7.4.2.\n\n5.1.2\n\nIMAGENET EXPERIMENTS\n\nFor the ImageNet dataset, we simulated a larger number of shifts than we did in the CIFAR-10 experiments (Section 5.1.1): Rotations: θ ∈ {10, 40, 90, 120, 150, 180}, the ImageNet-O dataset\n\n8\n\nAgg-AUROC = 99.15012345678910Window data (k/100)42916MarginOurs-EntKS-BBSD-SUnder review as a conference paper at ICLR 2023\n\n(Hendrycks et al., 2021), the ImageNet-A dataset (Hendrycks et al., 2021), and the ImageNet-C dataset (Hendrycks & Dietterich, 2019), which contains 75 common visual distortions with intensities ranging from 1 to 5; for our experiments we use {1, 3, 5}. We also consider adversarial attacks: fast gradient sign method (Goodfellow et al., 2015), projected gradient descent (Madry et al., 2018), Carlini Wagner (Carlini & Wagner, 2017), referred to as FGSM, PGD and CW, respectively. For each distribution shift type we considered three proportions p of window contamination, where p denotes the proportion of shifted instances in the window. For example, if p = 1 3 , a third of the window is contaminated with instances sampled from the shifted distribution, while the remaining instances are sampled from P . In our experiments we used proportions p ∈ {1, 2 3 }. Due to lack of space we present only the most successful baseline (we experimented with all), namely, the KS-BBSD-S detection model, which was also the most successful baseline regarding the CIFAR-10 experiment. For this ImageNet experiment, we used the strong EfficientNet-B0 network (Tan & Le, 2019) as the underlying model for our methods and the baseline. We took a model that was pretrained over ImageNet (Wightman, 2019). In Table 3 we summarize the aggregate results for all these shifts.\n\n3 , 1\n\nMethod\n\nEfficientNet-B0 p = 2/3 Agg-AUROC Agg-AUPR-Te/Tr Agg-AUROC Agg-AUPR-Te/Tr Agg-AUROC Agg-AUPR-Te/Tr\n\np = 1/3\n\np = 1\n\nOurs\n\nKS\n\nEnt SR BBSD-S\n\n99.15 99.12 98.3\n\n99.93 / 94.21 99.92 / 94.29 99.86 / 83.85\n\n98.23 98.18 96.06\n\n99.84 / 87.21 99.84 / 86.21 99.67 / 70.57\n\n93.50 93.38 89.16\n\n99.45 / 61.76 99.44 / 59.95 99.02 / 44.28\n\nTable 3: ImageNet Experiments. Orange bolds indicate the best detection score for a given metric, and contamination percentage (p). KS-BBSD-S (Rabanser et al., 2019), is the Kolmogorov-Smirnov (KS) statistical test (Massey Jr, 1951), using a Black Box Shift Detection (BBSD) (Lipton et al., 2018) method, using Softmax (S); namely, KS-BBSD-S. According to (Rabanser et al., 2019), and based on our analysis in Table 2, this is the best proposed baseline.\n\nBold orange entries indicate the highest evaluation scores. In line with the results of the CIFAR-10 experiment (Section 5.1.1), Ours-Ent dominates both other methods (Ours-SR, KS-BBSD-S) for all performance metrics, and for each contamination percentage (p) considered, with the exception of Agg-AUPR-Tr (p = 1), where in this case Ours-SR dominates. A significant margin separates OursEnt from KS-BBSD-S, and this margin becomes even larger when the contamination (p) decreases (see Appendix 7.4.3). In Figure 2(left), one can see the resulting AUROC scores as a function of the window size k for p = 1. In Figure 2(right) we also display the margin between Ours-Ent and the best baseline, KS-BBSD-S (the rest of the metrics, appear in the Appendix, Figure 8). For the vast majority of window sizes, Ours-Ent performs significantly better than the best baseline, KS-BBSD-S; Furthermore, see Appendix 7.4.3, Ours-Ent consistently outperforms KS-BBSD-S over all metrics, across all percentages of affected data (p ∈ {1, 2 3 , 1 3 }), and for all window sizes, where the biggest gap is over the AUPR-Tr metric, which accounts for lower false-alarms compared to the baselines. We conclude that in our setting, Soft-Entropy is the best CF. Appendix 7.6 provides a detailed comparison.\n\n6 CONCLUDING REMARKS\n\nWe presented a novel and powerful method for the detection of distribution shifts within a given window of samples. This coverage-based detection algorithm is theoretically motivated and can be applied to any pretrained model. Due to its low computational complexity, our method, unlike typical baselines, is practicable. Our comprehensive empirical studies demonstrate that the proposed method works very well, and overall significantly outperforms the baselines on both the CIFAR-10 and ImageNet datasets, across a number of neural architectures and a variety of distribution shifts, including adversarial examples. In addition, our coverage bound is of independent interest and allows for the creation of selective classifiers with guaranteed coverage. Several directions for future research are left open. Although we only considered classification, our method can be extended to regression using an appropriate confidence-rate function such as the MC-dropout (Gal & Ghahramani, 2016). Extensions to other tasks, such as object detection and segmentation, would be very interesting. It would also be interesting to examine other types of shift benchmarks such as (Koh et al., 2021). In our method, the information from the multiple coverage bounds was aggregated by averaging, but it is plausible that other statistics or weighted averages could provide more effective detections. Finally, an interesting open question is whether one can benefit from using outlier/adversarial detection techniques combined with population-based detection techniques (as discussed here).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nElla Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 245–250, 2001.\n\nJ Martin Bland and Douglas G Altman. Multiple significance tests: the bonferroni method. Bmj,\n\n1995.\n\nNicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In\n\nIEEE Symposium on Security and Privacy, 2017.\n\nLuigi Pietro Cordella, Claudio De Stefano, Francesco Tortorella, and Mario Vento. A method for improving classification reliability of multilayer perceptrons. IEEE Transactions on Neural Networks, 6(5):1140–1147, 1995.\n\nJesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In ICML,\n\n2006.\n\nClaudio De Stefano, Carlo Sansone, and Mario Vento. To reject or not to reject: that is the question-an answer in case of neural classifiers. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 30(1):84–94, 2000.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009.\n\nRan El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. jmlr, 2010.\n\nRan El-Yaniv et al. On the foundations of noise-free selective classification. JMLR, 2010.\n\nTom Fawcett. An introduction to roc analysis. Pattern recognition letters, 27(8):861–874, 2006.\n\nStanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution\n\ndetection. Advances in Neural Information Processing Systems, 2021.\n\nYarin Gal. Uncertainty in deep learning. 2016.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model\n\nuncertainty in deep learning. In ICML, 2016.\n\nYonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In NIPS, 2017.\n\nYonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep neural\n\nclassifiers. In ICLR, 2019.\n\nIzhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. In NeurIPS,\n\n2018.\n\nIan Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In ICLR, 2015.\n\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander J. Smola.\n\nA kernel two-sample test. Journal of Machine Learning Research, 13:723–773, 2012.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In CVPR, 2016.\n\nNicholas A Heard and Patrick Rubin-Delanchy. Choosing between methods of combining-values.\n\nBiometrika, 105(1):239–246, 2018.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common\n\ncorruptions and perturbations. In ICLR, 2019.\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\n\nexamples in neural networks. In ICLR, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier\n\nexposure. In ICLR, 2019.\n\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial\n\nexamples. CVPR, 2021.\n\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical\n\nlearning, volume 112. Springer, 2013.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning. PMLR, 2021.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nJohn Langford and Robert Schapire. Tutorial on practical prediction theory for classification. Journal\n\nof machine learning research, 6(3), 2005.\n\nShiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image\n\ndetection in neural networks. In ICLR, 2018.\n\nZachary C. Lipton, Yu-Xiang Wang, and Alexander J. Smola. Detecting and correcting for label shift\n\nwith black box predictors. In ICML, 2018.\n\nThomas M Loughin. A systematic comparison of methods for combining p-values from independent\n\ntests. Computational statistics & data analysis, 47(3):467–485, 2004.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\n\nTowards deep learning models resistant to adversarial attacks. In ICLR, 2018.\n\nFrank J Massey Jr. The kolmogorov-smirnov test for goodness of fit. Journal of the American\n\nstatistical Association, 1951.\n\nZachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael W. Dusenberry, Sebastian Farquhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah Liu, Zelda Mariet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim G. J. Rudner, Yeming Wen, Florian Wenzel, Kevin Murphy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin Gal, and Dustin Tran. Uncertainty baselines: Benchmarks for uncertainty & robustness in deep learning. CoRR, abs/2106.04015, 2021. URL https://arxiv.org/abs/2106.04015.\n\nEric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-ofdistribution inputs to deep generative models using typicality. arXiv preprint arXiv:1906.02994, 2019.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading\n\ndigits in natural images with unsupervised feature learning. In NIPS Workshop, 2011.\n\nYunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence Carin. Variational autoencoder for deep learning of images, labels and captions. In NIPS, 2016.\n\nStephan Rabanser, Stephan Günnemann, and Zachary Chase Lipton. Failing loudly: An empirical\n\nstudy of methods for detecting dataset shift. In NeurIPS, 2019.\n\nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. Advances in Neural Information Processing Systems, 2019.\n\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nRobert J Serfling. Approximation theorems of mathematical statistics, volume 162. John Wiley &\n\nSons, 2009.\n\nDougal J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In ICLR (Poster), 2017.\n\nMingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural\n\nnetworks. In ICML, 2019.\n\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ̇Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 2020.\n\nRoss Wightman.\n\nPytorch image models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nSvante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and\n\nintelligent laboratory systems, 2(1-3):37–52, 1987.\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\n\narXiv preprint arXiv:2106.04560, 2021.\n\n7 APPENDIX\n\n7.1 SHIFT-DETECTION GENERAL FRAMEWORK\n\nThe general framework for shift-detection can be found in the following figure, Figure 3.\n\nFigure 3: The procedure of detecting a dataset shift using dimensionality reduction and then a two-sample statistical test. The dimensionality reduction is applied to both the detection-training (source) and test (target) data, prior to being analyzed using statistical hypothesis testing. This figure is taken from (Rabanser et al., 2019).\n\n7.2 PROOFS\n\n7.2.1 PROOF FOR THEOREM 4.2\n\nProof. Define\n\nBθi\n\n≜ b∗\n\ni (m, m · ˆci(θi, Sm),\n\nδ k\n\n),\n\nCθi\n\n≜ c(θi, P ).\n\nConsider the ith iteration of SGR over a detection-training set Sm, and recall that, θi = κf (xz), xz ∈ Sm (see Algorithm 1). Therefore, θi is a random variable (between zero and one), since it is a function of a random variable (x ∈ Sm). Let PrSm{θi = θ′} be the probability that θi = θ′.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTherefore,\n\nPrSm{Cθi < Bθi}\n\n=\n\n=\n\n(cid:90) 1\n\n0 (cid:90) 1\n\n0\n\ndθ′PrSm{Cθi < Bθi|θi = θ′} · PrSm {θi = θ′}\n\ndθ′PrSm{Cθ′ < Bθ′} · PrSm {θi = θ′}.\n\nSince Bθi is obtained using Lemma 4.1 (see Algorithm 1), and θi = θ′,\n\nPrSm {Cθi < Bθi} = PrSm {Cθ′ < Bθ′} <\n\nδ k\n\n,\n\nso we get,\n\nPrSm{Cθi < Bθi}\n\n=\n\n<\n\n=\n\n=\n\n(cid:90) 1\n\n0 (cid:90) 1\n\n·\n\n0 δ\nk δ\nk\n\n.\n\ndθ′PrSm{Cθ′ < Bθ′} · PrSm{θi = θ′}\n\ndθ′ δ\n\nk (cid:18)(cid:90) 1\n\n0\n\n· PrSm {θi = θ′}\n\ndθ′PrSm {θi = θ′}\n\n(cid:19)\n\n(4)\n\nThe following application of the union bound completes the proof,\n\nPrSm {∃i : Cθi < Bθi } ≤\n\nk (cid:88)\n\ni=1\n\nPrSm{Cθi < Bθi} <\n\nk (cid:88)\n\ni=1\n\nδ k\n\n= δ.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n7.3 COMPLEXITY ANALYSIS\n\nThis section provides a brief complexity analysis of our method as well as the baselines (see Section 2). All baselines are lazy learners (analogous to nearest neighbors) in the sense that they require the entire source (detection-training) set for each detection decision they make. Using only a subset will result in sub-optimal performance. MMD is a permutation test (Gretton et al., 2012) that also employs a kernel. The complexity of kernel methods is dominated by the number of instances and, therefore, the time and space complexities of MMD are O(d(m2 + k2 + mk)) and O(m2 + k2 + mk), respectively, where in the case of DNNs, d is the dimension of the embedding or softmax layer used for computing the kernel. The KS test (Massey Jr, 1951) is a univariate test, which is applied on each dimension separately and then aggregates the results via a Bonferroni correction. Its time and space complexities are O(d(m log m + k log k)) and O(d(m + k)), respectively.\n\nOur coverage-based detection algorithm is trained (only once) at O(m log m) time and O(m) space complexities. Then, each detection round incurs O(k+log m) for both its time and space complexities. In practice, dm2 ≫ dm log m ≫ log m, which makes our method significantly more efficient. For example, both baselines cannot process large “Google-scale” datasets, which our method can handle. A summary of these complexities appears in Table 1 (Section 4.2).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n7.4 DETAILED EXPERIMENTS RESULTS\n\n7.4.1 SYNTHETIC DATA\n\nTo gain some insight into the operation of our coverage-based detection algorithm we consider the following synthetic setting. A simple binary linear classifier was trained to discriminate between two 2D Gaussians, which are centered at (−5, 0), (5, 0), respectively, whose covariance matrix is the identity. As the confidence-rate function (CF) we took the well-known softmax response (SR). For the shifted distribution we consider two cases where the modified distribution is defined by increasing or decreasing the distance between the Gaussians’ centers. When the two Gaussians move closer, the coverage lower bound is expected to be violated. On the other hand, when they are further apart, the lower bound will not be violated. Thus, for this case, we introduce a symmetric coverage upper bound. This upper bound appears in Appendix 7.5. Hence, when the modified Gaussians are further apart, we expect that the upper bound will be violated. G0 ≜ {(−5, 0), (5, 0)} are the centers of the Gaussians generating the detection-training data. G+ ≜ {(−6, 0), (6, 0)} and G− ≜ {(−4, 0), (4, 0)} represent two variations of G0, where G+ increases and G− decreases the distance between the centers.\n\nWe created two detection models by applying the training component of Algorithm 2 twice (to obtain the lower and upper bounds) with the following hyperparameters: δ = 0.001 and detection-training set Sm consisting of m = 50, 000 samples generated from the Gaussians G0 (25,000 from each). The set of desired coverages {c∗ is uniformly spread in the interval [0, 1] (excluding its end points 0 and 1); thus, we have ⌊log2 50, 000⌋ = 15 desired coverages. To challenge our method in this setting, we generated samples from G+, G− (expecting a violation of the upper and lower bounds, respectively), and also G0 (as a sanity check that the bounds hold tightly). We define the test windows W − k ∼ G0, each containing 25,000 samples from each of its two generating Gaussians (thus, k = 50, 000). For example, in the case of G−, we generate 25,000 samples from each Gaussian centered at (−4, 0), and (4, 0), respectively.\n\nk ∼ G−, W +\n\nk ∼ G+, W 0\n\ni }⌊log2 m⌋\n\ni=1\n\nk )}⌊log2 m⌋ In Figure (4) we show the relationship between the empirical coverages ({ˆcs the bounds ({b∗ k , s ∈ {−, +, 0}. Figures (4a) through (4d) show the results corresponding to the ‘no-shift’ and ‘shift’ cases, respectively.\n\nj=1 ), for each test window, W s\n\n), and the desired coverages ({c∗\n\nj }⌊log2 m⌋\n\nj }⌊log2 m⌋\n\nj(θj, W s\n\nj=1\n\nj=1\n\n),\n\n(a) W 0 k , bound.\n\nlower\n\n(b) W 0 k , bound.\n\nupper\n\n(c) W + k , bound.\n\nupper\n\n(d) W − k , bound.\n\nlower\n\nFigure 4: The empirical coverages and bounds as a function of the desired coverages, for each of the cases discussed in the text.\n\nIn particular, Figures (4c) and (4d) indicate that the windows W + k and W − k violate the upper and lower bounds, respectively. This behavior is expected, since the former (W + k ) should result in underconfident predictions, and the latter (W − k ), in over-confident predictions. The resulting p-values in both cases were nearly 0, which indicates a certain shift detection. The no-shift cases (W 0 k ) are shown in Figures (4a), (4b), from which it is evident that both bounds hold tightly (the p-value is nearly 1). Interestingly, when a shift occurs, see Figures 4c, 4d, the largest margin between the empirical coverage and the bound (in both the upper and lower cases) is obtained at the interior of the coverage range (e.g., around 0.7 coverage in the lower bound case, 4d). In other words, this synthetic experiment indicates that the detection effectiveness of our method is at its best in mid-range coverages. Specifically, Figure 5 shows the margin between the empirical coverages ({ˆcs ) for each\n\n) and the desired coverages ({c∗\n\n), the bounds ({b∗\n\nj(θj, W s\n\nk )}⌊log2 m⌋\n\nj=1\n\nj }⌊log2 m⌋\n\nj=1\n\nj }⌊log2 m⌋\n\nj=1\n\n15\n\n0.20.40.60.8Desired coverage0.20.40.60.8CoverageEmpirical coverageLower bounds0.20.40.60.8Desired coverage0.20.40.60.8CoverageEmpirical coverageUpper bounds0.20.40.60.8Desired coverage0.20.40.60.8CoverageEmpirical coverageUpper bounds0.20.40.60.8Desired coverage0.20.40.60.8CoverageEmpirical coverageLower boundsUnder review as a conference paper at ICLR 2023\n\ndata window W s k ) results in a tight bound and, therefore, a very small margin (see Figures 5a and 5b). When shifts occur (W − k ), the gap between the bounds and the empirical coverages is significantly larger and is not uniform (see Figures 5c and 5d).\n\nk , s ∈ {−, +, 0}. The no-shift case (W 0\n\nk , W +\n\nWe can thus infer that different data shifts would result in varying gap sizes, depending on the desired coverage being analyzed. In particular, the largest gap does not necessarily occur when c∗ = 1, indicating that the detection power lies within lower coverages.\n\n(a) Margin of a tight lower bound.\n\n(b) Margin of a tight upper bound.\n\n(c) Margin of a violated upper bound.\n\n(d) Margin of a violated lower bound.\n\nFigure 5: The margins between the bounds and the actual coverage.\n\n16\n\n0.20.40.60.8Desired coverage0.00000.00250.00500.00750.01000.0125MarginMarginLower bounds0.20.40.60.8Desired coverage0.0100.0080.0060.0040.0020.000MarginMarginUpper bounds0.20.40.60.8Desired coverage0.00.10.20.3MarginMarginUpper bounds0.20.40.60.8Desired coverage0.30.20.10.0MarginMarginLower boundsUnder review as a conference paper at ICLR 2023\n\n7.4.2 CIFAR-10 EXPERIMENTS\n\nThe score for each window Wk, of size k, as well as the margin between our best method (Ours-Ent) and the best baseline (KS-BBSD-S) considering the ResNet-18 and ResNet-50 architectures, can be found in Figures 6, 7, respectively.\n\n(a) AUROC\n\n(b) AURP-Te\n\n(c) AUPR-Tr\n\nFigure 6: CIFAR-10 Detection results using ResNet-18. The metric scores as a function of window size (upper), and the margin between our best method and the best baseline, (lower). The margin is the difference between the metrics (AUROC, AUPR-Te, AUPR-Tr) scores of Ours-Ent and KS-BBSD-S. One-σ error-bars are shadowed.\n\n(a) AUROC\n\n(b) AURP-Te\n\n(c) AUPR-Tr\n\nFigure 7: CIFAR-10 Detection results using ResNet-50. The metric scores as a function of window size (upper), and the margin between our best and the best baseline, (lower). The margin is the difference between the metrics (AUROC, AUPR-Te, AUPR-Tr) scores of Ours-Ent and KS-BBSD-S. One-σ error-bars are shadowed.\n\n17\n\n012345678910Window data (k/100)5564738291100AUROCMMD-BBSD-EKS-BBSD-SOurs-Ent012345678910Window data (k/100)91.092.894.696.498.2100.0AUPR-TeMMD-BBSD-EKS-BBSD-SOurs-Ent012345678910Window data (k/100)1028466482100AUPR-TrMMD-BBSD-EKS-BBSD-SOurs-Ent012345678910Window data (k/100)51815MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)20023MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)941731MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)6068768492100AUROCMMD-BBSD-EKS-BBSD-SOurs-Ent012345678910Window data (k/100)93.094.495.897.298.6100.0AUPR-TeMMD-BBSD-EKS-BBSD-SOurs-Ent012345678910Window data (k/100)12.029.647.264.882.4100.0AUPR-TrMMD-BBSD-EKS-BBSD-SOurs-Ent012345678910Window data (k/100)51158MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)10011MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)851832MarginOurs-EntKS-BBSD-SUnder review as a conference paper at ICLR 2023\n\n7.4.3\n\nIMAGENET EXPERIMENTS\n\nRegarding the ImageNet experiment, the score for each window Wk, of size k, as well as the margin between our best method (Ours-Ent) and the best baseline (KS-BBSD-S), considering the EfficientNet-B0 architecture, is given in Figures 8, 9, 10, for p = 1, p = 2/3, and p = 1/3, respectively.\n\nFigure 8: ImageNet Detection results using EfficientNet-B0 with p = 1. The metric scores as a function of window size (upper), and the margin between our best and the best baseline, (lower). The margin is the difference between the metrics (AUROC, AUPR-Te, AUPR-Tr) scores of Ours-Ent and KS-BBSD-S. One-σ error-bars are shadowed.\n\nFigure 9: ImageNet Detection results using EfficientNet-B0 with p = 2/3. The metric scores as a function of window size (upper), and the margin between our best and the best baseline, (lower).The margin is the difference between the metrics (AUROC, AUPR-Te, AUPR-Tr) scores of Ours-Ent and KS-BBSD-S. One-σ error-bars are shadowed.\n\n18\n\nAgg-AUROC = 99.15012345678910Window data (k/100)96.096.897.698.499.2100.0AUPR-TeKS-BBSD-SOurs-Ent012345678910Window data (k/100)12.029.647.264.882.4100.0AUPR-TrKS-BBSD-SOurs-Ent012345678910Window data (k/100)42916MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)10011MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)851832MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)48.058.468.879.289.6100.0AUROCKS-BBSD-SOurs-Ent012345678910Window data (k/100)94.095.296.497.698.8100.0AUPR-TeKS-BBSD-SOurs-Ent012345678910Window data (k/100)8.026.444.863.281.6100.0AUPR-TrKS-BBSD-SOurs-Ent012345678910Window data (k/100)641424MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)1012MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)7133353MarginOurs-EntKS-BBSD-SUnder review as a conference paper at ICLR 2023\n\nFigure 10: ImageNet Detection results using EfficientNet-B0 with p = 1/3. The metric scores as a function of window size (upper), and the margin between our best method and the best baseline, (lower). The margin is the difference between the metrics (AUROC, AUPR-Te, AUPR-Tr) scores of Ours-Ent and KS-BBSD-S. One-σ error-bars are shadowed.\n\n19\n\n012345678910Window data (k/100)44.055.266.477.688.8100.0AUROCKS-BBSD-SOurs-Ent012345678910Window data (k/100)92.093.695.296.898.4100.0AUPR-TeKS-BBSD-SOurs-Ent012345678910Window data (k/100)6.024.843.662.481.2100.0AUPR-TrKS-BBSD-SOurs-Ent012345678910Window data (k/100)641424MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)1013MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)941731MarginOurs-EntKS-BBSD-SUnder review as a conference paper at ICLR 2023\n\n7.5 SYMMETRIC UPPER BOUND\n\nHere we develop an upper coverage bound.\n\nFor a classifier f , a detection-training sample Sm ∼ P m, a confidence parameter δ > 0, and a desired coverage c∗ > 0, our goal is to use Sm to find a θ value (which implies a selection function gθ), such that the coverage satisfies,\n\nPrSm{c(θ, P ) > c∗} < δ.\n\n(5)\n\nThis means that over coverage should occur with probability of at most δ.\n\nThe pseudo code of the algorithm that finds the optimal coverage upper bound (with confidence δ), appears in Algorithm 3.\n\nAlgorithm 3: Selection with guaranteed coverage - Upper bound\n\n1 Input: train set: Sm, confidence-rate function: κf , confidence parameter δ, target coverage: c∗. 2 Sort Sm according to κf (xi), xi ∈ Sm (and now assume w.l.o.g. that indices reflect this\n\nordering).\n\n3 zmin = 1, zmax = m 4 for i = 1 to k = ⌈log2 m⌉ do\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\nz = ⌈(zmin + zmax)/2⌉ θi = κf (xz) Calculate ˆci(θi, Sm) Solve for b∗ if b∗\n\ni (m, m · ˆci(θi, Sm), δ zmin = z\n\nelse\n\nzmax = z\n\ni (m, m · ˆci(θi, Sm), δ\n\nk ) {see Lemma 4.1}\n\nk ) ≥ c∗ then\n\nend if\n\n13 14 end for 15 Output: bound: b∗\n\nk(m, m · ˆck(θk, Sm), δ\n\nk ), threshold: θk.\n\nSimilarly to SGC (Algorithm 1), Algorithm 3 uses Lemma 7.1. The proof of Lemma 7.1 can be easily deduced from the proof of Lemma 4.1.\n\nLemma 7.1. Let P be any distribution and consider a CF threshold θ with a coverage of c(θ, P ). Let 0 < δ < 1 be given and let ˆc(θ, Sm) be the empirical coverage w.r.t. the set Sm, sampled i.i.d. from P. Let b∗(m, m · ˆc(θ, Sm), δ) be the solution of the following equation:\n\n\n\n\n\nm·ˆc(θ,Sm) (cid:88)\n\nj=0\n\n(cid:19)\n\n(cid:18)m j\n\nbj(1 − b)m−j ≤ δ\n\n\n\n .\n\narg min b\n\nThen,\n\nPrSm {c(θ, P ) > b∗(m, ˆc(θ, Sm), δ)} < δ.\n\n7.6 COMPARISON BETWEEN OUR TWO PROPOSED METHODS\n\n(6)\n\n(7)\n\nThe following Figure (11) compares between our two proposed methods, Ours-Ent, Ours-SR, for a contamination value of p = 2/3. Figure 11(left) - shows the AUROC metric as a function of window size, Figure 11(right) - shows the margin between the two methods.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: Comparison between Ours-Ent, and Ours-SR. One-σ error-bars are shadowed.\n\n7.7 PERFORMANCE PER DISTRIBUTION SHIFT - IMAGENET\n\nThe following graphs demonstrate representative results regarding the detection performance per distribution shift. Specifically, the first row of Figure 12 represents the AUROC metric where the contamination value is two thirds (p = 2/3), for the following diverse shifts separately: ImageNet C-severity 1, Imagenet A, rotation of 40 degrees, rotation of 180 degrees and the best considered adversarial attack, CW. The second row of Figure 12 represents the margin between our best method (Our-Ent) and the best baseline (KS-BBSD-S).\n\nFigure 12: ImageNet Partial detection results, for individual distribution shifts. One-σ error-bars are shadowed.\n\n7.8 BROADER EXPLANATION OF LEMMA 4.1\n\nLemma 4.1 gives (the tightest possible) generalization numerical bound of a coverage, given a test sample, Sm, and a confidence parameter δ. Equation 2 solves for the inverse binomial, which stands for the probability, b, such that an event with at most m · ˆc(θ, Sm) \"successes\" will equal 1 − δ. Since the inverse binomial is a monotonic decreasing function of b, we look for the minimum probability. Lemma 4.1 states, that the solution of Equation 2, b∗, is a lower bound, which holds with probability 1 − δ, and therefore, the probability of violating this bound is δ. This is stated in Equation 3.\n\n21\n\n012345678910Window data (k/100)69.075.281.487.693.8100.0AUROCOurs-SROurs-Ent012345678910Window data (k/100)531013MarginOurs-EntOurs-SR012345678910Window data (k/100)3548617487100AUROCKS-BBSD-SOurs-Ent012345678910Window data (k/100)5564738291100AUROCKS-BBSD-SOurs-Ent012345678910Window data (k/100)44.055.266.477.688.8100.0AUROCKS-BBSD-SOurs-Ent012345678910Window data (k/100)5564738291100AUROCKS-BBSD-SOurs-Ent012345678910Window data (k/100)37.049.662.274.887.4100.0AUROCKS-BBSD-SOurs-Ent012345678910Window data (k/100)941731MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)821222MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)821222MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)851832MarginOurs-EntKS-BBSD-S012345678910Window data (k/100)821222MarginOurs-EntKS-BBSD-S",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a distribution shift detection method, with the information of k-window data instances. The paper proposes a coverage-based method with a guaranteed bound, based on this, a further detection algorithm is proposed for efficient processing with the advantages of algorithmic complexity in terms of both space and time. The evaluation results also demonstrate the potential of the proposed technique.\n\n# Strength And Weaknesses\n\nStrength:\n\n- Important problem\n- Group-based distribution shift detection is less investigated, the proposed techniques are Sound and feasible solution\n- Some formal guarantees of the bound are analyzed\n- Extensive evaluation to demonstrate the potential usefulness\n- Promising results\n\nWeakness:\n\n- Evaluation is only performed on image data for classification tasks.\n- The scenarios of the distribution shifts are mostly simulated, not real scenarios from the real world.\n- Unclear about the insights of “k” selection and its relation to the population size of training data or the original data distribution P.\n- Unclear how to extend to streaming data cases.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is mostly well-organized and written. The paper is somehow novel, which investigates the distribution shift detection from a multi-instance angle, which also gives a nice discussion of previous work.\n\n# Summary Of The Review\n\nOverall, I enjoyed reading this paper, the proposed techniques should be sound and feasible. The evaluation is relatively comprehensive on the case of CIFAR and ImageNet. In addition, the authors also provide a formal analysis of the bound analysis,  as well as the detection complexity analysis of the proposed algorithms.\n\nEven though, the paper still posts a few concerns that the authors could consider for further enhancement.\n\n- The proposed methods rely on quite a few hyper-parameters, I would recommend add more discussion on their insights, the heuristic to choose them, as well as evaluating the impact of such selection.\n\n- During the evaluation, the authors mostly compared with KS and MMD methods, and simply believed that the single-instance-based methods e.g., for OoD detection would not work, which is not fully convincing to me. I would highly recommend adding comparisons with some SOTA instance-based OoD/Uncertainty detection liked methods, under the window k, for comparative analysis. This could be critical from my perspective, especially if the instance-based-OoD detection already works quite well, which is also efficient in some cases, e.g., under what conditions.\n\n- Regarding the evaluation scenario, I understand authors might try hard to try simple scenarios to simulate distribution shift, but the evaluated scenarios are quite artificial, which might be quite far away from real-world cases. I would recommend authors (1) considering some real-world cases, (2) as well as evaluating on more diverse tasks.\n\nFor example, the following paper could be a good reference.\n\n**WILDS: A Benchmark of in-the-Wild Distribution Shifts**\n\n***Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, Percy Liang***\n *Proceedings of the 38th International Conference on Machine Learning*\n, PMLR 139:5637-5664, 2021.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2022\n\nADVERSARIAL LEARNED FAIR REPRESENTATIONS USING DAMPENING AND STACKING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAs more decisions in our daily life become automated, the need to have machine learning algorithms that make fair decisions increases. In fair representation learning we are tasked with finding a suitable representation of the data in which a sensitive variable is censored. Recent work aims to learn fair representations through adversarial learning. This paper builds upon this work by introducing a novel algorithm which uses dampening and stacking to learn adversarial fair representations. Results show that that our algorithm improves upon earlier work in both censoring and reconstruction.\n\n1\n\nINTRODUCTION\n\nThe need to have machine learning algorithms that make fair decisions becomes increasingly important in modern society. A decision is fair if it does not depend on a sensitive variable such as gender, race, or age. Models trained with biased data can lead to unfair decisions Mehrabi et al. (2021). In fair representation learning we are tasked with finding a suitable representation of the data in which the sensitive variable is censored. This ensures that these representations can be used for any downstream task, such as classification or segmentation, which should not rely on the value of the sensitive variable. Throughout this paper, we often refer to this sensitive variable as the protected variable.\n\nFairness can be applied to machine learning algorithms at roughly three stages of the process: during preprocessing, inprocessing or postprocessing. With preprocessing we aim to learn a new representation of the input data which is more fair. A well known example of this is Zemel et al. (2013), which obfuscates inputs when it can lead to unfairness. With inprocessing techniques the task is to make a machine learning algorithm more fair during training, typically by modifying the learning algorithm or by adding extra constraints to the learning objective. With postprocessing we are trying to correct the predictions of a machine learning algorithm after training in order to achieve fairness. In recent years, inprocessing techniques such as Zhang et al. (2018) have become very popular since they typically strike an optimal balance between accuracy and fairness. However, the major advantage of preprocessing over any other technique is that the transformed data can be used for any downstream task, both supervised and unsupervised. This makes preprocessing still invaluable in many practical applications where we know the protected variable beforehand, but have no specific machine learning task in mind yet. Hence the focus on preprocessing in this paper. Moreover, as shown in McNamara et al. (2017), preprocessing techniques can still provide us with theoretical fairness guarantees if required.\n\nIt is important to note that the notion of fairness is not trivial, and a multitude of fairness constraints have been proposed pertaining to both group fairness and individual fairness Mitchell et al. (2021). In this paper we adopt the demographic parity constraint due to its widespread use in benchmarking and evaluating fairness of machine learning algorithms. Demographic parity enforces that a classifier treats the data containing the protected variable statistically similar to the general population, and a major downside of this criterion is that it tends to cripple accuracy as long as we achieve equal acceptance rates. In reality, for every specific dataset and problem we need to assess which fairness criterion is most applicable and cannot simply select one as preferred Verma & Rubin (2018). The upside however is that in this paper we encode our fairness constraint in the form of a loss function, and as shown in Madras et al. (2018) we are able to associate different loss functions to different\n\n1\n\nUnder review as a conference paper at ICLR 2022\n\ngroup fairness constraints. This makes this approach applicable to far more fairness metrics than the one that we adopted in this paper.\n\nOften with learning a fair representation, the naive approach of dropping certain features of the data is insufficient. The origin of the bias might latently depend on some nonlinear combination of other variables, and can thus leak back into a decision making model. This inspired the work by Edwards & Storkey (2016), which aims to learn a fair representation through adversarial learning. They use an auto-encoder as a generator for the new representation whose aim is to learn a new latent representation which attempts to censor the protected variable for the adversary. This work was later extended in Madras et al. (2018) where they propose learning objectives for other fairness metrics such as equalized odds and equal opportunity. In Kenfack et al. (2021) this work was further extended by introducing stacked auto-encoders to enforce fairness and improve censoring at different latent spaces.\n\nThis work builds on the previous adversarial approach. In particular it focuses on the case where the downstream task we may encounter is unknown, i.e. it can be either some supervised classification objective or some unsupervised clustering or segmentation objective. The challenge with learning fair representations is that on one hand we want to censor the data, and on the other we want to retain as much information as possible. Since these objectives are often opposed, the approaches in Edwards & Storkey (2016), Madras et al. (2018), Kenfack et al. (2021), and various others define the global objective of the model as a weighted sum of reconstruction error and predictive loss. This requires the trainer of a model to select some suitable hyperparameter which defines how much we value reconstruction error over predictive loss. This hyperparameter often has a large impact on the learned representations we get, and we can identify at least three issues with it. Firstly, we have no a priori knowledge on how the reconstruction error and the predictive loss relate. It could be nonlinear, which makes it almost impossible to make an informed decision beforehand. Secondly, the value of this hyperparameter gives us no formal guarantee of the censoring capabilities of the model. Some values can cause a collapse of the model. Thirdly, the hyperparameter choice is not explainable to the relevant stakeholders of the model. This makes it impractical for most industry use cases where hyperparameter choices need to be justified. As such, many authors using this methodology such as Edwards & Storkey (2016), Beutel et al. (2017), Madras et al. (2018), Feng et al. (2019), Kenfack et al. (2021) use a trial-and-error approach, or an arbitrary chosen constant, with regard to the choice of this hyperparameter. More often than not, the censoring capabilities of the learned representation are a hard constraint of the model. Thus, in many industry use cases, we are only interested in finding solutions in some restricted hypothesis space abiding some censoring constraint.\n\nA second perhaps even greater issue with the previous work is its instability. In particular, due to the unstable dynamic between actor and adversary we often learn suboptimal solutions. This has been observed in many cases such as Edwards & Storkey (2016) and Kenfack et al. (2021), but never fully addressed. This paper attempts to mitigate these issues by introducing a novel algorithm for learning fair representations. In particular, it uses dampening to stabilize the interaction between actor and adversary, and uses stacking to learn strong censored representations within a restricted hypothesis space.\n\nThe remainder of the paper is structured as follows: in Section 2 we briefly reiterate related work, in Section 3 we formally define the problem, in Section 4 we introduce the algorithm, in Section 5 we discuss the experiments and results, and in Section 6 we conclude this work.\n\n2 RELATED WORK\n\nIn Zemel et al. (2013) the first fair representation learning approach was presented. Their methodology aims to map input data to a new representation in terms of a probabilistic mapping to a set of prototypes. Several other noteworthy algorithms for finding fair representations are further explored in Feldman et al. (2015) and Calmon et al. (2017).\n\nIn Louizos et al. (2016) an architecture based on the Variational Auto-Encoder (VAE) was proposed in order to learn fair representations, called the Variational Fair Auto-Encoder. A similar idea is explored in Locatello et al. (2019). Although the idea of disentanglement between the protected variable and other features seems promising, it has not found widespread use yet due to the difficulty of finding independence between the sensitive and latent factors of variations.\n\n2\n\nUnder review as a conference paper at ICLR 2022\n\nIn Edwards & Storkey (2016) the first adversarial approach was introduced to learning fair representations. They use an auto-encoder as a generator for the new representation whose aim is to learn a new latent representation which attempts to censor the protected variable for the adversary. Many paper in the fairness community have followed this line, noteworthy Beutel et al. (2017) and Feng et al. (2019). This work was extended by Madras et al. (2018) where they propose learning objectives for other fairness metrics. In Kenfack et al. (2021) this work was further extended by introducing stacked auto-encoders to enforce fairness at different latent spaces.\n\n3 PROBLEM DEFINITION\n\nThis paper focuses purely on representation learning rather than classification. The aim is to learn a fair representation independent of the downstream that may be encountered (supervised or unsupervised). The notation of Edwards & Storkey (2016) of using the letter X to represent the data, and S to represent the protected variable is adopted. Each xi ∈ X is assumed to be some real-valued vector xi ∈ Rn, and each si ∈ S is either 0 or 1, denoting if instance i is sensitive or not: si ∈ {0, 1}. As argued in the introduction, the demographic parity constraint is adopted Dwork et al. (2012): given data X and protected variable S, the aim is to learn a new representation f (X) for which it holds that for any predictor g derived from f (X) we have g(f (X)) ⊥ S, i.e. g(f (x)) and S are independent. In short, the aim is to find a representation f (X) which give no predictive preference towards S. Throughout this paper f (X) is referred to as the censored representation. It is important to note that the censored representation is not (necessarily) in the same space as the original data, and can have a different number of dimensions.\n\nOn one hand the aim is to censor the representation, while on the other hand the goal is to retain as much information as possible. In order to capture these opposing objectives, the learning objective can be framed as an adversarial learning problem. Similar to Edwards & Storkey (2016) and various papers following this, two agents are modeled with competing objectives: An auto-encoder e with corresponding decoder d representing the actor; and a classifier h representing the adversary. As per usual, e, d and h are implemented in this paper using a feed-forward neural network. The aim is to find a censored representation e(X). The objective of the adversary is to predict S from the censored representation e(X), while the aim of the actor is to learn this censored representation such that d(e(X)) is as close to X as possible (the normal objective of an auto-encoder) and to deny the adversary from being able to learn S from e(X).\n\nTo make these notions precise, let Lact squared error (MSE), alternatively reconstruction error:\n\ne,d be the loss of the auto-encoder, and set it to be the mean-\n\nLact\n\ne,d =\n\n1 |X|\n\n(cid:88)\n\nxi∈X\n\n∥xi − d(e(xi))∥2\n\n2\n\nMoreover, the loss of the adversary is set to be the negative cross-entropy loss over S:\n\nLadv\n\ne,h =\n\n1 |X|\n\n(cid:88)\n\nsi, ˆsi∈S,h(e(X))\n\nsi log( ˆsi) + (1 − si) log(1 − ˆsi)\n\nNote that in Madras et al. (2018) a multitude of other loss functions are proposed which lead to different notions of fairness (e.g. equal opportunity or equalized odds), but as argued earlier this paper optimizes for parity. Defining the loss to be the negative cross-entropy allows us frame it as a maximization problem rather than a minimization one, which entails that the joint objective of the actor and adversary is in the form of a min-max problem. Particularly, let L(e, d, h) be the joint loss, and define it as a weighted sum of Lact\n\ne,d and Ladv e,h :\n\nL(e, d, h) = Lact\n\ne,d + αLadv\n\ne,h\n\ne,d over Ladv Here α is some predetermined chosen hyperparameter denoting the importance of Lact e,h . Since the negative cross-entropy loss is considered, the aim is to minimize this loss under the assumption that the adversary is trying to maximize this. Thus, the aim is to find e and d which satisfy the following:\n\nmin e,d\n\nmax h\n\nL(e, d, h)\n\n3\n\nUnder review as a conference paper at ICLR 2022\n\nOnce these e and d have been found, the fair representations can be computed with e(X) and the censored original representation by d(e(X)). Reason to use the latter can be due to the fact that these representations share a lot of the inherent properties of X both dimension- and structure-wise. Thus, depending on the use case and task, e(X) or d(e(X)) can be used to characterize the censored data.\n\n3.1 RESTRICTING THE HYPOTHESIS SPACE\n\nA problem with the joint loss function L(e, d, h) is the correct choice of α. This hyperparameter needs to be selected beforehand, and it has a large impact on the representations that are learned. Since (1) there is typically no a priori knowledge on how Lact e,h relate for a given X and S, (2) choices of α give no formal guarantee on the censoring ability of the encoder, and (3) a choice of α is hard to explain to the relevant stakeholders, any choice of α is hard to justify and interpret. Additionally, low and high values for α could cause the trivial function to be learned, i.e. either the encoder learns an uncensored representation, or a constant function is learned.\n\ne,d and Ladv\n\nIn order to eliminate these problems, we propose a different objective function. More often than not, the censoring capabilities of the target function are a hard constraint on the resulting model. We recognize that perfect censoring is in most cases not feasible, and as such these hard constraints should define a hypothesis space of possible target functions. An example hard constraint could be that we do not wish that a very competent adversary receives above 60% accuracy on trying to classify the gender based on a loan application. Such a hard constraint solves the problem of not having a formal guarantee of the target function, and is both more intuitive and explainable to the relevant stakeholders.\n\nTo formalize this, a score function scoreX,S,e(h) and accompanying threshold T is assumed to be specified beforehand which evaluates the performance of adversary h based on data X, S and encoder e. The constrained hypothesis space ˆE for e can be defined as follows:\n\nˆE = {e | score\n\nX,S,e\n\n(arg max h\n\n(Ladv\n\ne,h )) ≤ T }\n\nIt is assumed that score and T are chosen in accordance with the distribution of S such that ˆE is nonempty, e.g. if an accuracy score is used, T should at least be 50%. The global objective is now e,d by only considering encoders from the viable hypothesis space ˆE, that to simply to minimize Lact e,d . In order to find solutions in ˆE, different optimization methods need is to optimize mine∈ ˆE,d Lact to be used. One of the main contributions of this paper is that such an algorithm is supplied.\n\n4 ADVERSARIAL LEARNING USING STACKING AND DAMPENING\n\nBefore delving into the technical details, it is worthwhile to discuss the shortcomings of the current approach. As mentioned in Edwards & Storkey (2016), Madras et al. (2018), and various other papers, it is very difficult to train these models due to the unstable dynamic between the actor and the adversary. This is true for adversarial learning in general because of the underlying saddle point optimization problem. Different approaches in literature have been proposed to stabilize adversarial networks, ranging from simple solutions such as early stopping or weight clipping Arjovsky et al. (2017) to more intricate ones such as adding extra stabilizing steps Yadav et al. (2018). A multitude of stabilization methods exist, but the downside is that they need to be adapted accordingly to the properties of the loss function Xing et al. (2021). Thus, in the context of our setting, it is worthwhile to investigate what the root cause of the instability is. During training, the actor is continuously updating in the direction to make the adversary worse at predicting the protected variable (recall that the objective of the actor is to minimize the negative cross-entropy loss of the adversary, while the adversary is trying to maximize this). A key insight here is that the loss signal that the adversary is giving to the actor is paradoxical in nature:\n\n• If the magnitude of the loss is high, the adversary is incompetent at predicting S. Since the adversary is also updating its own loss towards 0, it means we are at a point where the gradient of the loss will be high. This in turn will result in a big update of the actor.\n\n4\n\nUnder review as a conference paper at ICLR 2022\n\nHowever, the adversary was already incompetent at predicting S, but a big update in an uninformative direction is performed as opposed to a smaller conservative update.\n\n• If the magnitude of the loss is relatively low, the adversary is competent at predicting S. When the loss is relatively smooth at local maxima, a low magnitude of the loss will more often than not result in a small update of the actor. However, the adversary was already competent at predicting S, but we are performing a small update in an informative direction when we would rather perform a bigger less careful update.\n\nThe key issue is thus that if the adversary is too competent then the gradients will be weak, and if the adversary is too incompetent the gradients will be uninformative. This interplay between the competence of the adversary and the size of the gradients is also mentioned in Edwards & Storkey (2016), but not further explored.\n\nNow consider what this means when we are actually training and updating the actor and adversary. When the weights of the actor and adversary are adjusted in turns, as described in Goodfellow et al. (2014), there is a risk that the adversary will never be sufficiently competent in the task. This is particularly true when a strong adversary is used with a lot of parameters, which typically need more batches to converge. This means that constantly big weight adjustments in an imprecise direction are made, which again causes the adversary to be incompetent. On the other hand when the adversary is trained in the inner loop, apart from it being very inefficient, we would also run the risk of the adversary being too strong, and not being able to make any meaningful updates.\n\nTo mitigate these problems and to make the training process more stable, the notion of dampening is introduced.\n\n4.1 DAMPENING\n\nDampening is a function that serves as a modulating term of our algorithm in the interaction between actor and adversary. Similar ideas for weighted loss schemes such as focal loss have been proposed for imbalanced datasets Lin et al. (2017), but here it is applied to modulate the actor and adversary. Dampening returns a number between 0 and 1 denoting how much information the classifier has over a training sample. First, define g as a function over subsets S′ of our protected variable S′ ⊆ S:\n\ng(S′) =\n\n1 |S′|\n\nmax\n\n\n\n\n\n\n\n(cid:88)\n\ns′ i,\n\n(cid:88)\n\n1 − s′\n\ni\n\n\n\ns′\n\ni∈S′\n\ns′\n\ni∈S′\n\nIn words, g(S′) represents the best possible accuracy a predictor can receive when using only information about S′. Observe that since the protected variable is binary, g(S′) ∈ [0.5, 1]. The role of g(S′) is to serve as a baseline guessing accuracy.\n\nNow given a classifier f and training sample X ′, S′, dampening δ is defined as:\n\nδ(f, X ′, S′) =\n\nmax(0, acc(f, X ′, S′) − g(S′)) 1 − g(S′)\n\nIn the above definition, acc(f, X ′, S′) is used as shorthand notation to denote the accuracy score of f on training sample X ′, S′. Whenever g(S′) = 1, we set δ(f, X ′, S′) = 0. In words, dampening δ(f, X ′, S′) ∈ [0, 1] tells us the percentage decrease of number of misclassifications would we use f instead of guessing the most frequent label in the sample. Whenever dampening is 1 for f , we know that f achieves perfect accuracy on the training sample X ′, S′, and whenever dampening is 0 we would be no worse off by just informed guessing. Thus, dampening is a measure of information a classifier has over a certain classification task. An important property of dampening is that it is contained within the unit interval, meaning that it can be used as a modulating term since the results will never be larger than the original value. Multiple notions of bounded information were explored, but found dampening to work the best for a variety of tasks. We suspect it is due to its linear scaling with the number of correctly classified samples whenever its value is nonzero; a small increase in correctly classified samples translates in a small increase in dampening, and vice versa for a big increase.\n\n5\n\nUnder review as a conference paper at ICLR 2022\n\nAlgorithm 1 ALFR-DS\n\nInitialize e = id, initialize θact and θadv randomly. repeat\n\nInitialize enew randomly. e = enew ◦ e repeat\n\ne,d (X ′) e,h (X ′, S′)\n\nX ′, S′ = random mini-batch from X, S Lact = Lact Ladv = Ladv θact = θact − η · (cid:0)∇θact Lact + δ(h ◦ e, X ′, S′) · ∇θactLadv(cid:1) θadv = θadv + η · (1 − δ(h ◦ e, X ′, S′)) · ∇θadv Ladv\n\nuntil Sufficient epochs reached. Freeze encoder e.\n\n▷ Start with the ”empty” encoder.\n\n▷ Add new encoder to the (frozen) stack.\n\nuntil scoreX,S,e(h) ≤ T or Deadline reached.\n\n4.2 STACKING\n\nStacking is a technique for censoring which was recently introduced in Kenfack et al. (2021). The idea is that during training we start out with a simple encoder which learns a censored representation. After this initial training phase, we freeze the encoder and append a new trainable one. This process continues until we are completely done with training. Another perspective on this process is that once we learn a censored representation, we recursively start over a completely new training process, except that we use the censored representation as the new input. The key idea behind stacking is that once a censored representation is learned and frozen, it is highly likely that some information about the protected variable is lost for good. Thus in theory, repeating the stacking operation can give us representation with arbitrary strong censoring properties.\n\nIt is important to note that the authors found that stacking did increase censoring over the original approach, but unsurprisingly also comes at the cost of reconstruction error. In other words, stacking should preferably be combined with a very careful and stable censoring algorithm, which in our case is handled by the addition of dampening. Stacking together with dampening serves the basis for our algorithm.\n\n4.3 ALFR-DS\n\nIn Algorithm 1 a new algorithm is proposed called ALFR-DS (“Adversarial Learned Fair Representations using Dampening and Stacking”). This algorithm differs from basic ALFR, as discussed in Edwards & Storkey (2016), on three key aspects:\n\n• A different loss function for the actor and adversary is used, instead of the same function\n\nL(e, d, h) as given in Section 3 for both.\n\n• An inner loop for normal backpropagation is used, and an extra outer loop which incorporates stacking is added. An extra termination condition is added which allows us to find solutions in the restricted hypothesis space ̄E, as defined in Section 3.1. • The actor and adversary are trained concurrently instead of interleaved.\n\nIn the description of the algorithm, Lact e,h (X ′, S′) is used to denote the loss functions defined in Section 3 applied to X ′ and S′. Moreover θact and θadv refer to the model parameters of the actor (e and d) and subsequently the adversary (h). The fixed learning rate η can be replaced with a parameter-dependent dynamic one: in all of the experiments the Adam optimizer has been found to work the best Kingma & Ba (2014).\n\ne,d (X ′) and Ladv\n\nThe role of dampening in the algorithm is to act like a “fuzzy” turn-taking mechanism: whenever the adversary is weak, δ will be close to 0 in our algorithm. This means the actor will hardly use the loss of the adversary in updating the censored representations, i.e. it will act like a normal autoencoder. Since representation learning in a normal auto-encoder is stable, it gives the adversary time to learn and catch up. Whenever the adversary is strong, δ will be close to 1, meaning the adversary will hardly update itself. This allows the auto-encoder to incorporate the loss of the adversary and\n\n6\n\nUnder review as a conference paper at ICLR 2022\n\nlearn a new censored representation. In other words, the actor can catch up. This is how dampening attempts to stabilize the learning process: it gives either the actor or the adversary time to catch up, resulting in only informative updates of the model. In the experiments no extra stabilizing methods such as gradient clipping were needed.\n\nThe width and depth of the encoder that is added to the stack can be varied at any moment. In our implementation, every subsequent encoder after the first uses the same input and output size. In order to be able to censor nonlinear relations in the data at every step, every encoder is given a single hidden layer. It is important to note that the adversary h can have any neural architecture, depending on the desired censoring strength of the resulting model. It is also important to note that a strong adversary typically means a longer training period for convergence.\n\nThe termination condition scoreX,S,e(h) ≤ T tells us when an encoder is contained within the desired hypothesis space given adversary h. Although not explicitly mentioned in the algorithm, it is sometimes beneficial to fully train h on the training data X, S without additionally training the encoder after termination of the inner loop. This is to ensure that the adversary is fully converged before an assessment about the censoring capabilities of the model is made. Under the reasonable assumption that score is chosen in such a way that it eventually decreases as Ladv e,h decreases, and that Ladv e,h decreases after a completion of the inner loop, it can be concluded that scoreX,S,e(h) ≤ T will eventually hold. In other words, ALFR-DS will eventually find a solution in the constrained hypothesis space. In order to encourage that Ladv e,h decreases after a completion of the inner loop, the hidden size of each encoder that is added to the stack can be decreased, or the inner loop can be terminated early. However, since it is often undesirable that the model complexity of the encoder grows arbitrarily large, an extra deadline criterion is used to allow early stopping. Whenever an early termination occurs, a solution with the desired censoring capabilities was not found within the complexity bounds of the model.\n\n5 EXPERIMENTS AND RESULTS\n\nThis section consists of two different experiments that were conducted. Motivated by the image anonymization task proposed in Edwards & Storkey (2016), the aim of the first experiment is to closely compare ALFR with ALFR-DS on a widely used image dataset (MNIST; Deng (2012)). The second experiment is more general and aims to compare several variants of ALFR, ALFR-DS and other preprocessing algorithms from the widely used IBM AI Fairness 360 package Bellamy et al. (2018) on several widely used fairness-related datasets.\n\n5.1 MNIST\n\nThe dataset contains 60,000 handwritten images with corresponding labels. The goal is to censor all the 8s in the dataset, i.e. the protected variable si is set to 1 whenever the label is 8, and 0 otherwise. Even though this task does not serve a direct practical use, it is a great benchmark for its inherent challenging properties. In particular, the protected variable is unevenly distributed and the task is very nonlinear in nature. Moreover, due to its wide spread it allows for easy replication.\n\nIn this experiment the goal is to compare ALFR-DS to ALFR in its ability to reconstruct and censor. In order to ensure a fair comparison, every model was trained for 30 epochs. For ALFR-DS, 3 variants were considered: ALFR-DS(1) which runs the inner loop of Algorithm 1 once for 30 epochs, ALFR-DS(2) which runs the inner loop twice for 15 epochs, and ALFR-DS(3) which runs the inner loop three times for 10 epochs. Both ALFR-DS and ALFR used the same Multi-Layer Perceptron (MLP) adversary. However, we observed that ALFR typically performs better against a weak adversary, so we also considered a variant of ALFR against an adversary using simple Logistic Regression (LR). These two variants are referred to as ALFR (MLP) and ALFR (LR). For ALFR several values for α were tried, but only report for α = 1 since ALFR-DS outperforms ALFR for all nontrivial choices of α in both censoring and reconstruction. Both ALFR-DS and ALFR were given the same number of target dimensions to embed to. In order to measure how well a model censors, each model was trained on one slice of the data, after which another slice was used for evaluation. Two new classifiers, one using LR and one using an MLP, were freshly trained on these new representations and were asked to predict protected variable S. The resulting accuracy scores were used as a benchmark (lower is better). Additionally, a normal auto-encoder was trained that did\n\n7\n\nUnder review as a conference paper at ICLR 2022\n\nLR\n\nMLP\n\n0.91 ± 0.01 0.9 ± 0.0 0.91 ± 0.0 0.92 ± 0.03 0.93 ± 0.02 0.96 ± 0.0\n\n0.98 ± 0.01 0.95 ± 0.01 0.93 ± 0.02 0.95 ± 0.01 0.95 ± 0.01 0.99 ± 0.0\n\nALFR-DS(1)\n\nALFR-DS(2)\n\nALFR-DS(3)\n\nALFR (LR)\n\nALFR (MLP)\n\nUncensored\n\nFigure 1: Left shows the reconstruction error (MSE) over 30 epochs for the MNIST task. Top-right lists all the accuracy scores of different adversaries with increasing strength trained on the final representation, where lower is better. Bottom-right shows some examples of the original MNIST digits (black outline at the top) which were both censored by ALFR-DS(3) (green outline in the middle) or by ALFR (LR) (red outline at the bottom).\n\nnot use an adversary and called this Uncensored (which is equivalent of using ALFR with α = 0). All experiments were repeated 10 times to account for naturally occurring deviations in the results.\n\nIn Figure 1 the result of the experiment can be seen, which also provides a visual output of ALFRDS(3) versus ALFR (LR). In one instance ALFR-DS censors an 8 to a 3 and in another instance an 8 to a 6. From this visual inspection, it is clear that a lot more information is lost about the original images with ALFR. Moreover, an “imprint” of an 8 across most images in the censored representations can be observed. The “spikes” in the reconstruction graph occur at the times in which a new encoder is added to the stack; in this case the loss briefly rises since each new encoder is initiated randomly. It is also interesting to note is that for the first 3 to 5 epochs, the graphs of ALFR-DS and Uncensored are almost identical due to the fact that especially in the early phases dampening will be relatively small. The learned representations are still evolving a lot, meaning that the adversary has no time to become competent at predicting S. Only when the representations start to stabilize after approximately 5 epochs, a difference in the graphs can be observed.\n\nThe lowest theoretical accuracy that can be achieved due to the distribution of S is 0.9 for this task and the highest possible accuracies are reported for Uncensored. Again ALFR-DS achieves superior results in both censoring and reconstructing over ALFR. It is clear from the results that the number of stacked encoders in ALFR-DS does come at the cost of reconstruction. Particularly, ALFR-DS(1) is outperformed by ALFR-DS(3) in terms of censoring, however ALFR-DS(1) leaves much more of the data intact. It is thus clear that a balance has to be struck between censoring and reconstruction, which is normally handled by the algorithm with the use of the score function with accompanying threshold T . In both tasks when it comes to censoring, ALFR-DS(2) and ALFR-DS(3) perform similarly except when a strong nonlinear MLP adversary was tasked to predict S, in which case it becomes clear that ALFR-DS(3) has greater censoring capabilities. It is noteworthy to observe that ALFR(LR) outperforms ALFR(MLP) for censoring even though a weaker adversary was used during training. The reason is that LR converges much faster, and thus gives a more informative loss signal to the actor after each turn. Due to dampening, ALFR-DS does not have this problem and can reliably be used against strong adversaries.\n\n5.2 AIF360 BENCHMARK\n\nTo really see how ALFR-DS performs in practice, it is compared to several preprocessing algorithms on several datasets using AIF360. Finding the right comparison is non-trivial, since ALFR-DS is designed to learn a censored representation for any target variable as opposed to one specific variable. In practice this means that ALFR-DS does not have access to the target variable at training time, putting it at a major disadvantage. Taking this into consideration, it is still worthwhile to investigate how ALFR-DS compares.\n\n8\n\n051015202530Epoch0.010.020.030.040.050.060.070.08MSEAlgorithmALFR-DS(1)ALFR-DS(2)ALFR-DS(3)ALFR (LR)ALFR (MLP)UncensoredUnder review as a conference paper at ICLR 2022\n\nDataset\n\nProtected\n\nAdult\n\nGerman\n\nSex\n\nRace\n\nSex\n\nAge\n\nBank\n\nAge\n\nCOMPAS\n\nSex\n\nRace\n\nMetric\n\nBA ∆DP\n\nBA ∆DP\n\nBA ∆DP\n\nBA ∆DP\n\nBA ∆DP\n\nBA ∆DP\n\nBA ∆DP\n\nALFR-DS(1)\n\nALFR-DS(2)\n\nALFR-DS(3)\n\nALFR-S(1)\n\nALFR-S(2)\n\nALFR-S(3)\n\nDIR\n\nLFR\n\nUncensored\n\n69 68 67 73 70 69 75 ∗50 78\n\n17 15 14 18 17 17 17 ∗0 18\n\n68 66 67 73 68 69 76 ∗50 78\n\n8 8\n6 9\n8 8\n9 ∗0 9\n\n63 ∗58 ∗51 ∗55 ∗55 ∗55 65 ∗50 66\n\n5 ∗4 ∗2 ∗5 ∗5 ∗5 6\n∗0 7\n\n62 ∗58 ∗57 ∗57 ∗53 ∗54 68 ∗50 70\n\n10 ∗8 ∗7 ∗6 ∗7 ∗7 11 ∗0 16\n\n60 ∗58 ∗57 ∗57 ∗51 ∗51 83 ∗51 75\n\n3 ∗3 ∗3 ∗3 ∗1 ∗1 5\n∗2 8\n\n66 65 63 66 61 ∗58 67 ∗50 68\n\n14 13 11 12 11 ∗8 15 ∗1 20\n\n64 64 63 65 62 ∗58 67 ∗50 68\n\n15 13 12 16 12 ∗11 8\n∗1 18\n\nTable 1: Results (%). Best are given in bold and discarded (< 60% BA) marked with *.\n\nThis experiment is conducted on several datasets and protected variables. For each combination, a censored representation is learned on 80 percent of the data using the algorithm of choice. Afterwards a gradient boosting classifier was trained on these representations to predict the target variable. The performance was evaluated on the remaining 20 percent. Due to imbalanced datasets, the balanced accuracy (BA) was reported together with the demographic parity distance (∆DP ), which is the proportional distance of positive outcomes between the privileged and unprivileged groups. To strike a balance between censoring and accuracy, only results with a BA of 60% are considered. This is to remove all cases where there is too much information removed (e.g. the trivial uni-label function). From these candidates the one with the lowest ∆BP is elected as the winner.\n\nTo analyze the individual effects of stacking and dampening, an ablation study is performed where ALFR-S(n) is default ALFR performed with n stacks, and ALFR-DS(n) ALFR-DS with n stacks. Thus, vanilla ALFR is equal to ALFR-S(1) and ALFR-DS without stacking is equal to ALFRDS(1). Additionally, Learning Fair Representations (LFR) Zemel et al. (2013), and Disparate Impact Remover (DIR) Feldman et al. (2015) are compared. Note that the removal of disparate impact is equivalent to achieving demographic parity. The optimized stochastic preprocessing technique from Calmon et al. (2017) was not benchmarked due to unavailability of the required preprocessing and distortion functions for all datasets. The datasets that were used are UCI Adult (predict the income of a person) Asunci ́on & Newman (2007), German (predict defaults on consumer loans), Bank Marketing (predict subscription of a term deposit) Moro et al. (2014), and the well-known COMPAS dataset (predict recidivism).\n\nIn Table 1 the average percentage results of 10 runs are shown. In cases where careful censoring is needed to stay above 60% BA, ALFR-DS(1) gives good results (German and Bank). In cases where there is more room for censoring, ALFR-DS(3) performs better. Whenever accuracy outweighs fairness, DIR tends to outperform ALFR-DS, but note that DIR is conditioned on the target variable and in some cases (Adult) stays close to the uncensored representations. Only in the case of the COMPAS + race task does DIR outperform ALFR-DS in terms of censoring. ALFR-S(n) either seems to censor too strongly (German, Bank), or not strong enough (Compas, Bank). Moreover, LFR seems to give poor results overall, even after numerous attempts to optimize the hyperparameters. Overall, ALFR-DS(n) gives the best results when a careful balance between censoring and accuracy is needed, where higher n implies a stronger censoring.\n\n6 CONCLUSIONS\n\nIn this paper, we have given a novel algorithm that uses dampening to stabilize the interaction between actor and adversary, and uses stacking to learn strong censored representations within a restricted hypothesis space. This algorithm outperforms the current approach in both censoring and reconstruction, as shown in our empirical results.\n\nSince the aim is to learn meaningful representations which can be learned for fair downstream tasks, we believe that a good comparison should be made with VAE’s that attempt to learn these disentangled representation. Moreover, due to the empirical nature of this study, theoretical bounds on reconstruction and censoring using stacking and dampening should be further explored.\n\n9\n\nUnder review as a conference paper at ICLR 2022\n\nREFERENCES\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214–223. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/ arjovsky17a.html.\n\nA. Asunci ́on and D.J. Newman. UCI Machine Learning Repository. http://www.ics.uci. edu/ ̃mlearn/MLRepository.html, 2007. URL http://www.ics.uci.edu/$\\ sim$mlearn/{MLR}epository.html.\n\nRachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. AI fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. CoRR, abs/1810.01943, 2018. URL http://arxiv.org/abs/1810.01943.\n\nAlex Beutel, Jilin Chen, Zhe Zhao, and Ed H. Chi. Data decisions and theoretical implications when adversarially learning fair representations. CoRR, abs/1707.00075, 2017. URL http: //arxiv.org/abs/1707.00075.\n\nFlavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and In I. Guyon, Kush R Varshney. Optimized pre-processing for discrimination prevention. U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 9a49a25d845a483fae4be7e341368e36-Paper.pdf.\n\nLi Deng. The mnist database of handwritten digit images for machine learning research.\n\nIEEE\n\nSignal Processing Magazine, 29(6):141–142, 2012.\n\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS ’12, pp. 214–226, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450311151. doi: 10.1145/2090236.2090255. URL https://doi.org/10. 1145/2090236.2090255.\n\nHarrison Edwards and Amos Storkey. Censoring representations with an adversary. In International Conference in Learning Representations (ICLR2016), pp. 1–14, May 2016. URL https:// iclr.cc/archive/www/doku.php%3Fid=iclr2016:main.html. 4th International Conference on Learning Representations, ICLR 2016 ; Conference date: 02-05-2016 Through 04-05-2016.\n\nMichael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, pp. 259–268, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450336642. doi: 10.1145/2783258.2783311. URL https://doi.org/10.1145/2783258.2783311.\n\nRui Feng, Yang Yang, Yuehan Lyu, Chenhao Tan, Yizhou Sun, and Chunping Wang. Learning fair\n\nrepresentations via an adversarial framework. ArXiv, abs/1904.13341, 2019.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherIn (eds.), Information Processing Systems, volume 27. Curran Associates, URL https://proceedings.neurips.cc/paper/2014/file/\n\njil Ozair, Aaron Courville, and Yoshua Bengio. Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger Advances Inc., 5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.\n\nGenerative adversarial nets.\n\nin Neural\n\n2014.\n\nPatrik Joslin Kenfack, Adil Mehmood Khan, Rasheed Hussain, and S. M. Ahsan Kazmi. Adversarial\n\nstacked auto-encoders for fair representation learning, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2022\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, URL http://dblp.uni-trier.de/db/journals/corr/\n\nabs/1412.6980, 2014. corr1412.html#KingmaB14.\n\nTsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll ́ar. Focal loss for dense object detection. CoRR, abs/1708.02002, 2017. URL http://arxiv.org/abs/1708. 02002.\n\nFrancesco Locatello, Gabriele Abbati, Tom Rainforth, Stefan Bauer, Bernhard Sch ̈olkopf, and Olivier Bachem. On the fairness of disentangled representations. CoRR, abs/1905.13662, 2019. URL http://arxiv.org/abs/1905.13662.\n\nChristos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on fair autoencoder. Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.00830.\n\nDavid Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3384–3393. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr. press/v80/madras18a.html.\n\nDaniel McNamara, Cheng Soon Ong, and Robert C. Williamson. Provably fair representations.\n\nCoRR, abs/1710.04394, 2017. URL http://arxiv.org/abs/1710.04394.\n\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM Comput. Surv., 54(6), jul 2021. ISSN 0360-0300. doi: 10.1145/3457607. URL https://doi.org/10.1145/3457607.\n\nShira Mitchell, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. Algorithmic fairness: Choices, assumptions, and definitions. Annual Review of Statistics and Its Application, 8(1):141–163, 2021. doi: 10.1146/annurev-statistics-042720-125902. URL https://doi. org/10.1146/annurev-statistics-042720-125902.\n\nS ́ergio Moro, P. Cortez, and Paulo Rita. A data-driven approach to predict the success of bank\n\ntelemarketing. Decis. Support Syst., 62:22–31, 2014.\n\nSahil Verma and Julia Rubin. Fairness definitions explained. In Proceedings of the International Workshop on Software Fairness, FairWare ’18, pp. 1–7, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450357463. doi: 10.1145/3194770.3194776. URL https://doi.org/10.1145/3194770.3194776.\n\nYue Xing, Qifan Song, and Guang Cheng. On the algorithmic stability of adversarial training. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 26523–26535. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ df1f1d20ee86704251795841e6a9405a-Paper.pdf.\n\nAbhay Yadav, Sohil Shah, Zheng Xu, David Jacobs, and Tom Goldstein. Stabilizing adversarial nets with prediction methods. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Skj8Kag0Z.\n\nRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair repreIn Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th Insentations. ternational Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 325–333, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL https: //proceedings.mlr.press/v28/zemel13.html.\n\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. CoRR, abs/1801.07593, 2018. URL http://arxiv.org/abs/1801.07593.\n\n11",
    "reference": "# Summary Of The Paper\n\nThe paper focusses on adversarial fair representation where the model should not rely on the sensitive variable based on a preprocessing approach. The authors highlight the problem with adversarial min-max loss leading to either  (a) high gradient and high loss in case of incompetent adversary; or (b) small loss and slow actor updates in the case of good adversary. They  introduce dampening, a new measure of information captured from a protected variable scaled to [0, 1] which balances between the min-max adverasarial loss objective between actor and adversary. This is coupled with stacking [Kenfact et al., 2021] where exist encoder is frozen (leading to loss of some information about the protected variable) and a new trainable single-layer encoder is appended to the existing encoder per training stage. Empirical evaluation on MNIST and AIF360 datasets shows censoring of protected variable (e.g., \"8\" in the MNIST dataset) using the ALFR-DS approach with the stacking helping (ALFR-DS(3) vs ALFR-DS(1)).\n\n# Strength And Weaknesses\n\n+ Clearly written paper for the most part.\n+ MNIST experiments shows the influence of the approach with protecting censored data. \n+ Mixed results on AIF360 dataset per my limited understanding - I do not follow the argument on ablation study and improvement obtained on dampening.\n\n- Some recommendations: AIF360 benchmark and the dataset should be described better including what are the features, what is being censored. I strongly recommend adding a section to appendix/supplementary material to capture this information making the paper self-contained when reading the evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* The paper introduces a modified loss-objective (Dampening) and couples it with existing approach of Stacking [Kenfack et al., 2021] in order to obtain better demographic parity based on preprocessing. \n* Authors show using empirical evaluation that their approach performs better than existing approaches\n* Theoretical fairness guarantee is not investigated for dampening-based modified loss, though as the authors write \"preprocessing techniques can still provide us with theoretical fairness guarantees if required.\"\n\n# Summary Of The Review\n\nThe paper presents a new loss objective (dampening) coupled with stacking as a preprocessing method and show better censoring as part of empirical evaluation. There is no theoretical fairness guarantee accompanying the new loss and the empirical evaluation section could be written better to aid understanding of how dampening helps.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBI-LEVEL DYNAMIC PARAMETER SHARING AMONG INDIVIDUALS AND TEAMS FOR PROMOTING COLLABORATIONS IN MULTI-AGENT REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nParameter sharing has greatly contributed to the success of multi-agent reinforcement learning in recent years. However, most existing parameter sharing mechanisms are static, and parameters are indiscriminately shared among individuals, ignoring the dynamic environments and different roles of multiple agents. In addition, although a single-level selective parameter sharing mechanism can promote the diversity of strategies, it is hard to establish complementary and cooperative relationships between agents. To address these issues, we propose a bi-level dynamic parameter sharing mechanism among individuals and teams for promoting effective collaborations (BDPS). Specifically, at the individual level, we define virtual dynamic roles based on the long-term cumulative advantages of agents and share parameters among agents in the same role. At the team level, we combine agents of different virtual roles and share parameters of agents in the same group. Through the joint efforts of these two levels, we achieve a dynamic balance between the individuality and commonality of agents, enabling agents to learn more complex and complementary collaborative relationships. We evaluate BDPS on a challenging set of StarCraft II micromanagement tasks. The experimental results show that our method outperforms the current state-of-the-art baselines, and we demonstrate the reliability of our proposed structure through ablation experiments.\n\n1\n\nINTRODUCTION\n\nIn many areas, collaborative Multi-Agent Reinforcement Learning (MARL) has broad application prospects, such as robots cluster control (Bus ̧oniu et al., 2010), multi-vehicle auto-driving (Bhalla et al., 2020), and shop scheduling (Jim ́enez, 2012). In a multi-agent environment, an agent should observe the environment’s dynamics and understand the learning policies of other agents to form good collaborations. Real-world scenarios usually have a large number of agents with different identities or capabilities, which puts forward higher requirements for collaborations among agents. Therefore, how to solve the large-scale MARL problem and promote to form stable and complementary cooperation among agents with different identities and capabilities are particularly important.\n\nTo solve the large-scale agents issue, we can find that many collaborative MARL works adopting the centralized training paradigm use the full static parameter sharing mechanism (Gupta et al., 2017), which allows agents to share parameters of agents’ policy networks, thus simplifying the algorithm structure and improving performance efficiency. This mechanism is effective because agents generally receive similar observation information in the existing narrow and simple multiagent environments. In our Google Research Football (GRF) (Kurach et al., 2020) experiments, we can find that blindly applying the full parameter sharing mechanism does not improve the performance of algorithms because the observation information is very different due to the movement of different players. At the same time, because the full static parameter sharing mechanism ignores the identities and abilities of different agents, it constantly limits the diversity of agents’ behavior policies (Li et al., 2021; Yang et al., 2022), which makes it difficult to promote complementarity and reliable cooperation between agents in complex scenarios.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nRecently, in order to eliminate the disadvantage of full parameter sharing, a single-layer selective parameter sharing mechanism has been proposed (Christianos et al., 2021; Wang et al., 2022), that is, extracting deep features from agents’ observation information by an encoder and clustering them to achieve the combination of different agents in order to select different agents for parameter sharing.\n\nAlthough the single-level selective parameter sharing mechanism can promote the diversity of agents’ policies, it causes the relationship between agents that do not share parameters simultaneously fragmented. So that agents cannot establish complementary cooperative relationships in a broader range. More importantly, designing an effective selector is the key to sharing selective parameters, especially for the single-level dynamic selective parameter sharing mechanism, which needs to be done within several rounds of the selection operation. Most methods only use the realtime observation information of agents, which loses attention to agents’ history and is not conducive to correctly mining the implicit identity characteristics of the agents. For a football team, special training for different players, such as shooting training for the forwards and defense training for the defenders, will be carried out. However, winning a game requires special training and coordination between players of different roles. That is, not only do we need to share parameters with agents of the same role, but we also need to combine agents of different identities to ensure that they can form robust and complementary collaboration on a larger scale.\n\nTo address these issues, in this paper, we propose a bi-level dynamic parameter sharing mechanism among individuals and teams (BDPS). The advantage functions of agents can be expressed as the advantages of taking action relative to the average in the current state. We consider that the advantage function can better represent the actual roles of agents in current identity and grouping. In order to more accurately identify the role of agents, at the individual level, we compute the long-term advantage information of the agents as the key to virtual role identification and use the variational autoencoders (VAE) (Kingma & Welling, 2014) to learn the distribution of the role characteristics of the agents, and obtain more accurate virtual role directly by sampling the distribution of role features. To alleviate the split of the relationship between different virtual role agents by the singlelayer dynamic parameter sharing mechanism, we further use the graph attention network (GAT) (Velickovic et al., 2018) to learn the topological relationships between different roles based on the roles obtained at the individual level, so as to achieve the combination of different identity agents at a higher level and a broader range. Through the method we designed, we achieve dynamic and selective parameter sharing for agents at two different levels, individual and team, and achieve the goal of stabilizing complementary collaboration among agents in a more extensive scope while achieving the diversity of agents’ policies.\n\nWe test BDPS and algorithms using different parameter sharing mechanisms on the StarCraft II micromanagement environments (SMAC) (Samvelyan et al., 2019) and the Google Research Football (GRF) (Kurach et al., 2020). The experimental results show that our method not only outperforms other methods with single-level selective parameter sharing mechanism and full parameter sharing mechanism in general on all super hard maps and four hard maps of SMAC, but also performs well in the experimental scenarios in the used GRF. In addition, we carried out ablation experiments to verify the influence of different parameter sharing settings on the formation of complementary cooperation between agents, which fully proves the reliability of our proposed method.\n\n2 BACKGROUND\n\n2.1 DECENTRALIZED PARTIALLY OBSERVABLE MARKOV PROCESS\n\nA full cooperative MARL task can usually be modeled as a decentralized partially observable markov process (Dec-POMDP) (Oliehoek & Amato, 2016). We can define a tuple M = ⟨N , S, A, R, P, Ω, O, γ⟩ to represent it, where N is the finite set of n agents, s ∈ S is a finite set of global state, and γ ∈ [0, 1). At each time step t, each agent i ∈ N receives a local observation oi ∈ Ω according to the observation function O (s, i), takes an action ai ∈ A to form a joint action a ∈ An, and gets a shared global reward r = R (s, a). Due to the limitation of partial observability, each agent conditions its policy πi (ai|τi) on its own local action-observation history τi ∈ T ≡ (Ω × A)∗. Agents together aim to maximize the expected return, that is, to find a joint policy π = ⟨π1, ..., πn⟩ to maximize a joint action-value function Qπ = Es0:∞,a0:∞ [(cid:80)∞\n\nt=0 γtrt|s0 = s, a0 = a, π].\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 CENTRALIZED TRAINING WITH DECENTRALIZED EXECUTION\n\nIn many MARL settings, partial observability problems can be solved by centralized training with decentralized execution (CTDE) paradigm (Lowe et al., 2017; Foerster et al., 2018; Rashid et al., 2020; Wang et al., 2021a; Son et al., 2019), which is currently the mainstream of MARL methods. The centralized mode is adopted in training, and after the training, agents only make decisions based on their local observation and the trained policy network. Thus, the problems of unstable environment and large-scale agents can be overcome at the same time to a certain extent.\n\n2.3 VARIATIONAL AUTOENCODER\n\nVariational autoencoder (VAE) (Kingma & Welling, 2014) is a generative network model based on Variational Bayesian (VB) (Fox & Roberts, 2012) inference. Two probability density distribution models are established by the inference network qφ and the generative network pξ. The inference network is used for the variational inference of the original input data x to generate the variational probability distribution of hidden variables z. The generative network restores the approximate probability distribution of the original data according to the generated implicit variable variational probability distribution. VAE model can be divided into the following two processes: the approximate inference process of posterior distribution of hidden variables: qφ (z|x) and the generation process of conditional distribution of generated variables: pξ (z) pξ (ˆx|z).\n\nIn order to make qφ (z|x) and the true posterior distribution pξ (z|x) approximately equal, VAE uses KL-divergence to measure the similarity between them:\n\nDKL(qφ (z|x) || pξ (z|x)) = log pξ (x) + Eqφ(z|x) [log qφ (z|x) − log pξ (x, z)] ,\n\n(1)\n\nwhere the term log pξ (x) is called log-evidence and it is constant. Another term is the negative evidence lower bound (ELBO). The VAE with additional GRU network is widely used in sequence anomaly detection (Su et al., 2019). In this paper, we use it to identify the identity feature distribution mapped behind the advantage information sequence of agents for a period of time to better guide them in choosing appropriate parameter sharing partners.\n\n2.4 GRAPH ATTENTION NETWORK\n\nGraph attention network (GAT) (Velickovic et al., 2018) is a network architecture based on an attention mechanism, which can learn different weights for different neighbors through the attention mechanism. For calculating the output characteristics of node i, GAT first trains a shared weight matrix W for all nodes to obtain the weight of each neighbor node of node i. Then, according to the weight, the attention coefficients between node i and its neighbor nodes are calculated. Finally, these attention coefficients are weighted and summed to obtain the output features h′\n\ni of node i:\n\n\n\nh′\n\ni = σ1\n\n\n\n(cid:88)\n\nj∈Ni\n\nexp (cid:0)σ2\n\n(cid:0)aT [Whi||hj](cid:1)(cid:1) exp (σ2 (aT [Whi||hk]))\n\n(cid:80)\n\nk∈Ni\n\n\n\nWhj\n\n\n\n(1−Head)\n\n,\n\n(2)\n\nwhere σ1 and σ2 represent nonlinear functions, Ni represents the first-order neighborhood of node i, aT represents the transpose of the weight vector a and || is the concatenation operation.\n\nIn recent years, MARL with communication has widely used GAT to determine the explicit communication goals of agents (Niu et al., 2021; Seraj et al., 2021). In this paper, we use the GAT to establish the topological relationship between different virtual roles, and further combine the agents under different virtual role mappings to form complementary and reliable cooperative relationships.\n\n3 OUR METHOD\n\nIn this section, we introduce the proposed BDPS in detail. BDPS mainly comprises individuals and teams, as shown in Figure 1. We get inspiration from people: a person may play multiple roles and can freely switch roles in different scenes, which promotes the stable development of people, and the same is true for agents. Therefore, we choose to sacrifice some network parameters for agents to maintain multiple roles and groups at the same time. And according to the role selector U and the group selector V , choose the most suitable role and group for the agents.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The overall BDPS architecture.\n\nFirst, we explain the relationship between agents and individual virtual roles and groups.\n\nDefinition 1 Given n (n≥2) agents, k (2≤k≤n) roles and m (2≤m≤k) groups, we have the following mapping relationships: fU : N (cid:55)→ K and fV : K (cid:55)→ M, where K and M represent finite sets of k roles and m groups, respectively.\n\nFrom the fV , we can see the agents’ groups depend on agents’ roles, so we give the time relationship for agents to update the groups and roles: TV = e · TU , where TV represents the update period of the group, TU represents the identity’s update period and e ∈ Z +.\n\n3.1\n\nINDIVIDUAL LEVEL DESIGN\n\nDifferent from the existing methods of determining the roles of agents based on real-time observation information, we consider the impact of the long-term cumulative advantages of agents on the definition of the agents’ roles, because the advantage information can better represent the current state of agents than the observation information. At the same time, unlike the existing methods that only use the encoder to learn the role characteristics of agents for clustering, we learn the long-term advantages of agents through the VAE to obtain the distribution information of role. Because in many cases, the role of an agent will not change just because it makes a unique action. At the individual level in Figure 1, we first maintain the role selector U to select appropriate roles uU for agents, then apply parameter sharing to agents according to the same role.\n\n3.1.1 CHOOSE APPROPRIATE ROLES FOR AGENTS\n\nthe role selector U inputs the pre-calculated agents’ advantage information sequence Firstly, At−TU :t−1 = {At−TU , At−TU +1, ..., At−1} into a GRU network to capture complex temporal dependence between agents’ advantage information in the virtual roles’ update period. Secondly, when the time meets the periodic condition of updating virtual roles, we take the implicit output information ht−1 at this time as the characteristics of the agents and input it into the Encoder Eφ of the VAE. Finally, we obtain the virtual roles uU of agents from the k-dimensional Gaussian distribution output by the Encoder Eφ.\n\nt, where t represents the agent i’s local Q-function at time t, and unlike the classical calculating advantage t ≥ 0 is convenient for us to use the Decoder Dξ to reconstruct the U of agent i needs to update at time t, its virtual role can\n\nFor agent i, its advantage information at time t can be obtained by Ai Qi function A = Q − V , ours Ai input information. When the virtual role ui be obtained from the VAE’s bottleneck:\n\nt\n\nt = max (cid:8)Qi\n\n(cid:9) − Qi\n\nui\n\nU = argmax{zi\n\nU },\n\n(3)\n\nwhere zU = μU + σU · εU , εU ∼ N (0, I).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 TEAM LEVEL DESIGN\n\nThe purpose of introducing grouping is to eliminate the fragmentation of the relationship between agents caused by the single-layer parameter sharing mechanism, and to establish the cooperative relationship between agents of different roles in a wider range. As shown in Figure 1, the team level is mainly composed of the group selector V and grouping cooperative networks. The group selector V is realized through an additional reinforcement learning task. We introduce a GAT for this task to find the correlation between different dynamic roles and promote the role composition in the grouping results to be more comprehensive and complete. In the part of parameter sharing, we add additional implicit information hU t from the individual role level to give full play to the positive impact of the individual identities on the team results.\n\n3.2.1 COMBINE DIFFERENT ROLES TO ACHIEVE GROUPING\n\nThe group selector V takes the virtual roles uU as inputs. This input information is encoded into the feature vector of the agents via a multi-layer perceptron (MLP). Subsequently, these feature vectors (cid:9) to be input into the GAT. We use the self-attention mechanism in the form a set (cid:8)x1 GAT to calculate the attention coefficient eij between agents and use it as an essential basis for grouping:\n\nt , ..., xN\n\nt , x2\n\nt\n\neij = LeakyReLU\n\n(cid:16)\n\naT (cid:104)\n\nWxi\n\nt||Wxj\n\nt\n\n(cid:105)(cid:17)\n\n,\n\n(4)\n\nwhere i, j ∈ N . In our method, considering the similar observation information and close location arrangement between agents, and there is no rigid separation restriction between roles of different agents, we allow the agent’s attention to come from all undead agents (including itself) at the current moment. Of course, to facilitate the comparison of the attention coefficient between agents, we also use the softmax function for normalization.\n\n\n\n\n\n′\n\nxi\n\nt\n\n=\n\nK || k=1\n\nσ\n\n(cid:88)\n\n\n\nj∈Ni\n\nαk\n\nijWkxj\n\nt\n\n ,\n\n(5)\n\nwhere k represents the number of attention heads, which is consistent with the number of virtual roles we set. We hope to find the influence of different agents’ roles as the dominant factors on the formation of teams by agents. For other parts in Equation 5, Ni represents the first-order neighborhood of agent i, and αij = softmaxj (eij) represents the normalized attention coefficient that indicates the importance of agent j’s features to agent i.\n\n(cid:110)\n\n′(cid:111)\n\nA new set of feature vectors x′ t\nthe GRU network to get the hidden state affecting the grouping of agents. When the time t increment is equal to the period for agents to update groups TV , we use the hidden state to output the groups’ value and use the ε−greedy to select groups vV for agents.\n\nis obtained after the GAT, which is input into\n\nt =\n\nx1 t\n\nt\n\n′, x2\n\n′, ..., xN\n\n3.3 OVERALL OBJECTIVES\n\n3.3.1 GET THE LOCAL Q-FUNCTIONS REQUIRED BY AGENTS\n\nIn previous sections, we use the role selector U at the individual level and the group selector V at the team level to map agents from actual individuals to virtual roles uU and groups vV , respectively. The bi-level parameter sharing mechanism is also established for agents using roles and groups information obtained. Although contributions from hidden states at the individual level are used in calculating the local Q-functions of agents at the team level, we still may not completely discard explicit efforts at the individual level of agents. So we give the local Q-function:\n\nQ (τ , a) = QU (τ , a) + QV (τ , a) .\n\n(6)\n\n3.3.2 TRAIN THE MODULES INCLUDED IN OUR METHOD\n\nStarting from the individual level, we need to train the role selector U to select appropriate virtual roles for agents. The training objectives of the Role Selector U include the corrected reconstruction item and the KL-divergence item, and our goal is to minimize these two items:\n\nLU (ξ, φ; AU ) = Lmse\n\n(cid:16)\n\nAU , ˆAU\n\n(cid:17)\n\n+ λDKL [qφ (zlU |hlU ) ||pξ (zlU )] ,\n\n(7)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nwhere Lmse (·) is the mean squared error term for calculate the reconstruction loss of the advantage sequence, lU is the length of the AU , λ is a scaling factor and ˆAU is the agents’ advantage information reconstruction sequence, which is obtained by the Decoder’s output ˆhlU . For the group selector V at the team level, we used the QMIX (Rashid et al., 2018) and RODE (Wang et al., 2021b) methods because we introduced an additional deep reinforcement learning task to generate groups for agents:\n\nLV (θν) =\n\n(cid:88)\n\n(cid:32)TV −1 (cid:88)\n\n\n\n\n\nrt+∆t + γmaxv′\n\nV\n\n ̄QV\n\ntot (τ ′, a′, u′\n\nU , s′) − QV\n\ntot (τ , a, uU , s)\n\n(cid:33)2  ,\n\n(8)\n\nb\n\n∆t=0\n\nwhere b is the batch size of transitions sampled from the replay buffer.\n\nIn order for agents to use global rewards to learn local Q-functions, we input the local Q-value into the mixing network of QMIX (Rashid et al., 2018) again to estimate the global action value Qtot (τ , a):\n\nLT D(θμ) =\n\n(cid:88)\n\n(cid:104)(cid:0)r + γmaxa′ ̄Qtot (τ ′, a′, s′) − Qtot (τ , a, s)(cid:1)2(cid:105)\n\n.\n\n(9)\n\nb\n\n4 EXPERIMENTS\n\nIn this section, we demonstrate and evaluate the advantages of our proposed BDPS using the challenging tasks in the StarCraft II micromanagement enviroments (SMAC) and Google Research Football (GRF). We not only compare our proposed method with QMIX (Rashid et al., 2018) which adopts full parameter sharing mechanism and no parameter sharing mechanism, but also further compare with several baseline methods aimed at promoting the diversity of agents’ policies, such as CDS (Li et al., 2021), EOI (Jiang & Lu, 2021) and RODE(Wang et al., 2021b). Finally, we ablate our method in SMAC, verifying the true utility of the components in our method.\n\n4.1 PERFORMANCE ON GOOGLE RESEARCH FOOTBALL (DEC-POMDP)\n\nIn the official multi-agent example, agents are allowed to observe all information on the field, which is contrary to our research hypothesis. We provide an observation space setting for agents in the Dec-POMDP version. The observation space of agents can dynamically change with their motion vectors. See Appendix A.2 for details.\n\nFirst, we compare our method and baseline methods in the GRF of the Dec-POMDP version we provide. Compared with the experimental scenario provided by CDS (Li et al., 2021), we use the algorithm to control all agents in the left team, use the built-in scoring and checkpoints reward settings of GRF, and set that the agents in the left team cannot be moved by the built-in AI of the system when they do not touch the ball.\n\nselect\n\nshown in Figure 2, we\n\ntwo scenarios academy 3 vs 1 with keeper\n\nAs and academy pass and shoot with keeper for comparison. We compare the actual reward received by the agents with the goal score in both scenarios. In general, our method is able to achieve better results than other baseline algorithms in both scenarios. Among them, the full parameter sharing version of QMIX is better than the non-sharing version, and other improved baseline algorithms based on full parameter sharing QMIX can also achieve goals, but the effect is not obvious.\n\nAs we mentioned in Section 1, the full parameter sharing mechanism ignores the identities and capabilities of agents, which makes them lose the diversity of behavior policies. As a result, full parameter sharing cannot improve the effectiveness of the algorithm when agents observe scenes with large differences in information. For the single-layer dynamic selective parameter sharing, due to the hard cutting of the relationship between agents that do not participate in parameter sharing, although better results can be achieved by relying on the selective parameter sharing mechanism, the agents cannot form a sufficiently stable cooperative relationship, which makes the effect fluctuate significantly. See Appendix C for the specific results.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Performance comparison with baselines on Google Research Football.\n\nFigure 3: Performance comparison with baselines on three super hard maps and three hard maps. (In Appdendix E, we show results on the whole benchmark.)\n\nIn both scenarios, we can see that RODE differentiated the roles of the agents by limiting their action space, but this limitation also prevented the agents from achieving further training results. Through the experimental results, we can find that these baseline algorithms drop the ball in these two academy-scenarios, indicating that the agents do not form a sufficiently complementary collaborative relationship.\n\n4.2 PERFORMANCE ON STARCRAFT II\n\nAs can be seen from Figure 3, our method is superior to other baseline methods on these maps, especially in maps with a larger number of agents, where our method is able to maintain its advantage. Of course, EOI and RODE also show good stability in these maps, and the full parameter sharing version of QMIX is still better than the no parameter sharing version.\n\nSpecifically, our method has more advantages than the CDS, which emphasizes the balance of personalities and commonalities of agents. As we mentioned, exploring the dynamic balance between individual personalities and commonalities is essential. Still, this balance is difficult to define, so we do not give factors in Equation 6 that can adjust the different importance at the individual and team levels. This balance is dynamic and hides in the process of sharing dynamic parameters we design. We will describe it in detail in the ablation experiment in Section 4.3.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n4.3 ABLATION STUDY\n\nIn this section, we conduct ablation studies to understand the actual utility of each level in our bilevel dynamic parameter sharing mechanism. In addition to showing the winning rate of maps in SMAC, we calculate the entropy difference between individuals and teams to quantify the advantages of different components.\n\nFigure 4: Comparison of different parameter sharing mechanisms in 5m vs 6m.\n\nFigure 5: Comparison of different virtual roles in MMM2 (Only individual level).\n\nWe first use the homogeneous agents’ map 5m vs 6m in SMAC to conduct ablation research to analyze the advantages of dynamic parameter sharing over full and no parameter sharing. As shown in Figure 4, we can see that the method of applying parameter sharing has apparent advantages over the method without parameter sharing. Of course, our method is better than QMIX’s full static parameter sharing. Specifically, the single-level dynamic parameter sharing has a weak advantage over the full static parameter sharing. Our bi-level dynamic parameter sharing is more advantageous than the single-level dynamic and full static, which is exactly the goal of our proposed method.\n\nAs shown in Figure 5, we also compare the impact of different numbers of virtual roles on the collaboration of agents in the heterogeneous agents’ map MMM2. From the curve in the figure, we can find that the winning rate in the MMM2 map increases first and then decreases with the number of virtual roles, which indicates that blindly increasing the number of virtual roles can adversely affect the collaboration of agents.\n\nTo quantify the difference between our single-level and bi-level dynamic parameter sharing, we consider analyzing from the perspective of information theory. First, we introduce the evaluation indicator: Entropy difference.\n\nEntropy describes uncertainty. We describe the capabilities of agents in different layers by comparing the entropy of action value between the two layers.\n\n∆H =\n\n1 N\n\nN (cid:88)\n\n(cid:32)\n\n(cid:88)\n\ni=1\n\nai\n\npi\n\nV · log\n\n1 pi V\n\n(cid:88)\n\n−\n\npi\n\nU · log\n\nai\n\n(cid:33)\n\n,\n\n1 pi U\n\n(10)\n\nwhere, pi\n\nV = softmaxQi\n\nV\n\n(cid:0)τ i, ai\n\nt−1, ui\n\nU\n\n(cid:1) and pi\n\nU = softmaxQi\n\nU\n\n(cid:0)τ i, ai\n\nt−1\n\n(cid:1).\n\nFigure 6: Comparison of the entropy difference of two levels in 5m vs 6m and MMM2.\n\nAs shown in Figure 6, we can see that in 5m vs 6m, the information entropy at the team level is always smaller than that at the individual level, and the difference between the two is increasing. The change of entropy shows that the agents at the team level are more orderly than strategies\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nformed by agents only at the individual level, which also confirms that our team-level design can find commonalities among agents with different virtual roles to establish correct collaborations between agents. For MMM2 in Figure 6, our conclusion is still valid. No matter whether k = 3 or k = 5, the entropy difference between the team level and the individual level is constantly expanding with the training, and the entropy of the team level is constantly developing in a direction smaller than that of the individual level. And this phenomenon is consistent with our winning rate in Figure 4 and Figure 5.\n\n5 RELATED WORK\n\n5.1 PARAMETER SHARING\n\nParameter sharing plays an important role in MARL. Tan (1993) first studied the positive role of “sharing” in promoting collaborative agents in classical reinforcement learning algorithms. Gupta et al. (2017) proposed a parameter sharing variant of the single agent DRL algorithms, which introduced the parameter sharing mechanism into homogeneous MARL. Obviously, parameter sharing has been widely used as the implementation details of homogeneous multi-agent algorithms, such as QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019), Qatten (Yang et al., 2020), etc. Recently, Terry et al. (2020) applied parameter sharing extensions to heterogeneous agents algorithm by padding based method, demonstrating again the important use of parameter sharing for multiagent algorithms. Of course, recent works have pointed out that full parameter sharing mechanism tend to make the behavior strategies of agents more same, Christianos et al. (2021) proposed a selective parameter sharing mechanism to eliminate the limitations of full parameter sharing.\n\n5.2\n\nINDIVIDUALS AND TEAMS\n\nFocusing on the expected development of individuals and teams will not only help agents to maintain a variety of policies but also help them form a more stable collaboration. At the individual level, Wang et al. (2020; 2021b) focus on discovering the character traits behind the agents. Jiang & Lu (2021) studies the identifiability of agent trajectories and fixed identities. Du et al. (2019) proposes the use of internal rewards to stimulate diverse behaviors among agents. At the team level, Iqbal et al. (2021) randomly groups agents into related and unrelated groups, allowing agents to explore only specific entities in their environment. Wang et al. (2022) implements a dynamic grouping method by extracting the potential intentions of agents as tags. Li et al. (2021) divides agents’ value functions into shared and unshared parts to focus on the agents’ personality and commonality. In this paper, we focus on the impact of both individual and team levels on the complementary collaboration of agents. Of course, our approach is not to overlap the two layers of individuals and teams but to fully extend and combine the individual identities acquired by the agents.\n\n6 CONCLUSION AND FUTURE WORK\n\nIn this paper, we proposed BDPS, a novel bi-level dynamic parameter sharing mechanism in MARL. By maintaining a role selector and a group selector, BDPS provides a new solution for agents to select the partners for parameter sharing in a timely and dynamic manner at both individual and team levels. And we integrate the roles and groups of agents into a whole to achieve a dynamic balance between their personalities and commonalities. Our experiments on SMAC and GRF show that BDPS can significantly facilitate the formation of complementary and reliable collaboration between agents.\n\nAdditionally, although we defined that agents have the opportunity to be in any role or grouping, they cannot explicitly utilize past knowledge when they were in other roles or groups. Therefore, how to make explicit use of the role knowledge they have learned and how to make use of other similar knowledge are issues that we need to explore further. We will present this paradigm of knowledge flow across roles, groups, and agents in future work.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nStefano V Albrecht and Subramanian Ramamoorthy. A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems. arXiv preprint arXiv:1506.01170, 2015.\n\nStefano V Albrecht and Peter Stone. Reasoning about hypothetical agent behaviours and their pa-\n\nrameters. arXiv preprint arXiv:1906.11064, 2019.\n\nSushrut Bhalla, Sriram Ganapathi Subramanian, and Mark Crowley. Deep multi agent reinforcement In Canadian Conference on Artificial Intelligence, volume\n\nlearning for autonomous driving. 12109, pp. 67–78. Springer, 2020.\n\nLucian Bus ̧oniu, Robert Babuˇska, and Bart De Schutter. Multi-agent reinforcement learning: An\n\noverview. Innovations in Multi-Agent Systems and Applications-1, pp. 183–221, 2010.\n\nFilippos Christianos, Georgios Papoudakis, Muhammad A Rahman, and Stefano V Albrecht. Scaling multi-agent reinforcement learning with selective parameter sharing. In International Conference on Machine Learning, volume 139, pp. 1989–1998. PMLR, 2021.\n\nYali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao. Liir: Learning individual intrinsic reward in multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, volume 32, pp. 4405–4416, 2019.\n\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nCharles W Fox and Stephen J Roberts. A tutorial on variational bayesian inference. Artificial\n\nintelligence review, 38(2):85–95, 2012.\n\nJayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Autonomous Agents and Multiagent Systems, volume 10642, pp. 66–83. Springer, 2017.\n\nShariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin B ̈ohmer, Shimon Whiteson, and Fei Sha. Randomized entity-wise factorization for multi-agent reinforcement learning. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pp. 4596–4606. PMLR, 2021.\n\nJiechuan Jiang and Zongqing Lu. The emergence of individuality.\n\nIn Proceedings of the 38th\n\nInternational Conference on Machine Learning, volume 139, pp. 4992–5001. PMLR, 2021.\n\nYailen Mart ́ınez Jim ́enez. A generic multi-agent reinforcement learning approach for scheduling\n\nproblems. PhD, Vrije Universiteit Brussel, 128, 2012.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Con-\n\nference on Learning Representations, 2014.\n\nKarol Kurach, Anton Raichuk, Piotr Sta ́nczyk, Michal Zajkac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research football: A novel reinforcement learning environment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 4501–4510, 2020.\n\nChenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang. Celebrating diversity in shared multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3991–4002, 2021.\n\nRyan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379–6390, 2017.\n\nYaru Niu, Rohan R Paleja, and Matthew C Gombolay. Multi-agent graph-attention communication and teaming. In International Conference on Autonomous Agents and Multiagent Systems, pp. 964–973, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nFrans A Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.\n\nSpringer, 2016.\n\nGeorgios Papoudakis, Filippos Christianos, Lukas Sch ̈afer, and Stefano V Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, 2021.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.\n\nTabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in neural information processing systems, 33:10199–10210, 2020.\n\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2186–2188, 2019.\n\nEsmaeil Seraj, Zheyuan Wang, Rohan Paleja, Matthew Sklar, Anirudh Patel, and Matthew Gombolay. Heterogeneous graph attention networks for learning diverse communication. arXiv preprint arXiv:2108.09568, 2021.\n\nKyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, volume 97, pp. 5887–5896. PMLR, 2019.\n\nYa Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2828–2837, 2019.\n\nMing Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International\n\nConference on Machine Learning, pp. 330–337. PMLR, 1993.\n\nJustin K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, and Benjamin Black. Revisiting parameter sharing in multi-agent deep reinforcement learning. arXiv preprint arXiv:2005.13625, 2020.\n\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, 2018.\n\nHan Wang, Yang Yu, and Yuan Jiang. A cooperative multi-agent reinforcement learning algorithm based on dynamic self-selection parameters sharing. Chinese Journal of Intelligent Science and Technology, 4(1):75, 2022.\n\nJianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: duplex dueling multi-agent q-learning. In 9th International Conference on Learning Representations, 2021a.\n\nTonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement learning with emergent roles. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pp. 9876–9886. PMLR, 2020.\n\nTonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode: Learning roles to decompose multi-agent tasks. In 9th International Conference on Learning Representations, 2021b.\n\nMingyu Yang, Jian Zhao, Xunhan Hu, Wengang Zhou, and Houqiang Li. Ldsa: Learning dynamic subtask assignment in cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2205.02561, 2022.\n\nYaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA MULTI-AGENT ENVIRONMENTS\n\nIn this paper, we use three multi-agent environments, Starcraft II Multi-agent Challenge (SMAC)(Samvelyan et al., 2019), Google Research Football (GRF) (Kurach et al., 2020) and LevelBased Foraging (LBF) Papoudakis et al. (2021); Albrecht & Ramamoorthy (2015); Albrecht & Stone (2019), to conduct verification experiments.\n\nA.1 SMAC\n\nThe starcraft multi-agent challenge is a fully collaborative, partially observable set of multiagent tasks. This environment implements various micro-management tasks based on the popular realtime strategy game StarCraft II. Each mission is a specific battle scenario in which a group of agents, each of whom controls a single unit, fight against an army controlled by the built-in AI in StarCraft II.\n\nCompared with the LBF, SMAC provides more abundant battle scenes for agents. We conducted comparative experiments with baseline methods on hard and super hard maps. The characteristics of relevant maps are shown in Table 1.\n\nTable 1: Maps used in our experiments.\n\nMap Name bane vs bane MMM2 5m vs 6m 3s vs 5z 2c vs 64zg 27m vs 30m 6h vs 8z corridor 3s5z vs 3s6z\n\nNumber of Controlled Agents 24 10 5\n3 2\n27 6\n6 8\n\nMap Type\n\nHard Hard Hard Hard Hard\n\nSymmetric Heterogeneous Asymmetric Heterogeneous Asymmetric Homogeneous Asymmetric Homogeneous Asymmetric Homogeneous Super Hard Asymmetric Homogeneous Super Hard Asymmetric Homogeneous Super Hard Asymmetric Homogeneous Super Hard Asymmetric Heterogeneous\n\nA.2 GRF\n\nIn Google Research Football (GRF) tasks, agents need to cooperate according to the rules of football matches to win games or related training tasks. The reward settings of GRF can be divided into two types. One is that only goals can be rewarded (scoring), and the other is that when an agent reaches a specific coordinate point (checkpoints), it will get a certain reward. Compared with the two types of reward, the setting of the first type is more sparse. In the official multi-agent example, agents are allowed to observe all information on the field, which is contrary to our research hypothesis. We provide an observation space setting for agents in the Dec-POMDP version. The observation space of agents can dynamically change with their motion vectors, as shown in Figure 7. Considering the size of the field in the environment, we set the visual distance of the agents to 0.84, and the forward visual angle to 200◦. The agents can observe the position and direction of the ball, and within the range of viewing them, we also allow the agents to observe the position of their teammates and the position of their opponents.\n\nIn our experiment, we control all the players on the left side and set their movement state to be lazy (when not touching the ball, there is no built-in AI intervention), and the built-in AI in the system completely controls all the players on the right side.\n\nA.3 LBF\n\nThe LBF is a multi-agent collection task built on a grid world where each agent and object is assigned a level. The criteria for successful collection by agents is the sum of the levels of the related agent needs to be equal to or greater than the level of the item. The agent will receive a reward at the relative item level if the item is successfully collected. LBF provides a more sparse reward environment and collaboration constraints than SMAC, which puts forward higher requirements for the effectiveness of multi-agent algorithms.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: The Observation Space Design of Agents in GRF\n\nThe difficulty of the LBF can be adjusted by the number of agents, the size of the grid world, the type of items, and the hard collaboration constraints. In this paper, we select 15×15-4p-5f as a supplement to the ablation experiment to verify whether the different components of our method are still valid in a more challenging collaborative setting. The 15×15-4p-5f scene indicates that 4 agents in a 15×15 grid world are involved in collecting 5 items.\n\nFigure 8: 15×15-4p-5f in LBF.\n\nB CASE STUDY IN LBF\n\nAs discussed in Appendix A.3, the constraints from level and the sparse reward environment pose a greater challenge for agents to establish complementary and reliable collaborations. In order to verify the effectiveness of our proposed dynamic parameter sharing mechanism for the formation of complementary relationships between agents, we verify the performance of different parameter sharing mechanisms in 15×15-4p-5f, including single-layer dynamic parameter sharing (ours with only individual level), bi-layer dynamic parameter sharing (ours), full static parameter sharing (classic QMIX) and no parameter sharing (QMIX with non-sharing) at all.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Comparison of different parameter sharing mechanisms in 15×15-4p-5f.\n\nAs shown in Figure 9, our method still yields the best return, demonstrating that our approach promotes more collaboration among agents. Although single-layer dynamic parameter sharing can promote agent collaboration, it can not always form complementary collaborative relationships. The result again confirms that forming complementary collaborations among agents requires a dynamic balance of personality and commonality, which is hidden in dynamic parameter sharing at both the individual and team levels.\n\nC ABLATION STUDY IN GRF\n\nAs shown in Figure 8, we additionally verified the performance of the single-layer dynamic parameter sharing mechanism (only individual level) in two GRF scenarios.\n\nFigure 10: Performance comparison with baselines on Google Research Football.\n\nUnlike the experimental results of SMAC and LBF, the algorithm of single-layer dynamic parameter sharing is better than the QMIX algorithm of full static parameter sharing and even can achieve the result of bi-level parameter sharing in academy 3 vs 1 with keeper. The experimental phenomena is because, in the GRF environment, agents have a more extensive range of actions, which makes them have a unique observation space than the other two environments, which proves that the full parameter is more effective because agents have similar observation spaces(Christianos et al., 2021).\n\nWe need to point out that the single-layer dynamic parameter sharing mechanism in the two scenarios is not as stable as our bi-level dynamic parameter sharing mechanism. This is because, on the one hand, agents are unstable due to changes in shared partners; on the other hand, agents that do not share parameters cannot establish complementary cooperation relationships, which again proves that our approach is correct in considering both individual and team levels.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nD THE EFFECT OF THE NUMBER OF VIRTUAL ROLES\n\nDuring the experiment, we found that setting different numbers of virtual roles would affect the performance of the entire algorithm. As shown in Figure 11, we show that in the SMAC environment, we only consider the impact of different numbers of virtual roles on the experimental results.\n\nFigure 11: Compare the effects of different numbers of virtual roles (only individual level).\n\nAs we analyzed in the text, setting too many virtual roles will not improve the performance of the algorithm.\n\nE FULL EXPERIMENTAL RESULTS IN SMAC\n\nFigure 12: Performance comparison with baselines on SMAC.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nF HYPERPARAMETERS SETTING\n\nIn our method, we design a role selector and a group selector to guide agents in selecting appropriate partners for dynamic parameter sharing at the individual and team levels. The core of the role selector is a gated recurrent unit variational autoencoders (GRU-VAE). Besides the GRU unit, the encoder and decoder parts that makeup VAE are composed of simple linear layers and activation functions. The design of the group selector can be divided into a graph attention network with a 128dimensional output state and other corresponding dimensions of the network layers (a linear layer and a GRU unit with a 64-dimensional hidden state). To expand, we use the softmax function to get the roles of agents at the individual level through the bottleneck of the GRU-VAE. For the grouping of agents at the team level, considering that our grouping task is an extra-designed reinforcement learning task, we use the same ε − greedy method as agents’ selection of actions.\n\nWe need to state that, in addition to the newly designed components of our method, we maintain as much agreement as possible with the baseline method QMIX on the structure, parameters, and optimization methods that make up the rest of our method, so as to fully demonstrate in ablation experiments that the real utility of the different components that make up our method comes entirely from our design.\n\nG EXPERIMENTAL DETAILS\n\nFor our method and baseline algorithms used, we tested 2M and 5M steps on all hard maps and all super hard maps of SMAC using five random seeds, respectively. Due to the use of the ε − greedy method, we set ε to be linearly annealed from 1.0 to 0.05 in 70K time steps in all hard maps and three super hard maps and kept constant during the rest of the training period. We set the annealing time for the remaining super hard map 6h vs 8z to 500K.\n\nIn GRF, we respectively tested 10M and 8M steps on academy 3 vs 1 with keeper and academy pass and shoot with keeper using three random seeds. We set ε to be linearly annealed from 1.0 to 0.05 in 100K time steps in both scenarios. For part of the observable space we designed, it contains the location information of the agent itself, the position, direction and attribution information of the football, as well as the player information and opponent information within the visual range.\n\nTo test our method components’ effectiveness in the 15×15-4p-5f scenario of the LBF environment, we set the experimental test step as 20M and the annealing time of ε as 200K. The setting of ε remains the same in our method and baseline algorithms.\n\nFor the section on the virtual roles and groups setup in our method, in order to ensure that the collaborative relationship between agents is sufficiently complementary, we set the number of agents n, the number of virtual roles k, and the number of groups m satisfying the relationship n ≥ k ≥ m ≥ 2. Of course, as we obtained plots of the number of virtual roles and the win rate in our ablation experiments, the number of virtual roles could not be increased blindly. Blindly increasing the number of virtual roles not only brings a vast amount of parameters, but also makes complementary collaborations between agents impossible. In the SMAC and the LBF, the number of virtual roles we set is shown in Table 2. In response to agents’ virtual roles and groups update periods, we have TV = e·TU . To guarantee that the virtual roles and groups are updated normally in each episode, we require that the update periods of the virtual roles and the groups satisfies TU , TV < len(episode). We set update periods for virtual roles and groups based on the average length of a episode. In general, we set the update periods of 5, 7, 8, 10 or 14 with e = 1 or 2.\n\nOur experiments are completed at Ubuntu 18.04 with a GPU of NVIDIA GTX 3090, a CPU of Intel i9-12900k and a memory size of 128G. In addition, the SMAC version is 4.10, the experimental version of 15×15-4p-5f in LBF is v2 and the version of the GRF is 2.10. We will open source my algorithm and experimental environments code on time.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Number of virtual roles and groups in different maps.\n\nEnvironment Name\n\nSMAC\n\nLBF\n\nGRF\n\nMap Name bane vs bane MMM2 5m vs 6m 3s vs 5z 2c vs 64zg 27m vs 30m 6h vs 8z corridor 3s5z vs 3s6z 15×15-40-5f 3 vs 1 with keeper pass and shoot with keeper\n\nNumber of Roles Number of Groups\n\n3 5\n3 2\n2 3\n3 3\n3 2\n2 2\n\n2 2\n2 2\n2 2\n2 2\n2 2\n2 2\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a Bi-level Dynamic Parameter Sharing mechanism (BDPS) in MARL. The core idea is assigning different agents to different roles based on the long-term cumulative advantages and grouping multiple roles to a set of teams via the Graph Attention Network. And the authors verify the proposed method in some typical MARL benchmarks.\n\n# Strength And Weaknesses\n\n**Weaknesses**\n  * The motivation is not clear enough. From the paper, I do not see why using the advantage function to represent the roles of agents is better and why incorporating a two-level selective parameter sharing is important.\n  * The writing of the paper could be significantly improved. Lots of sentences are too long and many sentences are unclear, e.g., \"we add additional implicit information $h_U^t$ from the individual role level to give full play to the positive impact of the individual identities on the team results.\"\n  * Two important single-layer selective parameter sharing baselines ([1] and [2]) are mentioned in the introduction but are missing in the experiments.\n  * The experimental evaluations in SMAC is not convincing. Recently, [3] and [4] have verified that the optimized QMIX (completely sharing the parameters among all agents) could achieve 100% win rates on all Easy, Hard and Super Hard scenarios of SMAC. Therefore, SMAC may not be a good testbed to validate the benefits of the dynamic parameter sharing mechanism. The authors could verify the algorithm in more complex environments.\n  * minor:\n    * Typos:\n       * \"And according to the role selector and the U group selector, choose the most suitable role and group for the agents V.\"\n       * In equation (6), the local Q-function improperly takes the joint action **a** as input.\n\n**Reference**\n  * [1] Scaling multi-agent reinforcement learning with selective parameter sharing.\n  * [2] A cooperative multi-agent reinforcement learning algorithm based on dynamic self-selection parameters sharing.\n  * [3] Rethinking the implementation tricks and monotonicity constraint in cooperative multi-agent reinforcement learning\n  * [4] API: Boosting Multi-Agent Reinforcement Learning via Agent-Permutation-Invariant Networks\n\n# Clarity, Quality, Novelty And Reproducibility\n\nBoth the writing and the quality of the paper should be further improved.\n\nThe code is not attached in the Appendix.\n\n# Summary Of The Review\n\nThis paper study an important and an interesting problem in MARL. But the quality of the paper should be significantly improved. So I recommend a reject based on the current version of the paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPROPER SCORING RULES FOR SURVIVAL ANALYSIS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSurvival analysis is the problem of estimating probability distributions for future events, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty In this paper, quantification, little is known about those for survival analysis. we investigate extensions of four major strictly proper scoring rules for survival analysis. Through the extensions, we discuss and clarify the assumptions arising from the discretization of the estimation of probability distributions. We also discuss the relationship between the existing algorithms and extended scoring rules, and we propose new algorithms based on our extensions of the scoring rules for survival analysis.\n\n1\n\nINTRODUCTION\n\nThe theory of scoring rules is a fundamental theory in statistical analysis, and it is widely used in uncertainty quantification (see, e.g., Mura et al. (2008); Parmigiani & Inoue (2009); Benedetti (2010); Schlag et al. (2015)). Suppose that there is a random variable Y whose cumulative distribution function (CDF) is FY . Given an estimation ˆFY of FY and a single sample y obtained from Y , a scoring rule S( ˆFY , y) is a function that returns an evaluation score for ˆFY based on y. Since ˆFY is a CDF and y is a single sample of Y , it is not straightforward to choose an appropriate scoring rule S( ˆFY , y). The theory of scoring rules suggests strictly proper scoring rules that can be used to recover the true probability distribution FY by optimizing the scoring rules. This theory shows that there are infinitely many strictly proper scoring rules, and examples of them include the pinball loss, the logarithmic score, the Brier score, and the ranked probability score (see, e.g., Gneiting & Raftery (2007) for the definitions of these scoring rules).\n\nSurvival analysis, which is also known as time-to-event analysis, can be seen a problem in uncertainty quantification. Despite the long history of research on survival analysis (see, e.g., Wang et al. (2019) for a comprehensive survey), little is known about the strictly proper scoring rules for survival analysis. Therefore, this paper investigates extensions of these scoring rules for survival analysis.\n\nSurvival analysis is the problem of estimating probability distributions for future events. In healthcare applications, an event usually corresponds to an undesirable event for a patient (e.g., a death or the onset of disease). The time between a well-defined starting point and the occurrence of an event is called the survival time or event time. Survival analysis has important applications in many fields such as credit scoring (Dirick et al., 2017) and fraud detection (Zheng et al., 2019) as well as healthcare. Although we discuss survival analysis in the context of healthcare applications, we can use the extended scoring rules for any other applications.\n\nDatasets for survival analysis are censored, which means that events of interest might not be observed for a number of data points. This may be due to either the limited observation time window or missing traces caused by other irrelevant events. In this paper, we consider only right censored data, which is a widely studied problem setting in survival analysis. The exact event time of a right censored data point is unknown; we know only that the event had not happened up to a certain time for the data point. The time between a well-defined starting point and the last observation time of a right censored data point is called the censoring time.\n\nOne of the classical methods for survival analysis is the Kaplan-Meier estimator (Kaplan & Meier, 1958). It is a non-parametric method for estimating the probability distribution of survival times as a survival function κ(t), where the value κ(t) represents the survival rate at time t (i.e., the ratio of\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe patients who survived at time t). By definition, κ(0) = 1 and κ(t) is a monotonically decreasing function.\n\nSince there are many applications that require an estimate of the survival function for each patient rather than the overall survival function κ(t) for all patients, many algorithms have been proposed. In particular, many neural network models have been proposed (e.g., (Lee et al., 2018; Avati et al., 2019; Ren et al., 2019; Kamran & Wiens, 2021; Tjandra et al., 2021)).\n\nA problem with these neural network models is that most of them are not based on the theory of scoring rules except for (Rindt et al., 2022). Since we cannot directly use a known scoring rule due to censoring in survival analysis, the state-of-the-art neural network models for survival analysis use their own custom loss functions instead. Even though these custom loss functions can be seen as variants of known scoring rules, they are not proven to be strictly proper for survival analysis in terms of the theory of scoring rules.\n\nWe review variants of scoring rules used in survival analysis with respect to the four major strictly proper scoring rules.\n\n• Pinball loss. Portnoy’s estimator (Portnoy, 2003), which is a variant of the pinball loss, has been used in quantile regression-based survival analysis (Portnoy, 2003; Neocleous et al., 2006; Pearce et al., 2022). However, it is unknown if Portnoy’s estimator is proper or not.\n\n• Logarithmic score. Rindt et al. (2022) proved that a variant of the logarithmic score is strictly proper for survival analysis. This variant has been used in the loss function of many neural network models (e.g., (Lee et al., 2018; Avati et al., 2019; Ren et al., 2019; Kamran & Wiens, 2021; Kvamme & Borgan, 2021; Tjandra et al., 2021)). However, most of them use this variant in part of the loss functions, and these loss functions are used without the proof of properness.\n\n• Brier score. The IPCW Brier score (Graf et al., 1999) and integrated Brier score (Graf et al., 1999) are widely used in survival analysis (e.g., (Kvamme et al., 2019; Haider et al., 2020; Han et al., 2021; Zhong et al., 2021)) as variants of the Brier score. However, Rindt et al. (2022) show that neither of them are not proper in terms of the theory of scoring rules.\n\n• Ranked probability score. Variants of the ranked probability score have been proposed in (Avati et al., 2019; Kamran & Wiens, 2021), but (Rindt et al., 2022) show that they are not proper in terms of the theory of scoring rules.\n\nOur contributions. We analyze survival analysis through the lens of the theory of scoring rules. First, we prove that Portnoy’s estimator, which is an extension of the pinball loss, is proper under certain conditions. This result underpins the grid-search algorithm (Portnoy, 2003; Neocleous et al., 2006) and the CQRNN algorithm (Pearce et al., 2022), which is based on the expectation maximization (EM) algorithm. Second, we show another proof for an extension of the logarithmic score. This scoring rule has already been proven to be strictly proper in (Rindt et al., 2022), but our proof clarifies the implicit assumption in the proof. Third, we show that there are two other proper scoring rules for survival analysis under certain conditions by extending the Brier score and the ranked probability score. By using these extended scoring rules, we construct two new algorithms by using the EM algorithm.\n\n2 RELATED WORK\n\nSurvival analysis has been traditionally studied under the proportional hazard assumption. Its seminal work is the Cox model (Cox, 1972), and many other prediction models have been proposed under this strong assumption. See, e.g., Wang et al. (2019) for a comprehensive survey of the prediction models based on this assumption. Since we do not require the theory of scoring rules under this assumption, we consider survival analysis without this assumption. Note that most of the stateof-the-art neural network models for survival analysis do not use this assumption.\n\nRegarding evaluation metrics for survival analysis, the concordance index (C-index) (Harrell et al., 1982) has been widely used under the proportional hazard assumption. Some variants of the Cindex (Antolini et al., 2005; Uno et al., 2011) are proposed for survival analysis without the proportional hazard assumption. However, they are proven to not be proper in terms of the theory of\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n1 τ4 τ3 τ2 τ1\n\n0\n\nˆF (t)\n\nTime t\n\nzmax\n\n(a) Quantile regression\n\n1\n\nˆF (t)\n\n0 ζ1 ζ2 ζ3 ζ4 ζ5\n\nTime t\n\n(b) Distribution regression\n\nFigure 1: Two types of discretization of probability distribution ˆF (t) with B = 5\n\nscoring rules (Blanche et al., 2018; Rindt et al., 2022). Therefore, we do not use these variants of the C-index in this paper. We also note that Sonabend et al. (2022) discuss the problems of using these variants of the C-index in survival analysis.\n\n3 PRELIMINARIES\n\nWe define notation here before showing the extensions of the scoring rules for survival analysis. Unless otherwise stated, we consider a single patient x, and let T and C be random variables for the event time and censoring time of this patient, respectively. Let t ∼ T and c ∼ C be samples obtained from T and C, respectively. We assume that t and c are positive real values (i.e., t ∈ R+ and c ∈ R+). In survival analysis, we can observe only the minimum z = min{t, c}, and we use δ = 1(t ≤ c) to indicate whether z represents the true event time (i.e., δ = 1 means z is uncensored, and z = t) or z represents the censoring time (i.e., δ = 0 means z is censored, and z = c). In this paper, a pair of samples (t, c) is often represented as a pair of values (z, δ) to emphasize that we can observe only one of t and c. We assume that there exists zmax > 0 such that 0 < z ≤ zmax, which means that we have prior knowledge that z is at most zmax. Let F (t) be the CDF of T , which is defined as F (t) = Pr(T ≤ t). By the definition of F (t), we have F (0) = 0, and we can represent the probability that the true event time is between t1 and t2 by Pr(t1 < T ≤ t2) = F (t2) − F (t1).\n\nSurvival analysis is the problem of estimating the ˆF (t) of the true CDF F (t). For simplicity, we assume that both F (t) and ˆF (t) are monotonically increasing continuous functions. This means that F (t1) < F (t2) holds if and only if 0 ≤ t1 < t2 < ∞. This assumption enables us to calculate F (t) for any time 0 ≤ t < ∞ and to calculate F −1(τ ) for any quantile level 0 ≤ τ ≤ 1. When we estimate ˆF (t) by using a neural network, we usually discretize p = ˆF (t) along with the p-axis or the t-axis as shown in Fig. 1. In quantile regression-based survival analysis, p = ˆF (t) is discretized along the p-axis, ˆF −1(τi) is estimated for 0 = τ0 < τ1 < · · · < τB−1 < τB = 1, and we assume that ˆF −1(τ0) = 0 and ˆF −1(τB) = zmax. In distribution regression-based survival analysis, p = ˆF (t) is discretized along the t-axis, ˆF (ζi) is estimated for 0 = ζ0 < ζ1 < · · · < ζB−1 < ζB = zmax, and we assume that ˆF (ζ0) = 0 and ˆF (ζB) = 1.\n\nThroughout this paper we assume that the censoring time and the event time are independent of each other given a feature vector of patient x. This assumption is widely used in survival analysis, and this assumption is represented as\n\nAssumption 3.1. T ⊥⊥ C|X.\n\nNote that he Kaplan-Meier estimator (Kaplan & Meier, 1958), which is a classical non-parametric method for survival analysis, uses this assumption. D-calibration (Haider et al., 2020), which is one of the widely used metrics in survival analysis, also uses this assumption. We can find examples of the other stronger assumptions (e.g., unconditionally random right censoring) used in survival analysis in (Peng, 2021).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n4 PROPER SCORING RULES FOR SURVIVAL ANALYSIS\n\nWe briefly review the theory of scoring rules for uncertainty quantification. Let Y be a random variable, and let FY (y) be its CDF, which is defined as FY (y) = Pr(Y ≤ y). A scoring rule is a function S( ˆFY , y) that returns a real value for inputs ˆFY and y, where ˆFY is an estimation of FY and y is a sample obtained from Y . In this paper, we consider negatively-oriented scoring rules. Therefore, the inequality S( ˆF1, y) < S( ˆF2, y) means that ˆF1 is a better estimation than ˆF2. We can interpret the scoring rule S( ˆFY , y) as a penalty function for the misestimation of ˆFY for a sample y.\n\nThe proper and strictly proper scoring rules are defined by using the expected score of a scoring rule, which can be written as\n\n ̃S( ˆFY ; Y ) = Ey∼Y [S( ˆFY , y)].\n\nDefinition 4.1. A scoring rule S( ˆFY , y) is proper if ̃S( ˆFY ; Y ) ≥ ̃S(FY ; Y ) holds. Definition 4.2. A scoring rule S( ˆFY , y) is strictly proper if ̃S( ˆFY ; Y ) ≥ ̃S(FY ; Y ) holds and the equality holds only when ˆFY = FY .\n\nThese definitions ask a proper scoring rule to satisfy that the score S( ˆFY , y) for estimation ˆFY is always at least S(FY , y) for true CDF FY , and the score is minimized if ˆFY = FY . This is a natural property that any scoring rule should satisfy, and this means that we can recover the true FY if we can minimize the score of a strictly proper scoring rule. The theory of scoring rules shows that there are infinitely many strictly proper scoring rules (see, e.g., Gneiting & Raftery (2007)).\n\nNow we extend these definitions of the proper and strictly proper scoring rules for survival analysis. In survival analysis, the inputs of a scoring rule S( ˆF , (z, δ)) are changed from FY and y to F and (z, δ). The proper and strictly proper scoring rules are defined by using\n\n ̃S( ˆF ; T, C) = E(t,c)∼(T,C)[S( ˆF , (z, δ))].\n\nDefinition 4.3. A scoring rule S( ˆF , (z, δ)) is proper if ̃S( ˆF ; T, C) ≥ ̃S(F ; T, C) holds. Definition 4.4. A scoring rule S( ˆF , (z, δ)) is strictly proper if ̃S( ˆF ; T, C) ≥ ̃S(F ; T, C) holds and the equality holds only when ˆF = F .\n\nNow we investigate the extensions of the scoring rules for survival analysis. In Sec. 4.1, we consider quantile regression and survival analysis based on quantile regression. In Secs. 4.2–4.4, we consider distribution regression and survival analysis based on distribution regression.\n\n4.1 EXTENSION OF PINBALL LOSS\n\nWe first review quantile regression (Koenker & Bassett, 1978; Koenker & Hallock, 2001). Let Y be a real-valued random variable and FY be its CDF. In quantile regression, we estimate the τ -th quantile of Y , which can be written as\n\nF −1\n\nY (τ ) = inf{y | FY (y) ≥ τ }.\n\nThe pinball loss (Koenker & Bassett, 1978), which is also known as the check function, is a widely used scoring rule. The pinball loss for an estimation ˆFY of FY and a quantile level τ is defined as (cid:40)\n\nSPinball( ˆFY , y; τ ) = ρτ ( ˆF −1\n\nY (τ ), y) =\n\n(1 − τ )( ˆF −1 τ (y − ˆF −1\n\nY (τ ))\n\nY (τ ) − y)\n\nif ˆF −1 if ˆF −1\n\nY (τ ) ≥ y, Y (τ ) < y.\n\n(1)\n\nNote that the pinball loss with τ = 0.5 is equivalent to the mean absolute error (MAE) and it can be used to estimate the median (i.e., 0.5-th quantile) of Y . This means that the pinball loss is a generalization of MAE for any quantile level τ ∈ [0, 1]. Note also that we include the quantile level τ in the notation SPinball( ˆF −1\n\nY , y; τ ) to clarify that this scoring rule receives τ as an input.\n\nIt is known that the pinball loss is strictly proper (see e.g., (Gneiting & Raftery, 2007)), which means that we have\n\nEy∼Y [SPinball( ˆFY , y; τ )] ≥ Ey∼Y [SPinball(FY , y; τ )],\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nand the equality holds only when ˆF −1 sion can be formulated as the problem of computing\n\nY (τ ) = F −1\n\nY (τ ) by Definition 4.2. Therefore, quantile regres-\n\nEy∼Y [SPinball( ˆFY , y; τ )].\n\narg min ˆFY\n\nAs an extension of the pinball loss for quantile regression-based survival analysis, Portnoy’s estimator is proposed in Portnoy (2003), which is defined as\n\nSPortnoy( ˆF , (z, δ); w, τ ) =\n\n(cid:40)\n\nρτ ( ˆF −1(τ ), z) wρτ ( ˆF −1(τ ), z) + (1 − w)ρτ ( ˆF −1(τ ), z∞)\n\nif δ = 1, if δ = 0,\n\n(2)\n\nwhere ρτ is the pinball loss defined in Eq. (1), w is a weight parameter to control the balance between two pinball loss terms, and z∞ is any constant such that z∞ > zmax. In Portnoy’s estimator, we can set an arbitrary constant 0 ≤ w ≤ 1 for the parameter w if τc > τ , where τc = Pr(t ≤ c) = F (c), but we have to set w = Pr(F (c) < F (t) ≤ τ |t > c) = (τ − τc)/(1 − τc) otherwise (i.e., τc ≤ τ ). Since we do not know the true value τc = F (c), we have to resolve this problem to use this estimator.\n\nBefore showing how to resolve this problem, we prove that this estimator is proper under the condition that w is correct. Note that this is the first result for the quantile regression-based survival analysis in terms of the theory of scoring rules. Theorem 4.5. Portnoy’s estimator is proper under the condition that w is correct.\n\nProof. The proof is given in Appendix A.1.\n\nThis theorem means that the crucial part of Portnoy’s estimator is to set an appropriate value for w, and this theorem ensures that we can recover the true probability distribution F −1 by minimizing Eq. (2) if w is correct.\n\nNow we discuss how to set parameter w in Portnoy’s estimator. First, we emphasize that we cannot avoid the dependence on F (c) in the definition of any of the scoring rules for survival analysis due to the discretization of ˆF . Even if we know the true value F −1(τi) for all {τi}B i=0, we cannot compute F (c) because c is not always contained in {F −1(τi)}B i=0. The best we can do is to find quantile levels τi and τi+1 such that F −1(τi) < c ≤ F −1(τi+1) by using the assumption that F is a monotonically increasing function. Note that, even if we could find such τi and τi+1, we would not be able to calculate some important probabilities such as Pr(c < t ≤ F −1(τi+1)) = τi+1 − F (c). Therefore, we usually mitigate this problem by using a large B, which enables us to assume, for example, F (τi+1) − F (τi) ≈ 0 for all i.\n\nc such that c ≈ F −1(τ (cid:48)\n\nEven if we use a large B to assume that we can find the quantile level τ (cid:48) c) for any c, the problem that we do not know the true F −1 remains. One of the approaches to tackling this problem is the grid search algorithm (Portnoy, 2003; Neocleous et al., 2006). In this algorithm, we use a sufficiently large B, and we estimate ˆF −1(τi) of F −1(τi) in the increasing order of i = i=0 and we are going to estimate ˆF −1(τj). 0, 1, . . . , B. Suppose that we have estimated { ˆF −1(τi)}j−1 c ∈ {τi}j−1 The key idea of this algorithm is that we can find τ (cid:48) c) if τc = c, we estimate w by using τ (cid:48) F (c) < τj. If we cannot find such τ (cid:48) c, this algorithm assumes that τc > τj and we use an arbitrary constant 0 ≤ w ≤ 1. Portnoy (2003) discuss that this algorithm is analogous to the Kaplan-Meier estimator, and their theoretical analysis (Portnoy, 2003; Neocleous et al., 2006) proves that Portnoy’s estimator combined with linear regression can recover the true probability distribution F .\n\ni=0 such that c ≈ ˆF −1(τ (cid:48)\n\nIf we can find such τ (cid:48)\n\nc ≈ τc.\n\nAs for another approach, Pearce et al. (2022) propose the CQRNN algorithm, which combines a neural network and the EM algorithm. Unlike the grid search algorithm, this algorithm estimates { ˆF −1(τi)}B i=0 simultaneously by using a neural network. This algorithm starts with an arbitrary initial estimation ˆF , and the parameter w is estimated by using ˆF . Then, this algorithm updates ˆF by using the estimation ˆw of w, and it repeats this alternative estimation of ˆF and ˆw until these values converge. This EM algorithm can be implemented for “free” according to (Pearce et al., 2022), which means that we can implement it easily in the computation of the loss function of a neural network training algorithm and we do not need to construct two separate neural network models for estimating ˆF and ˆw. The experimental evaluation in (Pearce et al., 2022) shows that the CQRNN algorithm performs the best among the quantile regression-based survival analysis models.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 EXTENSION OF LOGARITHMIC SCORE\n\nWhile we estimate { ˆF −1(τi)}B which we estimate { ˆF (ζi)}B known as one of the strictly proper scoring rules, and it is defined as\n\ni=0 in quantile regression, we consider distribution regression, in i=0. For distribution regression, the logarithmic score (Good, 1952) is\n\nSlog( ˆF , y; {ζi}B\n\ni=0) = −\n\n= −\n\nB−1 (cid:88)\n\ni=0\n\nB−1 (cid:88)\n\ni=0\n\n1(ζi < y ≤ ζi+1) log( ˆF (ζi+1) − ˆF (ζi))\n\n1(ζi < y ≤ ζi+1) log ˆfi,\n\n(3)\n\nwhere ˆfi = ˆF (ζi+1) − ˆF (ζi) for i = 0, 1, . . . , B − 1.\n\nWe extend this logarithmic score for distribution regression-based survival analysis as\n\nSCen−log( ˆF , (z, δ); w, {ζi}B\n\ni=0)\n\n= −\n\nB−1 (cid:88)\n\ni=0\n\n1(ζi < z ≤ ζi+1)\n\n(cid:16)\n\nδ log ˆfi + (1 − δ)(w log ˆfi + (1 − w) log(1 − ˆF (ζi+1)))\n\n(cid:17)\n\n,(4)\n\nwhere w = Pr(c < t ≤ ζi+1|t > c) = (F (ζi+1) − F (c))/(1 − F (c)). If δ = 1, this scoring rule is equal to Eq. (3). Similar to Portnoy’s estimator, we cannot set the parameter w of this scoring rule because we do not know F (ζi+1) and F (c).\n\nEven though we do not know the correct w, we prove that this scoring rule is strictly proper if the parameter w is correct. Theorem 4.6. The scoring rule SCen−Log( ˆF , (z, δ); w, {ζi}B\n\ni=0) is strictly proper if w is correct.\n\nProof. The proof is given in Appendix A.2.\n\nSimilar to Portnoy’s estimator, we can use both the grid-search algorithm and an EM algorithm similar to the CQRNN algorithm to estimate w. In addition, we show another simpler approach by using the observation that w ≈ 0 if B is large. If B is large, 1 − F (c) is usually much larger than F (ζi+1) − F (c) (see Fig. 2(a)), and hence we have w = (F (ζi+1) − F (c))/(1 − F (c)) ≈ 0. Therefore, we obtain a simpler variant of SCen−log by setting w = 0:\n\nSCen−simple−log( ˆF , (z, δ); {ζi}B\n\ni=0)\n\n= −\n\nB−1 (cid:88)\n\ni=0\n\n1(ζi < z ≤ ζi+1)\n\n(cid:16)\n\nδ log ˆfi + (1 − δ) log(1 − ˆF (ζi+1))\n\n(cid:17)\n\n.\n\n(5)\n\nFurthermore, by increasing B to infinity (i.e., B → ∞), we obtain the continuous version of this scoring rule:\n\nSCen−cont−log( ˆF , (z, δ)) = −δ log\n\nd ˆF dt\n\n(z) − (1 − δ) log(1 − ˆF (z)),\n\nwhich is equal to the extension of the logarithmic score that is proven to be strictly proper in (Rindt et al., 2022). This clarifies that the proof in (Rindt et al., 2022) implicitly assumes that B is sufficiently large.\n\n4.3 EXTENSION OF BRIER SCORE\n\nIn distribution regression, the Brier score (Brier, 1950) is also known as a strictly proper scoring rule, which is defined as\n\nSBrier( ˆF , y; {ζi}B\n\ni=0) =\n\nB−1 (cid:88)\n\n(1(ζi < y ≤ ζi+1) − fi)2,\n\n(6)\n\ni=0\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n1\n\n1 − F (c)\n\nF (ζi+1) − F (c)\n\n1\n\n1 − F (c)\n\nF (ζ) − F (c)\n\n0\n\nc ζi+1\n\nTime t\n\n0\n\nc\n\nζ\n\nTime t\n\n(a) SCen−log\n\n(b) SCen−Binary−Brier\n\nFigure 2: Illustrations of computations of w\n\nwhere ˆfi = ˆF (ζi+1) − ˆF (ζi) for i = 0, 1, . . . , B − 1.\n\nWe extend this Brier score for distribution regression-based survival analysis as\n\nSCen−Brier( ˆF , (z, δ); {wi}B−1\n\ni=0 , {ζi}B\n\ni=0) =\n\nB−1 (cid:88)\n\n(cid:16)\n\ni=0\n\nwi(1 − ˆfi)2 + (1 − wi) ˆf 2\n\ni\n\n(cid:17)\n\n,\n\n(7)\n\nwhere\n\n\n\n0 1\n0 (F (ζi+1) − F (c))/(1 − F (c)) fj/(1 − F (c)) If δ = 1, it is easy to see that Eq. (7) is equal to Eq. (6).\n\n \n\nwi =\n\nif δ = 1 and ζi+1 < z = t if δ = 1 and ζi < z = t ≤ ζi+1 if z ≤ ζi if δ = 0 and ζi < z = c ≤ ζi+1 if δ = 0 and ζi+1 < z = c.\n\nWe prove that this scoring rule is strictly proper if the set of parameters {wi}B−1 Theorem 4.7. The scoring rule SCen−Brier( ˆF , (z, δ); {wi}B−1 correct for all i.\n\ni=0 , {ζi}B\n\ni=0 is correct. i=0) is strictly proper if wi is\n\nProof. The proof is given in Appendix A.3.\n\nWe can use the EM algorithm similar to the CQRNN algorithm to estimate w. However, unlike Portnoy’s estimator and the extension of the logarithmic score, we cannot use the grid-search algorithm in this extension of the Brier score because wi depends on fj such that i < j.\n\nNote that each wi in this scoring rule is close to zero if B is large and δ = 0. However, since wis are designed to satisfy (cid:80)\n\ni wi = 1, we cannot use the approximation wi ≈ 0 for this scoring rule.\n\n4.4 EXTENSION OF RANKED PROBABILITY SCORE\n\nThe ranked probability score (RPS) is also known as a strictly proper scoring rule (see e.g., (Gneiting & Raftery, 2007)). It is defined as\n\nSRPS( ˆF , y) =\n\nB (cid:88)\n\ni=1\n\nSBinary−Brier( ˆF , y; ζi),\n\nwhere SBinary−Brier is the binary version of SBrier (Eq. (6)) with single threshold ζ:\n\nSBinary−Brier( ˆF , y; ζ) = (1(y ≤ ζ) − 1)2.\n\nWe extend this scoring rule for survival analysis:\n\n(8)\n\n(9)\n\nSCen−RPS( ˆF , (z, δ); {wi}B−1\n\ni=1 , {ζi}B−1\n\ni=1 ) =\n\nSCen−Binary−Brier( ˆF , (z, δ); wi, ζi),\n\n(10)\n\nB−1 (cid:88)\n\ni=1\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nwhere SCen−Binary−Brier is the binary version of SCen−Brier (Eq. (7)) with single threshold ζ:\n\nSCen−Binary−Brier( ˆF , (z, δ); w, ζ) =\n\n \n\n\n\nˆF (ζ)2 (1 − ˆF (ζ))2 w(1 − ˆF (ζ))2 + (1 − w) ˆF (ζ)2\n\nif z > ζ if δ = 1 and z = t ≤ ζ if δ = 0 and z = c ≤ ζ,\n\nwhere w = (F (ζ) − F (c))/(1 − F (c)).\n\nSince this scoring rule is just the sum of the binary version of Brier scores, it is straightforward to prove this theorem. Theorem 4.8. The scoring rule SCen−RPS( ˆF , (z, δ); {wi}B−1 correct for all i.\n\ni=1 ) is strictly proper if wi is\n\ni=1 , {ζi}B−1\n\nNote that the scoring rule SCen−Binary−Brier is analogous to Portnoy’s estimator. The scoring rule SCen−Binary−Brier is designed to estimate ˆF (ζ), where ζ is an input, and we use F (c) and ζ to set w, whereas Portnoy’s estimator is designed to estimate ˆF −1(τ ), where τ is an input, and we use F (c) and τ to set w. As these two scoring rules are similar, we can use both the grid-search algorithm and an EM algorithm similar to the CQRNN algorithm for SCen−RPS.\n\nUnlike SCen−log defined in Eq. (4), the parameter w of the scoring rule SCen−Binary−Brier is usually not close to zero, because ζ and c are usually not close to each other as shown in Fig. 2(b). We note that the parameter w of Portnoy’s estimator is also not close to zero for a similar reason.\n\n5 EVALUATION METRICS FOR SURVIVAL ANALYSIS\n\nWhile we have discussed the extensions of the scoring rules as loss functions, we can use strictly proper scoring rules also for evaluation metrics. If we use a nonproper scoring rule Snonproper as an evaluation metric, a neural network model can find ˆF such that\n\nE(t,c)∼(T,C)[Snonproper( ˆF , (z, δ))] < E(t,c)∼(T,C)[Snonproper(F, (z, δ))] by using Snon−proper for the loss function. This suggests that we should avoid nonproper scoring rules as evaluation metrics, because we may obtain an over-optimized estimation ˆF , which has a lower score than F in terms of the evaluation metric Snonproper.\n\nAmong the extensions of the scoring rules for survival analysis, we can use only SCen−simple−log (Eq. (5)) as an evaluation metric for survival analysis, because the other scoring rules depend on the parameter w or {wi}B−1 i=1 . Note that this scoring rule SCen−simple−log is valid only if B is sufficiently large. In Appendix B, we conducted experiments on choosing an appropriate B, and the results suggested using B > 16.\n\nRegarding calibration metrics for survival analysis, while D-calibration (Haider et al., 2020) is widely used, we propose another metric for calibration, KM-calibration. We define this metric as\n\ndKM−cal(κ, ˆFavg; {ζi}B\n\ni=0) = dKL(κ||1 − ˆFavg; {ζi}B\n\ni=0) =\n\nB−1 (cid:88)\n\n(pi log pi − pi log qi),\n\ni=0\n\nwhere κ is the survival function estimated by using the Kaplan-Meier estimator (Kaplan & Meier, 1958), ˆFavg is the average of the estimated CDFs of all patients, pi = κ(ζi+1) − κ(ζi), and qi = (1 − ˆFavg(ζi+1)) − (1 − ˆFavg(ζi)). (In this computation, we assume that κ(ζB) = 0.) This metric is the Kullback-Leibler divergence between κ(t) and the average of the estimated survival function 1 − ˆFavg(t). This metric is based on the observation that the model’s predicted number of events within any time interval should be similar to the observed number (Goldstein et al., 2020).\n\nWe note here that calibration is particularly important for survival analysis especially in healthcare applications. If we use a prediction model that is miscalibrated, the predictions obtained from the miscalibrated model would be pessimistic or optimistic as a whole compared with the actual ones. If medical doctors were to make decisions on patient treatments on the basis of such a miscalibrated prediction model, the treatments could be harmful for patients because of the pessimistic or optimistic predictions. Calster et al. (2019) extensively discuss the importance of calibration in survival analysis.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Prediction performances (lower is better) of extended scoring rules with B = 32\n\nMetric SCen−log−simple\n\nD-calibration\n\nKM-calibration\n\nLoss Function SCen−log SCen−Brier SCen−RPS SPortnoy SCen−log SCen−Brier SCen−RPS SPortnoy SCen−log SCen−Brier SCen−RPS SPortnoy\n\nflchain 1.5054 ± 0.0508 1.5137 ± 0.0557 1.6737 ± 0.0821 1.6641 ± 0.0518 0.0003 ± 0.0001 0.0004 ± 0.0002 0.0005 ± 0.0003 0.0071 ± 0.0031 0.0206 ± 0.0049 0.0268 ± 0.0071 0.1553 ± 0.0349 0.0434 ± 0.0067\n\nprostateSurvival 1.3608 ± 0.0295 1.3680 ± 0.0291 1.4821 ± 0.0639 1.4352 ± 0.0420 0.0001 ± 0.0000 0.0001 ± 0.0000 0.0010 ± 0.0005 0.0055 ± 0.0041 0.0312 ± 0.0084 0.0324 ± 0.0090 0.5931 ± 0.3846 0.1895 ± 0.1413\n\nsupport 1.8307 ± 0.0452 1.8467 ± 0.0448 2.1036 ± 0.1012 2.0645 ± 0.0455 0.0063 ± 0.0009 0.0071 ± 0.0009 0.0045 ± 0.0011 0.0237 ± 0.0037 0.0299 ± 0.0115 0.0492 ± 0.0125 0.2668 ± 0.1192 0.0809 ± 0.0381\n\n6 EXPERIMENTS\n\nIn our experiments, we used three datasets for the survival analysis from the packages in R R Core Team (2016): the flchain dataset Dispenzieri et al. (2012), which was obtained from the ‘survival’ package and contains 7874 data points (69.9% of which are censored), the prostateSurvival dataset (Lu-Yao et al., 2009), which was obtained from the ‘asaur’ package and contains 14294 data points (71.7% of which are censored), and the support dataset Knaus et al. (1995), which was obtained from the ‘casebase’ package and contains 9104 data points (31.9% of which are censored).\n\nWe compared the prediction performances of the extended scoring rules: SCen−log (Eq. (4)), SCen−Brier (Eq. (7)), SCen−RPS (Eq. (10)), and SPortnoy (Eq. (2)). We used a neural network model with B = 32 to estimate ˆF , and we combined it with the EM algorithm to estimate w or {wi}B−1 i=0 . This means that we used the CQRNN algorithm (Pearce et al., 2022), which is the state-of-theart model for quantile regression-based survival analysis, for SPortnoy. We used SCen−log−simple (Eq. (5)) as a metric for discrimination performce. We used D-calibration (Haider et al., 2020) and KM-calibration as metrics for calibration performance, where we used 20 bins of equal-length for D-calibration.\n\nTable 1 shows the results, and each number shows the mean and standard deviation of the measurements of five-fold cross validation. These results showed that SCen−log and SCen−Brier performed the best. Note that the former one is almost equal to the variant of the logarithmic score used in (Rindt et al., 2022), and that the latter one is our new extension of Brier score. Compared to these two scoring rules, the prediction performance of SCen−RPS and SPortnoy were worse than expected and these results seemed to be not close to the true probability distribution, even though we prove that they are conditionally proper scoring rules. It is considered that the estimation ˆw of parameter w by the EM algorithm was not accurate enough to converge to the true probability distribution for SCen−RPS and SPortnoy. As we illustrate in Figure 2, the parameter w of SCen−RPS (and SPortnoy) is usually not close to zero unlike SCen−log, and this fact indicates that it was difficult to find correct w for these two scoring rules.\n\n7 CONCLUSION\n\nWe have discussed the extensions of the four scoring rules for survival analysis, and we have proved that these extensions are proper if the parameter w or {wi}B−1 i=0 is correct. By using these scoring rules, we present neural network models combined with the EM algorithm to estimate the parameter, and our experiments showed that the models with SCen−log and SCen−Brier performed the best. In addition, we clarified the hidden assumption in the proof of the variant of the logarithmic score for survival analysis (Rindt et al., 2022), which suggests us to use a sufficiently large B when we use it as an evaluation metric. We believe that our approach of extending scoring rules for survival analysis can be used for many other known strictly proper scoring rules.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nL. Antolini, P. Boracchi, and E. Biganzoli. A time-dependent discrimination index for survival data.\n\nStatistics in Medicine, 24(24):3927–3944, 2005.\n\nA. Avati, T. Duan, S. Zhou, K. Jung, N. H. Shah, and A. Y. Ng. Countdown regression: Sharp and\n\ncalibrated survival predictions. In Proceedings of UAI 2019, pp. 145–155, 2019.\n\nR. Benedetti. Scoring rules for forecast verification. American Meteorological Society, 138(1):\n\n203–211, 2010.\n\nP. Blanche, M. W. Kattan, and T. A. Gerds. The c-index is not proper for the evaluation of t-year\n\npredicted risks. Biostatistics, 20(2):347–357, 2018.\n\nG. W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,\n\n78(1):1–3, 1950.\n\nB. V. Calster, D. J. McLernon, M. van Smeden, L. Wynants, and E. W. Steyerberg. Calibration: the\n\nAchilles heel of predictive analytics. BMC Medicine, 17:230, 2019.\n\nD. R. Cox. Regression models and life-tables. Journal of the Royal Statistical Society, Series B, 34\n\n(2):187–220, 1972.\n\nL. Dirick, G. Claeskens, and B. Baesens. Time to default in credit scoring using survival analysis: a\n\nbenchmark study. Journal of the Operational Research Society, 68(6):652–665, 2017.\n\nA. Dispenzieri, J. A. Katzmann, R. A. Kyle, D. R. Larson, T. M. Therneau, C. L. Colby, R. J. Clark, G. P. Mead, S. Kumar, L. J. Melton III, and S. V. Rajkumar. Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population. Mayo Clinic Proceedings, 87(6):517–523, 2012.\n\nT. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of\n\nthe American Statistical Association, 102(477):359–378, 2007.\n\nM. Goldstein, X. Han, A. M. Puli, A. Perotte, and R. Ranganath. X-CAL: Explicit calibration for\n\nsurvival analysis. In Proceedings of NeurIPS 2020, pp. 18296–18307, 2020.\n\nI. J. Good. Rational decisions. Journal of the Royal Statistical Society. Series B (Methodological),\n\n14(1):107–114, 1952.\n\nE. Graf, C. Schmoor, W. Sauerbrei, and M. Schumacher. Assessment and comparison of prognostic classification schemes for survival data. Statistics in Medicine, 18(17–18):2529–2545, 1999.\n\nH. Haider, B. Hoehn, S. Davis, and R. Greiner. Effective ways to build and evaluate individual\n\nsurvival distributions. Journal of Machine Learning Research, 21(85):1–63, 2020.\n\nX. Han, M. Goldstein, A. Puli, T. Wies, A. Perotte, and R. Ranganath. Inverse-weighted survival\n\ngames. In Proceedings of NeurIPS 2021, pp. 2160–2172, 2021.\n\nF. E. Harrell, R. M. Califf, D. B. Pryor, K. L. Lee, and R. A. Rosati. Evaluating the yield of medical\n\ntests. Journal of the American Medical Association, 247(18):2543–2546, 1982.\n\nF. Kamran and J. Wiens. Estimating calibrated individualized survival curves with deep learning. In\n\nProceedings of AAAI 2021, pp. 240–248, 2021.\n\nE. L. Kaplan and P. Meier. Nonparametric estimation from incomplete observations. Journal of the\n\nAmerican Statistical Association, 53(282):457–481, 1958.\n\nW. A. Knaus, F. E. Harrell, Jr., J. Lynn, L. Goldman, R. S. Phillips, A. F. Connors, Jr., N. V. Dawson, W. J. Fulkerson, Jr., R. M. Califf, N. Desbiens, P. Layde, R. K. Oye, P. E. Bellamy, R. B. Hakim, and D. P. Wagner. The SUPPORT prognostic model. Objective estimates of survival for seriously ill hospitalized adults. Study to understand prognoses and preferences for outcomes and risks of treatments. Annals of Internal Medicine, 122(3):191–203, 1995.\n\nR. Koenker and B. Bassett, Jr. Regression quantiles. Econometrica, 46(1):33–50, 1978.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nR. Koenker and K. F. Hallock. Quantile regression. Journal of economic perspectives, 15(4):143–\n\n156, 2001.\n\nH. Kvamme and Ø. Borgan. Continuous and discrete-time survival prediction with neural networks.\n\nLifetime Data Analysis, 27:710–736, 2021.\n\nH. Kvamme, Ø. Borgan, and I. Scheel. Time-to-event prediction with neural networks and cox\n\nregression. Journal of Machine Learning Research, 20(129):1–30, 2019.\n\nC. Lee, W. R. Zame, J. Yoon, and M. van der Schaar. DeepHit: A deep learning approach to survival\n\nanalysis with competing risks. In Proceedings of AAAI-18, pp. 2314–2321, 2018.\n\nG. L. Lu-Yao, P. C. Albertsen, D. F. Moore, W. Shih, Y. Lin, R. S. DiPaola, M. J. Barry, A. Zietman, M. O’Leary, E. Walker-Corkery, and S.-L. Yao. Outcomes of localized prostate cancer following conservative management. Journal of the American Medical Association, 302(11):1202–1209, 2009.\n\nA. Mura, M.C. Galavotti, H. Hykel, and B. de Finetti. Philosophical Lectures on Probability: collected, edited, and annotated by Alberto Mura. Synthese Library. Springer Netherlands, 2008.\n\nT. Neocleous, K. V. Branden, and S. Portnoy. Correction to censored regression quantiles by S. Portnoy, 98 (2003), 1001–1012. Journal of the American Statistical Association, 101(474):860– 861, 2006.\n\nG. Parmigiani and L. Inoue. Decision Theory: Principles and Approaches. Wiley Series in Proba-\n\nbility and Statistics. Wiley, 2009.\n\nT. Pearce, J.-H. Jeong, Y. Jia, and J. Zhu. Censored quantile regression neural networks. In Pro-\n\nceedings of NeurIPS 2022 (to appear), 2022.\n\nL. Peng. Quantile regression for survival data. Annual Review of Statistics and Its Application, 8:\n\n413–437, 2021.\n\nS. Portnoy. Censored regression quantiles. Journal of the American Statistical Association, 98(464):\n\n1001–1012, 2003.\n\nR Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statis-\n\ntical Computing, Vienna, Austria, 2016. URL https://www.R-project.org/.\n\nK. Ren, J. Qin, L. Zheng, Z. Yang, W. Zhang, L. Qiu, and Y. Yu. Deep recurrent survival analysis.\n\nIn Proceedings of AAAI-19, pp. 4798–4805, 2019.\n\nD. Rindt, R. Hu, D. Steinsaltz, and D. Sejdinovic. Survival regression with proper scoring rules and\n\nmonotonic neural networks. In Proceedings of AISTATS 2022, 2022.\n\nK. H. Schlag, J. Tremewan, and J. J. van der Weele. A penny for your thoughts: a survey of methods\n\nfor eliciting beliefs. Experimental Economics, 18:457–490, 2015.\n\nR. Sonabend, A. Bender, and S. Vollmer. Avoiding c-hacking when evaluating survival distribution\n\npredictions with discrimination measures. Bioinformatics, 38(17):4178–4184, 2022.\n\nD. E. Tjandra, Y. He, and J. Wiens. A hierarchical approach to multi-event survival analysis. In\n\nProceedings of AAAI 2021, pp. 591–599, 2021.\n\nH. Uno, T. Cai, M. J. Pencina, R. B. D’Agostino, and L. J. Wei. On the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in Medicine, 30(10):1105–1117, 2011.\n\nP. Wang, Y. Li, and C. K. Reddy. Machine learning for survival analysis: A survey. ACM Computing\n\nSurveys, 51(6):1–36, 2019.\n\nP. Zheng, S. Yuan, and X. Wu. SAFE: A neural survival analysis model for fraud early detection. In\n\nProceedings of AAAI-19, pp. 1278–1285, 2019.\n\nQixian Zhong, Jonas Mueller, and Jane-Ling Wang. Deep extended hazard models for survival\n\nanalysis. In Proceedings of NeurIPS 2021, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS OF THEOREMS\n\nWe give proofs of the theorems, which are omitted from the main body of this paper.\n\nA.1 PORTNOY’S ESTIMATOR\n\nWe show a proof of Theorem 4.5.\n\nProof. We consider a fixed c ∼ C, and we prove\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )] ≥\n\nE t∼T |C=c\n\n[SPortnoy(F, (z, δ); w, τ )]\n\n(11)\n\nfor these four cases separately.\n\n• Case 1: c ≤ min{F −1(τ ), ˆF −1(τ )}.\n\n• Case 2: max{F −1(τ ), ˆF −1(τ )} < c.\n\n• Case 3: F −1(τ ) < c ≤ ˆF −1(τ ).\n\n• Case 4: ˆF −1(τ ) < c ≤ F −1(τ ).\n\nNote that, if Inequality (11) holds for any c ∼ C, we can marginalize the inequality with respect to C, and we can prove\n\nE t∼T,c∼C\n\n[SPortnoy( ˆF , (z, δ); w, τ )] ≥\n\nE t∼T,c∼C\n\n[SPortnoy(F, (z, δ); w, τ )],\n\nwhich means that SPortnoy( ˆF , (z, δ); w, τ ) is proper. Therefore, we prove Inequality (11) for the four cases.\n\nCase 1. We prove the case for c ≤ min{F −1(τ ), ˆF −1(τ )}. This means that τc ≤ τ and w = (τ − τc)/(1 − τc). Hence, we have\n\nSPortnoy( ˆF , (z, δ); w, τ ) =\n\n(cid:40)\n\nρτ ( ˆF −1(τ ), t) wρτ ( ˆF −1(τ ), c) + (1 − w)ρτ ( ˆF −1(τ ), z∞)\n\nif t ≤ c, if t > c,\n\n=\n\n(cid:40)\n\n(1 − τ )( ˆF −1(τ ) − t) −τc(1 − τ )( ˆF −1(τ ) − t)/(1 − τc)\n\nif t ≤ c, if t > c.\n\nBy Assumption 3.1, we have Pr(t ≤ c|C = c) = Pr(t ≤ c) = τc and Pr(t > c|C = c) = 1 − τc. Hence, we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )] = Pr(t ≤ c|C = c)(1 − τ ) ˆF −1(τ ) − (1 − τ )\n\nE t∼T |C=c,t≤c\n\n[t]\n\n−Pr(t > c|C = c)τc(1 − τ ) ˆF −1(τ )/(1 − τc)\n\n+\n\nτc(1 − τ ) 1 − τc\n\nE t∼T |C=c,t>c\n\n[t]\n\n= −(1 − τ )\n\nE t∼T |C=c,t≤c\n\n[t] +\n\nτc(1 − τ ) 1 − τc\n\nE t∼T |C=c,t>c\n\n[t].\n\nSince this value is the same for SPortnoy( ˆF , (z, δ); w, τ ) and SPortnoy(F, (z, δ); w, τ ), we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )] =\n\nE t∼T |C=c\n\n[SPortnoy, (z, δ); w, τ )].\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nCase 2. We prove the case for max{F −1(τ ), ˆF −1(τ )} < c.\n\nIf F −1(τ ) ≤ ˆF −1(τ ) < c, then we have\n\nSPortnoy( ˆF , (z, δ); w, τ ) ρτ ( ˆF −1(τ ), t) wρτ ( ˆF −1(τ ), c) + (1 − w)ρτ ( ˆF −1(τ ), z∞)\n\n(cid:40)\n\n=\n\nif t ≤ c, if t > c,\n\nif t ≤ ˆF −1(τ ), if ˆF −1(τ ) < t ≤ c, if t > c,\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −wτ ( ˆF −1(τ ) − t) − (1 − w)τ ( ˆF −1(τ ) − t)\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t)\n\nif t ≤ F −1(τ ), if F −1(τ ) < t ≤ ˆF −1(τ ), if ˆF −1(τ ) < t ≤ c, if t > c,\n\n(cid:40)\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t)\n\nif t ≤ F −1(τ ), if F −1(τ ) < t,\n\nwhere we used −wτ ( ˆF −1(τ ) − t) ≤ w(1 − τ )( ˆF −1(τ ) − t) when F −1(τ ) < t ≤ ˆF −1(τ ) and w ≥ 0 for the inequality.\n\nIf ˆF −1(τ ) ≤ F −1(τ ) < c, then we have\n\nSPortnoy( ˆF , (z, δ); w, τ ) ρτ ( ˆF −1(τ ), t) wρτ ( ˆF −1(τ ), c) + (1 − w)ρτ ( ˆF −1(τ ), z∞)\n\n(cid:40)\n\n=\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −wτ ( ˆF −1(τ ) − t) − (1 − w)τ ( ˆF −1(τ ) − t)\n\n(1 − τ )( ˆF −1(τ ) − t) (1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t)\n\nif t ≤ ˆF −1(τ ), if ˆF −1(τ ) < t ≤ F −1(τ ), if F −1(τ ) < t ≤ c, if t > c,\n\n(cid:40)\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t)\n\nif t ≤ F −1(τ ), if F −1(τ ) < t,\n\nif t ≤ c, if t > c,\n\nif t ≤ ˆF −1(τ ), if ˆF −1(τ ) < t ≤ c, if t > c,\n\n \n\n \n\n\n\n\n\n \n\n \n\n\n\n\n\n=\n\n≥\n\n=\n\n=\n\n>\n\n=\n\nwhere we used −wτ ( ˆF −1(τ ) − t) > w(1 − τ )( ˆF −1(τ ) − t) when ˆF −1(τ ) < t ≤ F −1(τ ) and w ≥ 0 for the inequality.\n\nBy Assumption 3.1, we have Pr(t ≤ F −1(τ )|C = c) = Pr(t ≤ F −1(τ )) = τ and Pr(F −1(τ ) < t|C = c) = 1 − τ . Hence, we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )]\n\n≥ Pr(t ≤ F −1(τ )|C = c)(1 − τ ) ˆF −1(τ ) − (1 − τ )\n\nE t∼T |C=c,t≤F −1(τ )\n\n[t]\n\n−Pr(F −1(τ ) < t|C = c)τ ˆF −1(τ ) + τ\n\nE t∼T |C=c,F −1(τ )<t\n\n[t]\n\n= −(1 − τ )\n\nE t∼T |C=c,t≤F −1(τ )\n\n[t] + τ\n\nE t∼T |C=c,F −1(τ )<t\n\n[t].\n\nBy using a similar argument, we have\n\nE t∼T |C=c\n\n[SPortnoy(F, (z, δ); w, τ )] = −(1 − τ )\n\nE t∼T |C=c,t≤F −1(τ )\n\n[t] + τ\n\nE t∼T |C=c,F −1(τ )<t\n\n[t].\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nNote that this equation holds with equality.\n\nHence, we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )] ≥\n\nE t∼T |C=c\n\n[SPortnoy(F, (z, δ); w, τ )].\n\nCase 3. We prove the case for F −1(τ ) < c ≤ ˆF −1(τ ).\n\nWe have\n\nSPortnoy( ˆF , (z, δ); w, τ ) =\n\n=\n\n≥\n\n(cid:40)\n\nρτ ( ˆF −1(τ ), t) wρτ ( ˆF −1(τ ), c) + (1 − w)ρτ ( ˆF −1(τ ), z∞)\n\nif t ≤ c, if t > c,\n\n(cid:40)\n\n(1 − τ )( ˆF −1(τ ) − t) w(1 − τ )( ˆF −1(τ ) − c) − (1 − w)τ ( ˆF −1(τ ) − c)\n\nif t ≤ c, if t > c,\n\n \n\n\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − c)\n\nif t ≤ F −1(τ ), if F −1(τ ) < t ≤ c, if t > c,\n\nwhere we used −wτ ( ˆF −1(τ ) − t) ≤ w(1 − τ )( ˆF −1(τ ) − t) when F −1(τ ) < t ≤ c ≤ ˆF −1(τ ) and w ≥ 0, and w(1 − τ )( ˆF −1(τ ) − c) > −wτ ( ˆF −1(τ ) − c) when ˆF −1(τ ) > t > c and w ≥ 0 for the inequality. By Assumption 3.1, we have Pr(t ≤ F −1(τ )|C = c) = Pr(t ≤ F −1(τ )) = τ , Pr(F −1(τ ) < t ≤ c|C = c) = τc − τ , and Pr(t > c|C = c) = 1 − τc. Hence, we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )]\n\n≥ Pr(t ≤ F −1(τ )|C = c)(1 − τ ) ˆF −1(τ ) − (1 − τ )\n\nE t∼T |C=c,t≤F −1(τ )\n\n[t]\n\n−Pr(F −1(τ ) < t ≤ c|C = c)τ ˆF −1(τ ) + τ\n\nE t∼T |C=c,F −1(τ )<t\n\n[t]\n\n−Pr(t > c|C = c)τ ˆF −1(τ ) + τ c [t] + τ\n\nE t∼T |C=c,t≤F −1(τ )\n\n= −(1 − τ )\n\nE t∼T |C=c,F −1(τ )<t≤c\n\n[t] + τ c.\n\nBy using a similar argument, we have\n\nSPortnoy(F, (z, δ); w, τ ) =\n\nand so we have\n\n \n\n\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − c)\n\nif t ≤ F −1(τ ), if F −1(τ ) < t ≤ c, if t > c,\n\nE t∼T |C=c\n\n[SPortnoy(F, (z, δ); w, τ )] = −(1 − τ )\n\nE t∼T |C=c,t≤F −1(τ )\n\n[t] + τ\n\nE t∼T |C=c,F −1(τ )<t≤c\n\n[t] + τ c.\n\nNote that this equation holds with equality.\n\nHence, we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )] ≥\n\nE t∼T |C=c\n\n[SPortnoy(F, (z, δ); w, τ )].\n\nCase 4. We prove the case for ˆF −1(τ ) < c ≤ F −1(τ ).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nWe have\n\nSPortnoy( ˆF , (z, δ); w, τ ) ρτ ( ˆF −1(τ ), t) wρτ ( ˆF −1(τ ), c) + (1 − w)ρτ ( ˆF −1(τ ), z∞)\n\n(cid:40)\n\n=\n\n(1 − τ )( ˆF −1(τ ) − t) −τ ( ˆF −1(τ ) − t) −wτ ( ˆF −1(τ ) − c) − (1 − w)τ ( ˆF −1(τ ) − c)\n\nif t ≤ c, if t > c,\n\nif t ≤ ˆF −1(τ ), if ˆF −1(τ ) < t ≤ c, if t > c,\n\n(1 − τ )( ˆF −1(τ ) − t) (1 − τ )( ˆF −1(τ ) − t) w(1 − τ )( ˆF −1(τ ) − c) − (1 − w)τ ( ˆF −1(τ ) − c)\n\nif t ≤ ˆF −1(τ ), if ˆF −1(τ ) < t ≤ c, if t > c,\n\n(1 − τ )( ˆF −1(τ ) − t) −τc(1 − τ )( ˆF −1(τ ) − c)/(1 − τc)\n\nif t ≤ c, if t > c.\n\n \n\n \n\n\n (cid:40)\n\n=\n\n>\n\n=\n\nwhere we used −wτ ( ˆF −1(τ ) − t) > w(1 − τ )( ˆF −1(τ ) − t) when ˆF −1(τ ) < t < c and w ≥ 0, and −wτ ( ˆF −1(τ ) − c) > w(1 − τ )( ˆF −1(τ ) − c) when c > ˆF −1(τ ) and w ≥ 0 for the inequality, and w = (τ − τc)/(1 − τc) when τc ≤ τ for the last equality. By Assumption 3.1, we have Pr(t ≤ c|C = c) = Pr(t ≤ c) = τc and Pr(t > c|C = c) = 1 − τc. Hence, we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )]\n\n≥ Pr(t ≤ c|C = c)(1 − τ ) ˆF −1(τ ) − (1 − τ )\n\nE t∼T |C=c,t≤c\n\n[t]\n\n−Pr(t > c|C = c)τc(1 − τ ) ˆF −1(τ )/(1 − τc) +\n\nτc(1 − τ ) 1 − τc\n\nc\n\n= −(1 − τ )\n\nE t∼T |C=c,t≤c\n\n[t] +\n\nτc(1 − τ ) 1 − τc\n\nc.\n\nBy using a similar argument, we have\n\nE t∼T |C=c\n\n[SPortnoy(F, (z, δ); w, τ )] = −(1 − τ )\n\nE t∼T |C=c,t≤c\n\n[t] +\n\nτc(1 − τ ) 1 − τc\n\nc.\n\nNote that this equation holds with equality.\n\nHence, we have\n\nE t∼T |C=c\n\n[SPortnoy( ˆF , (z, δ); w, τ )] ≥\n\nE t∼T |C=c\n\n[SPortnoy(F, (z, δ); w, τ )].\n\nA.2 VARIANT OF LOGARITHMIC SCORE\n\nWe show a proof of Theorem 4.6.\n\nProof. We consider a fixed c ∼ C, and let t be a sample obtained from T . Let i be the index such that ζi ≤ c < ζi+1. Since Assumption 3.1 holds, we have Pr(ζj < t ≤ ζj+1|C = c) = Pr(ζj < t ≤ ζj+1) = F (ζj+1) − F (ζj) = fj for any j < i, Pr(ζi < t ≤ c|C = c) = F (c) − F (ζi), and\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nPr(c < t|C = c) = Pr(c < t) = 1 − F (c). Hence, we have\n\nEt∼T |C=c[SCen−log( ˆF , (z, δ); w, {ζi}B\n\ni=0)]\n\n= −\n\n(cid:88)\n\nj<i\n\nPr(ζj < t ≤ ζj+1|C = c) log ˆfj\n\n−Pr(ζi < t ≤ c|C = c) log ˆfi\n\n−Pr(c < t|C = c)\n\n(cid:16)\n\nw log ˆfi + (1 − w) log(1 − ˆF (ζi+1))\n\n(cid:17)\n\n= −\n\n(cid:88)\n\nj<i\n\nfj log ˆfj\n\n−(F (c) − F (ζi)) log ˆfi\n\n−(1 − F (c))\n\n(cid:16)\n\nw log ˆfi + (1 − w) log(1 − ˆF (ζi+1))\n\n(cid:17)\n\n= −\n\n(cid:88)\n\nj≤i\n\nfj log ˆfj − (1 − F (ζi+1)) log(1 − ˆF (ζi+1)),\n\nwhere we used w = (F (ζi+1) − F (c))/(1 − F (c)) for the last equality.\n\nHence, we have\n\nEt∼T |C=c[SCen−log( ˆF , (z, δ); w, {ζi}B\n\ni=0)] − Et∼T |C=c[SCen−log(F, (z, δ); w, {ζi}B\n\ni=0)]\n\n(cid:88)\n\nj≤i\n\n= −\n\n≥ 0,\n\nfj(log ˆfj − log fj) − (1 − F (ζi+1))(log(1 − ˆF (ζi+1)) − log(1 − F (ζi+1)))\n\n(12)\n\nwhere we used the fact that the Kullback-Leibler divergence between two probability distributions is non-negative for the inequality. This means that the inequality\n\n(cid:88)\n\n−\n\nk\n\npk(log ˆpk − log pk) ≥ 0\n\nholds for any two probability distributions pk and ˆpk and the equality holds only if pk = ˆpk for all k. Here, we use an (i + 2)-dimensional vector p = (p0, p1, . . . , pi+1), and we set pk = fk for all k ≤ i and we set pi+1 = 1 − F (ζi+1). Note that the vectors p and ˆp constructed in this way is a probability distribution (i.e., (cid:80)\n\nk pk = 1).\n\nSince Inequality (12) holds for any c ∼ C, we marginalize the inequality with respect to C, and we have\n\nEt∼T,c∼C[SCen−log( ˆF , (z, δ); w, {ζi}B\n\ni=0)] ≥ Et∼T,c∼C[SCen−log(F, (z, δ); w, {ζi}B\n\ni=0)],\n\nwhich means that SCen−log( ˆF , (z, δ)) is proper. Moreover, the equality holds only if ˆF = F , and therefore, SCen−log( ˆF , (z, δ)) is strictly proper.\n\nA.3 VARIANT OF BRIER SCORE\n\nWe show a proof of Theorem 4.7.\n\nProof. We consider a fixed c ∼ C, and let t be a sample obtained from T . Let i be the index such that ζi < c ≤ ζi+1. Assuming that Assumption 3.1 holds, we have Pr(ζj < t ≤ ζj+1|C = c) = Pr(ζj < t ≤ ζj+1) = F (ζj+1) − F (ζj) = fj for any j < i, Pr(ζi < t ≤ c|C = c) = F (c) − F (ζi),\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nand Pr(c < t|C = c) = Pr(c < t) = 1 − F (c). Hence, we have\n\nEt∼T |C=c[SCen−Brier( ˆF , (z, δ); {wi}B−1\n\n=\n\n(cid:88)\n\nj<i\n\nPr(ζj < t ≤ ζj+1|C = c)\n\n+Pr(ζi < t ≤ c|C = c)\n\n (1 − ˆfj)2 +\n\ni=0)]\n\ni=0 , {ζi}B \n(1 − ˆfj)2 +\n\n\n\nˆf 2\n\nk\n\n\n\n(cid:88)\n\nj(cid:54)=k \n\nˆf 2\n\nk\n\n\n\n(cid:88)\n\nj(cid:54)=k\n\n+Pr(c < t|C = c)\n\n wi(1 − ˆfi)2 + (1 − wi) ˆf 2\n\ni +\n\nˆf 2 j +\n\n(cid:88)\n\nj<i\n\n(cid:88)\n\n(wj(1 − ˆfj)2 + (1 − wj) ˆf 2 j )\n\n\n\nj>i\n\n\n\n=\n\n(cid:88)\n\nj<i\n\nfj\n\n (1 − ˆfj)2 +\n\nˆf 2\n\nk\n\n(cid:88)\n\nj(cid:54)=k\n\n\n\n + (F (c) − F (ζi))\n\n (1 − ˆfj)2 +\n\n\n\nˆf 2\n\nk\n\n\n\n(cid:88)\n\nj(cid:54)=k\n\n+(1 − F (c))\n\n wi(1 − ˆfi)2 + (1 − wi) ˆf 2\n\ni +\n\nˆf 2 j +\n\n(cid:88)\n\nj<i\n\n=\n\n(cid:88)\n\nj\n\n( ˆf 2\n\nj − 2fj\n\nˆfj + 1),\n\n(cid:88)\n\n(wj(1 − ˆfj)2 + (1 − wj) ˆf 2 j )\n\n\n\nj>i\n\n\n\nwhere we used\n\nfor the first equality and\n\nwi =\n\n \n\n\n\n0 1\n0\n\nif δ = 1 and ζi+1 < z = t if δ = 1 and ζi < z = t ≤ ζi+1 if z ≤ ζi\n\nwi =\n\n(cid:26)(F (ζi+1) − F (c))/(1 − F (c))\n\nfj/(1 − F (c)\n\nif δ = 0 and ζi < z = c ≤ ζi+1 if δ = 0 and ζi+1 < z = c\n\nfor the last equality.\n\nHence we have\n\nEt∼T |C=c[SCen−Brier( ˆF , (z, δ))] − Et∼T |C=c[SCen−Brier(F, (z, δ))]\n\n(cid:88)\n\nj (cid:88)\n\n=\n\n=\n\nj ≥ 0.\n\n( ˆf 2\n\nj − f 2\n\nj − 2fj( ˆfj − fj))\n\n( ˆfj − fj)2\n\n(13)\n\nNote that the equality holds only if ˆfj = fj holds for all j.\n\nSince Inequality (13) holds for any c ∼ C, we have\n\nEt∼T,c∼C[SCen−Brier( ˆF , (z, δ))] ≥ Et∼T,c∼C[SCen−Brier(F, (z, δ))],\n\nwhich means that SCen−Brier( ˆF , (z, δ)) is strictly proper.\n\nB ADDITIONAL EXPERIMENTS\n\nIn this section, we report the results of our additional experiments. The source codes used in our experiments are attached as the supplementary material.\n\nIn our experiments, we used the flchain, prostateSurvival, and support datasets, and we split the data points into training (60%), validation (20%), and test (20%). For each dataset, we divided the\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Comparison between two variants of the logarithmic score for B = 8\n\nMetric SCen−log−simple\n\nD-calibration\n\nKM-calibration\n\nLoss Function SCen−log SCen−log−simple SCen−log SCen−log−simple SCen−log SCen−log−simple\n\nflchain 6.4618 ± 0.1204 6.4176 ± 0.1266 0.0045 ± 0.0004 0.0127 ± 0.0013 0.0048 ± 0.0026 0.0614 ± 0.0081\n\nprostateSurvival 1.3460 ± 0.0476 1.3447 ± 0.0451 0.0002 ± 0.0000 0.0002 ± 0.0001 0.0048 ± 0.0028 0.0083 ± 0.0024\n\nsupport 1.5422 ± 0.0704 1.5368 ± 0.0701 0.0370 ± 0.0032 0.0349 ± 0.0024 0.0057 ± 0.0027 0.0061 ± 0.0033\n\nTable 3: Comparison between two variants of the logarithmic score for B = 16\n\nMetric SCen−log−simple\n\nD-calibration\n\nKM-calibration\n\nLoss Function SCen−log SCen−log−simple SCen−log SCen−log−simple SCen−log SCen−log−simple\n\nflchain 3.6774 ± 0.0386 3.6676 ± 0.0424 0.0005 ± 0.0002 0.0013 ± 0.0004 0.0117 ± 0.0046 0.0162 ± 0.0049\n\nprostateSurvival 1.2880 ± 0.0247 1.3447 ± 0.0451 0.0001 ± 0.0000 0.0002 ± 0.0000 0.0142 ± 0.0036 0.0158 ± 0.0063\n\nsupport 1.6017 ± 0.0733 1.6008 ± 0.0731 0.0147 ± 0.0020 0.0143 ± 0.0021 0.0149 ± 0.0080 0.0158 ± 0.0100\n\nTable 4: Comparison between two variants of the logarithmic score for B = 32\n\nMetric SCen−log−simple\n\nD-calibration\n\nKM-calibration\n\nLoss Function SCen−log SCen−log−simple SCen−log SCen−log−simple SCen−log SCen−log−simple\n\nflchain 1.5054 ± 0.0508 1.5059 ± 0.0513 0.0003 ± 0.0001 0.0003 ± 0.0001 0.0206 ± 0.0049 0.0213 ± 0.0049\n\nprostateSurvival 1.3608 ± 0.0295 1.3609 ± 0.0301 0.0001 ± 0.0000 0.0001 ± 0.0000 0.0312 ± 0.0084 0.0343 ± 0.0102\n\nsupport 1.8307 ± 0.0452 1.8296 ± 0.0446 0.0063 ± 0.0009 0.0062 ± 0.0012 0.0299 ± 0.0115 0.0288 ± 0.0127\n\ntime interval [0, zmax + (cid:15)), where (cid:15) = 10−3, into B − 1 equal-length intervals to get the thresholds {ζi}B i=0 for distribution regression-based survival analysis, and we divided the unit interval [0, 1] into B − 1 equal-length intervals to get the quantile levels {τi}B i=0 for quantile regression-based survival analysis.\n\nAll our experiments were conducted on a virtual machine with an Intel Xeon CPU (3.30 GHz) processor without any GPU and 64 GB of memory running Red Hat Enterprise Linux Server 7.6. We used Python 3.7.4 and PyTorch 1.7.1 for the implementation.\n\nWe constructed a multi-layer perceptron (MLP) for our experiments. It consists of three hidden layers containing 128 neurons, and the number of outputs was B. The type of activation function after the hidden layer was the rectified linear unit (ReLU), and the activation function at the output node was softmax. We can satisfy the assumption that ˆF (t) is a monotonically increasing continuous function by using it. In distribution regression-based survival analysis, each output of MLP estimates ˆfi = ˆF (ζi+1) − ˆF (ζi) for i = 0, 1, . . . , B − 1. By using these outputs { ˆfi}B−1 i=0 , we can calculate i=0 and we can represent the function ˆF (t) as a piecewise linear function connecting the { ˆF (ζi)}B values { ˆF (ζi)}B i=0. Since fi > 0 holds for all i, ˆF (t) estimated in this way is a monotonically increasing continuous function. We can estimate ˆF for quantile regression-based survival analysis by using a similar way.\n\nFirst, we investigated the differences of the prediction performances between SCen−log (defined in Eq. (4)) and SCen−log−simple (defined in Eq. (5)) by using SCen−log−simple, D-calibration, and KMcalibration as metrics. Tables 2– 4 show the results for B = 8, 16, 32, respectively, where the bold numbers were used to emphasize the difference between two scoring rules. These results showed that the prediction performance of these two scoring rules were similar for the prostateSurvival and support datasets even for B = 8. However they showed different prediction performance for the flchain dataset for B = 8 and B = 16, but the performance difference were negligible for B = 32. Therefore, we used B = 32 in the other experiments in this paper.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Prediction performances (lower is better) with various loss functions for B = 32\n\nMetric SCen−log−simple DeepHit (α = 0.1)\n\nModel\n\nD-calibration\n\nKM-calibration\n\nDeepHit (α = 1) DeepHit (α = 10) DRSA S-CRPS IPCW BS(t) game SCen−log SCen−Brier SCen−RPS SPortnoy DeepHit (α = 0.1) DeepHit (α = 1) DeepHit (α = 10) DRSA S-CRPS IPCW BS(t) game SCen−log SCen−Brier SCen−RPS SPortnoy DeepHit (α = 0.1) DeepHit (α = 1) DeepHit (α = 10) DRSA S-CRPS IPCW BS(t) game SCen−log SCen−Brier SCen−RPS SPortnoy\n\nflchain 1.5200 ± 0.0398 1.5858 ± 0.0495 2.0313 ± 0.1648 1.6783 ± 0.0393 2.0470 ± 0.1575 1.9265 ± 0.1093 1.5054 ± 0.0508 1.5137 ± 0.0557 1.6737 ± 0.0821 1.6641 ± 0.0518 0.0005 ± 0.0002 0.0008 ± 0.0003 0.0138 ± 0.0046 0.0043 ± 0.0011 0.0032 ± 0.0005 0.0022 ± 0.0006 0.0003 ± 0.0001 0.0004 ± 0.0002 0.0005 ± 0.0003 0.0071 ± 0.0031 0.0264 ± 0.0071 0.0362 ± 0.0084 0.2077 ± 0.0543 0.1929 ± 0.0135 0.2759 ± 0.1279 0.2770 ± 0.0789 0.0206 ± 0.0049 0.0268 ± 0.0071 0.1553 ± 0.0349 0.0434 ± 0.0067\n\nprostateSurvival 1.3644 ± 0.0293 1.3813 ± 0.0318 1.5688 ± 0.0823 1.4631 ± 0.0273 1.4589 ± 0.0442 1.6413 ± 0.0743 1.3608 ± 0.0295 1.3680 ± 0.0291 1.4821 ± 0.0639 1.4352 ± 0.0420 0.0001 ± 0.0000 0.0003 ± 0.0001 0.0064 ± 0.0035 0.0047 ± 0.0004 0.0018 ± 0.0004 0.0083 ± 0.0018 0.0001 ± 0.0000 0.0001 ± 0.0000 0.0010 ± 0.0005 0.0055 ± 0.0041 0.0418 ± 0.0139 0.0599 ± 0.0341 0.4937 ± 0.1772 0.1845 ± 0.0050 0.6414 ± 0.3043 0.4246 ± 0.0841 0.0312 ± 0.0084 0.0324 ± 0.0090 0.5931 ± 0.3846 0.1895 ± 0.1413\n\nsupport 1.8481 ± 0.0453 1.9996 ± 0.0525 2.3657 ± 0.0441 2.0342 ± 0.0452 2.1162 ± 0.1095 2.3581 ± 0.1604 1.8307 ± 0.0452 1.8467 ± 0.0448 2.1036 ± 0.1012 2.0645 ± 0.0455 0.0056 ± 0.0009 0.0062 ± 0.0010 0.0179 ± 0.0053 0.0057 ± 0.0006 0.0072 ± 0.0011 0.0060 ± 0.0008 0.0063 ± 0.0009 0.0071 ± 0.0009 0.0045 ± 0.0011 0.0237 ± 0.0037 0.0249 ± 0.0067 0.0545 ± 0.0110 0.4273 ± 0.1188 0.2103 ± 0.0162 0.4090 ± 0.1499 0.5325 ± 0.1342 0.0299 ± 0.0115 0.0492 ± 0.0125 0.2668 ± 0.1192 0.0809 ± 0.0381\n\nNext, we computed the prediction performance of several loss functions used in the state-of-the-art neural network models. The loss function of DeepHit (Lee et al., 2018) consists of two terms. The first term is equal to the extension of the logarithmic score SCen−log−simple, and the second term is used to improve a ranking metric between patients. The parameter α is used to control the balance between these two terms, and the weight for the second term is increased by using a large α. The loss function of DRSA (Ren et al., 2019) can also be seen as a variant of logarithmic score, and we set α = 0.25 for the parameter. S-CRPS (Avati et al., 2019) is a variant of the ranked probability score, but Rindt et al. (2022) showed that this scoring rule is not proper in terms of theory of scoring rules. We also implemented the IPCW BS(t) game model, which is proposed in (Han et al., 2021). Table 5 shows the results. The prediction performance of DeepHit degraded by using a large α, which means that it is better to use SCen−log−simple by setting α = 0. The other prediction models did not outperform SCen−log and SCen−Brier.\n\nFinally, we show an ablation study on the training models with and without the EM algorithm. Figure 3 shows the average survival functions for B = 32, which means that the average of F (t) = 1 − F (t) for all patients in test dataset were shown. The parameter w (or {wi}B−1 i=0 ) is included in the computation of the gradient in the neural network training of the prediction model without the EM algorithm, whereas the prediction model with the EM algorithm handles the parameter as a constant. The actual survival functions were estimated by the Kaplan-Meier estimator. These results showed that the average predictions for the extension of the logarithmic score were close to the Kaplan-Meier curve regardless of the use of the EM algorithm for the three datasets. As for the other three estimators, the average predictions with the EM algorithm were closer than those without the EM algorithm to the Kaplan-Meier curves. These results mean that we need the EM algorithm except for the extension of the logarithmic score.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(a) SCen−log on flchain\n\n(b) SCen−log on prostateSurvival\n\n(c) SCen−log on support\n\n(d) SCen−Brier on flchain\n\n(e) SCen−Brier on prostateSurvival\n\n(f) SCen−Brier on support\n\n(g) SCen−RPS on flchain\n\n(h) SCen−RPS on prostateSurvival\n\n(i) SCen−RPS on support\n\n(j) SPortnoy on flchain\n\n(k) SPortnoy on prostateSurvival\n\n(l) SPortnoy on support\n\nFigure 3: Comparisons of average survival functions with and without EM algorithm for B = 32\n\n20\n\n010002000300040005000Time0.00.20.40.60.81.0Survival rateflchainlogarithmic w/ EMlogarithmic w/o EMKaplan-Meier020406080100Time0.00.20.40.60.81.0Survival rateprostateSurvivallogarithmic w/ EMlogarithmic w/o EMKaplan-Meier0500100015002000Time0.00.20.40.60.81.0Survival ratesupportlogarithmic w/ EMlogarithmic w/o EMKaplan-Meier010002000300040005000Time0.00.20.40.60.81.0Survival rateflchainBrier w/ EMBrier w/o EMKaplan-Meier020406080100Time0.00.20.40.60.81.0Survival rateprostateSurvivalBrier w/ EMBrier w/o EMKaplan-Meier0500100015002000Time0.00.20.40.60.81.0Survival ratesupportBrier w/ EMBrier w/o EMKaplan-Meier010002000300040005000Time0.00.20.40.60.81.0Survival rateflchainRPS w/ EMRPS w/o EMKaplan-Meier020406080100Time0.00.20.40.60.81.0Survival rateprostateSurvivalRPS w/ EMRPS w/o EMKaplan-Meier0500100015002000Time0.00.20.40.60.81.0Survival ratesupportRPS w/ EMRPS w/o EMKaplan-Meier010002000300040005000Time0.00.20.40.60.81.0Survival rateflchainPortnoy w/ EMPortnoy w/o EMKaplan-Meier020406080100Time0.00.20.40.60.81.0Survival rateprostateSurvivalPortnoy w/ EMPortnoy w/o EMKaplan-Meier0500100015002000Time0.00.20.40.60.81.0Survival ratesupportPortnoy w/ EMPortnoy w/o EMKaplan-Meier",
    "reference": "# Summary Of The Paper\n\nThis work is after finding proper scoring rules in survival analysis. They have a parameter vector w whose true specification underpins the proofs their provide for the discussed scoring rules being proper. They approximate the parameter vector w using an EM algorithm which can in turn be plugged into their scoring rule.\n\n# Strength And Weaknesses\n\nIt is not obvious from the paper how explanatory covariates can be integrated. The parameter vector which is central in the correctness of the deduced proofs is distribution dependent. For each covariate combination, the values for w changes. Therefore, I cannot think of applying what this paper proposes in a survival analysis application in which the sample is not thought to be drawn from a homogeneous population.\n\nThe experiments are not shedding any light on what I mentioned above. Table 1 has several metrics for which several loss functions have been tried but the author is left guessing what to take out of it. Since this paper is concerned with survival analysis, it would have been better to show that the metrics that this paper proposes to be proper, when minimized, prove to be better than those normally minimized for which such guarantees do not exist.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is quite clear. However, given the parameter set w is so central in this paper I would have liked it explained better. The first encounter with the parameter w is after equation 2 and I find the description quite terse.\n\n# Summary Of The Review\n\nI have doubts about the applicability of this work. It is not obvious to me how this work can be applied to a real-world scenario where explanatory covariates exist and the population from which the data is sampled is not homogeneous. The experiments did not seem to counter my current understanding.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nOFFLINE POLICY INTERVAL ESTIMATION WITHOUT SUFFICIENT EXPLORATION OR REALIZABILITY\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study the problem of offline policy evaluation (OPE), where the goal is to estimate the value of given decision-making policy without interacting with the actual environment. In particular, we consider the interval-based OPE, where the output is an interval rather than a point, indicating the uncertainty of the evaluation. The interval-based estimation is especially important in OPE since, when the data coverage is insufficient relative to the complexity of the environmental model, any OPE method can be biased even with infinite sample size. In this paper, we characterize such irreducible biases in terms of the discrepancy between the target policy and the data-sampling distribution, and show that the marginalimportance-sampling (MIS) estimator achieves the minimax bias with an appropriate importance-weight function. Motivated with this result, we then propose a new interval-based MIS estimator that asymptotically achieves the minimax bias.\n\n1\n\nINTRODUCTION\n\nThe offline policy evaluation (OPE) is the art of estimating the value of given decision-making policies based on offline datasets without interacting with the actual environment. Since the interaction with the environment is often infeasible or expensive in many real-world applications, it is better to evaluate the value offline rather than online.\n\nIn the literature, it is understood from theoretical perspectives that there are two fundamental conditions for OPE to be successful: sufficient exploration, the coverage of the data-sampling distribution over the state-action space relative to the target policy, and realizability, the knowledge of correct environmental model with bounded complexity. In particular, if neither of these two conditions are met in a certain manner, it is known that OPE is never sample efficient, i.e., it takes prohibitively large sample to make the estimation reasonably accurate (Wang et al., 2020; Zanette, 2021). In practice, given a problem instance of OPE, consisting of an environment and a dataset, it is difficult to confirm that these conditions hold or to modify the problem instance so that these conditions hold, making the existing theoretical guarantees less practical. Towards practical OPE, we set our research objective to develop a theoretically-sound value estimator without assuming these two conditions.\n\nTowards our objective, we first analyze the statistical performance of OPE methods when the two assumptions do not hold (Section 4). The key quantity is the information-theoretic worst-case bias of the value estimator (Eq. (5)) and its minimum termed the minimax bias (Eq. (6)), which is positive when there exist multiple indistinguishable environments, given only a problem instance of OPE. In fact, we show that the minimax bias can be non-zero if we do not assume the two conditions (Corollary 4.2). It suggests that, without the two assumptions, there exists a problem instance that any point-based value estimator is not reliable.\n\nGiven the existence of irreducible bias, we propose an alternative formulation of offline policy evaluation called minimax-bias offline policy interval estimation (minimax-bias OPI), where the objective is to estimate the shortest possible interval containing the true value, instead of a point estimate (Section 5). Since our characterization of the minimax bias allows us to define the optimal interval (Definition 5.1), the minimax-bias OPI is formulated as a problem to estimate the optimal interval (Problem 5.1).\n\nWe provide a theoretical foundation to solve the minimax-bias OPI based on the marginal importance sampling estimator (Section 6). The key result is that the optimal importance weight mini-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nmizing the distributional Bellman residual (DBR) allows us to construct an approximately optimal interval (Theorem 6.3). This illustrates that our problem setting is well-posed and can be solved under realistic assumptions if we can solve the minimization of DBR. Accordingly, we develop a novel algorithm in Section 7 to find the best importance weight function, which results in an interval estimator applicable even if the two fundamental conditions do not hold (Theorem 7.7).\n\nBefore proceeding to these results, we introduce basic mathematical notation in the rest of this section, review the related work in Section 2, and introduce the useful OPE-specific notation in Section 3.\n\nMathematical notation. Let I denote the identity operator and let a∨b := max{a, b} and a∧b := min{a, b} denote the maximum and minimum operators for a, b ∈ R, respectively. Let X be a metric space with Borel algebra Σ. Let B(X ) and C (X ) be the spaces of the real-valued measurable bounded functions and the continuous functions on X , respectively, both of which is equipped with the uniform norm ∥f ∥∞ := supx∈X |f (x)|. Let M (X ) denote the space of the finite signed measures on the same space X , equipped with the total variation (TV) norm ∥P ∥TV := supE++E−=X {P (E+) − P (E−)}. In particular, let δx ∈ M (X ), x ∈ X , denote Dirac’s delta measure. For any f ∈ B(X ) and any P ∈ M (X ), let ⟨f, P ⟩ := (cid:82) f (x)dP (x) be a shorthand for the (signed) expectation of f with respect to P . Let ⊙ denote the importance-weighting operation given by d(f ⊙ P )(x) := f (x)dP (x), f ∈ B(X ), P ∈ M (X ). Let L1(P ) be the space of the functions integrable with respect to P ∈ M (X ), i.e., ∥f ⊙ P ∥TV < ∞. Let L (V) denote the set of the bounded linear operator on a normed vector space V. For any A ∈ L (M (X )), let A∗ ∈ L (B(X )) denote the conjugate operator such that ⟨A∗f, P ⟩ = ⟨f, AP ⟩ for f ∈ B(X ) and P ∈ M (X ).\n\n2 RELATED WORK\n\nThe problem of estimating the interval containing the true value has been known as offline policy interval estimation (OPI). This section reviews the existing studies on OPI by dividing the previous OPI methods into two categories: non-asymptotic and asymptotic methods (see Table 1 for the summary of comparison). We also discuss our contribution to the literature.\n\nThe non-asymptotic methods typically put their emphasis on the validity of the interval with any finite sample size, where intervals are valid if they contain the true value J(π). For instance, Feng et al. (2020; 2021) compute intervals that contain the true policy value with high probability, under the realizability of the policy Q-functions qπ. Jiang and Huang (2020) also proposed an interval estimator with validity under more relaxed realizability condition that either the policy Q-function qπ or the marginal density ratio function wπ is realizable. One limitation of this approach is the theoretical understanding on the tightness of the interval is often unclear or partial. Another limitation of this approach is that they tend to require the realizability with known complexity. This requirement is not desirable for practical use; if we used a too complex hypothesis class such as a reproducing kernel Hilbert space with infinite radius, the resultant interval would be trivial, and thus, non-informative.\n\nThe asymptotic methods focus on the asymptotically dominant term of the uncertainty in the large sample limit, which typically allows us to theoretically understand their behavior, especially the tightness, in depth. For instance, Kallus and Uehara (2020); Shi et al. (2021) gave confidence interval estimators that achieve the efficiency lower bound. The bootstrap estimators (Hao et al., 2021) also enable us to compute the asymptotically exact confidence intervals in a more flexible manner. One major limitation is that they assume both the sufficient exploration and the realizability conditions of qπ and wπ hold, which can be hardly validated in real-world applications. These assumptions are essential to their analyses because they focus on estimation of the asymptotic variance of order O(n−1/2), assuming that the bias is negligible. Therefore, these methods are not applicable to our setting where the asymptotic bias of order O(1) dominates the asymptotic variance.\n\nIn this study, we take the asymptotic approach, but with a focus on the estimation of the bias rather than the variance, because the bias is dominant in our setting where the sufficient exploration and the realizability do not hold at all. Our contributions are threefold. First, we characterize the theoretical lower bound of the asymptotic bias through the asymptotic analysis, which serves as a theoretical\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison of OPI methods. qπ and wπ denote the Q-function and the marginal density ratio function, respectively, w♯ π denotes a generalization of wπ for the insufficient exploration setting.\n\nMethod\n\nAssumptions\n\nAsymptotic Exploration Realizability Complexity\n\nBONDIC (Feng et al., 2021)\n\nMVI (Jiang and Huang, 2020)\n\nDRL (Kallus and Uehara, 2020) D2OPE (Shi et al., 2021)\n\nOurs\n\n—\n\n—\n\nYes\n\nYes\n\n—\n\n—\n\nYes\n\n—\n\nqπ\n\nqπ or wπ\n\nKnown\n\nKnown\n\nqπ and wπ —\n\n— w♯\n\nπ\n\n—\n\nGuarantee\n\nValid\n\nValid\n\nEfficient\n\nValid Optimal\n\nfoundation of OPE without sufficient exploration or realizability assumptions. Second, without the two assumptions, we develop an interval estimation method that outputs an asymptotically valid interval, that is, an interval that contains the true value in the large sample limit. Third, under the realizability condition of the generalized marginal density ratio function w♯ π, we show that the estimated interval is optimal.\n\n3 PRELIMINARIES\n\nWe first introduce our formulation of reinforcement learning and offline policy evaluation. Then, we introduce two fundamental concepts in RL, a Q-function and an occupancy measure, along with shorthand notation for them.\n\nOffline policy evaluation. Let X := S × A be a compact Hausdorff space representing the stateaction space of the system with |X | < ∞.1 Let M := (ι, T, R) be the Markov decision process (MDP) of environment on X , where ι ∈ M (S) is the initial state distribution, T : X → M (S) is the transition dynamics and R : X → M ([−1, 1]) is the conditional reward distribution. Let π : S → M (A) be the target policy. Then, the value J(π) of π with respect to M is given by the γ-discounted expected average reward\n\nJ(π) ≡ JM(π) := EM,π\n\n(1 − γ)\n\n(cid:34)\n\n(cid:35)\n\nγt−1rt\n\n,\n\n∞ (cid:88)\n\nt=1\n\nwhere γ ∈ (0, 1) is a discounting factor and EM,π denotes the expectation with respect to the Markov chain generated with at ∼ π(st), rt ∼ R(st, at), st+1 ∼ T (st, at) for all t ≥ 1 and s1 ∼ ι.\n\nIn offline policy evaluation, we are given a dataset D := (Dι, DT,R) as input, where Dι := {sι,j}n is a set of initial states and DT,R := {(xi, s′\n\ni=1 is a set of transition records sampled from\n\ni, ri)}n\n\nj=1\n\ndGM,β(D) :=\n\nn (cid:89)\n\nj=1\n\ndι(sι,j) ·\n\nn (cid:89)\n\ni=1\n\ndβ(xi)dT (s′\n\ni|xi)dR(ri|xi),\n\nwhere β ∈ M (X ) is an arbitrary state-action-sampling distribution.2 Then, an instance of the offline policy evaluation (OPE) is identified by the quadruple P := (M, β, π, γ) and formalized as follows.\n\nProblem 3.1 (Offline policy evaluation, OPE). Given (D, π, γ) where D ∼ GM,β, estimate J(π).\n\nQ-function and occupancy measure. Let ιπ ∈ M (X ) and Tπ ∈ L (M (X )) be the initial stateaction distribution and the state-action transition operator associated with π such that dιπ(s, a) := dι(s)dπ(a|s) and d(TπP )(s, a) := (cid:82) dT (s|x)dπ(a|s)dP (x) for s ∈ S, a ∈ A and P ∈ M (X ),\n\n1This includes the cases where S and A are finite or compact subsets of finite-dimensional Euclidean spaces. 2For simplicity, we assume the sample sizes of Dι and DT,R are the same. The generalization with different\n\nsample sizes is possible with minor modification.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nrespectively. Also let ρ ∈ B(X ) be the expected reward function such that ρ(x) := (cid:82) rdR(r|x) for x ∈ X . Then, the value J(π) is rewritten as\n\nJ(π) = ⟨ρ, Γπιπ⟩ = ⟨Γ∗\n\nπρ, ιπ⟩ = ⟨ρ, μπ⟩ = ⟨qπ, ιπ⟩, where Γπ := (1 − γ) (cid:80)∞ t=1(γTπ)t−1 ∈ L (M (X )) is the accumulation operator, μπ := Γπιπ ∈ M (X ) is the normalized occupancy measure of π (henceforth the occupancy measure), and qπ := πρ ∈ B(X ) is the normalized Q-function of π (henceforth the Q-function). Note that we have Γ∗ ∥q∥∞ ≤ 1 and ∥μπ∥TV = 1 thanks to the normalization.\n\n(1)\n\nTwo Bellman equations. One of the essential difficulties of OPE lies in the fact the direct estimation of the accumulation operator Γπ (and hence μπ and qπ) is intractable due to the infinite sum. The Bellman equation is useful to mitigate this problem. Here, we introduce two variants of the Bellman equation, the functional and distributional Bellman equations, given by\n\nρ = ∆∗\n\nπqπ,\n\nιπ = ∆πμπ,\n\n(2)\n\nwhere ∆π := Γ−1 π = (I − γTπ)/(1 − γ) is the difference operator. Note that, in the Bellman equations, both qπ and μπ are uniquely characterized via more directly estimatable quantities (ρ, Tπ) and (ιπ, Tπ), respectively.\n\nThe errors of the Bellman equations are referred to as the Bellman residuals. distributional Bellman residual (DBR) is given by\n\nIn particular, the\n\nwhich plays an important role in our analysis.\n\nRπ(w) := ιπ − ∆π(w ⊙ β) ∈ M (X ),\n\nEmpirical estimates. Finally, we introduce the empirical estimates of (ιπ, Tπ, ρ, β) based on the dataset D as follows. For all P ∈ M (X ) and x ∈ X ,\n\nˆιπ :=\n\n1 n\n\nn (cid:88)\n\nj=1\n\nδxι,j ,\n\nˆTπP :=\n\nn (cid:88)\n\ni=1\n\nδx′ N (xi)\n\ni\n\nP ({xi}),\n\nˆρ(x) :=\n\n1 N (x)\n\n(cid:88)\n\nri,\n\nˆβ :=\n\ni:xi=x\n\n1 n\n\nn (cid:88)\n\ni=1\n\nδxi,\n\n(3)\n\nwhere N (x) := 1 ∨ |{i : xi = x}| is the data-counting function (with the zero-division safeguard) and xι,i := (sι,i, aι,i) and x′ i) are the state-action pairs associated with additional samples i ∼ π(s′ aι,i ∼ π(sι,i) and a′\n\ni := (s′ i, a′ i), respectively.\n\nThroughout this paper, we employ the conventional marginal importance sampling (MIS) estimator (Liu et al., 2018; Xie et al., 2019) to estimate the value in offline. The MIS estimator associated with a weight function w ∈ B(X ) is given by\n\nˆJ(w) := ⟨ˆρ, w ⊙ ˆβ⟩.\n\n(4)\n\nThe MIS estimator is justified if the weight function w is equal to the marginal density wπ := dμπ (assuming it exists) since, in that case, the MIS estimator is unbiased, E[ ˆJ(wπ)] = J(π), according to (1).\n\ndβ\n\nNote, however, that wπ does not exist when the exploration is insufficient, β ̸≫ μπ, and the unbiasedness cannot be guaranteed in general. Two natural questions thus arise: Does the MIS estimator still enjoy any theoretical guarantee in such a general setting? If so, what is the best weight function w? In short, the answer to the first question is affirmative and the answer to the second question is one of the main contributions of this work.\n\n4\n\nIRREDUCIBLE BIAS IN OFFLINE POLICY EVALUATION\n\nIn this section, we theoretically analyze the statistical performance of OPE methods without sufficient exploration or realizability assumptions. As a result, we will show that any OPE method must incur an irreducible bias that never disapears even when the sample size goes infinity. Given such a negative result, we instead propose a novel problem setting called the minimax-bias OPI, where the\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ngoal is to estimate the interval that contains the true value and is as short as possible. The proposed problem setting is expected to be solved without sufficient exploration or realizability assumptions, and thus, will be of practical use.\n\nTo study the statistical performance of OPE methods, we introduce the notion of the minimax bias of the point-based estimators. Let ˆJ be any random variable representing a point-based OPE estimator. Then, the information-theoretic worst-case bias of ˆJ is given by\n\nε[ ˆJ] ≡ ε[ ˆJ; P] :=\n\nsup (M′,β)∼(M,β)\n\n(cid:12) (cid:12)\n\n(cid:12)JM′(π) − E ˆJ\n\n(cid:12) (cid:12) (cid:12) ,\n\n(5)\n\nwhere the equivalence ∼ is defined by the equality with respect to the corresponding distributions of the dataset, i.e., GM′,β = GM,β. If there exist equivalent environments M and M′ that result in the different policy values JM(π) ̸= JM′(π) yet indistinguishable from the dataset, the worst-case bias ε[ ˆJ] is inevitable without an additional source of information, i.e., domain knowledge. The minimax bias is then defined as the minimum possible worst-case bias of OPE,\n\nε⋆(π) ≡ ε⋆(π; P) := inf ˆJ\n\nε[ ˆJ; P],\n\n(6)\n\nwhich can be thought of as a characteristic of the problem P indicating its hardness in terms of the irreducible uncertainty even with infinitely large sample. In fact, there exists the unique ˆJ achieving the infimum and we refer to it as the optimal point estimator J⋆(π). Our main objective is to understand the minimax bias in various settings.\n\nTo this end, we introduce a novel concept, the projection of the occupancy measure μπ with respect to β. Let Πβ be the projection operator onto the support of β such that ΠβP = χβ ⊙ P , P ∈ M (X ) and χβ(x) = 1{x ∈ supp β}. Definition 4.1 (Projected occupancy measure and its importance weight). We refer to\n\nμ♯\n\nπ := (1 − γ)Πβ\n\n∞ (cid:88)\n\nt=0\n\n(γTπΠβ)tιπ\n\n(7)\n\nas the projected occupancy measure of π. Correspondingly, we also refer to\n\nas the projected importance weight of π with respect to β.\n\nw♯\n\nπ :=\n\ndμ♯ π\ndβ\n\nπ = wπ whenever wπ exists. On the other hand, μ♯\n\nNote that w♯ w♯ μπ since it is always identifiable given GM,β, thanks to the projection Πβ, and μ♯ μπ is also identifiable.\n\nπ can be thought of as an extension of wπ in the sense it is always well-defined and π can be thought of as the known component of π = μπ whenever\n\nWe now present our main result, which discovers close relationships between the minimax bias ε⋆(π), the MIS estimator ˆJ(w), the DBR Rπ(w) and the projected importance weight w♯ π. Theorem 4.1. For all w ∈ B(X ), we have\n\nε⋆(π) ≤ ε\n\n(cid:104) ˆJ(w)\n\n(cid:105)\n\n≤ ∥Rπ(w)∥TV ≤ ε⋆(π) +\n\n1 + γ 1 − γ\n\n∥w − w♯\n\nπ∥L1(β).\n\n(8)\n\nProof (sketch). The most nontrivial part is the last inequality, which follows from the constructive proof of\n\nε⋆(π) ≥ ∥Rπ(w♯\n\nπ)∥TV.\n\n(9)\n\nIn particular, we construct two worst-case environments M± under the constraint (M±, β) ∼ (M, β). Roughly speaking, the environments are constructed to have a special state ⊥ in the underexplored region of X absorbing all the transition to that region, and have the extreme reward there, i.e., ρ(⊥) = ±1. With this explicit construction of the environments, we can give an analytic lower bound of ε⋆(π), which coincides with ∥Rπ(w♯\n\nπ)∥TV. See Section B for the complete proof.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAn immediate consequence of Theorem 4.1 is that it tells us when and how the minimax bias is positive. To see this, let μ̸♯ Corollary 4.2. We have ε⋆(π) = ∥∆πμ̸♯\n\nπ be the projection residual of μπ.\n\nπ := μπ − μ♯\n\nπ∥TV and thus\n\n∥μ̸♯\n\nπ∥TV ≤ ε⋆(π) ≤\n\n1 + γ 1 − γ\n\n∥μ̸♯\n\nπ∥TV.\n\nIn other words, the minimax bias is zero if and only if the projection residual μ̸♯ π is zero, or equivalently, if μπ is absolutely continuous with respect to the data distribution β. Moreover, the size of the minimax bias is proportional to the size of the projection residual μ̸♯ π. It thus formally asserts the limitation of the point-based estimators in the insufficient exploration settings.\n\nIn summary, any OPE methods must be biased in the worst case whenever the exploration is insufficient, μπ ̸≪ β, motivating the interval-based approach.\n\n5 PROBLEM SETUP: MINIMAX-BIAS OPI\n\nAs discussed above, the point-based estimator suffers from irreducible bias, suggesting a hardness in Problem 3.1 under realistic assumptions. Given such a limitation, we propose an alternative problem setting called the minimax-bias OPI so that we can solve it under realistic assumptions. Since there exists a bias, our idea is to estimate the value by an interval that contains the true value, instead of a point.\n\nLet us first define the target of the estimation, which we call the optimal interval. As discussed earlier, since J⋆(π) and ε⋆(π) are the best possible point estimator and its error guarantee, respectively, the optimal interval can be naturally formulated as follows.\n\nDefinition 5.1 (Optimal interval). The following is referred to as the optimal interval:\n\nI⋆(π) ≡ I⋆(π; P) := [J⋆(π) − ε⋆(π), J⋆(π) + ε⋆(π)].\n\nThen, the problem of offline policy interval estimation (OPI), an uncertainty-aware interval extension of Problem 3.1, is formalized as follows. Problem 5.1 (Minimax-bias OPI). Estimate I⋆(π) based on (D, π, γ), where D ∼ GM,β.\n\nTowards estimating the optimal interval, let us introduce two desirable properties of an interval. Definition 5.2 is stronger than Definition 5.3. Definition 5.2 (Approximate optimality). We refer to the interval I satisfying dH (I, I⋆(π)) ≤ ε, where dH (·, ·) is the Hausdorff distance, as ε-approximately optimal. Moreover, a sequence of intervals {In}n≥1 is said to be asymptotically (approximately) optimal if it converges to an (approximately) optimal interval. Definition 5.3 (Validity). An interval I ⊂ R is said to be valid if I ⊃ I⋆(π). Moreover, a sequence n≥k In is valid. of intervals {In}n≥1 is said to be asymptotically valid if its lower limit limk→∞\n\n(cid:84)\n\n6 THEORETICAL FOUNDATION OF MINIMAX-BIAS OPI\n\nWe provide a theoretical foundation to solve Problem 5.1, mainly referring to Theorem 4.1.\n\nFirst, Theorem 4.1 implies that the MIS estimator ˆJ(w) with the projected importance weight w = w♯ Corollary 6.1. There exists w ∈ L1(β) such that ε[ ˆJ(w)] = ε⋆(π).\n\nπ is optimal in the sense it achieves the minimax bias. More generally:\n\nThis motivates us to seek for the best weight function w within the MIS framework. The following corollary shows the next significant implications, that the optimal point-based and interval-based OPE is achieved with combining the MIS estimator and the minimization of DBR. Corollary 6.2. Let w⋆ be a minimizer of DBR in a compact hypothesis class W ⊂ B(X ),\n\nw⋆ ∈ argmin\n\nw∈W\n\n∥Rπ(w)∥TV.\n\n(10)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nThen, we have\n\nand\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) E ˆJ(w⋆) − J⋆(π) (cid:12) (cid:12) ≤\n\n1 + γ 1 − γ\n\nεW ,\n\n|∥Rπ(w⋆)∥TV − ε⋆(π)| ≤\n\n1 + γ 1 − γ\n\nεW ,\n\n(11)\n\n(12)\n\nwhere εW := minw∈W ∥w − w♯\n\nπ∥L1(β) is the realizability error of W.\n\nIn other words, given W is expressive enough to approximate w♯ π well and thus εW is negligible, the optimal point estimator J⋆(π) and its uncertainty ε⋆(π) can be estimated with a solution w⋆ to (10) and its objective value ∥Rπ(w⋆)∥TV, respectively. These observations naturally lead us to the following proxy to the optimal interval\n\nI(π; W) :=\n\n(cid:104)\n\nE ˆJ(w⋆) − ∥Rπ(w⋆)∥TV, E ˆJ(w⋆) + ∥Rπ(w⋆)∥TV\n\n(cid:105)\n\n.\n\n(13)\n\nIn fact, it satisfies two desirable properties: the validity and the approximate optimality. Theorem 6.3. The interval I(π; W) is valid and 2 1+γ\n\n1−γ εW -approximately optimal.\n\n7 ESTIMATION OF OPTIMAL INTERVAL\n\nAs discussed in the previous section, the estimation of the optimal interval I⋆(π) is reduced to the minimization of the TV norm ∥Rπ(w)∥TV. However, even the estimation of the exact TV norm is notorious for its difficulty (e.g., see Section 5 in Sriperumbudur et al. (2012)), let alone the minimization. This motivate us to develop new variational approximations for the TV norm. In particular, we newly introduce two approximations of ∥Rπ(w)∥TV, each of which is suitable for the evaluation and the optimization of the objective, respectively.\n\n7.1 EVALUATING THE OBJECTIVE\n\nThe first approximation reduces the evaluation of the TV norm to a conventional regression problem. To see this, let us begin with the approximation formula for the TV norm of general measures. Let F be a universal function approximator on X , i.e., a set of functions dense in C (X ), such as the reproducing kernel Hilbert space (RKHS) with a universal kernel (Sriperumbudur et al., 2010) or a set of neural networks (Hornik et al., 1989). Proposition 7.1. For all positive measures P ∈ M (X ), we have\n\n:= max{−1, min{1, t}} denotes the clipping of t ∈ R to [−1, 1].\n\n∥P ∥TV = sup f ∈F\n\n⟨\n\nf\n\n(cid:74)\n\n(cid:75)\n\n, P ⟩\n\n(14)\n\nwhere\n\nt\n\n(cid:74)\n\n(cid:75)\n\nThe proof is relegated to Section C. Letting P = Rπ(w), we immediately get the following special case useful for the evaluation of DBR. Corollary 7.2. For all w ∈ B(X ), we have\n\n∥Rπ(w)∥TV = sup f ∈F\n\n⟨\n\nf\n\n, Rπ(w)⟩. (cid:75)\n\n(cid:74)\n\n(15)\n\nThe supremum (15) is estimated via the regularized empirical risk minimization framework, minimizing the following objective\n\nf\n\nˆL(f ) := −⟨ (cid:74)\n\n(16) where ˆRπ(w) := ˆιπ − (1 − γ)−1(I − γ ˆTπ)(w ⊙ ˆβ) is the natural empirical counterpart of the DBR, Ψ : F → R is a penalty function that makes it easier to minimize and prevent the minimizer from overfitting. Once the regularized empirical risk minimizer ˆf := argminf ∈F ˆL(f ) is found, we then ˆf , ˆRπ(w)⟩ to approximate the RHS of (15) and get the desired estimate. The resultant evaluate ⟨ (cid:75) (cid:74) procedure of the evaluation of the TV norm is summarized in Algorithm A.1.\n\n, ˆRπ(w)⟩ + Ψ(f ), (cid:75)\n\nIn fact, under a reasonable choice on F and Ψ, it is shown that the output of Algorithm A.1 is consistent. The details on the specific choice of F and Ψ and the proof is provided in Section D.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 7.3. Let EvaluateDBR(D, F, w) be the output of Algorithm A.1, where F and Ψ are given as in Section D.1. Then, for all w ∈ W, we have EvaluateDBR(D, F, w) → ∥Rπ(w)∥TV in probability.\n\n7.2 MINIMIZING THE OBJECTIVE\n\nWe now turn to the minimization of ∥Rπ(w)∥TV with respect to w ∈ W. The previous variational formula (15) is not straightforwardly usable for this purpose since it ends up with the saddlepoint problem, which we found is too unstable in practice. To mitigate this issue, we introduce a minimization-based approximation of the TV norm.\n\nTo this end, we first introduce the convolution of the TV norm with the maximum mean discrepancy (MMD) (Sriperumbudur et al., 2009). Here, the MMD of a measure P ∈ M (X ) is given by MMDκ(P ) := (cid:112)⟨κ, P ⊗2⟩, where κ : X 2 → R is a c0-universal kernel in the sense of Sriperumbudur et al. (2010) and P ⊗2 denotes the product measure of P on X 2. Definition 7.1 (Convolution norm). For all P ∈ M (X ) and u ≥ 1, we refer to\n\n∥P ∥u,κ := inf Q≪P\n\n{u MMDκ(P − Q) + ∥Q∥TV}\n\n(17)\n\nas the u-convolution norm of P .\n\nThe following proposition shows the u-convolution norm is a reasonable approximation of the TV norm and admits a sample-based estimation unlike the TV norm. Proposition 7.4. For all P ∈ M (X ), we have\n\nMoreover, if P is a probability measure, for all δ ∈ (0, 1), we have\n\n∥P ∥TV = lim u→∞\n\n∥P ∥u,κ.\n\n∥ ˆPn − P ∥u,κ = O\n\n(cid:32)(cid:114)\n\n(cid:33)\n\nu2 + ln(1/δ) n\n\n(18)\n\n(19)\n\nwith probability ≥ 1 − δ, where ˆPn := 1 (x1, ..., xn) independently drawn from P , n ≥ 1.\n\nn\n\n(cid:80)n\n\ni=1 δxi is the empirical distribution of an n-sample\n\nProof. The key of the proof is Lemma E.2, which gives the dual representation of the convolution norm,\n\n∥P ∥u,κ = sup\n\n⟨f, P ⟩,\n\nf ∈B(X ) ∥f ∥H≤u ∥f ∥∞≤1\n\nwhere H is the RKHS generated by κ. Then, the density of the universal RKHS in C (X ) implies that limu→∞ ∥P ∥u,κ = sup∥f ∥∞≤1⟨f, P ⟩ = ∥P ∥TV, which proves the first statement. The second statement follows from the uniform law of large number, namely Theorem H.5 and Lemma H.6.\n\nSlightly extending it for the sample approximation of the DBR Rπ(w) ≈ ˆRπ(w), we obtain the following approximation formula useful for the weight optimization. The proof is given in Section F. Corollary 7.5. For all w ∈ B(X ), we have\n\n∥ ˆRπ(w)∥u,κ → ∥Rπ(w)∥TV\n\n(20)\n\nin probability as u → ∞ and n/u2 → ∞.\n\nIn the practical implementation, one may change the variable in the LHS of (20) from the measure Q ∈ M (X ) to the function g ∈ B(X ),\n\n∥ ˆRπ(w)∥u,κ =\n\n(cid:110)\n\nu MMDκ( ˆRπ(w) − Q) + ∥Q∥TV\n\n(cid:111)\n\nu MMDκ( ˆRπ(w) − g ⊙ ˆηπ) + ⟨|g|, ˆηπ⟩\n\n,\n\n(21)\n\n(cid:111)\n\ninf Q≪ ˆRπ(w) (cid:110)\n\n= min\n\ng∈B(X )\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 7.1: Minimax Optimal Interval Estimation (MOI)\n\nInput: Dataset D, universal approximator F, hypothesis class W, kernel κ; Output: Estimate of the optimal interval I⋆(π);\n\n1 ˆw := OptimizeDBR(D, F, W, κ) ; 2 ˆε := EvaluateDBR(D, F, ˆw) ; 3 return [ ˆJ( ˆw) − ˆε, J( ˆw) + ˆε];\n\n// Algorithm A.2 // Algorithm A.1\n\nˆβ + ˆβ)/3. Here, we have exploited the transitivity of the absolute continuity where ˆηπ := (ˆιπ + ˆTπ with Q ≪ ˆRπ(w) ≪ ˆηπ. Note that the minimization with respect to g is tractable since g is only evaluated on the support of ˆηπ, which is a finite set. Moreover, the convolution-based formula (21) is stable with the minimization with respect to w ∈ W resulting in the joint minimization problem of (w, g), unlike the regression-based formula (15) resulting in the saddle-point problem.\n\nSince the objective of (21) is lower bounded and convex with respect to (w, g), the minimizer\n\nˆwu := argmin\n\nw∈W\n\n∥ ˆRπ(w)∥u,κ\n\n(22)\n\nis computed with any convex optimization algorithms. The hyperparameter u is then chosen from a predefined grid U so that the TV norm of DBR is minimized. Specifically, one may choose it as (cid:112)min{n, m}⌋ the logarithmically-even grid U := {2k}kmax is determined according to the order of empirical approximation error (19). The entire procedure of the weight estimation is summarized in Algorithm A.2. By its derivation, we can formally show the consistency of Algorithm A.2. The proof is relegated to Section G. Theorem 7.6. Let OptimizeDBR(D, F, W, κ) be the output of Algorithm A.2. OptimizeDBR(D, F, W, κ) → w⋆ in probability as n → ∞.\n\nk=0 , where the upper limit kmax := ⌊log2\n\nThen,\n\nFinally, we present our OPI method in Algorithm 7.1, called the minimax optimal interval estimation (MOI), which is straightforwardly derived from Algorithm A.2 and the equation (13). We can also guarantee the validity and the approximate consistency of MOI in the asymptotic sense. The proof follows directly from Theorem 7.6. Theorem 7.7. Let MOI(D, F, W, κ) be the output of Algorithm 7.1. Then, MOI(D, F, W, κ) is asymptotically valid and 2 1+γ 1−γ εW -approximately optimal in probability.\n\n8 CONCLUSION\n\nIn this paper, we have studied OPI without the sufficient exploration and the realizability conditions. In particular, we have pointed out the existence of the irreducible bias in such a general setting, and correspondingly, introduced a novel formulation of the interval-based OPE. We have then revealed the connection between the conventional MIS estimator and the irreducible bias, which is eventually utilized to construct the proposed method, the minimax optimal interval estimator (MOI), and to prove its optimality.\n\nOne of the major limitations of the proposed method is its model agnosticity, lying at the opposite end to the model-based approach, e.g., Yu et al. (2020), that depends on the full correctness of the model. It is left for future work to extend and combine these methods to be applicable to partially correct models.\n\nREFERENCES\n\nBoucheron, S., Lugosi, G., and Bousquet, O. (2003). Concentration inequalities. In Summer school\n\non machine learning, pages 208–240. Springer.\n\nFeng, Y., Ren, T., Tang, Z., and Liu, Q. (2020). Accountable off-policy evaluation with kernel bellman statistics. In International Conference on Machine Learning, pages 3102–3111. PMLR.\n\nFeng, Y., Tang, Z., Zhang, N., and Liu, Q. (2021). Non-asymptotic confidence intervals of off-policy\n\nevaluation: Primal and dual bounds. arXiv preprint arXiv:2103.05741.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nHao, B., Ji, X., Duan, Y., Lu, H., Szepesvari, C., and Wang, M. (2021). Bootstrapping fitted qIn International Conference on Machine Learning, pages\n\nevaluation for off-policy inference. 4074–4084. PMLR.\n\nHornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal\n\napproximators. Neural networks, 2(5):359–366.\n\nJiang, N. and Huang, J. (2020). Minimax value interval for off-policy evaluation and policy opti-\n\nmization. Advances in Neural Information Processing Systems, 33:2747–2758.\n\nKallus, N. and Uehara, M. (2020). Double reinforcement learning for efficient off-policy evaluation\n\nin markov decision processes. Journal of Machine Learning Research, 21(167).\n\nLiu, Q., Li, L., Tang, Z., and Zhou, D. (2018). Breaking the curse of horizon: Infinite-horizon\n\noff-policy estimation. Advances in Neural Information Processing Systems, 31.\n\nShi, C., Wan, R., Chernozhukov, V., and Song, R. (2021). Deeply-debiased off-policy interval\n\nestimation. In International Conference on Machine Learning, pages 9580–9591. PMLR.\n\nSriperumbudur, B., Fukumizu, K., and Lanckriet, G. (2010). On the relation between universality, characteristic kernels and rkhs embedding of measures. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 773–780. JMLR Workshop and Conference Proceedings.\n\nSriperumbudur, B. K., Fukumizu, K., Gretton, A., Sch ̈olkopf, B., and Lanckriet, G. R. (2009). arXiv preprint\n\nOn integral probability metrics,\\phi-divergences and binary classification. arXiv:0901.2698.\n\nSriperumbudur, B. K., Fukumizu, K., Gretton, A., Sch ̈olkopf, B., and Lanckriet, G. R. (2012). On the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6:1550– 1599.\n\nWang, R., Foster, D. P., and Kakade, S. M. (2020). What are the statistical limits of offline rl with\n\nlinear function approximation? arXiv preprint arXiv:2010.11895.\n\nXie, T., Ma, Y., and Wang, Y.-X. (2019). Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling. Advances in Neural Information Processing Systems, 32.\n\nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. (2020). Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142.\n\nZanette, A. (2021). Exponential lower bounds for batch reinforcement learning: Batch rl can be In International Conference on Machine Learning, pages\n\nexponentially harder than online rl. 12287–12297. PMLR.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nCONTENTS\n\n1 Introduction\n\n2 Related Work\n\n3 Preliminaries\n\n4 Irreducible Bias in Offline Policy Evaluation\n\n5 Problem Setup: Minimax-Bias OPI\n\n6 Theoretical Foundation of Minimax-Bias OPI\n\n7 Estimation of Optimal Interval\n\n7.1 Evaluating the objective .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n7.2 Minimizing the objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n8 Conclusion\n\nA Algorithms\n\nB Proof of Theorem 4.1\n\nC Proof of Proposition 7.1\n\nD Details and Proof of Theorem 7.3\n\nD.1 Choice of Function Approximator and Penalty Function . . . . . . . . . . . . . . .\n\nD.2 Consistency Analysis .\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nE Dual Representation of Convolution Norm (Proof of Proposition 7.4)\n\nF Proof of Corollary 7.5\n\nG Proof of Theorem 7.6\n\nH Basic Definitions and Results\n\nH.1 Signed Measures\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nH.2 Rademacher Complexity and Uniform Law of Large Number . . . . . . . . . . . .\n\nH.3 Reproducing Kernel Hilbert Space . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nA ALGORITHMS\n\nB PROOF OF THEOREM 4.1\n\nWe first show the first two inequality.\n\n11\n\n1\n\n2\n\n3\n\n4\n\n6\n\n6\n\n7\n\n7\n\n8\n\n9\n\n11\n\n11\n\n14\n\n15\n\n15\n\n15\n\n19\n\n21\n\n22\n\n22\n\n22\n\n24\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm A.1: Evaluation of DBR\n\nInput: Dataset D, function approximator F, density-ratio estimate w; Output: Estimate of ∥Rπ(w)∥TV;\n\nˆL(f ), where ˆL(f ) is given by (16) ;\n\n// See also Section D\n\n1 ˆf ← argminf ∈F 2 ˆb ← ⟨ ˆf (cid:75) (cid:74) 3 return ˆb;\n\n, ˆRπ(w)⟩;\n\nAlgorithm A.2: Optimization of DBR\n\nInput: Dataset D, universal approximator F, hypothesis class W, kernel κ; Output: Approximate minimizer of ∥Rπ(w)∥TV;\n\n1 for u ∈ U do\n\n2\n\n3\n\nˆwu := argminw∈W ∥ ˆRπ(w)∥u,κ; ˆεu := EvaluateDBR(D, F, ˆwu) ;\n\n4 ˆu := argminu∈U ˆεu; 5 return ˆwˆu;\n\nLemma B.1. For all w ∈ B(X ),\n\n// Algorithm A.1\n\nε⋆(π) ≤ ε\n\n(cid:104) ˆJ(w)\n\n(cid:105)\n\n≤ ∥Rπ(w)∥TV\n\n(23)\n\nProof. The first inequality is trivial. To show the second one, observe\n\n(cid:12) (cid:12)\n\n(cid:12)JM(π) − E ˆJ(w)\n\n(cid:12) (cid:12) (cid:12) = |⟨ρ, Γπιπ − w ⊙ β⟩|\n\n= |⟨qπ, ιπ − ∆π(w ⊙ β)⟩| = |⟨qπ, Rπ(w)⟩| ≤ ∥Rπ(w)∥TV,\n\nwhere the last inequality is owing to ∥qπ∥∞ ≤ 1. Since the RHS is independent of M given GM,β, we thus have\n\n(cid:104) ˆJ(w)\n\n(cid:105)\n\nε\n\n=\n\nsup M′:GM′ ,β =GM,β\n\n(cid:12) (cid:12)\n\n(cid:12)JM′(π) − E ˆJ(w)\n\n(cid:12) (cid:12) (cid:12) ≤ ∥Rπ(w)∥TV.\n\n(24)\n\nNow, to prove the last inequality, we prepare two extreme, yet indistinguishable environments M± := (ι, ̃T , R±). Let ̃T be an arbitrary state-transition operator indistinguishable from T , which will be determined later. Also let ̃Tπ be the state-action transition operator associated with ̃T and π such that d ̃Tπ(s, a|x) = d ̃T (s|x)dπ(a|s) for x, (s, a) ∈ X . Let ̃μπ := (1 − γ)(I − γ ̃Tπ)−1ιπ be the common occupancy measure of M± induced with ̃T , and ̃μπ|̸≪β be the singular component of ̃μπ with respect to β. Let X0 be a set separating ̃μπ|̸≪β from β and Xβ := X \\ X0 be its complement.3 For convenience, let Πβ, Π0 : M (X ) → M (X ) denote the projections of measure onto Xβ and X0, respectively, given by Πβ := χβ ⊙ P , and Π0P := (1 − χβ) ⊙ P where χβ being the indicator function of Xβ such that χβ(x) = 1 if x ∈ Xβ and χ0(x) = 0 otherwise. Note that ̃μπ|̸≪β = Π0 ̃μπ by construction.\n\nFinally, put R±(x) = δ±1 for x ∈ X0 and R±(x) = R(x) otherwise, which is necessary for the indistinguishability of R±, and denote the associated expected reward by ρ±(x) := (cid:82) rdR±(r|x), x ∈ X . Then, we have\n\nJ+(π) − J−(π) = ⟨ρ+ − ρ−, ̃μπ⟩ = 2∥ ̃μπ|̸≪β∥TV,\n\n(25)\n\n3That is, {X0, Xβ} is a partition of X such that ̃μπ|̸≪β(E) = 0 for all measurable E ⊂ Xβ and β(E) = 0\n\nfor all measurable E ⊂ X0.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nwhere J±(π) are the policy values with respect to M±.\n\nNow, the following lemma connects the RHS of (25) with DBR. Lemma B.2. There exist ̃T indistinguishable from T such that\n\n∥ ̃μπ|̸≪β∥TV = ∥Rπ(w♯\n\nπ)∥TV.\n\n(26)\n\nProof. The proof is constructive. Consider an expanded state space S ← S ∪ {⊥}, where ⊥ denotes an absorbing state of ̃T . Accordingly, let X0 ← X0 ∪ ({⊥} × A). Now, put ̃T := ̃T |≪β + ̃T |̸≪β such that ̃T |≪β is the restriction of T onto Xβ and ̃T |̸≪β is the absorbing transition, respectively given by\n\n ̃T |≪β = T Πβ,\n\n ̃T |̸≪βP = P (X0) δ⊥,\n\nP ∈ M (X ).\n\nHere, ̃T |≪β is corresponding to the known component of ̃T , which necessarily coincides with that of T by definition, and ̃T |̸≪β is corresponding to the unknown component of ̃T . Note that ̃T is a proper transition operator indistinguishable from T .\n\nLet ̃Tπ|≪β and ̃Tπ|̸≪β be the state-action transition operator associated with Tβ and T0, respectively, such that d ̃Tπ|≪β(s, a|x) := d ̃T |≪β(s|x)dπ(a|s) and d ̃Tπ|̸≪β(s, a|x) := d ̃T |̸≪β(s|x)dπ(a|s) for x, (s, a) ∈ X . Then, we have\n\n(1 − γ)ιπ = (I − γ ̃Tπ) ̃μπ\n\n= (I − γ ̃Tπ|≪β − γ ̃Tπ|̸≪β) ̃μπ = (I − γ ̃Tπ|≪β) ̃μπ − γ ̃μπ(X0)δ⊥,π = (I − γ ̃Tπ|≪β) ̃μπ − γ∥ ̃μπ|̸≪β∥TVδ⊥,π,\n\nwhich implies, with ̃P := (I − γ ̃Tπ|≪β)−1ιπ,\n\n ̃μπ = (1 − γ) ̃P + γ∥ ̃μπ|̸≪β∥TV(I − γ ̃Tπ|≪β)−1δ⊥,π\n\n= (1 − γ) ̃P + γ∥ ̃μπ|̸≪β∥TVδ⊥,π.\n\n(∵ ̃Tπ|≪βδ⊥,π = 0)\n\nMeasuring the volumes on X0, we further get\n\n∥ ̃μπ|̸≪β∥TV = (1 − γ) ̃P (X0) + γ∥ ̃μπ|̸≪β∥TV,\n\nwhich yields\n\n ̃P (X0) = ∥ ̃μπ|̸≪β∥TV.\n\n(27)\n\nOn the other hand, since w♯\n\n∥Rπ(w♯\n\nπ)∥TV =\n\n=\n\n=\n\n(cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\nπ = (1 − γ)d(Πβ ̃P )/dβ, we have (cid:13) (cid:13)ιπ − (1 − γ)∆πΠβ ̃P (cid:13) (cid:13)TV (cid:13) (cid:13)ιπ − (Πβ − γ ̃Tπ|≪β) ̃P (cid:13) (cid:13)TV (cid:13) (cid:13)ιπ − Πβιπ + γΠ0Tπ|≪β ̃P (cid:13) (cid:13)TV ιπ + γTπ|≪β ̃P (cid:13) (cid:13) (cid:13)TV\n\n(cid:17)(cid:13) (cid:13) (cid:13)TV\n\n(cid:13) (cid:13)\n\n(cid:16)\n\n(cid:16)\n\n=\n\n(cid:13) (cid:13) (cid:13)Π0 (cid:13) (cid:13)Π0 ̃P (cid:13) = ̃P (X0).\n\n=\n\n(cid:16)\n\n∵ TπΠβ = ̃Tπ|≪β\n\n∵ ̃Tπ|≪β = Πβ ̃Tπ|≪β + Π0 ̃Tπ|≪β\n\n(cid:16)\n\n∵ (I − γ ̃Tπ|≪β) ̃P = ιπ\n\n(cid:17)\n\n(cid:17)\n\n(cid:17)\n\nCombining it with (27), we get the desired result.\n\nPlugging (26) into (25), we have\n\n∥Rπ(w♯\n\nπ)∥TV =\n\n1 2\n\n{J+(π) − J−(π)}\n\n(28)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nwith a specific configuration of ̃T . Since M± are indistinguishable from one another, any estimators must incur the bias of at least a half of the difference J+(π) − J−(π) in the worst case, i.e., Rπ(w♯\n\nπ) ≤ ε⋆(π). This proves the last inequality of (8) via the triangle inequality,\n\n∥Rπ(w)∥TV ≤ ∥Rπ(w♯\n\nπ)∥TV + ∥∆π(w − w♯\n\nπ) ⊙ β∥TV\n\n≤ ε⋆(π) +\n\n1 + γ 1 − γ\n\n∥w − w♯\n\nπ∥L1(β).\n\nand thus concludes the proof of Proposition 4.1.\n\nC PROOF OF PROPOSITION 7.1\n\nFirst, we introduce a saddle-point formulation of the TV norm. Let F1 := {f ∈ F : ∥f ∥∞ ≤ 1} be the intersection of F with the unit ball of B(X ). Lemma C.1 gives a general variational formula of the TV norm. Lemma C.1. For all P ∈ M (X ), we have\n\n∥P ∥TV = sup f ∈F1\n\n⟨f, P ⟩.\n\n(29)\n\nProof. Let us denote the unit ball of B(X ) with U1 := {f ∈ B(X ) : ∥f ∥∞ ≤ 1}. As an instance of the integral probability metrics (IPM), the TV norm is known to be written as\n\n∥P ∥TV = sup g∈U1\n\n⟨g, P ⟩.\n\n(30)\n\nNow fix g ∈ U1 such that |∥P ∥TV − ⟨g, P ⟩| ≤ c for a positive constant c > 0. Since F1 is dense in C (X ) ∩ U1, it is also dense in L1(P ) ∩ U1 and therefore there exists f ∈ F1 such that ∥f − g∥L1(P ) ≤ c. Then it follows that\n\n|∥P ∥TV − ⟨f, P ⟩| ≤ |∥P ∥TV − ⟨g, P ⟩| + ∥f − g∥L1(P ) ≤ 2c.\n\nSince c > 0 can be arbitrarily small, we finally have\n\n∥P ∥TV ≤ sup f ∈F1\n\n⟨f, P ⟩ ≤ sup g∈U1\n\n⟨g, P ⟩ ≤ ∥P ∥TV,\n\n(31)\n\n(32)\n\nwhich proves the desired result.\n\nSince the supremum in (29) is taken over a constrained domain F1, its computation is not necessarily tractable in general. The following lemma is useful to make the domain unconstrained. Lemma C.2. Let Ψ : B(X ) → R≥0 be a penalty function such that Ψ(f ) = 0 if f ∈ F1. Then, for all P ∈ M (X ), we have\n\nsup f ∈F1\n\n⟨f, P ⟩ = sup f ∈F\n\n{⟨\n\nf\n\n(cid:74)\n\n, P ⟩ − Ψ(f )} ,\n\n(cid:75)\n\n(33)\n\nwhere\n\nf\n\n(cid:74)\n\n(cid:75)\n\n(x) := max{−1, min{1, f (x)}} denotes the clipping of f (x) to [−1, 1].\n\nProof. Slightly extending (32), we have\n\n∥P ∥TV ≤ sup f ∈F1\n\n⟨f, P ⟩\n\n= sup f ∈F1\n\n{⟨f, P ⟩ − Ψ(f )}\n\n{⟨\n\nf\n\n, P ⟩ − Ψ(f )}\n\n(cid:75)\n\n(cid:74) ⟨g, P ⟩\n\n≤ sup f ∈F\n\n≤ sup g∈U1 ≤ ∥P ∥TV.\n\n(∵ domain expansion)\n\n(∵\n\nf\n\n(cid:74)\n\n(cid:75)\n\n∈ U1, Ψ(f ) ≥ 0)\n\nThis yields the desired claim.\n\nFinally, Proposition 7.1 is proved taking the penalty function as the trivial one, Ψ(f ) = 0 for all f ∈ F. We utilize a nontrivial penalty function in Section D.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nD DETAILS AND PROOF OF THEOREM 7.3\n\nWe first present a preferred choice of the function approximator F and the penalty function Ψ, which is needed to construct the objective function ˆL(f ) in (16). Then, we prove Theorem 7.3 to show the consistency of the resultant algorithm (Algorithm A.1).\n\nD.1 CHOICE OF FUNCTION APPROXIMATOR AND PENALTY FUNCTION\n\nAs for the function approximator F, we choose a universal RKHS. Let κ : X 2 → R be the corresponding symmetric positive-definite kernel. We assume κ is c0-universal in the sense of Sriperumbudur et al. (2010). Also, without loss of generality, we assume κ is normalized, ∥κ∥∞ := supx,x′∈X |κ(x, x′)| ≤ 1. For instance, the Gaussian kernel κ(x, y) = exp{−∥x − y∥2 2/(2α2)}, x, y ∈ Rd, d ≥ 1, α > 0, is one of such choices.\n\nAs for the penalty function Ψ, we employ\n\nˆΨλ(f ) := ⟨(|f | − 1)+, ˆRπ,+(w) + ˆRπ,−(w)⟩ +\n\nλ 2(1 − γ)\n\n∥f ∥2\n\nF ,\n\n(34)\n\nwhere λ > 0 is a hyperparameter, (g)+ := max{0, g} denotes the positive part of a function 1−γ (w ⊙ ˆβ) are the positive and g ∈ B(X ), ˆRπ,+(w) := ˆιπ − γ the negative part of the empirical DBR, respectively, and ∥ · ∥F is the RKHS norm. Then, letting C(P ) := 1 + 1+γ 1−γ ⟨w, P ⟩, P ∈ M (X ), the penalized objective (16) is simplified as\n\nˆTπ(w ⊙ ˆβ) and ˆRπ,−(w) := 1\n\n1−γ\n\nˆL(f ) = ⟨|f − 1|, ˆRπ,+(w)⟩ + ⟨|f + 1|, ˆRπ,−(w)⟩ +\n\nλ 2(1 − γ)\n\n∥f ∥2\n\nF − C( ˆβ),\n\n(35)\n\nwhich is convex with respect to f ∈ F. In other words, the minimizer\n\nˆf ≡ ˆfλ := argmin\n\nf ∈F\n\nˆL(f )\n\n(36)\n\ncan be found in a tractable manner with convex optimization methods. As for the choice of λ, as will be seen in the next section, we can achieve the consistency if λ → 0 and nλ → ∞. Thus, we may n or select the best hyperparameter within some fixed grid, e.g., employ some fixed default λ = 1/ Λn := {1, 2, ..., 2⌊log2 n⌋}, that best attains the supremum (15), possibly using the training-validation split technique.\n\n√\n\nD.2 CONSISTENCY ANALYSIS\n\nWe first introduce some notations useful for the analysis. Let us define probability measures P, ˆPn ∈ M (X 3) by\n\ndP (x1, x2, x3) := dιπ(x1)dβ(x2)dTπ(x3|x2), d ˆPn(x1, x2, x3) := dˆιπ(x1)d ˆβ(x2)d ˆTπ(x3|x2),\n\nloss functions lf , φf : X 3 → R, f ∈ F, by\n\nlf (x1, x2, x3) :=\n\n3 (cid:88)\n\nj=1\n\nfor x1, x2, x3 ∈ X , where\n\nlf,j(x1, x2, x3),\n\nφf (x1, x2, x3) :=\n\n3 (cid:88)\n\nj=1\n\nφf,j(x1, x2, x3),\n\nlf,1(x1, x2, x3) := (1 − γ)|f (x1) − 1|, lf,2(x1, x2, x3) := |w(x2)f (x2) + |w(x2)||, lf,3(x1, x2, x3) := γ|w(x2)f (x3) − |w(x2)||,\n\nLet us also define the associated risk functions\n\nLλ(f ; Q) := ⟨lf , Q⟩ +\n\nλ 2\n\n∥f ∥2\n\nF ,\n\n15\n\nφf,1(x1, x2, x3) := −(1 − γ) (cid:74) φf,2(x1, x2, x3) := w(x2) f (x2) (cid:74) φf,3(x1, x2, x3) := −γw(x2) (cid:74)\n\nf (x1) ,\n(cid:75) f (x3)\n\n,\n\n(cid:75)\n\n.\n\n(cid:75)\n\nΦ(f ; Q) := ⟨φf , Q⟩,\n\nUnder review as a conference paper at ICLR 2023\n\nfor probability measures Q ∈ M (X 3). By these definitions, we have\n\nand\n\nΦ(f ; P ) = (1 − γ)⟨ (cid:74) Φ(f ; ˆPn) = (1 − γ)⟨ (cid:74)\n\nf\n\nf\n\n(cid:75)\n\n(cid:75)\n\n, Rπ(w)⟩, , ˆRπ(w)⟩,\n\nLλ(f ; P ) = Φ(f ; P ) + (1 − γ) {Ψλ(f ) + C(β)} (cid:110) ˆΨλ(f ) + C( ˆβ) Lλ(f ; ˆPn) = Φ(f ; ˆPn) + (1 − γ)\n\n(cid:111)\n\nfor all λ > 0 and f ∈ F, where ˆΦλ is given by (34) and\n\nΨλ(f ) := ⟨(|f | − 1)+, Rπ,+(w) + Rπ,−(w)⟩ +\n\nλ 2(1 − γ)\n\n∥f ∥2\n\nF ,\n\n(37)\n\n(38)\n\n(39)\n\nRπ,+(w) := ιπ + γ parts of the DBR, respectively.\n\n1−γ Tπ(w ⊙ β) and Rπ,−(w) := 1\n\n1−γ (w ⊙ β) are the positive and the negative\n\nTherefore, we obtain an alternative expression of the objective (16)\n\nwhich implies\n\nˆL(f ) =\n\n1 1 − γ\n\nLλ(f ; ˆPn) − C( ˆβ),\n\nˆfλ = argmin\n\nf ∈F\n\nLλ(f ; ˆPn).\n\nMoreover, by Lemma C.2, we also obtain alternative expressions of the quantity of interest\n\n∥Rπ(w)∥TV = −\n\n1 1 − γ\n\n= C(β) −\n\nΦ(f ; P )\n\ninf f ∈F 1\n1 − γ\n\ninf f ∈F\n\nL0(f ; P ).\n\n(40)\n\n(41)\n\n(42)\n\nThe goal of this section is to reveal the relationship of ˆfλ and ∥Rπ(w)∥TV via (40), (41) and (42) The following lemma gives a key insight on the behavior of ˆfλ. Let G := 1 − γ + (1 + γ)∥w∥∞ be the Lipschitz constant of f (cid:55)→ lf and f (cid:55)→ φf . Let BF (0, 1) := {f ∈ F : ∥f ∥F ≤ 1} be the unit closed ball of F. Let Rn(H) is the Rademacher complexity of a function class H ⊂ B(X ) (see Definition H.4). Lemma D.1. Suppose the predictor attaining minf ∈F Lλ(f ; P ) exists and denote it by f ∗ Also suppose ∥f ∥∞ ≤ ∥f ∥F for all f ∈ F. Then, for all δ ∈ (0, 1), we have\n\nλ ∈ F.\n\nLλ( ˆfλ; P ) ≤ Lλ(f ∗\n\nλ; P ) +\n\n8G2 λ\n\n(cid:40)\n\nRn(BF (0, 1)) +\n\n(cid:41)2\n\n(cid:114)\n\nln(1/δ) 2n\n\nand\n\nwith probability 1 − δ.\n\nProof. Define\n\n∥ ˆfλ − f ∗\n\nλ∥F ≤\n\n4G λ\n\n(cid:40)\n\nRn(BF (0, 1)) +\n\n(cid:114)\n\n(cid:41)\n\nln(1/δ) 2n\n\nε(f ; Q) := Lλ(f ; Q) − Lλ(f ∗\n\nλ; Q)\n\nfor f ∈ F and Q ∈ M (X ). Let Fc := {f ∈ F : ε(f ; P ) ≤ c} and let ̃lg,j := lf ∗ for j = 1, 2, 3. Then, since ε(f ; P ) ≥ λ\n\nλ ,j ∞ by the strong convexity of\n\nλ +g,j − lf ∗\n\nF ≥ λ\n\n2 ∥f − f ∗\n\n2 ∥f − f ∗\n\nλ∥2\n\nλ∥2\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nf (cid:55)→ Lλ(f ; P ), the uniform law of large number (Theorem H.5) gives\n\n(cid:110)\n\nsup f ∈Fc\n\nε(f ; P ) − ε(f ; ˆPn)\n\n(cid:111)\n\n= sup f ∈Fc\n\n3 (cid:88)\n\nj=1\n\n⟨ ̃lf −f ∗\n\nλ ,j, P − ˆPn⟩\n\n≤\n\nsup √\n\n∥g∥F ≤\n\n3 (cid:88)\n\n⟨ ̃lg,j, P − ˆPn⟩\n\nj=1\n\n\n\n\n\n2c/λ \n\n\n3 (cid:88)\n\nj=1\n\n (cid:40)\n\n ̃lg,j : ∥g∥F ≤\n\n(cid:114) 2c λ\n\n \n\n\n\n\n\n + 2G\n\n(cid:114) 2c λ\n\nG\n\nRn(BF (0, 1)) +\n\n(cid:114)\n\n(cid:41)\n\nln(1/δ) 2n\n\n=: ̄ε(c, δ)\n\n(cid:114)\n\nln(1/δ) 2n\n\n(∵ strong convexity)\n\n(∵ Theorem H.5)\n\n(∵ see below)\n\nwith probability 1 − δ for all c > 0. Here, the last inequality follows from \n\n\n\n(cid:32)(cid:40)\n\nRn\n\n\n\n ̃lg,j : ∥g∥F ≤\n\nRn\n\n ̃lg,j : ∥g∥F ≤\n\n(cid:114) 2c λ\n\n \n\n\n\n(cid:41)(cid:33)\n\n(∵ subadditivity)\n\n(cid:114) 2c λ\n\n≤ 2Rn\n\n(cid:114) 8c λ\n\n≤\n\n \n\n3 (cid:88)\n\n\n\nj=1\n\n ≤\n\n≤\n\n≤\n\n3 (cid:88)\n\nj=1\n\n3 (cid:88)\n\nGj\n\nj=1\n\n(cid:114) 2c λ\n\n(cid:114) 2c λ\n\nRn(BF (0, 1))\n\n(∵ Lemma H.4)\n\nGRn(BF (0, 1)),\n\nwhere Gj is the Lipschitz constants of lf,j, j = 1, 2, 3. Now take ̃fc ∈ F such that ̃fc = ˆfλ if ε( ˆfλ; P ) ≤ c and, otherwise, ε( ̃fc; ˆPn) ≤ 0 and ε( ̃fc; P ) = c.4 Then, when c > ̄ε(c, δ), we get with probability 1 − δ\n\nε( ̃fc; P ) = ε( ̃fc; ˆPn) + ε( ̃fc; P ) − ε( ̃fc; ˆPn) (cid:111) (cid:110)\n\nε(f ; P ) − ε(f ; ˆPn)\n\n≤ sup\n\n(∵ ε( ̃fc; ˆPn) ≤ 0, ε( ̃fc; P ) ≤ c)\n\nf :ε(f ;P )≤c\n\n< c,\n\nwhich implies ̃fc = ˆfλ and hence ε( ˆfλ; P ) < c with the same probability. Thus, since it holds with any c > 0 such that c > ̄ε(c, δ), we finally have\n\nwith probability 1 − δ, where c∗ is the solution to c∗ = ̄ε(c∗, δ), or more concretely\n\nε( ˆfλ; P ) ≤ c∗\n\nc∗ =\n\n(cid:40)\n\n8G2 λ\n\nRn(BF (0, 1)) +\n\n(cid:114)\n\n(cid:41)2\n\n.\n\nln(1/δ) 2n\n\nThis concludes the proof.\n\nNow, verifying the assumptions of Lemma D.1 and evaluating the Rademacher complexity of the unit ball Rn(BF (0, 1)), we get the following proposition. Proposition D.2 (Generalization error bound of RERM with RKHS). For all δ ∈ (0, 1) and λ > 0, we have\n\nL0( ˆfλ; P ) ≤ inf\n\nf ∈H\n\nLλ(f ; P ) +\n\n8G2 ln(e2/δ) λn\n\n4Such an ̃fc exists at the intersection of the line segment [f ∗\n\nλ, ˆfλ] and the level set {f ∈ F : ε(f ; P ) ≤ c}\n\nsince f (cid:55)→ ε(f ; Q) is convex.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nwith probability 1 − δ.\n\n∥ ˆfλ − f ∗\n\nλ∥F ≤\n\n(cid:114)\n\n4G λ\n\nln(e2/δ) 2n\n\n(43)\n\nProof. It suffices to invoke Lemma D.1 with Lemma H.6. To this end, we need to verify the existence of minf ∈H Lλ(f ; P ) and the dominance of the norm ∥ · ∥∞ ≤ ∥ · ∥H. In fact, the minimum exists since f (cid:55)→ Lλ(f ; P ) is continuous with respect to L2(P ) and the inifimum inf f ∈H Lλ(f ; P ) does not change if we restrict the domain to the ball {f ∈ H : ∥f ∥H ≤ (cid:112)2Lλ(0; P )}, which is compact according to Lemma H.7. The dominance of the norm is shown by, for all f ∈ H,\n\n|f (x)|\n\n|⟨κ(·, x), f ⟩H|\n\n∥κ(·, x)∥H∥f ∥H\n\nκ(x, x)∥f ∥H\n\n∥f ∥∞ = sup x∈X = sup x∈X ≤ sup x∈X = sup x∈X ≤ ∥f ∥H.\n\nWe need one more lemma to connect Φ( ˆfλ; ˆPn) with Φ( ˆfλ; P ). Lemma D.3. For all δ ∈ (0, 1), we have, in addition to the statements of Proposition D.2,\n\n(cid:12) (cid:12)\n\n(cid:12)Φ( ˆfλ; ˆPn) − Φ( ˆfλ; P )\n\n(cid:12) (cid:12) (cid:12) ≤\n\n(cid:114)\n\n8G2 λn\n\nln(e2/δ) 2\n\n(cid:114)\n\n+ 2G\n\nln(2/δ) 2n\n\nwith probability 1 − 2δ.\n\nProof. It follows from the uniform law of large number (Theorem H.5) with the high probability (cid:113) ln(e2/δ) range of ˆfλ given by (43). Let G := {φf : f ∈ F, ∥f − f ∗ Now, applying Theorem H.5 with δ ← δ/2, F ← ±G and D ← 2G, we get\n\nλ∥F ≤ d}, where d := 4G\n\n2n\n\nλ\n\n.\n\n(cid:12) (cid:12)\n\n(cid:12)⟨φf , ˆPn − P ⟩\n\n(cid:12) (cid:12) (cid:12) ≤ 2Rn(G) + 2G\n\n(cid:114)\n\nln(2/δ) 2n\n\nsup\n\n∥f −f ∗\n\nλ ∥F ≤d\n\nwith probability 1 − δ. Since ∥ ˆfλ − f ∗\n\nλ∥F ≤ d with probability 1 − δ, we further get by union bound\n\n(cid:12) (cid:12) (cid:12)⟨φ ˆfλ\n\n, ˆPn − P ⟩\n\n(cid:12) (cid:12) (cid:12) ≤ 2Rn(G ∪ −G) + G\n\n(cid:114)\n\nln(2/δ) 2n\n\n.\n\nwith probability 1 − 2δ. Since Φ( ˆfλ; Q) := ⟨φ ˆfλ by\n\n, Q⟩ for all Q ∈ M (X 3) the proof is concluded\n\nRn(G) = Rn({φf : f ∈ F, ∥f − f ∗\n\nλ∥F ≤ d})\n\n≤\n\n3 (cid:88)\n\nj=1\n\nRn({φf,j : f ∈ F, ∥f − f ∗\n\nλ∥F ≤ d})\n\n≤ GdRn(BF (0, 1))\n\n≤\n\nGd √\nn\n\n.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFinally, we are ready to prove Theorem 7.3. Observe\n\n(1 − γ)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n−\n\n1 1 − γ\n\nΦ( ˆfλ; ˆPn) − ∥Rπ(w)∥TV\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:26)\n\n≤\n\nΦ( ˆfλ; P ) − inf\n\nf ∈F\n\n(cid:27)\n\nΦ(f ; P )\n\n+ |Φ( ˆfλ; ˆPn) − Φ( ˆfλ; P )|\n\n(∵ (41))\n\nDue to Proposition D.2 and Lemma D.3, we have the following inequalities bounding both terms of the RHS with probability 1 − 2δ: The first term is bounded by\n\nΦ( ˆfλ; P ) − inf\n\nf ∈F\n\nΦ(f ; P )\n\n≤ L0( ˆfλ; P ) − inf\n\nf ∈F\n\nL0(f ; P )\n\n≤ inf f ∈F\n\nLλ(f ; P ) − inf f ∈F\n\nL0(f ; P ) +\n\n≤ s(d) +\n\nλ 2\n\nd2 +\n\n8G2 ln(e2/δ) λn\n\n8G2 ln(e2/δ) λn\n\n(∵ (37) with Ψ0(f ) ≥ 0, (42))\n\n(∵ Proposition D.2)\n\nfor all d > 0, where s(d) := inf ∥f ∥F ≤d L0(f ; P ) − inf f ∈F L0(f ; P ). The second term is bounded by\n\n|Φ( ˆfλ; ˆPn) − Φ( ˆfλ; P )| ≤\n\n(cid:114)\n\n8G2 λn\n\nln(e2/δ) 2\n\n(cid:114)\n\n+ 2G\n\nln(2/δ) 2n\n\n.\n\n(∵ Lemma D.3)\n\nCombining these two inequalities, we get\n\n(1 − γ)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n−\n\n1 1 − γ\n\nΦ( ˆfλ; ˆPn) − ∥Rπ(w)∥TV\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ s(d) +\n\nλd2 2\n\n+\n\n8G2 λn\n\n(cid:40)\n\nln(e2/δ) +\n\n(cid:114)\n\nln(e2/δ) 2\n\n(cid:41)\n\n(cid:114)\n\n+ 2G\n\nln(2/δ) 2n\n\nwith probability 1 − 2δ. Now, since limd→∞ s(d) = 0, taking d = λ−1/3, we have just shown the following proposition, which directly translates into Theorem 7.3.\n\nProposition D.4. We have\n\n−\n\n1 1 − γ\n\nΦ( ˆfλ; ˆPn) → ∥Rπ(w)∥TV\n\nin probability as λ → 0 and nλ → ∞.\n\nE DUAL REPRESENTATION OF CONVOLUTION NORM (PROOF OF\n\nPROPOSITION 7.4)\n\nWe first show the following utility lemma. Lemma E.1. Let ∥f ∥A and ∥f ∥B be arbitrary norms of f ∈ B(X ). Also let ∥f ∥A∨B := ∥f ∥A ∨ ∥f ∥B be the norm defined by the maximum of these. Then, for all P ∈ M (X ), we have\n\n∥P ∥(A∨B)∗ ≤ inf Q≪P\n\n{∥P − Q∥A∗ + ∥Q∥B∗ } ,\n\nwhere we denote the dual norm of ∥ · ∥X on B(X ) by ∥P ∥X ∗ := supf ∈B(X ),∥f ∥X ≤1⟨f, P ⟩, P ∈ M (X ).\n\nMoreover, if | supp(P )| is finite and ∥ · ∥A and ∥ · ∥B dominate ∥ · ∥∞, then the equality is attained.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Observe\n\n∥P ∥(A∨B)∗ = sup\n\nf ∈B(X ) ∥f ∥A≤1 ∥f ∥B ≤1\n\n⟨f, P ⟩\n\n{⟨f, P ⟩ + ⟨g − f, Q⟩}\n\n(∵ Q as a Lagrange multiplier)\n\ninf Q≪P\n\n= sup\n\nf,g∈B(X ) ∥f ∥A≤1 ∥g∥B ≤1\n\n= sup\n\ninf Q≪P\n\n{⟨f, P − Q⟩ + ⟨g, Q⟩}\n\nf,g∈B(X ) ∥f ∥A≤1 ∥g∥B ≤1 \n\n\n≤ inf Q≪P\n\nsup f ∈B(X ) ∥f ∥A≤1\n\n\n\n⟨f, P − Q⟩ + sup\n\n⟨g, Q⟩\n\ng∈B(X ) ∥g∥B ≤1\n\n \n\n\n\n= inf Q≪P\n\n{∥P − Q∥A∗ + ∥Q∥B∗ } ,\n\nwhich proves the inequality.\n\nNow suppose d := supp(P ) is finite and ∥ · ∥A and ∥ · ∥B dominate ∥ · ∥∞. Label each element of supp(P ) by {xj}d\n\nj=1. Then, following the same equality as above, we get\n\n∥P ∥(A∨B)∗ = sup a∈BA b∈BB\n\ninf Q≪P\n\n \n\nd (cid:88)\n\n\n\nj=1\n\naj{P (xj) − Q(xj)} +\n\nd (cid:88)\n\nj=1\n\nbjQ(xj)\n\n \n\n\n\n,\n\nwhere\n\nBA := {(f (xj))d BB := {(g(xj))d\n\nj=1 : f ∈ B(X ), ∥f ∥A ≤ 1} ⊂ Rd, j=1 : g ∈ B(X ), ∥g∥B ≤ 1} ⊂ Rd.\n\nThe domination of A- and B-norms over the uniform norm implies there exists c < ∞ such that BA, BB ⊂ c Ud where Ud := {z ∈ Rd : max1≤j≤d |zj| ≤ 1} denotes the unit hypercube of Rd. Since c Ud is compact and BA and BB are closed subsets thereof, they are also compact. Thus, Sion’s minimax theorem yields\n\n∥P ∥(A∨B)∗ = sup a∈BA b∈BB\n\ninf Q≪P\n\n \n\nd (cid:88)\n\n\n\nj=1\n\naj{P (xj) − Q(xj)} +\n\nd (cid:88)\n\nj=1\n\nbjQ(xj)\n\n \n\n\n\n \n\n\n\nsup a∈BA\n\naj{P (xj) − Q(xj)} + sup b∈BB\n\nd (cid:88)\n\nj=1\n\nbjQ(xj)\n\n \n\n\n\n{∥P − Q∥A∗ + ∥Q∥B∗ } .\n\n= inf Q≪P\n\n= inf Q≪P\n\nThis concludes the proof.\n\nFor P ∈ M (X ), define\n\n⟨f, P ⟩.\n\nFu(P ) := sup\n\nf ∈B(X ) ∥f ∥∞≤1 ∥f ∥H≤u\n\nThe following lemma shows that Fu(P ) is equal to the u-convolution norm. In other words, it gives the dual representation of the u-convolution norm in Proposition 7.4. Lemma E.2. For all P ∈ M (X ), we have\n\nwhere ∥P ∥u,κ is given with respect to Definition 7.1.\n\n∥P ∥u,κ = Fu(P ),\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Without loss of generality, we assume ∥P ∥TV = 1. Let ˆPn be the empirical distribution of P given by Definition H.3.\n\nLet H be the RKHS associated with the kernel κ. Let ∥f ∥A = ∥f ∥H and ∥f ∥B = ∥f ∥∞ in Lemma E.1 and observe that, for all P ∈ M (X ),\n\nFu(P ) = ∥P ∥(A∨B)∗ , ∥P ∥u,κ = inf Q≪P\n\n{∥P − Q∥A∗ + ∥Q∥B∗ } ,\n\nsince ∥P ∥H∗ = MMDκ(P ) and ∥P ∥TV = ∥P ∥∞∗ . Then, we have\n\n0 ≤ ∥P ∥u,κ − Fu(P ) ≤ EFu( ˆPn − P )\n\nsince\n\nFu(P ) ≤ ∥P ∥u,κ\n\n≤ E∥ ˆPn∥u,κ = EFu( ˆPn) ≤ Fu(P ) + EFu( ˆPn − P ).\n\nThe proof is concluded remembering that\n\n(∵ Lemma E.1) (∵ Jensen’s ineq with Lemma H.3) (∵ Lemma E.1 with | supp( ˆPn)| < ∞) (∵ triangle inequality of Fu(·))\n\n0 ≤ EFu( ˆPn − P )\n\n≤ 2Rn({f ∈ B(X ) : ∥f ∥H ≤ u, ∥f ∥∞ ≤ 1}) ≤ 2Rn({f ∈ B(X ) : ∥f ∥H ≤ u})\n\n(cid:114)\n\n≤ 2u\n\nsupx∈X κ(x, x) n\n\n→ 0\n\n(∵ Theorem H.5)\n\n(∵ Lemma H.6)\n\nas n → ∞, where Rn(F) is the maximal Rademacher complexity (Definition H.4)\n\nF PROOF OF COROLLARY 7.5\n\nLet ̄w := w/∥w∥∞ be the normalization of w. By Lemma E.2, we have\n\n∥ ̄w ⊙ ( ˆβ − β)∥u,κ = sup\n\n⟨ ̄wf, ˆβ − β⟩\n\nf ∈B(X ) ∥f ∥H≤u ∥f ∥∞≤1\n\n(∵ ∥ ̄wf ∥Hw = ∥f ∥H, ∥ ̄wf ∥∞ ≤ ∥f ∥∞)\n\n(∵ ̄wB(X ) ⊂ B(X ))\n\n≤\n\n⟨ ̄wf, ˆβ − β⟩\n\nsup f ∈B(X ) ∥ ̄wf ∥Hw ≤u ∥ ̄wf ∥∞≤1\n\n⟨g, ˆβ − β⟩\n\n≤ sup\n\ng∈B(X ) ∥g∥Hw ≤u ∥g∥∞≤1\n\n= ∥ ˆβ − β∥u,κw ,\n\nwhere Hw is the RKHS associated with κw(x, y) := ̄w(x) ̄w(y)κ(x, y). Similarly, we have ∥ ˆTπ( ̄w ⊙ ˆβ) − Tπ( ̄w ⊙ β)∥u,κ\n\n⟨f, ˆTπ( ̄w ⊙ ˆβ) − Tπ( ̄w ⊙ β)⟩\n\n⟨ ̄w ⊗ f, ˆβ(2)\n\nπ − β(2) π ⟩\n\n= sup\n\nf ∈B(X ) ∥f ∥H≤u ∥f ∥∞≤1\n\n= sup\n\nf ∈B(X ) ∥f ∥H≤u ∥f ∥∞≤1\n\n≤ ∥ ˆβ(2)\n\nπ − β(2)\n\nπ ∥u,κ(2)\n\nw\n\n,\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nwhere β(2) π , ˆβ(2) d ˆβ(x)d ˆTπ(x′|x), and κ(2)\n\nπ ∈ M (X 2) are given by dβ(2)\n\nπ (x, x′) := dβ(x)dTπ(x′|x) and d ˆβ(2)\n\nπ (x, x′) :=\n\nw ((x, x′), (y, y′)) := ̄w(x) ̄w(y)κ(x′, y′). Therefore, we have\n\n|∥ ˆRπ(w)∥u,κ − ∥Rπ(w)∥u,κ|\n\n≤ ∥ˆιπ − ιπ∥u,κ +\n\n≤ ∥ˆιπ − ιπ∥u,κ +\n\nγ 1 − γ γ∥w∥∞ 1 − γ\n\n∥ ˆTπ(w ⊙ ˆβ) − Tπ(w ⊙ β)∥u,κ +\n\n1 1 − γ\n\n∥w ⊙ ( ˆβ − β)∥u,κ\n\n∥ ˆβ2\n\nπ − β2\n\nπ∥u,κ(2)\n\nw\n\n+\n\n∥w∥∞ 1 − γ\n\n∥ ˆβ − β∥u,κw .\n\nSince κ, κw and κ(2)\n\nw are all bounded, (19) now implies\n\n|∥ ˆRπ(w)∥u,κ − ∥Rπ(w)∥u,κ| = O\n\n(cid:114)\n\n(cid:32)\n\n∥w∥∞ 1 − γ\n\nu2 + ln(1/δ) n\n\n(cid:33)\n\n.\n\nCombining this with (18) and take the limit with u → ∞ and n/u2 → ∞, we get the desired result.\n\nG PROOF OF THEOREM 7.6\n\nNote that Theorem 7.3 combined with the compactness of W and the continuity of w (cid:55)→ ∥Rπ(w)∥TV implies that EvaluateDBR(D, F, w) converges to ∥Rπ(w)∥TV uniformly on w ∈ W. Thus, it suffice to show the following lemma.\n\nLemma G.1. We have\n\nmin u∈U\n\n∥Rπ( ˆwu)∥TV → min w∈W\n\n∥Rπ(w)∥TV.\n\nProof. Note also that Corollary 7.5 combined with the compactness of W and the continuity of w (cid:55)→ ∥Rπ(w)∥u,κ implies\n\n∥ ˆRπ(w)∥u,κ → ∥Rπ(w)∥TV\n\nuniformly for all w ∈ W, under suitable asymptotics of u and n as in Corollary 7.5. In other words, for all c > 0 and δ ∈ (0, 1), there exists u0 ≥ 1 and p0 > 0 such that, for all u ≥ u0 and n ≥ p0u2 such that supw∈W |∥ ˆRπ(w)∥u,κ − ∥Rπ(w)∥TV| ≤ c with probability ≥ 1 − δ. Therefore, taking such a pair (u, n) satisfying u ∈ U (which exists by the definition of U), we have\n\nmin w∈W\n\n∥Rπ(w)∥TV ≤ min u′∈U\n\n∥Rπ( ˆwu′)∥TV\n\n(∵ restriction of domain)\n\n≤ c + ∥ ˆRπ( ˆwu)∥u,κ ≤ c + ∥ ˆRπ(w⋆)∥u,κ ≤ 2c + ∥ ˆRπ(w⋆)∥TV = 2c + min w∈W\n\n∥Rπ(w)∥TV\n\n(∵ definition of ˆwu)\n\nwith probability ≥ 1 − δ. Since c > 0 can be arbitrary small, we have just proved the desired result.\n\nH BASIC DEFINITIONS AND RESULTS\n\nThis section presents basic results used in the proof of our results for completeness. The main purpose of this section is to show Proposition D.2.\n\nH.1 SIGNED MEASURES\n\nWe first introduce the absolute value and the positive and negative parts of a signed measure. Recall that Σ is the Borel algebra of X .\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition H.1 (Absolute value and positive and negative parts of signed measure). For all P ∈ M (X ), its absolute value is given by |P | ∈ M (X ) such that\n\n|P |(E) =\n\nsup E++E−=E\n\n{P (E+) − P (E−)}.\n\nMoreover, its positive and negative parts are given by P± := (|P | ± P )/2 ∈ M (X ).\n\nThe following properties are then immediately seen. We omit the proof since it is trivial from the definitions.\n\nLemma H.1. The following statements are true.\n\n1. P± are nonnegative measures.\n\n2. P = P+ − P− and |P | = P+ + P−.\n\n3. P, P± ≪ |P |.\n\n4. ∥|P |∥TV = ∥P+∥TV + ∥P−∥TV = ∥P ∥TV.\n\nNext, the sign function of a signed measure is defined with the absolute value. Definition H.2 (Sign of signed measure). For all P ∈ M (X ), its sign is given by sign P := dP B(X ).\n\nd|P | ∈\n\nWe note that the essential range of the sign function is bounded to [−1, 1].\n\nLemma H.2. We have |(sign P )(x)| ≤ 1 for |P |-almost every x ∈ X .\n\nProof. It follows from\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ndP d|P |\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ndP+ d|P |\n\n−\n\ndP− d|P |\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\ndP+ d|P |\n\n+\n\ndP− d|P |\n\n=\n\nd|P | d|P |\n\n= 1.\n\n(|P |-almost everywhere)\n\nFinally, we define the empirical distribution for signed measures. Definition H.3 (Empirical distribution of signed measure). For all P ∈ M (X ) such that ∥P ∥TV = 1, we define its n-th empirical distribution by\n\nˆPn :=\n\n1 n\n\nn (cid:88)\n\n(sign P )(xi)δxi,\n\ni=1\n\nwhere {xi}n\n\ni=1 is n-i.i.d. sample drawn independently from |P |, which is a probability distribution.\n\nNote that it coincides with the empirical distribution of probability measures if P is nonnegative. One of its most basic properties is the unbiasedness. Lemma H.3 (Unbiasedness). For all P ∈ M (X ) such that ∥P ∥TV = 1, we have\n\nP (E) = E ˆPn(E)\n\nfor all E ∈ Σ.\n\nProof. It follows from that, for all E ∈ Σ,\n\nE ˆPn(E) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nE[(sign P )(xi)1{xi ∈ E}]\n\n= (sign P ⊙ |P |)(E) = P (E).\n\n(∵ xi ∼ |P |) (∵ sign P = dP/d|P |)\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nH.2 RADEMACHER COMPLEXITY AND UNIFORM LAW OF LARGE NUMBER\n\nThe Rademacher complexity is a measure of the complexity of function class. It is mainly utilized to establish the concentration of the empirical processes corresponding to the function classes, i.e., the uniform law of large number. Throughout the section, let σn := {σi}n i=1 be a sequence of Rademacher random variables, each of which takes ±1 with probability 1/2 independently. Definition H.4 (Rademacher complexity). For a subset of n-dimensional vectors Θ ⊂ Rn, the Rademacher complexity of Θ is defined by\n\nR(Θ) := Eσn\n\n(cid:34)\n\nsup θ∈Θ\n\nn (cid:88)\n\ni=1\n\n(cid:35)\n\nσiθi\n\n.\n\nMoreover, for S ∈ X n and F ⊂ B(X ), the empirical Rademacher complexity of F with respect to S is defined by\n\nwhere F(S) := {(f (x1), ..., f (xn)) ∈ R : S = {xi}n i=1, f ∈ F} denotes the set of vectors obtained by applying f ∈ F on each element of S. Also, we define the n-th maximal Rademacher complexity of F by\n\nRS(F) := R(F(S)/n),\n\nRn(F) := sup S∈X n\n\nRS(F).\n\nThe following lemma is useful to bound the Rademacher complexity of the composition of functions. Lemma H.4 (Rademacher contraction lemma). For all Θ ∈ Rn and a family of 1-Lipschitz continuous functions (φi)n\n\ni=1, φi : R → R, we have\n\nwhere φ(Θ) := {(φ1(θ1), ..., φn(θn)) ∈ Rn : θ ∈ Θ}.\n\nR(φ(Θ)) ≤ R(Θ),\n\nProof. Observe that\n\nR(φ(Θ)) = Eσn sup\n\nθ∈Θ\n\n=\n\n1 2\n\nEσn−1\n\nn (cid:88)\n\ni=1 (cid:34)\n\nσiφi(θi)\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\n(cid:41)\n\nσiφi(θi) + φn(θn)\n\n+ sup θ′∈Θ\n\nsup θ∈Θ\n\nSince the expression inside the expectation is bounded by\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\nsup θ∈Θ\n\nσiφi(θi) + φn(θn)\n\n(cid:41)\n\n+ sup θ′∈Θ\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\nσiφi(θ′\n\ni) − φn(θ′\n\nn)\n\n(cid:41)\n\nσiφi(θ′\n\ni) − φn(θ′\n\nn)\n\n(cid:41)(cid:35)\n\n.\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\n(cid:41)\n\n= sup\n\nθ,θ′∈Θ\n\n≤ sup\n\nθ,θ′∈Θ\n\n= sup\n\nθ,θ′∈Θ\n\n(cid:40)n−1 (cid:88)\n\ni=1 (cid:40)n−1 (cid:88)\n\ni=1 (cid:40)n−1 (cid:88)\n\ni=1\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\n= sup θ∈Θ\n\nwe have\n\nσi{φi(θi) + φi(θ′\n\ni)} + {φn(θn) − φn(θ′\n\nn)}\n\nσi{φi(θi) + φi(θ′\n\ni)} + |θn − θ′\n\nn|\n\n(cid:41)\n\nσi{φi(θi) + φi(θ′\n\ni)} + θn − θ′\n\nn\n\n(cid:41)\n\n(∵ Symmetry of θ and θ′)\n\nσiφi(θi) + θn\n\n(cid:41)\n\n+ sup θ′∈Θ\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\nσiφi(θ′\n\ni) − θ′\n\nn\n\n(cid:41)\n\n,\n\nR(φ(Θ)) ≤\n\n1 2\n\nEσn−1\n\n(cid:34)\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\nsup θ∈Θ\n\nσiφi(θi) + θn\n\n(cid:41)\n\n+ sup θ′∈Θ\n\n(cid:40)n−1 (cid:88)\n\ni=1\n\n(cid:41)(cid:35)\n\nσiφi(θ′\n\ni) − θ′\n\nn\n\n= R( ̃φ(Θ)),\n\nwhere ̃φ = (φ1, ..., φn−1, I) and I is the identity map. Iterating the same argument to swap φj with I for all j = 1, ..., n − 1, we get the desired result.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nThe following theorem gives a sufficient condition for the concentration of the empirical process f (cid:55)→ ⟨f, ˆPn − P ⟩, f ∈ F, with respect to the Rademacher complexity Rn(F). Theorem H.5 (Uniform law of large number). For all probability measures P ∈ M (X ) and all F ⊂ B(X ), we have\n\nFurthermore, we have\n\nE sup\n\nf ∈F\n\n⟨f, ˆPn − P ⟩ ≤ 2Rn(F).\n\n⟨f, ˆPn − P ⟩ ≤ 2Rn(F) + D\n\nsup f ∈F\n\n(cid:114)\n\nln(1/δ) 2n\n\nwith probability 1 − δ, where D := supf ∈F ,x,y∈X {f (x) − f (y)}. Here, ˆPn is the empirical distribution of P given by Definition H.3.\n\nProof. Let {xi}n result follows from\n\nE sup\n\nf ∈F\n\n⟨f, ˆPn − P ⟩ = E sup\n\nf ∈F (cid:34)\n\ni=1 and {x′\n\ni}n\n\ni=1 are two n-i.i.d. sample drawn independently from P . The first\n\n(cid:34)\n\nE\n\n1 n\n\nn (cid:88)\n\ni=1\n\n{f (xi) − f (x′\n\ni)}\n\n(cid:35)\n\n{xi}n\n\ni=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(∵ Lemma H.3)\n\n{f (xi) − f (x′\n\ni)}\n\n(cid:35)\n\n(cid:35)\n\nσi{f (xi) − f (x′\n\ni)}\n\n(∵ symmetry of xi and x′ i)\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n≤ E\n\nsup f ∈F\n\n= E\n\n(cid:34)\n\nsup f ∈F (cid:34)\n\n(cid:35)\n\nσif (xi)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n≤ 2E\n\nsup f ∈F\n\n≤ 2Rn(F).\n\nTo show the second result, define\n\nA(S) := sup f ∈F\n\n(cid:40)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:41)\n\nf (xi) − ⟨f, P ⟩\n\nfor S := {xi}n\n\ni=1 ∈ X n and observe A(S) follows the same law as\n\nThus, it suffices to establish\n\n⟨f, ˆPn − P ⟩.\n\nsup f ∈F\n\nA(S) − EA(S) ≤ D\n\n(cid:114)\n\nln(1/δ) 2n\n\nwith probability 1 − δ, which follows from McDiarmid’s inequality (Boucheron et al., 2003) applied on A(S). Here, the assumption of McDiarmid’s inequality we need to verify is that A(S′)−A(S) ≤ i=1 ∈ X n that only differs from S at the j-th element, 1 ≤ j ≤ n. This is D/n for all S′ := {x′ verified by\n\ni}n\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\n(cid:41)\n\nf (x′\n\ni) − ⟨f, P ⟩\n\nf (xi) − ⟨f, P ⟩ +\n\n(cid:41)\n\n{f (x′\n\ni) − f (xi)}\n\n1 n\n\nA(S′) = sup\n\nf ∈F\n\n= sup f ∈F\n\n(cid:40)\n\n(cid:40)\n\n1 n\n\n1 n\n\n≤ A(S) +\n\n≤ A(S) +\n\nThis concludes the proof.\n\ni=1 1\nn\n\nsup f ∈F\n\nD n\n\n.\n\n{f (x′\n\ni) − f (xi)}\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nH.3 REPRODUCING KERNEL HILBERT SPACE\n\nThroughout this section, we assume H is the reproducing kernel Hilbert space generated with a continuous, symmetric, positive-definite kernel κ : X 2 → R. Also let ∥ · ∥H and ⟨·, ·⟩H be the associated norm and inner product, and let BH(0, 1) := {f ∈ H : ∥f ∥H ≤ 1} be the closed unit ball of H. The following lemma shows the Rademacher complexity of RKHS can be explicitly bounded. Note that c0-universal kernel is uniformly bounded and hence ∥κ∥∞ < ∞. Lemma H.6 (Rademacher complexity of RKHS). We have\n\nRn(BH(0, 1)) ≤\n\n(cid:114)\n\n∥κ∥∞ n\n\n.\n\nProof. It follows from that, for all S ∈ X n,\n\nRS(BH(0, 1)) = Eσn\n\n(cid:34)\n\n(cid:34)\n\nsup f ∈BH(0,1)\n\n= Eσn\n\nsup f ∈BH(0,1)\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n(cid:35)\n\nσif (xi)\n\n(cid:35)\n\nσi⟨κ(·, xi), f ⟩H\n\nσiκ(·, xi)\n\n= Eσn\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)Eσn\n\n≤\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)H (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nH\n\nσiκ(·, xi)\n\n(∵ Jensen’s ineq.)\n\n(cid:118) (cid:117) (cid:117) (cid:116)Eσn\n\n=\n\n(cid:42)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nσiκ(·, xi),\n\n(cid:43)\n\nσjκ(·, xj)\n\n1 n\n\nn (cid:88)\n\nj=1\n\nH\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:114)\n\n=\n\n=\n\n≤\n\n1 n2\n\n1 n2\n\nn (cid:88)\n\n⟨κ(·, xi), κ(·, xi)⟩H\n\ni=1\n\nn (cid:88)\n\ni=1\n\nκ(xi, xi)\n\nsupx∈X κ(x, x) n\n\n.\n\nThe following lemma shows the compactness of the closed RKHS balls in the L2-metrics. Lemma H.7 (Compactness of RKHS). BH(0, 1) is compact with respect to L2(P ) for all positive measures P ∈ M (X ).\n\nProof. Mercer’s theorem gives an eigen decomposition of κ such that\n\nκ(x, y) =\n\n∞ (cid:88)\n\nk=1\n\nλkφk(x)φk(y),\n\nwhere the convergence is uniform on X 2, {φk : X → R}∞ k=1 is a continuous orthonormal basis of L2(P ) and Λ := {λk ∈ R≥0}∞ k=1 is a nonnegative decreasing sequence with limk→∞ λk = 0. It then follows from the standard function analysis that BH,1 under the L2(P )-metric is isometric to U (Λ) := {a ≡ (ak)∞ k/λk ≤ 1} under the l2-metric via the mapping ak(f ) := (cid:82) f (x)φk(x)dP (x). Here, we evaluate 0/0 = ∞. Therefore, the compactness of BH,1 in L2(P ) follows from the compactness of U (Λ) in l2. To show the compactness of the latter, it\n\nk≥0:ak̸=0 a2\n\nk=1 : (cid:80)\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nsuffices to show its completeness and total boundedness. The completeness is trivial, while the total boundedness follows from the fact that the elements of U (Λ) is approximated with their projections onto the first K coordinates where the approximation error is bounded by λK+1, which tends to zero as K → ∞.\n\n27",
    "reference": "# Summary Of The Paper\n\nThis paper proposes the problem of minimax bias estimation of the value of a policy using offline data. The algorithm is based on marginal-importance-sampling and minimizing the Bellman residual error.\n\n# Strength And Weaknesses\n\nStrength:\n\n1) The problem of estimating the confidence interval of the value of a policy using offline data is very important. Since it is known that when the offline data is not exploratory enough, it is hard to get accurate point estimation, it's important to get good interval estimation.\n2) The theoretical analysis of this paper seems to be sound, although I did not check the details.\n\nWeakness:\n1) I think the notation in this paper is a bit heavy and I found it a bit hard to understand all the details. I also think it would be good to move the actual algorithms to the main sections of the paper instead of putting them in the appendix.\n2) It would be good to have some empirical evaluation.\n\nThere are a few places that I don't fully understand:\nPage 8 Proof of Proposition 7.4: why does the Rademacher complexity of the unit ball in H of the order O(n^{-1/2})? I assume it should also depend on certain intrinsic dimension of the space H.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAs mentioned above, I think the clarity of the paper needs to be improved. The paper has some novelty. Since it is a theoretical paper, I don't see any reproducibility issues.\n\n# Summary Of The Review\n\nI think this is a good paper overall. But I am not very confident in my review since I did not check all the details.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nOFFLINE REINFORCEMENT LEARNING FROM RANDOMLY PERTURBED DATA SOURCES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMost of the existing offline reinforcement learning (RL) studies assume the available dataset is sampled directly from the target task. However, in some practical applications, the available data are often coming from several related but heterogeneous environments. A theoretical understanding of efficient learning from heterogeneous offline datasets remains lacking. In this work, we study the problem of offline RL based on multiple data sources that are randomly perturbed versions of the target Markov decision process (MDP). A novel HetPEVI algorithm is first proposed, which simultaneously considers two types of uncertainties: sample uncertainties from a finite number of data samples per data source, and source uncertainties due to a finite number of data sources. In particular, the sample uncertainties from all data sources are jointly aggregated, while an additional penalty term is specially constructed to compensate for the source uncertainties. Theoretical analysis demonstrates the near-optimal performance of HetPEVI. More importantly, the costs and benefits of learning with randomly perturbed data sources are explicitly characterized: on one hand, an unavoidable performance loss occurs due to the indirect access to the target MDP; on the other hand, efficient learning is achievable as long as the sources collectively (instead of individually) provides a good data coverage. Finally, we extend the study to linear function approximation and propose the HetPEVI-Lin algorithm that provides additional efficiency guarantees beyond the tabular cases.\n\n1\n\nINTRODUCTION\n\nOffline reinforcement learning (RL) (Levine et al., 2020), a.k.a. batch RL (Lange et al., 2012), has received growing interest in the recent years. It aims at training RL agents using accessible datasets collected a priori and thus avoids expensive online interactions. Along with its tremendous empirical successes (Kidambi et al., 2020), recent studies have also established theoretical understandings of offline RL (Rashidinejad et al., 2021; Jin et al., 2021b; Duan et al., 2021; Uehara & Sun, 2021).\n\nDespite these advances, the majority of offline RL research focuses on learning via data collected exactly from the target task environment (Kumar et al., 2020). However, in practice, it is difficult to ensure that all such data are perfectly from one source environment. Instead, in many cases, it is more reasonable to assume that data are collected from different sources that are perturbed versions of the target task. For example, when training a chatbot (Jaques et al., 2020), the offline dialogue datasets typically consist of short conversations between different people, who naturally have varying language habits. The training objective is the common underlying language structure, e.g., basic grammar, which however cannot be reflected in any individual dialogue but must be holistically learned from the aggregation of them. More examples can be found in healthcare (Tang & Wiens, 2021), autonomous driving (Sallab et al., 2017) and others. While a few empirical investigations under the offline meta-RL framework have been reported (Dorfman et al., 2021; Lin et al., 2022; Mitchell et al., 2021), theoretical understandings of effectively and efficiently learning the underlying task using datasets from multiple heterogeneous sources are largely lacking.\n\nMotivated by both practical and theoretical limitations, this work makes progress in the underexplored RL problem of learning the target task using data from heterogeneously perturbed data sources. In particular, we study the problem of learning a target Markov decision process (MDP)\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nfrom offline datasets sampled from multiple heterogeneously realized source MDPs. Several provably efficient designs are proposed, targeting both tabular and linear MDPs. To the best of our knowledge, this is the first work that proposes provably efficient offline RL algorithms to handle perturbed data sources, which can benefit relevant applications and further shed light on the theoretical understanding of offline meta-RL. The contributions are summarized as follows:\n\n• We study a new offline RL problem where the datasets are collected from multiple heterogeneous source MDPs, with possibly different reward and transition dynamics, instead of directly from the target MDP. Motivated by practical applications, the data source MDPs are modeled as random perturbations of the target MDP. Compared with studies of offline RL using data directly from the target MDP (Rashidinejad et al., 2021; Jin et al., 2021b), we face new challenges of jointly considering uncertainties caused by the finite number of data samples per source (referred to as the sample uncertainties) and by the finite number of data sources (referred to as the source uncertainties).\n\n• A novel HetPEVI algorithm is proposed, which generalizes the idea of pessimistic value iteration (Jin et al., 2021b) and uses carefully crafted penalty terms to address the sample and source uncertainties simultaneously. Specifically, in the HetPEVI, the specially designed penalty term contains two parts: one aggregating the sample uncertainties associated with each dataset, and the other term compensating the source uncertainties. The combination of these two parts jointly characterizes the uncertainty associated with the collected datasets.\n\n• Theoretical analysis proves the effectiveness of HetPEVI with a corresponding lower bound, which first demonstrates that even with finite randomly perturbed MDPs and finite data samples from each of them, it is feasible to efficiently learn the target MDP. More importantly, the analysis reveals that learning with multiple perturbed data sources brings both costs and benefits. On one hand, due to indirect access to the target MDP, an unavoidable learning cost occurs. This cost scales only with the number of data sources and cannot be reduced by increasing the size of datasets, which highlights the importance of the diversity of data sources. On the other hand, effective learning only requires that the datasets collectively (instead of individually) provide a good coverage of the optimal policy, which may provide additional insights for practical data collections.\n\n• Moreover, we extend the study to linear function approximation where offline data are collected from linear MDPs with a shared feature mapping but heterogeneously realized system dynamics. The HetPEVI-Lin algorithm is developed to jointly consider the sample and source uncertainties while incorporating the linear structure. Theoretical analysis demonstrates the effectiveness of HetPEVI-Lin and verifies the sufficiency of a good collective coverage.\n\nRelated Works. With the empirical success of offline RL (Levine et al., 2020), its theoretical understandings have been gradually established in recent years. In particular, the principle of “pessimism” is incorporated and proved efficient for offline RL (Jin et al., 2021b; Rashidinejad et al., 2021). Following this line, Xie et al. (2021b); Li et al. (2022); Shi et al. (2022) further fine-tune the designs for the tabular setting and Zanette et al. (2021); Min et al. (2021); Yin et al. (2022); Xiong et al. (2022) for linear MDPs (Jin et al., 2020). These theoretical advances are mainly focused on learning with data directly from the target task. However, in the practical studies of RL, growing interests have been made to utilize data from heterogeneous sources, e.g., offline meta-RL (Mitchell et al., 2021; Dorfman et al., 2021; Lin et al., 2022; Li et al., 2020b). This work is thus motivated to provide a theoretical understanding of how to extract information about the target task from multiple sources. A more detailed literature review regarding both online and offline RL with single or heterogeneous environments is provided in Appendix A.1.\n\n2 PROBLEM FORMULATION\n\nPreliminaries of RL. We consider an RL problem characterized by an episodic MDP M := (H, S, A, P, r). In this tuple, H is the length of each episode, S is the state space, A is the action space, P is the transition matrix so that Ph(s′|s, a) gives the probability of transiting to state s′ if action a is taken for state s at step h, and rh(s, a) is the deterministic reward in the interval of [0, 1] of taking action a for state s at step h.1 Specifically, in each episode, starting from an initial state s1, at each step h ∈ [H], the agent observes state sh ∈ S, picks action ah ∈ A, receives reward\n\n1The assumption of deterministic rewards is standard in theoretical analysis of RL (Jin et al., 2018; 2020)\n\nas the uncertainties in estimating rewards are dominated by those in estimating transitions.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nrh(sh, ah), and then transits to a next state sh+1 ∼ Ph(·|sh, ah). The episode ends after H steps. In the tabular RL setting, the state and action spaces are finite with S := |S| and A := |A|.\n\nh(s, a) := Eπ,M[(cid:80)H\n\nA policy π := {πh(·|s) : (s, h) ∈ S × [H]} consists of distributions πh(·|s) over the action space A, whose value functions can be defined as V π h′=h rh′(sh′, ah′)|sh = s] and Qπ h′=h rh′(sh′, ah′)|sh = s, ah = a] for each (s, a, h) ∈ S × A × [H], where the expectation Eπ,M[·] is with respect to (w.r.t.) the random trajectory induced by policy π on MDP M. The optimal policy π∗ maximizes the value function, i.e., π∗ := arg maxπ V π 1 (s). For convenience, we denote V ∗ h (s, a) for all (s, a, h) ∈ S×A×[H], and use πh(s) to refer to the chosen action for state s at step h for a deterministic policy π.\n\nh (s) := Eπ,M[(cid:80)H\n\nh(s, a) := Qπ∗\n\nh (s) := V π∗\n\nh (s) and Q∗\n\n2.1 THE LEARNING TARGET AND OFFLINE DATASETS\n\nThe Target Task. This work considers a target task modeled by an MDP M = (H, S, A, P, r), which can be any task of interest (e.g., chatbot training in Sec. 1). The goal is to find a good output policy ˆπ with a small sub-optimality gap on the target MDP M, which is measured as:\n\nGap(ˆπ; M) := V ∗\n\n1 (s1) − V ˆπ\n\n1 (s1).\n\nNote that an output policy ˆπ is called ε-optimal if Gap(ˆπ; M) ≤ ε.\n\nh,l, sk\n\nh,l, rk\n\nh,l, ak\n\nMultiple Data Sources. The agent has access to datasets from L sources, i.e., D := {Dl, ∀l ∈ [L]}. Each dataset Dl := {(sk h+1,l) : k ∈ [K], h ∈ [H]} consists of K tuples for each step h ∈ [H] sampled by a (possibly different) unknown behavior policy ρl on a unknown data source MDP denoted as Ml = (H, S, A, Pl, rl). In particular, for each step h ∈ [H], the behavior policy ρl performs K times sampling over the state-action space S × A following a distribution denoted as dρl h,l), the h,l, ak reward rk h,l) realized from the date source MDP Ml is collected and aggregated as a tuple (sk h,l, ak h+1,l) for the dataset Dl. To ease the presentation, all the sampled tuples are assumed to be independent of each other. Such independence can be ensured by applying the sub-sampling technique in Li et al. (2022) to trajectories induced by behavior policies while maintaining the order of the dataset size.\n\nh,l(·), i.e., for each k ∈ [K], (sk h,l) and sk\n\nh,l) ∼ dρl h,l, ak h+1,l ∼ Ph,l(·|sk\n\nh,l(·). Then, for a sampled pair (sk\n\nh,l = rh,l(sk\n\nh,l, ak\n\nh,l, ak\n\nh,l, rk\n\nh,l, sk\n\n2.2 THE TASK–SOURCE RELATIONSHIP\n\nOn one hand, motivated by Sec. 1, each data source MDP Ml may not exactly match the target task M. Concretely, while having the same episodic length, state space, and action space, their transition and reward dynamics are not necessarily aligned, i.e., Ph,l(·|s, a) ̸= Ph(·|s, a), rh,l(s, a) ̸= rh(s, a).\n\nOn the other hand, in practical applications, while being heterogeneous, the data source MDPs are often still related to the target task (e.g., dialogue datasets in Sec. 1). In particular, data sources in offline meta-RL are often assumed to be sampled from a certain distribution (Mitchell et al., 2021). Thus, the following relationship between the target MDP and the data source MDPs is assumed.\n\nAssumption 1 (Task–source Relationship). Data source MDPs {Ml : l ∈ [L]} are independently sampled from an unknown distribution g such that for each (l, s, a, h) ∈ [L] × S × A × [H], the reward rh,l(s, a) is a random variable with mean rh(s, a), and the transition vector Ph,l(·|s, a) is a random vector with mean Ph(·|s, a). In particular, random variables and random vectors {rh,l(s, a), Ph,l(·|s, a) : (s, a, h, l) ∈ S × A × [H] × [L]} are independent with each other.\n\nThe requirement that rewards are random samples with the expectation as the target model is commonly adopted in bandits literature (Shi & Shen, 2021; Zhu & Kveton, 2022), and the same requirement on the transition vectors is a natural extension, where one representative example is to have them follow a Dirichlet distribution (Marchal & Arbel, 2017).\n\nMiscellaneous. Notations without subscripts l generally refer to the target MDP M, while subscripts l are always added when discussing each individual source Ml. For a clear presentation, the notation c is used throughout the paper with varing values to represent a poly-logarithmic term of order O(poly(log(LHSA/δ))) for the tabular setting and O(poly(log(LH/δ))) for the linear setting, where δ ∈ (0, 1) is a confidence parameter. Additionally, for x ∈ R, x+ denotes max{x, 0}.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFor any function f : S → R, the transition operator and Bellman operator of the target MDP M at each step h ∈ [H] are defined, respectively, as (Phf )(s, a) := E[f (s′)|s, a] and (Bhf )(s, a) := rh(s, a) + (Phf )(s, a), where the expectation is w.r.t. the transition s′ ∼ Ph(·|s, a). Expectation Eπ,M[·] is over the trajectory induced by π on the MDP M starting from s1, and dπ h(s, a) denotes the probability of visiting state-action pair (s, a) at step h with policy π on M starting from s1.\n\nAlgorithm 1 HetPEVI Input: Dataset D = {Dl : l ∈ [L]}; ˆVH+1(s) = 0, ∀s ∈ S 1: For each (l, s, a, h) ∈ [L] × S × A × [H], estimate ˆrh,l(s, a) = rh,l(s, a)1{Nh,l(s, a) ̸= 0}\n\n2: For each (s, a, h) ∈ S × A × [H], aggregate ˆrh(s, a) = 1\n\n(cid:80)\n\nl∈[L] ˆrh,l(s, a) and ˆPh(s′|s, a) =\n\nL\n\nand ˆPh,l(s′|s, a) = Nh,l(s,a,s′)\n\nNh,l(s,a)∨1\n\n1 L\n\n(cid:80)\n\nl∈[L]\n\nˆPh,l(s′|s, a)\n\n3: for h = H, H − 1, · · · , 1 do 4:\n\nPerform updates for all (s, a) ∈ S × A as Eqn. (1) with (ˆBh ˆVh+1)(s, a) := ˆrh(s, a) +\n\n(ˆPh ˆVh+1)(s, a) and Γh(s, a) = c\n\n(cid:113)(cid:80)\n\nl∈[L]\n\nH 2\n\n(L2Nh,l(s,a))∨L + c\n\n(cid:113) H 2\n\nL\n\n5: end for Output: policy ˆπ = {ˆπh(s) : (s, h) ∈ S × [H]}\n\n3 THE HETPEVI ALGORITHM\n\nIn this section, the HetPEVI algorithm is introduced for the tabular MDP setting (presented in Algorithm 1). HetPEVI follows the principle of pessimistic value iterations (PEVI) (Rashidinejad et al., 2021; Jin et al., 2021b), which is first briefly introduced, while the key challenges and our novel design in learning the target MDP from randomly perturbed data sources are then illustrated.\n\nThe HetPEVI algorithm begins by counting the number of visitations for each tuple (s, a, h, s′) ∈ S × A × [H] × S in each dataset l ∈ [L]. Especially, we denote Nh,l(s, a) and Nh,l(s, a, s′) as the amount of visitations on (s, a, h) and (s, a, h, s′) in dataset Dl, respectively. Empirical estimations of rewards and transitions are then given for all (l, s, a, h) ∈ [L] × S × A × [H] as\n\nˆrh,l(s, a) = rh,l(s, a)1{Nh,l(s, a) ̸= 0};\n\nˆPh,l(s′|s, a) =\n\nNh,l(s, a, s′) Nh,l(s, a) ∨ 1\n\n.\n\nThese individual estimates are then aggregated into overall estimates as:\n\nˆrh(s, a) =\n\n1 L\n\n(cid:88)\n\nl∈[L]\n\nˆrh,l(s, a);\n\nˆPh(s′|s, a) =\n\n1 L\n\n(cid:88)\n\nl∈[L]\n\nˆPh,l(s′|s, a).\n\nWith these estimations, HetPEVI iterates backward from the last step to the first step as\n\nˆQh(s, a) = min (cid:8)(cid:0)ˆBh ˆVh+1\n\nˆVh(s) = maxa∈A ˆQh(s, a),\n\n(cid:1)(s, a) − Γh(s, a), H − h + 1(cid:9)+ ,\nˆπh(s) = arg maxa∈A ˆQh(s, a),\n\n(1)\n\nwith ˆVH+1(s) = 0, ∀s ∈ S, and the empirical Bellman operator ˆBh is defined as\n\n(cid:0)ˆBh ˆVh+1\n\n(cid:1)(s, a) := ˆrh(s, a) + (cid:0)ˆPh ˆVh+1\n\n(cid:1)(s, a),\n\nwhere (ˆPh ˆVh+1)(s, a) is the empirical version of (Ph ˆVh+1)(s, a) using the estimated ˆPh(·|s, a).\n\nThe essence of the above procedure is that instead of directly setting ˆQh(s, a) as (ˆBh ˆVh+1)(s, a) (as in the standard value iteration), a penalty term Γh(s, a) is subtracted, which serves the important role of keeping the estimations ˆVh(s) and ˆQh(s, a) pessimistic. Especially, the penalty term Γh(s, a) should be carefully designed such that with a high probability, it holds that\n\n(cid:12) (cid:12)\n\n(cid:0)ˆBh ˆVh+1\n\n(cid:1)(s, a) − (cid:0)Bh ˆVh+1\n\n(cid:1)(s, a)(cid:12)\n\n(cid:12) ≤ Γh(s, a),\n\n∀(s, a, h) ∈ S × A × [H].\n\n(2)\n\nTechnical Challenges. Previous offline RL studies (Jin et al., 2021b; Rashidinejad et al., 2021; Yin et al., 2022; Xie et al., 2021b) only deal with one single target data source and only one type\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nof uncertainties (finite data samples) to ensure Eqn. (2). In this work, the agent needs to process multiple heterogeneous datasets, while none of them individually characterizes the learning target. As a result, the agent faces two coupled challenges. First, the uncertainties due to the finite sample sizes still needs to be considered. This is referred to as the sample uncertainties, but the key difference is that now the agent needs to jointly consider and aggregate uncertainties associated with all data sources. Second, even with perfect knowledge of each data source, the target MDP may not be fully revealed. Thus, the agent also needs to consider the uncertainties from the limited number of data sources, which are referred to as the source uncertainties.\n\nTo address the two uncertainties, the penalty term is designed to have two parts as follows:\n\nΓh(s, a) = Γα\n\nh (s, a) + Γβ\n\nh(s, a),\n\nh (s, a) aggregates the sample uncertainties from each data source while Γβ where Γα for the source uncertainties. Both of them are further elaborated on in the following.\n\nh(s, a) accounts\n\nPenalties to Aggregate Sample Uncertainties. For the first part of the penalty, i.e., Γα h (s, a), since the agent faces data from multiple heterogeneous sources, the penalty term needs to jointly consider the individual uncertainties from all sources. In HetPEVI, the following penalty term is proposed, which originates from the Hoeffding inequality:\n\nΓα\n\nh (s, a) = c\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\nH 2 (L2Nh,l(s, a)) ∨ L\n\n(cid:114)\n\n+ c\n\nH 2 L\n\n.\n\nNote that instead of treating each source individually and directly summing up their sample uncertainties (as c (cid:80) Nh,l(s,a)∨1 ), the adopted penalty term is a joint measure of sample uncer-\n\n(cid:113) H 2\n\nl∈[L]\n\n1 L\n\ntainties from all sources. This would lead to a O(\n\nL) speed-up in the performance guarantees.\n\n√\n\nPenalties to Account for Source Uncertainties. The second part of the penalty Γβ h(s, a) serves the important role of measuring the uncertainties due to the limited amount of data sources. With the observation that (Bh,l ˆVh+1)(s, a) is a bounded random variable with mean of (Bh ˆVh+1)(s, a), the following penalty term is proposed:\n\n(cid:114)\n\nΓβ\n\nh(s, a) = c\n\nH 2 L\n\n.\n\nIntuitively, it shrinks with the number of datasets, i.e., L, as more sources provide more information about the target task. Remark 1. With the two-part penalty term, with high probability, for all (s, a, h) ∈ S × A × [H], it simultaneously holds that |(ˆBh ˆVh+1)(s, a) − ( ̄Bh ˆVh+1)(s, a)| ≤ Γα h (s, a) and |( ̄Bh ˆVh+1)(s, a) − (Bh ˆVh+1)(s, a)| ≤ Γβ Remark 2. The adopted source uncertainty penalty Γβ h(s, a) is intended to accommodate any unknown variance between sources. However, if there is prior knowledge of the variance, it is feasible to incorporate such information. In particular, if the rewards and transition vectors are generated via σ-sub-Gaussian distributions, the penalty can be designed as Γβ\n\nh(s, a), which jointly ensure Eqn. (2).\n\nh(s, a) = c(cid:112)σ2H 2/L.\n\n4 THEORETICAL ANALYSIS\n\n4.1\n\nINDIVIDUALLY GOOD COVERAGE\n\nPrevious offline RL studies typically require that the behavior policy should provide information for all the state-action pairs that may be visited by the optimal policy on the target MDP, e.g., Rashidinejad et al. (2021); Xie et al. (2021b). Particularizing this requirement to each individual data source considered in this work leads to the following assumption, which requires each individual dataset covers the optimal policy on the target MDP. Assumption 2 (Individual Coverage). There exists a constant C ∗ < ∞ such that for all (s, a, h, l) ∈ S × A × [H] × [L], it holds that dπ∗\n\nh (s, a) ≤ C ∗dρl\n\nh,l(s, a).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nUnder this assumption, a minimax lower bound is first established as follows. Theorem 1 (Lower Bound; Individual Coverage). For any C ∗ ≥ 2, S ≥ 2, A ≥ 2, it holds that\n\ninf ˆπ\n\nsup M∈M,g∈G,{ρl:l∈[L]}∈B(C∗)\n\nE{Ml,Dl:l∈[L]} [Gap(ˆπ; M)] = Ω\n\n(cid:32)(cid:114)\n\nC ∗H 3S LK\n\n∨\n\n(cid:114)\n\n(cid:33)\n\n,\n\nH 2 L\n\nwhere M := {M(H, S, A, P, r)} is the family of all possible target MDPs, G := {g: {Ml ∼ g : l ∈ [L]} satisfies Assumption 1} is a family of data source generation distributions, and B(C) := {{ρl : l ∈ [L]} : Assumption 2 holds with C ∗ = C} is a family of behavior policies.\n\nThe first term in this lower bound is the same as the lower bound of standard offline RL with LK data samples directly from the target MDP (Rashidinejad et al., 2021), which represents the performance loss originating from finite data samples. The second term is unique for the setting studied in this work, as it represents the loss caused by learning with finite randomly perturbed data sources. Especially, it can be observed that the second term only scales with the number of data sources, i.e., L, and cannot be mitigated via sampling more data from each data source, i.e., increasing K. On one hand, this observation verifies the intuition that using data sampled from multiple randomly perturbed data sources poses additional learning difficulties. On the other hand, it also highlights the importance of the diversity of data sources, i.e., it is more important to involve more sources instead of more data from each source. This is reasonable as involving more data sources provides additional population coverage while also adding more data.\n\nWith the information-theoretic lower bound established, the performance guarantee of HetPEVI is also provided in the following theorem, which highlights its effectiveness and efficiency.\n\nTheorem 2 (HetPEVI; Individual Coverage). Under Assumptions 1, 2, w.p. at least 1−δ, the output policy ˆπ of HetPEVI satisfies\n\nGap(ˆπ; M) = ̃O\n\n(cid:18)(cid:114)\n\nC ∗H 4S LK\n\n+\n\n(cid:114)\n\n(cid:19) .\n\nH 4 L\n\nThis result first illustrates that even with finite randomly perturbed MDPs and finite data samples from each of them, it is still feasible to efficiently learn the target. Compared with the lower bound in Theorem 1, it can be observed that HetPEVI is optimal (up to logarithmic factors) on the dependency of L, K, C ∗ and S. The additional H factor in the first term (from the sample uncertainties) can be removed by invoking a carefully designed Bernstein-type penalty term to incorporate the variance information, which is deferred to Appendix E due to the space limitation. However, it is currently unclear how to alleviate the additional H factor in the second term (from the source uncertainties), which is left open to be further investigated.\n\n√\n\nAdditionally, Thm. 2 indicates that to obtain an ε-optimal policy, HetPEVI requires the overall number of samples T = LK to be of order ̃O(C ∗H 4S/ε2) and the available number of data sources L to be of order ̃O(H 4/ε2). Note that the first requirement can be viewed as on the sample complexity while the second one is on the source diversity.\n\n4.2 COLLECTIVELY GOOD COVERAGE\n\nIt can be noticed that Assumption 2 requires each data source to provide good coverage. With access to only one data source in previous offline RL studies, this requirement is intuitive as information about the optimal policy needs to be obtained. However, the multiple available datasets in this work provide opportunities to avoid this strong assumption via their aggregated information. In this section, the scenario where the datasets collectively provide a good coverage is discussed. In particular, to characterize the collective coverage, the following assumption is proposed. Assumption 3 (Collective Coverage). There exist constants L† > 0 and C † < ∞ such that for all (s, a, h) ∈ S × A × [H], it holds that |{l ∈ [L] : dπ∗\n\nh (s, a) ≤ C †dρl\n\nh,l(s, a)| ≥ L†.\n\nIt can be observed that Assumption 3 is weaker than Assumption 2 as the latter implies the former with L† = L and C † = C ∗. Moreover, instead of requiring each data source individually covers the optimal policy, Assumption 3 leverages their collective coverage. In particular, for each different\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nstate-action pair possibly visited by the optimal policy (i.e., dπ∗ h (s, a) > 0), it is sufficient to have potentially varying datasets cover it. In other words, different parts of the optimal policy can be covered by different datasets, which is a highly practical consideration. Under Assumption 3, the following performance guarantee of HetPEVI can be further obtained.\n\nTheorem 3 (HetPEVI; Collective Coverage). Under Assumptions 1, 3, w.p. at least 1−δ, the output policy ˆπ of HetPEVI satisfies\n\nGap(ˆπ; M) = ̃O\n\n(cid:32)(cid:114)\n\n(cid:114)\n\nC †H 4S LK\n\n+\n\n(L + 1 − L†)H 4 L\n\n(cid:33)\n\n.\n\nCompared with Theorem 2, Theorem 3 is more general and also more practically useful as it implies the former and indicates that good collective coverage is sufficient for efficient learning. Moreover, from another perspective, the performance dependence on L† also highlights the importance of collecting high-quality datasets.\n\nAs a summary of the obtained observations, learning via randomly perturbed datasets brings both costs and benefits. Especially, an unavoidable performance loss occurs due to the indirect access to the target MDP. This loss can only be reduced by increasing the diversity of data sources, i.e., a larger L, but cannot be mitigated by collecting a larger dataset from each data source, i.e., a larger K. Despite bringing the additional loss, the access to multiple heterogeneous datasets provides an opportunity to leverage their aggregated information, which is concretely reflected that efficient learning only requires a good collective (instead of individual) coverage.\n\n5 EXTENSION TO OFFLINE LINEAR MDP\n\nLastly, we extend the study from tabular RL to incorporating function approximations. Especially, the following linear MDP model is considered. Definition 1 (Linear MDP (Jin et al., 2020; 2021b)). An MDP M = (H, S, A, P, r) is a linear MDP with a feature map φ : S × A → Rd if there exist d unknown measures μh = (μ(1) h ) over S and an unknown vector θh ∈ Rd s.t. Ph(s′|s, a) = ⟨φ(s, a), μh(s′)⟩ and rh(s, a) = ⟨φ(s, a), θh⟩ for all (s, a, s′) ∈ S × A × S at each step h ∈ [H]. Without loss of generality, it is assumed that ∥φ(s, a)∥2 ≤ 1 for all (s, a) ∈ S × A and max{∥μh(S)∥2, ∥θh∥2} ≤ d for all h ∈ [H], where ∥μh(S)∥2 := (cid:82)\n\nS ∥μh(s)∥2 ds. For simplicity, we denote M = (H, S, A, μ, φ, θ).\n\nh , · · · , μ(d)\n\n√\n\nWith this definition, we consider the problem that the target MDP M is a linear MDP, i.e., M = (H, S, A, φ, μ, θ). Correspondingly, the data source MDPs {Ml : l ∈ [L]} are also assumed to be linear MDPs, which are denoted as {Ml = (H, S, A, φ, μl, θl) : l ∈ [L]}. In particular, the data source MDPs are assumed to share the same feature dimension d and feature mapping φ as the target MDP; however, their system dynamics may be different. We note that this shared feature mapping is commonly adopted in federated linear bandits (Huang et al., 2021; Li & Wang, 2022), which naturally extends to linear MDP. Then, similarly as Assumption 1 in the tabular MDP, the following task-source relationship is assumed for the linear MDP to model that the data sources are randomly perturbed versions of the target MDP.\n\nAssumption 4 (Task–source Relationship; Linear MDP). Data source MDPs {Ml : l ∈ [L]} are independently sampled from unknown distributions g such that for each (l, i, h) ∈ [L] × [d] × [H], the vector θh,l is a random vector with mean θh, and the measure μ(i) h,l is a random measure with mean μ(i) h . In particular, random vectors and measures {θh,l, μ(i) h,l : (h, i, l) ∈ [H] × [d] × [L]} are independent with each other.\n\nNote that when treating the tabular setting as d = SA and φ(s, a) = e(s,a), where e(s,a) denotes a canonical basis in Rd, Assumption 4 returns to Assumption 1.\n\n5.1 HETPEVI-LIN FOR LINEAR MDPS\n\nFrom the linear MDP definition, it can be shown that the Bellman operators of each data source MDP and the target MDP are all linear (but with different weights).\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nh,l + ˆVh+1(sk rk\n\nh+1,l)\n\n(cid:17)(cid:105)\n\n, ∀(h, l) ∈ [H] × [L]\n\nAlgorithm 2 HetPEVI-Lin Input: Dataset D = {Dl : l ∈ [L]}; ˆVH+1(s) = 0, ∀s ∈ S 1: Estimate ˆwh,l ← Λ−1 2: Set ˆwh ← (cid:80) 3: for h = H, H − 1, · · · , 1 do 4:\n\nk∈[K] φ(sk l∈[L] ˆwh,l/L, ∀h ∈ [H]\n\nh,l, ak\n\n(cid:104)(cid:80)\n\nh,l)\n\nh,l\n\n(cid:16)\n\nΓh(s, a) = c\n\n(cid:113) dH 2\n\n(cid:113)(cid:80)\n\nL2\n\nl∈[L] ∥φ(s, a)∥2\n\nΛ−1\n\nh,l\n\n(cid:113) dH 2\n\nL\n\n+ c\n\nPerform updates for all (s, a) ∈ S × A as Eqn. (1) with (ˆBh ˆVh+1)(s, a) = ˆw⊤\n\nh φ(s, a) and\n\n5: end for Output: policy ˆπ = {ˆπh(s) : (s, h) ∈ S × [H]}\n\nLemma 1. With any function f : S → R, there exists {wf h,l ∈ Rd : h ∈ [H], l ∈ [L]} and {wf Rd : h ∈ [H]} such that for any (s, a, h) ∈ S ×A×[H], it holds that (Bh,lf )(s, a) = ⟨φ(s, a), wf for each l ∈ [L] and (Bhf )(s, a) = ⟨φ(s, a), wf h⟩.\n\nh ∈ h,l⟩\n\n(cid:80)\n\nthe following lemma can be established for the sample average MDP ̄M = Furthermore, (H, S, A, ̄P, r), where ̄Ph(·|s, a) = 1 l∈[L] rh,l(s, a). It states that ̄M is also a linear MDP and the weights associated with its Bellman operator are the average of weights from each individual data source MDP. Lemma 2. The average MDP ̄M is a linear MDP of dimension d with feature mapping φ. Also, for any function f : S → R and any (s, a, h) ∈ S × A × [H], it holds that ( ̄Bhf )(s, a) = ⟨φ(s, a), ̄wf h⟩ with weights { ̄wf\n\nPh,l(·|s, a) and ̄rh(s, a) = 1\n\nl∈[L]\n\n(cid:80)\n\nL\n\nL\n\nh = (cid:80)\n\nl∈[L] wf\n\nh,l/L : h ∈ [H]}.\n\nWith the above observations, the HetPEVI-Lin algorithm is proposed (presented in Algorithm 2). First, for each step h ∈ [H] and each data source l ∈ [L], the weight ˆwh,l is estimated via a ridge regression:\n\nˆwh,l = arg minw∈Rd\n\n(cid:88)\n\n(cid:104)\n\nk∈[K]\n\nh,l + ˆVh+1(sk rk\n\nh,l, ak\n\nh,l)⊤w\n\n(cid:105)2\n\n+\n\n1 L\n\n∥w∥2\n\n2\n\n(cid:20)(cid:88)\n\n= Λ−1\n\nh,l\n\nk∈[K]\n\nφ(sk\n\nh,l, ak\n\nh,l)\n\n(cid:16)\n\nh+1,l) − φ(sk (cid:17)(cid:21)\n\nh,l + ˆVh+1(sk rk\n\nh+1,l)\n\n,\n\nh,l, ak\n\nh,l, ak\n\nh,l)φ(sk\n\nk∈[K] φ(sk\n\nh,l)⊤ + I/L. The agent aggregates ˆwh = (cid:80)\n\nwith Λh,l := (cid:80) l∈[L] ˆwh,l/L, which provides an estimation of the weight ̄wh associated with the average MDP ̄M. The PEVI in Eqn. (1) is then performed with the empirical Bellman operator defined as (ˆBh ˆVh+1)(s, a) = ⟨φ(s, a), ˆwh⟩ and a two-part penalty term Γh(s, a) = Γα h(s, a) illustrated in the following. Penalties to Aggregate Sample Uncertainties. The first part of the penalty term aggregates the sample uncertainties from each data source, which is designed as\n\nh (s, a)+Γβ\n\n(cid:114)\n\nΓα\n\nh (s, a) = c\n\n(cid:114)(cid:88)\n\ndH 2 L2\n\nl∈[L]\n\n∥φ(s, a)∥2\n\nΛ−1\n\nh,l\n\n.\n\nWhile this penalty term shares a similar format as those in linear bandits (Li et al., 2010; AbbasiYadkori et al., 2011) and linear MDPs (Jin et al., 2020; 2021b), its novelty comes from jointly considering the uncertainties of different sources. Especially, this design avoids applying the selfnormalized concentration (Abbasi-Yadkori et al., 2011) to each data source (which leads to an inferior dependency on L) by leveraging the statistical independence among the split datasets.\n\nPenalties to Account for Source Uncertainties. Besides the sample uncertainties, the following penalty is designed to measure the source uncertainties:\n\nΓβ\n\nh(s, a) = c\n\n(cid:114)\n\ndH 2 L\n\n.\n\nNote that compared with the tabular design, an additional d factor appears, which is from the covering argument over the d-dimensional feature space.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5.2 THEORETICAL ANALYSIS\n\nSimilar to the tabular setting, previous offline RL studies with linear MDPs often requires the behavior policy to cover the optimal policy (Jin et al., 2021b; Xiong et al., 2022) (or even stronger, to cover the entire feature space (Yin et al., 2022; Min et al., 2021)). Hence, following Sec. 4, we first discuss the scenario with good individual coverage characterized by the following assumption. Assumption 5 (Individual Coverage; Linear MDP). There exists a constant D∗ < ∞ it holds that Eπ∗,M[φ(sh, ah)φ(sh, ah)⊤] ⪯ such that D∗Eρl,Ml [φ(sh, ah)φ(sh, ah)⊤]. Theorem 4 (HetPEVI-Lin; Individual Coverage). Under Assumptions 4, 5 and assuming the matrices {(cid:80) h,l : h ∈ [H]} are invertible, w.p. at least 1 − δ, the output policy ˆπ of HetPEVI-Lin satisfies\n\nfor all (h, l) ∈ [H] × [L],\n\nl∈[L] Λ−1\n\nGap(ˆπ; M) = ̃O\n\n(cid:32)(cid:114)\n\nD∗d2H 4 KL\n\n+\n\n(cid:114)\n\n(cid:33)\n\n.\n\ndH 4 L\n\nSimilar to Thm. 2, the two terms in Thm. 4 originate from the finite data samples and the limited data sources, respectively. However, note that the gap guarantee in Thm. 4 does not have a dependency on the number of states S (which appears in the tabular analysis), instead it depends on the feature dimension d, thanks to the careful design that incorporates linear function approximation. As a result, to output an ε-optimal policy, the sample complexity requirement is T = KL = ̃O(D∗d2H 4/ε2) while the source diversity requirement is L = ̃O(dH 4/ε2).\n\nOne important observation from the tabular setting is that efficient learning only requires a good collective (instead of individual) coverage. To further verify this claim, the following collective coverage assumption is considered for linear MDPs, which shares a similar format as Assumption 3. Assumption 6 (Collective Coverage; Linear MDP). There exist constants L† > 0 and D† < : Eπ∗,M[φ(sh, ah)φ(sh, ah)⊤] ⪯ ∞ such that for all h ∈ [H], it holds that |l ∈ [L] D∗Eρl,Ml [φ(sh, ah)φ(sh, ah)⊤]| ≥ L†.\n\nIt can be observed that Assumption 5 implies Assumption 6 with L† = L and D† = D∗. With this relatively weaker collective coverage assumption, the following theorem can be established.\n\nTheorem 5 (HetPEVI-Lin; Collective Coverage). Under Assumptions 4, 6 and assuming the matrices {(cid:80) h,l : h ∈ [H]} are invertible, w.p. at least 1 − δ, the output policy ˆπ of HetPEVI-Lin satisfies\n\nl∈[L] Λ−1\n\nGap(ˆπ; M) = ̃O\n\n(cid:32)(cid:114)\n\n(cid:114)\n\nD†d2H 4 KL\n\n+\n\nd(L + 1 − L†)H 4 L\n\n(cid:33)\n\n.\n\nIt can be observed that Thm. 3 shares a similar form as Thm. 3, both of which illustrate that as long as the datasets collectively cover the optimal policy, efficient learning is achievable.\n\n6 CONCLUSIONS\n\nThis work studied a novel problem of efficient offline RL with randomly perturbed data sources. In particular, motivated by practical applications, the available offline datasets are assumed to be collected from multiple randomly perturbed versions of the target MDP. The HetPEVI algorithm is proposed, where novel penalty terms were designed to jointly consider the uncertainties from the finite data samples and the limited amount of data sources. Theoretical analyses proved its nearoptimal performance. More importantly, the costs and benefits of offline RL via randomly perturbed data sources are explicitly characterized. On one hand, an additional unavoidable performance loss occurs due to the finite data sources, which cannot be reduced by involving more data samples from each source. On the other hand, as long as the datasets collectively (instead of individually) provide a good data coverage, efficient learning is achievable. Lastly, linear function approximation was considered, and the HetPEVI-Lin algorithm was developed with penalties carefully designed to consider both uncertainties and incorporate the linear structure. Its analysis again verifies the importance of source diversity and the sufficiency of collective coverage.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAbbasi-Yadkori, Y., P ́al, D., & Szepesv ́ari, C. (2011).\n\nImproved algorithms for linear stochastic\n\nbandits. Advances in neural information processing systems, 24.\n\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2019). Reinforcement learning: Theory and\n\nalgorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep.\n\nAzar, M. G., Osband, I., & Munos, R. (2017). Minimax regret bounds for reinforcement learning.\n\nIn International Conference on Machine Learning (pp. 263–272).: PMLR.\n\nBrunskill, E. & Li, L. (2013). Sample complexity of multi-task reinforcement learning. In Uncer-\n\ntainty in Artificial Intelligence (pp. 122).: Citeseer.\n\nChua, K., Lei, Q., & Lee, J. D. (2021). Provable hierarchy-based meta-reinforcement learning. arXiv\n\npreprint arXiv:2110.09507.\n\nDann, C., Mohri, M., Zhang, T., & Zimmert, J. (2021). A provably efficient model-free posterior sampling method for episodic reinforcement learning. Advances in Neural Information Processing Systems, 34.\n\nDorfman, R., Shenfeld, I., & Tamar, A. (2021). Offline meta reinforcement learning–identifiability challenges and effective data collection strategies. Advances in Neural Information Processing Systems, 34.\n\nDuan, Y., Jin, C., & Li, Z. (2021). Risk bounds and rademacher complexity in batch reinforcement\n\nlearning. In International Conference on Machine Learning (pp. 2892–2902).: PMLR.\n\nFallah, A., Georgiev, K., Mokhtari, A., & Ozdaglar, A. (2021). On the convergence theory of debiased model-agnostic meta-reinforcement learning. Advances in Neural Information Processing Systems, 34.\n\nGupta, A., Mendonca, R., Liu, Y., Abbeel, P., & Levine, S. (2018). Meta-reinforcement learning of\n\nstructured exploration strategies. Advances in neural information processing systems, 31.\n\nHallak, A., Di Castro, D., & Mannor, S. (2015). Contextual markov decision processes. arXiv\n\npreprint arXiv:1502.02259.\n\nHuang, R., Wu, W., Yang, J., & Shen, C. (2021). Federated linear contextual bandits. In Proceedings\n\nof the 35th Conference on Neural Information Processing Systems (NeurIPS).\n\nJaques, N., Shen, J. H., Ghandeharioun, A., Ferguson, C., Lapedriza, `A., Jones, N., Gu, S., & Picard, R. W. (2020). Human-centric dialog training via offline reinforcement learning. In EMNLP (1).\n\nJiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., & Schapire, R. E. (2017). Contextual In International Conference on\n\ndecision processes with low bellman rank are pac-learnable. Machine Learning (pp. 1704–1713).: PMLR.\n\nJin, C., Allen-Zhu, Z., Bubeck, S., & Jordan, M. I. (2018). Is q-learning provably efficient? Ad-\n\nvances in neural information processing systems, 31.\n\nJin, C., Liu, Q., & Miryoosefi, S. (2021a). Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. Advances in Neural Information Processing Systems, 34.\n\nJin, C., Yang, Z., Wang, Z., & Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory (pp. 2137–2143).: PMLR.\n\nJin, H., Peng, Y., Yang, W., Wang, S., & Zhang, Z. (2022). Federated reinforcement learning with environment heterogeneity. In International Conference on Artificial Intelligence and Statistics (pp. 18–37).: PMLR.\n\nJin, Y., Yang, Z., & Wang, Z. (2021b). Is pessimism provably efficient for offline rl? In International\n\nConference on Machine Learning (pp. 5084–5096).: PMLR.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKidambi, R., Rajeswaran, A., Netrapalli, P., & Joachims, T. (2020). Morel: Model-based offline reinforcement learning. Advances in neural information processing systems, 33, 21810–21823.\n\nKumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline reinforce-\n\nment learning. Advances in Neural Information Processing Systems, 33, 1179–1191.\n\nKwon, J., Efroni, Y., Caramanis, C., & Mannor, S. (2021a). Reinforcement learning in reward-\n\nmixing mdps. Advances in Neural Information Processing Systems, 34.\n\nKwon, J., Efroni, Y., Caramanis, C., & Mannor, S. (2021b). Rl for latent mdps: Regret guarantees\n\nand a lower bound. Advances in Neural Information Processing Systems, 34.\n\nLange, S., Gabel, T., & Riedmiller, M. (2012). Batch reinforcement learning. In Reinforcement\n\nlearning (pp. 45–73). Springer.\n\nLevine, S., Kumar, A., Tucker, G., & Fu, J. (2020). Offline reinforcement learning: Tutorial, review,\n\nand perspectives on open problems. arXiv preprint arXiv:2005.01643.\n\nLi, C. & Wang, H. (2022). Asynchronous upper confidence bound algorithms for federated linear bandits. In International Conference on Artificial Intelligence and Statistics (pp. 6529–6553).: PMLR.\n\nLi, G., Shi, L., Chen, Y., Chi, Y., & Wei, Y. (2022). Settling the sample complexity of model-based\n\noffline reinforcement learning. arXiv preprint arXiv:2204.05275.\n\nLi, J., Vuong, Q., Liu, S., Liu, M., Ciosek, K., Christensen, H., & Su, H. (2020a). Multi-task batch reinforcement learning with metric learning. Advances in Neural Information Processing Systems, 33, 6197–6210.\n\nLi, L., Chu, W., Langford, J., & Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web (pp. 661–670).\n\nLi, L., Yang, R., & Luo, D. (2020b). Focal: Efficient fully-offline meta-reinforcement learning via\n\ndistance metric learning and behavior regularization. arXiv preprint arXiv:2010.01112.\n\nLin, S., Wan, J., Xu, T., Liang, Y., & Zhang, J. (2022). Model-based offline meta-reinforcement\n\nlearning with regularization. arXiv preprint arXiv:2202.02929.\n\nLu, R., Huang, G., & Du, S. S. (2021). On the power of multitask representation learning in linear\n\nmdp. arXiv preprint arXiv:2106.08053.\n\nMarchal, O. & Arbel, J. (2017). On the sub-gaussianity of the beta and dirichlet distributions.\n\nElectronic Communications in Probability, 22, 1–14.\n\nMaurer, A. & Pontil, M. (2009). Empirical bernstein bounds and sample variance penalization.\n\narXiv preprint arXiv:0907.3740.\n\nMin, Y., Wang, T., Zhou, D., & Gu, Q. (2021). Variance-aware off-policy evaluation with linear\n\nfunction approximation. Advances in neural information processing systems, 34.\n\nMirsky, L. (1975). A trace inequality of john von neumann. Monatshefte f ̈ur mathematik, 79(4),\n\n303–306.\n\nMitchell, E., Rafailov, R., Peng, X. B., Levine, S., & Finn, C. (2021). Offline meta-reinforcement learning with advantage weighting. In International Conference on Machine Learning (pp. 7780– 7791).: PMLR.\n\nPanaganti, K. & Kalathil, D. (2022). Sample complexity of robust reinforcement learning with a generative model. In International Conference on Artificial Intelligence and Statistics (pp. 9582– 9602).: PMLR.\n\nPanaganti, K., Xu, Z., Kalathil, D., & Ghavamzadeh, M. (2022). Robust reinforcement learning\n\nusing offline data. arXiv preprint arXiv:2208.05129.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nRashidinejad, P., Zhu, B., Ma, C., Jiao, J., & Russell, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34.\n\nSæmundsson, S., Hofmann, K., & Deisenroth, M. (2018). Meta reinforcement learning with laIn 34th Conference on Uncertainty in Artificial Intelligence tent variable gaussian processes. 2018, UAI 2018, volume 34 (pp. 642–652).: Association for Uncertainty in Artificial Intelligence (AUAI).\n\nSallab, A. E., Abdou, M., Perot, E., & Yogamani, S. (2017). Deep reinforcement learning framework\n\nfor autonomous driving. Electronic Imaging, 2017(19), 70–76.\n\nShi, C. & Shen, C. (2021). Federated multi-armed bandits. In Proceedings of the 35th AAAI Con-\n\nference on Artificial Intelligence (AAAI).\n\nShi, L. & Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with\n\nnear-optimal sample complexity. arXiv preprint arXiv:2208.05767.\n\nShi, L., Li, G., Wei, Y., Chen, Y., & Chi, Y. (2022). Pessimistic q-learning for offline reinforcement\n\nlearning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890.\n\nShrestha, A., Lee, S., Tadepalli, P., & Fern, A. (2020). Deepaveragers: offline reinforcement learning\n\nby solving derived non-parametric mdps. arXiv preprint arXiv:2010.08891.\n\nSi, N., Zhang, F., Zhou, Z., & Blanchet, J. (2020). Distributionally robust policy evaluation and In International Conference on Machine Learning (pp.\n\nlearning in offline contextual bandits. 8884–8894).: PMLR.\n\nSutton, R. S. & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n\nTang, S. & Wiens, J. (2021). Model selection for offline reinforcement learning: Practical considerations for healthcare settings. In Machine Learning for Healthcare Conference (pp. 2–35).: PMLR.\n\nTeh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., & Pascanu, R. (2017). Distral: Robust multitask reinforcement learning. Advances in neural information processing systems, 30.\n\nUehara, M. & Sun, W. (2021). Pessimistic model-based offline reinforcement learning under partial\n\ncoverage. arXiv preprint arXiv:2107.06226.\n\nVershynin, R. (2018). High-dimensional probability: An introduction with applications in data\n\nscience, volume 47. Cambridge university press.\n\nWainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48.\n\nCambridge University Press.\n\nXie, T., Cheng, C.-A., Jiang, N., Mineiro, P., & Agarwal, A. (2021a). Bellman-consistent pessimism\n\nfor offline reinforcement learning. Advances in neural information processing systems, 34.\n\nXie, T., Jiang, N., Wang, H., Xiong, C., & Bai, Y. (2021b). Policy finetuning: Bridging sampleefficient offline and online reinforcement learning. Advances in neural information processing systems, 34.\n\nXiong, W., Zhong, H., Shi, C., Shen, C., Wang, L., & Zhang, T. (2022). Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game. arXiv preprint arXiv:2205.15512.\n\nYang, J., Hu, W., Lee, J. D., & Du, S. S. (2020). Impact of representation learning in linear bandits.\n\nIn International Conference on Learning Representations.\n\nYang, J., Lei, Q., Lee, J. D., & Du, S. S. (2022). Nearly minimax algorithms for linear bandits with\n\nshared representation. arXiv preprint arXiv:2203.15664.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYang, W., Zhang, L., & Zhang, Z. (2021). Towards theoretical understandings of robust markov\n\ndecision processes: Sample complexity and asymptotics. arXiv preprint arXiv:2105.03863.\n\nYin, M., Duan, Y., Wang, M., & Wang, Y.-X. (2022). Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. arXiv preprint arXiv:2203.05804.\n\nYu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine, S., & Finn, C. (2021). Conservative data sharing for multi-task offline reinforcement learning. Advances in Neural Information Processing Systems, 34.\n\nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., & Ma, T. (2020). Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33, 14129–14142.\n\nZanette, A., Wainwright, M. J., & Brunskill, E. (2021). Provable benefits of actor-critic methods for\n\noffline reinforcement learning. Advances in neural information processing systems, 34.\n\nZhang, C. & Wang, Z. (2021). Provably efficient multi-task reinforcement learning with model\n\ntransfer. Advances in Neural Information Processing Systems, 34.\n\nZhang, K., Yang, Z., & Bas ̧ar, T. (2021). Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control, (pp. 321–384).\n\nZhang, Z., Zhou, Y., & Ji, X. (2020). Almost optimal model-free reinforcement learning via reference-advantage decomposition. Advances in Neural Information Processing Systems, 33, 15198–15207.\n\nZhou, Z., Zhou, Z., Bai, Q., Qiu, L., Blanchet, J., & Glynn, P. (2021). Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In International Conference on Artificial Intelligence and Statistics (pp. 3331–3339).: PMLR.\n\nZhu, R. & Kveton, B. (2022). Random effect bandits. In International Conference on Artificial\n\nIntelligence and Statistics (pp. 3091–3107).: PMLR.\n\nZhuo, H. H., Feng, W., Lin, Y., Xu, Q., & Yang, Q. (2019). Federated deep reinforcement learning.\n\narXiv preprint arXiv:1901.08277.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA ADDITIONAL DISCUSSIONS\n\nA.1 RELATED WORKS\n\nRL has seen much progress over the past few years, particularly in its theoretical understanding (see the recent monograph (Agarwal et al., 2019) for an overview). We will discuss the most related papers in the following, with a particular focus on the theoretical aspect as well as the offline setting.\n\nOnline RL with One Environment. The majority of RL studies focus on the online setting (Sutton & Barto, 2018), where the agent consistently improves her algorithm via online interactions with the environment. In recent years, many advances have deepened the theoretical understandings of online RL in interacting with a fixed and stable environment, with provably efficient designs (Azar et al., 2017; Jin et al., 2018; 2020; 2021a; Zhang et al., 2020; Jiang et al., 2017; Dann et al., 2021).\n\nOnline RL with Heterogeneous Environments. The importance of handling heterogeneous environments is well recognized. Especially, federated RL (Zhuo et al., 2019) and the general multiagent RL (Zhang et al., 2021) study the setting with multiple agents in the system, while each of them may view an agent-depend environment (Jin et al., 2022). In contrast, meta-RL (Sæmundsson et al., 2018; Gupta et al., 2018; Fallah et al., 2021; Chua et al., 2021) focuses on extracting knowledge from previous tasks and adapting them to new (potentially different) tasks. Also, multi-task RL (Teh et al., 2017) attempts to handle multiple tasks together via information shared among tasks (Brunskill & Li, 2013; Zhang & Wang, 2021; Lu et al., 2021; Yang et al., 2020; 2022) or distinguishing the latent structure associated with each task via past observations (Hallak et al., 2015; Kwon et al., 2021a;b).\n\nOffline RL with Single-source Datasets. Offline RL (Levine et al., 2020) attempts to avoid potentially expensive interactions with the environment as in online RL but instead use available offline datasets collected previously. Inspired by empirical advances (Yu et al., 2020; Kumar et al., 2020), the principle of “pessimism” is incorporated and proved efficient for offline RL (Jin et al., 2021b; Rashidinejad et al., 2021). Especially, it is illustrated that a nearly optimal policy can be learned via a dataset collected by a behavior policy that covers the trajectories of the optimal policy.\n\nFollowing this line, Xie et al. (2021b); Li et al. (2022); Shi et al. (2022) further fine-tune the designs for the tabular setting, and Jin et al. (2021b); Zanette et al. (2021); Min et al. (2021); Yin et al. (2022); Xiong et al. (2022) for the linear MDP. For the general function approximation, additional attempts are reported in Xie et al. (2021a); Uehara & Sun (2021). However, these results are mostly information-theoretical as computational intractable optimizations are required in general.\n\nBesides the above discussions, one particular line of research that relates to this work is under the category of offline robust RL (Zhou et al., 2021; Yang et al., 2021; Panaganti et al., 2022; Panaganti & Kalathil, 2022; Si et al., 2020; Shi & Chi, 2022). Especially, offline robust RL learns via data sampled from one MDP and tries to output a policy that is robustly good for a family of MDPs; however, our work attempts at learning the hidden task from data sampled from a family of MDPs. Nevertheless, we note that it would be an interesting question to investigate whether having access to multiple data source MDPs in the target family would make offline robust RL easier.\n\nAnother conceptually relative work is Shrestha et al. (2020). In particular, it looks for similar stateaction pairs with small distances in the dataset, which can be thought of as available data sources in this work. Then, the Lipschitz continuity assumption is posed, which serves a similar role as Assumption 1 to establish the connection between desired task information with the available datasets. From this perspective, the first term in Theorem 3.1 (Shrestha et al., 2020) can be interpreted as coming from the source uncertainty while the second term is from the sample uncertainty. However, we also note that the Lipschitz continuity assumption is a worst-case consideration that would not characterize the concentration of involving more data sources, which however is the key of this work.\n\nOffline RL with Multi-source Datasets. The aforementioned advances on offline RL are mainly focused on learning from a single data source, which limits their applicability. In the offline domain, growing interests have been made to utilize data from heterogeneous sources. The most related literature falls under the framework of “offline meta-RL” or “offline multi-task RL” (Mitchell et al., 2021; Dorfman et al., 2021; Lin et al., 2022; Li et al., 2020b;a; Yu et al., 2021). Especially, the target MDP can be viewed as a learning target for the “meta-training” process of offline meta-\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nRL (Mitchell et al., 2021), which aims to extract information from the available data (of multiple sources). In addition to “meta-training”, the empirically studied offline meta-RL systems often feature another step of “meta-testing”, which further utilizes the learned information and applies them to a specific task. Thus, we believe this work may contribute the (currently lacking) theoretical understanding of offline meta-RL systems, especially the meta-training process, which may also serve as the foundation for studies on the meta-testing process.\n\nA.2 FUTURE WORKS\n\nCoverage Assumptions. While the collective coverage assumption of Assumptions 3 and 6 is relatively weak, it is still of major interest to further explore how to perform offline RL (especially with heterogeneous data sources) under weaker assumptions. This direction is particularly interesting with multiple data sources since the heterogeneous sources naturally enrich the data diversity.\n\nUnknown Source Identities. This work considers the scenario where each data sample is known to belong to a particular source. One interesting direction is to investigate the scenarios without such information, i.e., unknown source identities. A potential solution is to first cluster the data samples and then adopt the algorithms proposed in this work. However, it is challenging to design provable clustering algorithms. One candidate clustering technique is developed in Kwon et al. (2021b) for the study of latent MDP, which however relies on strong assumptions of prior knowledge about the source MDPs.\n\nPersonalization. As mentioned in the discussions of related work, this work can be viewed as targeting at the “meta-training” process of offline meta-RL (Mitchell et al., 2021), which extracts common knowledge from available data of multiple sources. While the extracted common knowledge have individual values, in many applications, an additional step of personalization is performed to further use such knowledge to benefit a specific task, which is called the “meta-testing” process of offline meta-RL (Mitchell et al., 2021). Based on this work, it would be valuable to further study how to perform such a personalization step with theoretical guarantees.\n\nB LOWER BOUND ANALYSIS\n\nLemma 3. For any C ∗ ≥ 2, it holds that\n\ninf ˆπ\n\nsup M∈M,g∈G,{ρl:l∈[L]}∈B(C∗)\n\nE{Ml,Dl:l∈[L]} [Gap(ˆπ; M)] = Ω\n\n(cid:32)(cid:114)\n\n(cid:33)\n\n.\n\nC ∗SH 3 LK\n\nProof. We consider the case where g always generates Ml = M, and ρ1 = · · · = ρL = ρ. Then, the problem degenerates to offline RL with one dataset directly from the target. Thus, results from the case of C ∗ ≥ 2 in Theorem 7 of Rashidinejad et al. (2021) can be applied to obtain the final result.\n\nLemma 4. For any C ∗ ≥ 2, it holds that\n\ninf ˆπ\n\nsup M∈M,g∈G,{ρl:l∈[L]}∈B(C∗)\n\nE{Ml,Dl:l∈[L]} [Gap(ˆπ; M)] = Ω\n\n(cid:32)(cid:114)\n\n(cid:33)\n\n.\n\nH 2 L\n\nProof. This lemma is established with the following construction.\n\nTarget MDP M. We design the following family of target MDPs with two states denoted as S = {sg, sb} and two actions denotes as A = {ag, ab} for all steps (s1 can be fixed to be sg), which can be easily generalized to accommodate any number of states and actions:\n\nM = (cid:8)P1(sg|sg, ag) = p, Ph(sb|sg, ag) = 1 − p\n\nP1(sg|sg, ab) = P1(sb|sg, ab) = 0.5, ∀h ∈ [H]; Ph(s|s, a) = 1, ∀(s, a, h) ∈ S × A × [2, H]; rh(sg, a) = 1, rh(sb, a) = 0, ∀(a, h) ∈ A × [H](cid:9).\n\nA target MDP M with parameter p in M is referred to as M(p).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nData source generation distribution g. For target M(p), the data source generation, denoted as g(p), is designed as follows: with probability p, the generated MDP has P1(sg|sg, ag) = 1, P1(sb|sg, ag) = 0; otherwise it has P1(sg|sg, ag) = 0, P1(sb|sg, ag) = 1. The other parameters of the generated MDP are the same as M.\n\nBehavior policy ρl. For all l ∈ [L], the behavior policy ρl is specified as follows: ρh,l(a|s) = dπ∗ h (a|s), ∀(s, a, h) ∈ S × A × [H], which ensures Assumption 2 with any C ∗ ≥ 1. Then, with p1 := 1\n\n2 − δ, it can be obtained that E{Ml∼g(p1),Dl∼Ml:l∈[L]} [Gap(ˆπ; M(p1))] = (H − 1)δE{Ml∼g(p1),Dl∼Ml:l∈[L]} [π1(ab)] ; E{Ml∼g(p2),Dl∼Ml:l∈[L]} [Gap(ˆπ; M(p2))] = (H − 1)δE{Ml∼g(p2),Dl∼Ml:l∈[L]} [πh(ag)] ,\n\n2 + δ and p2 = 1\n\nwhich leads to\n\nE{Ml∼g(p1),Dl∼Ml:l∈[L]} [Gap(ˆπ; M(p1))] + E{Ml∼g(p2),Dl∼Ml:l∈[L]} [Gap(ˆπ; M(p2))] = (H − 1)δ (cid:0)E{Ml∼g(p1),Dl∼Ml:l∈[L]} [π1(ab)] + E{Ml∼g(p2),Dl∼Ml:l∈[L]} [1 − π1(ag)](cid:1)\n\nFurthermore, it holds that\n\nE{Ml∼g(p1),Dl∼Ml:l∈[L]} [π1(ab)] + E{Ml∼g(p2),Dl∼Ml:l∈[L]} [1 − π1(ag)] ≥ 1 − TV(P{Ml∼g(p1),Dl∼Ml:l∈[L]}, P{Ml∼g(p2);Dl∼Ml:l∈[L]})\n\n(cid:113)\n\n≥ 1 −\n\nKL (cid:0)P{Ml∼g(p1),Dl∼Ml:l∈[L]}||P{Ml∼g(p2),Dl∼Ml:l∈[L]}\n\n(cid:1) /2.\n\nWe can explicitly write down the ratio between the two desired probabilities as\n\nP{Ml∼g(p1),Dl∼Ml:l∈[L]}({Ml, Dl : l ∈ [L]}) P{Ml∼g(p2),Dl∼Ml:l∈[L]}({Ml, Dl : l ∈ [L]})\n\n=\n\n(p1)κ (1 − p1)L−κ (p2)κ (1 − p2)L−κ\n\nwhere κ = (cid:80)\n\nl∈[L]\n\n1{P1,l(sg|s, ag) = 1}.\n\nAs a result, with pβ\n\nh ∈ [ 1\n\n4 , 3\n\n4 ], ∀h ∈ [H] it holds that\n\nKL (cid:0)P{Ml∼g(p1),Dl∼Ml:l∈[L]}||P{Ml∼g(p2),Dl∼Ml:l∈[L]} (cid:20)\n\n(cid:1)\n\n= E{Ml∼g(p1),Dl∼Ml:l∈[L]}\n\nκ log\n\n+ (L − κ) log\n\n(cid:19)(cid:21)\n\n(cid:18) 1 − p1 1 − p2\n\n(cid:19)\n\n(cid:18) p1 p2 (cid:18) 1 − p1 1 − p2\n\n(cid:19)\n\n+ (L − Lp1) log\n\n(cid:19)\n\n= Lp1 log\n\n(cid:18) p1 p2 L(p1 − p2)2 p2(1 − p2) ≤ 16L(p1 − p2)2\n\n≤\n\nFinally, with δ = 1\n\n16\n\n(cid:113) 2\n\nL , it holds that\n\nE{Ml∼g(pα),Dl∼Ml:l∈[L]} [πh(ab)] + E{Ml∼g(pβ ),Dl∼Ml:l∈[L]} [1 − πh(ag)] KL (cid:0)P{Ml∼g(pα),Dl∼Ml:l∈[L]}||P{Ml∼g(pβ ),Dl∼Ml:l∈[L]}\n\n≥ 1 −\n\n(cid:1) /2\n\n(cid:113)\n\n≥\n\n1 2\n\nwhich concludes the proof as\n\nE{Ml∼g(p1),Dl∼Ml:l∈[L]} [Gap(ˆπ; M(p1))] + E{Ml∼g(p2),Dl∼Ml:l∈[L]} [Gap(ˆπ; M(p2))]\n\n≥ (H − 1)\n\n(cid:114) 2 L\n\n1 16\n\n1 2\n\n·\n\n= Ω\n\n(cid:18) H 2 L\n\n(cid:19)\n\n.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nC UPPER BOUND ANALYSIS: OVERVIEW\n\nIn this section, we provide an overview with the steps shared in the proofs of each proposed algorithm. In other words, the following results are stated for all three proposed algorithms in general. The more challenging and specific proofs for our designs are illustrated in the following sections. The basic logistics here follow the seminal work of Jin et al. (2021b) in provably efficient offline RL.\n\nThe first step is to establish the validness of the pessimism. For different settings, various styles of pessimism are incorporated. Specifically, in this work, three penalty constructions are adopted in HetPEVI, HetPEVI-Adv and HetPEVI-Lin, respectively. As an abstraction, the validness of pessimism induced by penalties {Γh(s, a) : (s, a, h) ∈ S × A × [H]} is defined in the following. Definition 2 (Validness of Pessimism). For pessimistic value iterations using an estimated Bellman operator ˆBh and penalties {Γh(s, a) : (s, a, h) ∈ S × A × [H]}, a valid pessimism is induced if the following event happens\n\nE :=\n\n(cid:110)(cid:12) (cid:12) (cid:12)\n\n(cid:16)ˆBh ˆVh+1\n\n(cid:17)\n\n(s, a) −\n\n(cid:16)\n\nBh ˆVh+1\n\n(cid:17)\n\n(cid:12) (cid:12) (cid:12) ≤ Γh(s, a), ∀(s, a, h) ∈ S × A × [H] (s, a)\n\n(cid:111)\n\n.\n\nWith a valid pessimism, we can obtain the following lemma.\n\nLemma 5. Suppose that event E in Definition 2 happens, it holds that\n\nζh(s, a) :=\n\n(cid:16)\n\nBh ˆVh+1\n\n(cid:17)\n\n(s, a) − ˆQh(s, a) ∈ [0, 2Γh(s, a)] ,\n\n∀(s, a, h) ∈ S × A × [H].\n\nProof. We can observe that for any (s, a, h) ∈ S × A × [H], with the adopted pessimistic value iteration, it holds that\n\nζh(s, a) =\n\n(cid:16)\n\nBh ˆVh+1\n\n(s, a) − ˆQh(s, a)\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n=\n\nBh ˆVh+1 (cid:110)(cid:16)\n\n≥ min\n\n(s, a) − min\n\n(cid:17)\n\nBh ˆVh+1\n\n(s, a) −\n\n(cid:110)(cid:16)ˆBh ˆVh+1 (cid:16)ˆBh ˆVh+1\n\n(cid:17)\n\n(cid:17)\n\n(s, a) − Γh(s, a), H − h + 1 (cid:17)\n\n(cid:16)\n\n(s, a) + Γh(s, a),\n\nBh ˆVh+1\n\n(cid:111)\n\n(s, a)\n\n(cid:111)+\n\n≥ 0,\n\nwhere the last inequality is due to event E. Similar, for the other direction, it holds that\n\nBh ˆVh+1\n\n(s, a) − ˆQh(s, a)\n\nζh(s, a) =\n\n=\n\n(cid:16)\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\nBh ˆVh+1 (cid:110)(cid:16)\n\n(s, a) − min\n\n≤ max\n\n(cid:17)\n\nBh ˆVh+1\n\n(s, a) −\n\n(cid:17)\n\n(cid:110)(cid:16)ˆBh ˆVh+1 (cid:16)ˆBh ˆVh+1\n\n(s, a) − Γh(s, a), H − h + 1 (cid:17)\n\n(cid:16)\n\n(cid:17)\n\n(s, a) + Γh(s, a),\n\nBh ˆVh+1\n\n(cid:111)+\n\n(s, a) − (H − h + 1)\n\n(cid:111)\n\n≤ 2Γh(s, a),\n\nwhere the last inequality is due to event E and the fact that (Bh ˆVh+1)(s, a) ≤ H − h + 1.\n\nFurthermore, the suboptimality gap on the target MDP M between the output policy ˆπ and the optimal policy π∗ can be generally bounded via the following lemma.\n\nLemma 6. Suppose that event E in Definition 2 happens, it holds that\n\nGap(ˆπ; M) ≤ 2\n\n(cid:88)\n\nh∈[H]\n\nEπ∗,M [Γh(sh, ah)] .\n\nProof. It holds that\n\nGap(ˆπ; M)\n\n(a) = −\n\n(cid:88)\n\nEˆπ,M [ζh(sh, ah)] +\n\nh∈[H]\n\n(cid:88)\n\nh∈[H]\n\nEπ∗,M [ζh(sh, ah)]\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:88)\n\nEπ∗,M\n\n(cid:104) ˆQh(sh, π∗(sh)) − ˆQh(sh, ˆπh(sh))\n\n(cid:105)\n\n+\n\nh∈[H]\n\n(b) ≤ 2\n\n(cid:88)\n\nh∈[H]\n\nEπ∗,M [Γh(sh, ah)] ,\n\nwhere equation (a) is from Lemma 3.1 in (Jin et al., 2021b) (provided as Lemma 23), and inequality (b) is due to Lemma 5 together with the fact that ˆQh(sh, ·) takes maximum at ˆπh(sh) in the proposed algorithms.\n\nIn other words, Gap(ˆπ; M) is bounded via the sum of penalties Γh(sh, ah) along the expected trajectory of optimal policy π∗ on the target MDP M.\n\nD THE HETPEVI ALGORITHM\n\nD.1 GOOD EVENT\n\nLemma 7. The following event holds with probability at least 1 − δ for HetPEVI\n\n(cid:26)\n\nG :=\n\n(i) The penalties {Γh(s, a)} induce a valid pessimism;\n\n(ii) Nh,l(s, a) ≥ cKdρl\n\nh,l(s, a), ∀(l, s, a, h) ∈ [L] × S × A × [H]\n\n(cid:27)\n\n.\n\nProof. Part (I) is from Lemma 8 and part (II) obtained via Lemma 9.\n\nLemma 8. The penalty\n\nΓh(s, a) = c\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\nH 2 (L2Nh,l(s, a)) ∨ L\n\n(cid:114)\n\n+ c\n\nH 2 L\n\nin HetPEVI induces a valid pessimism with probability at least 1 − δ.\n\nProof. For a fixed (s, a, h), it holds that\n\n(cid:16)ˆBh ˆVh+1\n\n(cid:17)\n\n(s, a) −\n\n(cid:16)\n\nBh ˆVh+1\n\n(cid:17)\n\n(s, a)\n\n= ˆrh(s, a) +\n\n(cid:16)ˆPh ˆVh+1\n\n(cid:17)\n\n(cid:88)\n\n=\n\n(cid:16)\n\nrh,l(s, a) +\n\n(s, a) − rh(s, a) − (cid:16)ˆPh,l ˆVh+1\n\n(s, a)\n\n(cid:17)\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n(s, a)\n\nrh,l(s, a) +\n\n(cid:16)\n\nPh,l ˆVh+1\n\n(cid:17)\n\n(cid:17)\n\n(s, a)\n\n(cid:16)\n\nPh ˆVh+1\n\n(cid:88)\n\n−\n\n1 L\n\nl∈[L] (cid:16)\n\n1 L\n\n1 L\n\nl∈[L]\n\n(cid:88)\n\nl∈[L]\n\n+\n\n=\n\n−\n\n+\n\n=\n\n(cid:16)\n\nrh,l(s, a) +\n\n(cid:16)\n\nPh,l ˆVh+1\n\n(cid:17)\n\n(s, a)\n\n(cid:17)\n\n−\n\nrh(s, a) + Ph ˆVh+1(s, a)\n\n(cid:17)\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)>0\n\n\n\nrh,l(s, a) +\n\n1 L\n\n(cid:88)\n\nk∈[K]:(sk\n\nh,l,ak\n\nh,l)=(s,a)\n\nˆVh+1(sk\n\nh+1,l)\n\nNh,l(s, a)\n\n\n\n\n\n(cid:16)\n\n(cid:16)\n\n(cid:88)\n\nl∈[L]\n\n(cid:88)\n\nl∈[L]\n\n1 L\n\n1 L\n\nrh,l(s, a) +\n\nrh,l(s, a) +\n\n(cid:16)\n\n(cid:16)\n\nPh,l ˆVh+1\n\nPh,l ˆVh+1\n\n(cid:17)\n\n(cid:17)\n\n(cid:17)\n\n(cid:17)\n\n(s, a)\n\n(s, a)\n\n(cid:16)\n\n−\n\nrh(s, a) + Ph ˆVh+1(s, a)\n\n(cid:17)\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)>0\n\n1 L\n\n\n\n\n\n(cid:88)\n\nk∈[K]:(sk\n\nh,l,ak\n\nh,l)=(s,a)\n\nˆVh+1(sk\n\nh+1,l)\n\nNh,l(s, a)\n\n(cid:16)\n\n−\n\nPh,l ˆVh+1\n\n(cid:17)\n\n\n\n(s, a)\n\n\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n+\n\n−\n\n(cid:88)\n\nl∈[L]\n\n(cid:16)\n\n1 L\n\nrh,l(s, a) +\n\n(cid:16)\n\nPh,l ˆVh+1\n\n(cid:17)\n\n(s, a)\n\n(cid:16)\n\n(cid:17)\n\n−\n\nrh(s, a) + Ph ˆVh+1(s, a)\n\n(cid:17)\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)=0\n\nrh,l(s, a) +\n\n(cid:16)\n\nPh,l ˆVh+1\n\n(cid:17)\n\n(cid:17)\n\n(s, a)\n\n(cid:16)\n\n1 L\n\nThen, via Hoeffding inequality, it can be recognized that with probability at least 1 − δ, (cid:17)\n\n(cid:16)\n\n(cid:17)\n\n(cid:16)ˆBh ˆVh+1\n\n(cid:12) (cid:12) (cid:12)\n\n(s, a) −\n\nBh ˆVh+1\n\n(s, a)\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n≤ c\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n= c\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n= c\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n= c\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)>0\n\nH 2 L2Nh,l(s, a)\n\n(cid:114)\n\n+ c\n\nH 2 L\n\n+\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)=0\n\nH L\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)>0\n\nH 2 L2Nh,l(s, a)\n\n\n\n+\n\n\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)=0\n\n 2\n\n(cid:114)\n\n\n\n+ c\n\nH L\n\nH 2 L\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)>0\n\nH 2 L2Nh,l(s, a)\n\n+\n\n(cid:88)\n\nl∈[L]:Nh,l(s,a)=0\n\nH 2 L\n\n(cid:114)\n\nH 2 L\n\n+ c\n\n(cid:88)\n\nl∈[L]\n\nH 2 (L2Nh,l(s, a)) ∨ L\n\n(cid:114)\n\nH 2 L\n\n+ c\n\nLemma 9. With probability at least 1 − δ, it holds that, Nh,l(s, a) ≥ cKdρl\n\nh,l(s, a),\n\n∀(l, s, a, h) ∈ [L] × S × A × [H].\n\nProof. The proof can be done similarly as in Lemma B.3 of (Xie et al., 2021b).\n\nD.2 SUBOPTIMAITY GAP\n\nProof of Theorems 2 and 3. By Lemma 6 and 7, with probability at least 1 − δ, it holds that\n\nGap(ˆπ; M) ≤ 2\n\n(cid:88)\n\nh∈[H]\n\nEπ∗,M [Γh(sh, ah)] .\n\nWith the individual coverage assumption (i.e., Assumption 2), it holds that\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\nh∈[H]\n\n(s,a)∈S×A\n\n\n\n \n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\nH 2 (L2Nh,l(s, a)) ∨ L\n\n+\n\n(cid:114)\n\n\n\n \n\nH 2 L\n\n(a) ≤\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\nh∈[H]\n\n(s,a)∈S×A\n\n\n\n \n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\n(cid:16)\n\nl∈[L]\n\nH 2\n\nL2Kdρl\n\nh,l(s, a)\n\n(cid:17)\n\n(cid:114)\n\n+\n\n\n\n \n\nH 2 L\n\n∨ L\n\n(b)\n\n= ̃O\n\n(cid:32)(cid:114)\n\nC ∗H 4S LK\n\n+\n\n(cid:114)\n\n(cid:33)\n\n,\n\nH 4 L\n\nwhere inequality (a) is from (ii) of event G in Lemma 7, and equality (b) uses Assumption 2 and Cauchy-Schwarz inequality.\n\nWith the overall coverage assumption (i.e., Assumption 3), it can be similarly obtained that\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\nh∈[H]\n\n(s,a)∈S×A\n\n\n\n \n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\nH 2 (L2Nh,l(s, a)) ∨ L\n\n+\n\n(cid:114)\n\n\n\n \n\nH 2 L\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:88)\n\n(cid:88)\n\n≤\n\ndπ∗ h (s, a)\n\nh∈[H]\n\n(s,a)∈S×A\n\n\n\n \n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]:d\n\nρl h,l(s,a)>0\n\nH 2 L2Kdρl h,l(s, a)\n\n(cid:114)\n\n\n\n \n\nH 2 L\n\nH 2 L\n\n+\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+\n\nl∈[L]:d\n\n(cid:88)\n\nρl h,l(s,a)=0 (cid:114)\n\nH 4 L\n\n(cid:114)\n\n(c) ≤\n\nC †H 2 LK\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\n(cid:113)\n\n1{a = π∗\n\nh(s)}dπ∗\n\nh (s, a) +\n\n(cid:114)\n\n(L − L†)H 4 L\n\n+\n\n(cid:32)(cid:114)\n\n= ̃O\n\n(cid:114)\n\nC †H 4S LK\n\n+\n\n(L + 1 − L†)H 4 L\n\n(cid:33)\n\n,\n\nwhere inequality (c) is from Assumption 3.\n\nE THE HETPEVI-ADV ALGORITHM\n\nE.1 ALGORITHM DESIGN\n\nHetPEVI-Adv is designed as an enhanced version of HetPEVI with a Bernstein-type penalty term for the sample uncertainties. Especially, it shares the same procedure as HetPEVI except that the adopted penalty is\n\nΓα\n\nh (s, a) = c\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:0) ˆVh,l ˆVh+1\n\n(cid:1)(s, a)\n\n(cid:88)\n\nL2Nh,l(s, a)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+ c\n\n(cid:88)\n\nl∈[L]\n\nH 2\n\nL2(Nh,l(s, a))2 ,\n\nwhere (cid:0) ˆVh,l ˆVh+1 compared with HetPEVI, the variance information is incorporated in HetPEVI-Adv.\n\n(cid:1)(s, a) is the empirical variance of ˆVh+1(s′) with s′ ∼ ˆPh,l(·|s, a). Note that\n\nE.2 THEORETICAL ANALYSIS\n\nTheorem 6. Under Assumptions 1, 2, w.p. at least 1 − δ, the output policy ˆπ of HetPEVI-Adv satisfies\n\nGap(ˆπ; M) = ̃O\n\n(cid:32)(cid:114)\n\nC ∗H 3S LK\n\n+\n\nC ∗H 3S √\n\nLK\n\n+\n\n(cid:114)\n\n(cid:114)\n\n(cid:33)\n\nC ∗H 7S L2K\n\n+\n\nH 4 L\n\n.\n\n(3)\n\nIt is noted that the first three terms come from finite samples and the last term from finite sources. Furthermore, when LK is sufficiently larger, the first term dominates the other second and third terms; thus, in this regime, it can be observed that HetPEVI-Adv has a tight performance loss to finite samples. However, the performance loss due to finite data sources still has an additional H factor, which is left open for further investigations.\n\nE.3 GOOD EVENT\n\nLemma 10. Under Assumptions 1, 2, with probability at least 1 − δ, the following good event G happens, where\n\n(cid:26)\n\nG :=\n\n(i) The penalties {Γh(s, a) : ∀(s, a, h) ∈ S × A × [H]} induce a valid pessimism;\n\n(ii) V ∗\n\nh (s) − ˆVh(s) = ̃O\n\n(cid:32)(cid:114)\n\nC ∗H 4S KL\n\n+\n\n(cid:114)\n\n(cid:33)\n\nH 4 L\n\n, ∀(s, h) ∈ S × [H];\n\n(iii) Nh,l(s, a) ≥ cKdρl\n\nh,l(s, a), ∀(l, s, a, h) ∈ [L] × S × A × [H];\n\n(cid:16) ˆVh,lf\n\n(cid:17)\n\n(iv)\n\n(s, a) ≤ (Vh,lf ) (s, a) + c\n\n(cid:115)\n\nH 4 Nh,l(s, a)\n\n, ∀(l, s, a, h) ∈ [L] × S × A × [H];\n\n(vii) (cid:0) ̄Vhf (cid:1) (s, a) ≤ (Vhf ) (s, a) +\n\n(cid:114)\n\nH 4 L\n\n, ∀(s, a, h) ∈ S × A × [H]\n\n,\n\n(cid:27)\n\nwhere f is any fixed function: S × A → [−H, H].\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Part (i) can be obtained similarly as Lemma 8 using the enhanced Bernstein inequality proved in Lemma 26, and the rest parts are established in the following lemmas.\n\nLemma 11. Under Assumptions 1, 2, with a probability of at least 1 − δ, the following crude bound of the output value of HetPEVI-Adv holds\n\nV ∗\n\nh (s) − ˆVh(s) = ̃O\n\n(cid:32)(cid:114)\n\nC ∗H 4S KL\n\n+\n\n(cid:114)\n\n(cid:33)\n\n,\n\nH 4 L\n\n∀(s, a) ∈ S × [H]\n\nProof. It can be observed that\n\nΓα\n\nh (s, a) = c\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:0) ˆVh,l ˆVh+1\n\n(cid:1)(s, a)\n\n(cid:88)\n\nL2Nh,l(s, a)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+ c\n\n(cid:88)\n\nl∈[L]\n\nH 2\n\nL2(Nh,l(s, a))2 ≤ c\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\nH 2 L2Nh,l(s, a)\n\nUsing the above upper bound of Γh(s, a), the lemma can be obtained via similar steps of Theorem 2.\n\nLemma 12. With probability at least 1 − δ, the following events happen,\n\nNh,l(s, a) ≥ cKdρl\n\nh,l(s, a),\n\n∀(l, s, a, h) ∈ [L] × S × A × [H]\n\nProof. The proof can be done similarly as in Lemma B.3 of (Xie et al., 2021b).\n\nLemma 13. With probability at least 1 − δ, for a fix functions f : S × A → [−H, H], the following events happen\n\n(cid:16) ˆVh,lf\n\n(cid:17)\n\n(s, a) ≤ (Vh,lf ) (s, a) + c\n\n(cid:115)\n\nH 4 Nh,l(s, a)\n\n,\n\n∀(l, s, a, h) ∈ [L] × S × A × [H].\n\nProof. With a union bound over (s, a, h) ∈ S × A × [H], according to Hoeffding inequality, with probability at least 1 − δ, it holds that\n\n(cid:16) ˆVh,lf\n\n(cid:17)\n\n(s, a) − (Vh,lf ) (s, a) f 2(cid:105)\n\n(s, a) +\n\n(cid:17)\n\n(cid:104)(cid:16)ˆPh,l − Ph,l\n\n=\n\n(cid:104)(cid:16)\n\nPh,l − ˆPh,l\n\n(cid:17)\n\n(cid:105)\n\nf\n\n(s, a) ·\n\n(cid:104)(cid:16)\n\nPh,l + ˆPh,l\n\n(cid:17)\n\n(cid:105)\n\nf\n\n(s, a)\n\n(cid:115)\n\n≤ c\n\nH 4 Nh,l(s, a)\n\n,\n\nwhich concludes the proof.\n\nLemma 14. With probability at least 1 − δ, for a fix functions f : S × A → [−H, H], the following events happen\n\n(cid:0) ̄Vhf (cid:1) (s, a) ≤ (Vhf ) (s, a) + c\n\n(cid:114)\n\nH 4 L\n\n,\n\n∀(s, a, h) ∈ S × A × [H].\n\nProof. With a union bound over (s, a, h) ∈ S × A × [H], according to Hoeffding inequality, with probability at least 1 − δ, it holds that\n\n(cid:0) ̄Vh,lf (cid:1) (s, a) − (Vh,lf ) (s, a) = (cid:2)(cid:0) ̄Ph − Ph\n\n(cid:1) f 2(cid:3) (s, a) + (cid:2)(cid:0) ̄Ph − Ph\n\n(cid:1) f (cid:3) (s, a) · (cid:2)(cid:0) ̄Ph + Ph\n\n(cid:1) f (cid:3) (s, a)\n\n(cid:114)\n\n≤ c\n\nH 4 L\n\n,\n\nwhich concludes the proof.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nE.4 SUBOPTIMAITY GAP\n\nLemma 15. It holds that\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\ndπ∗ h (s, a)[VhV ∗\n\nh+1](s, a) ≤ H 2.\n\nProof. The proof can be found in Lemma C.4 in Xie et al. (2021b).\n\nLemma 16. It holds that\n\n(cid:0)Vh,lV ∗\n\nh+1\n\n(cid:1) (s, a) ≤ L (cid:0) ̄VhV ∗\n\nh+1\n\n(cid:1) (s, a).\n\n(cid:88)\n\nl∈[L]\n\nProof. This lemma is a direct consequence of Lemma 28.\n\nLemma 17. With event G happening, it holds that\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\n(cid:16)\n\nVh,l ˆVh+1\n\n(cid:17)\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n(s, a) ≤ LH 2 + c\n\n(cid:114)\n\nC ∗H 8SL K\n\n√\n\n+ c\n\nH 8L.\n\nProof. First, the left-hand side can be decomposed as\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\n(cid:16)\n\nVh,l ˆVh+1\n\n(cid:17)\n\n(s, a)\n\nh∈[H]\n\n(s,a)∈S×A (cid:88)\n\n(cid:88)\n\nl∈[L]\n\nh (s, a)L (cid:0)VhV ∗ dπ∗\n\nh+1\n\n(cid:1) (s, a)\n\n=\n\n+\n\n+\n\n+\n\n(s,a)∈S×A\n\nh∈[H] (cid:124)\n\n(cid:88)\n\n(cid:88)\n\n(s,a)∈S×A\n\nh∈[H] (cid:124)\n\n(cid:123)(cid:122) :=term (I)\n\n(cid:125)\n\nh (s, a)L (cid:0)(cid:0) ̄VhV ∗ dπ∗\n\nh+1\n\n(cid:1) (s, a) − (cid:0)VhV ∗\n\nh+1\n\n(cid:1) (s, a)(cid:1)\n\n(cid:123)(cid:122) :=term (II)\n\n(cid:125)\n\n\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\n(s,a)∈S×A\n\nh∈[H] (cid:124)\n\n\n\n\n\n(cid:88)\n\nl∈[L]\n\n(cid:0)Vh,lV ∗\n\nh+1\n\n(cid:1) (s, a) − L (cid:0) ̄VhV ∗\n\nh+1\n\n(cid:1) (s, a)\n\n\n\n(cid:123)(cid:122) :=term (III)\n\n(cid:125)\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\n(cid:104)(cid:16)\n\nVh,l ˆVh+1\n\n(cid:17)\n\n(s, a) − (cid:0)Vh,lV ∗\n\nh+1\n\n(cid:105) (cid:1) (s, a)\n\n(s,a)∈S×A\n\nl∈[L]\n\nh∈[H] (cid:124)\n\n(cid:123)(cid:122) :=term (IV)\n\n(cid:125)\n\nThen, for term (I), with Lemma 15, it holds that\n\nterm (I) ≤ LH 2.\n\nFor term (II), with event (vii) in Lemma 10 it holds that\n\nterm (II) =\n\n(cid:88)\n\n(cid:88)\n\nh (s, a)L (cid:0)(cid:0) ̄VhV ∗ dπ∗\n\nh+1\n\n(cid:1) (s, a) − (cid:0)VhV ∗\n\nh+1\n\n(cid:1) (s, a)(cid:1)\n\nh∈[H]\n\n(s,a)∈S×A\n\n(cid:88)\n\n(cid:88)\n\n≤ c\n\n(s,a)∈S×A\n\nh∈[H] √\n\n= c\n\nH 6L\n\n(cid:114)\n\ndπ∗ h (s, a)L\n\nH 4 L\n\nFor term (III), with Lemma 16, it holds that\n\nterm (III) ≤ 0.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFor term (IV), we can obtain that (cid:88)\n\n(cid:88)\n\nterm (IV) =\n\n(cid:88)\n\ndπ∗ h (s, a)\n\n(cid:104)(cid:16)\n\nVh,l ˆVh+1\n\n(cid:17)\n\n(s, a) − (cid:0)Vh,lV ∗\n\nh+1\n\n(cid:105) (cid:1) (s, a)\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n(a) ≤ 4H\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n(cid:13) (cid:13) (cid:13)\n\nˆVh+1(·) − V ∗\n\nh+1(·)\n\n(cid:13) (cid:13) (cid:13)∞\n\n(b) ≤ cH\n\n(cid:114)\n\n= c\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a) ·\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\nC ∗H 8SL K\n\n√\n\n+ c\n\nH 8L,\n\n(cid:32)(cid:114)\n\nC ∗H 4S LK\n\n+\n\n(cid:114)\n\n(cid:33)\n\nH 4 L\n\nwhere inequality (a) is from simple algebraic based on the definition of variance and inequality (b) is from part (ii) of even G in Lemma 10.\n\nProof of Theorem 6. By Lemma 6 and 10, with probability at least 1 − δ, it holds that\n\nGap(ˆπ; M) ≤ 2\n\n(cid:88)\n\nh∈[H]\n\nEπ∗,M [Γh(sh, ah)] .\n\nFurthermore, it can be obtained that\n\nΓα\n\nh (sh, ah) = c\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n(cid:16) ˆVh,l ˆVh+1\n\n(cid:17)\n\n(s, a)\n\n(cid:88)\n\nL2Nh,l(s, a)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+ c\n\n(cid:88)\n\nl∈[L]\n\nH 2 L2(Nh,l(s, a))2\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n(a) ≤ c\n\n(cid:88)\n\nl∈[L]\n\n(cid:16)\n\n(cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n≤ c\n\nl∈[L]\n\n(cid:16)\n\nVh,l ˆVh+1\n\n(cid:17)\n\n(s, a)\n\nL2Nh,l(s, a)\n\n(cid:17)\n\n(s, a)\n\nVh,l ˆV ref\n\nh+1 L2Nh,l(s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+ c\n\n(cid:88)\n\nl∈[L]\n\nH 2 L2(Nh,l(s, a))3/2\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+\n\n(cid:88)\n\nl∈[L]\n\nH 2 L2(Nh,l(s, a))2\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+\n\n(cid:88)\n\nl∈[L]\n\n1 L2Nh,l(s, a)\n\n+\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\nH 4\n\nL2(Nh,l(s, a))2 ,\n\nwhere inequality (a) is from event (iv) of Lemma 10. Then, for each separate term, it holds that\n\nterm (I) := c\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\ndπ∗ h (s, a)\n\n(cid:16)\n\nVh,l ˆV\n\n(cid:17)\n\n(s, a)\n\nL2Nh,l(s, a)\n\n(cid:88)\n\nl∈[L]\n\n(a) ≤ c\n\n(cid:88)\n\n(cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\ndπ∗ h (s, a)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n(cid:114)\n\n≤ c\n\nC ∗HS L2K\n\n(cid:115) (cid:88)\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n(cid:17)\n\n(s, a)\n\n(cid:16)\n\nVh,l ˆVh+1 L2Kdρl\n\nh,l(s, a)\n\nh (s, a)1{a = π∗ dπ∗\n\nh(s)}\n\n(cid:16)\n\nVh,l ˆVh+1\n\n(cid:17)\n\n(s, a)\n\n(cid:115)\n\n(cid:114)\n\n(b) ≤ c\n\nC ∗HS L2K\n\nLH 2 +\n\n(cid:114)\n\nC ∗H 8SL K\n\n√\n\n+\n\nH 8L\n\n(cid:114)\n\n(cid:114)\n\n≤ c\n\n≤ c\n\n(cid:114)\n\nLH 2 +\n\nC ∗HS L2K\n\nC ∗H 2SL K\n\n(cid:114)\n\nC ∗H 3S LK\n\n+ c\n\nC ∗H 2S √\n\nLK\n\n+ c\n\n+ H 6\n\nC ∗H 7S L2K\n\n;\n\nterm (II) := c\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\ndπ∗ h (s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\n1 L2Nh,l(s, a)\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\n(c) ≤ c\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n1 KL2dρl h,l(s, a)\n\nC ∗ KL\n\n(cid:88)\n\n(cid:88)\n\n(cid:113)\n\nh∈[H]\n\n(s,a)∈S×A\n\nh (s, a)1{a = π∗ dπ∗\n\nh(s)}\n\n(cid:114)\n\n(cid:114)\n\n≤ c\n\n≤ c\n\nC ∗H 2S LK\n\n;\n\nterm (I.c) := c\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\n(d) ≤ c\n\n(cid:88)\n\n(cid:88)\n\ndπ∗ h (s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\ndπ∗ h (s, a)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\n(cid:88)\n\nH 4 L2(Nh,l(s, a))2\n\nL2(Kdρl\n\nH 4 h,l(s, a))2\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n(cid:88)\n\n(cid:88)\n\n≤ c\n\n1{π∗\n\nh(s) = a}\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nh∈[H]\n\n(s,a)∈S×A\n\nl∈[L]\n\n≤ c\n\nC ∗SH 3 √\n\nLK\n\n,\n\n(C ∗)2H 4 L2K 2\n\nwhere inequalities (a), (c) and (d) are from event (iv) of Lemma 10 and inequality (b) is from Lemma 17.\n\nBy aggregating these three terms together and adding the sum of source uncertainties, it can be observed that\n\nGap(ˆπ; M) := ̃O\n\n(cid:32)(cid:114)\n\nC ∗H 3S LK\n\n+\n\nC ∗H 3S √\n\nLK\n\n+\n\n(cid:114)\n\nC ∗H 7S L2K\n\n+\n\n(cid:114)\n\n(cid:33)\n\n.\n\nH 4 L\n\nF THE HETPEVI-LIN ALGORITHM\n\nF.1 PROPERTIES OF LINEAR MDPS\n\nProof of Lemma 1. This proof is standard for studies in linear MDPs (Jin et al., 2020; 2021b). We include it here for completeness and to facilitate the analysis of the next lemma. Based on the Bellman equation, it holds that\n\n(Bh,lf )(s, a) = rh,l(s, a) + (Ph,lf )(s, a) = ⟨φ(s, a), θh,l⟩ +\n\n(cid:90)\n\nS\n\nf (s′) · ⟨φ(s, a), dμh(s′)⟩,\n\nwhere means that (Bh,lf )(s, a) = ⟨φ(s, a), wf arguments hold for (Bhf )(s, a) = ⟨φ(s, a), wf\n\nh,l⟩ with wf h⟩ with wf\n\nh = θh + (cid:82)\n\nh,l = θh,l + (cid:82)\n\nS f (s′) dμh(s′).\n\nS f (s′) dμh,l(s′). Similar\n\nProof of Lemma 2. First, it can be observed that the transition and reward of the sample average MDP ̄M is linear since\n\n ̄Ph(s′|s, a) =\n\n(cid:88)\n\nl∈[L]\n\nPh,l(s′|s, a)/L =\n\n(cid:88)\n\nl∈[L]\n\n⟨φ(s, a), μh,l(s′)⟩/L = ⟨φ(s, a), ̄μh(s′)⟩;\n\n ̄rh(s, a) =\n\n(cid:88)\n\nl∈[L]\n\nrh,l(s, a)/L =\n\n(cid:88)\n\n⟨φ(s, a), θh,l⟩/L = ⟨φ(s, a), ̄θh⟩,\n\nl∈[L]\n\nwhere\n\n ̄μh(s′) :=\n\n(cid:88)\n\nl∈[L]\n\nμh,l(s′)/L;\n\n ̄θh :=\n\n(cid:88)\n\nl∈[L]\n\nθh,l/L.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nThen, we can verify the constraints in Definition 1 as\n\n∥φ(s, a)∥2 ≤ 1,\n\n∀(s, a) ∈ S × A,\n\nand\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nmax{∥ ̄μh(S)∥2, ∥ ̄θh∥2} = max\n\n≤ max\n\n√\n\nd,\n\n≤\n\n \n\n \n\n\n\n\nMoreover, it holds that\n\n( ̄Bhf )(s, a) = ̄rh(s, a) + ( ̄Phf )(s, a)\n\n(cid:88)\n\nμh,l(S)/L\n\nl∈[L]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nl∈[L]\n\nθh,l/L\n\n(cid:88)\n\nl∈[L]\n\n∥μh,l(S)∥2 /L,\n\n(cid:88)\n\nl∈[L]\n\n∥θh,l∥2 /L\n\n∀h ∈ [H].\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n \n\n \n\n\n\n\nf (s′) · ⟨φ(s, a), d ̄μh(s′)⟩\n\n(cid:42)\n\nf (s′) ·\n\nφ(s, a),\n\n(cid:90)\n\nS\n\nf (s′) dμh,l(s′)\n\n(cid:19)\n\n/L\n\n(cid:43)\n\ndμh,l(s′)/L\n\n(cid:88)\n\nl∈[L] (cid:43)\n\n= ⟨φ(s, a), ̄θh⟩ +\n\n(cid:42)\n\n=\n\nφ(s, a),\n\n(cid:42)\n\n=\n\nφ(s, a),\n\n(cid:42)\n\n=\n\nφ(s, a),\n\n(cid:88)\n\nl∈[L]\n\n(cid:88)\n\nl∈[L]\n\n(cid:88)\n\n(cid:90)\n\nS\n\n(cid:43)\n\nθh,l/L\n\n+\n\n(cid:18)\n\nθh,l +\n\n(cid:90)\n\nS\n\n(cid:43)\n\nwf\n\nh,l/L\n\nl∈[L] (cid:69)\n\n(cid:68)\n\nφ(s, a), ̄wf\n\nh\n\n,\n\n=\n\nwhich proves the claim.\n\nF.2 GOOD EVENT\n\nLemma 18. The following event holds with probability at least 1 − δ for HetPEVI-Lin\n\n(cid:26)\n\nG :=\n\n(i) The penalties {Γh(s, a)} induce a valid pessimism;\n\n(ii)\n\n(cid:13) (cid:13)\n\n(cid:13)Ξh,l − ˆΞh,l\n\n(cid:13) (cid:13) (cid:13)2\n\n≤ c\n\n(cid:114) 1 K\n\n, ∀(l, h) ∈ [L] × [H]\n\n,\n\n(cid:27)\n\nwhere\n\nΞh,l := Eρl,Ml (cid:88)\n\nˆΞh,l :=\n\n1 K\n\nk∈[K]\n\n(cid:2)φ(sh, ah)φ(sh, ah)⊤(cid:3)\n\nφ(sk\n\nh,l, ak\n\nh,l)φ(sk\n\nh,l, ak\n\nh,l)⊤.\n\nProof. The part (i) holds according to Lemma 22 and the part (ii) can be observed via the Lemma 30.\n\nIn the following, for all h ∈ [H], based on the assumption that the matrix (cid:80) we denote that\n\nl∈[L] Λ−1\n\nh,l is invertible,\n\nΥh :=\n\n(cid:18)(cid:88)\n\nl∈[L]\n\n(cid:19)−1\n\n;\n\nΛ−1\n\nh,l\n\nΥ−1\n\nh :=\n\n(cid:88)\n\nΛ−1 h,l .\n\nl∈[L]\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 19. With the penalties Γα\n\nh (s, a) = cH\n\n(cid:18)(cid:113) d\n\nL2 +\n\n(cid:19)\n\n(cid:113) dλ\n\nL\n\n∥φ(s, a)∥Υ−1\n\nh\n\nin HetPEVI-Lin,\n\nwith probability at least 1 − δ, it holds that for all (s, a, h) ∈ S × A × [H]\n\n(cid:16)ˆBh ˆVh+1\n\n(cid:17)\n\n(cid:12) (cid:12) (cid:12)\n\n(s, a) −\n\n(cid:16) ̄Bh ˆVh+1\n\n(cid:17)\n\n(s, a)\n\n(cid:12) (cid:12) ≤ Γα (cid:12)\n\nh (s, a).\n\nProof. For a fixed h and a fixed function ˆVh+1(·) : S → R, with Lemma 2, there exists wh ∈ Rd such that\n\n(cid:16) ̄Bh ˆVh+1\n\n(cid:17)\n\n(s, a) = ⟨φ(s, a), ̄wh⟩,\n\n∀(s, a) ∈ S × A.\n\nWith (ˆBh ˆVh+1)(s, a) := ⟨φ(s, a), ˆwh⟩, it holds that\n\n(cid:16) ̄Bh ˆVh+1\n\n(cid:17)\n\n(cid:12) (cid:12) (cid:12)\n\n(s, a) −\n\n(cid:16)ˆBh ˆVh+1\n\n(cid:17)\n\n(cid:12) (cid:12) (cid:12) = |⟨φ(s, a), ̄wh − ˆwh⟩| ≤ ∥φ(s, a)∥Υ−1 (s, a)\n\nh\n\n∥ ̄wh − ˆwh∥Υh ,\n\nand thus we can instead control ∥ ̄wh − ˆwh∥Υh. For this purpose, we can observe that\n\nwhere\n\n∥ ̄wh − ˆwh∥Υh = ⟨ ̄wh − ˆwh, (Υh)1/2X⟩,\n\nX :=\n\n(Υh)1/2 ( ̄wh − ˆwh) ∥ ̄wh − ˆwh∥Υh\n\n∈ Sd−1.\n\nWith Lemma 29, we can find a ε-covering Cε over Sd−1 with |Cε| ≤ (3/ε)d. Using Lemma 20 and a union bound, we can have that with probability 1 − δ, ∀(y, h) ∈ Cε × [H], it holds that\n\n(cid:12) (cid:12) (cid:12)⟨(Υh)1/2y, ̄wh − ˆwh⟩ (cid:12) (cid:12) (cid:12) ≤ cH\n\n(cid:32)(cid:114)\n\n(cid:32)(cid:114)\n\n≤ cH\n\nlog(H|Cε|/δ) L2\n\n+\n\n(cid:114)\n\ndλ L\n\n(cid:33)\n\nd log(H/(εδ)) L2\n\n+\n\n(cid:114)\n\ndλ L\n\n.\n\n(cid:13) (cid:13)(Υh)1/2y (cid:13) (cid:33)\n\n(cid:13) (cid:13) (cid:13)(Υh)−1\n\nWith this event happening, for any x ∈ Sd−1, there exists y ∈ Cε such that ∥x − y∥2 ≤ ε and further it holds that\n\n∥ ̄wh − ˆwh∥Υh = max x∈Sd−1\n\n⟨ ̄wh − ˆwh, (Υh)1/2x⟩\n\n= max x∈Sd−1\n\nmin y∈Cε\n\n(cid:104)\n\n(cid:105) ⟨ ̄wh − ˆwh, (Υh)1/2y⟩ + ⟨ ̄wh − ˆwh, (Υh)1/2(x − y)⟩\n\n(cid:34)\n\n(cid:32)(cid:114)\n\n≤ max x∈Sd−1\n\nmin y∈Cε\n\ncH\n\nd log(H/(εδ)) L2\n\n+\n\n(cid:114)\n\n(cid:33)\n\ndλ L\n\n+ ∥ ̄wh − ˆwh∥Υh\n\n∥x − y∥2\n\n(cid:35)\n\n≤ cH\n\n(cid:32)(cid:114)\n\nd log(H/(εδ)) L2\n\n+\n\n(cid:114)\n\n(cid:33)\n\ndλ L\n\n+ ε ∥ ̄wh − ˆwh∥Υh\n\n.\n\nThus, with ε = 1/2, with probability at least 1 − δ, ∀h ∈ [H], it holds that\n\n∥wh − ˆwh∥Υh ≤ cH\n\n(cid:32)(cid:114)\n\nd\n\nL2 +\n\n(cid:114)\n\n(cid:33)\n\n.\n\ndλ L\n\nThus, for any (s, a) ∈ S × A, it holds that\n\n(cid:17)\n\nBh ˆVh+1\n\n(cid:16)\n\n(cid:12) (cid:12) (cid:12)\n\n(s, a) −\n\n(cid:16)ˆBh ˆVh+1\n\n(cid:17)\n\n(cid:12) (cid:12) (cid:12) ≤ ∥φ(s, a)∥Υ−1 (s, a)\n\nh\n\n(cid:32)(cid:114)\n\n≤ cH\n\nWith a union bound over h ∈ [H], the lemma is proved.\n\n26\n\n∥wh − ˆwh∥Υh (cid:33)\n\n(cid:114)\n\nd\n\nL2 +\n\ndλ L\n\n∥φ(s, a)∥Υ−1\n\nh\n\n.\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 20. For a fixed vector x ∈ Rd and a fixed h ∈ [H], with probability at least 1 − δ, it holds that\n\n|⟨x, ̄wh − ˆwh⟩| ≤\n\n(cid:32)(cid:114)\n\n2 log(2/δ) L2\n\n+\n\n(cid:114)\n\n(cid:33)\n\ndH 2λ L\n\n∥x∥Υ−1\n\nh\n\n.\n\nProof. It holds that\n\nx⊤ ( ̄wh − ˆwh) = x⊤\n\n\n\n\n\n(cid:88)\n\nl∈[L]\n\nwh,l/L −\n\n\n\nˆwh,l/L\n\n\n\n(cid:88)\n\nl∈[L]\n\n(a)\n\n= x⊤ (cid:88)\n\nl∈[L]\n\n\n\n(cid:88)\n\nΛ−1\n\nh,l\n\n\n\nk∈[K]\n\n1 L\n\nφ(sk\n\nh,l, ak\n\nh,l)φ(sk\n\nh,l, ak\n\nh,l)⊤ + λI\n\n\n\n wh,l\n\n− x⊤ (cid:88)\n\nl∈[L]\n\n1 L\n\n= λx⊤ (cid:88)\n\nl∈[L]\n\n\n\n(cid:88)\n\nΛ−1\n\nh,l\n\n\n\nk∈[K]\n\n1 L\n\nΛ−1\n\nh,l wh,l\n\n(cid:124)\n\n(cid:123)(cid:122) term (I)\n\n(cid:125)\n\n+ x⊤ (cid:88)\n\nl∈[L]\n\n1 L\n\n\n\n(cid:88)\n\nΛ−1\n\nh,l\n\n\n\nk∈[K]\n\n(cid:124)\n\nφ(sk\n\nh,l, ak\n\nh,l)\n\nφ(sk\n\nh,l, ak\n\nh,l)\n\n(cid:16)\n\n(cid:16)\n\nh,l + ˆVh+1(sk rk\n\nh+1,l)\n\n(cid:17)\n\n\n\n\n\nφ(sk\n\nh,l, ak\n\nh,l)⊤wh,l −\n\n(cid:16)\n\n(cid:123)(cid:122) term (II)\n\nh,l + ˆVh+1(sk rk\n\nh+1,l)\n\n(cid:17)(cid:17)\n\n\n\n,\n\n\n\n(cid:125)\n\nwhere the definition of Λh,l and ˆwh,l is used in equation (a).\n\nFor term (I), we have (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nx⊤ (cid:88)\n\n1 L\n\nl∈[L]\n\nλ\n\nΛ−1\n\nh,l wh,l\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n= λ\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nl∈[L]\n\nx⊤Λ−1\n\n(cid:12) (cid:12) (cid:12) h,l wh,l/L (cid:12) (cid:12) (cid:12)\n\n≤\n\nλ L\n\n∥x∥Υ−1\n\nh\n\n(cid:115) (cid:88)\n\nl∈[L]\n\nw⊤\n\nh,lΛ−1\n\nh,l wh,l\n\n(a) ≤\n\nλ L\n\n∥x∥Υ−1\n\nh\n\n(cid:112)4H 2Ld/λ = c √\n\n(cid:114)\n\ndH 2λ L\n\n∥x∥Υ−1\n\nh\n\n,\n\nwhere inequality (a) is based on the fact that ∥wh,l∥2 ≤ 2H proved similarly to Lemma B.1 in Jin et al. (2020)) and the following observation:\n\nd, ∀(l, h) ∈ [L] × [H] (which can be\n\nw⊤\n\nh,lΛ−1\n\nh,l wh,l ≤ ∥wh∥2\n\n2∥Λ−1\n\nh,l ∥2 ≤ 4dH 2/λ.\n\nFor term (II), with\n\nconditioned on the state-action pairs at step h, we have\n\nt :=\n\n(cid:113)\n\nH L\n\n2x⊤Υ−1\n\nh x log(2/δ),\n\n\n\nP\n\n\n\n(cid:12) (cid:12) x⊤ (cid:88) (cid:12) (cid:12) (cid:12) (cid:12)\n\nl∈[L] \n\n1 L\n\n\n\n(cid:88)\n\nΛ−1\n\nh,l\n\n\n\nk∈[K]\n\nφ(sk\n\nh,l, ak\n\nh,l)\n\n(cid:16)\n\nφ(sk\n\nh,l, ak\n\nh,l)⊤wh,l −\n\n(cid:16)\n\nh,l + ˆVh+1(sk rk\n\nh+1,l)\n\n(cid:17)(cid:17)\n\n\n\n≥ t\n\n\n\n\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 2 exp\n\n −\n\n(cid:32)\n\n≤ 2 exp\n\n−\n\n4H 2 (cid:80)\n\nl∈[L]\n\n(cid:80)\n\nk∈[K]\n\n2t2 (cid:16)\n\n1 L2\n\nx⊤Λ−1\n\nh,l φ(sk\n\nh,l, ak\n\nh,l)\n\n(cid:80)\n\nl∈[L]\n\n(cid:80)\n\nk∈[K] x⊤Λ−1\n\nx⊤Υ−1\n\nh x log(2/δ) h,l, ak\n\nh,l φ(sk\n\nh,l)φ(sk\n\nh,l, ak\n\nh,l)⊤Λ−1\n\nh,l x\n\n\n\n \n\n(cid:17)2\n\n(cid:33)\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n\n\n≤ 2 exp\n\n−\n\n(cid:80)\n\nl∈[L]\n\n(cid:80)\n\nk∈[K] x⊤Λ−1\n\nh,l\n\nx⊤Υ−1 (cid:16)\n\nφ(sk\n\nh x log(2/δ) h,l)φ(sk h,l, ak\n\nh,l, ak\n\nh,l)⊤ + λI\n\n\n\n\n\n(cid:17)\n\nΛ−1\n\nh,l x\n\n(cid:18)\n\n≤ 2 exp\n\n−\n\nx⊤Υ−1\n\nh x log(2/δ) x⊤Υ−1 h x\n\n(cid:19)\n\n= δ,\n\nwhich leads to the claim.\n\nLemma 21. For the penalties Γβ holds that for all (s, a, h) ∈ S × A × [H],\n\nh(s, a) = c\n\n(cid:113) dH 2\n\nL in HetPEVI, with probability at least 1 − δ, it\n\n(cid:16) ̄Bh ˆVh+1\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:17)\n\n(s, a) −\n\n(cid:16)\n\nBh ˆVh+1\n\n(cid:17)\n\n(cid:12) (cid:12) (s, a)\n\n(cid:12) ≤ Γβ\n\nh(s, a).\n\nProof. For a fixed φ(s, a) ∈ Rd and a fixed ˆVh+1, since (Bh,l ˆVh+1)(s, a) is bounded between [0, H] and has an expectation of (Bh ˆVh+1)(s, a), it is then a H-sub-Gaussian random variable (Vershynin, 2018). Thus, it holds that\n\n(cid:32) (cid:12) (cid:12) (cid:12)\n\nP\n\n(cid:16) ̄Bh ˆVh+1\n\n(cid:17)\n\n(s, a) −\n\n(cid:16)\n\nBh ˆVh+1\n\n(cid:17)\n\n(s, a)\n\n(cid:114)\n\n(cid:12) (cid:12) (cid:12) ≥ c\n\nH 2 log(2H/δ) L\n\n(cid:33)\n\n≤ δ,\n\n∀h ∈ [H].\n\nSimilarly as in Lemma 19, using a covering argument, it holds that\n\n(cid:32) (cid:12) (cid:12) (cid:12)\n\nP\n\n(cid:16) ̄Bh ˆVh+1\n\n(cid:17)\n\n(s, a) −\n\n(cid:16)\n\nBh ˆVh+1\n\n(cid:17)\n\n(cid:12) (cid:12) (cid:12) ≥ c (s, a)\n\n(cid:33)\n\n(cid:114)\n\ndH 2 L\n\n≤ δ,\n\n∀(s, a, h) ∈ S × A × [H],\n\nwhich concludes the proof.\n\nh (s, a) + Γβ Lemma 22. The penalties {Γh(s, a) = Γα h(s, a) : (s, a, h) : (s, a, h) ∈ S × A × [H]} in HetPEVI induce a valid pessimism with probability at least 1 − δ with respect to the estimated Bellman operator ˆBh as (ˆBh ˆVh+1)(s, a) := ⟨φ(s, a), ˆwh⟩.\n\nProof. This result can be obtained by combining Lemmas 19 and 21.\n\nF.3 SUBOPTIMALITY GAP\n\nProof of Theorems 4. By Lemma 6 and 18, with probability at least 1 − δ, it holds that\n\nWe denote\n\nGap(ˆπ; M) ≤ 2\n\n(cid:88)\n\nh∈[H]\n\nEπ∗,M [Γh(sh, ah)] .\n\nΣh = Eπ∗,M\n\n(cid:2)φ(sh, ah)φ(sh, ah)⊤(cid:3) ,\n\nwhich is positive semi-definite as Ξh,l and ˆΞh,l for all h ∈ [H]. Correspondingly, we denote Rankh = Rank(Σh) for all h ∈ [H]. Also, for a positive semi-definite matrix Σ ∈ Rd×d, we denote its ordered eigenvalues as\n\nγ1(Σ) ≥ γ1(Σ) · · · ≥ γd(Σ) ≥ 0.\n\nWith Assumption 5, when K ≥ c maxh,l\n\n(cid:110)\n\nand λ = 1/L, it holds that\n\n(cid:111)\n\n(D∗)2 (γRankh (Σh))2 (cid:114)\n\n(cid:34)(cid:114)\n\n(cid:88)\n\nEπ∗,M\n\ndH 2 L2 ∥φ(sh, ah)∥Υ−1\n\nh\n\n+\n\n(cid:35)\n\ndH 2 L\n\nh∈[H] (cid:114)\n\n(a) ≤\n\ndH 2 L2\n\n(cid:88)\n\n(cid:115) (cid:88)\n\n(cid:16)\n\nTr\n\nh∈[H]\n\nl∈[L]\n\nEπ∗,M [φ(sh, ah)φ(sh, ah)⊤] Λ−1\n\nh,l\n\n(cid:114)\n\n(cid:17)\n\n+\n\ndH 4 L\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:114)\n\n(b) ≤ c\n\ndH 2 L2\n\n(cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\nl∈[L]\n\ni∈[d]\n\nγi(Σh) γi(Λh,l)\n\n+\n\n(cid:114)\n\ndH 4 L\n\n(cid:114)\n\n(c) ≤\n\ndH 2 L2\n\n(cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\nl∈[L]\n\ni∈[Rankl]\n\nγi(Σl)/K 1/(KL) + γi(ˆΞh,l)\n\n+\n\n(cid:114)\n\ndH 4 L\n\n(cid:114)\n\n(d) ≤ c\n\ndH 2 L2\n\n(cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\nl∈[L]\n\ni∈[Rankh]\n\nγi(Σh)/K 1/(KL) + γi(Σl)/D∗ +\n\n(cid:114)\n\ndH 4 L\n\n(cid:114)\n\n≤ c\n\nd2H 4D∗ LK\n\n+\n\n(cid:114)\n\ndH 4 L\n\nwhere inequality (a) is from the Cauchy-Schwarz inequality, inequality (b) is from the Von Neumann’s trace inequality (Mirsky, 1975) and removes the zero eigenvalues of Σh from the sum. Furthermore, for i ≤ Rankh (implying γi(Σh) > 0), inequality (d) is from the following observation\n\nγi(ˆΞh,l)\n\n(e)\n\n≥ γi(Ξh,l) − ∥Ξh,l − ˆΞh,l∥2\n\n(f ) ≥ γi(Ξh,l) −\n\nc √\n\nK\n\n(g) ≥\n\nγi(Σh)\n\nD∗ −\n\nc √\n\nK\n\n(h) ≥ c\n\nγi(Σh) D∗\n\n,\n\nwhere inequality (e) is from the Weyl’s inequality (see e.g., Chapter 8 in (Wainwright, 2019)), inequality (f) is from (ii) of event G in Lemma 18, inequality (g) is from the Assumption 5 (which implies γi(Σh) ≤ D∗γi(Ξh,l) from the Weyl’s inequality), and inequality (h) is due to γi(Σh) > 0, ∀i ∈ [Rankh] and the sufficiently large K.\n\nWith Assumption 6, we can similarly obtain\n\n(cid:88)\n\nEπ∗,M\n\nh∈[H]\n\n(cid:114)\n\n≤\n\ndH 2 L2\n\n(cid:34)(cid:114)\n\ndH 2 L2 ∥φ(sh, ah)∥Υ−1\n\nh\n\n+\n\n(cid:114)\n\n(cid:35)\n\ndH 2 L\n\n(cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\n(cid:88)\n\nh∈[H]\n\nl∈[L]\n\ni∈[Rankl]\n\nγi(Σl)/K 1/(KL) + γi(ˆΞh,l)\n\n+\n\n(cid:114)\n\ndH 4 L\n\n(cid:114)\n\n≤ c\n\ndH 2 L2\n\n(cid:114)\n\n(cid:88)\n\nh∈[H]\n\nd(L − L†)L +\n\nD∗dL K\n\n+\n\n(cid:114)\n\ndH 4 L\n\n(cid:114)\n\n≤ c\n\nd2H 4D∗ LK\n\n+\n\n(cid:114)\n\nd(L + 1 − L†)H 4 L\n\n.\n\nG SUPPORTING LEMMAS\n\nH SUPPORTING LEMMAS\n\nH.1 SUBOPTIMALITY DECOMPOSITION\n\nThe suboptimality gap between an output policy ˆπ from an offline RL algorithm and the optimal policy π∗ can be decomposed as follows. Lemma 23 (Lemma 3.1 in Jin et al. (2021b)). Let ˆπ = {ˆπh}h∈[H] be the policy such that ˆVh(s) = ⟨ ˆQh(s, ·), ˆπ(·|s)⟩. For any ˆπ and s ∈ S it holds that\n\nGap(ˆπ; M) = −\n\n+\n\n(cid:88)\n\nh∈[H] (cid:88)\n\nh∈[H]\n\nEˆπ,M [ζh(sh, ah)|s1 = s] +\n\n(cid:88)\n\nh∈[H]\n\nEπ∗,M [ζh(sh, ah)|s1 = s]\n\nEπ∗,M\n\n(cid:104)(cid:68) ˆQh(sh, ·), π∗(·|sh) − ˆπh(·|sh)\n\n(cid:69)\n\n(cid:105)\n\n|s1 = s\n\n,\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nwhere the expectations Eˆπ,M and Eπ∗,M are with respect to the trajectories induced by ˆπ and π∗, and\n\nζh(s, a) := (Bh ˆVh+1)(s, a) − ˆQh(s, a)\n\nis the model evaluation error at (s, a, h) ∈ S × A × [H].\n\nH.2 ENHANCED EMPIRICAL BERNSTEIN INEQUALITY\n\nIn following, an enhanced version of empirical Bernstein inequality is derived. First, the famous Bernstein inequality is presented, which serves as a starting point of the later generalization.\n\nLemma 24 (Bernstein Inequality). Let Z1, Z2, · · · , Zn be independent random variables with values |Zi − E[Zi]| ≤ c0. Then, it holds that\n\nP\n\n(cid:32)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\ni=1\n\nZi −\n\nn (cid:88)\n\ni=1\n\nE [Zi]\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:33)\n\n(cid:18)\n\n≥ t\n\n≤ 2 exp\n\n−\n\nt2\n\n2 (cid:80)n\n\ni=1\n\nV(Zi) + 2\n\n3 c0t\n\n(cid:19)\n\n,\n\nwhere V(Zi) denotes the variance of random variable Zi.\n\nIt is noted that the true variance is required in the original form of Bernstein inequality, which is hard to be obtained in practice. Thus, Maurer & Pontil (2009) established an empirical version of Bernstein inequality, where the estimated variance is used instead of the true variance.\n\nLemma 25 (Empirical Bernstein Inequality; Theorem 4 in Maurer & Pontil (2009)). Let Z, Z1, Z2, · · · , Zn be i.i.d. random variables and let δ > 0 with values |Zi − E[Z]| ≤ c0. Then, with probability at least 1 − δ, it holds that\n\n(cid:115)\n\n(cid:12) (cid:12) E[Z] − (cid:12) (cid:12) (cid:12)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nZi\n\n≤\n\n2 ˆV(Z) ln(4/δ) n\n\n+\n\n7c0 ln(4/δ) 3n\n\n,\n\nwhere ˆV(Z) denotes the empirical variance of Z with samples Z1, · · · , Zn\n\nHowever, in Lemma 25, the samples Z1, · · · , Zn are i.i.d, i.e., from the same source, which do not meet the need of handling data from heterogeneous sources in this work. Thus, a further enhanced version is provided in the following.\n\nLemma 26 (Enhanced Empirical Bernstein Inequality). Let Zl, Z1,l, Z2,l, · · · , Znl,l be i.i.d. random variables with values Zi,l ∈ [0, 1] respectively for each l ∈ [L], and let δ > 0. Then, denoting\n\nη :=\n\n1 L\n\n(cid:88)\n\nl∈[L]\n\nE[Zl] −\n\n1 L\n\n(cid:88)\n\nl∈[L]\n\n1 nl\n\nnl(cid:88)\n\ni=1\n\nZi,l,\n\nwith probability at least 1 − δ, it holds that\n\n|η| ≤\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\n8 ˆV(Zl) log( 2(L+1) L2nl\n\nδ\n\n)\n\n+\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n(cid:16)\n\n36\n\n(cid:88)\n\nl∈[L]\n\nlog( 2(L+1)\n\nδ L2(nl)2\n\n(cid:17)2 )\n\n,\n\nwhere ˆV(Zl) denotes the empirical variance of Zl with samples Z1,l, Z2,l, · · · , Znl,l\n\nProof. With nmin = minl∈[L]{nl} and\n\nt :=\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\n4V(Zl) log( 2(L+1) L2nl\n\nδ\n\n)\n\n+\n\n4 log( 2(L+1) δ\n3Lnmin\n\n)\n\n,\n\nwhich implies that\n\nt2\n\n2 (cid:80)\n\nl∈[L]\n\n(cid:80)nl\n\ni=1\n\nV(Z l\n\ni)/(L2n2\n\nl ) + 2t/(3Lnmin)\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n≤\n\nmax{4 (cid:80)\n\nl∈[L]\n\n≤ log\n\n(cid:18) 2(L + 1) δ\n\nt2\n\nV(Zl)/(L2nl), 4t/(3Lnmin)} (cid:19)\n\n,\n\nfrom Lemma 24, it holds that\n\n(cid:32)\n\nP (|η| ≥ t) ≤ 2 exp\n\n−\n\n2 (cid:80)\n\nl∈[L]\n\n(cid:80)nl\n\ni=1\n\n≤\n\nδ L + 1\n\n.\n\nt2\n\n(cid:33)\n\nV(Z l\n\ni)/(L2n2\n\nl ) + 2t/(3Lnmin)\n\nFurthermore, with Lemma 27 and a union bound, with probability at least 1 − Lδ\n\nL+1 , it holds that\n\n(cid:112)V(Zl) ≤\n\n(cid:113)\n\nˆV(Zl) +\n\n(cid:115)\n\n2 ln( L+1 δ ) nl\n\n,\n\n∀l ∈ [L].\n\nIf the above event happens, it holds that\n\nt =\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:88)\n\nl∈[L]\n\n4V(Zl) log( 2(L+1) L2nl\n\nδ\n\n)\n\n+\n\n4 log( 2(L+1) δ\n3Lnmin\n\n)\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:117) (cid:116)\n\n≤\n\n(cid:88)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n≤\n\n(cid:88)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n≤\n\n(cid:88)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n≤\n\n(cid:88)\n\nl∈[L]\n\n(cid:18)(cid:113)\n\n4\n\nˆV(Zl) +\n\n(cid:113) 2 log( L+1 δ )\n\nL2nl\n\n(cid:19)2\n\nlog( 2(L+1)\n\nδ\n\n)\n\n+\n\n4 log( 2(L+1) δ\n3Lnmin\n\nnl\n\n8 ˆV(Zl) log( 2(L+1) L2nl\n\nδ\n\n)\n\n+\n\n8 ˆV(Zl) log( 2(L+1) L2nl\n\nδ\n\n)\n\n+\n\n8 ˆV(Zl) log( 2(L+1) L2nl\n\nδ\n\n)\n\n+\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n(cid:16)\n\n16\n\n(cid:88)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n(cid:16)\n\n32\n\n(cid:88)\n\nl∈[L]\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)\n\n(cid:16)\n\n36\n\n(cid:88)\n\nl∈[L]\n\n(cid:17)2 )\n\nlog( 2(L+1)\n\nδ L2(nl)2\n\n+\n\n4 log( 2(L+1) δ\n3Lnmin\n\n)\n\n(cid:17)2\n\n)\n\nlog( 2(L+1)\n\nδ 9L2n2\n\nmin\n\nlog( 2(L+1)\n\nδ L2(nl)2\n\n(cid:17)2 )\n\n(cid:16)\n\n32\n\n+\n\nlog( 2(L+1)\n\nδ L2(nl)2\n\n(cid:17)2 )\n\n.\n\nCombining this the bound of t with the concentration inequality proved above, the lemma is proved.\n\nLemma 27 (Theorem 10 in (Maurer & Pontil, 2009)). Let Z, Z1, · · · , Zn be i.i.d. random variables with values in [0, 1], and let δ > 0, with probability at least 1 − δ, it holds that\n\n(cid:112)V(Z) <\n\n(cid:113)\n\nˆV(Z) +\n\n(cid:114)\n\n2 ln(1/δ) n\n\n.\n\nH.3 VARIANCE OF MIXTURE MODELS\n\nLemma 28. With random variable Xl following distribution Pl for each l ∈ [L], the random variable X is assumed to follow the distribution mixture, especially, P = (cid:80) l∈[L] αlPl, where α = [α1, · · · , αL] ∈ ∆L, the followings holds CV(X) ≥\n\n(cid:88)\n\nα2\n\nV(Xl),\n\nl\n\nif\n\nl∈[L]\n\nC ≥\n\n(cid:80)\n\n(cid:80)\n\nV(Xl) l∈[L] α2 l∈[L] αlV(Xl)\n\nl\n\n.\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nProof. For each side of the desired inequality, we have\n\nLHS := CV(X) = C\n\n\n\n \n\n\n\n(cid:88)\n\nαlE[X 2\n\nl ] −\n\n\n\n(cid:88)\n\nl∈[L]\n\n2\n\n\n\nαlE[Xl]\n\n\n\n \n\nα2\n\nl [EXl]2.\n\nl∈[L]\n\n(cid:88)\n\nl∈[L]\n\nRHS :=\n\n(cid:88)\n\nl∈[L]\n\nα2\n\nl\n\nV(Xl) =\n\n(cid:88)\n\nl∈[L]\n\nα2\n\nl\n\nE[X 2\n\nl ] −\n\nWith the above expressions, we can further obtain that\n\nLHS − RHS =\n\n=\n\n≥\n\n=\n\n=\n\n=\n\n(cid:88)\n\nl∈[L] (cid:88)\n\nl∈[L]\n\n(cid:88)\n\nl∈[L] (cid:88)\n\nl∈[L] (cid:88)\n\nl∈[L] (cid:88)\n\nl∈[L]\n\nαl(C − αl)E[X 2\n\nl ] − C\n\n 2\n\nαlE[Xl]\n\n\n\n+\n\n\n\n\n\n(cid:88)\n\nl∈[L]\n\n(cid:88)\n\nl∈[L]\n\nα2\n\nl [EXl]2\n\nαl(C − αl)E[X 2\n\nl ] +\n\nαl(C − αl)E[X 2\n\nl ] +\n\nαl(C − αl)E[X 2\n\nl ] +\n\nαl(C − αl)E[X 2\n\nl ] +\n\nαl(C − αl)V(Xl),\n\n(cid:88)\n\nl∈[L]\n\n(cid:88)\n\nl∈[L] (cid:88)\n\nl∈[L] (cid:88)\n\nl∈[L]\n\nα2\n\nl (1 − C)[EXl]2 − C\n\nα2\n\nl (1 − C)[EXl]2 − C\n\nα2\n\nl (1 − C)[EXl]2 − C\n\nαl(αl − C)[EXl]2\n\n(cid:88)\n\nl̸=n\n\n(cid:88)\n\nl̸=n (cid:88)\n\nl∈[L]\n\nαlαnE[Xl]E[Xn]\n\nαlαn\n\n[EXl]2 + [EXn]2 2\n\nαl(1 − αl)[EXl]2\n\nwhich leads to the lemma.\n\nH.4 MISCELLANEOUS\n\nLemma 29 (Covering Number of a Euclidean Ball; Lemma D.5 of Jin et al. (2020)). There exists a set Cε ∈ Rd with |Cε| ≤ (1 + 2R/ε)d such that for all x ∈ Sd−1 := {x ∈ Rd : ∥x∥2 ≤ R} there exists a y ∈ Cε with ∥x − y∥2 ≤ ε. Lemma 30 (Lemma H.4 of Min et al. (2021)). Let ψ : S × A → Rd satisfying ∥ψ(s, a)∥2 ≤ C for all (s, a) ∈ S × A. For any T > 0 and κ > 0, define ̄GT = (cid:80) τ ∈[T ] ψ(sτ , aτ )ψ(sτ , aτ )⊤ + κId where (sτ , aτ )’s are i.i.d. samples from some distribution ν over S × A. Then, for any δ ∈ (0, 1), with probability at least 1 − δ, it holds that\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n ̄GT T\n\n− Eν\n\n(cid:20) ̄GT T\n\n(cid:21)(cid:13) (cid:13) (cid:13) (cid:13)2\n\n≤\n\n4\n\n2C 2\n\n√\n\n√\n\nT\n\n(cid:18)\n\nlog\n\n2d δ\n\n(cid:19)1/2\n\n.\n\n32",
    "reference": "# Summary Of The Paper\n\nThis paper aims to tackle the offline MDP challenge with the heterogeneous data source. To this end, the authors propose an underlying MDP setup where the offline data are sampled from an (iid) variation of the underlying MDP. The authors further present several algorithms and the associated analyses (for both tabular case and linear MDP case) to show that the proposed algorithms are sample efficient.\n\n# Strength And Weaknesses\n\n$\\textbf{Strength:}$\n\nThe offline MDP challenge with the heterogeneous data source is important and has potential usage in practice. The algorithms are well-motivated and ready for practical realization. The analyses are well presented, with all the results discussed thoroughly.\n\n$\\textbf{Questions and suggestions:}$\n\nThe organization of this paper could be improved. I think this paper needs better motivation on the math problem it attempts to address. In particular,\n\n$\\textbf{Q1}$\nWhy do we want to solve a \"mean MDP\" (the underlying MDP) problem? How does solving such an underlying MDP contribute to the agent's performance in local environments (which the agent is supposed to serve)? Why not consider transitions and rewards with the variation that aligns with data source behavior? An ordinary (offline) RL algorithm should be able to handle such a problem. \n\n$\\textbf{Q2}$\nIn Definition 1, why are $r_{h, \\ell}$ (same for $P_{g, \\ell}$) identically distributed across (h, \\ell)? Does the analysis hinge on such an identical assumption? Given the motivation in the introduction, I think a more natural setting is if $r_{h, \\ell}$ are different distributions for different $\\ell$ due to the data source preference. They could still share the same mean, though. In addition, Definition 1 casts restrictions on the set of MDPs (iid distribution), so it is not exactly a definition of underlying MDP but a definition of both underlying MDP and the associated MDP set. The authors may clarify such a point for revisions to avoid misunderstanding.\n\nBesides, the extensions with the Bernstein-type technique and linear MDPs are less important and can be put into the appendix.\n\nIn addition, I have several inquiries.\n\n$\\textbf{Q3}$\nThe technical contribution seems marginal, given the results in, e.g., [1][2][3] (and the numerous previous Hoeffding- and Bernstein- type analyses of tabular and linear MDPs) and that all the rewards and transitions are iid distributed. Could the authors highlight their analysis's technical challenges and subtle parts, comparing against [1][2][3]?\n\n$\\textbf{Q5}$\nIs the achieved sample complexity in Theorem 1 optimal? What would be the lower bound for such a type of problem?\n\n$\\textbf{Q5}$\n(Minor) Assumption 1 seems strong for such a problem. Is it necessary that data collected from each element of the MDP set has to explore the environment sufficiently well? Is it possible that they do not have good coverage individually but together cover the underlying MDP sufficiently well?\n\n[1] Jin et al., Is Pessimism Provably Efficient for Offline RL? (2021)\n[2] Rashidinejad et al., Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. (2021)\n[3] Ming et al., Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. (2022)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee questions above. The reproducibility of proof is justified by the appendix.\n\n# Summary Of The Review\n\nThe problem is interesting but the underlying math model needs better motivation. The presentation could be improved to highlight the motivation and the technical challenges of the paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nBISPECTRAL NEURAL NETWORKS\n\nSophia Sanborn1,2 sanborn@berkeley.edu\n\nBruno Olshausen1,2 baolshausen@berkeley.edu\n\nChristian Shewmake1,2 shewmake@berkeley.edu\n\nChristopher Hillar3 hillarmath@gmail.com\n\n1Redwood Center for Theoretical Neuroscience 2University of California, Berkeley 3Awecom, Inc.\n\nABSTRACT\n\nWe present a neural network architecture, Bispectral Neural Networks (BNNs) for learning representations that are invariant to the actions of compact commutative groups on the space over which a signal is defined. The model incorporates the ansatz of the bispectrum, an analytically defined group invariant that is complete—that is, it preserves all signal structure while removing only the variation due to group actions. Here, we demonstrate that BNNs are able to simultaneously learn groups, their irreducible representations, and corresponding equivariant and complete-invariant maps purely from the symmetries implicit in data. Further, we demonstrate that the completeness property endows these networks with strong invariance-based adversarial robustness. This work establishes Bispectral Neural Networks as a powerful computational primitive for robust invariant representation learning.\n\n1\n\nINTRODUCTION\n\nA fundamental problem of intelligence is to model the transformation structure of the natural world. In the context of vision, translation, rotation, and scaling define symmetries of object categorization—the transformations that leave perceived object identity invariant. In audition, pitch and timbre define symmetries of speech recognition. Biological neural systems have learned these symmetries from the statistics of the natural world—either through evolution or accumulated experience. Here, we tackle the problem of learning symmetries in artificial neural networks.\n\nAt the heart of the challenge lie two requirements that are frequently in tension: invariance to transformation structure and selectivity to pattern structure. In deep networks, operations such as max or average are commonly employed to achieve invariance to local transformations. Such operations are invariant to many natural transformations; however, they are also invariant to unnatural transformations that destroy image structure, such as pixel permutations. This lack of selectivity may contribute to failure modes such as susceptibility to adversarial perturbations [1], excessive invariance [2], and selectivity for textures rather than objects [3]. Thus, there is a need for computational primitives that selectively parameterize natural transformations and facilitate robust invariant representation learning.\n\nAn ideal invariant would be complete. A complete invariant preserves all pattern information and is invariant only to specific transformations relevant to a task. Transformations in many datasets arise from the geometric structure of the natural world. The mathematics of groups and their associated objects give us the machinery to precisely define, represent, and parameterize these transformations, and hence the problem of invariant representation learning. In this work, we present a novel neural network primitive based on the bispectrum, a complete invariant map rooted in harmonic analysis and group representation theory [4].\n\nBispectral Neural Networks flexibly parameterize the bispectrum for arbitrary compact commutative groups, enabling both the group and the invariant map to be learned from data. The architecture is remarkably simple. It is comprised of two layers: a single learnable linear layer, followed by a fixed\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ncollection of triple products computed from the output of the previous layer. BNNs are trained with an objective function consisting of two terms: one that collapses all transformations of a pattern to a single point in the output (invariance), and another that prevents information collapse in the first layer (selectivity).\n\nWe demonstrate that BNNs trained to separate orbit classes in augmented data learn the group its Fourier transform, and corresponding bispectrum purely from the symmetries implicit in the data (Section 4.1). Because the model has learned the fundamental structure of the group, we show that it generalizes to novel, out-of-distribution classes with the same group structure and facilitates downstream group-invariant classification (Section 4.2). Further, we demonstrate that the trained network inherits the completeness of the analytical model, which endows the network with strong adversarial robustness (Section 4.3). Finally, we demonstrate that the weights of the network can be used to recover the group Cayley table—the fundamental signature of a group’s structure (Section 4.4). Thus, an explicit model of the group can be learned and extracted from the network weights. To our knowledge, our work is the first to demonstrate that either a bispectrum or a group Cayley table can be learned from data alone. Our results set the foundation of a new computational primitive for robust and interpretable representation learning.\n\n1.1 RELATED WORK The great success and efficiency of convolutional neural networks owe much to built-in equivariance to the group of 2D translations. In recent years, an interest in generalizing convolution to nonEuclidean domains such as graphs and 3D surfaces has led to the incorporation of additional group symmetries into deep learning architectures [5], [6]. In another line of work, Kakarala [7] and Kondor [8] pioneered the use of the analytical group-invariant bispectrum in signal processing and machine learning contexts. Both of these approaches require specifying the group of transformations a priori and explicitly building its structure into the network architecture. However, the groups that structure natural data are often either unknown or too complex to specify analytically—for example, when the structure arises from the interaction of many groups, or when the group acts on latent features in data.\n\nRather than building in groups by hand, another line of work has sought to learn underlying group structure solely from symmetries contained in data. The majority of these approaches use structured models that parameterize irreducible representations or Lie algebra generators, which act on the data through the group exponential map [9]–[14]. In these models, the objective is typically to infer the group element that acts on a template to generate the observed data, with inference accomplished through Expectation Maximization or other Bayesian approaches. A drawback of these models is that both the exponential map and inference schemes are computationally expensive. Thus, they are difficult to integrate into large-scale systems. A recent feed-forward approach [15] learns distributions over the group of 2D affine transformations in a deep learning architecture. However, the group is not learned in entirety, as it is restricted to a distribution on a pre-specified group.\n\nHere, we present a novel approach for learning groups from data in an efficient, interpretable, and fully feed-forward model that requires no prior knowledge of the group, computation of the exponential map, or Bayesian inference. The key insight in our approach is to harness the generality of the form of the group-invariant bispectrum, which can be defined for arbitrary compact groups. Here, we focus on the class of compact commutative groups.\n\n2 THE BISPECTRUM\n\nThe theory of groups and their representations provides a natural framework for constructing computational primitives for robust machine learning systems. We provide a one-page introduction to these mathematical foundations in Appendix A. Extensive treatment of these concepts can be found in the textbooks of Hall [16] and Gallier & Quaintance [17]. We now define the concepts essential to this work: invariance, equivariance, orbits, complete invariance, and irreducible representations.\n\nLet G be a group, X a space on which G acts, and f a signal defined on the domain X. The orbit of a signal is the set generated by acting on the domain of the signal with each group element, i.e. {f (gx) : g ∈ G}. In the context of image transformations this is the set of all transformed versions of a canonical image—for example, if G is the group SO(2) of 2D rotations, then the orbit contains all rotated versions of that image.\n\nA function φ : X → Y is G-equivariant if φ(gx) = g′φ(x) for all g ∈ G and x ∈ X, with g′ ∈ G′ homomorphic to G. That is, a group transformation on the input space results in a corresponding\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ngroup transformation on the output space. A function φ : X → Y is G-invariant if φ(x) = φ(gx) for all g ∈ G and x ∈ X. This means that a group action on the input space has no effect on the output. We say φ is a complete invariant when φ(x1) = φ(x2) if and only if x2 = gx1 for some g ∈ G and x1, x2 ∈ X. A complete invariant collapses each orbit to a single point while also guaranteeing that different orbits map to different points. In the context of rotation-invariant vision, for example, this would mean that the only information lost through the map φ is that of the specific “pose” of the object.\n\nA representation of a group G is a map ρ : G → GL(V ) that assigns elements of G to elements of the group of linear transformations (e.g. matrices) over a vector space V such that ρ(g1g2) = ρ(g1)ρ(g2). That is, ρ is a group homomorphism. In our case, V is Cn. A representation is reducible if there exists a change of basis that decomposes the representation into a direct sum of other representations. An irreducible representation is one that cannot be further reduced in this manner, and the set Irr(G) are often simply called the irreps of G. The irreducible representations of all finite commutative groups are one-dimensional (therefore equivalent to the characters of the irreps), and in bijection with the group itself.\n\n2.1 FOURIER ANALYSIS ON GROUPS A powerful approach for constructing invariants leverages the group theoretic generalization of the classical Fourier Transform [18]. Let f (g) : G → R be a signal on a group G, and let Irr(G) denote the irreducible representations of the group. We assume that ρ ∈ Irr(G) are also unitary representations (i.e., ρ−1 = ρ†, with ρ† the conjugate transpose). The Generalized Fourier Transform (GFT) is the linear map f (cid:55)→ ˆf , in which Fourier frequencies are indexed by ρ ∈ Irr(G):\n\nˆfρ =\n\n(cid:88)\n\ng∈G\n\nρ(g)f (g).\n\n(1)\n\nReaders familiar with the classical Fourier transform should identify ρ(g) with the exponential term in the usual sum or integral. Here, they arise naturally as the representations of Z/nZ or S1. Intuitively, these ρ are the “fundamental frequencies\" of the group. In particular, for G = Z/nZ—the group of integers modulo n—they are the classical n discrete Fourier frequencies: ρk(g) = e−i2πkg/n, for k = 0, . . . , n − 1 and g ∈ Z/nZ. For compact continuous groups G, the sum above is replaced by an integral (cid:82) ρ(u)f (u)du over a Haar measure du for the group.\n\nImportantly, the Fourier transform is equivariant to the action of translation on the domain of the input—a fundamental, if not defining, property. That is, if f t = f (g − t) is the translation of f by t ∈ G, its Fourier transform becomes:\n\n( ˆf t)ρ = ρ(t) ˆf . (2) For the classical Fourier transform, this is the familiar Fourier shift property: translation of the input results in phase rotations of the complex Fourier coefficients. This property is exploited in one well-known Fourier invariant—the power spectrum: qρ = ˆf †\n\nˆfρ.\n\n(3)\n\nρ\n\nThe power spectrum preserves only the magnitude of the Fourier coefficients, eliminating the phase information entirely by multiplying each coefficient by its conjugate: (cid:17)†(cid:16)\n\n(cid:17)\n\n(cid:16)\n\nqt k =\n\ne−i2πkt/n ˆfk\n\ne−i2πkt/n ˆfk\n\n= ei2πkt/ne−i2πkt/n ˆf †\n\nˆfk = qk.\n\nk\n\nAlthough this approach eliminates variations due to translations of the input, it also eliminates much of the signal structure, which is largely contained in relative phase relationships [19]. Thus, the power spectrum is incomplete. As a result, it is easy to create “adversarial examples” demonstrating the incompleteness of this invariant: any two images with identical Fourier magnitudes but non-identical phase have the same power spectra (Appendices C and D).\n\n2.2 THE BISPECTRUM The bispectrum is a lesser known Fourier invariant parameterized by pairs of frequencies in the Fourier domain. The translation-invariant bispectrum for 1D signals f ∈ Rn is the complex matrix B ∈ Cn×n:\n\n(4) where ˆf = ( ˆf0, . . . , ˆfn−1) is the 1D Fourier transform of f and the sum i + j is taken modulo n. Intuitively, bispectral coefficients Bij indicate the strength of couplings between different phase\n\nBi,j = ˆfi\n\nˆfj\n\nˆf † i+j,\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ncomponents in the data, preserving the phase structure that the power spectrum loses. Again, the equivariance property of the GFT is key to B’s invariance. Like the power spectrum, it cancels out phase shifts due to translation. However it does so while preserving the relative phase structure of the signal. This remarkable property may be seen as follows:\n\nBt\n\nk1,k2\n\n= e−i2πk1t/n ˆfk1e−i2πk2t/n ˆfk2 ei2π(k1+k2)t/n ˆf † ˆfk2 = e−i2πk1t/ne−i2πk2t/nei2πk1t/nei2πk2t/n ˆfk1\n\nk1+k2 ˆf † k1+k2\n\n= Bk1,k2.\n\nUnlike the power spectrum, the bispectrum (for compact groups) is complete and can be used to reconstruct the original signal up to translation [20]. More generally, the bispectrum is defined for arbitary compact groups:\n\nBρi,ρj = ˆfρi\n\nˆfρj\n\nˆf † ρiρj\n\n,\n\n(5)\n\nwith the addition modulo n replaced by the group product. For non-commutative groups, the bispectrum has a similar structure, but is modified to accommodate the fact that the irreps of noncommutative groups map to matrices rather than scalars (Appendix B).\n\nHistorically, the bispectrum emerged from the study of the higher-order statistics of non-Gaussian random processes, and was used as a tool to measure the non-Gaussianity of a signal [21]–[23]. The work of Yellott and Iverson [20] established that every integrable function with compact support is completely identified—up to translation—by its three-point autocorrelation and thus its bispectrum (Appendix F). Later work of Kakarala tied the bispectrum to the more general formulation of Fourier analysis and thus defined it for all compact groups [4], [7]. Kakarala went on to prove that it is complete [24] in both commutative and non-commutative formulations (Appendix D).\n\n3 BISPECTRAL NEURAL NETWORKS\n\nWe draw inspiration from the group-invariant bispectrum to define Bispectral Neural Networks, a neural network architecture that parameterizes the ansatz of the bispectrum. The key idea behind this approach is to constrain the optimization problem by building in the functional form of a solution known to exist. Importantly, we build in only the generic functional form and allow the network to learn the appropriate parameters (and thus the group over which it is defined) from data. We draw upon two key properties of the bispectrum, which allow it to be naturally parameterized and learned:\n\n1. The bispectrum is computed on a group-equivariant Fourier transform. The Fourier transform is a linear map and can be written as z = W x, with W a matrix of irreducible representations—the Discrete Fourier Transform (DFT) matrix, in the classical case. If the group is unknown, we can treat W as a matrix of parameters to be learned through optimization. The bispectrum computation is identical regardless of the structure of the irreps; thus we can use a single network architecture to learn finite approximations of arbitrary compact commutative groups.\n\n2. The bispectrum separates orbits. Here, we use orbit separation at the output of the bispectrum computation as the criterion to optimize the parameters defining the group-equivariant Fourier transform. This requires knowing only which datapoints are in the same orbit (but not the group that generates the orbit), and is similar in spirit to contrastive learning [25].\n\n3.1 MODEL ARCHITECTURE Concretely, given an input vector x ∈ Rn, the Bispectral Neural Network module is completely specified by one complex matrix W ∈ Cn×n, where each row Wi ∈ Cn plays the role of an irreducible representation. This matrix defines the linear transformation:\n\nz = W x. (6) If W defines a Fourier transform on G, then each element of z is a coefficient on a different irreducible representation of G. The analytical bispectrum can then be computed from triples of coefficients as: Bi,j = zizjz† (7) However, if the group is unknown, then we do not know the product structure of the group and thus do not know a priori which element in the vector z corresponds to the (ij)th irrep. Conveniently, for commutative groups, all irreducible representations are in bijection with the group elements. That is,\n\nij\n\n4\n\nPublished as a conference paper at ICLR 2023\n\neach irreducible representation can be uniquely identified with a group element. Consequently, the (ij)th representation can obtained from the element-wise multiplication of the ith and jth irreducible representations. Thus, the (conjugated) (ij)th coefficient can be obtained as\n\nz† ij = (Wi ⊙ Wj)†x, (8) with ⊙ indicating the Hadamard (element-wise) product and † the complex conjugate. Plugging this into Equation 7 and expanding the terms, we obtain the Bispectral Network:\n\nβi,j(x) = Wix · Wjx · (Wi ⊙ Wj)†x\n\n(9)\n\nFigure 1 illustrates the computation. Each linear term Wix is an inner product yielding a scalar value, and the three terms are combined with scalar multiplication in C. Note that this equation shows the computation of a single scalar output βi,j. In practice, this is computed for all non-redundant pairs.\n\nFigure 1: Bispectral Neural Networks. A single linear neural network layer parameterized by W generates the output z = W x. Pairs of coefficients zi and zj and the corresponding weights Wi and Wj are then used to compute the output of the network βi,j = zizjz† ij, with zij computed as (Wi ⊙ Wj)x. Here, the weights are depicted as the canonical Fourier basis on S1 for illustrative purposes. In practice, the weights are randomly initialized, but are expected to converge to the irreps of the group that structures the dataset.\n\nWhile the bispectrum is a third-order polynomial, it results in only n2 rather than n3 products, due to the constraint on the third term. Moreover, the bispectrum has many symmetries and is thus highly redundant. The most obvious symmetry is reflection over the diagonal—i.e. βi,j = βj,i. However, there are many more (see Appendix G). Here, we compute only the upper triangular of the matrix.\n\nAs the Bispectral Network computes third-order products of the input, the output values can be both very large and very small, which can result in numerical instability during optimization. To combat this, we normalize the output of the network to the unit sphere in Cn:\n\n ̄β(x) =\n\nβ(x) ||β(x)||2\n\n(10)\n\nWe demonstrate in Appendix E that the analytical bispectrum can be normalized to the unit sphere while preserving completeness up to a scalar factor (Appendix E, Theorem E.3). Thus, we similarly expect that this normalization will not impact the robustness of the network.\n\n3.2 ORBIT SEPARATION LOSS Given a dataset X = {x1, ..., xm} with latent group structure and labels Y = {y1, ..., ym} indicating which datapoints are equivalent up to transformation (but without information about the transformation itself, i.e. how they are related), we wish to learn an invariant map that collapses elements of the same group orbit and separates distinct orbits. Critically, we want this map to be both invariant to group actions and selective to pattern structure—that is, to be complete [24]. A network that successfully learns such a map will have implicitly learned the group that structures transformations in the data.\n\nTo that end, we introduce the Orbit Separation Loss. This loss encourages the network to map elements of the same orbit to the same point in the representation space, while ensuring degenerate solutions (i.e. mapping all inputs to the same point) are avoided: (cid:88)\n\n|| ̄β(xa) − ̄β(xb)||2\n\n2 + γ||xa − W †W xa||2 2.\n\nL(xa) =\n\n(11)\n\nj|yb=ya\n\nThe first term measures the Euclidean distance in the output space between points in the same orbit. Thus, minimizing the loss encourages them to be closer. In the second term, the input xa is reconstructed by inverting the linear transform as W †W xa. The second term is thus a measure of\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwhether information is preserved in the map. During training, each row vector in the weight matrix W is normalized to unit length after every gradient update, which pushes W towards the manifold of unitary matrices. The coefficient γ weights the contribution of the reconstruction error to the loss.\n\n4 EXPERIMENTS AND ANALYSES\n\nWe now test the capacity of Bispectral Networks to learn a group by learning to separate orbit classes. We conduct four experiments to analyze the properties of the architecture: learning Fourier transforms on groups (Section 4.1), group-invariant classification (Section 4.2), model completeness (Section 4.3), and extracting the Cayley table (Section 4.4).\n\n4.1 LEARNING FOURIER TRANSFORMS ON GROUPS We first examine whether Bispectral Networks trained to separate orbit classes will learn the Fourier transform and corresponding bispectrum of the group that structures the data. Here, we test the network on two groups, which act on the image grid to generate image orbits. The first is one whose bispectrum is known and well-understood: the group S1 × S1 of 2D cyclic translations, i.e. the canonical 2D Fourier transform. The second is the group SO(2). To our knowledge, a bispectrum on SO(2) acting on the grid (or disk) has not been defined in the mathematics literature, as the domain is not a homogeneous space for the group—an assumption fundamental to much of bispectrum theory. However, a Fourier transform for SO(2) acting on the disk has been defined [26] using the orthogonal basis of disk harmonics. We thus expect our weight matrix W to converge to the canonical Fourier basis for the group S1 × S1, and hypothesize the emergence of disk harmonics for SO(2).\n\nModels are trained on datasets consisting of 100 randomly sampled (16, 16) natural image patches from the van Hateren dataset [27] that have been transformed by the two groups to generate orbit classes. Further details about the dataset can be found in Appendix H. Networks are trained to convergence on the Orbit Separation Loss, Equation 11. All hyperparameters and other details about the training procedure can be found in Appendix I.1.\n\nRemarkably, we find that the relatively loose constraints of the orbit separation loss and the product structure of the architecture is sufficient to encourage the network to learn a group-equivariant Fourier transform and its corresponding bispectrum from these small datasets. This is observed in the structure of the network weights, which converge to the irreducible representations of the group in the case of S1 × S1, and filters resembling the disk harmonics for SO(2)) (Figure 2). Importantly, we observe that the learned linear map (Equation 6) is equivariant to the action of the group on the input, and the output of the network (Equation 10) is invariant to the group action on arbitrary input signals. We visualize this in Figure 3 for transformed exemplars from the MNIST dataset, which the model was not trained on, demonstrating the capacity of the model to generalize these properties to out-of-distribution data.\n\nFigure 2: Filters Learned in Bispectral Networks. Real components of weights learned in the Bispectral Network trained on the transformed van Hateren Natural Images datasets, for S1 × S1 2D Translation (left) and SO(2) 2D Rotation dataset (right). The full set of learned weights can be found in Appendix J.\n\n4.2 GROUP-INVARIANT CLASSIFICATION\n\nWe next examine the utility of the rotation model learned in Section 4.1 for facilitating performance on a downstream image classification task: digit classification for the Rotated MNIST dataset. For this experiment, we append a simple multi-layer fully-connected ReLU network (MLP; the “classification network”) to the output of the Bispectral Network and train it on the classification objective. While the Bispectral Network learns to eliminate factors of variation due to group actions, the classification network learns to eliminate other variations within digit classes irrelevant to classification, i.e. those due to handwriting style. We train the classification model using the scheme described in Appendix I. We compare the performance of our model to four leading approaches to group-invariant classification, in addition to a Linear Support Vector Machine (SVM) model that we train as a baseline (Table 1).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Model Outputs on Transformed Data. Real components of the outputs of the linear transform (W x, top row) and full network output (Eq. 10, bottom row). Data inputs are cyclically translated/rotated in one direction (x-axis) and layer/network outputs are computed for each transformation. Each colored line represents the output of a single neuron, with the activation value on the y-axis. In the top row, only four neuron outputs are visualized, for clarity. The equivariance of the first layer is observed in the sinuisoidal responses, which reveal equivariant phase shifts on each neuron’s output as the input transforms. The invariance of the network output is observed in the constant response across all translated or rotated images.\n\nOur model obtains a competitive 98.1% classification accuracy on this task—within 0.2 - 1.2% of the state-of-the-art methods compared to here. It also possesses several notable differences and advantages to the models presented in this table. First, our model learns the group from data, in contrast to Harmonic Networks and E2CNNs—which build in SO(2)-equivariance—and Augerino, which learns a distribution over the pre-specified group of 2D affine transformations. Although our method is marginally outperformed by these models in terms of classification accuracy, ours outperforms [13], the only other model that fully learns a group. Second, our model is fully feedforward, and it does not rely on the computationally intensive EM inference scheme used in [13], or the exponential map as used in [13] and [15]. Finally, in the next section, we demonstrate that our model is approximately complete. By contrast, we demonstrate, representations within the leading models E2CNN and Augerino exhibit excessive invariance.\n\nModel\n\nLearns Group\n\nFully Feedforward\n\nApproximately RotMNIST Accuracy\n\nComplete\n\nLinear SVM (Baseline)\n\nCohen & Welling (2014) [13]\n\nHarmonic Networks (2017) [28]\n\nAugerino (2020) [15]\n\nE2CNN (2019) [29]\n\nBispectral Network + MLP\n\nN\n\nY\n\nN\n\nY∗\n\nN\n\nY\n\nNA\n\nN\n\nY\n\nY\n\nY\n\nY\n\n-\n\n-\n\n-\n\nN\n\nN\n\nY\n\n54.7%\n\n∼ 97%†\n\n98.3%\n\n98.9%\n\n99.3%\n\n98.1%\n\nTable 1: Model Comparison on RotMNIST. Bispectral Networks achieve 98.1% classification accuracy on RotMNIST. The model is marginally outperformed by three models that require the partial or complete specification of the group structure in their design. Our model, by contrast learns the group entirely from data. Further, our model is approximately complete, while the leading models E2CNN and Augerino exhibit excessive invariance (Figure 4). †This number was estimated from the bar plot in Figure 4 of [13] and approved by the first author of the paper in personal communication. ∗ The model in [15] learns a distribution over a pre-specified group. It is different in kind to [13] and our model, which learn the group itself.\n\n4.3 MODEL COMPLETENESS\n\nA key property of the analytical bispectrum is its completeness: only signals that are exactly equal up to the group action will map to the same point in bispectrum space. Here, we analyze the completeness of the trained models, using Invariance-based Adversarial Attacks (IAAs) [2] as a proxy. IAAs aim to identify model metamers—inputs that yield identical outputs in the model’s representation space, but are perceptually dissimilar in the input space. For an approximately complete model, we expect\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nall inputs that map to the same point in model representation space to belong to the same orbit. That is, the only perturbations in pixel space that will give rise to identical model representations are those due to the group action. We test the completeness of the trained models as follows. A batch of targets {x1, ..., xN }, x ∈ Rm are randomly drawn from the RotMNIST dataset. A batch of inputs { ̄x1, ..., ̄xN } , ̄x ∈ Rm are randomly initialized, with pixels drawn from a standard normal distribution. Both the targets and random inputs are passed through the network to yield ̄β(xi) (target) and ̄β( ̄xi) (adversary). The following objective is minimized with respect to the ̄xi:\n\nL =\n\nN (cid:88)\n\ni=0\n\n|| ̄β(xi) − ̄β( ̄xi)||2\n\n2\n\n(12)\n\nAn optimized input ̄xi is considered a model metamer if it yields a model representation within ε of ̄β(xi), is classified equivalently as xi, but is not in the orbit of xi. Here, we aim to minimize distance between targets and adversaries in the model representation space—i.e. the output of the Bispectral Network, prior to the classification model. For the comparison models E2CNN and Augerino, we use the output of the layer prior to the softmax.\n\nFigure 4 shows the targets and the optimized adversarial inputs for the Bispectral Network trained on rotation in the van Hateren dataset, and the E2CNN and Augerino models trained on Rotated MNIST. For the Bispectral Network, we observe that all random initializations consistently converge to an element within the orbit of the target, indicating that the invariant map learned by the model is close to complete. By contrast, many unrelated or noisy metameric images can be found for E2CNN and Augerino. These metamers yield identical model representations (up to ε) and identical classifications as the targets, but are not contained in their orbits of the targets. Interestingly, the metamers optimized for Augerino possess structure that at times resembles features in the target image. We roughly estimate the percentage of these “perceptually similar” inputs as 35%. The model nonetheless exhibits excessive invariance and is not complete.\n\nFigure 4: Completeness in Bispectral Networks. Targets and optimized inputs for (left) E2CNN, (middle) Augerino, and (right) Bispectral Networks. Images are randomly initialized and optimized to yield representations identical to a target image. If a model possesses excessive invariance, there exist images that will yield equivalent representations while being non-equivalent up to the group action. For Bispectral Networks, all optimized inputs are within the orbits of the targets, indicating approximate completenes. By contrast, many model metamers are found for E2CNN and Augerino.\n\n4.4 LEARNING THE GROUP: EXTRACTING THE CAYLEY TABLE\n\nFinally, we demonstrate that the converged network weights can be used to construct the group itself —i.e. the product structure of the group, as captured by the group’s Cayley table. For finite groups, a Cayley table provides a complete specification of group structure. Here, we demonstrate a\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nnovel method for constructing a Cayley table from learned irreps, described in in Appendix K, which exploits the bijection between irreps and group elements in commutative groups. We test this approach on all unique finite commutative groups of order eight: Z/8Z, Z/4Z × Z/2Z, and Z/2Z × Z/2Z × Z/2Z. For each group, we generate a synthetic dataset consisting of 100 random functions over the group, f : G → R, with values in R drawn from a standard normal distribution, and generate the orbit of each exemplar under the group. Networks are trained to convergence on each dataset using the Orbit Separation Loss, Equation 11. Details about the training procedure can be found in Appendix I.4. After a model has converged, we use its learned weights to compute a Cayley table using Algorithm 1 in Appendix K. We check the isomorphism of the learned Cayley table with the ground truth using Algorithm 3 in Appendix K.\n\nFigure 5: Learned Cayley Tables for All Unique Groups of Order Eight. Rows and columns are labeled with integers that index group elements. Each cell of the table contains the index of the group element obtained by composing the group elements indexed in the row and column. The Cayley table thus contains all information about the group structure. Bispectral Networks perfectly recover the Cayley tables for all groups tested here.\n\nStrikingly, we are able to recover the exact structure of the groups that generated the data orbits from the converged models’ weights, in the form of the Cayley tables displayed in Figure 5. For each dataset, the recovered Cayley tables are identical to the known ground-truth Cayley tables. To our knowledge, this is the first demonstration that an explicit representation of the product structure of an unknown group can be learned from data. Our results on all finite commutative groups of order eight suggest that this will generalize more broadly to all finite commutative groups.\n\n5 DISCUSSION In this work, we introduced a novel method for learning groups from data in a neural network architecture that simultaneously learns a group-equivariant Fourier transform and its corresponding group-invariant bispectrum. The result is an interpretable model that learns the irreducible representations of the group, which allow us to recover an explicit model of the product structure of the group—i.e. the group itself, in the form of its Cayley table. The resulting trained models are both powerful and robust—capable of generalizing to arbitrary unseen datasets that are structured by the same transformation group, while exhibiting approximate completeness. To our knowledge, these results provide the first demonstration that a bispectrum or Cayley table can be learned simply from the symmetries in data alone. We view the work presented here as a starting point for learning the more complex groups needed to recognize 3D objects in natural imagery. In ongoing work, we are extending the model to the non-commutative form of the bispectrum (Appendix B), and developing a localized version amenable to stacking hierarchically in deep networks.\n\nThe precise convergence of the Bispectral Network parameters to canonical forms from representation theory—under the relatively weak constraints of the third-order model ansatz and the orbit separation loss—suggests the uniqueness of this solution within the model class. Indeed, the Fourier solution is likely guaranteed by the uniqueness of the bispectrum as a third-order polynomial invariant [30]. Future work should explore the bounds of these guarantees in the context of statistical learning. We venture to suggest further that the uniqueness and completeness of the bispectrum make it a compelling candidate computational primitive for invariant perception in artificial and biological systems alike. Indeed, results from across areas of visual cortex suggest that group representations may play an important role in the formation of visual representations in the brain [31]. Models like the one presented here may provide concrete hypotheses to test in future mathematically motivated neuroscience research.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nThe code to implement all models and experiments in this paper can be found at github.com/sophiaas/bispectral-networks. We provide files containing the trained models, configs containing the hyperparameters used in these experiments, notebooks to generate the visualizations in the paper, and download links for the datasets used in these experiments.\n\nACKNOWLEDGMENTS\n\nThe authors thank Nina Miolane, Khanh Dao Duc, Giovanni Luca Marchetti, David Klindt, Mathilde Papillon, Adele Myers, Taco Cohen, and Ramakrishna Kakarala for helpful suggestions, feedback, and proofreading.\n\nREFERENCES\n\n[1]\n\n[2]\n\nI. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014. J.-H. Jacobsen, J. Behrmann, R. Zemel, and M. Bethge, “Excessive invariance causes adversarial vulnerability,” arXiv preprint arXiv:1811.00401, 2018.\n\n[3] W. Brendel and M. Bethge, “Approximating cnns with bag-of-local-features models works\n\nsurprisingly well on imagenet,” arXiv preprint arXiv:1904.00760, 2019.\n\n[4] R. Kakarala, “A group-theoretic approach to the triple correlation,” in [1993 Proceedings] IEEE Signal Processing Workshop on Higher-Order Statistics, IEEE, 1993, pp. 28–32. [5] T. Cohen and M. Welling, “Group equivariant convolutional networks,” in International\n\nconference on machine learning, PMLR, 2016, pp. 2990–2999.\n\n[6] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi ́c, “Geometric deep learning: Grids,\n\ngroups, graphs, geodesics, and gauges,” arXiv preprint arXiv:2104.13478, 2021.\n\n[7] R. Kakarala, “The bispectrum as a source of phase-sensitive invariants for fourier descriptors: A group-theoretic approach,” Journal of Mathematical Imaging and Vision, vol. 44, no. 3, pp. 341–353, 2012.\n\n[8] R. Kondor, “A novel set of rotationally and translationally invariant features for images based\n\non the non-commutative bispectrum,” arXiv preprint cs/0701127, 2007.\n\n[9] R. P. Rao and D. L. Ruderman, “Learning lie groups for invariant visual perception,” Advances\n\nin neural information processing systems, pp. 810–816, 1999.\n\n[10] X. Miao and R. P. Rao, “Learning the lie groups of visual invariance,” Neural computation,\n\nvol. 19, no. 10, pp. 2665–2693, 2007.\n\n[11] B. J. Culpepper and B. A. Olshausen, “Learning transport operators for image manifolds.,” in\n\n[12]\n\nNIPS, 2009, pp. 423–431. J. Sohl-Dickstein, C. M. Wang, and B. A. Olshausen, “An unsupervised algorithm for learning lie group transformations,” arXiv preprint arXiv:1001.1027, 2010.\n\n[13] T. Cohen and M. Welling, “Learning the irreducible representations of commutative lie groups,”\n\nin International Conference on Machine Learning, PMLR, 2014, pp. 1755–1763.\n\n[14] H. Y. Chau, F. Qiu, Y. Chen, and B. Olshausen, “Disentangling images with lie group transfor-\n\nmations and sparse coding,” arXiv preprint arXiv:2012.12071, 2020.\n\n[15] G. Benton, M. Finzi, P. Izmailov, and A. G. Wilson, “Learning invariances in neural networks from training data,” Advances in neural information processing systems, vol. 33, pp. 17 605– 17 616, 2020.\n\n[16] B. C. Hall, “Lie groups, lie algebras, and representations,” in Quantum Theory for Mathemati-\n\n[17]\n\ncians, Springer, 2013, pp. 333–366. J. Gallier and J. Quaintance, Differential Geometry and Lie Groups: A Computational Perspective. Springer Nature, 2020, vol. 12.\n\n[18] W. Rudin, Fourier analysis on groups. Courier Dover Publications, 1962. [19] A. V. Oppenheim and J. S. Lim, “The importance of phase in signals,” Proceedings of the\n\nIEEE, vol. 69, no. 5, pp. 529–541, 1981.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\n[20]\n\n[21]\n\nJ. Yellott Jr and G. Iverson, “Uniqueness theorems for generalized autocorrelations,” Journal of the Optical Society of America, vol. 9, pp. 388–404, 1992. J. Tukey, “The spectral representation and transformation properties of the higher moments of stationary time series,” Reprinted in The Collected Works of John W. Tukey, vol. 1, pp. 165–184, 1953.\n\n[22] D. Brillinger, “Some history of higher-order statistics and spectra,” Stat. Sin., vol. 1, pp. 465–\n\n476, 1991.\n\n[23] C. L. Nikias and J. M. Mendel, “Signal processing with higher-order spectra,” IEEE Signal\n\nprocessing magazine, vol. 10, no. 3, pp. 10–37, 1993.\n\n[24] R. Kakarala, “Completeness of bispectrum on compact groups,” arXiv preprint\n\narXiv:0902.0196, vol. 1, 2009.\n\n[25] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by learning an invariant mapping,” in 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), IEEE, vol. 2, 2006, pp. 1735–1742.\n\n[26] S. C. Verrall and R. Kakarala, “Disk-harmonic coefficients for invariant pattern recognition,”\n\n[27]\n\nJOSA A, vol. 15, no. 2, pp. 389–401, 1998. J. H. Van Hateren and A. van der Schaaf, “Independent component filters of natural images compared with simple cells in primary visual cortex,” Proceedings of the Royal Society of London. Series B: Biological Sciences, vol. 265, no. 1394, pp. 359–366, 1998.\n\n[28] D. E. Worrall, S. J. Garbin, D. Turmukhambetov, and G. J. Brostow, “Harmonic networks: Deep translation and rotation equivariance,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5028–5037.\n\n[29] M. Weiler and G. Cesa, “General e (2)-equivariant steerable cnns,” Advances in Neural\n\nInformation Processing Systems, vol. 32, 2019.\n\n[30] B. Sturmfels, Algorithms in invariant theory. Springer Science & Business Media, 2008. [31] S. Sanborn, A Group Theoretic Framework for Neural Computation. UC Berkeley, PhD Thesis,\n\n2021.\n\n[32] G. B. Giannakis, “Signal reconstruction from multiple correlations: Frequency-and time-\n\ndomain approaches,” JOSA A, vol. 6, no. 5, pp. 682–697, 1989.\n\n[33] R. Kondor, Group theoretical methods in machine learning. Columbia University, PhD Thesis,\n\n2008.\n\n[34] C. A. Haniff, “Least-squares fourier phase estimation from the modulo 2π bispectrum phase,”\n\n[35]\n\nJOSA A, vol. 8, no. 1, pp. 134–140, 1991. J. H. van Hateren and A. van der Schaaf, “Independent component filters of natural images compared with simple cells in primary visual cortex,” Proceedings: Biological Sciences, vol. 265, no. 1394, pp. 359–366, Mar. 1998.\n\n[36] Y. LeCun, C. Cortes, and C. Burges, “The mnist dataset of handwritten digits (images),” NYU:\n\nNew York, NY, USA, 1999.\n\n[37] A. Paszke, S. Gross, F. Massa, et al., “Pytorch: An imperative style, high-performance deep learning library,” Advances in neural information processing systems, vol. 32, pp. 8026–8037, 2019. I. Nazarov and H. Schroter, Cplxmodule, https : / / github . com / ivannz / cplxmodule, 2021.\n\n[38]\n\n[39] C. Trabelsi, O. Bilaniuk, Y. Zhang, et al., “Deep complex networks,” arXiv preprint\n\narXiv:1705.09792, 2017.\n\n[40] K. Musgrave, S. Belongie, and S.-N. Lim, “Pytorch metric learning,” arXiv preprint\n\narXiv:2008.09164, 2020.\n\n[41] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint\n\narXiv:1412.6980, 2014.\n\n[42] L. N. Smith, “Cyclical learning rates for training neural networks,” in 2017 IEEE winter\n\nconference on applications of computer vision (WACV), IEEE, 2017, pp. 464–472.\n\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026–1034.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX A MATHEMATICAL BACKGROUND\n\nGroups. A group (G, ·) is a set G with a binary operation ·, which we can generically call the product. The notation a · b denotes the product of two elements in the set; however, it is standard to omit the operator and write simply ab. Concretely, a group G may define a class of transformations, such as two-dimensional translations or rotations in the plane. The elements of the group g ∈ G define particular transformations, such as rotation by 30◦ or rotation by 90◦. The binary operation · provides a means for combining two particular transformations—for example, first rotating by 30◦ and then rotating by 90◦. For a set of transformations G to be a group under the operation ·, the four following axioms must hold:\n\n1. Closure: The product of any two elements of the group is also an element of the group, i.e.\n\nfor all a, b ∈ G, ab ∈ G.\n\n2. Associativity: The grouping of elements under the operation does not change the outcome,\n\nso long as the order of elements is preserved, i.e. (ab)c = a(bc).\n\n3. Identity: There exists a “do-nothing” identity element e that such that the product of e with\n\nany other element g returns g, i.e. ge = eg = g for all g ∈ G.\n\n4. Inverse: For every element g, there exists an inverse element g−1 such that the product of g\n\nand g−1 returns the identity, i.e. gg−1 = g−1g = e.\n\nHomomorphisms. Two groups (G, ·) and (H, ∗) are homomorphic if there exists a correspondence between elements of the groups that respect the group operation. Concretely, a homomorphism is a map ρ : G → H such that ρ(u · v) = ρ(u) ∗ ρ(v). An isomorphism is a bijective homomorphism.\n\nProduct Groups. A product group as a set is the cartesian product G × H of two given groups G and H. The new group product is given by (g1, h1) · (g2, h2) = (g1g2, h1h2).\n\nCommutativity. A group (G, +) is commutative or abelian if the order of operations does not matter, i.e. ab = ba. If this does not hold for all elements of the group, then the group is called non-commutative. The classification of finite commutative groups says that each such group is a product of cyclic groups.\n\nGroup Actions. A group action is a map T : G × X → X that maps (g, x) pairs to elements of X. We say a group G acts on a space X if the following properties of the action T hold:\n\n1. The identity e ∈ G maps an element of x ∈ X to itself, i.e. T (e, x) = x\n\n2. Two elements g1, g2 ∈ G can be combined before or after the map to yield the same result,\n\ni.e. T (g1, T (g2, x)) = T (g1g2, x)\n\nFor simplicity, we will use the shortened notation Tgx to denote T (g, x), often expressed by saying that a point x maps to gx (= Tg(x)).\n\nInvariance. A function φ : X (cid:55)→ Y is G-invariant if φ(x) = φ(gx) for all g ∈ G and x ∈ X. This means that group actions on the input space have no effect on the output.\n\nEquivariance. A function φ : X (cid:55)→ Y is G-equivariant if φ(gx) = g′φ(x) for all g ∈ G and x ∈ X, with g′ ∈ G′, a group homomorphic to G that acts on the output space. This means that a group action on the input space results in a corresponding group action on the output space.\n\nOrbits. Given a point x ∈ X, the orbit Gx of x is the set {gx : g ∈ G}. In the context of image transformations, the orbit defines the set of all transformed versions of a canonical image—for example, if G is the group of translations, then the orbit contains all translated versions of that image.\n\nHomogeneous Spaces. We say that X is a homogeneous space for a group G if G acts transitively on X—that is, if for every pair x1, x2 ∈ X there exists an element of g ∈ G such that gx1 = x2. A homogeneous space X equipped with the action of G is called a G-space. The concept can be clearly illustrated by considering the surface of a sphere, the space S2. The group SO(3) of orthogonal 3 × 3 matrices with determinant one defines the group of 3-dimensional rotations. The sphere S2 is a homogeneous space for SO(3). For every pair of points on the sphere, one can define a 3D rotation matrix that takes one to the other.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nRepresentations. A representation of a group G is a map ρ : G → GL(V ) that assigns elements of G to elements of the group of linear transformations over a vector space V such that ρ(g1g2) = ρ(g1)ρ(g2). That is, ρ is a group homomorphism. In many cases, V is Rn or Cn. A representation is reducible if there exists a change of basis that decomposes the representation into a direct sum of other representations. An irreducible representation is one that cannot be further reduced in this manner, and the set of them Irr(G) are often simply called the irreps of G. The irreducible representations of a finite commutative group are all one-dimensional and in bijection with the group itself. In particular, it is straightforward to compute Irr(G) for such groups given their classification as products of cyclic groups Z/nZ.\n\nAPPENDIX B THE BISPECTRUM ON NON-COMMUTATIVE GROUPS\n\nThe theory of the bispectrum applies also in the setting of non-commutative groups and can be expressed in terms of the generalized Fourier transform and the (not necessarily one-dimensional) representations of the group. The more general, non-commutative form of the bispectrum is:\n\nβρi,ρj = [ ˆf ρi ⊗ ˆf ρj]Cρi,ρj\n\n(cid:104) (cid:77)\n\n(cid:105)\n\nˆf †\n\nρ\n\nC †\n\nρi,ρj\n\n,\n\n(13)\n\nwhere ⊗ is the tensor product, ⊕ is a direct sum over irreps, and the unitary matrix Cρi,ρj defines a Clebsch-Gordan decomposition on the tensor product of a pair of irreps ρi, ρj [8].\n\nρ∈ρi⊗ρj\n\nAPPENDIX C INCOMPLETENESS OF THE POWER SPECTRUM\n\nFigure 6: The Power Spectrum is a Non-Selective Invariant. Two images with identical power spectra (top and middle) can have very different image structure. However, images with identical phase spectra and non-identical power spectra (top and bottom) share much of the meaningful image structure—that which defines edges and contours. May require enlargement to see. Note that the sharp horizontal and vertical lines in the power and phase spectra are artifacts due to the wrap-around of the image boundaries.\n\n13\n\nThe Power Spectrum is a Non-Selective InvariantLog Power0π2π3π2Phase0π3π2Power SpectrumPhase SpectrumImageLog Powerπ2PhaseTargetRandom PhaseRandom Power0π2π3π2PhasePowerPublished as a conference paper at ICLR 2023\n\nAPPENDIX D COMPLETENESS OF THE BISPECTRUM\n\nThe bispectrum is a (generically) complete invariant map with respect to a given compact group G [24]. Here, generic is a technical term that means “outside of a set of measure zero.” For instance, in the finite commutative group case, this amounts to uniqueness for all elements of the space that have all-nonzero Fourier coefficients. In practice, this property is easily derived from the exactly computable inverse of the bispectrum; see, for instance, algorithms spelled out in [32], [33]. In these computations, an inverse is defined whenever denominators—which end up being Fourier coefficients recursively computed—appearing in the calculations are nonzero.\n\nAPPENDIX E COMPLETENESS OF THE NORMALIZED BISPECTRUM\n\nA useful observation is that this generalizes somewhat to a normalized form of the bispectrum, where we project features to the unit sphere in Cn (normalization).\n\nDefinition E.1. The normalized bispectrum of a function f is defined as\n\n ̄β(f ) =\n\nβ(f ) ||β(f )||2\n\n.\n\nFirst, we show that any scalar multiple of the bispectrum preserves completeness up to positive scalar multiplication of the input signal f . Using this result, we show that the normalized bispectrum is generically complete up to the action of G and scalar multiplication.\n\nLemma E.1. The bispectrum of a scalar c > 0 times a function f is:\n\nβ(cf ) = c3β(f ).\n\nProof. By linearity of the Fourier transform, we have: ˆcf = c ˆf . From the definition, the commutative bispectrum of cf therefore satisfies:\n\nβ(cf ) = c ˆfic ˆfjc ˆfi+j = c3 ˆfi\n\nˆfj\n\nˆfi+j = c3β(f ).\n\n(14)\n\nThis is easily seen to also work for the non-commutative formulation.\n\nLemma E.2. If the bispectra of two (generic) functions f and g differ by a constant factor c > 0, then the orbits of the two functions differ by a constant factor, i.e.\n\nβ(f ) = cβ(g) =⇒ Gf = c\n\n1\n\n3 Gg.\n\nProof. Assume β(f ) = cβ(g) and let h = c 1 3 g. By Lemma E.1, we have: β(h) = cβ(g), and by assumption, β(f ) = cβ(g) so that β(f ) = β(h). By (generic) completeness of the bispectrum, the orbits of f and h are equivalent; that is, Gf = Gh. Because h is a scalar multiple of g, the orbits of h and g are related by a constant factor: Gh = c 1 3 Gg. Finally, because the orbit of f equals that of h, we see that the orbit of f is related to that of g by the same factor: Gf = c 1\n\n3 Gg.\n\nTheorem E.3. Two (generic) functions f and g have equivalent normalized bispectra if and only if their orbits under G are related by a scalar factor c ∈ R.\n\nˆβ(f ) = ˆβ(g) ⇐⇒ Gf = cGg.\n\nProof. (⇐) By assumption, Gf = cGg, which, together with Lemma E.1, implies β(f ) = c3β(g).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nWriting out the definition of the normalized bispectrum, we see\n\n ̄β(f ) =\n\n=\n\n=\n\n=\n\nβ(f ) ||β(f )|| c3β(g) ||c3β(g)|| c3β(g) c3||β(g)||\n\nβ(g) ||β(g)||\n\n= ̄β(g).\n\n(⇒) Given ̄β(f ) = ̄β(g), we expand and see the following relation:\n\n ̄β(f ) = ̄β(g)\n\nβ(f ) ||β(f )||\n\n=\n\nβ(f ) =\n\nβ(g) ||β(g)||\n\n||β(f )|| ||β(g)||\n\nβ(g).\n\nLetting c = ||β(f )||\n\n||β(g)|| , we have:\n\nwhich by Lemma E.2 implies that Gf = cGg.\n\nβ(f ) = cβ(g),\n\n(15)\n\n(16)\n\nAPPENDIX F POLYSPECTRA AND AUTOCORRELATIONS\n\nBoth the power spectrum and bispectrum are intimately related to the second- and third-order statistics of a signal, as the Fourier transforms of the two- and three-point autocorrelation functions, respectively. The two-point autocorrelation A2 of a complex-valued function f on the real line is the integral of that function multiplied by a shifted copy of it:\n\nA2,f (s) =\n\n(cid:90) ∞\n\n−∞\n\nf †(x)f (x + s)dx,\n\nwhere s denotes the increment of the shift. Taking its Fourier transform, it becomes:\n\nˆA2,f (s) = ˆfk\n\nˆf † k,\n\n(17)\n\n(18)\n\nwhich is the power spectrum. The three-point autocorrelation, also called the triple correlation, is the integral of a function multiplied by two independently shifted copies of it:\n\nA3,f (s1, s2) =\n\n(cid:90) ∞\n\n−∞\n\nf †(x)f (x + s1)f (x + s2)dx.\n\nTaking its Fourier transform it becomes:\n\nˆA2,f (s1, s2) = ˆfk1\n\nˆfk2\n\nˆf † k1+k2\n\n,\n\nwhich is the bispectrum.\n\n(19)\n\n(20)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX G SYMMETRIES IN THE BISPECTRUM\n\nThe bispectrum has n2 terms (with n = number of frequencies). However, the object is highly redundant and is reducible to n2 12 terms without loss of information for a real-valued signal [23]. This is due to the following symmetries, depicted in Figure 7. For ω1 = 2πk1:\n\nβω1,ω2 = βω2,ω1\n\n= β−ω2,−ω1 = β−ω1−ω2,ω2 = βω1,−ω1−ω2 = β−ω1−ω2,ω1 = βω2,−ω1−ω2.\n\nThus, the subset of bispectral coefficients for which ω2 >= 0, ω1 >= ω2, ω1 + ω2 <= π is sufficient for a complete description of the bispectrum.\n\nFigure 7: Symmetries of the Triple Correlation and Bispectrum. For a real-valued signal, the six-fold symmetry of the triple product becomes a twelve-fold symmetry in which six paris of regions are conjugate symmetric (with the complex conjugate indicated in grey). Adapted from [23].\n\nIt is also known that, in fact, the bispectrum can be further reduced. For example, for the bispectrum on 1D translation, only n + 1 coefficients are needed to reconstruct the input up to translation, and thus define a complete description of the signal structure in the quotient space [33]. The reconstruction algorithm proposed in [33] requires that the signal has no Fourier coefficients equal to zero. More robust reconstruction algorithms, such as least squares approaches, use more elements of the bispectrum matrix [34] for stability purposes.\n\nAPPENDIX H DATASETS\n\nH.1 VAN HATEREN NATURAL IMAGES\n\nWe use natural image patches from the Van Hateren database [27] as the data on which the groups act, and generate two datasets, each using a different group action: Cyclic 2D Translation (S1 × S1) and 2D Rotation SO(2). For each group, we randomly select 100 image patches as the “exemplars” on which the group acts. Each image patch is augmented by each element of the group to generate the orbit of the image under the transformation. The group S1 × S1 is discretized into integer translations, and all 256 possible discrete translations are used to generate the data. The group SO(2) is discretized into 360 integer degree rotations and 30% of these rotations are randomly drawn for each image to generate its orbit. Following orbit generation, image patches are normalized to have zero mean and unit standard deviation. For the 2D rotation dataset, patterns are cropped to a circle with diameter equal to the side length of the image, with values outside of the circle set to zero, to maintain perfect rotational symmetry. Appendix H shows examples from the dataset. A random 20% of each dataset is set aside for model validation and is used to tune hyperparameters. The remaining 80% is used for training.\n\n16\n\nSymmetries of the Triple Correlation and BispectrumTriple Correlations1s2Bispectrumω1ω2Published as a conference paper at ICLR 2023\n\nFigure 8: Examples from the Cyclic 2D Translation and 2D Rotation Datsets. Image patches are randomly sampled from the Van Hateren natural image database [35]. The orbit of each patch under 2D translation (Left) and rotation (Right) are generated.\n\nH.2 ROTATED MNIST\n\nThe Rotated MNIST dataset is generated by rotating each digit in the MNIST [36] training and test datasets by a random angle. This results in a training and test sets with the standard sizes of 60, 000 and 10, 000. A random 10% of the training dataset is set aside for model validation and is used to tune hyperparameters. The remaining 90% is used for training. Images are additionally downsized with interpolation to 16 × 16 pixels.\n\nFigure 9: Examples from the Rotated MNIST Dataset.\n\nAPPENDIX I TRAINING PROCEDURES\n\nAll networks were implemented and trained in PyTorch [37]. We additionally made use of the opensource CplxModule library [38], which provides implementations of the various complex-valued operations and initializations used in this work.\n\nI.1 LEARNING FOURIER TRANSFORMS ON GROUPS\n\nThe weight matrix W is a square complex-valued matrix of dimension 256 × 256, matching the dimension of the (raveled) (16, 16) image patches in the dataset. Weights were initialized using the complex orthogonal weight initialization method proposed in [39]. Each parameter vector in W was normalized to unit length after every gradient step. We used a batch sampler from the Pytorch Metric Learning library [40] to load batches with M random examples per class, with M = 10 and a batch size of 100. Networks were trained with the Adam optimizer [41] until convergence, using an initial learning rate of 0.002 and a cyclic learning rate scheduler [42], with 0.0001 and 0.005 as the lower and upper bounds, respectively, of the cycle.\n\n17\n\n2D Cyclic Translation S1×S12D Rotation SO(2)Van Hateren Image PatchesPublished as a conference paper at ICLR 2023\n\nI.2 GROUP-INVARIANT CLASSIFICATION\n\nThe images in the Rotated MNIST dataset were down-scaled with interpolation to 16 × 16 pixels to match the trained model size. The dataset is passed through the Bispectral Network to yield a rotation-invariant representation of the images. The complex-valued output vector is reduced to 500 dimensions with PCA. The real and imaginary components of the output vectors are then horizontally concatenated before being passed to the classifier network.\n\nThe classifier model consists of a four-layer feedforward network with hidden layer dimension [128, 128, 64, 32], ReLU activation functions between each hidden layer and a softmax activation at the output that generates a distribution over the 10 digit classes. The MLP is trained with the cross-entropy loss on the labeled digits using the Adam optimizer with an initial learning rate of 0.002, which is reduced by a factor of 0.95 when the validation loss plateaus for 30 epochs, until a minimum learning rate of 0.00005 is reached. Weights in the network are initialized using the Kaiming initialization [43] and biases are initialized to zero. The network is trained until convergence at 1500 epochs.The baseline Linear SVM model is trained with the L2-penalty, a squared-hinge loss, and a regularization parameter of 1.0.\n\nI.3 COMPLETENESS\n\nInputs { ̄x1, ..., ̄xN } are optimized to convergence using the Adam optimizer [41]. An initial learning rate of 0.1 is used for all models, which is reduced by 0.1 when the loss plateaus for 10 epochs.\n\nI.4 RECOVERING CAYLEY TABLES\n\nModels were trained to convergence with the Adam optimizer [41] using cyclic triangular learning rate scheduler [42], a base learning rate og 10−5, a mximum learning rate of 0.001, a and a step size up of 5 epochs.\n\nAPPENDIX J LEARNING FOURIER TRANSFORMS ON GROUPS\n\nWe show the model’s learned weights W for each experiment: 2D Cyclic Translation and 2D Rotation. Each tile is the real part of one row of W reshaped into a 16 × 16 image.\n\nFigure 10: Learned Filters. Real components of weights learned in Bispectral Networks trained on the van Hateren Natural Images dataset under (left) Cyclic 2D Translation and (right) 2D Rotation.\n\n18\n\n2D Cyclic Translation S1×S12D Rotation SO(2)Published as a conference paper at ICLR 2023\n\nAPPENDIX K RECOVERING CAYLEY TABLES FROM IRREPS\n\nBelow we list the algorithms used to recover Cayley tables from a group’s irreducible representations.\n\nAlgorithm 1 GETCAYLEYFROMIRREPS Compute the Cayley table from the (unordered) irreducible representations of a commutative group.\n\nprocedure GETCAYLEYFROMIRREPS(W )\n\nfor i = 1, ..., n do\n\nAlgorithm 2 GETCAYLEYFROMGROUP Compute the Cayley table of a group G.\n\nprocedure GETCAYLEYFROMGROUP(G)\n\nfor g1 ∈ G do\n\nfor g2 ∈ G do\n\nC(g1, g2) ← g1 · g2\n\nfor j = 1, ..., n do ρ ← Wi ⊙ Wj k∗ ← argmax\n\n|⟨ρ, Wk⟩|\n\nk\n\nˆC(i, j) ← k∗\n\nend for\n\nend for return ˆC end procedure\n\nend for\n\nend for return C end procedure\n\nAlgorithm 3 ISISOMORPHIC Check whether the learned group and the provided group are isomorphic by checking equality of their Cayley tables under permutation of group elements. Let Sn be the group of all permutations of n elements.\n\nprocedure ISISOMORPHIC(G, W )\n\nfor π ∈ Sn do\n\n▷ Permute the elements of G according to the permutation element π\n\nGπ ← π ◦ G Cπ ← GETCAYLEYFROMGROUP(Gπ) ˆC ← GETCAYLEYFROMIRREPS(W ) if C = ˆC then return True\n\nend if\n\nend for return False\n\nend procedure\n\n19",
    "reference": "# Summary Of The Paper\n\nThe paper presents the Bispectral Neural Network (BNN), a network primitive which learns to be invariant to transformations present in the data. The model is defined in terms of two main components. Firstly, the model treats the irreducible representations of a group of transformations as weights in the network, thus essentially learning a Fourier transform on the group, which is equivariant to the group action. The Fourier representation of the input is then mapped to the bispectrum, which is invariant to the transformation but retains the information needed to uniquely restore the image (up to the transformation), by the network. Secondly, a novel loss function encourages the bispectrum for inputs of the same class to be identical, effectively making the trained network invariant to the transformation. A crucial point here is that the group of transformations need not be defined a priori; the network discovers the transformations automatically.\n\nThe authors test the model in four different experiments, demonstrating that it can construct the transformation group from data alone, that the learnt irreducible representations are transferable to other datasets, and that the model is robust to adversarial attacks.\n\n# Strength And Weaknesses\n\n### Strengths\nOverall, I find this paper very strong. While the bispectrum is extensively used in other fields, it does not seem to be the case in machine learning (as far as my searching abilities go). Simply introducing this incredibly useful concept to the machine learning community has value in itself, and then further parametrising it in terms of a neural network, which, given a novel loss function, learns the group of transformations directly from data, makes this paper very strong. The simplicity and elegance of the proposed method are intriguing, and I could see the ideas introduced here forming the basis of many future papers on invariant and equivariant models.\n\nIn summary, the paper\n- presents an incredibly useful concept (the bispectrum),\n- introduces a novel and interesting loss function, and\n- demonstrates how to learn irreducible representations of the transformation group using a neural network.\n\n### Weaknesses\nThe paper appears to be mostly a proof-of-concept. The datasets used in the experiments are quite simple, which the authors acknowledge themselves. It is also unclear to me if the method is usable in practice in terms of the computations that are required to compute the bispectrum. Since the complex weight matrix needs to be of size $n \\times n$, $n$ being the number of input dimensions, does the method scale to inputs of even moderately large dimensionality? A study of this, e.g., in terms of the training time and memory cost for increasing image sizes, would have been interesting. The authors say that they are currently working on a localised version of the model, such that it can be used in a convolutional neural network, which would address this issue. Still (and without knowing exactly how much work this entails), one might have expected such an extension to be included in the current paper.\n\nI see these weaknesses as minor, though, given the significance of the model and the ideas and concepts the paper introduces.\n\nIn summary, I see the main weaknesses as being that\n- the model is only tested on simple datasets, and that\n- it is unclear if the model scale to inputs of moderately large dimensionality.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe quality of the paper is exceptionally high. As someone relatively new to the field of equivariances in machine learning and entirely new to group representation theory, I found the paper extraordinarily pedagogical. Appendix A is very helpful and section 2 is well-written and easy to understand despite the abstract topic. The method itself appears to be novel (I wasn't able to find anything similar) and the ideas and concepts could have a significant impact on the fields of invariances and equivariances in machine learning. The authors provide the code to reproduce all experiments and figures, though I did not try to run this. In general, the paper should be of significant interest to the ICLR community. \n\n**Questions for the authors**\n\n1. For the loss, did you experiment with other norms? For images, in particular, the $L_2$ norm is not very informative, but I cannot intuitively see if this could cause problems.\n2. As mentioned under \"weaknesses\", it is unclear to me how well the method scales. I don't expect any new experiments, but do you have a feeling for how large inputs the method can handle?\n\n# Summary Of The Review\n\nThe paper presents a novel method for learning representations of data that are invariant to transformations of the data. Not only is the method novel, the ideas and concepts presented in the paper are intriguing and should be of significant interest to the ICLR community. The experiments are somewhat weak, though, and the paper seems to be mostly a proof-of-concept, yet the strengths far outweigh these weaknesses. I view the paper as a very significant and impactful contribution to the fields of invariance and equivariance in machine learning.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nH2RBOX: HORIZONTAL BOX ANNOTATION IS ALL YOU NEED FOR ORIENTED OBJECT DETECTION\n\nXue Yang1, Gefan Zhang1,2, Wentong Li3, Xuehui Wang1, Yue Zhou1, Junchi Yan1,4,∗ 1MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University 2COWAROBOT Co. Ltd. {yangxue-2019-sjtu,lizaozhouke}@sjtu.edu.cn, liwentong@zju.edu.cn {wangxuehui,sjtu zy,yanjunchi}@sjtu.edu.cn Jittor Code: https://github.com/yangxue0827/h2rbox-jittor PyTorch Code:\n\nhttps://github.com/yangxue0827/h2rbox-mmrotate\n\n4Shanghai AI Laboratory\n\n3Zhejiang University\n\nABSTRACT\n\nOriented object detection emerges in many applications from aerial images to autonomous driving, while many existing detection benchmarks are annotated with horizontal bounding box only which is also less costive than fine-grained rotated box, leading to a gap between the readily available training corpus and the rising demand for oriented object detection. This paper proposes a simple yet effective oriented object detection approach called H2RBox merely using horizontal box annotation for weakly-supervised training, which closes the above gap and shows competitive performance even against those trained with rotated boxes. The cores of our method are weakly- and self-supervised learning, which predicts the angle of the object by learning the consistency of two different views. To our best knowledge, H2RBox is the first horizontal box annotation-based oriented object detector. Compared to an alternative i.e. horizontal box-supervised instance segmentation with our post adaption to oriented object detection, our approach is not susceptible to the prediction quality of mask and can perform more robustly in complex scenes containing a large number of dense objects and outliers. Experimental results show that H2RBox has significant performance and speed advantages over horizontal box-supervised instance segmentation methods, as well as lower memory requirements. While compared to rotated box-supervised oriented object detectors, our method shows very close performance and speed. The source code is available at PyTorch-based MMRotate and Jittor-based JDet.\n\n1\n\nINTRODUCTION\n\nIn addition to the relatively matured area of horizontal object detection (Liu et al., 2020), oriented object detection has received extensive attention, especially for complex scenes, whereby fine-grained bounding box (e.g. rotated/quadrilateral bounding box) is needed, e.g. aerial images (Ding et al., 2019; Yang et al., 2019a), scene text (Zhou et al., 2017), retail scenes (Pan et al., 2020) etc.\n\nDespite the increasing popularity of oriented object detection, many existing datasets are annotated with horizontal boxes (HBox) which may not be compatible (at least on the surface) for training an oriented detector. Hence labor-intensive re-annotation1 have been performed on existing horizontalannotated datasets. For example, DIOR-R (Cheng et al., 2022) and SKU110K-R (Pan et al., 2020) are rotated box (RBox) annotations of the aerial image dataset DIOR (192K instances) (Li et al., 2020) and the retail scene SKU110K (1,733K instances) (Goldman et al., 2019), respectively.\n\nOne attractive question arises that if one can achieve weakly supervised learning for oriented object detection by only using (the more readily available) HBox annotations than RBox ones. One poten-\n\n∗Correspondence author is Junchi Yan. The work was in part supported by National Key Research and Development Program of China (2020AAA0107600), National Natural Science Foundation of China (62222607), and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).\n\n1The annotation cost (in price) of the RBox is about 36.5% ($86 vs. $63) higher than that of the HBox\n\naccording to https://cloud.google.com/ai-platform/data-labeling/pricing.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) BoxInst-RBox\n\n(b) BoxLevelSet-RBox\n\n(c) H2RBox\n\nFigure 1: Visual comparison of three HBox-supervised rotated detectors on aircraft detection (Wei et al., 2020), ship detection (Yang et al., 2018), vehicle detection (Azimi et al., 2021), etc. The HBox-Mask-RBox style methods, i.e. BoxInst-RBox (Tian et al., 2021) and BoxLevelSet-RBox (Li et al., 2022b), perform not well in complex and object-cluttered scenes.\n\ntial and verified technique in our experiments is HBox-supervised instance segmentation, concerning with BoxInst (Tian et al., 2021), BoxLevelSet (Li et al., 2022b), etc. Based on the segmentation mask by these methods, one can readily obtain the final RBox by finding its minimum circumscribed rectangle, and we term the above procedure as HBox-Mask-RBox style methods i.e. BoxInst-RBox and BoxLevelSet-RBox in this paper. Yet it in fact involves a potentially more challenging task i.e. instance segmentation whose quality can be sensitive to the background noise, and it can influence heavily on the subsequent RBox detection step, especially given complex scenes (in Fig. 1(a)) and the objects are crowded (in Fig. 1(b)). Also, involving segmentation is often more computational costive and the whole procedure can be time consuming (see Tab. 1-2).\n\nIn this paper, we propose a simple yet effective approach, dubbed as HBox-to-RBox (H2RBox), which achieves close performance to those RBox annotation supervised methods e.g. (Han et al., 2021b; Yang et al., 2023a) by only using HBox annotations, and even outperforms in considerable amount of cases as shown in our experiments. The cores of our method are weakly- and selfsupervised learning, which predicts the angle of the object by learning the enforced consistency between two different views. Specifically, we predict five offsets in the regression sub-network based on FCOS (Tian et al., 2019) in the WS branch (see Fig. 2 left) so that the final decoded outputs are RBoxes. Since we only have horizontal box annotations, we use the horizontal circumscribed rectangle of the predicted RBox when computing the regression loss. Ideally, predicted RBoxes and corresponding ground truth (GT) RBoxes (unlabeled) have highly overlapping horizontal circumscribed rectangles. In the SS branch (see Fig. 2 right), we rotate the input image by a randomly angle and predict the corresponding RBox through a regression sub-network. Then, the consistency of RBoxes between the two branches, including scale consistency and spatial location consistency, are learned to eliminate the undesired cases to ensure the reliability of the WS branch. Our main contributions are as follows:\n\n1) To our best knowledge, we propose the first HBox annotation-based oriented object detector. Specifically, a weakly- and self-supervised angle learning paradigm is devised which closes the gap between HBox training and RBox testing, and it can serve as a plugin for existing detectors.\n\n2) We prove through geometric equations that the predicted RBox is the correct GT RBox under our designed pipeline and consistency loss, and does not rely on not-fully-verified/ad-hoc assumptions, e.g. color-pairwise affinity in BoxInst or additional intermediate results whose quality cannot be ensured, e.g. feature map used by many weakly supervised methods (Wang et al., 2022).\n\n3) Compared with the potential alternatives e.g. HBox-Mask-RBox whose instance segmentation part is fulfilled by the state-of-the-art BoxInst, our H2RBox outperforms by about 14% mAP\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Our H2RBox consists of two branches respectively fed with two augmented views (View 1 and View 2) of the input image. The left Weakly-supervised Branch in general can be any rotated object detector (FCOS here) for RBox prediction, whose circumscribed HBox is used for supervised learning given the GT HBox label in the sense of weakly-supervised learning. This branch is also used for test-stage inference. The right Self-supervised Branch tires to achieve RBox prediction consistency of the two views with self-supervised learning. Image is from the DIOR-R dataset.\n\n(67.90% vs. 53.59%) on DOTA-v1.0 dataset, requiring only one third of its computational resources (6.25 GB vs. 19.93 GB), and being around 12× faster in inference (31.6 fps vs. 2.7 fps).\n\n4) Compared with the fully RBox annotation-supervised rotation detector FCOS, H2RBox is only 0.91% (74.40% vs. 75.31%) and 1.01% (33.15% vs. 34.16%) behind on DOTA-v1.0 and DIOR-R, respectively. Furthermore, we do not add extra computation in the inference stage, thus maintaining a comparable detection speed, about 29.1 FPS vs. 29.5 FPS on DOTA-v1.0.\n\n2 RELATED WORK\n\nRBox-supervised Oriented Object Detection. Oriented object detection in visual images has received increasing attention across different areas e.g. aerial image (Xu et al., 2020; Yang et al., 2022; 2023a; Hou et al., 2023), scene text (Zhou et al., 2017; Liao et al., 2018), retail (Pan et al., 2020; Chen et al., 2020), etc. Earlier methods including RRPN (Ma et al., 2018), ROI-Transformer (Ding et al., 2019) and ReDet (Han et al., 2021b) directly perform angle regression. To address the loss discontinuity and regression inconsistency due to periodicity of angle, subsequent works either convert the parameterization of the rotated bounding box into 2-D Gaussian distributions (Yang et al., 2021c;d) or transform the angle regression to classification (Yang et al., 2021a; Yang & Yan, 2022). (Hou et al., 2022; Li et al., 2022a) introduce the adaptive point set for object representation to mitigate the angle regression sensitivity and meanwhile captures instances’ semantic information.\n\nHBox-supervised Instance Segmentation and Its Potential for Oriented Object Detection. The bold idea of purely using HBox-annotations to train a rotated object detector is attractive yet still rarely studied in literature, which can be seen as a weakly-supervised (WS) learning paradigm for oriented object detection. A related and better-studied technique is HBox-supervised instance segmentation, which tries to segment instance based on the HBox annotations for WS training. For instance, SDI (Khoreva et al., 2017) relies on the region proposals generated by MCG (Pont-Tuset\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(a) O2O by zeros padding.\n\n(b) O2M by reflect padding.\n\n(c) The two re-assignment strategies.\n\nFigure 3: Comparison of different padding methods (Sec. 3.1) and re-assignment strategies (Sec. 3.4). Green and red RBox represent the target rboxws∗ and rboxss, respectively.\n\net al., 2016) and uses an iterative training process to refine the segmentation. BBTP (Hsu et al., 2019) formulates the HBox-supervised instance segmentation into a multiple instance learning problem based on Mask R-CNN (He et al., 2017). BoxInst (Tian et al., 2021) uses the color-pairwise affinity with box constraint under an efficient RoI-free CondInst (Tian et al., 2020). BoxLevelSet (Li et al., 2022b) introduces an energy function to predict the instance-aware mask as the level set.\n\nThough one can obtain the final object orientation by certain means based on the segmentation mask from the above instance segmentation methods, e.g. by finding the minimum circumscribed rectangle, we argue and show in our experiments that such an HBox-Mask-RBox pipeline can be complex (segmentation can be even more difficult than rotation detection – see Fig. 1) and expensive in the presence of dense objects and background noises. Hence we aim to skip the segmentation step and build an HBox-to-RBox paradigm which has not been studied before to our best knowledge.\n\n3 PROPOSED METHOD\n\nThe overview of the H2RBox is shown in Fig. 2. Two augmented views are generated and information leakage is avoided for training overfitting. There are two branches. One branch is used for weakly-supervised (WS) learning where the supervision is the GT HBox from the training data, and the regression loss is calculated between the circumscribed HBox derived from the predicted RBox by this branch and GT HBox. The other branch is trained by self-supervised (SS) learning that involves two augmented views of the raw input image, which encourages to obtain the consistent RBox prediction between the two views. The final loss is the weighted sum of the WS loss and SS loss. Note that the test-stage prediction is concerned only with the WS branch.\n\n3.1 AUGMENTED VIEW GENERATION\n\nIn line with the general idea of self-supervised learning by data augmentation, given the input image, we perform random rotation to generate View 2 while keeping View 1 consistent with the input image, as shown in Fig. 2. However, rotation transformation will geometrically and inevitably introduce an artificial black border area and leads to the risk of GT angle information leakage. We provide two available techniques to resolve this issue:\n\n1) Center Region Cropping: Crop a\n\n√\n\n2\n\n2 s ×\n\n√\n\n2\n\n2 s area2 in the center of the image.\n\n2) Reflection Padding: Fill the black border area by reflection padding.\n\nIf the Center Region Cropping is used in View 2, View 1 also needs to perform the same operation and filter the corresponding ground truth. In contrast, Reflection Padding works better than Center Region Cropping because it preserves as much of the area as possible while maintaining a higher image resolution. Fig. 3(a) and Fig. 3(b) compare zeros padding and reflection padding. Note that the black border area does not participate in the regression loss calculation in the SS branch, so it does not matter that this region is filled with unlabeled foreground objects by reflection padding.\n\n2When the rotation angle is a multiple of 45◦, the black border area reaches its peak, so the side length of\n\nthe largest crop area is\n\n2 of the side length of the original image (s), refer to the View 2 in Fig. 2.\n\n√\n\n2\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a) WS loss only\n\n(b) WS + SS loss\n\nFigure 4: Visual comparison of our methods with and without the SS loss used in the SS branch. It can help learn the scale and spatial location consistency between the two branches.\n\n3.2 THE WEAKLY-SUPERVISED (WS) BRANCH\n\nThe two generated views (View 1 and View 2) are respectively fed into the two branches with the parametershared backbone and neck, specified as ResNet (He et al., 2016) and FPN (Lin et al., 2017a) as shown in Fig. 2. The WS branch here is specified by a FCOS-based rotated object detector, as involved for both training and inference. This branch contains regression and classification sub-networks to predict RBox, category, and center-ness. Recall that we can not use the predicted RBox to calculate the final regression directly as there is no RBox annotation but HBox only. Therefore, we first convert the predicted RBox into the corresponding minimum horizontal circumscribed rectangle, for calculating the regression loss between the derived HBox and the GT Hbox annotation (we defer the details of the loss formulation to Sec. 3.5). As the network is better trained, an indirect connection (horizontal circumscribed rectangle constraint) occurs between predicted RBox and GT RBox (unlabeled): No matter how an object is rotated, their corresponding horizontal circumscribed rectangles are always highly overlapping. However, as shown in Fig. 4(a), only using WS loss can only localize the objects, while still not effective enough for accurate rotation estimation.\n\nFigure 5: Proof of the relationship between predicted RBox and GT RBox under horizontal circumscribed rectangle constraint and scale constraint. Green and orange RBoxes represent correct coincident prediction Bc and undesired symmetric prediction Bs.\n\nw · | cos θ| + h · | sin θ| = wws w · | sin θ| + h · | cos θ| = hws\n\nw · | cos φ| + h · | sin φ| = wss w · | sin φ| + h · | cos φ| = hss\n\n3.3 THE SELF-SUPERVISED (SS) BRANCH\n\nAs complementary to the WS loss, we further introduce the SS loss. The SS branch only contains one regression sub-network for predicting RBox in the rotated View 2. Given a (random) rotation transformation R (with degree ∆θ) as adopted in View 2, the relationship between location (x, y) of View 1 in the WS branch and location (x∗, y∗) of View 2 with rotation R in the SS branch is:\n\n(x∗, y∗) = (x − xc, y − yc)R⊤ + (xc, yc), R =\n\n(cid:18) cos ∆θ − sin ∆θ cos ∆θ\n\nsin ∆θ\n\n(cid:19)\n\n(1)\n\nwhere (xc, yc) is the rotation center (i.e. image center). Recall the label of the black border area (in Fig. 3) in the SS branch is set as invalid and negative samples, which will not participate in the subsequent losses designed below.\n\nSpecifically, a scale loss Lwh accounts for the scale consistency to enhance the indirect connection described above: For augmented objects obtained from the same object through different rotations, a set of RBoxes of the same scale are predicted by the detector, and these predicted RBoxes and corresponding GT RBoxes (unlabeled) shall have highly overlapping horizontal circumscribed rectangles. With such an enhanced indirect connection, including horizontal circumscribed rectangle\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nconstraint and scale constraint, we can limit the prediction results to a limited number of feasible cases, explained as follows:\n\nFig. 5 shows two cases based on the above enhanced indirect connection, and lists four different expressions for the four variables (w, h, θ, φ). Due to the periodicity of the angles, there are only two feasible solutions to the four equations within the angle definition, i.e. the green GT RBox and the orange symmetric RBox. In other words, with such a strengthened indirect connection, the relationship between predicted RBox and GT RBox is coincident Bc(w, h, θ) or symmetrical about the center of the object Bs(w, h, π − θ). It can be seen from Fig. 4(a) that there are still many bad cases with extremely inaccurate angles after using Lwh. Interestingly, if we make a symmetry transformation of these bad cases with their center point, the result becomes much better. When generating views, a geometric prior can be obtained, that is, the spatial transformation relationship between the two views, denoted as R in Eq. 1. Thus, we can get the following four transformation relationships, marked as T ⟨Bws, Bss⟩, between the two branches:\n\nT ⟨Bc\n\nT ⟨Bs\n\nws, Bc ws, Bs\n\nss⟩ = {R}, T ⟨Bc ss⟩ = {R⊤}, T ⟨Bs\n\nws, Bs\n\nss⟩ = {R, S} = {S, R⊤}\n\nws, Bc\n\nss⟩ = {R⊤, S} = {S, R}\n\n(2)\n\nws and Bs\n\nwhere Bc ss represent the coincident bounding box predicted in WS branch and the symmetric bounding box predicted in SS branch, respectively. Here S denotes symmetric transformation. Take T ⟨Bc\n\nss⟩ = {R, S} as an example, it means Bs\n\nss = S(R · Bc\n\nws, Bs\n\nws).\n\nTherefore, an effective way to eliminate the symmetric case is to let the model know that the relationship between the RBoxes predicted by the two branches can only be R. Inspired by above analysis, spatial location loss is used to construct the spatial transformation relationship R of RBoxes predicted by two branches. Specifically, the RBox predicted by WS branch is first transformed by R, and then several losses (e.g. center point loss Lxy and angle loss Lθ) are used to measure its location consistency with the RBox predicted by SS branch. In fact, the spatial location consistency, especially the angle loss, provides a fifth angle constraint equation (φ − θ = ∆θ ̸= 0) so that the system of equations in Fig. 5 have a unique solution (i.e. the predicted RBox is the GT RBox) with non-strict proof, because system of equations are nonlinear. The final SS learning consists of scale-consistent and spatial-location-consistent learning:\n\nSim⟨R · Bws, Bss⟩ = 1\n\n(3)\n\nFig. 4(b) shows the visualization by using the SS loss, with accurate predictions. The appendix shows visualizations of feasible solutions for different combinations of constraints.\n\n3.4 LABEL RE-ASSIGNER\n\nSince the consistency of the prediction results of the two branches needs to be calculated, the labels need to be re-assigned in the SS branch. Specifically, the labels at the location (x∗, y∗) of the SS branch, including center-ness (cn∗), target category (c∗) and target GT HBox (gtboxh∗), are the same as in the location (x, y) of the WS branch. Besides, we also need to assign the rboxws(xws, yws, wws, hws, θws) predicted by the WS branch as the target RBox of the SS branch to calculate the SS loss. We propose two reassignment strategies:\n\n1) One-to-one (O2O) assignment: With cn, c and gtboxh, the rboxws predicted at location (x, y) in the WS branch is used as the target RBox at (x∗, y∗) of the SS branch (see Fig. 3(a)).\n\n2) One-to-many (O2M) assignment: Use the rboxws closest to the center point of the gtboxh as the target RBox at location (x∗, y∗) of SS branch, as shown in Fig. 3(b).\n\n(x,y)\n\n3(c) visualizes the difference between the two re-assignment strategies.\n\nFig. re-assigning, we need to perform an rotation transformation on the rboxws to get rboxws∗(x∗\n\nws) for calculating the SS loss according to Eq. 3:\n\nws, w∗\n\nws, h∗\n\nws, y∗\n\nws, θ∗\n\nAfter the\n\n(x∗\n\nws, y∗\n\nws) = (xws − xc, yws − yc)R⊤ + (xc, yc),\n\n(w∗\n\nws, h∗\n\nws) = (wws, hws),\n\nθ∗ ws = θws + ∆θ (4)\n\nThe the visualized label assignment in Fig. 3 further shows that the SS loss effectively eliminates prediction of the undesired case. The label reassignment of different detectors may require different strategies. The key is to design a suitable matching strategy for the prediction results of the two views, which can allow the network to learn the consistency better.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n3.5 THE OVERALL LOSS BY COMBINING THE WS AND SS LOSSES\n\nSince the WS branch is a rotated object detector based on FCOS, the losses in this part mainly include the regression Lreg, classification Lcls, and center-ness Lcn. We define the WS loss in the WS branch as follows:\n\nLws =\n\nμ1 Npos\n\n(cid:88)\n\n(x,y)\n\nLcls(p(x,y), c(x,y)) +\n\nμ2 Npos\n\n+\n\nμ3 (cid:80) cnpos\n\n(cid:88)\n\n(x,y)\n\n1{c(x,y)>0}cn(x,y)Lreg\n\n(cid:88)\n\n′\n\nLcn(cn\n\n(x,y), cn(x,y))\n\n(x,y) (cid:16)\n\nr2h(rboxws\n\n(x,y)), gtboxh\n\n(x,y)\n\n(cid:17)\n\n(5)\n\nwhere Lcls is the focal loss (Lin et al., 2017b), Lcn is cross-entropy loss, and Lreg is IoU loss (Yu et al., 2016). Npos denotes the number of positive samples. p and c denote the probability distribution of various classes calculated by Sigmoid function and target category. rboxws and gtboxh represent the predicted RBox in the WS branch and horizontal GT box, respectively. cn and cn indicate the predicted and target center-ness. 1{c(x,y)>0} is the indicator function, being 1 if c(x,y) > 0 and 0 otherwise. The r2h(·) function converts the RBox to its corresponding horizontal circumscribed rectangle. We set the hyperparameters μ1 = 1, μ2 = 1 and μ3 = 1 by default.\n\n′\n\nThen, the SS loss between rboxws∗(x∗ predicted by the SS branch is:\n\nws, y∗\n\nws, w∗\n\nws, h∗\n\nws, θ∗\n\nws) and rboxss(xss, yss, wss, hss, θss)\n\nLss =\n\n1 (cid:80) cn∗\n\npos\n\n(cid:88)\n\n(x∗,y∗)\n\n1{c∗\n\n(x∗ ,y∗ )\n\n>0}cn∗\n\n(x∗,y∗)Lreg(rboxws∗\n\n(x∗,y∗), rboxss\n\n(x∗,y∗))\n\nLreg(rboxws∗, rboxss) = γ1Lxy + γ2Lwhθ, Lxy =\n\n(cid:88)\n\nl1(t∗\n\nws, tss)\n\nt∈(x,y)\n\n(6)\n\n(7)\n\nLwhθ = min{Liou(Bws, B1 ws, h∗\n\nws − θss)|} where Bws(−w∗ ss(−hss, −wss, hss, wss). We set γ1 = 0.15 and γ2 = 1 by default. Lwhθ takes into account the loss discontinuity caused by the boundary issues (Yang et al., 2021c), such as periodicity of angle and exchangeability of edges.\n\nws − θss)|, Liou(Bws, B2 ss(−wss, −hss, wss, hss) and B2\n\nss) + | sin(θ∗ ws), B1\n\nss) + | cos(θ∗\n\nws, −h∗\n\nws, w∗\n\nThe overall loss is a weighted sum of the WS loss and the SS loss where we set λ = 0.4 by default.\n\nLtotal = Lws + λLss\n\n(8)\n\n4 EXPERIMENTS\n\n4.1 DATASETS AND IMPLEMENTATION DETAILS\n\nDOTA-v1.0 (Xia et al., 2018) is one of the largest datasets for oriented object detection in aerial images, which contains challenging cases, e.g. large-scale dense scenes and complex background. It contains 15 categories, 2,806 images and 188,282 instances with both RBox and HBox annotations, and the latter are directly derived from the former one. The proportion of the training set, validation set, and testing set is 1/2, 1/6, and 1/3, respectively. For training and testing, we follow a standard protocol by cropping images into 1,024×1,024 patches with a stride of 824. DIOR-R (Cheng et al., 2022) is an aerial image dataset annotated by RBoxes based on its horizontal annotation version DIOR (Li et al., 2020). There are 23,463 images and 190,288 instances with 20 classes.\n\nMethods are implemented both by PyTorch (Paszke et al., 2019)-based framework MMRotate (Zhou et al., 2022) and Jittor (Hu et al., 2020)-based framework JDet. We adopt the FCOS (Tian et al., 2019) with ResNet50 (He et al., 2016) backbone and FPN neck (Lin et al., 2017a) as the baseline method and building block based on which we develop our approach (see Fig. 1). To implement the weakly-supervised HBox-Mask-RBox alternatives for comparison, we use two strong HBox annotation-based instance segmentation methods: BoxInst and BoxLevelSet, followed by finding its minimum compact surrounding rectangle as the detected RBox and we dub them BoxInst-RBox and BoxLevelSet-RBox respectively. All models are trained with AdamW (Loshchilov & Hutter, 2018) on GeForce RTX 3090 GPU, except BoxLevelSet (Li et al., 2022b) which requires NVIDIA V100 with larger memory. The initial learning rate is 10−4 with 2 images per mini-batch. The weight decay is 0.05. Besides, we adopt learning rate warm-up for 500 iterations, and the learning rate is divided by 10 at each decay step. Random flipping is adopted without any additional tricks.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Results of box the default AP50 (%) on the DOTA-v1.0. All models are trained with ResNet50. ‘1x’ and ‘3x’ schedules indicate 12 epochs and 36 epochs for training. ∗ indicates using NV V100 GPU with more memory. MS denotes multi-scale (Zhou et al., 2022) training and testing. See the appendix for performance of specific categories.\n\nMethod\n\nRBox-supervised:\n\nRepPoints (Yang et al., 2019b) RetinaNet (Lin et al., 2017b) RetinaNet (Lin et al., 2017b) CSL (Yang & Yan, 2020) GWD (Yang et al., 2021c) KLD (Yang et al., 2021d) KFIoU (Yang et al., 2023b) SASM (Hou et al., 2022) R3Det (Yang et al., 2021b) S2A-Net (Han et al., 2021a) FCOS (Tian et al., 2019) FCOS (Tian et al., 2019) FCOS (Tian et al., 2019)\n\nHBox-supervised:\n\nBoxInst-RBox (Tian et al., 2021) BoxLevelSet-RBox∗ (Li et al., 2022b) H2RBox (FCOS-based) H2RBox (FCOS-based) H2RBox (FCOS-based) H2RBox (FCOS-based) H2RBox (FCOS-based)\n\nSched. MS\n\nSize Mem. (GB)\n\nFPS AP50\n\n1x 1x 1x 1x 1x 1x 1x 1x 1x 1x 1x 3x 1x\n\n1x 1x 1x 1x 3x 3x 1x\n\n1,024 1,024 ✓ 1,024 1,024 1,024 1,024 1,024 1,024 1,024 1,024 1,024 1,024 ✓ 1,024\n\n960 960 960 1,024 960 1,024 ✓ 1,024\n\n3.44 3.61 4.17 3.93 3.61 3.61 3.61 3.69 3.78 3.37 4.66 4.66 6.23\n\n19.93 26.81 6.25 7.02 6.25 7.02 8.58\n\n24.5 25.4 –\n24.6 25.4 25.4 25.4 24.4 20.0 23.3 29.5 29.5 –\n\n2.7 4.7 31.6 29.1 31.6 29.1 –\n\n64.18 67.83 73.30 68.26 69.25 69.64 70.05 70.35 71.17 74.13 70.78 72.22 75.31\n\n53.59 56.44 67.90 67.82 70.73 70.41 74.40\n\nTable 2: Results of box AP (%) on the DIOR-R test. All models are trained with ResNet50. The input image size is 800×800. ‘1x’ and ‘3x’ schedules indicate 12 epochs and 36 epochs. ∗ indicates using NV V100 GPU with more memory.\n\nMethod\n\nRBox-supervised:\n\nRetinaNet (Lin et al., 2017b) KLD (Yang et al., 2021d) GWD (Yang et al., 2021c) FCOS (Tian et al., 2019)\n\nHBox-supervised:\n\nBoxLevelSet-RBox∗ (Li et al., 2022b) BoxInst-RBox (Tian et al., 2021) H2RBox (FCOS-based)\n\nSched. Mem. (GB) FPS\n\nAP\n\nAP50 AP75\n\n1x 1x 1x 1x\n\n1x 1x 1x\n\n2.48 2.48 2.48 3.06\n\n11.44 9.23 4.52\n\n33.3 33.3 33.3 40.8\n\n4.7 3.1 34.9\n\n33.47 35.77 37.01 34.16\n\n29.96 31.73 33.15\n\n54.60 58.00 57.80 58.60\n\n56.56 57.40 57.00\n\n33.80 37.00 38.20 31.90\n\n24.36 28.10 32.60\n\n4.2 MAIN RESULTS\n\nResults on DOTA-v1.0. As shown in Tab. 1, our method significantly outperforms BoxInst-RBox and BoxLevelSet-RBox by 14.31% and 11.46% in terms of AP50, respectively. Moreover, our methods are also more memory and inference efficient. Specifically, compared to BoxInst, we only need less than one-third of its memory (6.25 GB vs. 19.93 GB) and have a about 12× speed advantage (31.6 fps vs. 2.7 fps). In contrast to BoxLevelSet, our memory costs only a quarter of its memory (6.25 GB vs. 26.81 GB), and inference is about 7 times faster (31.6 fps vs. 4.7 fps). In fact, the main cost of the -RBox methods come from the costive post-processing step for find the compact surrounding box as RBox which is fulfilled by calling an OpenCV function in our implementation. Even compared with RBox-supervised methods, our method has outperformed several methods, such as RepPoints and RetinaNet. Under the ‘1x’ and ‘3x’ training schedules, our method slightly lags behind the baseline method, i.e. FCOS (recall it is RBox-supervised), by 2.96% and 1.81%. After using multi-scale training and testing, the gap is reduced to only 0.91% (75.31% vs. 74.40%).\n\nResults on DIOR-R. Note that some categories in this dataset including Chimney, Wind mill, Airport, Golf field, are all forcefully annotated by horizontal boxes though the objects are not exactly\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Ablation for H2RBox with different border effect dismissing strategies for view generation by padding/cropping on DOTA-v1.0.\n\nTable 4: Ablation with different label reassignment strategies. O2M and O2O represent one-to-many and one-to-one.\n\nPadding Zeros Zeros Reflection Reflection\n\nCropping\n\n✓\n\n✓\n\nAP 20.17 33.72 35.92 33.60\n\nAP50 AP75 12.91 51.76 30.00 63.95 32.78 67.31 30.02 64.09\n\nDataset Assigner\n\nDOTA\n\nDIOR-R\n\nO2M O2O O2M O2O\n\nAP 21.60 35.92 31.10 33.15\n\nAP50 AP75 14.14 53.96 32.78 67.31 29.80 56.00 32.60 57.00\n\nTable 5: Ablation with two strategies S1, S2 dealing with circular category: ST & RA on DOTA-v1.0.\n\nTable 6: Ablation with using SS loss (Lss) or not on DOTA-v1.0 and DIOR-R.\n\nS1\n\n✓\n\nS2\n\nST 69.82 85.29 ✓ 84.58 ✓ ✓ 85.41\n\nRA 38.87 64.04 65.98 63.38\n\nAP 31.90 36.36 35.92 36.41\n\nAP50 AP75 27.11 64.52 33.26 67.25 67.31 32.78 33.40 67.22\n\nDataset Lss\n\nDOTA\n\nDIOR-R\n\nAP 12.63 ✓ 35.92 15.27 ✓ 33.15\n\nAP50 AP75 7.54 37.13 32.78 67.31 13.60 29.60 32.60 57.00\n\nhorizontal, which may affect the learning and the final results. As shown in Tab. 2, compared with DOTA-v1.0, DIOR-R is less challenging for the instance segmentation methods. This may explain the observation that the performance of H2RBox and BoxInst-RBox on AP50 is close. Yet for high-precision detection i.e. with high AP75 that requires more accurate segmentation, H2RBox outperforms BoxLevelSet-RBox and BoxInst-RBox on AP75 by 8.24% (32.6% vs. 24.36%) and 4.50% (32.6% vs. 28.10%), and with lower memory and high inference speed. Similarly, H2RBox performs slightly inferior than the RBox-supervised FCOS: 33.15% vs. 34.16%.\n\n4.3 ABLATION STUDIES\n\nThe ablation study is performed on the proposed H2RBox with 12 training epochs.\n\nBorder effect elimination for view generation. Tab. 3 studies the impact of different border effect elimination strategies for view generation, in terms of padding and/or cropping (see Sec. 3.1). Such techniques are essential to avoid ground truth angle information leakage, otherwise the model will suffer overfitting and leads to significant performance drop as verified in the first row of the table. Note that when both reflection padding and cropping are applied the AP slightly drops from 35.92% to 33.60% compared with only using reflection padding. The reason may be due to that reduced size of input image by cropping. Hence in all other experiments we always use reflection padding alone.\n\nLabel re-assignment. Tab. 4 shows the one-to-one strategy outperforms one-to-many strategy.\n\nStrategies for dealing with sotropic circular object classes. For circular objects like Storage Tank (ST) and Roundabout (RA), the self-supervised loss takes no effect as it is insensitive to isotropic information. We take two treatments to handle such circular objects. S1: for training, we mask the SS loss for circular classes. S2: for testing, the horizontal circumscribed rectangle of the circular category is taken as the final output. Tab. 5 shows that, when either or both strategies is used, the performance can be greatly improved, about 15% on ST and about 25% on RA.\n\nSelf-supervised loss. Without using SS loss, Tab. 6 shows that our method only achieves 12.63% and 15.27% on DOTA-v1.0 and DIOR-R, respectively. In contrast, the use of SS loss leads to a substantial increase in overall performance, reaching 35.92% and 33.15%. Figure 4(b) also shows that the SS loss can effectively help the model learn the correct object angle information.\n\n5 CONCLUSION\n\nThis paper presents H2RBox, the first (to the best of our knowledge) HBox-supervised oriented object detector. H2RBox learns the rotation via self-supervised learning, whose loss measures the consistency of the predicted angles in two different views. Compared to the alternative HBox-supervised instance segmentation methods, H2RBox achieves much higher detection accuracy especially for complex scenes, yet with lower memory and higher speed. Compared with fully RBox-supervised algorithms, our method still shows competitive.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nSeyed Majid Azimi, Reza Bahmanyar, Corentin Henry, and Franz Kurz. Eagle: Large-scale vehicle detection dataset in real-world scenarios using aerial imagery. In 25th International Conference on Pattern Recognition, pp. 6920–6927. IEEE, 2021.\n\nZhiming Chen, Kean Chen, Weiyao Lin, John See, Hui Yu, Yan Ke, and Cong Yang. Piou loss: Towards accurate oriented object detection in complex environments. In European Conference on Computer Vision, pp. 195–211, 2020.\n\nGong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo Lang, Yanqing Yao, and Junwei Han. Anchor-free oriented proposal generator for object detection. IEEE Transactions on Geoscience and Remote Sensing, 2022.\n\nJian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu. Learning roi transformer for oriented object detection in aerial images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2849–2858, 2019.\n\nEran Goldman, Roei Herzig, Aviv Eisenschtat, Jacob Goldberger, and Tal Hassner. Precise detection in densely packed scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5227–5236, 2019.\n\nJiaming Han, Jian Ding, Jie Li, and Gui-Song Xia. Align deep features for oriented object detection.\n\nIEEE Transactions on Geoscience and Remote Sensing, 60:1–11, 2021a.\n\nJiaming Han, Jian Ding, Nan Xue, and Gui-Song Xia. Redet: A rotation-equivariant detector for aerial object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2786–2795, 2021b.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask r-cnn. In Proceedings of the\n\nIEEE International Conference on Computer Vision, pp. 2961–2969, 2017.\n\nLiping Hou, Ke Lu, Jian Xue, and Yuqiu Li. Shape-adaptive selection and measurement for oriented\n\nobject detection. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.\n\nLiping Hou, Ke Lu, Xue Yang, Yuqiu Li, and Jian Xue. G-rep: Gaussian representation for arbitrary-\n\noriented object detection. Remote Sensing, 15(3):757, 2023.\n\nCheng-Chun Hsu, Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, and Yung-Yu Chuang. Weakly supervised instance segmentation using the bounding box tightness prior. Advances in Neural Information Processing Systems, 32, 2019.\n\nShi-Min Hu, Dun Liang, Guo-Ye Yang, Guo-Wei Yang, and Wen-Yang Zhou. Jittor: a novel deep learning framework with meta-operators and unified graph execution. Science China Information Sciences, 63:1–21, 2020.\n\nAnna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does it: Weakly supervised instance and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 876–885, 2017.\n\nKe Li, Gang Wan, Gong Cheng, Liqiu Meng, and Junwei Han. Object detection in optical remote sensing images: A survey and a new benchmark. ISPRS Journal of Photogrammetry and Remote Sensing, 159:296–307, 2020.\n\nWentong Li, Yijie Chen, Kaixuan Hu, and Jianke Zhu. Oriented reppoints for aerial object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1829– 1838, 2022a.\n\nWentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Xiansheng Hua, and Lei Zhang. Boxsupervised instance segmentation with level set evolution. In European Conference on Computer Vision, 2022b.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMinghui Liao, Baoguang Shi, and Xiang Bai. Textboxes++: A single-shot oriented scene text\n\ndetector. IEEE Transactions on Image Processing, 27(8):3676–3690, 2018.\n\nTsung-Yi Lin, Piotr Doll ́ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117–2125, 2017a.\n\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ́ar. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2980–2988, 2017b.\n\nLi Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietik ̈ainen. Deep learning for generic object detection: A survey. International journal of computer vision, 128(2):261–318, 2020.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\n\nence on Learning Representations, 2018.\n\nJianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Yingbin Zheng, and Xiangyang Xue. Arbitrary-oriented scene text detection via rotation proposals. IEEE Transactions on Multimedia, 20(11):3111–3122, 2018.\n\nXingjia Pan, Yuqiang Ren, Kekai Sheng, Weiming Dong, Haolei Yuan, Xiaowei Guo, Chongyang Ma, and Changsheng Xu. Dynamic refinement network for oriented and densely packed object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 11207–11216, 2020.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. In Advances in neural information processing systems, 2019.\n\nJordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Ferran Marques, and Jitendra Malik. MulIEEE\n\ntiscale combinatorial grouping for image segmentation and object proposal generation. transactions on pattern analysis and machine intelligence, 39(1):128–140, 2016.\n\nZhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE International Conference on Computer Vision, pp. 9627– 9636, 2019.\n\nZhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n\nEuropean conference on computer vision, pp. 282–298. Springer, 2020.\n\nZhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen. Boxinst: High-performance instance segmentation with box annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5443–5452, 2021.\n\nXuehui Wang, Kai Zhao, Ruixin Zhang, Shouhong Ding, Yan Wang, and Wei Shen. Contrastmask: Contrastive learning to segment every thing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 11604–11613, 2022.\n\nHaoran Wei, Yue Zhang, Bing Wang, Yang Yang, Hao Li, and Hongqi Wang. X-linenet: Detecting aircraft in remote sensing images by a pair of intersecting line segments. IEEE Transactions on Geoscience and Remote Sensing, 59(2):1645–1659, 2020.\n\nGui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3974– 3983, 2018.\n\nYongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Gui-Song Xia, and Xiang Bai. Gliding vertex on the horizontal bounding box for multi-oriented object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(4):1452–1459, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nXue Yang and Junchi Yan. Arbitrary-oriented object detection with circular smooth label. In Euro-\n\npean Conference on Computer Vision, pp. 677–694, 2020.\n\nXue Yang and Junchi Yan. On the arbitrary-oriented object detection: Classification based ap-\n\nproaches revisited. International Journal of Computer Vision, 130(5):1340–1365, 2022.\n\nXue Yang, Hao Sun, Kun Fu, Jirui Yang, Xian Sun, Menglong Yan, and Zhi Guo. Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks. Remote Sensing, 10(1):132, 2018.\n\nXue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo, Xian Sun, and Kun Fu. Scrdet: Towards more robust detection for small, cluttered and rotated objects. In Proceedings of the IEEE International Conference on Computer Vision, pp. 8232–8241, 2019a.\n\nXue Yang, Liping Hou, Yue Zhou, Wentao Wang, and Junchi Yan. Dense label encoding for boundary discontinuity free rotation detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 15819–15829, 2021a.\n\nXue Yang, Junchi Yan, Ziming Feng, and Tao He. R3det: Refined single-stage detector with feature refinement for rotating object. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3163–3171, 2021b.\n\nXue Yang, Junchi Yan, Qi Ming, Wentao Wang, Xiaopeng Zhang, and Qi Tian. Rethinking rotated object detection with gaussian wasserstein distance loss. In International Conference on Machine Learning, pp. 11830–11841. PMLR, 2021c.\n\nXue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian, and Junchi Yan. Learning high-precision bounding box for rotated object detection via kullback-leibler divergence. Advances in Neural Information Processing Systems, 34:18381–18394, 2021d.\n\nXue Yang, Gefan Zhang, Xiaojiang Yang, Yue Zhou, Wentao Wang, Jin Tang, Tao He, and Junchi Yan. Detecting rotated objects as gaussian distributions and its 3-d generalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nXue Yang, Junchi Yan, Wenlong Liao, Xiaokang Yang, Jin Tang, and Tao He. Scrdet++: Detecting small, cluttered and rotated objects via instance-level feature denoising and rotation loss smoothing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2):2384–2399, 2023a.\n\nXue Yang, Yue Zhou, Gefan Zhang, Jirui Yang, Wentao Wang, Junchi Yan, Xiaopeng Zhang, and Qi Tian. The kfiou loss for rotated object detection. In International Conference on Learning Representations, 2023b.\n\nZe Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen Lin. Reppoints: Point set representation for object detection. In Proceedings of the IEEE International Conference on Computer Vision, pp. 9657–9666, 2019b.\n\nJiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and Thomas Huang. Unitbox: An advanced object detection network. In Proceedings of the 24th ACM International Conference on Multimedia, pp. 516–520, 2016.\n\nShifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9759–9768, 2020.\n\nXinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and Jiajun Liang. East: an efficient and accurate scene text detector. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5551–5560, 2017.\n\nYue Zhou, Xue Yang, Gefan Zhang, Jiabao Wang, Yanyi Liu, Liping Hou, Xue Jiang, Xingzhao Liu, Junchi Yan, Chengqi Lyu, Wenwei Zhang, and Kai Chen. Mmrotate: A rotated object detection benchmark using pytorch. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 7331–7334, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA FEASIBLE SOLUTIONS UNDER DIFFERENT CONSTRAINTS\n\nThree different constraints, including horizontal circumscribed rectangle constraint (HCRC), scale constraint (SC) and angle constraint (AC), are introduced in this paper to guide the model to learn the correct result. Fig. 6(a) shows when there are only horizontal circumscribed rectangle constraint, the feasible solutions are still infinite. After adding scale constraint, only the symmetric case and the correct case are left, as shown in Fig. 6(b). The final angle constraint allows the correct solution to be preserved, refer to Fig. 6(c).\n\n(a) HCRC only\n\n(b) HCRC + SC\n\n(c) HCRC + SC + AC\n\nFigure 6: Visualization of feasible solutions under different constraints.\n\nTable 7: Results of box the default AP50 (%) on the DOTA-v1.0. All models are trained with ResNet50. ‘1x’ and ‘3x’ schedules indicate 12 epochs and 36 epochs for training. ∗ indicates using NV V100 GPU with more memory. MS denotes multi-scale (Zhou et al., 2022) training and testing.\n\nMethod\n\nRBox-supervised:\n\nRepPoints (Yang et al., 2019b) RetinaNet (Lin et al., 2017b) RetinaNet (Lin et al., 2017b) CSL (Yang & Yan, 2020) GWD (Yang et al., 2021c) KLD (Yang et al., 2021d) KFIoU (Yang et al., 2023b) SASM (Hou et al., 2022) R3Det (Yang et al., 2021b) ATSS (Zhang et al., 2020) S2A-Net (Han et al., 2021a) FCOS (Tian et al., 2019) FCOS (Tian et al., 2019) FCOS (Tian et al., 2019)\n\nHBox-supervised:\n\nBoxInst-RBox (Tian et al., 2021) BoxLevelSet-RBox∗ (Li et al., 2022b) H2RBox (ATSS-based) H2RBox (FCOS-based) H2RBox (FCOS-based) H2RBox (FCOS-based) H2RBox (FCOS-based) H2RBox (FCOS-based)\n\nSched. MS\n\nSize Mem. (GB)\n\nFPS AP50\n\n1,024 1,024 ✓ 1,024 1,024 1,024 1,024 1,024 1,024 1,024 1,024 1,024 1,024 1,024 ✓ 1,024\n\n960 960 1,024 960 1,024 960 1,024 ✓ 1,024\n\n3.44 3.61 4.17 3.93 3.61 3.61 3.61 3.69 3.78 3.32 3.37 4.66 4.66 6.23\n\n19.93 26.81 5.50 6.25 7.02 6.25 7.02 8.58\n\n24.5 25.4 –\n24.6 25.4 25.4 25.4 24.4 20.0 26.5 23.3 29.5 29.5 –\n\n2.7 4.7 25.7 31.6 29.1 31.6 29.1 –\n\n64.18 67.83 73.30 68.26 69.25 69.64 70.05 70.35 71.17 71.98 74.13 70.78 72.22 75.31\n\n53.59 56.44 67.24 67.90 67.82 70.73 70.41 74.40\n\n1x 1x 1x 1x 1x 1x 1x 1x 1x 1x 1x 1x 3x 1x\n\n1x 1x 1x 1x 1x 3x 3x 1x\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Results of each category on the DOTA-v1.0 test set.\n\nMethod\n\nPL\n\nBD\n\nBR\n\nGTF\n\nSV\n\nLV\n\nSH\n\nTC\n\nBC\n\nST\n\nSBF\n\nRA\n\nHA\n\nSP\n\nHC mAP50\n\nRBox-supervised: RepPoints-1x(2019b) RetinaNet-1x (2017b) RetinaNet-1x-ms (2017b) CSL-1x (2020) GWD-1x (2021c) KLD-1x (2021d) KFIoU-1x (2023b) SASM-1x (2022) R3Det-1x (2021b) ATSS-1x (2020) S2A-Net (2021a) FCOS-1x (2019) FCOS-3x (2019) FCOS-1x-ms (2019) HBox-supervised: BoxInst-RBox-1x (2021) BoxLevelSet-RBox∗-1x (2022b) H2RBox-ATSS-1x H2RBox-FCOS-1x H2RBox-FCOS-3x H2RBox-FCOS-1x-ms\n\n84.79 89.11 88.10 89.03 88.68 88.27 88.83 87.44 88.96 88.34 89.07 88.41 88.41 88.72\n\n68.43 63.48 87.82 88.47 88.24 88.93\n\n73.35 74.52 84.43 78.25 78.59 76.22 77.51 71.31 76.99 76.87 82.76 75.61 76.77 78.77\n\n40.75 71.27 74.73 73.51 79.30 78.89\n\n40.68 44.69 50.54 40.04 45.41 46.22 47.79 48.46 47.09 50.92 51.94 47.98 49.00 51.73\n\n33.07 39.34 43.22 40.81 42.76 46.27\n\n56.51 72.18 79.12 68.52 71.46 72.73 74.28 68.07 70.89 71.29 72.17 60.10 59.16 71.27\n\n32.29 61.06 69.57 56.89 55.79 68.79\n\n71.56 71.80 73.65 77.20 72.27 72.11 71.27 73.93 77.54 76.39 78.85 79.78 79.23 81.03\n\n46.91 41.89 72.67 77.48 78.90 81.12\n\n52.21 63.59 59.80 67.14 68.26 67.84 62.72 74.24 76.19 76.21 79.56 77.81 79.04 83.70\n\n55.43 41.03 53.95 65.42 72.70 75.45\n\n73.40 74.94 72.94 78.25 77.05 77.63 74.75 83.55 86.24 83.47 87.37 86.64 86.86 87.99\n\n56.55 45.83 70.91 77.87 77.54 86.68\n\n90.64 90.78 90.39 90.87 90.80 90.77 90.72 90.91 90.91 90.64 90.90 90.08 90.06 90.28\n\n79.49 90.87 90.39 90.88 90.85 90.89\n\n76.25 78.71 86.45 82.77 80.56 80.67 82.34 80.36 79.45 81.38 85.97 78.23 75.83 83.70\n\n66.81 74.12 85.58 83.19 81.96 86.71\n\n85.15 80.56 87.24 81.30 81.93 83.03 81.61 84.59 83.60 83.59 84.92 84.95 83.75 86.75\n\n82.14 72.13 83.44 85.27 84.38 87.33\n\n58.77 50.48 65.02 52.17 46.48 52.74 58.44 57.98 52.98 58.86 59.67 52.80 58.59 65.18\n\n41.24 47.59 54.77 55.27 55.28 64.15\n\n61.43 59.17 65.55 60.33 60.14 62.23 64.23 62.84 62.50 60.38 63.37 66.25 59.54 65.77\n\n52.83 62.99 63.77 62.90 64.49 68.83\n\n54.91 62.86 67.09 56.14 63.87 64.91 64.39 66.51 64.65 65.23 67.24 64.45 69.25 74.90\n\n52.80 50.00 47.15 52.41 61.91 62.81\n\n64.43 64.35 70.72 65.71 67.39 65.95 67.87 63.82 67.32 67.96 68.59 68.28 72.44 78.18\n\n65.04 56.42 66.28 63.63 70.63 69.39\n\n18.57 39.69 58.44 36.10 46.06 43.22 44.07 41.17 42.29 48.19 49.57 40.31 53.54 41.68\n\n29.99 28.63 44.29 43.26 51.51 59.79\n\n64.18 67.83 73.30 68.26 69.25 69.64 70.05 70.35 71.17 71.98 74.13 70.78 72.22 75.31\n\n53.59 56.44 67.24 67.82 70.41 74.40\n\nB VERIFICATION EXPERIMENTS ON DIFFERENT DETECTORS\n\nAs shown in Tab. 7, we have conducted experiments on different basic detectors, including anchor based method (ATSS (Zhang et al., 2020)) and anchor free method (FCOS (Tian et al., 2019)). It can be seen that the method proposed in this paper has excellent portability. Tab. 8 lists the performance of each class of each method in Tab. 7.\n\n14",
    "reference": "# Summary Of The Paper\n\nThis paper presents HBox-supervised oriented object detector.\nH2RBox learns the rotation via self-supervised learning, whose loss measures the consistency of the predicted angles in two different views.\n\n# Strength And Weaknesses\n\nCompared to the alternative HBox-supervised instance segmentation methods, H2RBox achieves much higher detection accuracy especially for complex scenes, yet with lower memory and higher speed.\nCompared with fully RBox-supervised algorithms, our method still shows competitive, and sometimes even better performance.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n--- \nIn equ. 7, what are t^{*}_{ws} and t_{ss} denoted?\n\n---\n It is still unclear why the combination of wsl and ssl can figure out the oriented boxes.\nThis is my main concern. \nThe wsl branch may predict a lot of boxes with the same horizontal circumscribed rectangle, and the ssl branch tries to learn the same boxes as wsl with random rotation transformation.\n\n# Summary Of The Review\n\nThis paper has significant contribution to the community, and the results are quite good (very close performance and speed compared to rotated box-supervised methods).\n It may reveal that we can use HBox annotations than RBox one to learn oriented object detectors.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDIFFUSION-BASED IMAGE TRANSLATION USING DISENTANGLED STYLE AND CONTENT REPRESENTATION\n\nGihyun Kwon1, Jong Chul Ye2,1 Department of Bio and Brain Engineering1, Kim Jaechul Graduate School of AI2, KAIST cyclomon,jong.ye@kaist.ac.kr\n\nFigure 1: Image translation results by DiffuseIT. Our model can generate high-quality translation outputs using both text and image conditions. More results can be found in the experiment section.\n\nABSTRACT\n\nDiffusion-based image translation guided by semantic texts or a single target image has enabled flexible style transfer which is not limited to the specific domains. Unfortunately, due to the stochastic nature of diffusion models, it is often difficult to maintain the original content of the image during the reverse diffusion. To address this, here we present a novel diffusion-based unsupervised image translation method, dubbed as DiffuseIT, using disentangled style and content representation. Specifically, inspired by the slicing Vision Transformer (Tumanyan et al., 2022), we extract intermediate keys of multihead self attention layer from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer. To further accelerate the semantic change during the reverse diffusion, we also propose a novel semantic divergence loss and resampling strategy. Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks.\n\n1\n\nINTRODUCTION\n\nImage translation is a task in which the model receives an input image and converts it into a target domain. Early image translation approaches (Zhu et al., 2017; Park et al., 2020; Isola et al., 2017) were mainly designed for single domain translation, but soon extended to multi-domain translation (Choi et al., 2018; Lee et al., 2019). As these methods demand large training set for each domain, image translation approaches using only a single image pairs have been studied, which include the one-to-one image translation using multiscale training (Lin et al., 2020), or patch matching strategy (Granot et al., 2022; Kolkin et al., 2019). Most recently, Splicing ViT (Tumanyan et al., 2022) exploits a pre-trained DINO ViT (Caron et al., 2021) to convert the semantic appearance of a given image into a target domain while maintaining the structure of input image.\n\nOn the other hand, by employing the recent text-to-image embedding model such as CLIP (Radford et al., 2021), several approaches have attempted to generate images conditioned on text prompts (Patashnik et al., 2021; Gal et al., 2021; Crowson et al., 2022; Couairon et al., 2022). As these methods rely on Generative Adversarial Networks (GAN) as a backbone generative model, the semantic changes are not often properly controlled when applied to an out-of-data (OOD) image generation.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nRecently, score-based generative models (Ho et al., 2020; Song et al., 2020b; Nichol & Dhariwal, 2021) have demonstrated state-of-the-art performance in text-conditioned image generation (Ramesh et al., 2022; Saharia et al., 2022b; Crowson, 2022; Avrahami et al., 2022). However, when it comes to the image translation scenario in which multiple conditions (e.g. input image, text condition) are given to the score based model, disentangling and separately controlling the components still remains as an open problem.\n\nIn fact, one of the most important open questions in image translation by diffusion models is to transform only the semantic information (or style) while maintaining the structure information (or content) of the input image. Although this could not be an issue with the conditional diffusion models trained with matched input and target domain images (Saharia et al., 2022a), such training is impractical in many image translation tasks (e.g. summer-to-winter, horse-to-zebra translation). On the other hand, existing methods using unconditional diffusion models often fail to preserve content information due to the entanglement problems in which semantic and content change at the same time (Avrahami et al., 2022; Crowson, 2022). DiffusionCLIP (Kim et al., 2022) tried to address this problem using denoising diffusion implicit models (DDIM) (Song et al., 2020a) and pixel-wise loss, but the score function needs to be fine-tuned for a novel target domain, which is computationally expensive.\n\nIn order to control the diffusion process in such a way that it produces the output that simultaneously retain the content of the input image and follow the semantics of the target text or image, here we introduce a loss function using a pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2020). Specifically, inspired by the recent idea (Tumanyan et al., 2022), we extract intermediate keys of multihead self attention layer and [CLS] classification tokens of the last layer from the DINO ViT model and used them as our content and style regularization, respectively. More specifically, to preserve the structural information, we use the similarity and contrastive loss between intermediate keys of the input and denoised image during the sampling. Then, an image guided style transfer is performed by matching the [CLS] token between the denoised sample and the target domain, whereas additional CLIP loss is used for the text-driven style transfer. To further improve the sampling speed, we propose a novel semantic divergence loss and resampling strategy.\n\nExtensive experimental results including Fig. 1 confirmed that our method provide state-of-the-art performance in both text- and image- guided style transfer tasks quantitatively and qualitatively. To our best knowledge, this is the first unconditional diffusion model-based image translation method that allows both text- and image- guided style transfer without altering input image content.\n\n2 RELATED WORK\n\nText-guided image synthesis. Thanks to the outstanding performance of text-to-image alignment in the feature space, CLIP has been widely used in various text-related computer vision tasks including object generation (Liu et al., 2021; Wang et al., 2022a), style transfer (Kwon & Ye, 2021; Fu et al., 2021), object segmentation (L ̈uddecke & Ecker, 2022; Wang et al., 2022b), etc. Several recent approaches also demonstrated state-of-the-art performance in text-guided image manipulation task by combining the CLIP with image generation models. Previous approaches leverage pre-trained StyleGAN (Karras et al., 2020) for image manipulation with a text condition (Patashnik et al., 2021; Gal et al., 2021; Wei et al., 2022). However, StyleGAN-based methods cannot be used in arbitrary natural images since it is restricted to the pre-trained data domain. Pre-trained VQGAN (Esser et al., 2021) was proposed for better generalization capability in the image manipulation, but it often suffers from poor image quality due to limited power of the backbone model.\n\nWith the advance of score-based generative models such as Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020), several methods (Ramesh et al., 2022; Saharia et al., 2022b) tried to generate photo-realistic image samples with given text conditions. However, these approaches are not adequate for image translation framework as the text condition and input image are not usually disentangled. Although DiffusionCLIP (Kim et al., 2022) partially solves the problem using DDIM sampling and pixelwise regularization during the reverse diffusion, it has major disadvantage in that it requires fine-tuning process of score models. As a concurrent work, DDIB(Su et al., 2022) proposed diffusion model based image translation using deterministic probability flow ODE formulation.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nSingle-shot Image Translation. In image translation using single target image, early models mainly focused on image style transfer (Gatys et al., 2016; Huang & Belongie, 2017; Park & Lee, 2019; Yoo et al., 2019). Afterwards, methods using StyleGAN adaptation (Ojha et al., 2021; Zhu et al., 2021; Kwon & Ye, 2022; Chong & Forsyth, 2021) showed great performance, but there are limitations In order to overcome this, methods for as the models are domain-specific (e.g. human faces). converting unseen image into a semantic of target (Lin et al., 2020; Kolkin et al., 2019; Granot et al., 2022) have been proposed, but these methods often suffer from degraded image quality. Recently, Splicing ViT (Tumanyan et al., 2022) successfully exploited pre-trained DINO ViT(Caron et al., 2021) to convert the semantic appearance of given image into target domain while preserving the structure of input.\n\n3 PROPOSED METHOD\n\n3.1 DDPM SAMPLING WITH MANIFOLD CONSTRAINT\n\nIn DDPMs (Ho et al., 2020), starting from a clean image x0 ∼ q(x0), a forward diffusion process q(xt|xt−1) is described as a Markov chain that gradually adds Gaussian noise at every time steps t:\n\nq(xT |x0) :=\n\nT (cid:89)\n\nt=1\n\nq(xt|xt−1), where\n\nq(xt|xt−1) := N (xt; (cid:112)1 − βtxt−1, βtI),\n\n(1)\n\nwhere {β}T diffused sample at t, i.e. xt, can be sampled in one step as:\n\nt=0 is a variance schedule. By denoting αt := 1 − βt and ̄αt := (cid:81)t\n\ns=1 αs, the forward\n\n√\n\n√\n\nxt =\n\n ̄αtx0 +\n\n1 − ̄αtε, where\n\nε ∼ N (0, I).\n\n(2)\n\nAs the reverse of the forward step q(xt−1|xt) is intractable, DDPM learns to maximize the variational lowerbound through a parameterized Gaussian transitions pθ(xt−1|xt) with the parameter θ. Accordingly, the reverse process is approximated as Markov chain with learned mean and fixed variance, starting from p(xT ) = N (xT ; 0, I):\n\npθ(x0:T ) := pθ(xT )\n\nT (cid:89)\n\nt=1\n\nwhere\n\npθ(xt−1|xt), where pθ(xt−1|xt) := N (xt−1; μθ(xt, t), σ2\n\nt I).\n\n(3)\n\nμθ(xt, t) :=\n\n(cid:16)\n\n1 √\nαt\n\nxt −\n\n1 − αt √\n1 − ̄αt\n\nεθ(xt, t)\n\n(cid:17) ,\n\nHere, εθ(xt, t) is the diffusion model trained by optimizing the objective:\n\nL(θ), where L(θ) := Et,x0,ε\n\nmin θ\n\n(cid:104)\n\n∥ε − εθ(\n\n√\n\n ̄αtx0 +\n\n√\n\n1 − ̄αtε, t)∥2(cid:105)\n\n.\n\n(4)\n\n(5)\n\nAfter the optimization, by plugging learned score function into the generative (or reverse) diffusion process, one can simply sample from pθ(xt−1|xt) by\n\nxt−1 = μθ(xt, t) + σtε =\n\n(cid:16)\n\n1 √\nαt\n\nxt −\n\n1 − αt √\n1 − ̄αt\n\n(cid:17)\n\nεθ(xt, t)\n\n+ σtε\n\n(6)\n\nIn image translation using conditional diffusion models (Saharia et al., 2022a; Sasaki et al., 2021), the diffusion model εθ in (5) and (6) should be replaced with εθ(y, 1 − ̄αtε, t) where y denotes the matched target image. Accordingly, the sample generation is tightly controlled by the matched target in a supervised manner, so that the image content change rarely happen. Unfortunately, the requirement of the matched targets for the training makes this approach impractical.\n\n ̄αtx0 +\n\n√\n\n√\n\nTo address this, Dhariwal & Nichol (2021) proposed classifier-guided image translation using the unconditional diffusion model training as in (5) and a pre-trained classifier pφ(y|xt). Specifically, μθ(xt, t) in (4) and (6) are supplemented with the gradient of the classifier, i.e. ˆμθ(xt, t) := μθ(xt, t) + σt∇xt log pφ(y|xt). However, most of the classifiers, which should be separately trained, are not usually sufficient to control the content of the samples from the reverse diffusion process.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Given the input image xsrc, we guide the reverse diffusion process {xt}0 t=T using various losses. (a) lcont: the structural similarity loss between input and outputs in terms of contrastive loss between extracted keys from ViT. (b) lCLIP : relative distance to the target text dtrg in CLIP space in terms of xsrc and dsrc. (c) lsty: the [CLS] token distances between the outputs and target xtrg. (d) lsem: dissimilarity between the [CLS] token from the present and past denoised samples.\n\nInspired by the recent manifold constrained gradient (MCG) for inverse problems (Chung et al., 2022a), here we formulate our content and style guidance problem as an inverse problem, which can be solved by minimizing the following total cost function with respect to the sample x:\n\nltotal(x; xtrg, xsrc),\n\nor\n\nltotal(x; dtrg, xsrc, dsrc)\n\n(7)\n\nwhere xsrc and xtrg refer to the source and target images, respectively; and dsrc and dtrg refer to the source and target text, respectively. In our paper, the first form of the total loss in (7) is used for image-guided translation, where the second form is for the text-guided translation. Then, the sampling from the reverse diffusion with MCG is given by\n\n(cid:16)\n\nx′\n\nt−1 =\n\n1 √\nαt t−1 − ∇xtltotal( ˆx0(xt)) where ˆx0(xt) refers to the estimated clean image from the sample xt using the Tweedie’s formula (Kim & Ye, 2021):\n\n1 − αt √\n1 − ̄αt\n\nxt−1 = x′\n\nεθ(xt, t)\n\n+ σtε\n\nxt −\n\n(9)\n\n(8)\n\n(cid:17)\n\n√\n\nˆx0(xt) :=\n\nxt√ ̄αt\n\n−\n\n1 − ̄αt√ ̄αt\n\nεθ(xt, t).\n\n(10)\n\nIn the following, we describe how the total loss ltotal is defined. For brevity, we notate ˆx0(xt) as x in the following sections.\n\n3.2 STRUCTURE LOSS\n\nAs previously mentioned, the main objective of image translation is maintaining the content structure between output and the input image, while guiding the output to follow semantic of target condition. Existing methods (Couairon et al., 2022; Kim et al., 2022) use pixel-wise loss or the perceptual loss for the content preservation. However, the pixel space does not explicitly discriminate content and semantic components: too strong pixel loss hinders the semantic change of output, whereas weak pixel loss alters the structural component along with semantic changes. To address the problem, we need to separately process the semantic and structure information of the image.\n\nRecently, (Tumanyan et al., 2022) demonstrated successful disentanglement of both components using a pre-trained DINO ViT (Caron et al., 2021). They showed that in ViT, the keys kl of multi-head\n\n4\n\n(cid:33)\n\n,\n\n(cid:32)\n\nPublished as a conference paper at ICLR 2023\n\nself attention (MSA) layer contain structure information, and [CLS] token of last layer contains the semantic information. With above features, they proposed a loss for maintaining structure between input and network output with matching the self similarity matrix Sl of the keys, which can be represented in the following form for our problem:\n\nlssim(xsrc, x) = ∥Sl(xsrc) − Sl(x)∥F , where\n\n(cid:2)Sl(x)(cid:3)\n\ni,j = cos(kl\n\ni(x), kl\n\nj(x)),\n\n(11)\n\ni(x) and kl\n\nwhere kl j(x) indicate i, jth key in the l-th MSA layer extracted from ViT with image x. The self-similarity loss can maintain the content information between input and output, but we found that only using this loss results in a weak regularization in our DDPM framework. Since the key ki contains the spatial information corresponding the i-th patch location, we use additional regularization with contrastive learning as shown in Fig. 2(a), inspired by the idea of using both of relation consistency and contrastive learning(Jung et al., 2022). Specifically, leveraging the idea of patch contrastive loss (Park et al., 2020), we define the infoNCE loss using the DINO ViT keys:\n\nlcont(xsrc, x) = −\n\n(cid:88)\n\nlog\n\ni\n\nexp(sim(kl\n\ni(x), kl\n\nexp(sim(kl i(xsrc))/τ + (cid:80)\n\ni(x), kl\n\ni(xsrc))/τ ) j̸=i exp(sim(kl\n\ni(x), kl\n\nj(xsrc))/τ )\n\n(12) where τ is temperature, and sim(·, ·) represents the normalized cosine similarity. With this loss, we regularize the key of same positions to have closer distance, while maximizing the distances between the keys at different positions.\n\n3.3 STYLE LOSS\n\nCLIP Loss for Text-guided Image Translation Based on the previous work of (Dhariwal & Nichol, 2021), CLIP-guided diffusion (Crowson, 2022) proposed to guide the reverse diffusion using pre-trained CLIP model using the following loss function:\n\nlCLIP (dtrg, x) := −sim (ET (dtrg), EI (x)) , (13) where dtrg is the target text prompt, and EI , ET refer to the image and text encoder of CLIP, respectively. Although this loss can give text-guidance to diffusion model, the results often suffer from poor image quality.\n\nInstead, we propose to use input-aware directional CLIP loss (Gal et al. (2021)) which matches the CLIP embedding of the output image to the target vector in terms of dtrg, dsrc, and xsrc. More specifically, our CLIP-based semantic loss is described as (see also Fig. 2(b)):\n\nlCLIP (x; dtrg, xsrc, dsrc) := −sim(vtrg, vsrc)\n\n(14)\n\nwhere\n\nvtrg := ET (dtrg) + λiEI (xsrc) − λsET (dsrc), vsrc := EI (aug(x)) (15) where aug(·) denotes the augmentation for preventing adversarial artifacts from CLIP. Here, we simultaneously remove the source domain information −λsET (dsrc) and reflect the source image information to output +λiEI (xsrc) according to the values of λs and λi. Therefore it is possible to obtain stable outputs compared to using the conventional loss.\n\nFurthermore, in contrast to the existing methods using only single pre-trained CLIP model (e.g. ViT/B-32), we improve the text-image embedding performance by using the recently proposed CLIP model ensemble method (Couairon et al. (2022)). Specifically, instead of using a single embedding, we concatenate the multiple embedding vectors from multiple pre-trained CLIP models and used the it as our final embedding.\n\nSemantic Style Loss for Image-guided Image Translation In the case of image-guide translation, we propose to use [CLS] token of ViT as our style guidance. As explained in the previous part 3.2, the [CLS] token contains the semantic style information of the image. Therefore, we can guide the diffusion process to match the semantic of the samples to that of target image by minimizing the [CLS] token distances as shown in Fig. 2(c). Also, we found that using only [CLS] tokens often results in misaligned color values. To prevent this, we guide the output to follow the overall color statistic of target image with weak MSE loss between the images. Therefore, our loss function is described as follows:\n\nlsty(xtrg, x) = ||eL\n\n[CLS](xtrg) − eL\n\n[CLS](x)||2 + λmse||xtrg − x||2.\n\n(16)\n\nwhere eL\n\n[CLS] denotes the last layer [CLS] token.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n3.4 ACCELERATION STRATEGY\n\nSemantic Divergence Loss With the proposed loss functions, we can achieve text- or imageguided image translation outputs. However, we empirically observed that the generation process requires large steps to reach the the desired output. To solve the problem, we propose a simple approach to accelerate the diffusion process. As explained before, the [CLS] token of ViT contains the overall semantic information of the image. Since our purpose is to make the semantic information as different from the original as possible while maintaining the structure, we conjecture that we can achieve our desired purpose by maximizing the distance between the [CLS] tokens of the previous step and the current output during the generation process as described in Fig. 2(d). Therefore, our loss function at time t is given by\n\nlsem(xt; xt+1) = −||eL\n\n[CLS]( ˆx0(xt)) − eL\n\n[CLS]( ˆx0(xt+1))||2,\n\n(17)\n\nSpecifically, we maximize the distance between the denoised output of the present time and the previous time, so that next step sample has different semantic from the previous step. One could think of alternatives to maximize pixel-wise or perceptual distance, but we have experimentally found that in these cases, the content structure is greatly harmed. In contrast, our proposed loss has advantages in terms of image quality because it can control only the semantic appearance.\n\nResampling Strategy As shown in CCDF acceleration strategy (Chung et al., 2022b), a better initialization leads to an accelerated reverse diffusion for inverse problem. Empirically, in our image translation problem we also find that finding the good starting point at time step T for the reverse diffusion affects the overall image quality. Specifically, in order to guide the initial estimate xT to be sufficiently good, we perform N repetition of one reverse sampling xT −1 followed by one forward step xT = (cid:112)1 − βT −1xT −1 + βT −1ε to find the xT whose gradient for the next step is easily affected by the loss. With this initial resampling strategy, we can empirically found the initial xT that can reduce the number of reverse steps. The overall process is in our algorithm in Appendix.\n\n3.5 TOTAL LOSS\n\nPutting all together, the final loss in (7) for the text-guided reverse diffusion is given by\n\nltotal = λ1lcont + λ2lssim + λ3lCLIP + λ4lsem + λ5lrng,\n\n(18)\n\nwhere lrng is a regularization loss to prevent the irregular step of reverse diffusion process suggested in (Crowson (2022)). If the target style image xtrg is given instead of text conditions dsrc and dtrg, then lCLIP is simply substituted for lsty.\n\n4 EXPERIMENT\n\n4.1 EXPERIMENTAL DETAILS\n\nFor implementation, we refer to the official source code of blended diffusion (Avrahami et al. (2022)). All experiments were performed using unconditional score model pre-trained with Imagenet 256×256 resolution datasets (Dhariwal & Nichol (2021)). In all the experiments, we used diffusion step of T = 60 and the resampling repetition of N = 10; therefore, the total of 70 diffusion reverse steps are used. The generation process takes 40 seconds per image in single RTX 3090 unit. In lCLIP , we used the ensemble of 5 pre-trained CLIP models (RN50, RN50x4, ViTB/32, RN50x16, ViT-B/16) for the text-guidance, following the setup of Couairon et al. (2022). Our detailed experimental settings are elaborated in Appendix.\n\n4.2 TEXT-GUIDED SEMANTIC IMAGE TRANSLATION\n\nTo evaluate the performance of our text-guided image translation, we conducted comparisons with state-of-the-art baseline models. For baseline methods, we selected the recently proposed models which use pre-trained CLIP for text-guided image manipulation: VQGAN-CLIP (Crowson et al. (2022)), CLIP-guided diffusion (CGD) (Crowson (2022)), DiffusionCLIP (Kim et al. (2022)), and FlexIT (Couairon et al. (2022)). For all baseline methods, we referenced the official source codes.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Qualitative comparison of text-guided translation on Animals dataset. Our model generates realistic samples that reflects the text condition, with better perceptual quality than the baselines.\n\nFigure 4: Qualitative comparison of text-guided image translation on Landscape dataset. Our model generates outputs with better perceptual quality than the baselines.\n\nSince our framework can be applied to arbitrary text semantics, we tried quantitative and qualitative evaluation on various kinds of natural image datasets. We tested our translation performance using two different datasets: animal faces (Si & Zhu (2012)) and landscapes (Chen et al. (2018)). The animal face dataset contains 14 classes of animal face images, and the landscapes dataset consists of 7 classes of various natural landscape images.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nVQGAN-CLIP CLIP-GD DiffusionCLIP FlexIT Ours\n\nSFID↓ 30.01 12.50 25.09 32.71 9.98\n\nAnimals CSFID↓ 65.51 53.05 66.50 57.87 41.07\n\nLPIPS↓ 0.462 0.468 0.379 0.215 0.372\n\nSFID↓ 33.31 18.13 29.85 18.04 16.86\n\nLandscapes CSFID↓ 82.92 62.19 76.29 60.04 54.48\n\nLPIPS↓ 0.571 0.458 0.568 0.243 0.417\n\nText ↑ 2.78 2.61 2.50 2.22 3.68\n\nUser Study Realism ↑ 2.05 2.24 2.54 3.15 4.28\n\nContent ↑ 2.16 2.28 3.06 3.89 4.11\n\nTable 1: Quantitative comparison in the text-guided image translation. Our model outperforms baselines in overall scores for both of Animals and Landscapes datasets as well as user study.\n\nTo measure the performance of the generated images, we measured the FID score (Heusel et al. (2017)). However, when using the basic FID score measurement, the output value is not stable because our number of generated images is not large. To compensate for this, we measure the performance using a simplified FID (Kim et al. (2020)) that does not consider the diagonal term of the feature distributions. Also, we additionally showed a class-wise SFID score that measures the SFID for each class of the converted output because it is necessary to measure whether the converted output accurately reflects the semantic information of the target class. Finally, we used the averaged LPIPS score between input and output to verify the content preservation performance of our method. Further experimental settings can be found in our Appendix.\n\nIn Table 1, we show the quantitative comparison results. In image quality measurement using SFID and CSFID, our model showed the best performance among all baseline methods. Especially for Animals dataset, our SFID value outperformed others in large gain. In the content preservation by LPIPS score, our method scored the second best. In case of FlexIT, it showed the best score in LPIPS since the model is directly trained with LPIPS loss. However, too low value of LPIPS is undesired as it means that the model failed in proper semantic change. This can be also seen in qualitative result of Figs. 3 and 4, where our results have proper semantic features of target texts with content preservation, whereas the results from FlexIT failed in semantic change as it is too strongly confined to the source images. In other baseline methods, most of the methods failed in proper content preservation. Since our method is based on DDPM, our model can generate diverse images as shown in the additional outputs in our Appendix.\n\nTo further evaluate the perceptual quality of generated samples, we conducted user study. In order to measure the detailed opinions, we used custom-made opinion scoring system. We asked the users in three different parts: 1) Are the output have correct semantic of target text? (Text-match), 2) are the generated images realistic? (Realism), 3) do the outputs contains the content information of source images? (Content). Detailed user-study settings are in our Appendix. In Table 1, our model showed the best performance, which further shows the superiority of our method.\n\n4.3\n\nIMAGE-GUIDED SEMANTIC IMAGE TRANSLATION\n\nSince our method can be easily adapted to the image translation guided by target images, we evaluate the performance of our model with comparison experiments. We compare our model with appearance transfer models of Splicing ViT (Tumanyan et al. (2022)), STROTSS (Kolkin et al. (2019)), and style transfer methods WCT2 (Yoo et al. (2019)) and SANet (Park & Lee (2019)).\n\nMethod SANet WCT2 STROTSS SplicingViT Ours\n\nStyle ↑ 2.75 2.59 3.92 3.50 4.23\n\nRealism ↑ 4.08 4.64 2.91 2.08 4.25\n\nContent ↑ 4.37 4.90 3.17 2.15 4.51\n\nTable 2: User study comparison of image-guided translation tasks. Our model outperforms baseline methods in overall perceptual quality.\n\nFig. 5 is a qualitative comparison result of image guided translation task. Our model successfully generated outputs that follow the semantic styles of the target images while maintaining the content of the source images. In the case of other models, we can see that the content was severely deformed or the semantic style was not properly reflected. We also measured the overall perceptual quality through a user study. As with text-guided translation, we investigated user opinion through three different questions. In Table 2, our model obtained the best score in style matching score and the second best in realism and content preservation scores. Baseline WCT2 showed the best in realism and content scores, but it shows the worst score in style matching because the outputs are hardly changed from the inputs except for overall colors. The opinions scores confirm that our model outperforms the baselines. More details are in our Appendix.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Qualitative comparison of image-guided image translation. Our results have better perceptual quality than the baseline outputs.\n\nFigure 6: Qualitative comparison on ablation study. Our full setting shows the best results.\n\n4.4 ABLATION STUDY\n\nTo verify the proposed components in our framework, we compare the generation performance with different settings. In Fig. 6, we show that (a) the outputs from our best setting have the correct semantic of target text, with preserving the content of the source; (b) by removing lsem, the results still have the appearance of source images, suggesting that images are not fully converted to the target domain; (c) without lcont, the output images totally failed to capture the content of source images; (d) by using LPIPS perceptual loss instead of proposed lcont, the results can only capture the approximate content of source images; (e) using pixel-wise l2 maximization loss instead of proposed lsem, the outputs suffer from irregular artifacts; (f) without using our proposed resampling trick, the results cannot fully reflect the semantic information of target texts. (g) With using VGG16 network instead of DINO ViT, the output structure is severely degraded with artifacts. Overall, we can obtain the best generation outputs by using all of our proposed components. For further evaluation, we will show the quantitative results of ablation study in our Appendix.\n\n5 CONCLUSION\n\nIn conclusion, we proposed a novel loss function which utilizes a pre-trained ViT model to guide the generation process of DDPM models in terms of content preservation and semantic changes. We further propose a novel strategy of resampling technique for better initialization of diffusion process. For evaluation, our extensive experimental results show that our proposed framework has superior performance compared to baselines in both of text- and image-guided semantic image translation tasks. Despite the successful results, our method often fails to translate the image styles when there is large domain gap between source and target. With respect to this, we show the failure cases and discussions on limitations in Appendix.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nAcknowledgement\n\nThis research was supported by Field-oriented Technology Development Project for Customs Administration through National Research Foundation of Korea(NRF) funded by the Ministry of Science & ICT and Korea Customs Service(NRF-2021M3I1A1097938), the KAIST Key Research Institute (Interdisciplinary Research Group) Project, and the National Research Foundation of Korea under Grant (NRF-2020R1A2B5B03001980).\n\nREFERENCES\n\nAsha Anoosheh, Eirikur Agustsson, Radu Timofte, and Luc Van Gool. Combogan: Unrestrained scalability for image domain translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 783–790, 2018.\n\nOmri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18208–18218, 2022.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\n\nYang Chen, Yu-Kun Lai, and Yong-Jin Liu. Cartoongan: Generative adversarial networks for photo cartoonization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9465–9474, 2018.\n\nYunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8789–8797, 2018.\n\nMin Jin Chong and David Forsyth.\n\nJojogan: One shot face stylization.\n\narXiv preprint\n\narXiv:2112.11641, 2021.\n\nHyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for\n\ninverse problems using manifold constraints. arXiv preprint arXiv:2206.00941, 2022a.\n\nHyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conIn Proceedings ditional diffusion models for inverse problems through stochastic contraction. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12413–12422, 2022b.\n\nGuillaume Couairon, Asya Grechka, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Flexit: In Proceedings of the IEEE/CVF Conference on\n\nTowards flexible semantic image translation. Computer Vision and Pattern Recognition, pp. 18270–18279, 2022.\n\nKatherine Crowson. Clip-guided diffusion. 2022. URL https://github.com/afiaka87/\n\nclip-guided-diffusion.\n\nKatherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. arXiv preprint arXiv:2204.08583, 2022.\n\nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4690–4699, 2019.\n\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\n\nin Neural Information Processing Systems, 34:8780–8794, 2021.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873–12883, 2021.\n\nTsu-Jui Fu, Xin Eric Wang, and William Yang Wang. Language-driven image style transfer. 2021.\n\nRinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-\n\nguided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414–2423, 2016.\n\nNiv Granot, Ben Feinstein, Assaf Shocher, Shai Bagon, and Michal Irani. Drop the gan: In defense of patches nearest neighbors as single image generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469, 2022.\n\nChristopher Hahne and Amar Aggoun. Plenopticam v1.0: A light-field imaging framework. IEEE\n\nTransactions on Image Processing, 30:6757–6771, 2021. doi: 10.1109/TIP.2021.3095671.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. pp. 6626– 6637, 2017.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020.\n\nXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normal-\n\nization. pp. 1501–1510, 2017.\n\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\n\nImage-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125–1134, 2017.\n\nChanyong Jung, Gihyun Kwon, and Jong Chul Ye. Exploring patch-wise semantic relation for contrastive learning in image-to-image translation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 18260–18269, June 2022.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\nChung-Il Kim, Meejoung Kim, Seungwon Jung, and Eenjun Hwang. Simplified fr ́echet distance for generative adversarial nets. Sensors, 20(6), 2020. ISSN 1424-8220. doi: 10.3390/s20061548. URL https://www.mdpi.com/1424-8220/20/6/1548.\n\nGwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2426–2435, 2022.\n\nKwanyoung Kim and Jong Chul Ye. Noise2score: tweedie’s approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Systems, 34:864– 874, 2021.\n\nNicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal transport and self-similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10051–10060, 2019.\n\nGihyun Kwon and Jong Chul Ye. Clipstyler: Image style transfer with a single text condition. arXiv\n\npreprint arXiv:2112.00374, 2021.\n\nGihyun Kwon and Jong Chul Ye. One-shot adaptation of gan in just one clip. arXiv preprint\n\narXiv:2203.09301, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDongwook Lee, Junyoung Kim, Won-Jin Moon, and Jong Chul Ye. Collagan: Collaborative gan for missing image data imputation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2487–2496, 2019.\n\nJianxin Lin, Yingxue Pang, Yingce Xia, Zhibo Chen, and Jiebo Luo. Tuigan: Learning versatile In European Conference on Computer\n\nimage-to-image translation with two unpaired images. Vision, pp. 18–35. Springer, 2020.\n\nXingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream: Training-free text-to-image generation with improved clip+ gan space optimization. arXiv preprint arXiv:2112.01573, 2021.\n\nTimo L ̈uddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7086– 7096, 2022.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\n\nIn International Conference on Machine Learning, pp. 8162–8171. PMLR, 2021.\n\nUtkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros, Yong Jae Lee, Eli Shechtman, and Richard In Proceedings of the\n\nZhang. Few-shot image generation via cross-domain correspondence. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\nDae Young Park and Kwang Hee Lee. Arbitrary style transfer with style-attentional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5880–5888, 2019.\n\nTaesung Park, Alexei A. Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 319–345, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58545-7.\n\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Textdriven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2085–2094, 2021.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1–10, 2022a.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n\nHiroshi Sasaki, Chris G Willcocks, and Toby P Breckon. Unit-ddpm: Unpaired image translation\n\nwith denoising diffusion probabilistic models. arXiv preprint arXiv:2104.05358, 2021.\n\nZhangzhang Si and Song-Chun Zhu. Learning hybrid image templates (hit) by information projection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1354–1367, 2012. doi: 10.1109/TPAMI.2011.227.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\n\ntional Conference on Learning Representations, 2020a.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b.\n\nXu Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-\n\nto-image translation. ArXiv, abs/2203.08382, 2022.\n\nNarek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10748–10757, 2022.\n\nCan Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-andimage driven manipulation of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3835–3844, 2022a.\n\nZhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11686–11695, 2022b.\n\nTianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, and In Proceedings of the\n\nNenghai Yu. Hairclip: Design your hair by text and reference image. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18072–18081, 2022.\n\nJaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha. Photorealistic style transfer via wavelet transforms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9036–9045, 2019.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nPeihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka. Mind the gap: Domain gap conarXiv preprint\n\ntrol for single shot domain adaptation for generative adversarial networks. arXiv:2110.08398, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA EXPERIMENTAL DETAILS\n\nA.1\n\nIMPLEMENTATION DETAILS\n\nFor implementation, in case of using text-guided image manipulation, our initial sampling numbers are set as T = 100, but we skipped the initial 40 steps to maintain the abstract content of input image. Therefore, the total number of sampling steps is T = 60. With resampling step of N = 10, we use total of 70 iterations for single image output. We found that using more resampling steps does not show meaningful performance improvement. In image-guided manipulation, we set initial sampling number T = 200, and skipped the initial 80 steps. We used resampling step N = 10. Therefore, we use total of 130 iterations. Although we used more iterations than text-guided translation, it takes about 40 seconds.\n\nFor hyperparameters, we use λ1 = 200, λ2 = 100, λ3 = 2000, λ4 = 1000, λ5 = 200. For imageguided translation, we set λmse = 1.5. For our CLIP loss, we set λs = 0.4 and λi = 0.2. For our ViT backbone model, we used pre-trained DINO ViT that follows the baseline of Splicing ViT (Tumanyan et al., 2022). For extracting keys of intermediate layer, we use layer of l = 11, and for [CLS] token, we used last layer output. Since ViT and CLIP model only take 224×224 resolution images, we resized all images before calculating the losses with ViT and CLIP.\n\nTo further improve the sample quality of our qualitative results, we used restarting trick in which we check the lreg loss calculated at initial time step T , and restart the whole process if the loss value is too high. If the initial loss lreg > 0.01, we restarted the process. For quantitative result, we did not use the restart trick for fair comparison.\n\nFor augmentation, we use the same geometrical augmentations proposed in FlexIT(Couairon et al., 2022). Also, following the setting from CLIP-guided diffusion(Crowson, 2022), we included noise augmentation in which we mix the noisy image to ˆx0(xt) as it further removes the artifacts.\n\nIn our image-guided image translation on natural landscape images, we matched the color distribution of output image to that of target image with (Hahne & Aggoun, 2021), as it showed better perceptual quality. Our detailed implementation can be found in our official GitHub repository.1\n\nFor baseline experiments, we followed the official source codes in all of the models2345. For diffusion-based models (DiffusionCLIP, CLIP-guided diffusion), we used unconditional score model pre-trained on 256×256 resolutions. In DiffusionCLIP, we fine-tuned the score model longer than suggested training iteration, as it showed better quality. In CLIP-guided diffusion, we set the CLIP-guided loss as 2000, and also set initial sampling number as T = 100 with skipping initial 40 steps. For VQGAN-based models (FlexIT, VQGAN-CLIP), we used VQGAN trained on imagenet 256×256 resolutions datasets. In VQGAN-CLIP, as using longer iteration results in extremely degraded images, therefore we optimized only 30 iterations, which is smaller than suggested iterations ( ≥80). In the experiments of FlexIT, we followed the exactly same settings suggested in the original paper.\n\nFor baselines of image-guided style transfer tasks, we also referenced the original source codes6789. In all of the experiments, we followed the suggested settings from the original papers.\n\nA.2 DATASET DETAILS\n\nFor our quantitative results using text-guided image translation, we used two different datasets Animals and Landscapes. In Animals dataset, the original dataset contains 21 different classes, but we filtered out the images from 14 classes (bear, cat, cow, deer, dog, lion, monkey, mouse, panda, pig,\n\n1https://github.com/anon294384/DiffuseIT 2https://github.com/afiaka87/clip-guided-diffusion 3https://github.com/nerdyrodent/VQGAN-CLIP 4https://github.com/gwang-kim/DiffusionCLIP 5https://github.com/facebookresearch/SemanticImageTranslation 6https://github.com/omerbt/Splice 7https://github.com/nkolkin13/STROTSS 8https://github.com/clovaai/WCT2 9https://github.com/GlebSBrykin/SANET\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nrabbit, sheep, tiger, wolf) which can be classified as mammals. Remaining classes (e.g. human, chicken, etc.) are removed since they have far different semantics from the mammal faces.Therefore we reported quantitative scores only with filtered datasets for fair comparison. The dataset contains 100-300 images per each class, and we selected 4 testing images from each class in order to use them as our content source images. With selected samples, we calculated the metrics using the outputs of translating the 4 images from a source class into all the remaining classes. Therefore, in our animal face dataset, total of 676 generated images are used for evaluation.\n\nIn Landscapes dataset, we manually classified the images into 7 different classes (beach, desert, forest, grass field, mountain, sea, snow). Each class has 300 different images except for desert class which have 100 different images. Since some classes have not enough number of images, we borrowed images from seasons (Anoosheh et al., 2018) dataset. For metric calculation, we selected 8 testing images from each class, and used them as our content source images. Again, we translated the 8 images from source class into all the remaining classes. Therefore, a total of 336 generated images are used for our quantitative evaluation.\n\nFor single image guided translation, we selected random images from AFHQ dataset for animal face translation; and for natural image generation, we selected random images from our Landscapes datasets.\n\nA.3 USER STUDY DETAILS\n\nFor our user study in text-guided image translation task, we generated 130 different images using 13 different text conditions with our proposed and baseline models. Then we randomly selected 65 images and made 6 different questions. More specifically, we asked the participants question about three different parts: 1) Are the outputs have correct semantic of target text? (Text-match), 2) Are the generated images realistic? (Realism), 3) Do the outputs contain the content information of source images (Content). We randomly recruited a total of 30 users, and provided them the questions using Google Form. The 30 different users come from age group 20s and 50s. We set the minimum score as 1, and the maximum score is 5. The users can score among 5 different options : 1-Very Unlikely, 2-Unlikely, 3-Normal, 4-Likely, 5-Very Likely.\n\nFor the user study on image-guided translation task, we generated 40 different images using 8 different images conditions. Then we followed the same protocol to user study on text-guided image translation tasks, except for the content of questions. We asked the users in three different parts: 1) Are the outputs have correct semantic of target style image? (Style-match), 2) Are the generated images realistic? (Realism), 3) Do the outputs contain the content information of source images (Content).\n\nA.4 ALGORITHM\n\nFor detailed explanation, we include Algorithm of our proposed image translation mathods in Algorithm 1.\n\nB QUANTITATIVE ABLATION STUDY\n\nFor more thorough evaluation of our proposed components, we report ablation study on quantitative metrics. In this experiment, we only used Animals dataset due to the time limit. In Table 3, we show the quantitative results on various settings. When we remove one of our acceleration strategies, in setting (b) and (f), we can see that the fid score is degraded as the outputs are not properly changed from the original source images. (e) When we use L2 maximization instead of our proposed lsem, FID scores are improved from setting (b), but still the performance is not on par with our best settings. (d) When we use weak content regularization using LPIPS, we can see that the overall scores are degraded. When we remove our proposed lcont, we can observe that SFID and CSFID scores are lower than other settings. However, we can see that LPIPS score is severely high as the model hardly reflect the content information of original source images. (g) we use pre-trained VGG instead of using ViT for ablation study. Instead of ViT keys for structure loss, we substitute it with features extracted from VGG16 relu3 1 activation layer. Also, we substitute ViT [CLS] token with VGG16 relu5 1 feature as it contains high-level semantic features. We can see that the model\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Semantic image translation: given a diffusion score model εθ(xt, t), CLIP model, and VIT model Input: source image xsrc, diffusion steps T , resampling steps N , target text dtrg, source text dsrc or target image xtrg Output: translated image ˆx which has semantic of dtrg (or xtrg) and content of xsrc √\nxT ∼ N (\n\n ̄αtxsrc, (1 − ̄αt)I), index for resampling n = 0\n\n4: 5: 6: 7: 8: 9:\n\n10:\n\n1: for all t from T to 0 do ε ← εθ(xt, t) 2: ˆx0(xt) ← xt√ −\n ̄αt if text-guided then\n\n3:\n\n√\n\n1− ̄αt√ ̄αt\n\nε\n\n∇total ← ∇xtltotal( ˆx0(xt); dtrg, xsrc, dsrc)\n\nelse if image-guided then\n\n∇total ← ∇xtltotal( ˆx0(xt); xtrg, xsrc)\n\nend if z ∼ N (0, I) t−1 = 1√ x′ xt−1 = x′ if t = T and n < N then\n\nαt t−1 − ∇total\n\nxt − 1−αt√ 1− ̄αt\n\n(cid:16)\n\n(cid:17)\n\nε\n\n+ σtz\n\nxt ← N ((cid:112)1 − βt−1xt−1, βt−1I) n ← n + 1 go to 2\n\n11: 12: 13: 14: 15: 16: 17: end for 18: return x−1\n\nend if\n\nSettings\n\nVGG instead of ViT (g) No resampling (f) L2 Max instead of lsem (e) LPIPs instead of lcont (d) No lcont (c) No lsem (b) Ours (a)\n\nSFID↓ 9.72 11.88 13.18 11.15 9.90 15.00 9.98\n\nAnimals CSFID↓ 43.08 59.09 49.47 58.67 33.07 53.43 41.07\n\nLPIPS↓ 0.518 0.316 0.324 0.400 0.477 0.347 0.372\n\nTable 3: Quantitative comparison of ablation studies.\n\nshows decent SFID and CSFID scores, but the LPIPS score is very high. The result show that using VGG does not properly operate as regularization tool, rather it degrades the generation process with damaging the structural consistency. Overall, when using our best setting, we can obtain the best output considering all of the scores.\n\nC ARTISTIC STYLE TRANSFER\n\nWith our framework, we can easily adapt our method to artistic style transfer. With simply changing the text conditions, or using artistic paintings as our image conditions, we can obtain the artistic style transfer results as shown in Fig. 7.\n\nD FACE IMAGE TRANSLATION\n\nInstead of using score mode pre-trained on Imagenet dataset, we can use pre-trained score model on FFHQ human face dataset. In order to keep the face identity between source and output images, we include λidlid which leverage pre-trained face identification model ArcFace(Deng et al., 2019). We calculate identity loss between xsrc and denoised image ˆx0(xt). We use λid = 100.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Various outputs of artistic style transfer. We can translation natural images into artistic style paintings with both of text or image conditions.\n\nFigure 8: Outputs from face image translation models. The outputs from our model successfully translated the human face images with proper target domain semantic information.\n\nIn Fig. 8, we show that our method also can be used in face image translation tasks. For comparison, we included baseline models of face editing method StyleCLIP (Patashnik et al., 2021), and oneshot face stylization model of JojoGAN (Chong & Forsyth, 2021). The results show that our method can translate the source faces into target domain with proper semantic change. In baseline models, although some images show high quality outputs, in most cases the image failed in translating the images. Also, since the baseline models rely on pre-trained StyleGAN, they require additional GAN inversion process to translate the source image. Therefore, the content information is not perfectly matched to the source image due to the limitation of GAN inversion methods.\n\nE INFERENCE TIME COMPARISON\n\nTo evaluate the time-efficiency of our method, we calculate the inference times of the various imageguided translation models. All experiments are conducted with single RTX3090 GPU, on the same\n\n17\n\nPublished as a conference paper at ICLR 2023\n\ntime\n\nOurs 37s\n\nSplicingVit 25m 30s\n\nSTROTSS WCT2 0.18s\n\n53s\n\nSANET 0.12s\n\nTable 4: Quantitative comparison on inference times of image-guided translation models.\n\nFigure 9: Comparison results on semantic segmentation maps from baseline outputs. When comparing segmentation maps, our model outputs show high structural consistency with the source images.\n\nhardware and software environment. We use the images of resolution 256×256 for experiments. In Table 4, we compare the times taken for single image translation. For single-shot semantic transfer models of Splicing ViT, the inference time is relatively long as we need to optimize large U-Net model for each image translation. In STROTSS, it requires texture matching calculation for single image translation, so it takes long time. For arbitrary style transfer models of WCT2 and SANet, the inference is done with only single-step network forward process, as the model is already trained with large dataset. Our model takes about 40 seconds, which is moderate when compared to the one-shot semantic transfer models (SplicingVit,STROTSS). However, the time is still longer than the style transfer models, as our model need multiple reverse DDPM steps for inference. In the future work, we are planning to improve the inference time with leveraging recent approaches.\n\nF SEMANTIC SEGMENTATION OUTPUTS\n\nTo further verify the structural consistency between output and source images, we compared the semantic segmentation maps from outputs and source images. For experiment, we use semantic segmentation model (Zhou et al., 2017) which is pre-trained on ADE20K dataset. We referenced the official source code10 for segmentation model. Figure 9 shows the comparison results. In case of the baseline models VQGAN-CLIP, CLIP-guided diffusion, we can see that the segmentation maps are not properly aligned to the source maps, which means the model cannot keep the structure of source images. In case of FlexIT, the model outputs maps have high similarity to the source maps, but the semantic change is not properly applied. In our model, we can see the output maps have high similarity to the source maps, while semantic information is properly changed.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Additional comparison on image-guided translation. For fair experiment conditioning, we trained the baseline SANet with ViT-based losses.\n\nFigure 11: Ablation study results on pixel-wise l2 loss. Without pixel loss, the output image color is not matched to the color scale of the target images.\n\nG ADDITIONAL COMPARISON ON IMAGE-GUIDED TRANSLATION\n\nFor fair comparison with the baseline models, we trained the baseline SANet with our proposed ViTbased loss functions. When we train SANet with replacing the existing style and content loss with our lcont and lsty, we found that the training is not properly working. Therefore, we simultaneously used existing VGG-based style and content loss with our proposed ViT-based losses. In Fig. 10, we can see that when training SANet with ViT, the results still show incomplete semantic transfer results. Although the output seems to contain more complex textures than basic model, the model performance is still confined to simple color transformation.\n\n10https://github.com/CSAILVision/semantic-segmentation-pytorch\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 12: Failure case outputs. If the semantic distance between source and target conditions are extremely far, semantic translation sometimes fails.\n\nTo further evaluate the effect of pixel-wixe l2 loss for image-guided translation, we conducted additional experiments in Fig. 11. When we remove the pixel-wise l2 loss in our image-guided translation task, we can see the semantic of output images follow the target images, but the overall color of the output images are slightly unaligned with the target image color. The result show that using weak l2 loss help the model to accurately apply the color of target images to outputs.\n\nH LIMITATION AND FUTURE WORK\n\nAlthough our method has shown successful performance in image conversion, it still has limitations to solve. First, if the semantic distance between the source image and the target domain is too far (e.g building → Tiger), the output is not translated properly as shown in Fig. 12. We conjecture that this occurs when the text-image embedding space in the CLIP model is not accurately aligned, therefore it can be solved by using the advanced text-to-image embedding model. Second, our method has limitation that the image generation quality heavily relies on the performance of the pre-trained score model. This can also be solved if we use a diffusion model backbone with better performance. In future work, we plan to improve our proposed method in these two directions.\n\nI ADDITIONAL RESULTS\n\nFor additional results , in Fig. 13 we show the image translation outputs using text conditions. In Fig. 14, we additionally show the results from our image-guided image translation. We can successfully change the semantic of various natural images with text and image conditions.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: Qualitative results of text-guided image translation.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 14: Qualitative results of image-guided image translation.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper studies the problem of (text/image) guided (single) image editing using a pre-trained diffusion model. Specifically, it proposed a novel diffusion-based image translation method by disentangling style and content representations. Borrowed from the disentangling technique introduced recently (Tumanyan et al., 2022), a pre-trained Vision Transformer (ViT) has been used to aid the style and content disentanglement during the diffusion process. To accelerate the reverse diffusion process, this paper also applied two techniques, namely, semantic divergence loss and resampling strategy (Chung et al., 2022b). Experimental evaluations have been conducted on Animals and Landscapes datasets. Both quantitative and qualitative results demonstrate the strength of the proposed method over existing baselines such as Splicing ViT (Tumanyan  et al., 2022), VQGAN-CLIP (Crowson et al., 2022), and Flexit (Couairon et al., 2022).\n\n# Strength And Weaknesses\n\nStrengths:\n* [S1] This paper presents a unified framework to improve guided image translation with a pre-trained diffusion model. Results on both text-guided and image-guided applications look more realistic (in terms of shape and texture) compared to the existing baselines, including very recent ones published this year.\n* [S2] The combination of different techniques including slicing Vision Transformer, Manifold Constrained Gradient (MCG), and Come-closer-diffuse-faster (CCDF) make a lot of sense. It is great to see a combination of recent techniques really make a big difference on the guided image translation quality.\n\nWeaknesses:\n* [W1] Although results are very impressive on the animals and landscapes dataset, the technical novelty of the paper is very constrained. In the Figure 2, almost all the components have been explored in the previous work. To be fair, this paper is the first one that shows great results by combining all of them in a unified framework.\n\n* [W2] As this paper leverages many pre-trained models such as a pre-trained image diffusion model, a ViT model, and a CLIP model, the reviewer feels the comparison to existing baselines in the guided image translation setting is a bit unfair. For example, what happens if we leverage pre-trained models and apply loss functions to aid the SANet (Park and Lee, 2019). It would be good to show some fair qualitative comparisons in the image translation setting (in addition to the Section 4.4). \n\n* [W3] It is questionable if the guided translation results look very diverse or not. The reviewer would like to know if this is a sign of memorizing the training set (from the pre-trained models). For example, it would be good to find the closest example in the ImageNet and show them on the side. Alternatively, it would be good to show multiple translation results from either image-guided and text-guided applications.\n\n* [W4] The reviewer noticed that FlexIT actually achieved better LPIPS score in Table 1. It is true that FlexIT is trained directly with LPIPS score, as explained in the paper. The reviewer still feels that the proposed model is structure-aware (due to disentanglement) but not very content-preserving. It would be good to elaborate a bit on this.\n\n* [W5] What are the failure cases of the proposed method? It is important to mention the failure cases in the main text and show more results in the supplementary material.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written with a good amount of details.\nWhile the results look impressive, the novelty is a bit limited as mentioned in the review.\nThe paper contains sufficient details for a domain expert to reproduce. However, it would be great if the code can be open-sourced in the future.\n\n# Summary Of The Review\n\nOverall, I think this is a good paper. I am happy to raise my score if the concerns can be addressed in the rebuttal.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nNEURAL GROUNDPLANS: PERSISTENT NEURAL SCENE REPRESENTATIONS FROM A SINGLE IMAGE\n\nPrafull Sharma1∗\n\nAyush Tewari1\n\nYilun Du1\n\nSergey Zakharov4\n\nRares Ambrus4\n\nAdrien Gaidon4\n\nWilliam T. Freeman1,5\n\nFrédo Durand1\n\nJoshua B. Tenenbaum1,2,3 Vincent Sitzmann1\n\n1MIT CSAIL\n\n2MIT BCS\n\n3MIT CBMM 4Toyota Research Institute\n\n5The NSF AI Institute for Artificial Intelligence and Fundamental Interactions prafullsharma.net/neural_groundplans\n\nABSTRACT\n\nWe present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird’seye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthesis, instance-level segmentation, 3D bounding box prediction, and scene editing. This highlights the value of neural groundplans as a backbone for efficient 3D scene understanding models.\n\n1\n\nINTRODUCTION\n\nWe study the problem of inferring a persistent 3D scene representation given a few image observations, while disentangling static scene components from movable objects (referred to as dynamic). Recent works in differentiable rendering have made significant progress in the long-standing problem of 3D reconstruction from small sets of image observations (Yu et al., 2020; Sitzmann et al., 2019b; Sajjadi et al., 2021). Approaches based on pixel-aligned features (Yu et al., 2020; Trevithick & Yang, 2021; Henzler et al., 2021) have achieved plausible novel view synthesis of scenes composed of independent objects from single images. However, these methods do not produce persistent 3D scene representations that can be directly processed in 3D, for instance, via 3D convolutions. Instead, all processing has to be performed in image space. In contrast, some methods infer 3D voxel grids, enabling processing such as geometry and appearance completion via shift-equivariant 3D convolutions (Lal et al., 2021; Guo et al., 2022), which is however expensive both in terms of computation and memory. Meanwhile, bird’s-eye-view (BEV) representations, 2D grids aligned with the ground plane of a scene, have been fruitfully deployed as state representations for navigation, layout generation, and future frame prediction (Saha et al., 2022; Philion & Fidler, 2020; Roddick et al., 2019; Jeong et al., 2022; Mani et al., 2020). While they compress the height axis and are thus not a full 3D representation, 2D convolutions on top of BEVs retain shift-equivariance in the ground plane and are, in contrast to image-space convolutions, free of perspective camera distortions.\n\nInspired by BEV representations, we propose conditional neural groundplans, 2D grids of learned features aligned with the ground plane of a 3D scene, as a persistent 3D scene representation\n\n∗Email: prafull@mit.edu, sitzmann@mit.edu\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Given a single image, our model infers separate 3D representations for static and dynamic scene elements, enabling high-quality novel view synthesis with plausible completion, unsupervised instance-level segmentation, 3D bounding box prediction, 3D scene editing, and extraction of objectcentric 3D representations. Our model is trained self-supervised using unlabeled multi-view videos.\n\nreconstructed in a feed-forward manner. Neural groundplans are a hybrid discrete-continuous 3D neural scene representation (Chan et al., 2022; Peng et al., 2020; Philion & Fidler, 2020; Roddick et al., 2019; Mani et al., 2020) and enable 3D queries by projecting a 3D point onto the groundplan, retrieving the respective feature, and decoding it via an MLP into a full 3D scene. This enables self-supervised training via differentiable volume rendering. By compactifying 3D space with a nonlinear mapping, neural groundplans can encode unbounded 3D scenes in a bounded region. We further propose to reconstruct separate neural groundplans for 3D regions of a scene that are movable and 3D regions of a scene that are static given a single input image. This requires that objects are moving in the training data, enabling us to learn a prior to predict which parts of a scene are movable and static from a single image at test time. We achieve this additional factorization by training on multi-view videos, such as those available from cameras at traffic intersections or sports game footage. Our model is trained self-supervised via neural rendering without pseudo-ground truth, bounding boxes, or any instance labels. We demonstrate that separate reconstruction of movable objects enables instance-level segmentation, recovery of 3D object-centric representations, and 3D bounding box prediction via a simple heuristic leveraging that connected regions of 3D space that move together belong to the same object. This further enables intuitive 3D editing of the scene.\n\nSince neural groundplans are 2D grids of features without perspective camera distortion, shiftequivariant processing using inexpensive 2D CNNs effectively completes occluded regions. Our model thus outperforms prior pixel-aligned approaches in the synthesis of novel views that observe 3D regions that are occluded in the input view. We further show that by leveraging motion cues at training time, our method outperforms prior work on the self-supervised discovery of 3D objects.\n\nIn summary, our contributions are:\n\n• We introduce self-supervised training of conditional neural groundplans, a hybrid discretecontinuous 3D neural scene representation that can be reconstructed from a single image, enabling efficient processing of scene appearance and geometry directly in 3D.\n\n• We leverage object motion as a cue for disentangling static background and movable\n\nforeground objects given only a single input image.\n\n• Using the 3D geometry encoded in the dynamic groundplan, we demonstrate single-image 3D instance segmentation and 3D bounding box prediction, as well as 3D scene editing.\n\n2 RELATED WORK\n\nNeural Scene Representation and Rendering. Several works have explored learning neural scene representations for downstream tasks in 3D. Emerging neural scene representations enable reconstruction of geometry and appearance from images as well as high-quality novel view synthesis via differentiable rendering. A large part of recent work focuses on the case of reconstructing a single 3D scene given dense observations (Cheng et al., 2018; Tung et al., 2019; Sitzmann et al., 2019a; Lombardi et al., 2019; Mildenhall et al., 2020; Yariv et al., 2020; Tewari et al., 2021). Alternatively, differentiable rendering may be used to supervise encoders to reconstruct scenes from a single or few images in a feedforward manner. Pixel-aligned conditioning enables reconstruction of compositional scenes (Yu et al., 2020; Trevithick & Yang, 2021), but does not infer a compact 3D representation. Methods with a single latent code per scene do, but do not generalize to compositional scenes (Sitzmann et al., 2019c; Jang & Agapito, 2021; Niemeyer et al., 2020; Sitzmann et al., 2021; Kosiorek\n\n2\n\nInputStaticDynamicBird’s-eye viewInstance-levelsegmentationBounding boxsegmentationNovel viewsDepthReconstructionPublished as a conference paper at ICLR 2023\n\net al., 2021). Voxel grid based approaches offer both benefits, but are computationally costly(Lal et al., 2021; Sajjadi et al., 2021; Dupont et al., 2020). Hybrid discrete-continuous neural scene representations offer a compromise by factorizing a dense 3D field into several lower-dimensional representations that are used to condition an MLP (Chan et al., 2022; Chen et al., 2022a). In particular, neural groundplans and axis-aligned 2D grids enable high-quality unconditional generation of 3D scenes (DeVries et al., 2021; Chan et al., 2022) as well as reconstruction of 3D geometry from pointclouds (Peng et al., 2020). We similarly use axis-aligned 2D grids of features for self-supervised scene representation via neural rendering, but reconstruct them directly from few or a single 2D image observations.\n\nBird’s-Eye View Representations. Bird’s-eye view has been explored as a 3D representation in vision and robotics, particularly for autonomous driving applications. Prior work uses ground-plane 2D grids as representations for object detection and segmentation (Saha et al., 2022; Harley et al., 2022; Philion & Fidler, 2020; Reiher et al., 2020; Roddick et al., 2019), layout generation and completion (Cao & de Charette, 2022; Jeong et al., 2022; Mani et al., 2020; Yang et al., 2021b), and next-frame prediction (Hu et al., 2021; Zi ̨eba et al., 2020). The bird’s-eye view is generated either directly without 3D inductive biases (Mani et al., 2020), or similar to our proposed approach, by using 3D geometry-driven inductive biases such as unprojection into a volume (Harley et al., 2022; Chen et al., 2022b; Roddick et al., 2019), or by generating a 3D point cloud (Philion & Fidler, 2020; Hu et al., 2021). However, prior approaches are supervised, using ground truth bounding boxes or semantic segmentation as supervision. In contrast, we present a self-supervised conditional groundplan representation, learned only from images via neural rendering. While we show that our self-supervised representation can be used for rich inference tasks using simple heuristics, our method may be extended for more challenging tasks using the techniques developed in prior work.\n\nDynamic-Static Disentanglement. Our work is related to prior work on learning to disentangle dynamic objects and static background. Some prior work leverages object motion across video frames to learn separate representations for movable foreground and static background in 2D (Kasten et al., 2021; Ye et al., 2022; Bao et al., 2022), while other recent work can also learn 3D representations (Yuan et al., 2021; Tschernezki et al., 2021). Our approach is similar in using object motion as cue for disentanglement and multi-view as cue for 3D reconstruction, but uses it as supervision to train an encoder-based approach that enables reconstruction from a single image instead of scene-specific disentanglement from multiple video frames.\n\nObject-centric Scene Representations. Prior work has aimed to infer object-centric representations directly from images, with objects either represented as localized object-centric patches (Lin et al., 2020; Eslami et al., 2016; Crawford & Pineau, 2019; Kosiorek et al., 2018; Jiang et al., 2019) or scene mixture components (Engelcke et al., 2020; Burgess et al., 2019; Greff et al., 2019; 2016; 2017; Du et al., 2021a), with the slot attention module (Locatello et al., 2020) increasingly driving object-centric inference. Resulting object representations may be decoded into object-centric 3D representations and composed for novel view synthesis (Yu et al., 2022; Smith et al., 2022; Elich et al., 2022; Chen et al., 2021; Bear et al., 2020; Zakharov et al., 2020; 2021; Beker et al., 2020; Du et al., 2021b). BlockGAN and GIRAFFE (Nguyen-Phuoc et al., 2020; Niemeyer & Geiger, 2021) build unconditional generative models for compositions of 3D-structured representations, but are restricted to only generation. Some methods rely on annotations such as bounding boxes, object classes, 3D object models, or instance segmentation to recover object-centric neural radiance fields (Ost et al., 2021; Yang et al., 2021a; Guo et al., 2020; Yang et al., 2022). Several scene reconstruction methods (Zakharov et al., 2020; 2021; Beker et al., 2020; Nie et al., 2020) use direct supervision to train an object representation and detector to infer an editable 3D scene from a single frame observation. Kipf et al. (2021) leverage motion as a cue for self-supervised object disentanglement, but do not reconstruct 3D and require additional conditioning in the form of bounding boxes. In this work, we demonstrate that a representation factorized into static and movable 3D regions can serve as a powerful backbone for object discovery. While not explored in this work, slot attention and related object-centric algorithms could be run on our already sparse groundplan of movable 3D regions, faced with a dramatically easier task than when run on images directly.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Groundplan inference. Given a context image, we first extract a set of CNN features. We unproject the features into 3D and re-sample them at “pillars” on top of the location of groundplan vertices. Pillars are aggregated into groundplan features using a softmax-weighted sum. The resulting 2D grid of features is decomposed into separate dynamic and static groundplans by a 2D CNN. The coordinate-encoding MLP is not visualized in this figure. Please refer to Sec. 3 for details.\n\n3 CONDITIONAL NEURAL GROUNDPLANS\n\nIn this section, we describe the process of inferring a neural ground plan given one or several image observations in a feed-forward manner, as well as subsequent novel view synthesis. The method will be trained on a dataset of multi-view videos with calibrated cameras with wide baselines. Please see Fig. 2 for an overview.\n\nCompactified neural groundplans for unbounded scene representations. A neural groundplan is a 2D grid of features aligned with the ground plane of the 3D scene, which we define to be the xz−plane. A 3D point is decoded by projecting it onto the groundplan and retrieving the corresponding feature vector using bilinear interpolation. This feature is then concatenated with the vertical y-coordinate of the query point and decoded into radiance and density values via a fully connected network, enabling novel view synthesis using volume rendering (Mildenhall et al., 2020). In this definition, however, it is only possible to decode 3D points that lie within the boundaries of the neural groundplan, which precludes reconstruction and representation of unbounded scenes. We thus compactify R3 by implementing a non-linear coordinate re-mapping as proposed by Barron et al. (2021). Points x within a radius rinner around the groundplan origin remain unaffected, but points outside this radius are contracted. For any 3D point x, the contracted 3D coordinate can be computed as x′ = C(x) = ((1 + k) − k/||u||)(u/||u||)rinner, where u = x/rinner, and k is a hyperparameter which controls the size of the contracted region. Note that C is invertible, such that x = C −1(x′) is a function that takes a 3D point in contracted space x′ to the original 3D point x in linear space.\n\nReconstructing neural groundplans from images. Inferring a neural groundplan from one or several images proceeds in three steps: (1) feature extraction, (2) feature unprojection, (3) pillar aggregation. Given a single image I, we first extract per-pixel features via a CNN encoder to yield a feature tensor F. We define the camera as the world origin and center the neural groundplan accordingly, approximately aligned with the ground level. The image features are unprojected to a 3D feature volume v in contracted world space using the inverse of the contract function defined earlier. We extract the feature at a contracted 3D point x′ in v as v(x′) = F[π(C −1(x′))], where C −1(x′) first maps the contracted point to linear world space and π(·) projects it onto the image plane of the context view using camera extrinsics and intrinsics. At any vertex of the groundplan, the discretized y-coordinates of the volume form a “pillar”. Next, we aggregate each pillar into a point to create the 2D groundplan. We first use a coordinate-encoding MLP D(·) to transform the volume as f (x′) = D(v(x′), xc, d), where xc denotes the 3D point in linear camera coordinates of the context camera, and d denotes the ray direction from the camera center to that point. Since all features along a camera ray are identical in v, coordinate encoding is used to add the depth information to the features. In the case of multi-view input images, the volumes corresponding to each input view are mean pooled. Associated to each 2D vertex of the groundplan is now a set of features {fi}N i=1, where N is the number of samples along the y-dimension. We use a “pillar-aggregation” MLP to compute softmax scores as αi = P (fi, xi), where P (·) denotes the MLP and xi is the linear coordinate of the i-th point on the pillar. Finally, the features are aggregated by computing the weighted sum of the features, g = (cid:80)\n\ni αifi.\n\nDifferentiable Rendering. We can render images from novel camera views via differentiable volume rendering (DeVries et al., 2021; Lombardi et al., 2019; Mildenhall et al., 2020). To resolve points closer to the camera more finely, we adopt logarithmic sampling of points along the ray with more\n\n4\n\nPillar AggregationStatic-Dynamic DisentanglementsoftmaxscoresStatic ground planDynamic ground planInputCameraCNNFeatures3D SceneEntangledNeural Ground PlanMLP2D CNNPublished as a conference paper at ICLR 2023\n\nFigure 3: Learning Static-Dynamic Disentanglement. Given multiple frames of a video, we extract per-frame, compactified, static and dynamic groundplans according to Fig. 2. Static groundplans are pooled into a time-invariant groundplan. We then composite per-frame dynamic and static time-invariant groundplans via differentiable volume rendering. Our model is supervised only via a re-rendering loss on video frames. We encourage the model to explain as much of the scene density as possible with the static groundplan via a sparsity loss on per-frame dynamic volume rendering densities. The surface loss is not visualized here.\n\nsamples close to the camera (Neff et al., 2021). For each sampled point x on the camera ray, we need to compute its density and color for volume rendering. This is accomplished using a rendering MLP, as (cx, σx) = R(gx, yx), where R(·) denotes the MLP, gx are the groundplan features for the point x computed by projecting the query 3D coordinates onto the groundplane and bilinearly interpolating the nearest grid points, and yx is the y value of the sampled point x.\n\n4 LEARNING STATIC-DYNAMIC DISENTANGLEMENT\n\nWe now describe training on multi-view video to learn to disentangle static and dynamic components of the scene. Furthermore, we describe a method for performing self-supervised 3D object discovery and 3D bounding box prediction using the geometry encoded in the dynamic groundplan representation. Please see Fig. 3 for an overview of the multi-frame training for static-dynamic disentanglement.\n\nDisentangling static and dynamic neural groundplans. We leverage the fact that objects move in the given multi-view videos as the training signal. We pick two random frames of a video. For each frame, we infer an entangled neural groundplan as described in the previous section. Features in this entangled neural groundplan parameterize both static and dynamic features of the scene, for instance, a car as well as the road below it. We feed this groundplan into a fully convolutional 2D network, which disentangles it into two separate groundplans containing static and dynamic features. The per-frame static groundplans are mean-pooled to obtain a single, time-independent static groundplan.\n\nCompositing groundplans. To render a scene using the disentangled static and dynamic groundplans, we first decode query points using both groundplans, yielding two sets of (density, color) values for each point. We use the compositing operation proposed by Yuan et al. (2021) to compose the contribution from static and dynamic components along the ray. Given the color and density for static (cS, σS) and dynamic (cD, σD) parts, the density of the combined scene is calculated as σS +σD. The color at the sampled point is computed as a weighted linear combination wScS +wDcD, where wS = (1 − exp(−δσS))/(1 − exp(−δ(σS + σD)), wD = (1 − exp(−δσD))/(1 − exp(−δ(σS + σD)), and δ is the distance between adjacent samples on the camera ray.\n\nLosses and Training. We train our model on multi-view video, where multi-view information is used to learn the 3D structure, while motion is used to disentangle the static and dynamic components in the scene. During training, we sample two time-steps per video. For each time-step, we sample multiple images from different camera views; some of the views are used as input to the method while others are used to compute the loss function. We use the input images to infer static and dynamic groundplans, and use them to render out per-frame query views. Our per-frame loss consists of an image reconstruction term, a hard surface constraint, and a sparsity term.\n\nL = ||R − I||2\n\n(cid:124)\n\n2 + λLPIPSLLPIPS(R, I) (cid:125)\n\n(cid:123)(cid:122) Limg\n\n− λsurface\n\n(cid:88)\n\nlog(P(wi))\n\n+ λsparse\n\n(cid:88)\n\n|σD i |\n\n.\n\n(1)\n\ni\n\n(cid:123)(cid:122) Lsurface\n\n(cid:125)\n\n(cid:124)\n\ni (cid:123)(cid:122) Ldyn_sparsity\n\n(cid:125)\n\n(cid:124)\n\n5\n\nPer-timestep Ground Plan InferenceDynamic Sparsity LossContextFramesStatic Ground PlanMLPPoolCompositeCompositing & RenderingStaticDynamicDynamic Ground PlansReconstruction LossesLossPublished as a conference paper at ICLR 2023\n\nCLEVR (1 view) PSNR↑ SSIM↑ LPIPS↓\n\nCoSY (1 view) PSNR↑ SSIM↑ LPIPS↓\n\nCoSY (5 views) PSNR↑ SSIM↑ LPIPS↓\n\nOurs PixelNeRF uORF\n\n34.5 33.98 29.35\n\n0.956 0.945 0.898\n\n0.15 0.200 0.151\n\n15.71 14.61 —\n\n0.43 0.34 —\n\n0.53 0.64 —\n\n18.29 17.31 —\n\n0.57 0.49 —\n\n0.43 0.50 —\n\nTable 1: Quantitative baseline comparison of novel view synthesis results. We outperform PixelNeRF (Yu et al., 2020) and uORF (Yu et al., 2022) in terms of PSNR, SSIM, and LPIPS on both CLEVR and CoSY datasets.\n\nLimg measures the difference between the rendered and ground truth images, R and I respectively, using a combination of l2 and patch-based LPIPS perceptual loss. Lsurface encourages both static and dynamic weight values (the weight for each sample in the rendering equation) wi for all samples along the rendered rays to be either 0 or 1, encouraging hard surfaces (Rebain et al., 2022). Here, P(wi) = exp(−|wi|) + exp(−|1 − wi|). The sparsity term Ldyn_sparsity takes as input densities decoded from the dynamic groundplan for all the rendered rays, and encourages the values to be sparse. This forces the model to explain most of the non-empty 3D structure as possible via the static groundplan and only expressing the moving objects using the dynamic groundplan, leading to reliable static-dynamic disentanglement. Without this loss, the model could explain the entire scene with just the dynamic component. The loss functions are weighed using the hyperparameters λLPIPS, λsurface, and λsparse. While we describe the loss functions for a single sample of ground-truth and rendered image, in practice, we construct mini-batches by randomly choosing multiple views of a scene at different time steps, and evaluate the loss function on each sample.\n\nUnsupervised object detection and extracting object-centric 3D representations. Our formulation yields a model that maps a single image to two radiance fields, parameterizing static and dynamic 3D regions respectively. Please see Fig. 1 for an example. We now perform a search for connected components in the dynamic neural groundplan to perform 3D instance-level segmentation, monocular 3D bounding box prediction, and the extraction of object-centric 3D representations. Specifically, given a dynamic groundplan, we first sample points in a 3D grid around the groundplan origin and decode their densities. We now perform conventional connected-component labeling in the groundplan space using accumulated density values, identifying the disconnected dynamic objects. We perform 2D instance-level segmentation for a queried viewpoint using volume rendering based on the densities expressed by the dynamic groundplan and assigning a color to the points corresponding to each of the identified objects, see Fig. 1 for an example. Furthermore, we compute the smallest box that contains the connected component to get a 3D bounding box for each identified object. Finally, we crop tiles of the dynamic groundplan that belong to a given object instance to obtain object-centric 3D representations, enabling editing of 3D scenes such as deletion, insertion, and rigid-body transformation of objects. This approach is not limited to a fixed number of objects during training or at test time. As we will show, this simple method is at par with the state of the art on self-supervised learning of object-centric 3D representations, uORF (Yu et al., 2022). Note that our approach is compatible with prior work leveraging slot attention (Locatello et al., 2020; Kipf et al., 2021) and other inference modules, which can be run on the disentangled dynamic groundplan which, in contrast to image space, enables shift-equivariant processing free from perspective distortion and encodes 3D structure. For implementation details, refer to Appendix A.\n\n5 RESULTS\n\nWe demonstrate that our method infers a 3D scene representation from a single image while disentangling static and dynamic components of the scene into static and dynamic groundplans respectively. We then show that connected components analysis suffices to leverage the densities in the dynamic groundplan for instance-level segmentation, bounding box prediction, and scene editing.\n\nDatasets. Our method is trained on multi-view observations of dynamic scenes. We present results on the moving CLEVR dataset (Yu et al., 2022), commonly used for self-supervised object discovery benchmarks (Yu et al., 2022), and the procedurally generated autonomous driving dataset CoSY (Bhandari, 2018). CoSY enables generation of a high-quality, path-traced dataset of multi-view videos with large camera baselines. We rendered multi-view observations of 9000 scenes with moving cars, sampled using 15 background city models and 95 car models. We train on 8000 scenes, and\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Qualitative comparisons. Comparison for novel-view synthesis given a single context view with PixelNeRF (Yu et al., 2020) and uORF (Yu et al., 2022).\n\nFigure 5: Single-image reconstruction, disentanglement of static and dynamic objects, and novel view synthesis. Given a single input image, our method can disentangle the observed scene into static and object components based on what the model observed as not-moving and moving in the training data. In these examples, the cars are isolated in the object component as the model was training on video data of cars moving on the road.\n\nevenly split the rest into validation and test sets. Further details about dataset generation are presented in the Appendix A.6. Datasets and code will be made publicly available.\n\nNovel View Synthesis and Scene Completion. We present novel views rendered from groundplans inferred from a single image from CLEVR and CoSY. For single-shot 3D reconstruction and novel view synthesis, we compare against PixelNeRF (Yu et al., 2020), a state-of-the-art single-image 3D reconstruction method, and uORF (Yu et al., 2022), state-of-the-art unsupervised object-centric 3D reconstruction method. We train PixelNeRF models on our datasets using publicly available code. We finetune the uORF model pretrained on CLEVR on our CLEVR renderings, and train it from scratch on CoSY using publicly available code. Fig. 4 provides a qualitative comparison to PixelNeRF and uORF in terms of single-image 3D novel view synthesis on both CoSY and CLEVR. Note that our model produces novel views with plausible completions of parts of the scene that are unobserved in the context image such as the back-side of objects. As expected from a non-generative method, regions that are entirely unconstrained such as occluded parts of the background (such as buildings) are blurry. While PixelNeRF succeeds in novel view synthesis on CLEVR, renderings on the complex CoSY dataset show significant artifacts, possibly caused by the linear sampling employed by PixelNeRF. uORF does not synthesize realistic images when trained on CoSY. Please refer to the supplemental webpage for results of uORF on the CoSY (Appendix B.5). On CLEVR, uORF generally produces high-quality renderings, but lacks high-frequency detail. In contrast to\n\n7\n\nGTInputpixelNeRFuORFOursCLEVRGTInputpixelNeRFOursCoSYInputReconstructionStaticObjectNovel viewsDepthPublished as a conference paper at ICLR 2023\n\nFigure 6: Object Localization. Given a single input image, we can use our inferred 3D representation of the scene to compute (a) bird’s-eye view rendering, (b) object localization in the bird’s-eye view space, (c) instance-level segmentation, and (d) 3D bounding box prediction.\n\nFigure 7: Qualitative Comparison for instance-level segmentation. Qualitative comparison of instance-level segmentation result with uORF (Yu et al., 2022).\n\nARI↑ NV-ARI↑\n\nOurs uORF\n\n0.84 0.83\n\n0.84 0.82\n\nTable 2: Quantitative segmentation accuracy evaluation on CLEVR using ARI and NVARI metrics.\n\nthese methods, our method reliably synthesizes novel views with high-frequency detail for both datasets. Quantitatively, we outperform both methods on novel-view synthesis in terms of PSNR, SSIM, and LPIPS metrics on both datasets (refer to Table 1). Note that qualitatively, the performance gap to baseline methods is significantly larger than quantitative results would suggest. This is due to the fact that much of the pixels used to compute PSNR are observing scene regions far outside the frustum of the input view. Here, all methods fail to reconstruct the true 3D appearance and geometry, as it is completely uncertain given the context view, resulting in low PSNR numbers for all methods (refer to Appendix B.1). As can be seen in the qualitative results, our method achieves significantly better reconstruction quality in parts of the 3D scene that lie in the frustum of the input camera, even if these areas are occluded in the input view. Our method further succeeds at fusing information across multiple context views, increasing the quality of the renderings with an increasing number of context views from varied viewpoints (refer to Appendix B.2).\n\nStatic-Dynamic Disentanglement. Given only a single image, our method computes separate static and dynamic groundplans that can be used to individually render the static and movable parts of the scene respectively. Fig. 5 shows results on single-image reconstruction of static and movable scene elements. Note that cars are reliably encoded by the dynamic groundplan, and our method inpaints regions occluded in the input view.\n\nInstance-level Segmentation and Bounding Box prediction. The separate reconstruction of movable scene components in the dynamic groundplan enables object detection via instance-level segmentation and bounding box prediction. Fig. 6 presents the instance-level segmentation and 3D bounding box prediction results of the proposed 3D object discovery via connected component discovery using the density inferred using the dynamic groundplan from the bird’s-eye view. Fig. 7 provides a qualitative comparison of object discovery with uORF on CLEVR dataset. While uORF succeeds at segmenting CLEVR scenes with fidelity comparable to ours, it fails to provide reconstruction and instance-level segmentation for our diverse and visually complex street-scale CoSY dataset.\n\n8\n\nInputReconstructionBirds-eye viewBirds-eye viewlocalizationInstance levelsegmentation3D Bounding-boxsegmentationGTInputOursuORFRGBMasksRGBMasksGTInputOursuORFPublished as a conference paper at ICLR 2023\n\nFigure 8: Object-centric representations and scene editing. The proposed static-dynamic neural groundplans enables object discovery using connected components labeling on the density of objects encoded in the dynamic groundplan (left). This enables straight-forward scene editing such as deletion, addition, or rigid-body transformation via directly editing the neural groundplan (right).\n\nOur method reliably segments separate car instances and predicts the 3D bounding boxes, including for cars that are only partially observed. Table 2 quantitatively compares the computed segmentation maps on CLEVR to uORF. We use the Adjusted Rand Index (ARI) metrics following uORF. We evaluate this metric in the input view (ARI), as well as in a novel view (NV-ARI). We perform at par with uORF on both of these metrics, demonstrating that our 3D ground plan representation reaches state of the art results with simple heuristics. Please refer to the supplemental webpage for video results (Appendix B.5). In addition, as mentioned before, we achieve higher-quality novel-view synthesis results, and also achieve significantly better results on the challenging CoSY dataset. Since uORF is based on slot attention, it can only attend to a finite number of objects, whereas groundplans can support any number of objects and require a single forward pass to render all objects.\n\nScene Editing. Instance-level segmentation, dynamic-static disentanglement, and 3D bounding boxes enable straight-forward 3D editing, such as translation, rotation, deletion, and insertion of individual objects in the scene. Objects can be rotated by arbitrary angles by simple bilinear interpolation of the groundplan features (refer to Appendix B.3). As the dynamic groundplan does not encode static scene regions such as the street below cars, cars can easily be moved from one scene to another. Fig. 8 provides scene editing results of our method. Note that such editing is difficult with methods that lack a persistent 3D representation, such as PixelNeRF.\n\n6 DISCUSSION\n\nLimitations and Future Work. Although our method achieves high-quality novel view synthesis from a single image, generated views are not photorealistic, and unobserved scene parts are blurry commensurate with the amount of uncertainty. Future work may explore plausible hallucinations of unobserved scene parts. Future work may further explore the use of more sophisticated downstream processing of the groundplan to enable, for instance, prior-based inference of object-centric representations (Locatello et al., 2020). Finally, we plan to investigate the combination of the proposed approach with flow-based dynamics reasoning, which would negate the need for multi-view video.\n\nConclusion. Our paper demonstrates self-supervised learning of 3D scene representations that are disentangled into movable and immovable scene elements. By leveraging multi-view video at training time, we can reconstruct disentangled 3D representations from a single image observation at test time. We show the potential of neural ground plans as a representation that may enable data-efficient solutions to downstream processing of the 3D scene, such as completion, instance-level segmentation, 3D bounding box prediction, and 3D scene editing. We hope that our paper will inspire future work on the use of self-supervised neural scene representations for general scene understanding tasks.\n\n9\n\nInputReconstructionIndividual ObjectsDeletionAdditionRearrangementPublished as a conference paper at ICLR 2023\n\n7 ACKNOWLEDGEMENTS AND DISCLOSURE OF FUNDING\n\nThis work is in part supported by DARPA under CW3031624 (Transfer, Augmentation and Automatic Learning with Less Labels) and the Machine Common Sense program, Singapore DSTA under DST00OECI20300823 (New Representations for Vision), NSF CCRI # 2120095, NSF RI #2211259 the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/), Stanford Institute for HumanCentered Artificial Intelligence (HAI), Stanford Center for Integrated Facility Engineering (CIFE), Qualcomm Innovation Fellowship (QIF), Samsung, Ford, Amazon, and Meta. The Toyota Research Institute also partially supported this work. This article solely reflects the opinions and conclusions of its authors and not other entity.\n\nREFERENCES\n\nZhipeng Bao, Pavel Tokmakov, Allan Jabri, Yu-Xiong Wang, Adrien Gaidon, and Martial Hebert.\n\nDiscovering objects that can move. In Proc. CVPR, pp. 11789–11798, 2022.\n\nJonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf\n\n360: Unbounded anti-aliased neural radiance fields. Proc. CVPR, 2021.\n\nDaniel Bear, Chaofei Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz, Li F Fei-Fei, Jiajun Wu, Josh Tenenbaum, et al. Learning physical graph representations from visual scenes. Proc. NeurIPS, 33, 2020.\n\nDeniz Beker, Hiroharu Kato, Mihai Adrian Morariu, Takahiro Ando, Toru Matsuoka, Wadim Kehl, and Adrien Gaidon. Monocular differentiable rendering for self-supervised 3d object detection. In Proc. ECCV, pp. 514–529. Springer, 2020.\n\nNishchal Bhandari. Procedural synthetic data for self-driving cars using 3D graphics. PhD thesis,\n\nMassachusetts Institute of Technology, 2018.\n\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.\n\nAnh-Quan Cao and Raoul de Charette. Monoscene: Monocular 3d semantic scene completion. In\n\nProc. CVPR, pp. 3991–4001, 2022.\n\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proc. CVPR, pp. 16123–16133, 2022.\n\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields.\n\narXiv preprint arXiv:2203.09517, 2022a.\n\nChang Chen, Fei Deng, and Sungjin Ahn. Roots: Object-centric representation and rendering of 3d\n\nscenes. JMLR, 22:259–1, 2021.\n\nShaoyu Chen, Tianheng Cheng, Xinggang Wang, Wenming Meng, Qian Zhang, and Wenyu Liu. Efficient and robust 2d-to-bev representation learning via geometry-guided kernel transformer. arXiv preprint arXiv:2206.04584, 2022b.\n\nRicson Cheng, Ziyan Wang, and Katerina Fragkiadaki. Geometry-aware recurrent neural networks\n\nfor active visual recognition. Proc. NeurIPS, 31, 2018.\n\nEric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional\n\nneural networks. In Proc. AAAI, 2019.\n\nTerrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and Joshua M. Susskind. Unconstrained scene generation with locally conditioned radiance fields. Proc. ICCV, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYilun Du, Shuang Li, Yash Sharma, B. Joshua Tenenbaum, and Igor Mordatch. Unsupervised learning\n\nof compositional energy concepts. In Proc. NeurIPS, 2021a.\n\nYilun Du, Kevin Smith, Tomer Ulman, Joshua B Tenenbaum, and Jiajun Wu. Unsupervised discovery\n\nof 3d physical objects from video. In Proc. ICLR, 2021b.\n\nEmilien Dupont, Miguel Bautista Martin, Alex Colburn, Aditya Sankar, Josh Susskind, and Qi Shan.\n\nEquivariant neural rendering. In Proc. ICML, pp. 2761–2770. PMLR, 2020.\n\nCathrin Elich, Martin R. Oswald, Marc Pollefeys, and Joerg Stueckler. Weakly supervised learning of multi-object 3d scene decompositions using deep shape priors. Computer Vision and Image Understanding, 220:103440, 2022. ISSN 1077-3142. doi: https://doi.org/10.1016/j.cviu. 2022.103440. URL https://www.sciencedirect.com/science/article/pii/ S1077314222000583.\n\nMartin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. In Proc. ICLR, 2020. URL https://openreview.net/forum?id=BkxfaTVFwH.\n\nSM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. Proc. NeurIPS, 29, 2016.\n\nSM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and rendering. Science, 360(6394):1204–1210, 2018.\n\nKlaus Greff, Antti Rasmus, Mathias Berglund, Tele Hao, Harri Valpola, and Jürgen Schmidhuber.\n\nTagger: Deep unsupervised perceptual grouping. Proc. NeurIPS, 29, 2016.\n\nKlaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. Neural expectation maximization.\n\nProc. NeurIPS, 30, 2017.\n\nKlaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning, 2019.\n\nMichelle Guo, Alireza Fathi, Jiajun Wu, and Thomas Funkhouser. Object-centric neural scene\n\nrendering. arXiv preprint arXiv:2012.08503, 2020.\n\nPengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht, Joshua M. Susskind, and Qi Shan. Fast and explicit neural view synthesis. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 3791–3800, January 2022.\n\nAdam W Harley, Zhaoyuan Fang, Jie Li, Rares Ambrus, and Katerina Fragkiadaki. A simple baseline\n\nfor bev perception without lidar. arXiv preprint arXiv:2206.07959, 2022.\n\nPhilipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, and David Novotny. Unsupervised learning of 3d object categories from videos in the wild. In Proc. CVPR, pp. 4700–4709, 2021.\n\nAnthony Hu, Zak Murez, Nikhil Mohan, Sofía Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and Alex Kendall. Fiery: Future instance prediction in bird’s-eye view from surround monocular cameras. In Proc. CVPR, pp. 15273–15282, 2021.\n\nZhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Rife: Real-time intermediate flow estimation for video frame interpolation. arXiv preprint arXiv:2011.06294, 2020.\n\nWonbong Jang and Lourdes Agapito. Codenerf: Disentangled neural radiance fields for object\n\ncategories. In Proc. ICCV, pp. 12949–12958, October 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJaebong Jeong, Janghun Jo, Sunghyun Cho, and Jaesik Park. 3d scene painting via semantic image\n\nsynthesis. In Proc. CVPR, pp. 2262–2272, June 2022.\n\nJindong Jiang, Sepehr Janghorbani, Gerard de Melo, and Sungjin Ahn. Scalable object-oriented\n\nsequential generative models. Unknown Journal, 2019.\n\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2901–2910, 2017.\n\nYoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video\n\nediting. Proc. TOG, 40(6):1–12, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nThomas Kipf, Gamaleldin F Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric learning from video. arXiv preprint arXiv:2111.12594, 2021.\n\nAdam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat:\n\nGenerative modelling of moving objects. Proc. NeurIPS, 31, 2018.\n\nAdam R Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soˇna Mokrá, and Danilo J Rezende. Nerf-vae: A geometry aware 3d scene generative model. Proc. ICML, 2021.\n\nShamit Lal, Mihir Prabhudesai, Ishita Mediratta, Adam W Harley, and Katerina Fragkiadaki. CoIn Proc. CVPR, pp. 12487–12496,\n\nconets: Continuous contrastive 3d scene representations. 2021.\n\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. In Proc. ICLR, 2020. URL https://openreview.net/forum?id= rkl03ySYDH.\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Proc. NeurIPS, 33:11525–11538, 2020.\n\nStephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. Proc. SIGGRAPH, 2019.\n\nKaustubh Mani, Swapnil Daga, Shubhika Garg, Sai Shankar Narasimhan, Madhava Krishna, and Krishna Murthy Jatavallabhula. Monolayout: Amodal scene layout from a single image. In Proc. CVPR, pp. 1689–1697, 2020.\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proc. ECCV, 2020.\n\nThomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton Kaplanyan, and Markus Steinberger. Donerf: Towards real-time rendering of compact neural radiance fields using depth oracle networks. In Computer Graphics Forum. Wiley Online Library, 2021.\n\nThu H Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, and Niloy Mitra. Blockgan: Learning 3d object-aware scene representations from unlabelled images. Proc. NeurIPS, 33: 6767–6778, 2020.\n\nYinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Jun Zhang. Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image. In Proc. CVPR, pp. 55–64, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMichael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative\n\nneural feature fields. In Proc. CVPR, pp. 11453–11464, 2021.\n\nMichael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric In Proc. CVPR, pp.\n\nrendering: Learning implicit 3d representations without 3d supervision. 3504–3515, 2020.\n\nJulian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for\n\ndynamic scenes. In Proc. CVPR, 2021.\n\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011.\n\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolu-\n\ntional occupancy networks. In Proc. ECCV, 2020.\n\nJonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by\n\nimplicitly unprojecting to 3d. In Proc. ECCV, pp. 194–210. Springer, 2020.\n\nDaniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry Lagun, and Andrea Tagliasacchi. Lolnerf:\n\nLearn from one look. In Proc. CVPR, pp. 1558–1567, 2022.\n\nLennart Reiher, Bastian Lampe, and Lutz Eckstein. A sim2real deep learning approach for the transformation of images from multiple vehicle-mounted cameras to a semantically segmented image in bird’s eye view. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pp. 1–7. IEEE, 2020.\n\nThomas Roddick, Alex Kendall, and Roberto Cipolla. Orthographic feature transform for monocular\n\n3d object detection. In BMVC, 2019.\n\nAvishkar Saha, Oscar Mendez, Chris Russell, and Richard Bowden. Translating images into maps. In Proc. ICRA, 2022. URL https://www.amazon.science/publications/ translating-images-into-maps.\n\nMehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. Proc. CVPR, 2021.\n\nVincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael\n\nZollhöfer. Deepvoxels: Learning persistent 3d feature embeddings. In Proc. CVPR, 2019a.\n\nVincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: ConIn Advances in Neural Information\n\ntinuous 3d-structure-aware neural scene representations. Processing Systems, pp. 1121–1132, 2019b.\n\nVincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Contin-\n\nuous 3d-structure-aware neural scene representations. In Proc. NeurIPS 2019, 2019c.\n\nVincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. In Proc. NeurIPS, 2021.\n\nCameron Smith, Hong-Xing Yu, Sergey Zakharov, Fredo Durand, Joshua B Tenenbaum, Jiajun Wu, and Vincent Sitzmann. Unsupervised discovery and composition of object light fields. arXiv preprint arXiv:2205.03923, 2022.\n\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural rendering. Proc. Eurographics STAR, 2021.\n\nAlex Trevithick and Bo Yang. Grf: Learning a general radiance field for 3d representation and rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 15182–15192, October 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nVadim Tschernezki, Diane Larlus, and Andrea Vedaldi. Neuraldiff: Segmenting 3d objects that move\n\nin egocentric videos. In Proc. 3DV, pp. 910–919. IEEE, 2021.\n\nHsiao-Yu Fish Tung, Ricson Cheng, and Katerina Fragkiadaki. Learning spatial common sense with\n\ngeometry-aware recurrent networks. In Proc. CVPR, pp. 2595–2603, 2019.\n\nBangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In Proc. ICCV, pp. 13779–13788, October 2021a.\n\nBangbang Yang, Yinda Zhang, Yijin Li, Zhaopeng Cui, Sean Fanello, Hujun Bao, and Guofeng Zhang. Neural rendering in a room: Amodal 3d understanding and free-viewpoint rendering for the closed scene composed of pre-captured objects. arXiv preprint arXiv:2205.02714, 2022.\n\nWeixiang Yang, Qi Li, Wenxi Liu, Yuanlong Yu, Yuexin Ma, Shengfeng He, and Jia Pan. Projecting your view attentively: Monocular road scene layout estimation via cross-view transformation. In Proc. CVPR, pp. 15536–15545, 2021b.\n\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Proc. NeurIPS, 2020.\n\nVickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, and Noah Snavely. Deformable sprites\n\nfor unsupervised video decomposition. In Proc. CVPR, pp. 2657–2666, 2022.\n\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from\n\none or few images. Proc. CVPR, 2020.\n\nHong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance fields.\n\nProc. ICLR, 2022.\n\nWentao Yuan, Zhaoyang Lv, Tanner Schmidt, and Steven Lovegrove. Star: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering. In Proc. CVPR, pp. 13144– 13152, 2021.\n\nSergey Zakharov, Wadim Kehl, Arjun Bhargava, and Adrien Gaidon. Autolabeling 3d objects with\n\ndifferentiable rendering of sdf shape priors. In Proc. CVPR, 2020.\n\nSergey Zakharov, Rares Andrei Ambrus, Vitor Campagnolo Guizilini, Dennis Park, Wadim Kehl, Fredo Durand, Joshua B Tenenbaum, Vincent Sitzmann, Jiajun Wu, and Adrien Gaidon. Single-shot scene reconstruction. In Proc. CORL, 2021.\n\nMaciej Zi ̨eba, Marcin Przewi ̨e ́zlikowski, Marek ́Smieja, Jacek Tabor, Tomasz Trzcinski, and Przemysław Spurek. Regflow: Probabilistic flow-based regression for future prediction. arXiv preprint arXiv:2011.14620, 2020.\n\nA IMPLEMENTATION DETAILS\n\nIn this section, we provide more training details of our method to enable reproducibility.\n\nA.1 NETWORK ARCHITECTURES\n\nCNN Encoder. Following PixelNeRF, we use the first 4 convolutional blocks of the ResNet34 architecture to create a feature grid. For a 128 × 128 resolution image, this results in a feature grid of 64 × 64 resolution with 512 features per point. We further use a shallow CNN to reduce the number of feature channels, and to aggregate information along the y-axis. This step helps us reduce the memory footprint for further processing in 3D. This feature aggregation CNN has two convolutional hidden layers with 256 hidden units and a stride of 2 along the height of the feature tensor. The output is a 16 × 64 resolution feature grid with 128 features per point. We use ReLU as the activation function for all layers in the encoder.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nUnprojection. We populate a coarse 3D feature volume of shape 64 × 16 × 64 with 128-dimensional latents for points in the camera frustum by bilinearly sampling the aggregated encoder feature tensor. This volume is processed as explained in the main paper: a coordinate encoding MLP transforms each feature into a new 128 dimensional feature vector. This MLP has two hidden layers with 128 hidden units each and a ReLU activation after each hidden layer.\n\nMulti-view input. In case of multi-view inputs, we average the unprojected latents for all 3D points in the canonical 3D coordinate system for all views. This results, like in the monocular case, in a 3D volume of 128 × 64 × 16 × 64 (CH × X × Y × Z).\n\nAggregation along the height. To construct an entangled groundplan, we aggregate the 3D feature volume into a 2D groundplan using a pillar aggregation MLP. Consider a pillar of features orthogonal to the groundplane for a particular point (x, z), in our case that is 128 × 16. The MLP takes in each latent with its corresponding 3D coordinate in the world coordinate system. It outputs the softmax scores for each of these latents which can be used to sum these latents along the pillar into a single 128-dimensional latent, as explained in the main paper.\n\nDisentanglement CNN. We use a shallow CNN with 4 hidden convolutional layers to disentangle the entangled groundplan of shape 128 × 64 × 64. The first two convolutional layers have 128 hidden units with kernel size of 3, stride of 1, and reflection padding of 1.\n\nThe last two convolutional layers comprise of 256 hidden units with the same configuration for other parameters. These are alsso followed by a 2× bilinear upsampling layers. This shallow CNN outputs groundplan of shape 256 × 256 × 256, in which the first 128 channels of the feature tensor are attributed as the static groundplan and the rest of the 128 channels are used as the dynamic groundplan for that timestep.\n\nProjections. Our method performs project of a 3D point on the image plane to get the corresponding feature tensor. This is performed using the intrinsic matrix of the camera. For a given 3D point in camera coordinates xc and intrinsic matrix K, the resulting 2D point on the image plane is computed as Kx. For projecting a 3D query point x on the groundplan, we use grid sample using (x, z)-coordinates of the 3D query points x.\n\nNeural renderer. Similar to PixelNeRF, we use a MLP as a renderer with 4 hidden layers that have 128 hidden units. Our renderer and the input latent are significantly smaller than the ones used in PixelNeRF, making our rendering cheaper.\n\nWe use two rendering MLPs, for coarse and fine sampling with 256 samples for the coarse MLP and 128 samples for the fine rendering along with 32 samples at the predicted depth based on the coarse renderings.\n\nInitialization. We use the kaiming normal initialization for initializing all the weights. All biases were initialized with uniform distribution in [−1e−3, 1e−3].\n\nA.2 HYPERPARAMETERS\n\nWe use Adam Kingma & Ba (2014) with a learning rate of 3e−4 to train our pipeline with the image reconstruction loss (L2), hard surfaces loss, and the alpha sparsity loss for 200 epochs. The losses were weighted by λimg = 1, λHSL = 0.1, and λsparse = 0.01. The model was then further finetuned by adding the LPIPS loss weighted by λlpips = 0.5. We found that using LPIPS loss from the beginning of the training process made the training unstable. We sampled 1e4 rays to compute the loss for each training sample in the input batch in the initial phase. During the first phase of training, the rays are sampled randomly and in the second phase when LPIPS loss is applied to the training, we sample rays to render image patches of 16 × 16. LPIPS loss using VGG is applied to these patches by normalizing the range of the output RGB values to be between [−1, 1]. The model was trained on a single 32G V100 GPU with a batch of 4 input samples with 2 timesteps for N views (N=5).\n\nA.3 CURRICULUM TRAINING\n\nOur model supports multi-view, as well as monocular reconstruction. We start our training only using multi-view reconstruction with 5 input views for the first 200 epochs, and then switch to a variable mode where the model is given a varying number of input views ranging between 1-5 views. This\n\n15\n\nPublished as a conference paper at ICLR 2023\n\ncurriculum approach helps the network to first learn the relevant scene priors, before learning to complete the 3D structure of the scene.\n\nA.4 HEURISTICS FOR LOCALIZATION\n\nFor localization of objects, we consider the dynamic groundplan and render it from an orthographic bird’s-eye view. Along with the bird’s-eye view RGB image, we also generate the occupancy map (per-pixel accumulated alpha) as shown in the manuscript in Fig. 5. Since the groundplans have a spatial extent of 256 × 256, the pphic occupancy map is rendered at the same resolution. The hard-surfaces loss encourages densities to be close to either 0 or 1. Thus, we threshold the density map with a threshold of 0.9. We find the regions of the map with connected components in the thresholded density map using label and regionsprop functions from the sklearn Pedregosa et al. (2011). To remove any remaining artifacts, we only keep the regions which have an area larger than 6 in the pixel space of the groundplan. This is a hyperparameter based on the size of the objects in the scene. Given the localization in the orthographic bird’s-eye view, we can find the height of the objects by computing the depth within each region. This information gives us a 3D bounding box around each of the localized object. The instance level segmentation is produced via volume rendering, by overriding the RGB values within the detected region to a chosen color for the object. These predictions can be made more accurate, for example, by (1) further tuning these hyperparameters, (2) sampling the orthographic density map at a higher resolution, (3) increasing the resolution of the groundplans at training time, and (4) training with more samples per camera ray.\n\nA.5 SCENE EDITING\n\nScene editing is performed by editing the dynamic groundplan. Once the different objects have been localized in the dynamic groundplan, we can edit the dynamic groundplan to carry out object deletion, insertion, and rearrangement. Localizing an object gives us its features in the spatial region of the groundplan used for rendering. To delete the object, we replace the features in this regions with features from the dynamic groundplan that encode zero density. For inserting an object at a given location, we find the corresponding (x, z) location in the dynamic groundplan and set the features at that location to the features corresponding to the object. Rearrangement of objects can be seen as a combination of deletion and insertion where we first delete the object from the existing location, followed by inserting the object at the new location in a possibly new orientation. To perform rotation of an object, we rotate the patch of features that correspond to the object to be rotated.\n\nA.6 DATASETS\n\nCLEVR. We generate scenes using the default configuration from (Johnson et al., 2017) (BSD License1), using the rubber material and default object sizes. We render the scene with 6 different cameras, all at the same fixed distance from the origin as the camera used in CLEVR, but with azimuth angles increasing at 60 degree increments. Objects in the CLEVR dataset are captured at 2 timesteps, and are simulated to move a distance of 0.25 to 0.75 meters between the two timesteps. Images were rendered across 6 different cameras, with resolution of 128 × 128 using CYCLES renderer with 512 samples per pixel. Our dataset consists of 1500 samples, divided into 1000 train and 500 test samples.\n\nCoSY. We develop this dataset using the city generation code provided by Bhandari (2018). We generate 15 different configurations of the city using CityEngine, with variations in building shapes, heights, and materials. This city layout is further processed in Blender using Python to add cameras, trees, bus stops, and moving cars. Cameras are sampled on hemispheres of radii in range [4-6m] a maximum height of 4m viewing the center of the circular base of the hemisphere located at randomly chosen points. We sample 15 cameras for each of the randomly chosen centers, with the varying radius of the hemisphere for each camera. The field-of-view for all sampled cameras is 60 degrees, with a symmetric sensor size of 32mm, resulting in a focal length of 110.85 in pixel space. Note that we never sample any bird’s-eye view as the maximum height of the camera is clipped at 4m. We choose 50-100 cars to be spawned in different locations of the generated city. We use a fixed environment map with diffused white light. In addition to the rendering capabilities of CoSY, we add\n\n1https://github.com/facebookresearch/clevr-dataset-gen/blob/main/LICENSE\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: Visual quality of views for a given input image.\n\nthe ability to move cars over timesteps. Given the location and direction of the movement (direction where the car is facing), we change the location of the car over 10 timesteps to a sampled total translation from range 2-4 m. For each sampled camera, we render the 10 frames with a resolution of 128 × 128 using CYCLES renderer with 512 samples per pixel. Our dataset has 9000 such samples, which are further divided into 8000 train, 500 validation, and 400 test samples. Thus each sample has images, poses, focal length, and principal point for 15 cameras. The dataset will be publicly released for further research in this direction.\n\nB ADDITIONAL RESULTS\n\nB.1 VISUALIZATION OF VIEWS OUTSIDE THE OBSERVED VIEW\n\nFig. 9 presents the output renderings for various target camera viewpoints for the given input image. We observe that as the target view shifts away from the context view, our method renders the geometry with appropriate texture of the car in the view, but outputs a blurry background as expected from a non-generative model.\n\nB.2 UNCERTAINTY WITH RESPECT TO THE NUMBER OF INPUT VIEWS\n\nFig. 10 provides results of street-scale scenes reconstructed using an increasing number of input images from different camera viewpoints. Our method successfully integrates information across observations into a single, multi-view consistent representation. The background reconstructions improve with more input views, as a result of less uncertainty for the occluded regions.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Resolving uncertainty with increasing number of context views Our model can take a variable number of viewpoints of a scene as input. We find that the model gets a good prior on the object from a single view but struggles to reconstruct high-quality details in the unobserved regions of the background. This uncertainty gets resolved with more input views.\n\n18\n\nGroud truthInput context views12345Reconstructions11,21,2,31,2,3,41,2,3,4,5Groud truthInput context views12345Reconstructions11,21,2,31,2,3,41,2,3,4,5Published as a conference paper at ICLR 2023\n\nFigure 11: Scene editing with rotated objects. We present images of scenes with edited groundplans where cars are rotated at various angles in the ground plane.\n\nApart from effect of uncertainty in the unobserved regions on the visual quality, there are two factors that contribute to blurriness in the results. First, our method is in the regime of prior-based reconstruction from a few images. This is in contrast to the regime of single-scene overfitting (e.g. NeRF) in which prior work has demonstrated photo-realistic results. In our task setup of prior based reconstruction, a certain degree of uncertainty exists. For instance, there is uncertainty about the exact depth and geometry of the 3D scene given the input images. In these cases, the model will learn to blur proportionate to the amount of uncertainty. We note that this is not a limitation of our method specifically - all prior-based 3D reconstruction methods share this property. We outperform pixelNeRF, a strong baseline in this regime of 3D reconstruction from few images, both quantitatively and qualitatively. Second, our rendering quality is limited by computational cost. In contrast to single-scene methods, our method needs to fit not only the differentiable rendering in GPU memory, but also the whole inference pipeline - CNNs, 3D lifting, groundplans, etc. This limits the resolution of the groundplans (a car is expressed by ∼ 6 latents) as well as the number of volume rendering samples. Increasing computational budget would lead to better renderings.\n\nB.3 ROTATING OBJECTS\n\nFig. 11 provides rendered images from edited groundplans where the cars are rotated at different angles. Note that the representation only allows for rotations in the xz-plane.\n\nB.4 MOTION INTERPOLATION\n\nThe dynamic groundplan can be further used to perform motion interpolation in 3D, using optical flow for groundplan interpolation prediction using off-the-shelf, state-of-the-art optical flow and frame interpolation methods. To demonstrate the efficacy of the groundplans for frame interpolation, we trained RIFE Huang et al. (2020) on our model trained on the GQN-rooms Eslami et al. (2018) dataset. We generated simple linear motion trajectories for objects over 10 frames. We added tall static cylinders as pillars in the room which generate occlusions. The scene was rendered from 15 different camera views. We first trained our model on the GQN dataset and used the output dynamic groundplans to train the frame interpolation method. The training was done in 2 steps. Firstly, we extracted dynamic groundplan for the samples over different timesteps by passing the multi-view observation through our groundplan generation pipeline. Two dynamic groundplans at different timesteps were given as input to RIFE, and an intermediate timestep was queried. An L2 loss on the output dynamic plan against the output of our pipeline for that timestep was sufficient to obtain a good initialization. In the training stage, we combined the RIFE model with our method for higher-quality results. All losses discussed in the main paper were applied on the rendered novel views generated using the output dynamic floorplans on the intermediate timesteps. In Fig. 12, we show the rendered output of our model for the intermediate timesteps given the leftmost and rightmost frames as input. The proposed method succeeds at inferring the correct object motion, and enables novel view synthesis through space and time.\n\nB.5 SUPPLEMENTAL WEBPAGE\n\nWe strongly encourage the readers to refer to our supplemental webpage for more novel-view synthesis, static-dynamic disentanglement, localization, and scene editing results, as well as video comparisons with the state of the art.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 12: Temporal 3D scene interpolation for novel view synthesis in space and time.\n\n20\n\nInput t=0Input t=1Predicted intermediate frames",
    "reference": "# Summary Of The Paper\n\nAn approach is presented in this paper for mapping 2D image observations to persistent 3D scene representations in order to create a novel view synthesis and disentangle the movable and immovable components of the scene. A ground-aligned 2D feature grid based on conditional neural groundplans can be employed as a memory-efficient representation of scene data. This notion is inspired by the bird's-eye-view (BEV) representation commonly used in vision and robotics. Self-supervised learning is conducted using differentiable rendering from unlabeled multi-view observations, and the method learns to complete the geometry and appearance of occluded regions.\n\n# Strength And Weaknesses\n\nStrength：\n1. The conditional neural groundplans proposed are intriguing. This allows efficient processing of scene appearance and geometry directly in 3D, through the self-supervised learning of conditional neural groundplans, a hybrid discrete continuous representation of 3D neural scenes.\n2. The proposed disentanglement of static background and movable foreground objects is novel. This approach uses object motion for training an encoder-based system that can reconstruct scenes from a single image instead of decomposing video frames based on scene specifics.\n3. The proposed method is promising for understanding 3D from a single image. The dynamic groundplan encodes 3D geometry, allowing us to segment and animate objects in 3D using single images and 3D bounding boxes.\n\nWeaknesses：\n1. The biggest weakness is that only results on simple synthetic data are presented. Currently, there are a number of well-annotated 3D driving scenes available. An example is KITTI 360 (https://www.cvlibs.net/datasets/kitti-360/). Could the authors explain why results on the real datasets are not presented? Is it because of some specific annotation (such as BEV correspondence or the number of views) needed by the method? Otherwise, the generalizability on natural scenes and the practical value of the method are questioned.\n2. I think there are some ambiguity behind the proposed Static-Dynamic Disentanglement. A definition of Static and Dynamic should be made clear in the proposed framework. A car remaining still in the sequence will be considered as Static. Considering a situation when two moving and static cars looks similar or exactly the same, how can we distinguish from a single image if a car is static or dynamic? The method should mention the above cases as an assumption, or making a clear definition of being Static and Dynamic.\n3. Groundplans implicitly assume that objects are aligned on a plane. However, in real life, it is common that scenes/objects are not placed on a flat ground plane (e.g., a curved plane). The ability of persistent 3D scene representation is a little overclaimed, if we consider the implicit assumption behind groundplans.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-organized and clearly written. Ideas are novel. Results in the paper are easily reproducible.\n\n# Summary Of The Review\n\nThe method is good and the idea is interesting. But only synthetic data is used for evaluation, and the access of real data should not be a problem.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSU RCO: LEARNING LINEAR SURROGATES FOR COMBINATORIAL NONLINEAR OPTIMIZATION PROBLEMS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOptimization problems with expensive nonlinear cost functions and combinatorial constraints appear in many real-world applications, but remain challenging to solve efficiently. Existing combinatorial solvers like Mixed Integer Linear Programming can be fast in practice but cannot readily optimize nonlinear cost functions, while general nonlinear optimizers like gradient descent often do not handle complex combinatorial structures, may require many queries of the cost function, and are prone to local optima. To bridge this gap, we propose SurCo that learns linear Surrogate costs which can be used by existing Combinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We learn these linear surrogates end-to-end with the nonlinear loss by differentiating through the linear surrogate solver. Three variants of SurCo are proposed: SurCo-zero operates on individual nonlinear problems, SurCo-prior trains a linear surrogate predictor on distributions of problems, and SurCo-hybrid uses a model trained offline to warm start online solving for SurCo-zero. We analyze our method theoretically and empirically, showing smooth convergence and improved performance. Experiments show that compared to state-of-the-art approaches and expert-designed heuristics, SurCo obtains lower cost solutions with comparable or faster solve time for two realworld industry-level applications: embedding table sharding and inverse photonic design.\n\n1\n\nINTRODUCTION\n\nCombinatorial optimization problems with linear objective functions, like linear programming (LP) (Chvatal et al., 1983) and mixed integer linear programming (MILP) (Wolsey, 2007), have been extensively studied in operations research (OR). The resulting high-performance solvers like Gurobi (Gurobi Optimization, LLC, 2022) can solve industrial-scale optimization problems with ten of thousands of variables in a few minutes.\n\nHowever, even with perfect solvers, one issue remains: the cost functions f (x) in many practical problems are nonlinear, and the highly-optimized solvers mainly handle linear or convex formulations while real-world problems have less constrained objectives. For example, in embedding table sharding (Zha et al., 2022a) one needs to distribute embedding tables to multiple GPUs for the deployment of recommendation systems. Due to the batching behaviors within a single GPU and communication cost among different GPUs, the overall latency (cost function) in this application depends on interactions of multiple tables and thus can be highly nonlinear (Zha et al., 2022a).\n\nTo obtain useful solutions to the real-world problems, one may choose to directly optimize the nonlinear cost, which is either a black-box output of a simulator (Gosavi et al., 2015; Ye et al., 2019), or a cost estimator learned by machine learning techniques (e.g., deep models) from offline data (Steiner et al., 2021; Koziel et al., 2021; Wang et al., 2021b; Cozad et al., 2014). However, many of these direct optimization approaches either rely on human-defined heuristics (e.g., greedy (Korte & Hausmann, 1978; Reingold & Tarjan, 1981; Wolsey, 1982), local improvement (Voß et al., 2012; Li et al., 2021)), or resort to general nonlinear optimization techniques like gradient descent (Ruder, 2016), reinforcement learning (Mazyavkina et al., 2021), or evolutionary algorithms (Simon, 2013). While these approaches can work in practice, they may lead to a slow optimization process, in\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nparticular when the cost function is expensive to evaluate, and they often ignore the combinatorial nature of most real-world applications (encoded in the feasible set x ∈ Ω).\n\nIn this work, we propose a systematic framework SurCo that leverages existing efficient combinatorial solvers to find solutions to nonlinear combinatorial optimization problems arising in realworld scenarios. There are three versions of SurCo, SurCo-zero, SurCo-prior, and SurCohybrid. In SurCo-zero, given a nonlinear differentiable cost f (x) to be minimized, we optimize a linear surrogate cost ˆc so that the surrogate optimizer (SO) minx∈Ω ˆc⊤x outputs a solution that is expected to be optimal w.r.t. the original nonlinear cost f (x). Due to its linear nature, SO can be solved efficiently with existing solvers, and the surrogate cost ˆc can be optimized in an endto-end manner by back-propagating through the solver (Poganˇci ́c et al., 2019; Niepert et al., 2021; In SurCo-prior, we consider a family of nonlinear differentiable funcBerthet et al., 2020). tions f (x; y), where y parameterizes problem descriptions. We train the linear surrogate ˆc(y) on a set of optimization problems (called the training set {yi}), and evaluate on a held-out problem y′, by directly optimizing SO: x∗(y′) := arg minx∈Ω(y) ˆc⊤(y′)x, which avoids optimizing the cost f (x; y′) from scratch. Finally, in SurCo-hybrid we use initial surrogate costs predicted by a fully-trained SurCo-prior and then fine-tune the surrogate costs further using SurCo-zero to leverage both domain knowledge synthesized offline and information about the specific instance.\n\nAll versions of SurCo are evaluated in two real-world nonlinear optimization problems: embedding table sharding (Zha et al., 2022a), and photonic inverse design (Schubert et al., 2022). In both cases, we show that in the on-the-fly setting, SurCo achieves higher quality solutions in comparable or less runtime, faster optimization in wall-clock time with lower solution cost, thanks to the help of an efficient combinatorial solver; in prior, our method obtains better solutions in held-out problems compared to other methods that require training (e.g., reinforcement learning). More specifically, in table sharding SurCo-zero obtains between 14% to 85% improvement in solution quality with between 2% and 23% increase in runtime overhead compared to the greedy baseline, SurCo-prior obtains between 47% and 71% solution quality improvement against the state of the art RL-based table sharding algorithm Zha et al. (2022b). SurCo-hybrid obtains better solutions than either SurCo-zero or SurCo-prior, with a similar runtime overhead as SurCo-zero. In photonic inverse design, SurCo-zero finds 21% more viable solutions for the beam splitter and twice as many solutions for the wavelength demultiplexers with all problems solving successfully for the mode converter and bend problems, taking between 10% to 64% less time than the pass-through approach from previous work (Schubert et al., 2022). While the offline trained SurCo-prior misses some optimal solutions in the different settings, it frequently obtains solutions in 0.5% to 2% of the runtime due to not needing to evaluate the objective and perform gradient steps. Again, SurCo-hybrid is able to obtain solutions more often than the other approaches, with a runtime overhead comparable to SurCo-zero. We additionally present theoretical results that help motivate why training a model to predict surrogate linear coefficients exhibits better sample complexity than directly predicting the optimal solution (Li et al., 2018; Ban & Rudin, 2019).\n\n2 PROBLEM SPECIFICATION\n\nOur goal is to solve the following nonlinear optimization problem describe by y:\n\nmin x\n\nf (x; y)\n\ns.t. x ∈ Ω(y)\n\n(1)\n\nwhere x ∈ Rn are the variables to be optimized, f (x; y) is the nonlinear differentiable cost function to be minimized, Ω(y) is the feasible region, typically specified by linear (in)equalities and integer constraints, and y ∈ Y are the problem instance parameters drawn from a distribution D over Y . For example, in the traveling salesman problem, y can be the distance matrix among cities. We often consider solving a family of optimization problems, described as y ∈ Y .\n\nDifferentiable cost function. The nonlinear cost function f (x; y) can either be the result of a simulator made differentiable via finite differencing (e.g., JAX (Bradbury et al., 2018)), or a cost model that is learned from an offline dataset, often generated via sampling multiple feasible solutions within Ω(y), and recording their costs. The cost model often takes the form of a deep neural network. In this work, we assume the following property of f (x; y): Assumption 2.1 (Cost function). During optimization, the cost function f (x; y) and its partial derivative ∂f /∂x are accessible.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nLearning a good nonlinear cost model f is highly non-trivial for practical applications (e.g., AlphaFold (Jumper et al., 2021), Density Functional Theory (Nagai et al., 2020), cost model for embedding tables (Zha et al., 2022a)) and is beyond the scope of this work.\n\nEvaluation Metric. In real-world applications, querying f can be slow and expensive, and thus a lower number of queries while getting better quality solution is the goal. We mainly focus on two aspects: how good the solution ˆx is, by checking the value of f ( ˆx; y), and how many queries of the nonlinear function f are needed during optimization in order to achieve the solution ˆx.\n\nLinear/nonlinear cost function. When f (x; y) is linear w.r.t x, and the feasible region can be encoded using mixed integer programs or other mathematical programs, the problem can be solved efficiently using existing scalable optimization solvers. When f (x; y) is nonlinear, we propose SurCo that learns a surrogate linear objective function, which allow us to leverage these existing scalable optimization solvers, and which results in a solution that has high quality with respect to the original hard-to-encode objective function f (x; y). We will elaborate in the following sections.\n\nFigure 1: Overview of our proposed algorithm SurCo.\n\n3 SU RCO: LEARNING LINEAR SURROGATES\n\n3.1 SU RCO- Z E R O: ON-THE-FLY OPTIMIZATION\n\nWe start from the simplest case in which we focus on a single instance with f (x) = f (x; y) and Ω = Ω(y). SurCo-zero aims to optimize the following objective:\n\n(SurCo-zero) : min\n\nc\n\nLzero(c) := f (gΩ(c))\n\n(2)\n\nwhere the surrogate optimizer gΩ : Rn (cid:55)→ Rn is the output of certain combinatorial solvers with linear cost weight c ∈ Rn and feasible region Ω ⊆ Rn. For example, gΩ can be the following (n is the number of variables to be optimized):\n\ngΩ(c) := arg min\n\nx\n\nc⊤x s.t. x ∈ Ω := {Ax ≤ b, x ∈ Zn}\n\n(3)\n\nwhich is the output of a MILP solver. Thanks to previous works (Ferber et al., 2020; Poganˇci ́c et al., 2019), we can efficiently compute the partial derivative ∂gΩ(c)/∂c. Intuitively, this means that gΩ(c) can be backpropagated through.\n\nSince f is also differentiable with respect to the solution it is evaluating, we thus can optimize Eqn. 2 in an end-to-end manner using any gradient-based optimizer. That is, c(t + 1) = c(t) − α ∂gΩ ∂x , where α is the learning rate. The procedure starts from a randomly initialized c(0) and converges at a local optimal solution of c.\n\n∂c\n\n∂f\n\nWhile Eqn. 2 is still nonlinear optimization and there is no guarantee about the quality of the final solution c, we argue that optimizing Eqn. 2 is better than optimizing the original nonlinear cost minx∈Ω f (x). Furthermore, while we cannot guarantee optimality, we are able to guarantee feasibility by leveraging a linear combinatorial solver. We note that SurCo is somewhat limited to problems without interior integer solutions, since the linear surrogate cannot yield interior points. However, many real-world settings, such as our two domains, consider making binary decisions which lack interior integer points. Intuitively, instead of optimizing directly over the solution space\n\n3\n\nminxf(x;y)s.tx∈Ω=Nonlinear optimizationwithcombinatorial constraintsPredict surrogate cost c=c(y)x∗y=argminxc(y)Txs.tx∈Ωx∗yoptimizes f(x;y)as much as possibleSurrogate optimization combinatorial constraintssolved by existing combinatorial solversUnder review as a conference paper at ICLR 2023\n\nx, we optimize over the space of surrogate costs c, and delegate the combinatorial feasibility requirements of the nonlinear problem to SoTA combinatorial solvers. Compared to naive approaches that directly optimize f (x) via general optimization techniques, our method readily handles complex constraints of the feasible regions, and thus makes the optimization procedure easier. Furthermore, it also helps escape from local minima, thanks to the embedded search component of existing combinatorial solvers (e.g., branch-and-bound (Land & Doig, 2010) in MILP solvers). As we see in the experiments, this is particularly important when the problem becomes large-scale with more local optima. This approach works well when we are optimizing individual instances and may not have access to offline training data or the training time is cost-prohibitive.\n\n3.2 SU RCO- P R I O R: OFFLINE SURROGATE TRAINING\n\nWe now discuss more general cases, where the nonlinear loss function f (x; y) represents a family of cost function to be optimized. Here the description of each problem instance y is drawn from a fixed problem distribution D. We then ask the following question: how can we find solutions to a batch of training instances Dtrain := {yi}N i=1, gain useful knowledge of the cost functions, and leverage such knowledge in held-out evaluation problem instances Deval to accelerate the optimization procedure?\n\nFollowing standard machine learning practice, let us first consider a naive two-stage approach. In the data collection stage, we simply apply SurCo-zero(Eqn. 2) to every yi separately to get N surrogate cost vectors ci. Then in the training stage, we train a regressor ˆc = ˆc(y; θ) on the dataset {(yi, ci)} to learn to predict the surrogate costs from the problem features. Here ˆc is a parameterized model (e.g., a deep network) with the parameters θ to be learned. This learned regressor ˆc(y; θ) can thus be used for a held-out problem instance y′ to directly predict c′ = ˆc(y′; θ) and get the solution x′ = gΩ(y′)(c′) via surrogate optimizer (SO).\n\nWhile this approach is simple, the N optimization procedures in the data collection stage are independent of each other, and can lead to excessive number of calls to f that are not helpful. E.g., if an optimization procedure converges to a bad local solution, then even if it achieves perfect convergence, which requires a lot of function calls, the resulting data point is still of low quality.\n\nThis motivates us to add a regularizer for the optimization:\n\n(SurCo-prior-λ) : min θ,{ci}\n\nLprior(θ, {ci}; λ) :=\n\nN (cid:88)\n\ni=1\n\nf (gΩ(yi)(ci); yi)+λ∥ci−ˆc(yi; θ))∥2 (4)\n\nNote that when λ = 0, it reduces to N independent optimizations, while when λ > 0, the surrogate costs {ci} interact with each other. The intuition is that, the regressor ˆc(y; θ), even if not trained fully, can be very useful to guide ci rather than just using its randomly initialized version. Furthermore, if ˆc is a mapping to global optimal solution of x, then it will pull the solutions out of local optima to re-target towards global ones, even when starting from poor initialization, yielding fast convergence and better final solutions for individual optimization instances.\n\nA special case is when λ → +∞, we arrive at a novel objective that jointly learns the surrogate cost function, given the training set Dtrain:\n\n(SurCo-prior) : min\n\nθ\n\nLprior(θ) :=\n\nN (cid:88)\n\ni=1\n\nf (gΩ(yi)(ˆc(yi; θ)); yi)\n\n(5)\n\nThis approach is useful when the goal is to find high-quality solutions for unseen instances of a problem distribution when the upfront cost of offline training is acceptable but the cost of optimizing on-the-fly is prohibitive. Here, we require access to a distribution of training optimization problems, but at test time only require the feasible region and not the nonlinear objective.\n\n3.3 SU RCO- H Y B R I D: FINE-TUNING A PREDICTED SURROGATE\n\nNaturally, we consider SurCo-hybrid, a hybrid approach which initializes the coefficients of SurCo-zero with the coefficients predicted from SurCo-prior which was trained on offline data. This allows SurCo-hybrid to start out optimization from an initial prediction which has good performance for the distribution at large but which is then fine-tuned for the specific instance.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFormally, we initialize c(0) = ˆc(yi; θ) and then continue optimizing c based on the update from SurCo-zero. This approach is geared towards optimizing the nonlinear objective using a highquality initial prediction that is based on the problem distribution and then fine-tuning the objective coefficients based on the specific problem instance at test time. Here, high performance comes at the runtime cost of both having to train offline on a problem distribution as well as performing fine-tuning steps on-the-fly. However, this additional cost is often worthwhile when the main goal is to find the best possible solutions by leveraging synthesized domain knowledge in combination with individual problem instances as arises in chip design (Mirhoseini et al., 2021) and compiler optimization (Zhou et al., 2020).\n\n3.4 COST REGRESSION VERSUS SOLUTION REGRESSION: A THEORETICAL ANALYSIS\n\nWe also want to compare SurCo with the previous works on ML optimizers (Ban & Rudin, 2019) that try to directly learn the mapping from problem description y to the solution, i.e. solution regression. Given a set of training instances Dtrain from distribution D, these approaches first collect a set of training samples Ddirect := {y, x∗(y) : y ∈ Dtrain}, and then learn a function ̃x∗(y) to fit the training samples.\n\nWhile this is conceptually simple, there exist fundamental difficulties to learn such a direct mapping. First, as mentioned above, it can be quite expensive to obtain the optimal solution x∗(y) due to the nature of nonlinear optimization and the query cost. Second, even if a perfect dataset Ddirect is accessible, the number of samples needed to learn a mapping to directly predict x∗(y) is related to the Lipschitz constant L of the mapping, and for a direct mapping, L can be very large.\n\n3.4.1 LIPSCHITZ CONSTANT AND SAMPLE COMPLEXITY\n\nLet us first consider the sample complexity of solution regression methods as described above. Formally, consider fitting any function φ : Rd ⊇ Y (cid:55)→ Rm with a dataset {yi, φi}. Here Y is a compact region with finite volume vol(Y ). The Lipschitz constant L is the smallest number so that ∥φ(y1) − φ(y2)∥2 ≤ L∥y1 − y2∥2 holds for any y1, y2 ∈ Y . The following theorem shows that if the dataset covers the space Y , we could achieve high accuracy prediction: ∥φ(y) − ˆφ(y)∥2 ≤ ε for any y ∈ Y . Definition 3.1 (δ-cover). A dataset Ddirect := {(yi, φi)}N there exists at least one yi so that ∥y − yi∥2 ≤ δ.\n\ni=1 δ-covers the space Y , if for any y ∈ Y ,\n\nLemma 3.1 (Sufficient condition of prediction with ε-accuracy). If the dataset Ddirect (ε/L)-cover Y , then for any y ∈ Y , a 1-nearest-neighbor regressor ˆφ leads to ∥ ˆφ(y) − φ(y)∥2 ≤ ε.\n\nLemma 3.2 (Lower bound of sample complexity for ε/L-cover). To achieve ε/L-cover of Y , the size of the training set N ≥ N0(ε) := vol(Y )\n\n, where vol0 is the volume of unit ball in d-dimension.\n\n(cid:1)d\n\n(cid:0) L ε\n\nvol0\n\nPlease find all proofs in the Appendix. While we do not rule out a more advanced regressor than 1nearest-neighbor that leads to better sample complexity, the lemmas demonstrate that the Lipschitz constant L plays an important role in sample complexity.\n\n3.4.2 DIFFERENCE BETWEEN COST AND SOLUTION REGRESSION\n\nIn the following we will show that in certain cases, the direct prediction y (cid:55)→ x∗(y) could have an infinitely large Lipschitz constant L. To show this, let us consider a general mapping φ : Rd ⊇ Y (cid:55)→ Rm. Let φ(Y ) be the image of Y under mapping φ and κ(Y ) be the number of connected components for region Y . Theorem 3.1 (A case of infinite Lipschitz constant). If the minimal distance dmin for different connected components of φ(Y ) is strictly positive, and κ(φ(Y )) > κ(Y ), then the Lipschitz constant of the mapping φ is infinite.\n\nNote that this theorem applies to a wide variety of combinatorial optimization problems. For example, when Y is a connected region and the optimization problem can be formulated as an integer program, the optimal solution set x∗(Y ) := {x∗(y) : y ∈ Y } is a discrete set of integral vertices,\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nso the theorem applies. Combined with analysis in Sec. 3.4.1, we know the mapping y (cid:55)→ x∗(y) is hard to learn even with a lot of samples.\n\nOn the other hand, the mapping y (cid:55)→ c(y) can avoid too many connected components in its image c(Y ), by connecting disjoint components of x∗(Y ) together.\n\n4 EMPIRICAL EVALUATION\n\nWe evaluate the two variants of SurCo on two real-world settings, embedding table sharding and inverse photonic design. Both have industrial application. Each setting consists of a family of problem instances with varying feasible region and nonlinear objective function.\n\n4.1 EMBEDDING TABLE SHARDING\n\n(a)\n\n(b)\n\nFigure 2: Table placement plan latency (a) and solver runtime (b). We evaluate SurCo against Dreamshard (Zha et al., 2022b) a SoTA offline RL sharding tool, a domain-heuristic of assigning tables based on dimension, and a greedy heuristic based on the estimated runtime increase. Striped approaches require pre-training.\n\nThe task of sharding embedding tables arises in the deployment of large scale neural network models which operate over both sparse and dense inputs (e.g., in recommendation systems (Zha et al., 2022a;b; Sethi et al., 2022)). Given T embedding tables and D homogeneous devices, the goal is to distribute the tables among the devices such that no device’s memory limit is exceeded, while the tables are processed efficiently. Formally, let xt,d be the binary variable indicating whether table t is assigned to device d, and x := {xt,d} ∈ {0, 1}T D be the collection of the variables. The optimization problem is:\n\nmin x\n\nf (x; y)\n\ns.t. x ∈ Ω(y) :=\n\nx :\n\n∀t,\n\n(cid:40)\n\n(cid:88)\n\nt\n\nxt,d = 1,\n\n∀d,\n\n(cid:41)\n\nmtxt,d ≤ M\n\n(6)\n\n(cid:88)\n\nt\n\nd xt,d = 1 means each table t should be assigned to exactly one device, and (cid:80)\n\nHere the problem description y includes table memory usage {mt}, and capacity M of each device. (cid:80) t mtxt,d ≤ M means the memory consumption at each device d should not exceed its capacity. The nonlinear cost function f (x; y) is the latency, i.e., the runtime of the longest-running device. Due to shared computation (e.g., batching) among the group of assigned tables, and communication costs across devices, the objective is highly nonlinear. f (x; y) is well-approximated by a sharding plan runtime estimator proposed by Dreamshard (Zha et al., 2022b).\n\nSurCo learns to predict T × D surrogate cost ˆct,d, one for each potential table-device assignment. During training, the gradients through combinatorial solver ∂g/∂c are computed via CVXPYLayers (Agrawal et al., 2019a) and the integrality constraints are relaxed. We found that in practice, we obtained solutions that were mostly integral in that only one table on any given device was fractional. At test time we solve for the integer solution using SCIP (Achterberg, 2009).\n\nSettings. We evaluate SurCo on the publicly available Deep Learning Recommendation Model (DLRM) dataset (Naumov et al., 2019). We consider 6 settings: 10, 20, 30, 40, 50, and 60 tables are placed to 4 devices with each GPU device having a 5GB memory limit. Each setting has 100 problem instances (50 training and 50 test).\n\n6\n\nDLRM-10DLRM-20DLRM-30DLRM-40DLRM-50DLRM-60Setting01020304050Solution Loss (Latency)Table Sharding Solution Loss (Latency)Domain HeuristicGreedySurCo-zeroDreamShardSurCo-priorSurCo-hybridDLRM-10DLRM-20DLRM-30DLRM-40DLRM-50DLRM-60Setting0.00.51.01.52.02.5Deployment Runtime (s)Table Sharding Deployment Runtime (s)Under review as a conference paper at ICLR 2023\n\nBaselines. For SurCo-zero baselines, we use Greedy that greedily allocates tables to devices while observing memory limits according to the predicted latency f , and Domain-Heuristic, the domain-expert algorithm of allocating tables to balance the aggregate dimension (Zha et al., 2022b). For SurCo-prior, we use Dreamshard, the SoTA embedding table sharding algorithm that requires training an offline RL policy.\n\nResults. Fig. 2, SurCo-zero finds lower latency sharding plans than the baselines, while it takes slightly longer than Domain-Heuristic and DreamShard due to taking optimization steps rather than selecting based on a heuristic feature or reinforcement learned policy. SurCo-prior obtains lower latency solutions in about the same time as DreamShard with a slight increase in overhead due to using SCIP (Achterberg, 2009), a branch and bound MILP solver. Lastly, SurCo-hybrid obtains the best solutions in terms of solution quality and has runtime comparable to SurCo-zero since at test time it performs similar operations. In smaller problem instances (T = 10 to T = 40), SurCo-prior obtains better quality solutions than its impromptu counterpart, SurCo-zero, likely due to training on a variety of examples and being able to better escape local optima in any given problem instance as might be the case with the impromptu solver. However, as the problem size increases and more tables are available for placement, SurCo-zero gives better performance by optimizing for the test instances in question as opposed to SurCo-prior which only uses training data to obtain surrogate costs. Using SurCo-hybrid, we are able to obtain the best quality solutions but incur the upfront cost of pretraining and the deployment-time cost of optimizing the coefficients on-the-fly.\n\n4.2\n\nINVERSE PHOTONIC DESIGN\n\n(a)\n\n(b)\n\nFigure 3: (a) The solution loss (% of failed instances when the design loss is not 0), and (b) test time solver runtime in log scale. For both, lower is better. We compare against the pass-through gradient approach proposed in Schubert et al. (2022). We observe that SurCo-prior achieves similar success rates to the previous approach Pass-through with a substantially improved runtime. Additionally, SurCo-zero runs comparably or faster, while finding more valid solutions than Pass-through. SurCo-hybrid obtains valid solutions most often and is faster than SurCo-zero at the expense of pretraining. Striped approaches use pretraining.\n\nPhotonic devices play an important role in high-speed communication (Marpaung et al., 2019), quantum computing (Arrazola et al., 2021), and machine learning hardware acceleration (Wetzstein et al., 2020). The photonic components can be formulated as a binary 2D grid, with each cell being filled or void. There are constraints for binary patterns: only those that can be drawn by a physical brush instrument with certain cross shape can be manufactured.\n\nIt remains challenging to find designs that are manufacturable and satisfy design specifications (e.g. beam splitting). An example solution developed by SurCo is shown in Figure 4b: coming from the top, beams are routed to the left or right, depending on wavelength. The solution is also manufacturable: a 3-by-3 brush cross can fit in all filled and void space.\n\nGiven the design, existing work (Hughes et al., 2019) enables differentiation of the design misspecification cost, evaluated as how far off the transmission of the wavelengths of interest is from the desired locations, with zero design loss meaning that the specification is satisfied. Researchers also develop a standard benchmark of inverse photonic design problems (Schubert et al., 2022).\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nSettings. We compare our approaches against the “Pass-Through” method (Schubert et al., 2022) on randomly generated instances of the four types of problems in Schubert et al. (2022): Waveguide Bend, Mode Converter, Wavelengths Division Multiplexer, and Beam Splitter. We generate 50 instances in each setting (25 training/25 test). Further generation details are in the appendix. We evaluated several algorithms described in the appendix, such as genetic algorithms and derivativefree optimization, which failed to find physically feasible solutions. We consider two wavelengths (1270nm/1290nm), and optimize at a resolution of 40nm, visualizing the test results in Fig. 3.\n\nResults. Fig. 3, SurCo-zero consistently finds as many or more valid devices compared to the Pass-Through baseline (Schubert et al., 2022). Additionally, since the on-the-fly solvers stop when they either find a valid solution, or reach a maximum of 200 steps, the runtime of SurCozero is slightly lower than the Pass-Through baseline. SurCo-prior obtains similar success rates as Pass-Through while taking two orders of magnitude less time as it does not require expensive impromptu optimization, making SurCo-prior a promising approach for large-scale settings or when solving many slightly-varied instances. Lastly, SurCo-hybrid performs best in terms of solution loss, finding valid solutions more often than the other approaches. It also takes less runtime than the other on-the-fly approaches since it is able to reach valid solutions faster, although it still requires optimization on-the-fly so it takes longer than SurCo-prior. We visualize the convergence of impromptu solvers in Fig. 4a where SurCo-zero has smoother and faster convergence compared to the Pass-through approach.\n\n(a) Loss Convergence\n\n(b) Device\n\n(c) Wave Mangitude\n\nFigure 4: Inverse photonic design convergence for a single instance (Schubert et al., 2022). SurCozero smoothly lowers the loss while the pass-through baseline converges noisily. Also, SurCohybrid starts out with a high-quality solution and fine-tunes until an optimal solution is reached. We also visualize the SurCo-zero solution with magnitudes of the two wavelengths of interest which we successfully route from the input at the top to the two different waveguides at the bottom.\n\n5 RELATED WORK\n\nDifferentiable Optimization Previous work differentiated through several optimization problems, calculating how changes in input parameters impact the optimal solution. Initially, a differentiable convex quadratic programming solver called OptNet (Amos & Kolter, 2017) proposed to implicitly differentiate the optimal solution with respect to input parameters through the KKT optimality con-\n\n8\n\n0255075100125150175200Step0.00.20.40.60.81.0DesignMisspecificationInversePhotonicsLossConvergenceMethodPass-ThroughSurCo-zeroSurCo-hybridDevice DesignEz magnitudefirst wavelengthEz magnitudesecond wavelengthUnder review as a conference paper at ICLR 2023\n\nditions, a set of linear equations that determined the optimal solution. Following this, researchers differentiated through linear programs (Wilder et al., 2019a), submodular optimization problems (Djolonga & Krause, 2017; Wilder et al., 2019a), cone programs (Agrawal et al., 2019a;b), MaxSAT (Wang et al., 2019), Mixed Integer Linear Programming (Ferber et al., 2020; Mandi et al., 2020), Integer Linear Programming (Mandi et al., 2020), dynamic programming solvers Demirovic et al. (2020), blackbox discrete linear optimizers (Poganˇci ́c et al., 2019; Rol ́ınek et al., 2020a;b), maximum likelihood estimation (Niepert et al., 2021), kmeans clustering (Wilder et al., 2019b), knapsack (Guler et al., 2022; Demirovi ́c et al., 2019), the cross-entropy method (Amos & Yarats, 2020), and SVM training (Lee et al., 2019). Additionally, Wang et al. (2020a) learned to linearly combine LP variables. SurCo can use these differentiable surrogates based on the problem domain.\n\nTask Based Learning Task-based learning solves distributions of linear or quadratic optimization problems with the true objective hidden at test time but available for training (Elmachtoub & Grigas, 2022; Donti et al., 2017; El Balghiti et al., 2019; Liu & Grigas, 2021; Hu et al., 2022). (Donti et al., 2021) predicts and corrects solutions for continuous nonlinear optimization. Bayesian optimization (BO) (Shahriari et al., 2016), optimizes blackbox functions by approximating the objective with a learned model that can be optimized over. Recent work optimizes individual instances over discrete spaces like hypercubes (Baptista & Poloczek, 2018), graphs (Deshwal et al., 2021), and MILP (Papalexopoulos et al., 2022). Data reuse from previous runs is proposed to optimize multiple correlated instances (Swersky et al., 2013; Feurer et al., 2018). However, the surrogate Gaussian Process (GP) models are memory and time intensive in high-dimensional settings. Recent work has addressed GP scalability via gradient updates (Ament & Gomes, 2022); however, it is unclear whether GP can scale in conjunction with combinatorial solvers. Machine learning is also used to guide combinatorial algorithms. Several approaches produce combinatorial solutions (Zhang & Dietterich, 1995; Khalil et al., 2017; Kool et al., 2018; Nazari et al., 2018; Zha et al., 2022a;b). Here, approaches are limited to simple feasible regions by iteratively building solutions for problems like routing, assignment, or covering. However, these approaches fail to handle more complex constraints. Other approaches set parameters that improve solver runtime (Khalil et al., 2016; Bengio et al., 2021).\n\nLearning Latent Space for Optimization As we learn latent linear objectives to optimize nonlinear functions, other approaches learn latent embeddings for faster solving. Faloutsos & Lin (1995) proposed FastMap, which learns latent object embeddings for efficient search. Variants of FastMap are used in graph optimization and shortest path (Cohen et al., 2018; Hu et al., 2022; Li et al., 2019). Wang et al. (2020b; 2021a); Yang et al. (2021); Zhao et al. (2022) use monte carlo tree search to perform single and multi-objective blackbox optimization by learning to split the search space.\n\nMixed Integer Nonlinear Programming (MINLP) SurCo-zero falls into the broad family of MINLP solvers, optimizing nonlinear and nonconvex objectives over discrete linear feasible regions. Specialized solvers handle many problem variants in the MINLP space (Burer & Letchford, 2012; Belotti et al., 2013); however, scalabliliy in the nonconvex setting is usually obtained by optimization experts who rely on problem-specific solving techniques such as making piecewise linear approximations, convexifying the objective, or exploiting special structure.\n\n6 CONCLUSION\n\nWe introduced SurCo, a method for learning linear surrogates for combinatorial nonlinear optimization problems. SurCo learns linear objective coefficients for a surrogate solver which results in solutions that minimize the nonlinear loss via gradient descent. At its core, SurCo differentiates through the surrogate solver which maps the predicted coefficients to a combinatorially feasible solution, combining the flexibility of gradient-based optimization with the structure of combinatorial solvers. We presented three variants of SurCo, SurCo-zero which optimizes individual instances, SurCo-prior which trains a coefficient prediction model offline, and SurCo-hybrid which fine-tunes the coefficients predicted by SurCo-prior on individual test instances. While SurCo’s performance is somewhat limited to binary problems due to the lack of interior integer points, we find that many real-world domains operate on binary decision variables. We evaluated variants of SurCo on two domains against the state of the art approaches used in industry, obtaining better solution quality for similar or better runtime in the embedding table sharding domain, and quickly identifying viable photonic devices.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nTobias Achterberg. Scip: solving constraint integer programs. Mathematical Programming Compu-\n\ntation, 1(1):1–41, 2009.\n\nAkshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. Differentiable convex optimization layers. Advances in neural information processing systems, 32, 2019a.\n\nAkshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa M Moursi. Differentiating\n\nthrough a cone program. J. Appl. Numer. Optim, 1(2):107–115, 2019b.\n\nSebastian E Ament and Carla P Gomes. Scalable first-order bayesian optimization via structured automatic differentiation. In International Conference on Machine Learning, pp. 500–516. PMLR, 2022.\n\nBrandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.\n\nIn International Conference on Machine Learning, pp. 136–145. PMLR, 2017.\n\nBrandon Amos and Denis Yarats. The differentiable cross-entropy method. In International Con-\n\nference on Machine Learning, pp. 291–302. PMLR, 2020.\n\nJuan M Arrazola, Ville Bergholm, Kamil Br ́adler, Thomas R Bromley, Matt J Collins, Ish Dhand, Alberto Fumagalli, Thomas Gerrits, Andrey Goussev, Lukas G Helt, et al. Quantum circuits with many photons on a programmable nanophotonic chip. Nature, 591(7848):54–60, 2021.\n\nGah-Yi Ban and Cynthia Rudin. The big data newsvendor: Practical insights from machine learning.\n\nOperations Research, 67(1):90–108, 2019.\n\nRicardo Baptista and Matthias Poloczek. Bayesian optimization of combinatorial structures.\n\nIn\n\nInternational Conference on Machine Learning, pp. 462–471. PMLR, 2018.\n\nPietro Belotti, Christian Kirches, Sven Leyffer, Jeff Linderoth, James Luedtke, and Ashutosh Ma-\n\nhajan. Mixed-integer nonlinear optimization. Acta Numerica, 22:1–131, 2013.\n\nYoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d’horizon. European Journal of Operational Research, 290(2): 405–421, 2021.\n\nQuentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, and Francis Bach. Learning with differentiable pertubed optimizers. Advances in neural information processing systems, 33:9508–9519, 2020.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao JAX: composable transformations of Python+NumPy programs, 2018. URL http: Zhang. //github.com/google/jax.\n\nSamuel Burer and Adam N Letchford. Non-convex mixed-integer nonlinear programming: A sur-\n\nvey. Surveys in Operations Research and Management Science, 17(2):97–106, 2012.\n\nVasek Chvatal, Vaclav Chvatal, et al. Linear programming. Macmillan, 1983.\n\nLiron Cohen, Tansel Uras, Shiva Jahangiri, Aliyah Arunasalam, Sven Koenig, and TK Satish Kumar.\n\nThe fastmap algorithm for shortest path computations. In IJCAI, 2018.\n\nAlison Cozad, Nikolaos V Sahinidis, and David C Miller. Learning surrogate models for simulation-\n\nbased optimization. AIChE Journal, 60(6):2211–2227, 2014.\n\nEmir Demirovi ́c, Peter J Stuckey, James Bailey, Jeffrey Chan, Christopher Leckie, Kotagiri Ramamohanarao, and Tias Guns. Predict+ optimise with ranking objectives: Exhaustively learning linear functions. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 1078–1085. International Joint Conferences on Artificial Intelligence, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nEmir Demirovic, Peter J Stuckey, Tias Guns, James Bailey, Christopher Leckie, Kotagiri Ramamohanarao, and Jeffrey Chan. Dynamic programming for predict+ optimise. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 1444–1451. AAAI Press, 2020.\n\nAryan Deshwal, Syrine Belakaria, and Janardhan Rao Doppa. Mercer features for efficient combinatorial bayesian optimization. Proceedings of the AAAI Conference on Artificial Intelligence, 35 (8):7210–7218, May 2021. doi: 10.1609/aaai.v35i8.16886. URL https://ojs.aaai.org/ index.php/AAAI/article/view/16886.\n\nJosip Djolonga and Andreas Krause. Differentiable learning of submodular models. Advances in\n\nNeural Information Processing Systems, 30, 2017.\n\nPriya Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning in stochastic\n\noptimization. Advances in neural information processing systems, 30, 2017.\n\nPriya L. Donti, David Rolnick, and J Zico Kolter. DC3: A learning method for optimization with hard constraints. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=V1ZHVxJ6dSS.\n\nOthman El Balghiti, Adam N Elmachtoub, Paul Grigas, and Ambuj Tewari. Generalization bounds in the predict-then-optimize framework. Advances in neural information processing systems, 32, 2019.\n\nAdam N Elmachtoub and Paul Grigas. Smart “predict, then optimize”. Management Science, 68(1):\n\n9–26, 2022.\n\nChristos Faloutsos and King-Ip Lin. Fastmap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets. In Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, SIGMOD ’95, pp. 163–174, New York, NY, USA, 1995. Association for Computing Machinery. ISBN 0897917316. doi: 10.1145/223784. 223812. URL https://doi.org/10.1145/223784.223812.\n\nAaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. Mipaal: Mixed integer program In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.\n\nas a layer. 1504–1511, 2020.\n\nMatthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for bayesian opti-\n\nmization. stat, 1050(6), 2018.\n\nAhmed Fawzy Gad. Pygad: An intuitive genetic algorithm python library, 2021.\n\nAbhijit Gosavi et al. Simulation-based optimization. Springer, 2015.\n\nAli Ugur Guler, Emir Demirovi ́c, Jeffrey Chan, James Bailey, Christopher Leckie, and Peter J Stuckey. A divide and conquer algorithm for predict+ optimize with non-convex problems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3749–3757, 2022.\n\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.\n\ngurobi.com.\n\nYichun Hu, Nathan Kallus, and Xiaojie Mao. Fast rates for contextual linear optimization. Manage-\n\nment Science, 2022.\n\nTyler W Hughes, Ian AD Williamson, Momchil Minkov, and Shanhui Fan. Forward-mode differen-\n\ntiation of maxwell’s equations. ACS Photonics, 6(11):3010–3016, 2019.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nElias Khalil, Pierre Le Bodic, Le Song, George Nemhauser, and Bistra Dilkina. Learning to branch in mixed integer programming. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.\n\nElias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems, 30, 2017.\n\nWouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems!\n\nIn\n\nInternational Conference on Learning Representations, 2018.\n\nBernhard Korte and Dirk Hausmann. An analysis of the greedy heuristic for independence systems.\n\nIn Annals of Discrete Mathematics, volume 2, pp. 65–74. Elsevier, 1978.\n\nSlawomir Koziel, Nurullah C ̧ alık, Peyman Mahouti, and Mehmet A Belen. Accurate modeling of antenna structures by means of domain confinement and pyramidal deep neural networks. IEEE Transactions on Antennas and Propagation, 70(3):2174–2188, 2021.\n\nAilsa H Land and Alison G Doig. An automatic method for solving discrete programming problems.\n\nIn 50 Years of Integer Programming 1958-2008, pp. 105–132. Springer, 2010.\n\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with In Proceedings of the IEEE/CVF conference on computer\n\ndifferentiable convex optimization. vision and pattern recognition, pp. 10657–10665, 2019.\n\nJiaoyang Li, Ariel Felner, Sven Koenig, and TK Satish Kumar. Using fastmap to solve graph problems in a euclidean space. In Proceedings of the international conference on automated planning and scheduling, volume 29, pp. 273–278, 2019.\n\nSirui Li, Zhongxia Yan, and Cathy Wu. Learning to delegate for large-scale vehicle routing. Ad-\n\nvances in Neural Information Processing Systems, 34:26198–26211, 2021.\n\nZhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in neural information processing systems, 31, 2018.\n\nHeyuan Liu and Paul Grigas. Risk bounds and calibration for a smart predict-then-optimize method.\n\nAdvances in Neural Information Processing Systems, 34:22083–22094, 2021.\n\nGiampaolo Liuzzi, Stefano Lucidi, and Francesco Rinaldi. Derivative-free methods for mixedinteger constrained optimization problems. Journal of Optimization Theory and Applications, 164(3):933–965, 2015.\n\nJayanta Mandi, Peter J Stuckey, Tias Guns, et al. Smart predict-and-optimize for hard combinatorial optimization problems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 1603–1610, 2020.\n\nDavid Marpaung, Jianping Yao, and Jos ́e Capmany. Integrated microwave photonics. Nature pho-\n\ntonics, 13(2):80–90, 2019.\n\nNina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021.\n\nAzalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. A graph placement methodology for fast chip design. Nature, 594(7862):207–212, 2021.\n\nRyo Nagai, Ryosuke Akashi, and Osamu Sugino. Completing density functional theory by machine\n\nlearning hidden messages from molecules. npj Computational Materials, 6(1):1–8, 2020.\n\nMaxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. Deep learning recommendation model for personalization and recommendation systems. CoRR, abs/1906.00091, 2019. URL https://arxiv.org/abs/1906.00091.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nMohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak ́ac. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems, 31, 2018.\n\nMathias Niepert, Pasquale Minervini, and Luca Franceschi. Implicit mle: backpropagating through discrete exponential family distributions. Advances in Neural Information Processing Systems, 34:14567–14579, 2021.\n\nTheodore P Papalexopoulos, Christian Tjandraatmadja, Ross Anderson, Juan Pablo Vielma, and David Belanger. Constrained discrete black-box optimization using mixed-integer programming. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 17295–17322. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/papalexopoulos22a.html.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, deep learning library. E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.\n\nMarin Vlastelica Poganˇci ́c, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations, 2019.\n\nJ. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub.\n\ncom/FacebookResearch/Nevergrad, 2018.\n\nEdward M Reingold and Robert E Tarjan. On a greedy heuristic for complete matching. SIAM\n\nJournal on Computing, 10(4):676–681, 1981.\n\nMichal Rol ́ınek, V ́ıt Musil, Anselm Paulus, Marin Vlastelica, Claudio Michaelis, and Georg MarIn Proceedings of the\n\ntius. Optimizing rank-based metrics with blackbox differentiation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7620–7630, 2020a.\n\nMichal Rol ́ınek, Paul Swoboda, Dominik Zietlow, Anselm Paulus, V ́ıt Musil, and Georg Martius. Deep graph matching via blackbox differentiation of combinatorial solvers. In European Conference on Computer Vision, pp. 407–424. Springer, 2020b.\n\nSebastian Ruder. An overview of gradient descent optimization algorithms.\n\narXiv preprint\n\narXiv:1609.04747, 2016.\n\nMartin F. Schubert, Alfred K. C. Cheung, Ian A. D. Williamson, Aleksandra Spyra, and David H. Alexander. Inverse design of photonic devices with strict foundry fabrication constraints. ACS Photonics, 9(7):2327–2336, 2022. doi: 10.1021/acsphotonics.2c00313.\n\nGeet Sethi, Bilge Acun, Niket Agarwal, Christos Kozyrakis, Caroline Trippel, and Carole-Jean Wu. Recshard: statistical feature-based memory optimization for industry-scale neural recommendaIn Proceedings of the 27th ACM International Conference on Architectural Support for tion. Programming Languages and Operating Systems, pp. 344–358, 2022.\n\nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1): 148–175, 2016. doi: 10.1109/JPROC.2015.2494218.\n\nDan Simon. Evolutionary optimization algorithms. John Wiley & Sons, 2013.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nBenoit Steiner, Chris Cummins, Horace He, and Hugh Leather.\n\nValue learning for throughput optimization of deep learning workloads. In A. Smola, A. Dimakis, and I. Stoica (eds.), Proceedings of Machine Learning and Systems, volume 3, pp. URL https://proceedings.mlsys.org/paper/2021/file/ 323–334, 2021. 73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf.\n\nKevin Swersky,\n\nJasper Snoek,\n\nMulti-task bayesian optimizaIn C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger tion. (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/ f33ba15effa5c10e873bf3842afb46a6-Paper.pdf.\n\nand Ryan P Adams.\n\nGuido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley,\n\nCA, 2009. ISBN 1441412697.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nStefan Voß, Silvano Martello, Ibrahim H Osman, and Catherine Roucairol. Meta-heuristics: Advances and trends in local search paradigms for optimization. Springer Science & Business Media, 2012.\n\nKai Wang, Bryan Wilder, Andrew Perrault, and Milind Tambe. Automatically learning compact quality-aware surrogates for optimization problems. Advances in Neural Information Processing Systems, 33:9586–9596, 2020a.\n\nLinnan Wang, Rodrigo Fonseca, and Yuandong Tian. Learning search space partition for black-box optimization using monte carlo tree search. Advances in Neural Information Processing Systems, 33:19511–19522, 2020b.\n\nLinnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, and Yuandong Tian. Sample-efficient neural architecture search by learning actions for monte carlo tree search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021a.\n\nPo-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In International Conference on Machine Learning, pp. 6545–6554. PMLR, 2019.\n\nXiaodi Wang, Youbo Liu, Junbo Zhao, Chang Liu, Junyong Liu, and Jinyue Yan. Surrogate model enabled deep reinforcement learning for hybrid energy community operation. Applied Energy, 289:116722, 2021b.\n\nGordon Wetzstein, Aydogan Ozcan, Sylvain Gigan, Shanhui Fan, Dirk Englund, Marin Soljaˇci ́c, Cornelia Denz, David AB Miller, and Demetri Psaltis. Inference in artificial intelligence with deep optics and photonics. Nature, 588(7836):39–47, 2020.\n\nBryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: DecisionIn Proceedings of the AAAI Conference on\n\nfocused learning for combinatorial optimization. Artificial Intelligence, volume 33, pp. 1658–1665, 2019a.\n\nBryan Wilder, Eric Ewing, Bistra Dilkina, and Milind Tambe. End to end learning and optimization\n\non graphs. Advances in Neural Information Processing Systems, 32, 2019b.\n\nLaurence A Wolsey. An analysis of the greedy algorithm for the submodular set covering problem.\n\nCombinatorica, 2(4):385–393, 1982.\n\nLaurence A Wolsey. Mixed integer programming. Wiley Encyclopedia of Computer Science and\n\nEngineering, pp. 1–10, 2007.\n\nKevin Yang, Tianjun Zhang, Chris Cummins, Brandon Cui, Benoit Steiner, Linnan Wang, Joseph E Gonzalez, Dan Klein, and Yuandong Tian. Learning space partitions for path planning. Advances in Neural Information Processing Systems, 34:378–391, 2021.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nYingjun Ye, Xiaohui Zhang, and Jian Sun. Automated vehicle’s behavior decision making using deep reinforcement learning and high-fidelity simulation environment. Transportation Research Part C: Emerging Technologies, 107:155–170, 2019.\n\nDaochen Zha, Louis Feng, Bhargav Bhushanam, Dhruv Choudhary, Jade Nie, Yuandong Tian, Jay Chae, Yinbin Ma, Arun Kejariwal, and Xia Hu. Autoshard: Automated embedding table sharding for recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4461–4471, 2022a.\n\nDaochen Zha, Louis Feng, Qiaoyu Tan, Zirui Liu, Kwei-Herng Lai, Bhushanam Bhargav, Yuandong Tian, Arun Kejariwal, and Xia Hu. Dreamshard: Generalizable embedding table placement for recommender systems. In Advances in Neural Information Processing Systems, 2022b.\n\nWei Zhang and Thomas G Dietterich. A reinforcement learning approach to job-shop scheduling.\n\nIn IJCAI, volume 95, pp. 1114–1120. Citeseer, 1995.\n\nYiyang Zhao, Linnan Wang, Kevin Yang, Tianjun Zhang, Tian Guo, and Yuandong Tian. Multiobjective optimization by learning space partition. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=FlwzVjfMryn.\n\nYanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu, Phitchaya Phothilimtha, Shen Wang, Anna Goldie, et al. Transferable graph optimizers for ml compilers. Advances in Neural Information Processing Systems, 33:13844–13855, 2020.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nLemma 3.1 (Sufficient condition of prediction with ε-accuracy). If the dataset Ddirect (ε/L)-cover Y , then for any y ∈ Y , a 1-nearest-neighbor regressor ˆφ leads to ∥ ˆφ(y) − φ(y)∥2 ≤ ε.\n\nProof. Since the dataset is a ε/L-cover, for any y ∈ Y , there exists at least one yi so that ∥y − yi∥2 ≤ ε/L. Let ynn be the nearest neighbor of y, and we have:\n\n∥y − ynn∥2 ≤ ∥y − yi∥2 ≤ ε/L\n\n(7)\n\nFrom the Lipschitz condition and the definition of 1-nearest-neighbor classifier ( ˆφ(y) = φ(ynn)), we know that\n\n∥φ(y) − ˆφ(y)∥2 = ∥φ(y) − φ(ynn)∥2 ≤ L∥y − ynn∥2 ≤ ε\n\n(8)\n\nLemma 3.2 (Lower bound of sample complexity for ε/L-cover). To achieve ε/L-cover of Y , the size of the training set N ≥ N0(ε) := vol(Y )\n\n, where vol0 is the volume of unit ball in d-dimension.\n\n(cid:1)d\n\n(cid:0) L ε\n\nvol0\n\nProof. We prove by contradiction. If N < N0(ε), then for each training sample (yi, φi), we create a ball Bi := B (yi, ε/L). Since\n\nvol\n\n(cid:32) N (cid:91)\n\ni=1\n\n(cid:33)\n\nBi ∩ Y\n\n≤ vol\n\n(cid:33)\n\nBi\n\n≤\n\n(cid:32) N (cid:91)\n\ni=1\n\nN (cid:88)\n\ni=1\n\nvol(Bi) = N vol0\n\n(cid:17)d\n\n(cid:16) ε L\n\n< vol(Y )\n\n(9)\n\nTherefore, there exists at least one y ∈ Y so that y /∈ Bi for any 1 ≤ i ≤ N . This means that y is not ε/L-covered.\n\nTheorem 3.1 (A case of infinite Lipschitz constant). If the minimal distance dmin for different connected components of φ(Y ) is strictly positive, and κ(φ(Y )) > κ(Y ), then the Lipschitz constant of the mapping φ is infinite.\n\nProof. Let R1, R2, . . . , RK be the K = κ(φ(Y )) connected components of φ(Y ), and Y1, Y2, . . . , YJ be the J = κ(Y ) connected components of Y . From the condition, we know that mink̸=k′ dist(Rk, Rk′) = dmin > 0.\n\nWe have Rk ∩ Rk′ = ∅ for k ̸= k′. Each Rk has a pre-image Sk := φ−1(Rk) ⊆ Y . These pre-images {Sk}K\n\nk=1 form a partition of Y since\n\n• Sk ∩ Sk′ = ∅ for k ̸= k′ since any y ∈ Y cannot be mapped to more than one connected\n\ncomponents;\n\n• (cid:83)K\n\nk=1 Sk = (cid:83)K\n\nk=1 φ−1(Rk) = φ−1 (cid:16)(cid:83)K\n\nk=1 Rk\n\n(cid:17)\n\n= φ−1(φ(S)) = S.\n\nSince K = κ(φ(Y )) > κ(Y ), by pigeonhole principle, there exists one Yj that contains at least part of the two pre-images Sk and Sk′ with k ̸= k′. This means that\n\nSk ∩ Yj ̸= ∅, Sk′ ∩ Yj ̸= ∅\n\n(10)\n\nThen we pick y ∈ Sk ∩ Yj and y′ ∈ Sk′ ∩ Yj. Since y, y′ ∈ Yj and Yj is a connected component, there exists a continuous path γ : [0, 1] (cid:55)→ Yj so that γ(0) = y and γ(1) = y′. Therefore, we have φ(γ(0)) ∈ Rk and φ(γ(1)) ∈ Rk′. Let t0 := sup{t : t ∈ [0, 1], φ(γ(t)) ∈ Rk}, then 0 ≤ t0 < 1. For any sufficiently small ε > 0, we have:\n\n• By the definition of sup, we know there exists t0 − ε ≤ t′ ≤ t0 so that φ(γ(t′)) ∈ Rk.\n\n• Picking t′′ = t0 + ε < 1, then φ(γ(t′′)) ∈ Rk′′ with some k′′ ̸= k.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nOn the other hand, by continuity of the curve γ, there exists a constant C(t0) so that ∥γ(t′) − γ(t′′)∥2 ≤ C(t0)∥t′ − t′′∥2 ≤ 2C(t0)ε. Then we have\n\nL = max y,y′∈Y\n\n∥φ(y) − φ(y′)∥2 ∥y − y′∥2\n\n≥\n\n∥φ(γ(t′)) − φ(γ(t′′))∥2 ∥γ(t′) − γ(t′′)∥2\n\n≥\n\ndmin 2C(t0)ε\n\n→ +∞\n\n(11)\n\nB EXPERIMENT DETAILS\n\nB.1 SETUPS\n\nExperiments are performed on a cluster of identical machines, each with 4 Nvidia A100 GPUs and 32 CPU cores, with 1T of RAM and 40GB of GPU memory. Additionally, we perform all operations in Python (Van Rossum & Drake, 2009) using Pytorch (Paszke et al., 2019). For embedding table placement, the nonlinear cost estimator is trained for 200 iterations and the offlinetrained models of Dreamshard and SurCo-prior are trained against the pretrained cost estimator for 200 iterations. The DLRM Dataset Naumov et al. (2019) is available at https: //github.com/facebookresearch/dlrm_datasets, and the dreamshard (Zha et al., 2022b) code is available at https://github.com/daochenzha/dreamshard. Additional details on dreamshard’s model architecture and features can be obtained in the paper and codebase. Training time for the networks used in SurCo-prior and SurCo-hybrid are on average 8 hours for the inverse photonic design settings and 6, 21, 39, 44, 50, 63 minutes for DLRM 10, 20, 30, 40, 50, 60 settings respectively.\n\nB.2 NETWORK ARCHITECTURES\n\nB.2.1 EMBEDDING TABLE SHARDING\n\nThe table features are the same used in Zha et al. (2022b), and sinusoidal positional encoding Vaswani et al. (2017) is used as device features so that the learning model is able to break symmetries between the different tables and effectively group them onto homogeneous devices. The table and device features are concatenated and then fed into Dreamshard’s initial fully-connected table encoding module to obtain scalar predictions ˆct,d for each desired objective coefficient. The architecture is trained with the Adam optimizer with learning rate 0.0005.\n\nB.2.2\n\nINVERSE PHOTONIC DESIGN\n\nNetwork architectures. The input design specification (a 2D image) is passed through a 3 layer convolutional neural network with ReLU activations and a final layer composed of filtering with the known brush shape. Then a tanh activation is used to obtain surrogate coefficients ˆc, one component for each binary input variable. The architecture is trained with the Adam optimizer with learning rate 0.001.\n\nThis is motivated by previous work (Schubert et al., 2022) that also uses the fixed brush shape filter and tanh operation to transform the latent parameters into a continuous solution that is projected onto the space of physically feasible solutions.\n\nIn each setting, optimization is done on a binary grid of different sizes to meet fabrication constraints, namely that a 3 by 3 cross must fit inside each fixed and void location. In the beam splitter the design is an 80 × 60 grid, in mode converter it is a 40 × 40 grid, in waveguide bend it is a 40 × 40 grid, in wavelength division multiplexer it is an 80 × 80 grid.\n\nPrevious work formulated the projection as finding a discrete solution that minimized the dot product of the input continuous solution and proposed discrete solution. The authors then updated the continuous solution by computing gradients of the loss with respect to the discrete solution and using pass-through gradients to update the continuous solution. By comparison, our approach treats the projection as an optimization problem and updates the objective coefficients so that the resulting projected solution moves in the direction of the desired gradient.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTask mode converter bend setting beam splitter wavelength division multiplexer\n\nRandomization randomize the right and left waveguide width randomize the waveguide width and length randomize the waveguide separation, width and length randomize the input and output waveguide locations\n\nTable 1: Task randomization of 4 different tasks in inverse photonic design.\n\nTo compute the gradient of this blackbox projection solver, we leverage the approach suggested by Poganˇci ́c et al. (2019) which calls the solver twice, once with the original coefficients, and again with coefficients that are perturbed in the direction of the incoming solution gradient as being an “improved solution”. The gradient with respect to the input coefficients are then the difference between the “improved solution” and the solution for the current objective coefficients.\n\nC PSEUDOCODE\n\nHere is the pseudocode for the different variants of our algorithm. Each of these leverage a differentiable optimization solver to differentiate through the surrogate optimization problem.\n\nAlgorithm 1 SurCo-zero\n\nInput: Ω, y, f\n\n1: c ← init surrogate coefs(y) 2: while not converged do 3: 4: 5: 6: end while 7: return x\n\nx ← arg minx∈Ω(y) c⊤x loss ← f (x; y) c ←grad update(c, ∇closs)\n\nAlgorithm 2 SurCo-prior Training\n\nInput: Ω, Dtrain = {yi}N\n\nSample batch B = {yi}k for y ∈ B do\n\ni=1, f 1: θ ← init surrogate model() 2: while not converged do 3: 4: 5: 6: 7: 8: 9:\n\nˆc ← ˆc(y; θ) x ← arg minx∈Ω(y) c⊤x loss += f (x; y)\n\nend for θ ←grad update(θ, ∇θloss)\n\ni ∼ Dtrain\n\nAlgorithm 3 SurCo-prior Deployment\n\nAlgorithm 4 SurCo-hybrid\n\n10: end while\n\nInput: Ω, Dtrain = {yi}N\n\ni=1, f , ytest\n\n1: θ ← train SurCo-prior(Ω, Dtrain, f ) 2: c ← ˆc(y; θ) 3: x ← arg minx∈Ω(y) c⊤x 4: return x\n\nInput: Ω, Dtrain = {yi}N\n\ni=1, f , ytest\n\n1: θ ← train SurCo-prior(Ω, Dtrain, f ) 2: c ← ˆc(y; θ) 3: while not converged do 4: 5: 6: 7: end while 8: return x\n\nx ← arg minx∈Ω(y) c⊤x loss ← f (x; y) c ←grad update(c, ∇closs)\n\nD ADDITIONAL FAILED BASELINES\n\nSOGA - Single Objective Genetic Algorithm Using PyGAD (Gad, 2021), we attempted several approaches for both table sharding and inverse photonics settings. While we were able to obtain feasible table sharding solutions, they underperformed the greedy baseline by 20%. Additionally, they were unable to find physically feasible inverse photonics solutions. We varied between random, swap, inversion, and scramble mutations and used all parent selection methods but were unable to find viable solutions.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nDFL - A Derivative-Free Library We could not easily integrate DFLGEN (Liuzzi et al., 2015) into our pipelines since it operates in fortran and we needed to specify the feasible region with python in the ceviche challenges. DFLINT works in python but took more than 24 hours to run on individual instances which reached a timeout limit. We found that the much longer runtime made this inapplicable for the domains of interest.\n\nNevergrad We enforced integrality in Nevergrad (Rapin & Teytaud, 2018) using choice variables which selected between 0 and 1. This approach was unable to find feasible solutions for inverse photonics in less than 10 hours. For table sharding we obtained solutions by using a choice variable for each table, selecting one of the available devices. This approach was not able to outperform the greedy baseline and took longer time so it was strictly dominated by the greedy approach.\n\n19",
    "reference": "# Summary Of The Paper\n\nThe paper presents a learning-based approach to solve combinatorial optimization problems with non-linear objectives and linear constraints. The paper introduces a linear surrogate cost function that can be used by existing solvers, and proposes to learn this surrogate cost in order to efficiently approximate the original problem. This idea is developed in two settings, either for solving individual instances or training a surrogate cost prediction model -- that can possibly be fine-tuned at test time. The approach is evaluated on two industrial problems.\n\n# Strength And Weaknesses\n\n**Strengths**\n1. The paper is well-written \n1. It addresses an important and challenging problem: non-linear combinatorial optimization \n1. Strong empirical results: clear improvements over the baselines in 2 problems.\n\n**Weaknesses**\n1. The paper proposes a surrogate g(c) that is used with the non-linear function f — and not instead of f. Theoretically and intuitively, it is not clear why optimizing f(g(c)) (ie Eq 2) is better than directly optimizing f (Eq 1)\n   * I agree with the paper that the proposed optimization (Eq 2) allows to easily handle the linear constraints. But MINLP solvers also generally handle linear constraints (e.g. SCIP).\n   * I don’t agree or did not understand the other arguments (see questions 1 to 7)\n\n1. The theoretical analysis (Sec 3.4), that compares “learning solutions” versus the proposed “learning surrogate costs” only holds for a nearest neighbor regressor, which is a very special model and not realistic in practice. Therefore I don’t see how it supports the claims of the paper. \n   * For example, learning solutions for CO problems is often formulated as an auto-regressive task (e.g. Pointer Networks by [Vinyals et al 2015] or the Attention Model by [Kool et al 2019]) \n   * Are there any works that use nearest neighbor regression to predict the solutions of CO problems?\n\n1. Important information is missing in the empirical evaluation:\n    * How long did the training take, esp. in terms of number of calls to f?\n    * Training datasets are very small: 50 instances for the Embedding Table Sharding problem and 25 instances for the Inverse Photonic Design problem. More information about how training converges and esp. if/how overfitting is avoided would be beneficial. \n    * Regarding Inverse Photonic Design, it is not clear what's the objective and what are the constraints. \n    * It looks like the objective may be the “design misspecification loss”. Which means that the complex constraints are in fact penalized in the objective. This is a fair strategy when feasibility is challenging but then the argument that the proposed method is more able to handle complex constraints than existing works does not seem fair. [Sec 5: “However, these approaches are unable to handle more complex combinatorial constraints that arise in practice such as those in inverse photonic design”]. Or maybe there are other constraints that are handled — hence the need for a clear formulation of the problem.\n    * What’s the size of considered instances of the Inverse Photonic Design?\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity & Quality** \nThe paper is generally well organized and well written. However there are several claims that need to be clarified and better motivated (see questions below).\n\n**Novelty**\nThe novelty of the paper is the idea of introducing a linear surrogate to replace the variable of the non-linear optimization problem.\n\n**Reproducibility**\nLinks to the datasets are provided. The precise description of the models hyperparameters is not provided. \n\n**Questions**\n1. Sec 3.1: “it also helps escape from local minima, thanks to the embedded search component of existing combinatorial solvers”. To which local minima the paper is referring to here? The MIP solver indeed returns a global minimum for (3). But I don’t see the implication on escaping local minima w.r.t c in Eq (2).\n2. Sec 3.2: “the N optimization procedures in the data collection stage are independent of each other, and can lead to excessive number of calls to f that are not helpful. ”. Data collection is solving N SurCo-Zero problems. Why would that lead to an excessive number of f evaluations? Isn't having a labeled training set a requirement of the proposed SurCo-Prior-lambda approach?\n3. Sec 3.2: There is a confusion with the c_i: they are first introduced as being the labels in the training set {(y_i, c_i)} then $c_i$’s appear in the variables in Eq (4); then they don’t appear in Eq (5) although it is said “given the training set Dtrain”. Is the training set different at the end of this section?\n4. Sec 3.2: “if $\\hat{c}$ is a mapping to global optimal solution of c, then it will pull the solutions out of local optima to re-target towards global ones, even when starting from poor initialization, yielding fast convergence and better final solutions for individual optimization instances.” What is meant here by “a mapping to global optimal solution of c”? \n5. Sec 3.2: What’s the advantage of “SurCo-prior-λ” (Eq 4) w.r.t. “SurCo-prior” (Eq 5)?\n6. Sec 3.2: “but at test time only require the feasible region and not the nonlinear objective.” Since the y is required to predict the c and the objective is defined as a family f(x,y), it looks characterized by y, then I don't understand what is meant by not requiring the objective at test time.\n7. Is there any theoretical guarantee or justification on the number of calls needed to optimize f (Eq 1) versus f(g(c)) (ie Eq 2)? \n8. Sec 3.4.2: “the mapping y → c(y) can avoid too many connected components in its image c(Y ), by connecting disjoint components of x∗(Y ) together.” What does “too many” means here? Why would this mapping connect disjoint components?\n9. Regarding the baselines: \n   * Why not using SCIP directly? The paper mentions that scale is a challenge for MINLP solvers but at least for the Embedding Table Sharding problem, it seems that the largest instances have 60x4=240 variables, which should be fine for SCIP?\n   * What’s the motivation of using derivative-free methods for optimizing differentiable functions? (Appendix)\n\n# Summary Of The Review\n\nI would vote for borderline reject.\n\nThe main contribution of the paper is the introduction of a linear surrogate cost in order to approximate non-linear combinatorial optimization problems. Although it leads to very good experimental results, the main idea is not well-motivated in my opinion, neither theoretically nor intuitively. This makes it hard to see why the proposed approach would work in general, beyond the two presented problems. Some parts of the paper lack clarity and several general claims/arguments need to be clarified or better justified.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nE-CRF: EMBEDDED CONDITIONAL RANDOM FIELD FOR BOUNDARY-CAUSED CLASS WEIGHTS CONFUSION IN SEMANTIC SEGMENTATION\n\nJie Zhu1,2 Huabin Huang3 Banghuai Li3 Leye Wang1,2∗ Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education, China1 School of Computer Science, Peking University, Beijing, China2 MEGVII Technology3 zhujie@stu.pku.edu.cn, {huanghuabin1994, libanghuai}@gmail.com, leyewang@pku.edu.cn\n\nABSTRACT\n\nModern semantic segmentation methods devote much effect to adjusting image feature representations to improve the segmentation performance in various ways, such as architecture design, attention mechnism, etc. However, almost all those methods neglect the particularity of class weights (in the classification layer) in segmentation models. In this paper, we notice that the class weights of categories that tend to share many adjacent boundary pixels lack discrimination, thereby limiting the performance. We call this issue Boundary-caused Class Weights Confusion (BCWC). We try to focus on this problem and propose a novel method named Embedded Conditional Random Field (E-CRF) to alleviate it. E-CRF innovatively fuses the CRF into the CNN network as an organic whole for more effective end-to-end optimization. The reasons are two folds. It utilizes CRF to guide the message passing between pixels in high-level features to purify the feature representation of boundary pixels, with the help of inner pixels belonging to the same object. More importantly, it enables optimizing class weights from both scale and direction during backpropagation. We make detailed theoretical analysis to prove it. Besides, superpixel is integrated into E-CRF and served as an auxiliary to exploit the local object prior for more reliable message passing. Finally, our proposed method yields impressive results on ADE20K, Cityscapes, and Pascal Context datasets.\n\n1\n\nINTRODUCTION\n\nSemantic segmentation plays an important role in practical applications such as autonomous driving, image editing, etc. Nowadays, numerous CNN-based methods (Chen et al., 2014; Fu et al., 2019; Ding et al., 2019) have been proposed. They attempt to adjust the image feature representation of the model itself to recognize each pixel correctly. However, almost all those methods neglect the particularity of class weights (in the classification layer) that play an important role in distinguishing pixel categories in segmentation models. Hence, it is critical to keep class weights discriminative. Unfortunately, CNN models have the natural defect for this. Generally speaking, most discriminative higher layers in the CNN network always have the larger receptive field, thus pixels around the boundary may obtain confusing features from both sides. As a result, these ambiguous boundary pixels will mislead the optimization direction of the model and make the class weights of such categories that tend to share adjacent pixels indistinguishable. For the convenience of illustration, we call this issue as Boundary-caused Class Weights Confusion (BCWC). We take DeeplabV3+ (Chen et al., 2018a) as an example to train on ADE20K (Zhou et al., 2017) dataset. Then, we count the number of adjacent pixels for each class pair and find a corresponding category that has the most adjacent pixels for each class. Fig 1(a) shows the similarity of the class weight between these pairs in descending order according to the number of adjacent pixels. It is clear that if two categories share more adjacent pixels, their class weights tend to be more similar, which actually indicates that BCWC makes class representations lack discrimination and damages the overall segmentation performance.\n\n∗Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nPrevious works mainly aim to improve boundary pixel segmentation, but they seldom explicitly take class weights confusion i.e., BCWC, into consideration 1.\n\nFigure 1: (a) Observations on ADE20K. We find a corresponding category that shares the most adjacent pixels for each class and calculate the similarity of their class weights. X-axis stands for the number of adjacent pixels for each class pair in descending order, and Y-axis represents the similarity of their class weights. Blue line denotes baseline model while orange line denotes E-CRF. Apparently, two categories that share more adjacent pixels are inclined to have more similar class weights, while E-CRF effectively decreases the similarity between adjacent categories and makes their class weights more discriminative. (b) Message passing procedure of E-CRF. F is the original feature maps of the CNN network. E-CRF utilizes pairwise module ψf s on F to obtain refined feature maps F p and F s respectively. Then F , F p and F s are fused as F ∗ to further segment the image.\n\np and auxiliary superpixel-based module ψf\n\nConsidering the inherent drawback of CNN networks mentioned before, delving into the relationship between raw pixels becomes a potential alternative to eliminate the BCWC problem, and Conditional Random Field (CRF) (Chen et al., 2014) stands out. It is generally known that pixels of the same object tend to share similar characteristics in the local area. Intuitively, CRF utilizes the local consistency between original image pixels to refine the boundary segmentation results with the help of inner pixels of the same object. CRF makes some boundary pixels that are misclassified by the CNN network quite easy to be recognized correctly. But these CRF-based methods (Chen et al., 2014; Zhen et al., 2020a) only adopt CRF as an offline post-processing module, we call it Vanilla-CRF, to refine the final segmentation results. They are incapable of relieving BCWC problem as CRF and the CNN network are treated as two totally separate modules.\n\nBased on Chen et al. (2014; 2017a), Lin et al. (2015); Arnab et al. (2016); Zheng et al. (2015) go a step further to unify the segmentation model and CRF in a single pipeline for end-to-end training. We call it Joint-CRF for simplicity. Same as Vanilla-CRF, Joint-CRF inclines to rectify those misclassified boundary pixels via increasing the prediction score of the associated category, which means it still operates on the object class probabilities. But it can alleviate the BCWC problem to some extent as the probability score refined by CRF directly involves in the model backpropagation. Afterwards, the disturbing gradients caused by those pixels will be relieved, which will promote the class representation learning. However, as shown in Fig 3, the effectiveness of Joint-CRF is restricted as it only optimizes the scale of the gradient and lacks the ability to optimize class representations effectively due to the defective design. More theoretical analysis can be found in Sec. 3.3.\n\nTo overcome the aforementioned drawbacks, in this paper, we present a novel approach named Embedded CRF (E-CRF) to address the BCWC problem more effectively. The superiority of E-CRF lies in two main aspects. On the one hand, by fusing CRF mechanism into the segmentation model, E-CRF utilizes the local consistency among original image pixels to guide the message passing of high-level features. Each pixel pair that comes from the same object tends to obtain higher message passing weights. Therefore, the feature representation of the boundary pixels can be purified by the corresponding inner pixels from the same object. In turn, those pixels will further contribute to the discriminative class representation learning. On the other hand, it extends the fashion of optimizing class weights from one perspective (i.e., scale) to two (i.e., scale and direction) during\n\n1These methods improve boundary segemetation and may have effect on class weights. But they are not explicit and lack theoretical analysis. We show great benifit of explicitly considering BCWC issue. See A.4.1.\n\n2\n\nbaselineoursPairwise message passingSuperpixel message passingfp*FfSfS++sFFFpFPairwise message passingSuperpixel message passingfp*FfS+sFFFpF(a)(b)Adjacent pixels number(descending order)Class weight similarityPublished as a conference paper at ICLR 2023\n\nFigure 2: Illustration of Joint-CRF and E-CRF. The first row is the simplified structure of Joint-CRF, which unifies the CNN network and CRF in a single pipeline for end-to-end training. However, CRF only serves as a post-processing module. The second row is the overview of our E-CRF, which fuses CRF into CNN network as an organic whole to eliminate BCWC problem.\n\nbackpropagation. In Sec. 3.3, we prove theoretically that E-CRF outperforms other CRF-based methods on eliminating the BCWC problem by optimizing both direction and scale of the disturbing gradient of class weights. However, during this process, the noise information can also have a direct influence on the class weights (likely to hinder the optimization for the BCWC problem). In addition, E-CRF adopts superpixel (Ren & Malik, 2003) as an auxiliary and leverage its local prior to suppress the noise and further strengthen the reliability of the message passing to the boundary pixels. Superpixel groups adjacent pixels that share similar characteristics to form a block. It is prone to achieve clear and smooth boundaries and increases the potential for higher segmentation performance. In E-CRF, we average the deep feature representation of all inner pixels in the same superpixel block and then add this local object prior to each pixel back to enhance the representation of boundary pixels.\n\nIn this work, we explicitly propose the BCWC problem in semantic segmentation and an effective approach to alleviate it. We conduct extensive experiments on three challenging semantic segmentation benchmarks, i.e., ADE20K (Zhou et al., 2017), Cityscapes (Cordts et al., 2016), and Pascal Context (Mottaghi et al., 2014), and yeild impressive results. For example, E-CRF outperforms baselines (DeeplabV3+ (Chen et al., 2018a) with ResNet-101 (He et al., 2016)) by 1.42% mIoU on ADE20K and 0.92% mIoU on Cityscapes in single scale. In addition, we make an exhaustive theoretical analysis in Sec. 3.3 to prove the effectiveness of E-CRF. Code is available at https://github.com/JiePKU/E-CRF.\n\n2 RELATED WORK\n\nSemantic Segmentation. Fully convolutional network (FCN) (Long et al., 2015) based methods have made great progress in semantic segmentation by leveraging the powerful convolutional features of classification networks (He et al., 2016; Huang et al., 2017) pre-trained on large-scale data (Russakovsky et al., 2015). There are several model variants proposed to enhance contextual aggregation. For example, DeeplabV2 (Chen et al., 2017a) and DeeplabV3 (Chen et al., 2017b) take advantage of the astrous spatial pyramid pooling (ASPP) to embed contextual information, which consists of parallel dilated convolutions with different dilated rates to broaden the receptive field. Inspired by the encoder-decoder structures (Ronneberger et al., 2015; Ding et al., 2018), DeeplabV3+ (Chen et al., 2018a) adds a decoder upon DeeplabV3 to refine the segmentation results especially along object boundaries. With the success of self-attention mechanism in natural language processing, Non-local (Wang et al., 2018) first adopts self-attention mechanism as a module for computer vision tasks, such as video classification, object detection and instance segmentation. A2Net (Chen et al., 2018b) proposes the double attention block to distribute and gather informative global features from the entire spatio-temporal space of the images.\n\nConditional Random Fields. Fully connected CRFs have been used for semantic image labeling in (Payet & Todorovic, 2010; Toyoda & Hasegawa, 2008), but inference complexity in fully connected models has restricted their application to sets of hundreds of image regions or fewer. To address this issue, densely connected pairwise potentials (Krähenbühl & Koltun, 2011) facilitate interactions\n\n3\n\nClass weightSuperpixelsBackboneClass weightE-CRFJoint-CRFF*FY*Y*YPublished as a conference paper at ICLR 2023\n\nbetween all pairs of image pixels based on a mean field approximation to the CRF distribution. Chen et al. (2014) show further improvements by post-processing the results of a CNN with a CRF. Subsequent works (Lin et al., 2015; Arnab et al., 2016; Zheng et al., 2015) have taken this idea further by incorporating a CRF as layers within a deep network and then learning parameters of both the CRF and CNN together via backpropagation. In terms of enhancements to conventional CRF models, Ladický et al. (2010) propose using an off-the-shelf object detector to provide additional cues for semantic segmentation.\n\nSuperpixel. Superpixel (Ren & Malik, 2003) is pixels with similar characteristics that are grouped together to form a large block. Since its introduction in 2003, there have been many mature algorithms (Achanta et al., 2012; Weikersdorfer et al., 2013; Van den Bergh et al., 2012). Owing to their representational and computational efficiency, superpixels are widely-used in computer vision algorithms such as target detection (Shu et al., 2013; Yan et al., 2015), semantic segmentation (Gould et al., 2008; Sharma et al., 2014; Gadde et al., 2016), and saliency estimation (He et al., 2015; Perazzi et al., 2012). Yan et al. (2015) convert object detection problem into superpixel labeling problem and conducts an energy function considering appearance, spatial context and numbers of labels. Gadde et al. (2016) use superpixels to change how information is stored in the higher level of a CNN. In (He et al., 2015), superpixels are taken as input and contextual information is recovered among superpixels, which enables large context to be involved in analysis.\n\nWe give a detailed discussion about the difference between E-CRF and three highly related works including PCGrad (Yu et al., 2020b), OCNet (Yuan & Wang, 2018), and SegFix (Yuan et al., 2020b) in Appendix A.5.\n\n3 METHOD\n\n3.1 REVISITING CONDITIONAL RANDOM FIELD (CRF)\n\nCRF is a typical discriminative model suitable for prediction tasks where contextual information or the state of the neighbors affects the current prediction. Nowadays, it is widely adopted in the semantic segmentation field (Krähenbühl & Koltun, 2011; Chen et al., 2014). CRF utilizes the correlation between original image pixels to refine the segmentation results by modeling this problem as the maximum a posteriori (MAP) inference in a conditional random field (CRF), defined over original image pixels. In practice, the most common way is to approximate CRF as a message passing procedure among pixels and it can be formulated as:\n\nY ∗\n\ni =\n\n1 Zi\n\n(ψu(i) +\n\nG (cid:88)\n\nj̸=i\n\nψp(i, j)Yj) ,\n\n(1)\n\nwhere Yi and Y ∗ i are defined as the classification scores of CNN model and CRF respectively for pixel i, Zi is the normalization factor known as the partition function, and ψu(i) is a unary function which often adopts Yi as the default value. G is the associated pixel set with pixel i. For example, DenseCRF (Krähenbühl & Koltun, 2011) takes all other pixels except pixel i itself as the set G. Moreover, the pairwise function ψp(i, j) is defined to measure the message passing weight from pixel j to pixel i. It is formulated as:\n\nψp(i, j) = μ(i, j)\n\nM (cid:88)\n\nm=1 (cid:124)\n\nω(m)k(m)(fi, fj)\n\n,\n\n(cid:123)(cid:122) k(fi,fj )\n\n(cid:125)\n\n(2)\n\nwhere μ(i, j) is a label compatibility function that introduces the co-occurrent probability for a specific label pair assignment at pixel i and j, while k(fi, fj) is a set of hand-designed Gaussian kernels, fi and fj are feature vectors of pixel i and j in any arbitrary feature space, such as RGB images. w(m) is the corresponding linear combination weight for each Gaussian kernel. When dealing with multi-class image segmentation, M =2 is a common setting. Then, k(fi, fj) is carefully designed as contrast-sensitive two-kernel potentials, defined in terms of color vectors (Ii, Ij) and\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nposition coordinates (pi, pj) for pixel i and j respectively:\n\nk(fi, fj) = w(1) exp\n\n−\n\n(cid:32)\n\n|pi − pj|2 2θ2 α\n\n−\n\n|Ii − Ij|2 2θ2 β\n\n(cid:124)\n\n(cid:123)(cid:122) appearance\n\nkernel\n\n(cid:33)\n\n(cid:125)\n\n(cid:32)\n\n+ w(2) exp\n\n−\n\n|pi − pj|2 2θ2 γ\n\n(cid:33)\n\n.\n\n(cid:124) (cid:123)(cid:122) smoothness\n\n(cid:125) kernel\n\n(3)\n\nThe appearance kernel is inspired by the observation that nearby pixels with similar colors are more likely to share the same class. θα and θβ are scale factors to control the degree of these two elements, i.e., similarity and distance between two pixels. Apart from this, the smoothness kernel further removes the influence of some small isolated regions (Krähenbühl & Koltun, 2011) and θγ is the associated scale factor. Notably, all these parameters are learnable during the model training.\n\nUnfortunately, current CRF-based methods (Chen et al., 2014; 2017a; Lin et al., 2015; Liu et al., 2015) for semantic segmentation always adopt CRF as a post-processing module. For example, Vanilla-CRF (Chen et al., 2014; 2017a) utilizes CRF to refine segmentation scores offline, which has no impacts on BCWC since the CNN network and CRF are treated as two separate modules. Joint-CRF (Lin et al., 2015; Liu et al., 2015; Lin et al., 2016) works in a similar way although CRF involves in the backpropagation of CNN networks, restricting its ability to relieve BCWC.\n\n3.2 EMBEDDED CRF\n\nTo solve the BCWC problem in a more intrinsical way, we propose a novel method named Embedded CRF (E-CRF) to tackle the tough problem via fusing the CRF mechanism into the CNN network as an organic whole for more effective end-to-end training. An overview of E-CRF can be found in Fig 2 and we formulate its core function based on Eq (1) as:\n\nF ∗\n\ni =\n\n1 Zi\n\n \n\n\n\nψf\n\nu(i) +\n\nG (cid:88)\n\nj̸=i\n\nψf\n\np (i, j)Fj + F S\n\ni\n\n \n\n\n\n.\n\n(4)\n\nSpecifically, the first two terms are analogous to Eq (1) but we perform CRF mechanism on the high-level features. Fi stands for the original output of feature extractors for pixel i, ψf u(i) and ψf u(i) takes Fi as the default value. In addition, we reformulate ψf\n\np (i, j) to perform message passing between pixel pairs in the high-level feature:\n\np (i, j) play the same role as they do in Eq (1). ψf\n\nψf\n\np (i, j) = μf (i, j)k(fi, fj) .\n\n(5)\n\nIt is worth noting that k(fi, fj) is no longer hand-designed Gaussian kernels as it is in Eq (1) but simple convolution operators instead to make the whole model more flexible for end-to-end training and optimization. Experiments in Sec. 4 prove this modification is a more suitable choice:\n\nk(fi, fj) = fi · fj = conv([Ii, pi]) · conv([Ij, pj]) ,\n\n(6)\n\nwhere [x, y] denotes the concatenation operator. Different from Eq (3), we normalize the input image I into the range [0, 1] to eliminate the scale variance between pixels and we replace original absolute position coordinates p with cosine position embeddings (Vaswani et al., 2017) to make it more compatible with CNN networks. E-CRF encodes the appearance and position of pixels into more discriminative tokens via the flexible convolution operation, then the dot product is adopted to measure the similarity between pixel pairs. As indicated in Eq (6), E-CRF intends to make nearby pixel pairs that share same appearance to achieve higher k(fi, fj). Its intention is the same as Eq (3). Correspondingly, we also adjust μf (i, j) as the feature compatibility to measure the co-occurrent probability of Fi and Fj:\n\nμf (i, j) = sigmoid(conv[Fi, Fj]) .\n\n(7)\n\nAnother component in Eq (4) is F S i . It relies on the superpixel algorithm (Ren & Malik, 2003; Weikersdorfer et al., 2013; Van den Bergh et al., 2012; Gadde et al., 2016) to divide the whole image I into several non-overlapping blocks. Pixels in the same superpixel block tend to share the same characteristics. Thus, we adopt this local object prior to achieve the more effective message passing between pixels in the high-level feature space. Concretely, we design F S\n\ni as:\n\nF S\n\ni =\n\nQ (cid:88)\n\nl\n\nψf\n\ns (l)Fl =\n\nQ (cid:88)\n\nl\n\n1 n\n\nFl\n\n5\n\n(8)\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Different optimization effects for baseline, Joint-CRF and E-CRF. W1 and W2 are two class weight vectors that share adjacent pixels. ∇W is the gradient variation for W1 and W ∗ 1 is the new class weight after gradient descent. Fk is a sample boundary pixel whose ground-truth label keeps consistent with W1 but contains confusing features from both sides. θ measures the distance between W2 and W ∗ 1 . (a) ∇W tends to push W1 towards W2 due to the confusing features from both classes. (b) Joint-CRF eases the disturbing gradients and reduces the scale of ∇W . Obviously, θ2 is larger than θ1. (c) E-CRF aims to enhance the feature representation of Fk via the inner pixels like Fj from the same object. It adjusts both scale and direction of ∇W to make θ3 > θ2 > θ1.\n\nQ is the associated superpixel block that contains pixel i and ψf s (l) devotes the re-weighting factor for the deep features of pixel l in Q. We adopt ψf n and n is the total number of pixels in Q. F S i serves as a supplement in Eq (4) to add the local object prior to each pixel back, which increases the reliability of message passing in E-CRF. What’s more, superpixel (Gould et al., 2008; Sharma et al., 2014; Gadde et al., 2016) always tends to generate clearer and smoother boundary segmentation results than traditional CNN networks or CRFs do, which also increases the potential for more accurate segmentation results. Detailed experiments can be found in Sec. 4.\n\ns (l) = 1\n\n3.3 HOW E-CRF RELIEVES BCWC\n\nIn this section, without loss of generality, we take multi-class segmentation problem as an example to dive into the principle of E-CRF from the perspective of gradient descent. Suppose Fk is the feature vector of a foreground boundary pixel k whose class label is c ∈ [0, n − 1] and its prediction probability is P c k . Then, considering the label yk is one-hot form, the typical cross-entropy loss Lk can be defined as:\n\ni<n (cid:88)\n\nLk = −\n\nk ln P i yi\n\nk = − ln P c k ,\n\nP c\n\nk = sof tmax(Y c\n\nk ) =\n\ni=0\n\n( (cid:80)\n\nm̸=c\n\nk\n\neY c eY m k ) + eY c\n\nk\n\n, and Y c\n\nk = W T\n\nc · Fk ,\n\n(9)\n\n(10)\n\nwhere Wc is the class weight of c-th category. Y m with Wc and Y c\n\nk . Below the gradient variation ∇Wc can be formulated as:\n\nk is calculated by other class weights and unrelated\n\n∇Wc =\n\n∂Lk ∂Wc\n\n=\n\n∂Lk ∂P c k\n\n·\n\n∂P c k\n∂Y c k\n\n·\n\n∂Y c k\n∂Wc\n\nThrough Eq (9), Eq (10) and Eq (11), class weight in the next iteration will be updated 2:\n\nW ∗\n\nc = Wc − ∇Wc = Wc + (1 − P c\n\nk ) · Fk\n\n(11)\n\n(12)\n\nAs shown in Eq (12), the direction of the gradient descent keeps the same as Fk while the magnitude of the gradient is decided by P c k . What happens if we integrate the CRF into the segmentation pipeline? As we have discussed in Sec. 1, Vanilla-CRF has nothing to do with the optimization process of CNN networks, while if we adopt Joint-CRF, ∇Wc can be reformulated as:\n\n−∇Wc = (1 − ˆP c\n\nk ) · Fk = (1 −\n\n1 Zk\n\n(cid:88)\n\n(\n\nj∈G\n\nwjP c\n\nj + P c\n\nk ))\n\n·Fk\n\n(13)\n\n2The detailed derivation process can be found in our Appendix A.1.\n\n(cid:124)\n\n(cid:123)(cid:122) scale\n\n(cid:125)\n\n6\n\n2W1W*1W2W1W*1W2W1W*1W1(a)(b)(c)23kFkFkF*kFjFWWWPublished as a conference paper at ICLR 2023\n\nwhere ˆP c k is the refined score by CRF, wj is the message passing weight from pixel j to pixel k and P c j is the original score of pixel j. In general, boundary pixel k is hard to classify correctly due to the confusing features from both sides. Thus the original probability P c k is always small. In contrast, other inner pixels of the same object are easy to recognize and tend to achieve a higher probability. Consequently, ˆP c k and disturbing gradients caused by boundary pixel will be relieved to some extent, which makes inter-class distance further as shown in Fig 3(b). However, Eq (13) only adjusts the scale of the gradient descent while the direction still keeps the same as Fk, which weakens its effects for better representation learning. When it comes to our proposed E-CRF, ∇Wc can be further defined as:\n\nk is usually larger than P c\n\n−∇Wc = (1 − P c\n\nk\n\n∗) · F ∗\n\nk = (1 − P c (cid:124)\n\n(cid:123)(cid:122) scale\n\nk\n\n·\n\n1 Zk\n\n∗) (cid:125)\n\n(cid:88)\n\n(\n\nwjFj\n\n+Fk)\n\nj∈G (cid:125) (cid:123)(cid:122) (cid:124) direction\n\nP c\n\nk\n\n∗ = sof tmax(\n\n(cid:88)\n\nwjY c\n\nj + Y c k )\n\nj∈G\n\n(14)\n\n(15)\n\nk is the refined feature representations by E-CRF, and P c∗\n\nwhere F ∗ k is the refined score which is analogous to ˆP c k in Eq (13). Comparing with Joint-CRF, it is clear that E-CRF not only changes the scale of the gradient descent but also adjusts its optimization direction. The optimization process is directly applied to the class weight matrix (in the final layer), which opens up room for more discriminative class weights. In other words, we can adjust the class weight from both the scale and direction to make the class weights more discriminative to decrease the class weights similarity (or class weights confusion). As depicted in Fig 3(c), assume W1 is the class weight vector that a pixel belongs to, while W2 is the other one which has a higher co-occurrent probability with W1 in the same image. E-CRF designs an effective message passing procedure to purify the feature representation of boundary pixels assisted by inner pixels from the same object (Fj in Fig 3(c)). In this way, it relieves the influence of disturbing gradients and makes the inter-class distance between W1(W ∗\n\n1 ) and W2 further, which means more discriminative feature representations.\n\n4 EXPERIMENT\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nWe follow the previous works (Chen et al., 2014; He et al., 2019b; Chen et al., 2018a) and perform experiments on three challenging semantic segmentation benchmarks, i.e., ADE20K (Zhou et al., 2017), Cityscapes (Cordts et al., 2016) and Pascal Context (Mottaghi et al., 2014). Due to the space limit, a detailed description of these three datasets can be found in our Appendix A. We adopt DeeplabV3+ (Chen et al., 2018a) with ResNet (He et al., 2016) pretrained on ImageNet (Russakovsky et al., 2015) as our baseline to implement E-CRF. The detailed information follows standard settings in (Chen et al., 2014; 2018a) and we add it into our Appendix A. Specially, we employ SLIC (Achanta et al., 2012), a common superpixel segmentation algorithm, to divide each image of ADE20K, Cityscapes and Pascal Context into 200, 600, and 200 blocks respectively. Note that the superpixel is generated offline. To verify the effectiveness of our approach for semantic segmentation, we adopt two common metrics in our experiments, i.e., class-wise mIoU to measure the overall segmentation performance and 1-pixel boundary F-score (Takikawa et al., 2019; Tan et al., 2023) to measure the boundary segmentation performance.\n\n4.2 ABLATION STUDY\n\n4.2.1 COMPARISONS WITH RELATED METHODS\n\nAs shown in Table 1, we compare our proposed E-CRF with other traditional CRF-based methods, i.e., Vanilla-CRF and Joint-CRF. First of all, it is clear that all the CRF-based methods outperform the baseline model by a large margin, which well verifies the main claim in (Chen et al., 2014; 2017a; Lin et al., 2015; Liu et al., 2015; Lin et al., 2016) that CRF is beneficial to boundary segmentation (F-score). What’s more, E-CRF achieves the best result among all those methods, which surpasses the baseline model with up to 1.48% mIoU and 2.20% F-score improvements. E-CRF fuses the CRF mechanism into the CNN network as an organic whole. It relieves the disturbing gradients caused\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparisons with baseline, Vanilla-CRF and Joint-CRF on ADE20K val dataset. 3It stands for DeeplabV3+ followed by DenseCRF. 4An end-to-end manner of Vanilla-CRF, similar to (Zheng et al., 2015).\n\nMethod\n\nDeeplabV3+ Vanilla-CRF 3 Joint-CRF 4 E-CRF (Ours)\n\nResNet-50\n\nResNet-101\n\nF-score (%)\n\nmIoU (%)\n\nF-score (%)\n\nmIoU (%)\n\n14.25 16.26 16.32 16.45\n\n42.72 43.18 (+0.46) 43.69 (+0.96) 44.20 (+1.48)\n\n16.15 17.89 18.03 18.32\n\n44.60 45.14 (+0.54) 45.61 (+1.01) 46.02 (+1.42)\n\nby the BCWC problem and adjusts the feature representations to boost the overall segmentation performance and the boundary segmentation. Fig 1(a) also proves that E-CRF can decrease the interclass similarity consistently which results in more discriminative feature representations. Experiments on Cityscapes dataset can be found in our Appendix A.\n\n4.2.2 ABLATION ON MESSAGE PASSING STRATEGIES\n\ns , play vital roles in our proposed E-CRF. Table 2 shows that ψf\n\nAs we have discussed in Sec. 3.2, two message passing components, i.e., pairwise module ψf p and superpixel-based module ψf p and ψf s\ncan boost the overall segmentation performance on ADE20K val dataset with up to 1.19% mIoU and 1.25% mIoU gains when integrated into the baseline model respectively. Moreover, if we fuse them as a whole into E-CRF, they can further promote the segmentation performance by up to 1.48% mIoU improvements. We also compare with Non-local (Wang et al., 2018), another famous attention-based message passing method, into our experiments for comprehensive comparisons even though it actually has different design concepts from ours. Unfortunately, we find that although Non-local achieves improvements over the baseline, it is still inferior to our E-CRF.\n\nTable 2: Comparisons between message passing strategies, and ablation studies for different message passing components in E-CRF, pairwise ψf\n\np and auxiliary superpixel-based ψf s . mIoU(%)\n\nMethod\n\nDeeplabV3+ + Non-local\n\nE-CRF\n\nψf\n\np\n\n✓\n\n✓\n\nψf\n\ns\n\n✓ ✓\n\nResNet-50\n\n42.72 43.52 (↑ 0.80)\n\n43.91 (↑ 1.19) 43.83 (↑ 1.11) 44.20 (↑ 1.48)\n\nResNet-101\n\n44.60 45.34 (↑ 0.74)\n\n45.47 (↑ 0.87) 45.85 (↑ 1.25) 46.02 (↑ 1.42)\n\n4.2.3 ABLATION OF SUPERPIXEL NUMBERS\n\nWe follow standard settings in our paper and take DeeplabV3+ based on ResNet-50 as the baseline model to present the performance of E-CRF under different superpixel numbers. Detailed comparisons on ADE20K dataset are reported in Table 3 and SP denotes SuperPixel. As shown in Table 3, different numbers of superpixels indeed affect the performance of E-CRF. Intuitively, when the number of superpixels is 200, E-CRF acquires the best performance as it achieves a better trade-off between the superpixel purity and the long-range dependency. Moreover, it is worth noting that when the pairwise message passing strategy (i.e., ψf p ) is also adopted in E-CRF, it becomes more robust to the different numbers of superpixels that may introduce noise, as our adaptive message passing mechanism (including ψf s ) can be compatible with the variance.\n\nTable 3: Comparisons with different superpixel numbers on ADE20K val dataset.\n\n42.72 43.43 43.83 43.56 43.22\n\n43.91 44.02 44.20 44.13 43.96\n\nNo 100 200 300 400\n\nmIoU w\\o ψf\n\nmIoU w\\ ψf\n\np and ψf\n\nSP num\n\np (%)\n\np (%)\n\nMore ablation studies including comparison of different boundary refinement and computational cost are presented in Appendix A.4.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Comparisons with other state-of-the-art methods on ADE20K val dataset, Cityscapes val and test, and Pascal Context val dataset.\n\nbackbone\n\nmIoU(%)\n\nMethod\n\nCCNet (Huang et al., 2019) ANL (Zhu et al., 2019) GFFNet (Li et al., 2020c) APCNet (He et al., 2019b) DMNet (He et al., 2019a) SpyGR (Li et al., 2020a) RecoNet (Chen et al., 2020) SPNet (Hou et al., 2020) DNL (Yin et al., 2020) RANet (Shen et al., 2020) ACNet (Fu et al., 2019) HANet (Choi et al., 2020) RPCNet (Zhen et al., 2020b) CaCNet (Liu et al., 2020) CPNet (Yu et al., 2020a) STLNet (Zhu et al., 2021) E-CRF (Ours)\n\nResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101\n\nADE-val 45.22 45.24 45.33 45.38 45.50 -\n45.54 45.60 45.82 -\n45.90 -\n- 46.12 46.27 46.48 46.83\n\nCity-val 81.3 -\n81.8 -\n- 80.5 81.6 -\n- 81.9 82.0 82.05 82.1 -\n- 82.3 82.74\n\nCity-test 81.9 -\n82.3 -\n- 81.6 82.3 -\n- 82.4 82.3 82.1 81.8 -\n- 82.3 82.5\n\nPas-Con -\n52.8 54.2 54.7 54.4 52.8 54.8 54.5 55.3 54.9 54.1 -\n- 55.4 53.9 55.6 56.1\n\n4.3 COMPARISONS WITH SOTA METHODS\n\nIn this research, we mainly focus on the Boundary-caused Class Weight Confusion (BCWC) in CNN models. Hence, in this section, we choose CNN-based methods for fair comparisons.5\n\nADE20K. We first compare our E-CRF (ResNet101 as backbone) with existing methods on the ADE20K val set. We follow standard settings in (Huang et al., 2019; Yuan et al., 2020a; Zhu et al., 2021) to adopt multi-scale testing and left-right flipping strategies. Results are presented in Table 4. It is shown that E-CRF outperforms existing approaches. Segmentation visualization is presented in our Appendix\n\nCityscapes. To verify the generalization of our method, we perform detailed comparisons with other SOTA methods on Cityscapes val and test set. Multi-scale testing and left-right flipping strategies are also adopted. The results with ResNet101 as backbone are reported in Table 4. Remarkably, our algorithm achieves 82.74% mIoU in val set and outperforms previous methods by a large margin.\n\nPascal Context. To further verify the generalization of E-CRF (ResNet101 as backbone), we compare our method with other SOTA method on Pascal Context dataset as shown in Table 4. We adopt multi-scale testing and left-right flipping strategies as well. The result suggests the superiority of our method.\n\n5 CONCLUSION AND FUTURE WORKS\n\nIn this paper, we focus on the particularity of class weights in semantic segmentation and explicitly consider an important issue , named as Boundary-caused Class Weights Confusion (BCWC). We dive deep into it and propose a novel method, E-CRF, via combining CNN network with CRF as an organic whole to alleviate BCWC from two aspects (i.e., scale and direction). In addition, we make an exhaustive theoretical analysis to prove the effectiveness of E-CRF. Eventually, our proposed method achieves new results on ADE20K, Cityscapes, and Pascal Context datasets.\n\nThere are two important directions for future research. In this work, we use SLIC, a common cluster-based algorithm for fast implementation. There exist many other superpixel algorithms such as graphical-based (Felzenszwalb & Huttenlocher, 2004) and CNN-based (Jampani et al., 2018) that may give better boundary results for objects. Therefore how these different methods influence the performance in our framework is interesting. Besides, We find that transformer-based networks suffer from BCWC issue as well and make a preliminary exploration. More works are expected to focus on this issue.\n\n5We also conduct experiments based on SegFormer (Xie et al., 2021) to make a preliminary exploration in\n\nour Appendix B as we found that BCWC issue also exists in transformer-based models.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe thank the anonymous reviewers for their constructive comments. We also sincerely thank Tiancai Wang for useful discussion. This work is supported by the NSFC Grants no. 61972008.\n\nREFERENCES\n\nRadhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence, 34(11):2274–2282, 2012.\n\nAnurag Arnab, Sadeep Jayasumana, Shuai Zheng, and Philip HS Torr. Higher order conditional random fields in deep neural networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 524–540. Springer, 2016.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062, 2014.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017a.\n\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\n\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017b.\n\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoderdecoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pp. 801–818, 2018a.\n\nWanli Chen, Xinge Zhu, Ruoqi Sun, Junjun He, Ruiyu Li, Xiaoyong Shen, and Bei Yu. Tensor low-rank reconstruction for semantic segmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 52–69, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58520-4.\n\nYunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. Aˆ2-nets: Double attention networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018b.\n\nSungha Choi, Joanne T. Kim, and Jaegul Choo. Cars can’t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34, 2021.\n\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nHenghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Context contrasted feature and gated multi-scale aggregation for scene segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2393–2402, 2018.\n\nHenghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, and Gang Wang. Boundaryaware feature propagation for scene segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6819–6829, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. Inter-\n\nnational journal of computer vision, 59:167–181, 2004.\n\nJun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive context network for scene parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6748–6757, 2019.\n\nRaghudeep Gadde, Varun Jampani, Martin Kiefel, Daniel Kappler, and Peter V Gehler. Superpixel convolutional networks using bilateral inceptions. In European conference on computer vision, pp. 597–613. Springer, 2016.\n\nStephen Gould, Jim Rodgers, David Cohen, Gal Elidan, and Daphne Koller. Multi-class segmentation with relative location prior. International Journal of Computer Vision, 80(3):300–316, 2008.\n\nJunjun He, Zhongying Deng, and Yu Qiao. Dynamic multi-scale filters for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3562–3572, 2019a.\n\nJunjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7519–7528, 2019b.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nShengfeng He, Rynson WH Lau, Wenxi Liu, Zhe Huang, and Qingxiong Yang. Supercnn: A superpixelwise convolutional neural network for salient object detection. International journal of computer vision, 115(3):330–344, 2015.\n\nQibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pooling: Rethinking spatial pooling for scene parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4003–4012, 2020.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.\n\nZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019.\n\nVarun Jampani, Deqing Sun, Ming-Yu Liu, Ming-Hsuan Yang, and Jan Kautz. Superpixel sampling networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 352–368, 2018.\n\nPhilipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian\n\nedge potentials. Advances in neural information processing systems, 24:109–117, 2011.\n\nL’ubor Ladický, Paul Sturgess, Karteek Alahari, Chris Russell, and Philip H. S. Torr. What, where and how many? combining object detectors and crfs. In Kostas Daniilidis, Petros Maragos, and Nikos Paragios (eds.), Computer Vision – ECCV 2010, pp. 424–437, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg.\n\nXia Li, Yibo Yang, Qijie Zhao, Tiancheng Shen, Zhouchen Lin, and Hong Liu. Spatial pyramid based graph reasoning for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8950–8959, 2020a.\n\nXiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, and Yunhai Tong. Improving semantic segmentation via decoupled body and edge supervision. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16, pp. 435–452. Springer, 2020b.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nXiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Shaohua Tan, and Kuiyuan Yang. Gated fully fusion for semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 11418–11425, 2020c.\n\nGuosheng Lin, Chunhua Shen, Ian Reid, and Anton van den Hengel. Deeply learning the messages in message passing inference. Advances in Neural Information Processing Systems, 28, 2015.\n\nGuosheng Lin, Chunhua Shen, Anton Van Den Hengel, and Ian Reid. Efficient piecewise training of deep structured models for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3194–3203, 2016.\n\nJianbo Liu, Junjun He, Yu Qiao, Jimmy S. Ren, and Hongsheng Li. Learning to predict contextadaptive convolution for semantic segmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 769–786, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58595-2.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 10012–10022, 2021.\n\nZiwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Semantic image segmentation via deep parsing network. In Proceedings of the IEEE international conference on computer vision, pp. 1377–1385, 2015.\n\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431–3440, 2015.\n\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 891–898, 2014.\n\nNadia Payet and Sinisa Todorovic. ˆ 2–random forest random field. Advances in Neural Information\n\nProcessing Systems, 23, 2010.\n\nChao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, and Jian Sun. Megdet: A large mini-batch object detector. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6181–6189, 2018.\n\nFederico Perazzi, Philipp Krähenbühl, Yael Pritch, and Alexander Hornung. Saliency filters: Contrast based filtering for salient region detection. In 2012 IEEE conference on computer vision and pattern recognition, pp. 733–740. IEEE, 2012.\n\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12179–12188, 2021.\n\nXiaofeng Ren and Jitendra Malik. Learning a classification model for segmentation. In Computer Vision, IEEE International Conference on, volume 2, pp. 10–10. IEEE Computer Society, 2003.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n\nAbhishek Sharma, Oncel Tuzel, and Ming-Yu Liu. Recursive context propagation network for\n\nsemantic scene labeling. In NIPS, volume 1, pp. 2, 2014.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nDingguo Shen, Yuanfeng Ji, Ping Li, Yi Wang, and Di Lin. Ranet: Region attention network for semantic segmentation. Advances in Neural Information Processing Systems, 33:13927–13938, 2020.\n\nGuang Shu, Afshin Dehghan, and Mubarak Shah. Improving an object detector and extracting regions using superpixels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3721–3727, 2013.\n\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7262–7272, 2021.\n\nTowaki Takikawa, David Acuna, Varun Jampani, and Sanja Fidler. Gated-scnn: Gated shape cnns for semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 5229–5238, 2019.\n\nHaoru Tan, Sitong Wu, and Jimin Pi. Semantic diffusion network for semantic segmentation. arXiv\n\npreprint arXiv:2302.02057, 2023.\n\nTakahiro Toyoda and Osamu Hasegawa. Random field model for integration of local information and global information. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(8): 1483–1489, 2008.\n\nMichael Van den Bergh, Xavier Boix, Gemma Roig, Benjamin de Capitani, and Luc Van Gool. Seeds: Superpixels extracted via energy-driven sampling. In European conference on computer vision, pp. 13–26. Springer, 2012.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nChi Wang, Yunke Zhang, Miaomiao Cui, Peiran Ren, Yin Yang, Xuansong Xie, Xian-Sheng Hua, Hujun Bao, and Weiwei Xu. Active boundary loss for semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 2397–2405, 2022.\n\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794–7803, 2018.\n\nDavid Weikersdorfer, Alexander Schick, and Daniel Cremers. Depth-adaptive supervoxels for rgb-d video segmentation. In 2013 IEEE International Conference on Image Processing, pp. 2708–2712. IEEE, 2013.\n\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077–12090, 2021.\n\nJunjie Yan, Yinan Yu, Xiangyu Zhu, Zhen Lei, and Stan Z Li. Object detection by labeling superpixels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5107– 5116, 2015.\n\nMinghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 191–207, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58555-6.\n\nChangqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context prior for scene segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12416–12425, 2020a.\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33: 5824–5836, 2020b.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nYuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene parsing. arXiv preprint\n\narXiv:1809.00916, 2018.\n\nYuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16, pp. 173–190. Springer, 2020a.\n\nYuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang. Segfix: Model-agnostic boundary refinement for segmentation. In European Conference on Computer Vision, pp. 489–506. Springer, 2020b.\n\nMingmin Zhen, Shiwei Li, Lei Zhou, Jiaxiang Shang, Haoan Feng, Tian Fang, and Long Quan. Learning discriminative feature with crf for unsupervised video object segmentation. In European Conference on Computer Vision, pp. 445–462. Springer, 2020a.\n\nMingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, and Long Quan. Joint semantic segmentation and boundary detection using iterative pyramid contexts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13666–13675, 2020b.\n\nShuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE international conference on computer vision, pp. 1529–1537, 2015.\n\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633–641, 2017.\n\nLanyun Zhu, Deyi Ji, Shiping Zhu, Weihao Gan, Wei Wu, and Junjie Yan. Learning statistical texture for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12537–12546, 2021.\n\nZhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xiang Bai. Asymmetric non-local neural networks for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 593–602, 2019.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 FORMULA DERIVATION\n\nFirstly, we give the gradient equation of ∇Wc mentioned in this paper:\n\n∂Y c k\n∂Wc According to it, we present the derivative of each term respectively:\n\n∂P c k\n∂Y c k\n\n∂Lk ∂Wc\n\n∂Lk ∂P c k\n\n∇Wc =\n\n=\n\n·\n\n·\n\neY m\n\nk ) + eY c\n\nk ) · eY c\n\nk − (eY c\n\n∂Lk ∂P c k\nk )2\n\n= −\n\n1 P c k\n\n,\n\n(( (cid:80)\n\nm̸=c\n\neY m\n\nk ) + eY c\n\nk )2\n\n=\n\n( (cid:80)\n\nm̸=c\n\nk\n\neY c k ) + eY c eY m\n\nk\n\n(( (cid:80)\n\nm̸=c\n\n∂P c k\n∂Y c k\n\n=\n\nand\n\n∂Y c k\n∂Wc\n\n= Fk .\n\n.\n\n− (\n\n( (cid:80)\n\nm̸=c\n\nk\n\neY c k ) + eY c eY m\n\nk\n\nFurther, it is worth noting that P c\n\nk is given by:\n\nP c\n\nk =\n\n( (cid:80)\n\nm̸=c\n\nk\n\neY c eY m k ) + eY c\n\nk\n\n.\n\nTo simplify Eq (18), we take Eq (20) into account. Thus, Eq (18) is formulated as:\n\n∂P c k\n∂Y c k\n\n= P c\n\nk − P c\n\nk · P c\n\nk = P c\n\nk · (1 − P c\n\nk ) .\n\nSo after integrating Eq (17), Eq (21), and Eq (19), Eq (16) can be formulated as:\n\n∇Wc =\n\n∂Lk ∂Wc\n\n= −\n\n1 P c k\n\n· P c\n\nk · (1 − P c\n\nk ) · Fk = −(1 − P c\n\nk ) · Fk .\n\nFinally, the class weights W ∗\n\nc in the next iteration will be updated: c = Wc − ∇Wc = Wc + (1 − P c W ∗\n\nk ) · Fk .\n\n(16)\n\n(17)\n\n)2 , (18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\nA.2 EXPERIMENT SETUP\n\nADE20K ADE20K (Zhou et al., 2017) is one of the most challenging benchmarks, containing 150 fine-grained semantic concepts and a variety of scenes with 1,038 image-level labels. There are 20210 images in training set and 2000 images in validation set.\n\nCityscapes Cityscapes (Cordts et al., 2016) has 5,000 images captured from 50 different cities. Each image has 2048 × 1024 pixels, which have high quality pixel-level labels of 19 semantic classes. There are 2,975 images in training set, 500 images in validation set and 1,525 images in test set. We do not use coarse data in our experiments.\n\nPascal Context PASCAL Context (Mottaghi et al., 2014) is a challenging scene understanding dataset, which provides the semantic labels for the images. There are 4, 998 images for training and 5, 105 images for validation on PASCAL Context dataset. In our experiment, the 59 most frequent categories are used for training.\n\nImplementation Details. The initial learning rate is set as 0.01 for both datasets. We employ a poly learning rate strategy where the initial learning rate is multiplied by (1 − iter/totaliter)0.9 after each iteration. We set training time to 80000 iterations for ADE20K and Pascal Context, and 180 epochs for Cityscapes. Momentum and weight decay coefficients are set as 0.9 and 0.0005, respectively. For data augmentation, we apply the common scale (0.5 to 2.0), cropping and flipping of the image to augment the training data. Input size for ADE20K dataset is set to 512 × 512, and 480 × 480 is for Pascal Context while input size for Cityscapes dataset is set to 832 × 832. The syncBN (Peng et al., 2018) is adopted in all experiments, and batch size on ADE20K and Pascal Context is set to 16 and it is set to 8 for Cityscapes.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA.3 COMPARISONS WITH RELATED METHODS ON CITYSCAPES\n\nTo further evaluate our proposed method, we thoroughly compare our approach with baseline and other traditional CRF-based methods, i.e., Vanilla-CRF and Joint-CRF on Cityscapes dataset. As shown in Table 5, E-CRF achieves the best result among all those methods, which outperforms the baseline model by 0.92% in mIoU and 3.81% in F-score respectively. Obviously, our method is more effective than both Vanilla-CRF and Joint-CRF.\n\nMethod\n\nResNet-50\n\nResNet-101\n\nF-score (%)\n\nmIoU (%)\n\nF-score (%)\n\nmIoU (%)\n\nDeeplabV3+ Vanilla-CRF Joint-CRF E-CRF (Ours)\n\n60.48 62.38 63.44 64.29\n\n79.54 79.65 (+0.11) 79.78 (+0.24) 80.35 (+0.81)\n\n61.94 63.46 64.43 65.57\n\n80.85 80.92 (+0.07) 81.05 (+0.20) 81.77 (+0.92)\n\nTable 5: Comparisons with baseline, Valina-CRF and Joint-CRF on Cityscapes val dataset.\n\nA.4 MORE ABLATION STUDIES\n\nA.4.1 DIFFERENT BOUNDARY REFINEMENT\n\nWe consider three typical methods inlcuding Segfix Yuan et al. (2020b), DecoupleSegNet Li et al. (2020b), and ABL 6 (Wang et al., 2022). They refine boundary segmentation via post-processing, improving boundary representation, and adding boundary allignment loss respectively. But all of them ignore the existence of BCWC issue, which may restrict their capability. In Table 4, we use DeeplabV3+ with ResNet101 as our baseline method. For Segfix, we use the official code (It uses HRNet as backbone) to boost the performance of DeeplabV3+. For DecoupleSegNet which is constructed based on DeeplabV3+, we also use the official code (It uses ResNet101 as backbone). All the models are trained on ADE20K for 80K iterations with batch size set to 16. When testing, we adopt the single-scale testing strategy (i.e., raw image) because using a single scale (i.e., raw images) when comparing with baselines or performing ablation studies is a traditional default setting in the semantic segmentation field. The goal is to eliminate the effect of other elements (e.g., image augmentation).\n\nIt is observed that all the methods enhance the performance and E-CRF achieves the highest mIoU and F-score. We speculate that this is because E-CRF has explicitly considered the BCWC problem and optimizes the class weights from both scale and direction aspects while refining boundary representation. It also indicates the importance of obtaining distinguishable class weights in semantic segmentation.\n\nTable 6: Comparisons with other boundary refining methods on ADE20K val dataset.\n\nMethod DeeplabV3+ (Chen et al., 2018a) SegFix (Yuan et al., 2020b) DecoupleSegNet (Li et al., 2020b) ABL (Wang et al., 2022) E-CRF\n\nmIoU (%) F-score (%)\n\n44.60 45.62 45.73 45.38 46.02\n\n16.15 18.14 18.02 -\n18.32\n\nA.4.2 COMPARISONS ON COMPUTATIONAL COSTS\n\nWe take DeeplabV3+ based on ResNet101 as the baseline model to perform the training time comparisons. Image size is set to 512 × 512 and all the experiments are conducted on 8 GeForce RTX 2080Ti GPUs with two images per GPU. The FLOPs, parameter size, and inference FPS are also reported in Table 7. We can find that our proposed E-CRF brings negligible extra costs over the baseline model. The cost difference between E-CRF and Joint-CRF is marginal. We also measure the time consuming of superpixel method (5ms), which is much smaller than that of inference (55ms).\n\n6The authors do not provide the code in their paper. Hence, we just report the result based on OCRNet (Yuan\n\net al., 2020a) in their paper.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 7: Comparisons on Computational costs on ADE20K dataset.\n\nMethod DeeplabV3+ Vanilla-CRF Joint-CRF E-CRF\n\nBackbone ResNet101 ResNet101 ResNet101 ResNet101\n\nTraining Time(s) 0.71 0.71 0.73 0.74\n\nFLOPs(G) 254.8 254.8 254.9 255.0\n\nParameters(M) 60.1 60.1 60.2 60.2\n\nFPS 19.75 1.86 19.04 18.32\n\nA.5 DISCUSSION\n\nIn this section, we discuss the difference between E-CRF and three related works including PCGrad (Yu et al., 2020b), OCNet (Yuan & Wang, 2018), and SegFix (Yuan et al., 2020b).\n\nDifference between Projecting Conflicting Gradients (PCGrad) and E-CRF: PCGrad and E-CRF are both gradient-based methods that focus on adjusting the gradient properly to optimize the learning process more effectively and efficiently. PCGrad is designed to mitigate a key optimization issue in multi-task learning caused by conflicting gradients, where gradients for different tasks point away from one another as measured by a negative inner product. If two gradients are conflicting, PCGrad alters the gradients by projecting each onto the normal plane of the other, preventing the interfering components of the gradient from being applied to the network. The idea behind PCGrad is simple and the method is effective. PCGrad is a task-level gradient optimization method, mainly focusing on conflicting gradients caused by multiple tasks during training (e.g., in semantic segmentation and depth estimation). E-CRF is a finer-grained pixel-level gradient optimization method. E-CRF mainly aims at mitigating the boundary-caused class weights confusion in semantic segmentation via adjusting class weights from both scale and direction.\n\nDifference between OCNet and E-CRF: OCNet uses self-attention to implement the object context pooling module. The object context pooling estimates the context representation of each pixeliby aggregating the representations of the selected subset of pixels based on the estimated dense relation matrix. Further, OCNet combines context pooling module with the conventional multi-scale context schemes including PPM and ASPP. In E-CRF, we follow the idea behind Conditional Random Filed and embed it from logit space to deep-feature space. We instance the unary function and reformulate the pairwise function with a convolutional-based kernel. The kernel takes raw image RGB value and relative position embeddings as inputs (See Eq (6)), which is different from self-attention that takes extracted deep features as inputs. We also maintain one special term in CRF called label compatibility and transfer it to feature compatibility (See Eq (7)). Such is missing in self-attention. Besides, we do not measure the similarity between superpixel center and boundary pixels. We simply leverage the local prior in superpixel and use it to guide deep feature averaging. The motivation is to suppress noise information. Finally, the motivation between OCNet and E-CRF is different. OCNet mainly focuses on integrating as much object context as possible while E-CRF explicitly targets on the BCWC problem and optimizes class weights from both scale and direction.\n\nDifference between SegFix and E-CRF: SegFix first encodes input image and predicts a boundary map and a direction map. Then SegFix uses the predicted boundary map and offset map derived from the direction map to correct the wrongly classified boundary pixels via internal points with high confidence. SegFix is beneficial for refining boundary segmentation and is served as a postprocessing method, thus lacking the capability to alleviate BCWC problem. Our method is derived from traditional CRF (a post-processing method) and can be regarded as a plug-and-play module. It can be easily integrated with other methods. Besides, our method extends the optimization flexibility for BCWC problem. Empirically, we have compared Segfix and E-CRF based on DeeplabV3+ in Table 6 (Please see A.4.1). E-CRF produces 46.02% for mIoU on ADE20K, outperforming SegFix (45.62%) by 0.4%. This indicates the importance of alleviating BCWC problem.\n\nA.6 PAIRWISE MESSAGE PASSING VISUALIZATION\n\nE-CRF takes an equivalent transformation to replace hand-designed Gaussian kernels in VanillaCRF with simple convolution operators for more flexible end-to-end optimization. The convolution operation involves two aspects. One is the appearance similarity and the other one is the relative position between pixels. As depicted in Fig 4(a), we take a pixel k in the stool for an example and\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nshow its relationship with other pixels in the image. Fig 4(b) and Fig 4(c) show its appearance similarity and cosine position embedding with other pixels respectively. It is clear that pixels share similar colors or close to the target pixel k tend to be highlighted. Subsequently, in Fig 4(d), we directly visualize the results of our pairwise message passing module ψf p defined in Eq.(5). We can find that ψf p becomes concentrated on the most relevant pixels compared with the pixel k, which verifies the reliability of our pairwise message passing design.\n\nFigure 4: Visualization of pairwise message passing module ψf p in E-CRF. (a) A target pixel k of the stool in the image. (b) The appearance similarity between other pixels and k. Pixels share the similar colors with k tend to be highlighted. (c) The visualization of the relative position between k p in E-CRF. ψf and other pixels. Pixels close to k achieve higher values. (d) The visualization of ψf focuses more on most relevant pixels compared to k.\n\np\n\nFigure 5: Visualization comparisons between our method and baseline on ADE20K validation set. (a) Images from ADE20K dataset. (b) Segmentation output from DeeplabV3+. (c) Segmentation output from our method. Obviously, compared with baseline, the results are segmented well by E-CRF. (d) Image labels.\n\n18\n\nTarget point(a)(b)(c)(d)kPublished as a conference paper at ICLR 2023\n\nB BCWC IN TRANSFORMER\n\nAre transformer-based models also suffering from Boundary-caused Class Weight Confusion? Is our method effective to transformer-based models? To answer these questions, we make a preliminary exploration in this section.\n\nB.1 OBSERVATIONS ON ADE20K\n\nFollowing the same idea in Fig.1(a) of this paper, we take Segformer (Xie et al., 2021) (a transformer-based segmentation model) as an example to train on ADE20K (Zhou et al., 2017) dataset. We count the number of adjacent pixels for each class pair and find a corresponding category that has the most adjacent pixels for each class. Then, we calculate the similarity of their class weights and depict it in Fig 6. X-axis stands for the number of adjacent pixels for each class pair in descending order, and Yaxis represents the similarity of their class weights. Blue line denotes Segformer while orange line denotes ECRF based on Segformer. As shown in Fig 6, two categories that share more adjacent pixels are inclined to have more similar class weights, while E-CRF effectively decreases the similarity between adjacent categories and makes their class weights more discriminative. These observations on transformer-based model are quite similar to previous results in CNN-based models. Apparently, transformer-based models are also suffering from Boundary-caused Class Weight Confusion.\n\nFigure 6: Class weight similarity on transformer-based model\n\nB.2 EFFECTIVENESS ON TRANSFORMER\n\nTo evaluate the effectiveness of our method, we take Segformer (Xie et al., 2021) (based on MiT-B5) as our transformer baseline and incoporate E-CRF into it. Experiments are conducted on ADE20K and Cityscapes datasets. Similarly, we also compare our method with other traditinal CRF-based methods, i.e., Vanilla-CRF and Joint-CRF 7. As shown in Table 8, E-CRF achieves the best result among all those methods, which surpasses the baseline model with up to 1.01% mIoU and 3.81% F-score improvements. By E-CRF relieves the disturbing gradients caused by the BCWC problem boost the overall segmentation performance and the boundary segmentation. Fig 6 also proves that E-CRF can decrease the inter-class similarity consistently which results in more discriminative feature representations.\n\nMethod\n\nADE20K\n\nCityscapes\n\nF-score (%)\n\nmIoU (%)\n\nF-score (%)\n\nmIoU (%)\n\nSegformer Vanilla-CRF Joint-CRF E-CRF (Ours)\n\n18.53 21.72 21.91 22.34\n\n49.13 49.36 (+0.23) 49.55 (+0.42) 50.14 (+1.01)\n\n62.42 64.06 64.93 66.05\n\n82.25 82.31 (+0.06) 82.41 (+0.16) 83.07 (+0.82)\n\nTable 8: Comparisons with baseline, Valina-CRF, and Joint-CRF on ADE20K and Cityscapes val datasets.\n\n7Note that they are also based on Segformer\n\n19\n\nAdjacent pixels number(descending order)0.050.100.15Class weight similaritybaselineoursPublished as a conference paper at ICLR 2023\n\nB.3 COMPARISONS WITH SOTA METHODS\n\nTo further verify the effectiveness, we compare our methods with other transformer-based SOTA methods with similar number of parameters (except SETR) for fair comparisons in both ADE20K and Cityscapes datasets. Multi-scale testing and left-right flipping strategies are adopted. As shown in Table 9, our method achieves the best results among all the SOTA methods in both ADE20K and Cityscapes datasets. Besides, our method also has the smallest number of parameters.\n\nTable 9: Comparisons with other transformer-based SOTA methods on ADE20K val dataset and Cityscapes val and test dataset.\n\nMethod\n\nBackbone\n\nmIoU(%)\n\nADE-val City-val City-test Params (M)\n\n82.2 -\n- 82.2 -\n- -\n\n82.5\n\n310M 121M 133M 85M 109M 112M 86M\n\n85M\n\nSETR (Zheng et al., 2021) UperNet UperNet SegFormer (Xie et al., 2021) UperNet DPT (Ranftl et al., 2021) Segmentor (Strudel et al., 2021)\n\nViT-L (307M) Swin-B (Liu et al., 2021) (88M) Twins-L (Chu et al., 2021) (99M) MiT-B5 (81M)\n\n50.20 49.65 50.20 50.22 XCiT-M24 (Chu et al., 2021) (84M) 48.40 48.34 50.08\n\nViT-B (86M) DeiT-B (86M)\n\n82.15 -\n- 83.48 -\n- 80.60\n\nE-CRF (Ours)\n\nMiT-B5 (81M)\n\n51.28\n\n83.7\n\n20",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors observe the Boundary-caused Class Weights Confusion (BCWC) problem in semantic segmentation. To solve this problem, they propose E-CRF to fuse the CRF into CNN networks for more effective optimization. The E-CRF owns two advantages: use CRF to purify the feature representation of boundary pixels; enables optimizing class weights from both scale and direction.  Moreover, they add the superpixel as an auxiliary to boost the performance. Experiment results on popular datasets like ADE20K and Cityscapes are provided.\n\n# Strength And Weaknesses\n\nStrength:\n1. Although it is well known that boundary pixels are difficult to classify, this paper views this problem from a new perspective – class weight. They identify this as BCWC problem and make statistics to prove that there is a positive relationship between class weight similarity and adjacent pixel number.  \n2. The authors use both theoretical derivation and figure to prove that the proposed E-CRF can adjust both the scale and direction of optimization, which makes the presentation clearer and convincing. \n3. The ablation study is good, fully studying the effects of various components like E-CRF and superpixel. \n\nWeaknesses:\n1. It is strange to use superpixel in this paper, which has limited relation to the BCWC problem (the motivation). The superpixel is more like an auxiliary module to boost the performance, since E-CRF alone is not enough for SOTA performance.   \n2. In the ablation study 4.2.1, there is no obvious advantages of E-CRF over joint-CRF, and the F-score gain is less than 0.3%, and mIoU gain is around 0.5%. Moreover, without superpixel. E-CRF is even worse than joint-CRF with ResNet101 backbone (45.47% vs 45.61%). \n\n3. In the Table 4, the comparison of different boundary refinement methods might be unfair. The backbone and test augmentation (e.g. multi-scale testing) of various methods are not specified. For example, in the ABL paper, they can achieve 52.40 with Swin-B and multi-scale testing. The Table 6 has the same problem.    \n4. The section 3.3 can only prove that E-CRF can adjust both the scale and direction of optimization but cannot prove that E-CRF can relief BCWC. There lacks a connection between optimization flexibility and BCWC problem.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n1. Most content of this paper is clear, and the graph is good for understanding. However, the motivation of superpixel is not clear, and why E-CRF can relief the BCWC problem is not well explained. \n2. The novelty is good but not outstanding. Both boundary problem and CRF are already studied for a long time in semantic segmentation. \n3. Not sure about reproducibility.\n\n# Summary Of The Review\n\nThis paper studied the boundary problem in semantic segmentation from a new perspective and observe the BCWC problem. They propose E-CRF and superpixel to solve the BCWC problem. However, the motivation of superpixel is unclear and the explanation for E-CRF is weak. Even worse, there are some unfair comparisons in the experiment, which makes the effectiveness of proposed method unconvincing. Based on these reasons, I think this paper is below the acceptance threshold. \nUpdate: From the response, the authors have addressed my key concern, and I would like to raise my rating.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nε-INVARIANT HIERARCHICAL REINFORCEMENT LEARNING FOR BUILDING GENERALIZABLE POLICY\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGoal-conditioned Hierarchical Reinforcement Learning (HRL) has shown remarkable potential for solving complex control tasks. However, existing methods struggle in tasks that require generalization since the learned subgoals are highly task-specific and therefore hardly reusable. In this paper, we propose a novel HRL framework called ε-Invariant HRL that uses abstract, task-agnostic subgoals reusable across tasks, resulting in a more generalizable policy. Although such subgoals are reusable, a transition mismatch problem caused by the inevitable incorrect value evaluation of subgoals can lead to non-stationary learning and even collapse. We mitigate this mismatch problem by training the high-level policy to be adaptable to the stochasticity manually injected into the low-level policy. As a result, our framework can leverage reusable subgoals to constitute a hierarchical policy that can effectively generalize to unseen new tasks. Theoretical analysis and experimental results in continuous control navigation tasks and challenging zero-shot generalization tasks show that our approach significantly outperforms state-of-the-art methods.\n\n1\n\nINTRODUCTION\n\nGoal-conditioned Hierarchical Reinforcement Learning (HRL) methods have shown remarkable potential to solve complex tasks(Nachum et al., 2018b)(Kim et al., 2021)(Costales et al., 2021), such as continuous control tasks that require long-horizon navigation (Li et al., 2021)(G ̈urtler et al., 2021). Goal-conditioned HRL uses subgoals to decompose the original task into several sub-tasks, training a high-level policy to output subgoals and a low-level policy that executes raw actions conditioned on the subgoals. Therefore, the performance of goal-conditioned HRL mainly relies on a well-designed subgoal space. In particular, in many complex realistic scenarios such as robotics, reusable subgoals are necessary for building a generalizable policy that is widely applicable to different tasks.\n\nPrevailing strategies usually utilize task-specific subgoals such as representations extracted from essential states or trajectories (Nachum et al., 2018a)(Li et al., 2021)(Jiang et al., 2019b), or select a subgoal space based on prior knowledge such as (a subspace of) the raw state space (Nachum et al., 2018b) (Zhang et al., 2020). Such subgoals depend on the specific task and thus often cannot be reused in different tasks. For instance, in a maze navigation task, a subgoal representing coordinates in a maze may not be reachable in another maze, although the two mazes can be similar. As a result, policies based on these subgoals cannot be reused in different tasks.\n\nTo construct reusable subgoals, a readily applicable choice is to use invariable abstract physical quantities as subgoals such as directions in a navigation task. These abstract subgoals are usually task-agnostic and thereby naturally reusable. However, how to build a generalizable policy based on these abstract subgoals remains a challenge due to the transition mismatch problem, which is common in HRL (Zhang et al., 2022) (Levy et al., 2018) and can lead to the non-stationary learning process. In general, the transition mismatch problem emerges when the high-level policy evaluates the subgoals with incorrect transition and rewards due to an inadequately trained low-level policy. To introduce the problem clearly, we will show it in a prevailing challenging task considered by prior HRL methods, i.e., a long-horizon maze navigation problem based on a legged robot, despite our idea is not limited to this particular task. Consider that the high-level produces a subgoal to go to position with coordinates (x, y), but the immature low-level policy reached (x′, y′). Then, the high-level will evaluate the subgoal by the incorrect reward from (x′, y′), which will lead to\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ninaccurate value estimation and even the collapse of the hierarchical policy learning process (Igl et al., 2020b)(Wang et al., 2021). Previous work proposes to alleviate the mismatch problem in single-task settings by refining the wrong transition, such as relabeling the wrong transition by the correct transition obtained from the replay buffer (Levy et al., 2018) (Nachum et al., 2018b) (Kim et al., 2021). However, these methods fail to solve the transition mismatch problem in the multi-task setting, where this problem can be exacerbated by the confusion of changeable tasks with the same abstract subgoals. As successful trajectories are not identical across different tasks, relabeling by sampled trajectories may aggravate the mismatch problem.\n\nIn this paper, we propose a novel HRL framework called ε-Invariant HRL to leverage task-agnostic abstract subgoals as well as alleviate the influence of the transition mismatch problem. We propose a method called ε-Invariant Randomization to inject controllable stochasticity into the low-level policy during the training of the high-level policy. The key idea is to see the mismatch as a kind of randomness and train the high-level policy to be adaptable to this randomness. We term the subgoals introduced with randomness ε-Invariant subgoals (the definition is in 3.1). Based on this framework, we propose a parallel synchronous algorithm from A2C Mnih et al. (2016) to evaluate the online policy and update the network by an expected gradient from multiple trajectories sampled with the same parameter instead of updating by a single gradient, which can alleviate the mismatch problem with the expected transition, reducing of influence of incorrect transitions and rewards. The parallel algorithm is proven to possess the potential to solve changeable scenarios and randomness in the environments (Hou et al., 2022)(Espeholt et al., 2018). After solving the mismatch problem, our HRL policy can leverage general task-agnostic subgoals and generalize even to unseen tasks.\n\nTo demonstrate the superiority of our HRL framework, we extend the widely-used benchmark based on (Duan et al., 2016)’s work using the MuJoCo (Todorov et al., 2012) simulator. These experiments are designed to control high-dimensional robots to solve long-horizon maze navigation tasks in different mazes with sparse rewards (see details in section 4.1). Some tasks are in a stochastic environment with changeable structures to check the robustness of policy. In these difficult tasks, our method achieves state-of-the-art results compared with the most advanced RL and HRL methods. Our method also shows novel abilities in generalization tasks, in which the structures of the mazes are unseen, even with unseen pre-trained robots. To the best of our knowledge, such complex generalization tasks can hardly be solved by previous methods, and we are the first to build an HRL policy that exhibits generalization capability across different mazes and different robots.\n\nIn summary, our contributions are three-fold:\n\n1. We devise an HRL framework for generalizing in high-dimensional controlling maze nav-\n\nigation tasks with a theoretical guarantee.\n\n2. We propose a randomization method for the transition mismatch problem in generalization\n\ntasks along with an algorithm that enables stable learning.\n\n3. We provide a new benchmark for evaluating the approaches for high-dimensional maze navigation tasks, and our method outperforms SOTA algorithms. To the best of our knowledge, we are the first to build policies that can generalize to such zero-shot navigation tasks with unseen mazes and unseen robots.\n\n2 PRELIMINARIES\n\nWe formulate the task in this paper as a goal-conditioned Markov decision process (MDP) (Sutton & Barto, 2018), defined as a tuple < S, G, A, P, R, γ >. S is the state space, A is the action space, and G is the goal space, which is a set of consistent invariant actions. In this paper we focus on mazenavigating tasks, so that we choose the relative displacement of directions “up, down, left, right” (or “x+, x−, y+, y−” in the coordinate system of the environment) as the goals (see details in Section 3.1). P is the transition probability matrix and P (s′|s, a) is the one-step transition probability. R(s, a) is the reward function, γ ∈ [0, 1) is the discount factor.\n\nGoal-conditional HRL. We consider the framework that consists of two hierarchies: a high level policy πH = π(g|s; θh) and a low-level policy πL = π(a|s, g; θl), where θh, θl is the parameter of the two policies parameterized by neural networks respectively. At a high-level timestep t, the high-level policy generates a high-level action, i.e. subgoal gt ∈ G by gt ∼ π(g|st; θh). The\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of Zero-shot Generalization Maze Task. From left to right is Maze-g2, Mazeg3, Maze-g1. Details are in section 4.1\n\nhigh-level policy gives a goal every k steps and the low-level policy executes the subgoal gt in k steps. Then the high-level policy receives the accumulative external rewards from the environment t = (cid:80)tk+k−1 rh i=tk R(si, ai) in k steps. The goal of the high-level is to maximize the expected return E[(cid:80)H t=0 γtrh\n\nt ] by driving the low-level policy.\n\nThe low-level policy receives additional intrinsic reward rl for efficient learning as follows: rl(stk, gt, atk, stk+1) = α · cos < gt, φ(stk+1) − φ(stk) > ·||φ(stk+1) − φ(stk)||\n\n(1)\n\nwhere gt = gt is an invariable direction vector of “{x+, x−, y+, y−}”, the φ is a coordinate extracting function form state s and α is a constant coefficient. The reward means that the further the agent goes towards the goal direction, the more reward can it obtain. Different from current HRL methods as done in (Kim et al., 2021)(Nachum et al., 2018b), we do not use the relative distance of specific coordinates as the intrinsic reward function but use the fixed distance towards invariable and orthogonal goal directions. By this reward function, the agent will learn to walk towards an abstracted direction according to the subgoal instead of towards a specific location, which preserves the generalization potential for different tasks.\n\n3 APPROACH WITH ANALYSIS\n\nMotivation. We hope to build an HRL policy to solve maze-navigation and generalization tasks of high-dimensional continuous controlling robots. The difficulty of these tasks is usually caused by large exploration space and sparse rewards. Thus we aim to build a subgoal setting, which can not only reduce the difficulty of exploration but also are reusable in different tasks. Consider that many previous RL works research on tabular mazes, such as the BabyAI platform (Chevalier-Boisvert et al., 2018), if the maze with continuous state space can be discretized into several blocks, the learning process will be more efficient than the original one. So that we set the subgoal space by abstracted task-agnostic subgoals like directions of ”front, back, left, right”. These subgoals mean the agent should move in the direction of a fixed step. In such setting, the maze is divided into blocks according to the distance of the movement in the high-level perspective. The high-level policy can make a decision on finite discrete space, which will significantly improve the exploration efficiency. Meanwhile, such subgoals are physically invariable so that can be reused in any maze. Policy based on these subgoals can also be general among tasks.\n\nFigure 2: HRL framework illustration. By the εinvariant subgoals, the high-level policy can adapt to different maze tasks and different pre-trained low-level robots.\n\nChallenges. Although the motivation is concise, it needs to overcome three challenges for implementation. (1) Will the hierarchical policy in our setting seriously break the performance of the original optimal policy? How to evaluate the error? Is the error controllable? (2) In HRL the fixed oracle subgoals setting with respective learning process often lead to collapse due to the mismatch of the two hierarchies . How to let the agent learn stably and safely and overcome the mismatch?\n\nIn the following paragraphs, we will answer the two questions respectively by theoretical analysis for discussing the effectiveness of our method, and framework building with algorithm design for stable policy learning.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.1 THEORETICAL ANALYSIS\n\nLimited by the length of the paper, in this section we just show the main idea of proof. The details can be seen in Appendix A.\n\nOur main idea to prove the effectiveness of our method is to prove the error between optimal policy and ours in a metric is bounded and can be controlled so that our methods can be used as a suboptimal policy to approximate the optimal one. The results in stochastic MDPs show that the error between our hierarchical policy and the optimal policy can be bounded.\n\nFirstly, we give the mathematical definition of the ε-invariant subgoals (as the high-level abstracted actions). Definition 3.1. (ε-invariant subgoals) For every environment E and an ε-invariant subgoal g∆ ∈ G∆, transitions of the subgoal from ’s’ to ’s′’ and ’s′’ to ’s′′’ with optimal low-level policy, for any s, satisfying:\n\nEs′[φ(s) − φ(s′)] = Es′′[Es′[φ(s′)] − φ(s′′)]\n\n(2)\n\nwhere σφ(s′),x, σφ(s′),y ≤ ε\n\nHere πH (g|s) is the high-level policy. s′ and s′′ are the achieved states by the ε-invariant subgoal g∆ with optimal low-level policy. φ is the coordinate extracting function and φ(s) is the coordinate vector of observation s. σφ(s′),x, σφ(s′),y are the variance of achieved state in x, y directions. We consider our high-level policy learning in stochastic MDP so that the s′ is a random variable depending on s and g∆, and we limit the variance of s′ by a little constant ε. The equation means such subgoals will give an excepted invariable direction in the coordinate system with a little randomness.\n\nError Bound in Stochastic MDP. As shown in Fig 3, the trajectories of RL method, traditional HRL method, and our method are different, and for that, we build our high-level policy in stochastic MDP with stochastic subgoals.\n\nWe consider a goal-conditional HRL where the high-level policy makes a decision every k steps. To compare with the HRL method, the optimal value function of original RL methods in deterministicMDP can be rewritten with k step as:\n\nk (st) = Eτk [R(τk) + γkV (St+k)] where τk is a trajectory in k steps with optimal policy π∗(τk|st), R(τk) is the accumulative discounted reward in k steps. And equation 3 can be rewritten as: V ∗ P π(τk|st)[R(τk) + γkV (St+k)].\n\nk (st) = (cid:80)\n\nV ∗\n\n(3)\n\nτk\n\nWith equation 3, we can define the error between traditional RL method and our method: |V ∗ ˆV ε with coordinates as subgoals:\n\nH (st)|. To analyze and control the error, we introduce the value V ∗\n\nk (st) − H (st) in optimal stochastic HRL\n\n|V ∗\n\nk (st) − ˆV ε\n\nH (st)| ≤ |V ∗\n\nk (st) − V ∗\n\n(cid:124)\n\n(cid:123)(cid:122) HRL error\n\nH (st)| (cid:125)\n\n+ |V ∗ (cid:124)\n\nH (st) − ˆV ε\n\n(cid:123)(cid:122) subgoal error\n\nH (st)| (cid:125)\n\nHere we only give the conclusion of our analysis, that is:\n\n||V ∗\n\nk − V ∗\n\nH ||∞ ≤\n\nνkkRmax 2(1 − γk)\n\n+\n\nγkνkRmax 2(1 − γk)2\n\nand\n\n||V ∗\n\nH − ˆV ε\n\nH ||∞ ≤\n\nLφ(2δmax + 3\n\n√\n\n2ε)Rmax\n\n2(1 − γk)\n\n(k +\n\nγk 1 − γk )\n\n(4)\n\n(5)\n\n(6)\n\nwhere δmax = max g,g∆\n\n{||g||, ||g∆||}, Rmax is the bound of reward function, νk is the transition mis-\n\nmatch rate between optimal RL method and HRL method with stochastic coordinate subgoals. Lφ is the Lipschitz constant. The proof is shown in Appendix A.\n\nThe result shows that the error between our method and optimal policy indeed can be bounded and controlled. That means our policy can be used as a sub-optimal policy to approximate the original optimal one.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 FRAMEWORK\n\nIn this section, we will show how to build a hierarchical framework to handle the abstracted εinvariant subgoals. Generally, the high-level policy decides the direction as subgoals by pixel observation, and the low-level policy receives the expected direction and walks towards the direction for a distance. Thanks to the abstractness of the subgoals defined in 3.1, although the high-level and the low-level policy can learn together, we train them respectively to accelerate the learning process. So that our method decomposes the learning process into two stages with discrete general subgoals, where the high level learns by external rewards and the low level learns by intrinsic rewards. Essentially, the discrete general subgoals are invariable displacement of fixed directions, which will reduce the difficulty of exploration and improve learning efficiency by discretizing the state space into little blocks. Meanwhile, they are reusable and general in any maze-navigation tasks, so as to preserve the generalization abilities of the high-level policy.\n\nModular Pixel Observation. As we utilize the direction as abstracted subgoals for the high-level policy, the influence of the subgoals can hardly be perceived by the agent with the posture of the robot. We add a pixel observation at the top view to look down at the agent. (See in figure 2) It will observe a region of fixed size by a camera following the robot. From this perspective, the influence of the subgoal of directions is invariable and consistent among maze environments. The agent can observe the change in the environment close to the robot. Meanwhile, pixel observation is more adaptable than raw data of posture, which can improve generalization abilities. We provide the pixel observation for the high-level policy to decide which direction to go.\n\nε-Invariant Randomization. As we train the two hierarchies respectively with abstracted subgoals, we train the high-level by the movement directly from the simulator instead of the real walking of the robot for faster learning. However, abstracted subgoals will bring the mismatch problem between the two levels. Because to control a robot moving by legs strictly towards a direction is very hard for RL methods. So that even the well-learned walk policy will contain a slight deviation in the vertical direction of the goal direction. Such inevitable deviation (a performance of mismatch) will cause incorrect evaluation of the subgoals, which will lead to nonstationarity and even collapse in a changeable environment.\n\nFigure 3: Comparison of different frameworks. ε-invariant subgoals (invariable directions with little randomness) can be reused in different navigating tasks.\n\nTo alleviate the problem, we introduce the ε-Invariant Randomization method to build the high-level learning process as a stochastic MDP. When training the high-level policy, the simulator moves the robot with random postures and random positions. The postures are sampled from the walking postures of well-learned low-level policy with random subgoals and recorded as offline data. The random position means that a random deviation will be added to the original movement. For instance, the high-level policy gives a “x+” direction as subgoals, then the simulator will move the robot to “x+” with a little offset of ∆x, ∆y ∼ N (0, σ), where σ ≤ ε is much less than the moving distance. As a result, if the high-level policy can overcome the randomness, the wrong execution of the low-level policy will be seen as a sample of the distribution of randomness, which will improve the affordance of the high-level policy.\n\nNetwork structure and more details can be seen in Appendix C.\n\nAlgorithm. As we introduce randomness into the learning process to alleviate the mismatching problem, the high level should overcome the stochasticity in the environment to obtain a wellperformed policy. To solve the randomness, we propose a parallel algorithm called the parallel expected gradient advantage actor-critic (PEG-A2C) algorithm because the parallel algorithm is proven to possess the potential to adapt to a changeable environment (Hou et al., 2022)(Espeholt\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: An illustration of the shape of maze environments of our benchmark. They are Random Square, ⊃ Maze, S-shaped Maze, and Spiral Maze respectively. The blue arrow is the successful trajectory.\n\net al., 2018). The main idea of our algorithm is to sample multiple trajectories with the same parameter by several agents, then concentrate the trajectories and calculate the expected gradients of different trajectories and different steps and update the parameters synchronously. That is, different from previous works which update parameters θ by θ ← θ + α∇θ, we utilize θ ← θ + αEτ [∇θ(τ )] to train the high-level policy. Such gradients can hopefully reduce the impact of randomness, as taking the expectation can be seen as a special kind of automatic gradient clipping by tasks (Zhang et al., 2019)(Bjorck et al., 2021). That is our algorithm evaluates subgoals by expected return and high-level transition. It will alleviate the non-stationarity of the learning process.\n\nThe pseudo code is shown in Appendix B.\n\n4 EXPERIMENTS\n\nIn our experiments we aim to answer the following questions: (1) Can our method learn stably in different complex maze navigation by a robot with high-dimensional action space? (2) How does our method perform compared with advanced RL and HRL algorithms? (3) Can our method generalize to different unseen maze tasks even unseen robots without retraining?\n\n4.1 ENVIRONMENTS SETTING\n\nWe evaluate a suite of MuJoCo (Todorov et al., 2012) maze-navigating tasks modified from the benchmark from (Duan et al., 2016). Different from the setting of (Kim et al., 2021)(Nachum et al., 2018b), we utilize sparse external rewards to make them more challenging. In these tasks, the agent should control an ant robot to achieve a specified area in the maze which is far away from the initial position. To compare completely, we also add experiments on dense reward, where the reward is 1/(1 + d) of Euclidean distance d between the agent and the current goal in the coordinate system for every step. The mazes are Ant ⊃-Maze, Ant Random Square Maze, Ant S-shaped Maze, Ant Spiral Maze, Ant Spiral Maze, and Generalization Maze (See in Fig 4). They are all difficult mazes with the long navigating horizon. In these mazes, the goal is to pass the door or go to a specific region. Especially Ant Random Square Maze is a task in a stochastic environment with random initial positions of both the agent and the door. Generalization Maze is a task with three unseen mazes of different structures. Ant S-shaped Maze and Ant Spiral Maze are extremely long-horizon mazes that require at least thousands of steps by the optimal policy. More details can be seen in Appendix D.\n\nImplementation. We build our HRL agent by two policies. The high-level policy is learned by our PAG-A2C algorithm, and the low-level policy is a goal-conditioned policy modified from DroQ (Hiraoka et al., 2021) algorithm, of which the subgoals are directions of ’x+’,’y+’,’x-’,’y-’. As the simulator in MoJoCo allows direct movement of coordinates, we train the two policies respectively in two stages. The high-level policy learns with direct movement with randomness, and the low-level learns by intrinsic reward defined in equation 1. As the subgoals and the low-level policy can be reused in different tasks, we mainly show the high-level training curves and success rates in different tasks and use the same well-learned low-level policy for the ant robot.\n\n4.2 BASELINES\n\nWe compare our method to several state-of-the-art (SOTA) model-free RL and HRL algorithms in the high-dimensional continuous control tasks said above.\n\nDroQ. It is the SOTA RL algorithm for high-dimensional continuous control tasks (Hiraoka et al., 2021), which is the most efficient RL method. It is effective for both simple robots like Hopper and complex robots with large numbers of degrees of freedom like Humanoid.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Comparative experiment results with strong baselines in sparse-reward tasks. The mean and variance are calculated by 3 runs.\n\nHIGL. It is the SOTA HRL algorithm for high-dimensional maze-navigating tasks with both sparse rewards and dense rewards for MuJoCo suite (Kim et al., 2021).\n\nHESS. It is one of the most advanced subgoal learning HRL methods for continuous control tasks (Li et al., 2021).\n\nRAND-H. It is a variety of our HRL method, of which the high-level policy is a random policy and the low-level policy is well-learned. The baseline is used as an ablation study of our method to show the capabilities of our subgoal setting.\n\nour-oracle. It is a variety of our HRL method, of which the high-level policy is trained without the low-level policy. The robot will walk by the oracle movement of coordinates executed directly by the simulator instead of the low-level policy. The baseline is also used as an ablation study of our method to show the efficiency of the high-level policy.\n\n4.3 RESULTS OF COMPARATIVE EXPERIMENTS\n\nFor a fair comparison, we utilize the evaluated reward curves with both the high-level and the lowlevel, although our hierarchical policies can be trained respectively. The curves are shown in figure 5. We can see that our method outperforms both the SOTA RL and HRL methods. The curves are cut by the same episode to align the results of different methods. All the curves are smoothed by a sliding window. More details can be seen in Appendix D.\n\nRobustness to Stochasticity and Mismatch. In these tasks, the ’Ant Random Square’ is a square maze with a random initial position of robots and goals. In this task, the mismatch problem will become critical as the successful trajectory is episodically changeable. So that correction by historical samples is invalid. As shown in the curve in figure 5, the HIGL method can obtain a reward at first, but gradually reduced it with a downward trend. Our method can learn with a gradually increasing return. The curve shows that our method can adapt to such a stochastic environment and learn stably, proving that our algorithm indeed can overcome randomness. But other methods can hardly adapt to such a stochastic environment.\n\nComparative Result. The non-hierarchical method DroQ performs poorly in all the tasks with sparse rewards, demonstrating the strength of the hierarchical structures in solving long-horizon tasks with sparse rewards. HESS method also performs not so well with little accumulate return, due to the requirement of large exploration episode steps for sparse reward, which is consistent with the results reported in their paper (Li et al., 2021). The SOTA HIGL method outperforms other baselines while underperforming our method, which learns with a gradually rising average reward but slower than ours, showing the superiority of our methods.\n\nIn tasks with dense rewards (figure 6), we utilize average step reward and goal-reaching success rate to evaluate the performance of these methods. The performance of previous methods is improved a lot but the curves of success rate are still under our method.\n\nAblation Study. The RAND-H and our-oracle show the ablation result of our method. The highlevel policy with oracle movement (our-oracle) can learn stably with our algorithm. We can see that the low-level policy (RAND-H) can obtain rewards in a certain frequency with our subgoal setting even with random high-level policy, so as to preserve stable feedback for the learning process. The two ablation experiment curves show the reason why our method learns faster.\n\n7\n\n26675333800000.20.40.60.81Random-square20004000600000.511.52-shape13332667400000.511.522.53S-shape 66713332000Episodes012345Average RewardSpiral HiglourDroQHessour-oracleRAND-HUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 6: Comparative experiment results in dense-reward tasks. (a) is curve of average reward of every step. (b) is curve of average success rate = achieved goals/total goals\n\nTable 1: Generalization Task Result for Different Mazes (Zero-shot Success Rate %). ’Maze-ori’ is the trained maze, and the other three are zero-shot generalization maze .\n\nMethod/Maze Maze-ori\n\nMaze-g1\n\nMaze-g2\n\nMaze-g3\n\nOurs Ours-Oracle HIGL\n\n51.8 ± 4.4 66.4 ± 5.4 25.4 ± 5.3\n\n45.6 ± 5.2 72.0 ± 6.3 ≤ 1\n\n27.8 ± 6.7 58.4 ± 8.3 ≤ 1\n\n73.6 ± 4.6 84.8 ± 6.6 ≤ 1\n\n4.4 RESULTS OF ZERO-SHOT GENERALIZATION EXPERIMENTS\n\nZero-shot Maze. In these experiments, we utilize the policy trained in ’Ant Square Maze’ and test in new unseen mazes of fixed shapes without retraining. We compare our method with the HIGL method by the well-performed policy of training process in the Ant Random Square environment with sparse reward. Result is shown in table 1. The test mazes are fixed shapes without randomness. so that sometimes the test result can be better than the training result.\n\nZero-shot Robot. In this experiment, we test the single high-level policy with the oracle movement and the whole HRL policy respectively. In table 2 we show zero-shot result of the mere high-level policy in ’Maze ⊃ shape’. It is to show the adaptive capabilities of our high-level policy with our subgoal setting.\n\nFrom the results, we can see that such generalization tasks can hardly be solved by previous RL and HRL methods due to the specific subgoal or goal setting. They can neither adapt to different maze tasks nor change the low-level policy without retraining the high-level policy. Our HRL method uses invariable abstracted subgoals and can adapt to different scenarios. Our method is also flexible and can adapt to different unseen robots with the low-level policy receiving the same subgoals with a certain success rate.\n\nVisualization Results. To show the generalization ability of our HRL policy, we show the heatmap of trajectories of our method and HIGL (figure 7). We can see that the HIGL agent mainly moves near the initial position and is blocked by unseen structures, but our agent can achieve the door more frequently.\n\nTable 2: Generalization Task Result for Different Robot (Zero-shot Success Rate %). Except ’Ant’, the other robot are unseen. Average success rate = achieved goals/total goals\n\nMethod/Robot\n\nAnt\n\nPoint\n\nSwimmer Humanoid\n\nOurs-Oracle HIGL\n\n99.6 ± 0.5 67.0 ± 5.2\n\n53.2 ± 1.4 —\n\n47.0 ± 2.4 —\n\n48.2 ± 1.3 —\n\n5 RELATED WORK\n\nGoal-conditioned HRL methods. In the recent few years, hierarchical reinforcement learning methods have been widely studied for complex high-dimensional control and long-horizon tasks. There are many goal-conditioned HRL methods for subgoals learning, handling and discovery for these difficult tasks (Nachum et al., 2018b)(Li et al., 2021)(Kim et al., 2021)(G ̈urtler et al., 2021)(Li et al., 2020)(Zhang et al., 2020). These methods usually leverage task-specific subgoals such as spe-\n\n8\n\n40008000120000.10.120.140.160.180.20.220.240.26-shape-dense2666533480000.10.120.140.160.180.20.220.240.26Average Step RewardS-shape-dense Higlourour-oracle20004000600000.20.40.60.81-shape-dense133326674000Episodes00.20.40.60.81Average Success Rate %S-shape-dense Higlourour-oracleUnder review as a conference paper at ICLR 2023\n\nFigure 7: Visualization result of zero-shot maze generalization tasks. The red and blue points represent the achieved positions in the maze. The more lightly the position is, the more frequently the agent achieving. The histogram is to straightly show the frequency of the achievement of every position.\n\ncific coordinate (Nachum et al., 2018b), sampled abstracted or original states (Nachum et al., 2018a) or abstracted trajectory descriptions (Jiang et al., 2019a) to complete complex tasks. Different from previous methods, our framework utilizes the ε-invariant subgoals (direction with randomness) as abstracted high-level actions, which do not stand for task-specific descriptions but the invariable relative effects of action sequences. Such subgoals can be reused ignoring the change of states in different tasks, which are more general.\n\nGeneralizable Policy Learning. Constructing policies can be reuse (or generalize) in new (even unseen) tasks is a long-standing challenge, which is researched by many works in different perspectives (Igl et al., 2020a)(Xu et al., 2022)(Wang et al., 2019)(Wang et al., 2020). Recent works are mainly three lines for generalizable policy learning. (1) Learning reusable or transferable skills, option or policies for different tasks(Shah et al., 2021) (Nam et al., 2021)(Klissarov & Precup, 2021). (2) Learning policies adapting to a distribution of environments or tasks with different dynamics by risk-sensitive objective functions(Lyle et al., 2022) (Lei & Ying, 2020)(Kirsch et al., 2019). (3) Learning to extract reusable abstracted representations like language, logical symbols or graph from visual observation or state-action trajectories (Jain et al., 2020)(Agarwal et al., 2020)(Vaezipoor et al., 2021).\n\nOur method is related to the third line, i.e., utilize the abstracted representations for improving the generalization abilities of the policy. But the difference is that we do not leverage representations expressing tasks in a distribution or invariable abstracted trajectories. Instead, we use the abstracted representation to represent the physically invariable relative locomotion. Such representations are usually general in the kind of maze-navigation tasks, thus leading to strong generalization abilities.\n\n6 CONCLUSION\n\nIn this paper, we propose a novel HRL framework to build a generalizable hierarchical policy with abstract task-agnostic subgoals. We give theoretical analysis to prove the effectiveness of our method and design algorithm to overcome the transition mismatch problem in generalization tasks to construct a stable policy learning process. Strong results in challenging experiments show the superiority of our method. Meanwhile, our method can achieve zero-shot generalization in different unseen tasks, which cannot be dealt with by previous methods. We believe that idea of our method could be used beyond our task setting. For future work, we will focus on more general policy learning to solve more complex tasks, not limiting to HRL.\n\nREFERENCES\n\nRishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive behavioral similarity embeddings for generalization in reinforcement learning. In International\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nConference on Learning Representations, 2020.\n\nJohan Bjorck, Carla P Gomes, and Kilian Q Weinberger. Is high variance unavoidable in rl? a case study in continuous control. In International Conference on Learning Representations, 2021.\n\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. In International Conference on Learning Representations, 2018.\n\nRobby Costales, Shariq Iqbal, and Fei Sha. Possibility before utility: Learning and using hierarchical\n\naffordances. In International Conference on Learning Representations, 2021.\n\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International conference on machine learning, pp. 1329–1338. PMLR, 2016.\n\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 1407–1416. PMLR, 2018.\n\nFlorin Gogianu, Tudor Berariu, Mihaela C Rosca, Claudia Clopath, Lucian Busoniu, and Razvan Pascanu. Spectral normalisation for deep reinforcement learning: an optimisation perspective. In International Conference on Machine Learning, pp. 3734–3744. PMLR, 2021.\n\nNico G ̈urtler, Dieter B ̈uchler, and Georg Martius. Hierarchical reinforcement learning with timed\n\nsubgoals. Advances in Neural Information Processing Systems, 34:21732–21743, 2021.\n\nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka. Dropout q-functions for doubly efficient reinforcement learning. In International Conference on Learning Representations, 2021.\n\nXiaohan Hou, Zhenyang Guo, Xuan Wang, Tao Qian, Jiajia Zhang, Shuhan Qi, and Jing Xiao. Parallel learner: A practical deep reinforcement learning framework for multi-scenario games. Knowledge-Based Systems, 236:107753, 2022.\n\nMaximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. In International\n\nTransient non-stationarity and generalisation in deep reinforcement learning. Conference on Learning Representations, 2020a.\n\nMaximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. In International\n\nTransient non-stationarity and generalisation in deep reinforcement learning. Conference on Learning Representations, 2020b.\n\nAyush Jain, Andrew Szot, and Joseph Lim. Generalization to new actions in reinforcement learning.\n\nIn International Conference on Machine Learning, pp. 4661–4672. PMLR, 2020.\n\nYiding Jiang, Shixiang Shane Gu, Kevin P Murphy, and Chelsea Finn. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019a.\n\nYiding Jiang, Shixiang Shane Gu, Kevin P Murphy, and Chelsea Finn. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019b.\n\nJunsu Kim, Younggyo Seo, and Jinwoo Shin. Landmark-guided subgoal generation in hierarchical reinforcement learning. Advances in Neural Information Processing Systems, 34:28336–28349, 2021.\n\nLouis Kirsch, Sjoerd van Steenkiste, and Juergen Schmidhuber. Improving generalization in meta reinforcement learning using learned objectives. In International Conference on Learning Representations, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMartin Klissarov and Doina Precup. Flexible option learning. Advances in Neural Information\n\nProcessing Systems, 34:4632–4646, 2021.\n\nYunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated\n\nobjective functions. In International Conference on Learning Representations, 2020.\n\nAndrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierarchies\n\nwith hindsight. In International Conference on Learning Representations, 2018.\n\nSiyuan Li, Lulu Zheng, Jianhao Wang, and Chongjie Zhang. Learning subgoal representations with\n\nslow dynamics. In International Conference on Learning Representations, 2020.\n\nSiyuan Li, Jin Zhang, Jianhao Wang, Yang Yu, and Chongjie Zhang. Active hierarchical exploration with stable subgoal representation learning. In International Conference on Learning Representations, 2021.\n\nClare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, and Yarin Gal. Learning dynamics and generalization in deep reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 14560–14581. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr. press/v162/lyle22a.html.\n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016.\n\nOfir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation learning In International Conference on Learning Representa-\n\nfor hierarchical reinforcement learning. tions, 2018a.\n\nOfir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical\n\nreinforcement learning. Advances in neural information processing systems, 31, 2018b.\n\nTaewook Nam, Shao-Hua Sun, Karl Pertsch, Sung Ju Hwang, and Joseph J Lim. Skill-based meta-\n\nreinforcement learning. In International Conference on Learning Representations, 2021.\n\nDevavrat Shah, Dogyoon Song, Zhi Xu, and Yuzhe Yang. Sample efficient reinforcement learning via low-rank matrix estimation. Advances in Neural Information Processing Systems, 33:12092– 12103, 2020.\n\nDhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander T Toshev, Sergey Levine, et al. Value function spaces: Skill-centric state abstractions for long-horizon reasoning. In International Conference on Learning Representations, 2021.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026–5033. IEEE, 2012.\n\nPashootan Vaezipoor, Andrew C Li, Rodrigo A Toro Icarte, and Sheila A Mcilraith. Ltl2action: Generalizing ltl instructions for multi-task rl. In International Conference on Machine Learning, pp. 10497–10508. PMLR, 2021.\n\nHuan Wang, Stephan Zheng, Caiming Xiong, and Richard Socher. On the generalization gap in reparameterizable reinforcement learning. In International Conference on Machine Learning, pp. 6648–6658. PMLR, 2019.\n\nRundong Wang, Runsheng Yu, Bo An, and Zinovi Rabinovich. I2hrl: Interactive influence-based hierarchical reinforcement learning. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 3131–3138, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. In International Conference on Learning Representations, 2020.\n\nMengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot policy generalization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 24631–24645. PMLR, 17–23 Jul 2022. URL https: //proceedings.mlr.press/v162/xu22g.html.\n\nJingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations, 2019.\n\nTianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, and Feng Chen. Generating adjacencyconstrained subgoals in hierarchical reinforcement learning. Advances in Neural Information Processing Systems, 33:21579–21590, 2020.\n\nTianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, and Feng Chen. Adjacency constraint for efficient hierarchical reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA THEORETICAL ANALYSIS\n\nWith equation 3, we can define the error between traditional RL method and our method: |V ∗ ˆV ε with coordinates as subgoals:\n\nH (st)|. To analyze and control the error, we introduce the value V ∗\n\nk (st) − H (st) in optimal stochastic HRL\n\n|V ∗\n\nk (st) − ˆV ε\n\nH (st)| ≤ |V ∗\n\nk (st) − V ∗\n\n(cid:124)\n\n(cid:123)(cid:122) HRL error\n\nH (st)| (cid:125)\n\n+ |V ∗ (cid:124)\n\nH (st) − ˆV ε\n\n(cid:123)(cid:122) subgoal error\n\nH (st)| (cid:125)\n\n(7)\n\nso that we can control the error by two parts: (1) the error from RL policy to stochastic HRL policy and (2) the error from ordinates subgoals to ε-invariant subgoals.\n\nHRL Error. For the former (denote as eH = |V ∗ H (st)|), the error bound can be controlled by the corollary of conclusion of Zhang’s work (Zhang et al., 2022). We firstly formalize a critical factor termed transition mismatch rate between RL policy and stochastic HRL policy. (The difference between our work and Zhang’s is that the factor in our work represents the difference between RL and HRL policy, but the difference between two HRL methods in Zhang’s work.)\n\nk (st) − V ∗\n\nDefinition A.1. (k-step transition mismatch)\n\nFor a goal-conditioned MDP M with P k(st+k|st, gt) as its k-step transition probability under an optimal goal-conditioned policy πH (gt|st), the k-step transition mismatch rate of M is defined as:\n\nνk ≜ max\n\nst,gt,st+k\n\n|P π(τk|st) − πH (gt|st)P k(st+k|st, gt)|\n\n(8)\n\nThe mismatch rate νk captures the difference between RL policy and HRL policy by the part of trajectories in k steps. The less uncertainty the high-level policy performs with, the less νk is. The deterministic-MDP of high-level policy will lead to νk = 0. Then for error eH , we have the conclusion as follows:\n\nLemma A.2. (Corollary of Theorem 2 in (Zhang et al., 2022)) With discounted factor γ, bounded reward function Rmax = maxs,a R(s, a) and νk defined in equation 8, there is\n\n||V ∗\n\nk − V ∗\n\nH ||∞ ≤\n\nνkkRmax 2(1 − γk)\n\n+\n\nγkνkRmax 2(1 − γk)2\n\n(9)\n\nProof. See Appendix.A.1\n\nLemma A.2 means the error between RL method and stochastic HRL method will be controlled by νk. If the distribution of the randomness of subgoals is in a little region with little variance, the HRL policy can approximate the optimal RL policy with controllable error.\n\nH (st)− ˆV ε\n\nSubgoal Error. To analyses the error caused by our subgoals (the latter error is denoted as eS = |V ∗ H (st)| ), we should introduce a new metric. We first consider the actually reached region caused by traditional coordinate subgoals and our ε-invariant subgoals. If the region and trajectories of the two HRL policies are close to each other in the coordinate system, to some extent, that means the policy and transition are similar. On the other hand, to measure the influence of the traditional subgoals and the ε-invariant subgoals, the distance and trajectories in the coordinate system can directly reflect the difference between them. Thus, we assume that:\n\nAssumption A.3. (Lipschitz condition) In the k-step reachable region under an optimal goalconditioned policy, for a traditional goal-conditioned MDP M with subgoal gt and P k(st+k|st, gt) as its k-step transition probability, as well as an Mε with ε-invariant subgoals g∆ and transitions P k(st+k|st, g∆), there is:\n\n|P k(st+k|st, gt) − P k(s′\n\nt+k|st, g∆)| ≤ Lφ||φ(st+k) − φ(s′\n\nt+k)||\n\n(10)\n\nwhere Lφ is the Lipschitz constant, φ is the coordinate extracting function defined above.\n\nThe Lipschitz condition is common and widely used in many other RL or HRL works (Wang et al., 2019)(Shah et al., 2020)(Gogianu et al., 2021). Assumption A.3 means that, if the low-level policy learns well and can execute the subgoals given by the high-level policy, the difference between the\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\ntwo HRL policies can be measured by the Euclidean distance of displacement in the coordinate system. The distance is in the reachable region with optimal low-level policy in k steps, from the same start state st. If the reached states are closed, the policies are considered similar.\n\nBy the setting above, we have the following theorem which provides a suboptimality upper bound for error eS: Theorem A.4. With discounted factor γ, bounded reward function Rmax = maxs,a R(s, a), Lipschitz constant Lφ, variance bound ε defined above, with a high probability there is\n\n||V ∗\n\nH − ˆV ε\n\nH ||∞ ≤\n\nLφ(2δmax + 3\n\n2ε)Rmax\n\n√\n\n2(1 − γk)\n\n(k +\n\nγk 1 − γk )\n\n(11)\n\nwhere δmax = max g,g∆\n\n{||g||, ||g∆||}. ||g|| and ||g∆|| are the distance of the relative displacement from\n\nany st to st+k by subgoal gt and g∆ respectively.\n\nProof. See in Appendix. A.2\n\nThe theorem A.4 means that the error between the traditional HRL method with coordinates as subgoals and our method can be constrained by the similarity metric of the two policies. By the conclusion above, our method has the theoretical guarantee to approximat to the original optimal policy with a controllable error bound. Meanwhile, in our methods, the subgoals can be reused in different maze environments, which leads to superiority in generalization tasks.\n\nA.1 PROOF OF LEMMA 1\n\nProof. Consider a state st ∈ S, for the value function of k step:\n\nV ∗\n\nk (st) =\n\n(cid:88)\n\nτk\n\nP π(τk|st)[R(τk|st) + γkV ∗(St+k)]\n\n(12)\n\nwhere P π(τk|st) = P π(st+k|st) is the k-step transition probability with policy π, R(τk|st) = R(st+k, st) = (cid:80)t+k−1 γiR(si) is the bounded accumulative return in τk, τk is the possible k-step trajectory by optimal policy π∗ executing k times. The optimal policy should maximize the external reward from the environment:\n\ni=t+1,si∈τk\n\nπ∗ = arg max\n\nπ\n\nE(s,a)∼π[\n\nT (cid:88)\n\nt=1\n\nγtR(st, at)]\n\nequation 12 can be rewritten as vector form for any state st ∈ S, i.e.:\n\nV ∗\n\nk (st) =\n\n(cid:42)\n\n(cid:88)\n\nτk\n\nP π(st), R(st) + γkV ∗\n\n(cid:43)\n\n(13)\n\n(14)\n\nwhere ⟨·, ·⟩ is the inner product of vectors, P π(st) ∈ [0, 1]|S|, ∀s ∈ S, ||P π(st)||1 = 1 denote the probablistic distribution vector of the k-step trajectory starting from st under the policy π∗. R(st) ∈ [0, kRmax]|S| is the accumulative rewards vector of k-step of the k-step trajectory. V ∗ is the optimal value vector function of all the states.\n\nSimilarly, value of HRL method with stochastic subgoals can be written as follow:\n\nV ∗\n\nH =\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)\n\n(cid:88)\n\nst+k∈S\n\nP k(st+k|st, gt)[R(st+k, st) + γkV ∗\n\nH (St+k)]\n\n=\n\n(cid:42)\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)P k(gt, st), R(st) + γkV ∗\n\nH\n\n(cid:43)\n\n(15)\n\nwhere πH (gt|st) is the high-level policy to choose subgoal gt, P k(st+k|st, gt) is the k-step transition probability of the low-level policy with subgoal gt. R(st+k, st) is also the k-step accumulative rewards. P k(st, gt) is the probablistic distribution vector of the k-step trajectory starting from st with subgoal gt.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nDenote that ̄PH (st, gt) ≜ (cid:80)\n\ng∈G πH (gt|st)P k(gt, st), so that definition A.1 can be rewritten as:\n\nνk = max st,gt\n\n||P π(st) − ̄PH (st, gt)||∞\n\nThen for every st ∈ S, there is:\n\nH (st)|\n\n(cid:88)\n\n|V ∗(st) − V ∗ (cid:12) (cid:42) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\n(cid:42)\n\nτk\n\nτk\n\n=\n\n≤\n\nP π(st) −\n\nP π(st), R(st) + γkV ∗\n\n(cid:43)\n\n(cid:42)\n\n−\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)P k(gt, st), R(st) + γkV ∗\n\nH\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nπH (gt|st)P k(gt, st), R(st)\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ng∈G\n\n+γk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:42)\n\n(cid:88)\n\nτk\n\n(cid:43)\n\n(cid:42)\n\nP π(st), V ∗\n\n−\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)P k(gt, st), V ∗\n\nH\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nnote that (cid:10)(cid:80) vector of |S|-dimension, for the first term of equation 17, we have\n\nP π(st), 1(cid:11) = 1 and\n\ng∈G πH (gt|st)P k(gt, st), 1\n\nτk\n\n(cid:68)(cid:80)\n\n(cid:69)\n\n= 1, where 1 is an all-one\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:42)\n\n(cid:88)\n\nτk\n\n(cid:42)\n\n(cid:88)\n\nτk\n\n=\n\nP π(st) −\n\nP π(st) −\n\n(cid:88)\n\ng∈G\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)P k(gt, st), R(st)\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nπH (gt|st)P k(gt, st), R(st) −\n\nkRmax 2\n\n· 1\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nby Holder inequality, there is:\n\n(18) ≤\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nτk\n\nP π(st) −\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)P k(gt, st)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nR(st) −\n\n·\n\nkRmax 2\n\n· 1\n\n(cid:13) (cid:13) (cid:13) (cid:13)∞\n\n≤ max st,gt\n\n(cid:13)P π(st) − ̄PH (st, gt)(cid:13) (cid:13)\n\n(cid:13)∞\n\n=\n\nνkkRmax 2\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nR(st) −\n\nkRmax 2\n\n· 1\n\n(cid:13) (cid:13) (cid:13) (cid:13)∞\n\n(cid:43)\n\n(cid:42)\n\nγk\n\n(cid:88)\n\nP π(st), V ∗\n\nThe second term can be similarly bounded by: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:42) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:42) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nP π(st), V ∗\n\nP π(st), V ∗\n\n≤γk\n\n+γk\n\n(cid:88)\n\n(cid:88)\n\n(cid:43)\n\n(cid:43)\n\n−\n\n−\n\nτk\n\nτk\n\nτk\n\nH\n\n−\n\nwhere\n\nπH (gt|st)P k(gt, st), V ∗\n\nH\n\nP π(st), V ∗\n\nH\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nπH (gt|st)P k(gt, st), V ∗\n\nH\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:42)\n\n(cid:88)\n\ng∈G\n\n(cid:42)\n\n(cid:88)\n\n(cid:42)\n\nτk\n\n(cid:88)\n\ng∈G\n\n15\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\nUnder review as a conference paper at ICLR 2023\n\nγk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:42)\n\n(cid:88)\n\nτk\n\n(cid:43)\n\nP π(st), V ∗\n\n−\n\n(cid:42)\n\n(cid:88)\n\nτk\n\nP π(st), V ∗\n\nH\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ γk ∥V ∗ − V ∗\n\nH ∥∞\n\n(21)\n\nand\n\nγk\n\n≤γk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:42)\n\n(cid:88)\n\nτk\n\n(cid:43)\n\n(cid:42)\n\nP π(st), V ∗\n\nH\n\n−\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)P k(gt, st), V ∗\n\nH\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nτk\n\nP π(st) −\n\n(cid:88)\n\ng∈G\n\nπH (gt|st)P k(gt, st)\n\nkRmax 2(1 − γk)\n\n· 1\n\n(cid:13) (cid:13) (cid:13) (cid:13)∞\n\n(22)\n\nH −\n\n·\n\nV ∗\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1 kRmax 2(1 − γk)\n\n· 1\n\n(cid:13) (cid:13) (cid:13) (cid:13)∞\n\n≤γk max st,gt\n\n(cid:13)P π(st) − ̄PH (st, gt)(cid:13) (cid:13)\n\n(cid:13)∞\n\n≤γkνk\n\nkRmax 2(1 − γk)\n\nSo that with 19, 21 and 22, there is\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nV ∗\n\nH −\n\n|V ∗(st) − V ∗\n\nH (st)| ≤\n\nνkkRmax 2\n\n+\n\nγkνkkRmax 2(1 − γk)\n\n+ γk ∥V ∗ − V ∗\n\nH ∥∞\n\n(23)\n\nSince 24 holds for all s ∈ S, so there is:\n\n∥V ∗ − V ∗\n\nH ∥∞ ≤\n\nνkkRmax 2(1 − γk)\n\n+\n\nγkνkkRmax 2(1 − γk)2\n\nA.2 PROOF OF THEOREM 1\n\nProof. Consider assumption A.3 there is:\n\n(cid:13) (cid:13)φ(st+k) − φ(s′ (cid:13)φ(st+k) − φ(st) + φ(st) − φ(s′\n\nt+k)(cid:13) (cid:13)\n\n= (cid:13)\n\nt+k)(cid:13) (cid:13)\n\n(24)\n\n(25)\n\nrestate that the φ(st) is the coordinate of state st, so that φ(st+k) − φ(st) is the direction vector of the movement from step t to t + k. By definition 3.1, we denote the relative displacement ∆t = ||g∆|| = Est+k [φ(st) − φ(st+k)] for any state st with ε-invariant subgoal g∆. So that the displacement vector can be written as ⃗g∆t = ⃗∆t + ⃗∆ε, where ⃗∆t is a fixed vector of expected direction and ⃗∆ε is a random vector. ∆ε,x, ∆ε,y ∼ N (0, σ), where σ ≤ ε is much less than the moving distance ∆t. Thus, the equation 25 can be rewritten as:\n\n25 =\n\n≤\n\n(cid:13) (cid:13)\n\n(cid:13) (cid:13)φ(st+k) − φ(st) − (⃗∆t + ⃗∆ε) (cid:13) (cid:13) (cid:13) (cid:13)φ(st+k) − φ(st) − ⃗∆t ⃗∆ε (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) +\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n(26)\n\nConsider that in normal distribution, the random variable has a high probability fall into the region of [−3σ, 3σ]. So that with a high probability, there is ||⃗∆ε|| ≤ (cid:112)2 × (3σ)2 ≤ 3 2ε. Thus, the assumption A.3 can be rewritten as follows:\n\n√\n\n|P k(st+k|st, gt) − P k(s′\n\nt+k|st, g∆)| ≤ Lφ(2δmax + 3\n\n√\n\n2ε)\n\n(27)\n\nwhere δmax = max g,g∆\n\n{||g||, ∆t}.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nThe equation 27 can be seen as the HRL transition mismatch rate between our method and optimal HRL method with stochastic coordinate subgoals. It is bounded by the maximal k-step movement of the agent with the variance of the random variable. With lemma A.2, the similar conclusion can be obtained by changing the k-step transition mismatch to equation 27. Then we can get the result of A.4.\n\nB ALGORITHM\n\nOur algorithm for the high-level policy learning is as follows (algorithm 1):\n\nAlgorithm 1 PEG-A2C Algorithm\n\na for i ∈ [1, n] v for i ∈ [1, n]\n\nv\n\na and dθi\n\nReset gradients: dθi Synchronize thread-specific parameters repeat\n\n1: Initialize multi-process actor parameters θi 2: Initialize multi-process value parameters θi 3: for episodes in 1,M do for i ∈ [1, n] do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:\n\nuntil terminal sT or t == tmax Set R = r for j ∈ {t − 1, . . . , 0} do\n\nPerform at according to policy π(at|st) Receive reward rt and new state st+1 t ← t + 1\n\nR ← rj + γR Accumulate gradients w.r.t. θ′ a\n\ndθa ← dθa +\n\n1 n\n\n1 t\n\n∇θ′\n\na\n\n16:\n\nAccumulate gradients w.r.t. θ′ v\n\nlog π(aj|sj; θ′\n\na)(Ri − V (si; θ′\n\nv))\n\ndθv ← dθv +\n\n1 n\n\n1 t\n\n∂ ∂θv\n\n(Rj − V (sj; θ′\n\nv))2\n\nend for\n\n17: 18: 19: 20: end for\n\nend for Synchronize and update parameters\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Structure of the network.\n\nC DETAILS OF FRAMEWORK\n\nC.1 NETWORKS\n\nThe structure of our high-level network is shown as follows (figure 8):\n\nD DETAILS OF EXPERIMENTS\n\nAnt ⊃-Maze. In this task, the agent should navigate from the bottom to the top. Different from the setting in previous works, the maze is larger, and the agent will only attain reward ’1’ once when it passes the corner and reach the final region.\n\nAnt Random Square Maze. It is an empty room with a door and a robot. In this task, the agent will start at a random initial position every episode, to walk towards the yellow door, which also has a random position on the wall. Only when the agent passes the door will it obtain a reward.\n\nAnt S-shaped Maze. In this task, the agent will start at the left region and should pass three doors. The trajectory is circuitous and long-horizon, especially the final door is more difficult to achieve than the region in Ant ⊃-Maze. Every first-time transiting of the doors will give the agent a reward.\n\nAnt Spiral Maze. This task is in a large maze with spiral routes and five doors and the agent will start at the middle region. Such a long-horizon task requires at least thousands of steps to move to the final region. Also, every first time transiting the doors will give the agent a reward.\n\nGeneralization Maze. This task includes three fixed mazes of ’Maze-g1’, ’Maze-g2’, and ’Mazeg3’ (figure 1), which are variants of ’Ant Random Square Maze’ with different unseen structures. Only when the agent passes the door will it obtain a reward.\n\nD.1 STEPS OF EVERY EPISODE OF DIFFERENT TASKS\n\nTable 3 shows the maximal steps of every episode of every task.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Steps of every episode of different tasks\n\nMaze\n\nMaze-random Maze-U Maze-S Maze-spiral Maze-g1 Maze-g2 Maze-g3\n\nSteps per ep\n\n600\n\n600\n\n1000\n\n5000\n\n600\n\n600\n\n600\n\nD.2 DETAILS OF REWARD SETTING\n\nFor the sparse reward, it will be obtained by the agent when the agent goes across the door, i.e., the coordinates of the agent fall into a region of the door. For the dense reward, it is 1/(1 + d) of Euclidean distance d between the agent and the current goal in the coordinate system for every step. Once the agent goes to the current goal and gets the reward, the goal will update, and the reward will be calculated by the new goal. As a result, the curves of average reward in these tasks may decline sometimes.\n\nD.3 HYPER-PARAMETERS\n\nTable 4: Steps of every episode of different tasks\n\nHyper-parameters\n\nValue\n\nDetails\n\nSubgoal dimension Subgoals Room size Grid size Sparse reward Dense reward High-level frequency Intrinsic reward coefficient α\n\n8 ↑, ↓, ←, → {7*7, 7*7, 14*7, 21*21} 3*3 1\n[0.03, 1.0] 25 3\n\ntwo-hot vector of four subgoals abstract subgoals Represented by vector grids of ’random’,’⊃’,’S’,’spiral’ size in coordinates system of MuJoCo\n\nrange of dense reward steps of the low-level to execute\n\n19",
    "reference": "# Summary Of The Paper\n\nThis work proposes to use four directions (top, down, left, right) as the general subgoals to learn the high-level policy in a hierarchical reinforcement learning (HRL) setting. Since the directions are task-agnostic and agent-agnostic for the typical navigation jobs, the proposed framework could be more generalizable. Further, to overcome the mismatch problem while training an HRL policy, the authors (1) trained the high-level and low-level policies separately and (2) introduced a slight noise controlled by \\epsilon into the perfect low-level controller in the environment. The experimental results in MuJoCo suggest that the proposed framework outperforms various baselines.\n\n# Strength And Weaknesses\n\n+) The studied problem is essential, and the proposed idea is reasonable. Using four directions as subgoals for navigation tasks makes more sense than using absolute coordinates in prior works. The results also suggest that the learnable high-level policy is reusable for different low-level controllers (different agents).\n\n-) The manuscript has not been well written. Several typos, grammar errors, duplicated words, or redundant spaces exist. In addition, some notations are misleading. Taking “x+, x-, y+, y-“ as an example, are the x and y variables? Or do they only indicate direction? All of them make the paper hard to follow.\n\n-) Some algorithms proposed in prior works use collected on-policy rollouts to compute empirically expected policy gradients to perform the policy optimization, such as SAC, PPO, and TRPO. Is there a specific reason for using A2C in the experiments? Since the authors claimed the expected A2C algorithm is one of the contributions, a comparison to the mentioned policy gradient algorithm is necessary.\n\n-) There are many missing details in the experiment section. For example, what is the action space for different agents (e.g., the low-level controllers for different agents?)? What are the learning rates to train the high-level policy and the low-level controller? What optimization algorithm (e.g., SGD?) is used in the training stage?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper needs to be more carefully polished. The writing quality is concerned. Moreover, the novelty is also a concern since the main contribution is only the discrete subgoals for better generalization. Because there are many missing details in the experiment section mentioned above, I have no confidence in the reproducibility.\n\n# Summary Of The Review\n\nThe work proposed a simple but intuitive way to discretize subgoal space to improve the generalization ability of an HRL policy. The experimental results show the effectiveness of the proposed framework. However, since the paper is not well polished, it is hard to read and understand the keys under the hood. In addition, because there are many missing details in the experiment section, I have no confidence in the reproducibility. Finally, the usefulness of one of the claimed contributions about “expected policy gradients by A2C” is not validated, which makes the technical contributions weak.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSELF-CONDITIONED EMBEDDING DIFFUSION FOR TEXT GENERATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nCan continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion (SED), a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models — while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion.\n\n1\n\nINTRODUCTION\n\nContinuous diffusion models (Sohl-Dickstein et al., 2015) have taken the world of image generation by storm, advancing the state of the art further than ever before (Rombach et al., 2021; Ramesh et al., 2022). Can the same framework encounter as much success on the text modality? Diffusion for language is indeed an attractive prospect. Compared to autoregressive (AR) models (Bengio et al., 2000; Sutskever et al., 2011; Austin et al., 2021; Hoffmann et al., 2022), diffusion models can predict all tokens in a sequence at once. This allows for bidirectional, rather than causal attention— increasing interactions between tokens, potentially leading to more coherent samples. Diffusion models can make a better usage of hardware accelerators during inference than AR models, since computations are parallelizable over the sequence axis. Yet AR models remain the mainstream approach for modelling text. A major obstacle to text diffusion is that diffusion processes typically operate in continuous space. While this naturally handle images, text is inherently discrete. Consequently, most previous attempts to apply diffusion to text have focused on discrete diffusion-like approaches. These methods do not benefit from the refinements made to continuous diffusion in the image domain. Crucially, they cannot make use of guidance (Dhariwal & Nichol, 2021), which drastically improves diffusion models sample quality.\n\nWe address this gap by making a simple observation: language models operate mostly in continuous space, with discrete tokens only as inputs and outputs. A natural idea is then to conduct diffusion directly in a continuous token embedding space. For simplicity, we use a fixed embedding space, either random or stemming from a trained language model. Combined with the “self-conditioning” (Chen et al., 2022) refinement, this forms the basis of the method we propose, Self-conditioned Embedding Diffusion (SED). SED models rival mainstream AR models in both conditional and unconditional text generation. We make the following contributions:\n\n• In section 3, we introduce SED, the first continuous diffusion approach for text with good scaling properties (testing models up to 420M parameters). We analyze several continuous text diffusion settings, and identify self-conditioning and diffusion on small fixed embeddings as key factors to make continuous text diffusion work.\n\n• In section 4, we apply classifier-free guidance (Ho & Salimans, 2022) to text data—an original achievement. We show that SED can rival AR models on generic language tasks, for similar models sizes. SED samples achieve a better likelihood-entropy trade-off compared to these models, and are deemed comparable (if slightly worse) by human raters.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nWe provide an overview of diffusion models with a focus on modeling discrete data, as well as AR models and sample-based metrics for evaluating text generation.\n\nContinuous diffusion on continuous image data. Continuous diffusion has recently established itself as the method of choice for modeling continuous data such as images. While our main focus in this paper is on discrete data, we review some key works in continuous data modeling as this literature was the major source of inspiration for SED. The first continuous diffusion formulation was introduced in the seminal work by Sohl-Dickstein et al. (2015). Ho et al. (2020) improved and simplified this formulation, relating it to denoising score matching, and creating a new method called DDPM. Nichol & Dhariwal (2021) further improved upon DDPM, showcasing impressive diffusion results compared to GANs. Rombach et al. (2021, Stable Diffusion) introduced diffusion in latent space. Conceptually similar to SED, it was specifically targeted at image modeling. Classifier-free guidance was proposed by Ho & Salimans (2022) as a mean to improve image fidelity at the cost of reduced diversity. GLIDE (Nichol et al., 2022) scaled up the ideas of guided diffusion, while DALL-E 2 (Ramesh et al., 2022) and Imagen (Saharia et al., 2022) are the latest, most advanced image generation systems to date, combining most of the improvements proposed in previous works.\n\nDiscrete diffusion on discrete data. One cannot simply reuse the methods that are successful on continuous image data in the discrete text domain. A number of bespoke methods have been explored instead, forming the family of discrete diffusion approaches. In discrete diffusion, the data is corrupted by switching from one discrete value to another. This was first proposed in the seminal work by Sohl-Dickstein et al. (2015), where it was tested on simplistic binary heartbeat data. It was extended to multinomial text modeling (Hoogeboom et al., 2021) and further scaled up in the D3PM work (Austin et al., 2021). Most recently, a similar discrete diffusion approach was applied to image modeling in VQ-Diffusion (Gu et al., 2022). In parallel, a few diffusion-like approaches were proposed in the denoising autoencoders literature. CMLM (Ghazvininejad et al., 2019) tackled machine translation. SUNDAE (Savinov et al., 2022) was the first non-AR method to show strong results both in machine translation and unconditional text generation. MaskGIT (Chang et al., 2022) demonstrated excellent results in modeling VQ-discretized images. These approaches rely on training models to predict masked tokens from their context, and iterating this reconstruction step multiple times at sampling time. Despite those positive developments, the samples from discrete diffusion methods for text modeling remains less coherent than those produced by AR methods.\n\nContinuous diffusion on discrete data. Fewer works try to tackle diffusion on discrete data from the same angle as SED – starting by turning the data into continuous representations before modeling it with continuous diffusion formulations. Mittal et al. (2021) used a VAE to generate such representations for discrete music modeling, with exciting results. Closest to SED, Diffusion-LM (Li et al., 2022) trains a token embedding together with the diffusion model itself. Diffusion-LM meets success on specific language applications, in low data regime and on constrained, very formatted textual data. Most recently, Analog Bits (Chen et al., 2022) introduced self-conditioning, closely related to step-unrolls in SUNDAE (Savinov et al., 2022), together with bit-level modeling to improve the generation of discretized images. While the qualitative results of those continuous methods on text modeling show promise, they have not been shown to scale to large realistic text datasets like C4 (Raffel et al., 2020) yet, or to compare with AR approaches on generic language tasks.\n\nAuto-regressive modelling on discrete data. AR models remain the method of choice for modeling discrete data. In combination with neural networks, they were first explored by Bengio et al. (2000) and later combined with RNNs (Sutskever et al., 2011). Their breakthrough moment came with the advent of the Transformer architecture, introduced by Vaswani et al. (2017) for machine translation. Even more impressive results were shown with GPT-3 (Brown et al., 2020), which trained a large AR language model unconditionally, and used few-shot prompting to adapt it to new tasks. A few works later improved upon the results of GPT-3, including Hoffmann et al. (2022).\n\nSample-based evaluation of text generative models. There are traditionally two classes of metrics for generative modeling: likelihood-based and sample-based. While the likelihood-based way is mathematically appealing, its usefulness for measuring progress is reduced by the fact that not\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nall models readily provide likelihood computation. Just like the sampled-based FID metric was important for driving the progress of diffusion in image modeling, there is a need for a sample-based metric which would be universally accepted for text modeling. Caccia et al. (2018) investigated fidelity/variance metrics for evaluating text GANs. Semeniuta et al. (2018) suggested using FID for texts. De Masson d’Autume et al. (2019) later used those previously proposed metrics to iterate on ScratchGAN but did not provide conclusive guidance on which metric a practitioner should choose – essentially finding serious vulnerabilities in all investigated metrics. We opted for a middle ground, reporting both sample likelihood according to a strong AR model and human preferences.\n\n3 METHOD\n\nIn this section, we outline the different components of SED: continuous diffusion in the space of token embeddings and self-conditioning, which form the basis of our approach for unconditional text generation; span masking and guided diffusion to enable conditional generation.\n\n3.1 DIFFUSION MODELS FOR UNCONDITIONAL TEXT GENERATION\n\nDiffusion models in continuous space. We consider diffusion models as introduced by SohlDickstein et al. (2015) and improved by Ho et al. (2020). A diffusion model aims at modelling a data distribution x0 ∈ Rn ∼ q ∈ D(Rn) by estimating a sequence of latent variables xT , ..., x1 of the same dimensionality as the data x0. Starting from x0, the latent variables are generated with a Markov chain called the forward process: xt ∼ q(· |xt−1, t). It is defined by gradually interpolating the iterate with Gaussian noise according to noise levels defined by a schedule β1, ..., βT :\n\nxt ∼ q(· | xt−1, t) = N ((cid:112)1 − βtxt−1, βtI).\n\nThis parametrization gives us a closed form to sample xt for any arbitrary t ≥ 1, given x0:\n\n√\n\nxt =\n\nαtxt−1 +\n\n√\n\n1 − αtεt =\n\n√\n\nαtx0 +\n\n√\n\n1 − αtε,\n\n(1)\n\n(2)\n\nwhere αt := 1 − βt, αt := (cid:81)t\n\ns=1 αs, εt ∼ N (cid:0)0, I(cid:1) and ε ∼ N (cid:0)0, I(cid:1). We define our generative model by approximately inverting the diffusion process of Eq. 1 to obtain a reverse process. The reverse process starts from xT ∼ N (0, I) and is defined as a Markov chain with learned Gaussian transitions (parameterized by θ, the weights of a neural network): xt−1 ∼ pθ(· |xt) = N (cid:0)μθ(xt, t), σ(t)2I(cid:1). We train a neural network to predict an estimate ˆx0(xt, t, θ) of the data x0 and approximate the reverse process by using the following parametrization, with learnable means but fixed variances, and a fixed schedule β1, ..., βT :\n\nμθ(xt, t) =\n\n√\n\nαt−1βt 1 − αt\n\nˆx0(xt, t, θ) + σ(t)2 =\n\n1 − αt−1 1 − αt\n\nβt·\n\n(3)\n\nWhile there exists a tractable variational lower-bound (VLB) on log pθ(x0), Ho et al. (2020) showed that better results are obtained by optimizing a simplified objective that re-weights the terms in the VLB. We follow this approach, which simplifies the loss to a sum of mean-squared errors between the ground truth data x0 and its estimates ˆx0(xt, t, θ):\n\nLdiffusion = Ex0∼q(x0), t∼U (1,T )∥x0 − ˆx0(xt, t, θ)∥2·\n\n(4)\n\nThough this framework works out of the box on images, which are close to continuous, we cannot apply it directly to the discrete tokens of the text modality. To resolve this issue, we perform continuous diffusion in a continuous space in which we embed text tokens.\n\nDiffusion on word embeddings. We consider textual data w = (w1, . . . , wN ), where each wi is a one-hot representation in RV of a discrete token in {1, ..., V }. Each token w has an associated embedding ew ∈ RD, with fixed norm D to match the norm of a random gaussian sample in dimension D used to noise clean data. We denote by E ∈ RD×V the matrix of all embeddings.\n\n√\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nWe define our diffusion process in embedding space, rather than in token space. To that end, we define a forward discrete-to-continuous step qV (x0|w) = N (Ew, σ2 0I), where σ0 is a constant scale factor with a similar order of magnitude as β1. Conversely, we define a reverse continuousto-discrete step pR(w|x0) = (cid:81)N k=1 Cat(wk|E′(x0)k), where R ∈ RV ×D is a learnable readout matrix initialized to E⊤ and Cat(wk|l) is the softmax probability of token k with logits l ∈ RV .\n\nTo train the readout step, we add a reconstruction loss to Ldiffusion during training. Conveniently, it naturally arises when deriving the VLB of pθ(w) with this discretization step (Li et al., 2022), introducing a simple cross-entropy loss to maximise pθ(w|x0):\n\nLrecon = Ew∼D,x0∼qV (w)[− log pR(w|x0)],\n\nwith\n\nLtotal = Ldiffusion + Lrecon.\n\n(5)\n\nContrary to what is done in Li et al. (2022), we do not learn the embedding matrix E, as we identified that it was empirically unstable and could lead to drops in unigram entropy. The reconstruction loss Lrecon therefore only depends on the trainable readout weights R.\n\nAt sampling time, we run the reverse process for T = 1000 steps, ultimately yielding a continuous embedding x0 of size dembed. We multiply it by R to obtain logits in RV , and then use the index of the maximum component to convert it to a token wi, with i = arg max1≤j≤V (R x0). This entails running T full forward passes which is quite expensive compared to cached AR sampling; however each forward pass computes all timesteps at once which is naturally parallelisable. Further, we hope to benefit from many diffusion sampling improvements to get T down to low double-digits.\n\n0 = ˆxt\n\nIn standard diffusion sampling, at each timestep t the Self-conditioning (Chen et al., 2022). denoising network generates an estimate ̃x0 = ˆx0(xt, t, θ) of x0 given only xt as input. Selfconditioning progressively refines x0 estimates by passing the estimate ̃xt+1 obtained at the previous sampling step as input to the denoising network; the self-conditioned estimate is then defined as 0(xt, ̃xt+1 ̃xt , t, θ), and sets the diffusion direction. In practice conditioning is performed by concatenating xt and ̃xt+1 on the feature axis. To approximate the inference behavior at train time while remaining computationally efficient, we compute a first estimate xt 0 = ˆx0(xt, 0, t, θ) with the self-conditioning set to zero, then perform a second forward pass using a stop gradient on xt 0 to obtain ̃xt 0, t, θ). The denoising network is then optimized using the output from the two forward passes in order to estimate x0 accurately with and without self-conditioning.\n\n0 = ˆx0(xt, xt\n\n0\n\n0\n\n0\n\nEquipped with these 3 components we can train models to generate text, though only unconditionally. To add conditional generation to our system’s capabilities, we use two additional methods.\n\n3.2 SPAN MASKING AND GUIDANCE FOR CONDITIONAL TEXT GENERATION\n\nBy design diffusion models for text generation are flexible and can handle a wide variety of infilling tasks. This is a key advantage over the predominant auto-regressive language models that typically generate text in a left-to-right fashion.\n\nSpan masking. We train our model on a rich set of infilling tasks with the following method. We optimize the diffusion loss from Eq. 4 only over x while fixing the conditioning tokens c. Conditioning tokens c are defined by a binary conditioning mask m set to one on conditioning positions and zero on positions to be infilled.\n\nWe sample conditioning mask m randomly as follows. Given a sequence of length L and a maximum number of spans M , we sample a number of spans n uniformly in [1, M ]. Span starting positions are defined by n − 1 integers (i1, ..., in−1) sampled uniformly without replacement and sorted in increasing order to satisfy 0 < i1 < ... < in−1 < L. The tuple (i1, ..., in−1) partitions the sequence of tokens in n spans satisfying E[ik|n] = k n L. The conditioning mask m is defined using even spans for conditioning and odd spans for infilling, and then m is flipped with a 50% probability. The case n = 1 corresponds to unconditional generation; we then set m to 0 everywhere.\n\nThis span masking strategy defines a collection of text generation tasks with a large variety of conditioning which on average evenly splits the sequence between conditioning and infilling spans. It enables conditional generation, and opens the door for additional diffusion improvements.\n\nGuided diffusion. Guidance (Dhariwal & Nichol, 2021) often improves the sample quality of conditional diffusion models. We use classifier-free guidance (Ho & Salimans, 2022), which alleviates the need for a separately-trained guide model. In the conditional case, our estimator ̃x0 is now\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: SED samples on unconditional generation, fill-in-the-middle and several spans in-filling.\n\nTask\n\nSamples\n\nUnconditional We make use of the very best supplies and solutions to ensure that the work is going to stand up to the test of time, and we help you save money with techniques that do not change the quality of your mission. We’ll achieve this by offering you the best deals in the field and avoiding pricey mistakes. If you want to spend less, Refrigerator Unit Repair Guys is the company to contact.\n\nFill-in-themiddle\n\nSpans in-filling\n\nA year ago in Paris, I had the opportunity to take a field trip to La Rite-en-Laurences International de France where I met David Nigel Johnson, a professor of social studies. What a great trip and what a great day!\n\nThere was no evidence, only fleeting glimpses of the killer and his fate. In fact, it seemed that there was no evidence. It was all guesswork, and one of the most unusual murder cases throughout history.\n\na function ˆx0(xt, c, ̃xt 0, t, θ), where c are fixed conditioning tokens. During training, with fixed probability the conditioning tokens c used in the estimator ˆx0 are dropped and set to a null label ∅. During sampling, the model prediction is extrapolated in the direction of ˆx0(xt, c, ̃xt+1 , t, θ) and away from ˆx0(xt, ∅, ∅, t, θ) as follows:\n\n0\n\n ̃xt\n\n0 = ˆx0\n\n(cid:0)xt, ∅, ∅, t, θ(cid:1) + s ·\n\n(cid:16)\n\nˆx0\n\n(cid:0)xt, c, ̃xt+1\n\n0\n\n, t, θ(cid:1) − ˆx0\n\n(cid:0)xt, ∅, ∅, t, θ(cid:1)(cid:17)\n\n,\n\n(6)\n\nwhere s ≥ 1 is the guidance scale. Remark that we jointly drop conditioning and the selfconditioning ̃xt+1 , concretely setting both values to zero. Classifier-free guidance allows leveraging both the unconditional and conditional abilities of a model to improve its conditional generations.\n\n0\n\n4 EXPERIMENTS\n\n4.1 TRAINING DETAILS\n\nWe train all our models on the C4 dataset (Raffel et al., 2020), using a SentencePiece tokenizer (Kudo & Richardson, 2018) composed of 32000 words. We use a non-causal transformer model (Vaswani et al., 2017) as our diffusion model (see Appendix A for details). SED models are trained with sequence length 256, while for ablations models are trained with sequence length 128. We insert uniformly, i.e. not necessarily at the end of the sequence, 10% of padding tokens in the training set to allow SED models to generate samples of varying size and provide more flexibility.\n\nTo generate word embeddings, we train a BERT model of fixed size (150m parameters) and feature dimension dmodel = 896. The diffusion space is defined by the initial lookup table of this BERT model. We bottleneck the dimension of the word embeddings dembed and add a linear projection layer from dembed to dmodel at the beginning of the model. We found this helped diffusion (see section 4.4). SED models are trained with a cosine noise schedule (Dhariwal & Nichol, 2021), with β1 = 2.10−3, σ0 = 10−2 and T = 1000. We use batches of 65.536 tokens, thus for sequence length 256 the batch size is set to 256. We use a maximum span count of 5 for all runs except for its specific ablation. We train SED models at two different scales: SED-S (135m parameters, 106 training steps) and SED-L (420m, 2.106 steps). Their detailed architectures can be found in Appendix A.\n\n4.2 VALIDATION\n\nWhile optimizing the perplexity of AR models for text leads to improved language models, directly optimizing the ELBO of diffusion models for images does not correlate strongly with sample quality as observed by Nichol & Dhariwal (2021); Kingma et al. (2021); Ho & Salimans (2022). For images, the sample based metric FID (Heusel et al., 2017) has been introduced as a measure of sample quality and is now widely adopted. Similarly, we need a sample-based metric for text generation that is reliable and allows comparison between a large variety of generative models. To provide a fair comparison to AR models, we rely on three metrics.\n\nThe first metric measures how likely the samples produced by a model are according to an AR language model with 70B parameters, trained on 1.4B tokens (Hoffmann et al., 2022); we denote\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nthis metric AR NLL for auto-regressive negative log-likelihood. It provides a continuous measure of sample quality that has proven useful when combined with a measure of sample diversity, e.g. in the development of nucleus sampling (Holtzman et al., 2020) for improved AR model decoding.\n\nTo measure diversity we rely on a second metric, the unigram entropy of samples, which helps balance the AR NLL that can be gamed by unnatural repetitive samples. For both these metrics, our target is the score of the validation set data. Deviating from the data unigram entropy in particular is a sign of degenerate modeling.\n\nThough this initial combination has provided us with a reliable signal to iterate over our model design, it remains imperfect; it too can be gamed, though it is harder to do so. To address this limitation, we also report human preferences. We presented 6 colleagues with 20 pairs of samples for each comparison, asking them to pick the best one. For all three metrics, we report results on two tasks: unconditional language modeling and suffix in-filling, the later a heavily conditioned task.\n\n4.3 RESULTS\n\nSamples. We present samples generated with our SED models in Table 10. We use a single model to perform a wide variety of text generation tasks, such as unconditional generation, filling-in-the-middle or filling several spans of text. We show strong performance in the unconditional case, with samples that are syntactically correct and stay coherent on long sequences. In the conditioned case, SED models are able to infill spans with coherent transitions and links to the conditioning but also exhibit a rich diversity. By design, SED yields flexible bi-directional masking models that can perform text generation on a diverse set of conditioned task. To compare SED with AR baselines we next restrict conditioning to a prefix and consider a task of suffix in-filling.\n\nComparison to AR models. To assess the generation ability of SED, we compare against AR baselines of similar capacity and trained following optimal scaling laws from Hoffmann et al. (2022) on suffix in-filling. We sample a batch of sequences from C4 and use the first 128 tokens as conditioning given to the model to generate a suffix of 128 tokens. Figure 1 reports AR NLL and unigram entropy of the generated suffixes for AR and SED models. As a reference point, we compute the AR NLL and unigram entropy of the ground truth C4 suffixes and report it on the plot. Several methods can be used to improve sampling quality at the cost of samples diversity; we use nucleus sampling (Holtzman et al., 2020) for AR models and guidance (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) for SED models. We show the impact of guidance on samples quality in Table 3. To our knowledge, we are the first to show sample quality improvement when using guidance for text generation.\n\nFigure 1: Comparison of sample quality and diversity of SED versus AR models on suffix SED uses guidance with scales in in-filling. {1, 2, 4, 8} and AR uses nucleus sampling with a top-p in {1.00, 0.95, 0.90, 0.85}. Top-right points are SED models with a guidance scale of 1 or AR models with a top-p of 1.\n\nAs shown in Figure 1, both SED-S and SED-L perform strongly when compared against AR baselines – even though we report a metric favoring AR models on a task AR models have been designed to optimize. Similar to nucleus sampling for AR models, guidance has a strong positive impact on sample quality that is both observed quantitatively with improved AR NLL in Figure 1 and qualitatively in Table 3. We observe that using a top-p nucleus sampling below 0.8 for AR models or a guidance scale above 4 for SED models leads to samples exhibiting a lot of repetitions, a degenerate case reflected by a lower entropy of samples even though sample AR NLL improves.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: We compare SED-L (guidance scale 2.5) and AR-L (nucleus sampling, p = 0.95) samples.\n\nSED L\n\nAR-L\n\nYou’re going to love wearing this traditional tee from our latest Wilson collection. Designed in a scrapped floral styled knit with a sleeve of asymmetric lines across the round sole. Lightly fluffy, the square pleats will take you right to\n\nA Koda Ram 25 is presented in sedan and a Maxima saloon. Based on the Acenta car, the powerful Koda 2014 hits Indian roads in the ”Maxima” body-con shape. Being powered by a Hyundai i20 1.4 litre diesel engine, the Koda 2014 is coupled\n\nThe beaver is an interesting animal that lives in rivers and lakes. He is not mainly concerned with finding wolves and dolphin but also has a great hunger for fish. The beaver has sharp legs, large eyes, and a black coat\n\nThe beaver is an interesting animal that lives in rivers and streams. It is usually seen in big numbers in the fields or upstream, and is quite docile. On cold days when its pattern is perfect, the beaver will have some interesting, and sometimes\n\nOnce upon a time in Spain, Leonardo Pueleva had the pleasure of meeting guests at Spanish restaurant, Buva Casinos. While driving, he got a chance to get to know the people behind the restaurant and, of course, how they made his experience very interesting. After his conversation, he got to\n\nOnce upon a time in Spain, which seems pretty much the same way now, the question that was posed to each of us at the end of our interview was ”would you like to see Froome one day?” In retrospect, after our interview, we have grown ever closer to that answer. As you will read in the article, I know that\n\nTable 3: Impact of guidance on samples quality using our SED model.\n\nGuidance 1.0\n\n2.5\n\n5.0\n\nIn the cold, cold night sky, a fairy princess sits in a chair and surrounded by tea leaves in a pond. Meanwhile, she bies back into the cold, with bluish hair on her hips and elbows on her forehead - and her fingers numbed by the freezing temperature.\n\nBarbara was one of our many wonderful women that really helped so I am so blown off by her purpose, civility; and adversity. Once I started interacting with her, it proved to me that no matter how hard this was, she always strove for excellence.\n\nIn the cold, cold night of November 2018, a little girl sits in a chair hidden under a light blanket on a patio. Meanwhile, she bends back into the chair with bluish hair on her forehead, her hands on her face, her fingers numbed by the freezing temperature.\n\nBarbara was one of the most gifted women in the world. She was creative and stood up by her integrity and civility; against adversity. Although she placed herself it higher proved to me that no matter how hard this was, she always strove for excellence.\n\nthan her peers,\n\nIn the cold, cold night of December, my oldest daughter sits in a chair accentuated in cotton fabrics and a pillow. Meanwhile, she yearns straight in the cold air, her wrists covering her neck, her eyes straight on her forehead, and her fingers numbed by the freezing temperature.\n\nBarbara was one of the most brilliant women in the world. She was amazing in her heart, her spirit, her mind and in the soul. She never turned people off in her absence. It proved to me that no matter how hard this was, she always strove for excellence.\n\nOur human preference scores temper our observations in Table 4. They show that our NLL and entropy metrics do not tell the whole story, as humans still prefer AR models at equivalent size. While SED-L performs slightly worse than AR-L (38% preference in suffix in-filling, 44% on unconditional generation), its scores remain comparable. SED-L is roughly on par with AR-S.\n\nFinally, we compare SED and AR models’ qualitative examples with short prompts in Table 2.\n\nTable 4: SED-L vs other models human preference scores on conditional and unconditional tasks.\n\nSED-S (cond)\n\nAR-S (cond)\n\nAR-L (uncond)\n\nAR-L (cond)\n\nSED-L 63.4% ± 4.3% 51.0% ± 5.0% 43.8% ± 4.4% 37.7% ± 4.4%\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n4.4 ABLATIONS\n\nTable 5: Ablation of the proposed SED approach on unconditional generation. Both selfconditioning and embeddings pretraining play a key role in the model performance.\n\nDiffusion space\n\nSelf-conditioning Unigram entropy\n\nAR NLL\n\nBits (Chen et al., 2022)\n\nRandom embeddings\n\nPretrained embeddings\n\n✗ ✓\n\n✗ ✓\n\n✗ ✓\n\n6.97 7.47\n\n6.90 6.86\n\n6.75 6.77\n\n7.01 6.05\n\n6.80 5.31\n\n5.66 4.57\n\nData\n\n6.70 ± 0.04\n\n1.81 ± 0.15\n\nSelf-conditioning and embedding pretraining. Results from Table 5 and samples from Table 6 show the influence of both the diffusion space and self-conditioning. AR NLL decreases very significantly when using self-conditioning, regardless of the rest of the setup. Diffusing at the bit-level (Chen et al., 2022) yields very high NLLs. While using random embeddings performs markedly better, using pretrained embeddings results in further improved numbers.\n\nSamples from Table 6 highlight that models trained on random word embeddings exhibit topic modelling abilities with the co-occurrence of words like child and mother even though the paragraph remains globally incoherent and meaningless tokens like gluc are generated. Self-conditioning dramatically improves sample quality; the diffusion model gets the low-level structure right and generates syntactically correct sentences, even though the global text is not intelligible. Combining self-conditioning and pretrained embeddings leads to globally coherent paragraphs that stay on topic with proper sentence structure.\n\nEmbedding dimension. An important design choice for SED is the word embeddings space. We study the influence of pretrained embedding size in Table 7. Surprisingly, there is a threshold after which performance degrades when increasing the dimension of embeddings. We visualize the forward process for different embedding sizes by displaying the nearest neighbor of a noised token while running the forward process. In high dimension we observe that the nearest neighbor of a noised token remains the starting token itself until it switches to a completely random, unrelated token. In low dimension, we often observe that the closest neighbor of a noised token goes through several semantically related tokens (nearest neighbor of the starting token) before ultimately becoming random. We hypothesize that the random walks defined by diffusion are more likely to drift towards neighbors of the starting token in low dimension. As a result, when diffusing in low dimension information is destroyed in a more semantically meaningful fashion, which leads to an easier learning problem for the denoising function.\n\nNumber of spans. In order to enable in-filling, we train the model not only to do unconditional generation but also to conditionally fill spans of tokens. For each data point we sample a span number uniformly at random and span delimiters to generate the span mask. Picking the maximum allowable number of spans has a significant effect on model performance, as we can see in Table 8. Somewhat counter-intuitively, adding span masking improves even unconditional generation NLLs. It also appears that using a relatively high maximum span number is optimal. We hypothesize that this results in a varied mix of task difficulty at training time, between ”easy”, very conditioned problems on the one hand and ”harder”, unconditional ones on the other.\n\nScaling. We show encouraging results when scaling from SED-S (150m) to SED-L (420m). We train both models on sequences of 256 tokens and report a AR NLL of 4.20 for SED-S compared to 3.68 for SED-L. This improvement translates to improved sample quality, as is confirmed by our human preference scores, which are much higher for the larger model (63%, see Table 4).\n\n5 LIMITATIONS\n\nWhile our results are promising and show that continuous diffusion for text can be an exciting alternative to AR models, the current approach does present some significant limitations.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: On unconditional generation, self-conditioning results in better sentence modelling, pretrained embeddings enhances topic modelling.\n\nRandom embeddings\n\nRandom embeddings with self-conditioning\n\nPretrained embeddings with self-conditioning\n\ndid the buildingroom granted a lighter distance. On it though, salaries about clients that a child, which dispersed gluc so many events and certainly wanted the Mother’s project, discovered by their child would keep\n\nTree brings the sound, bearing features and capabilities that we are set in. For the first time, she uses a customizable framework to use that we help students publicly solve the weather conditions that we only offer students\n\nalmost six decades ago - 72 percent of Americans didn’t feel they’d actually rent their own cars this year. Conversely, 90 percent of Americans feel that the decision to rent a car is something they feel it’s impossible\n\nTable 7: Word embeddings with small dimension have higher AR likelihood.\n\nTable 8: Span masking tasks improves unconditional text generation.\n\nEmbed. dim. 16 AR NLL\n\n64 128 256 896 4.65 4.57 4.71 4.61 4.77 4.92\n\n32\n\nMax span count AR NLL\n\n1\n\n3\n\n11 4.99 4.82 4.82 4.67 4.48 4.56\n\n5\n\n7\n\n9\n\nFirst, much more could be done in terms of model tuning, including scaling to much bigger models to better understand SED’s limits, and to be able to compare it with state-of-the-art AR models. Our training regime in particular would certainly benefit from more hyperparameter optimisation.\n\nSecond, one compelling reason we chose to explore continuous diffusion for text is to leverage the improvements produced by the literature on image generation. While we have ported some (e.g. selfconditioning), a lot more remains unexplored. The most obvious example is the sampling process itself, where the number of required steps has been considerably reduced for images (e.g. Karras et al. (2022) goes from 1000 to 35, and Salimans & Ho (2022) all the way down to 4 on simple images). Our current sampling is very inefficient, and this direction is one of the first improvements to make over SED.\n\nThird, SED crucially relies on diffusing in a pretrained embedding space. This means relying on a second model, and using embeddings that may not be optimal for diffusion. Ideally, we’d train the full model end-to-end, which could yield even better results. While Li et al. (2022) found some success with this approach, it was in a specific setting at a small scale; in practice we found it difficult to avoid competition between the diffusion and reconstruction loss.\n\nFinally, our work would benefit from improved metrics in the experimental section. Because the current state of the art involves AR models, the field lacks established benchmarks for tasks diffusion models are potentially better suited for, such as text in-filling. We opted for a reasonable mix, evaluating the negative log-likelihood of generated samples according to a very strong AR model as well as their token entropy and complementing it with a human evaluation. However, both NLL and unigram entropy are gameable (e.g. AR models assign very low NLL to repetitive snippets, and long enough repetitions can fool even entropy). Further, our NLL is inherently tied to its AR model and could thus be providing an unfair advantage to AR models. All told, we still found both metrics quite useful for measuring research progress, and our human evaluation confirmed our results. Moving forward, defining a clean in-filling benchmark would help produce even more convincing results.\n\n6 CONCLUSION\n\nWe propose SED, the first generally-capable continuous diffusion model for text generation. SED models can perform both conditional and unconditional generation, and their performance rivals AR models while being more flexible in their use (e.g. enabling in-filling). We demonstrate their performance and study the impact of the main design choices.\n\nDespite its limitations, this work lays the foundation for more exciting research. Promising directions include speeding up the sampling following the lessons learnt in the image domain, devising better embedding spaces for diffusion and investigating new in-filling capabilities.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured\n\ndenoising diffusion models in discrete state-spaces. In NeurIPS, pp. 17981–17993, 2021.\n\nYoshua Bengio, R ́ejean Ducharme, and Pascal Vincent. A neural probabilistic language model.\n\nAdvances in neural information processing systems, 13, 2000.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Char-\n\nlin. Language gans falling short. arXiv preprint arXiv:1811.02549, 2018.\n\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315–11325, 2022.\n\nTing Chen, Ruixiang Zhang, and Geoffrey E. Hinton. Analog bits: Generating discrete data using\n\ndiffusion models with self-conditioning. CoRR, abs/2208.04202, 2022.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019. URL https: //arxiv.org/abs/1901.02860.\n\nCyprien De Masson d’Autume, Shakir Mohamed, Mihaela Rosca, and Jack Rae. Training language\n\ngans from scratch. Advances in Neural Information Processing Systems, 32, 2019.\n\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In\n\nNeurIPS, pp. 8780–8794, 2021.\n\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In EMNLP/IJCNLP (1), pp. 6111–6120. Association for Computational Linguistics, 2019.\n\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696–10706, 2022.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2016. URL https://\n\narxiv.org/abs/1606.08415.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, pp. 6626–6637, 2017.\n\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS,\n\n2020.\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\n\ndegeneration. In ICLR. OpenReview.net, 2020.\n\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr ́e, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454–12465, 2021.\n\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\n\nbased generative models. arXiv preprint arXiv:2206.00364, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDiederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.\n\nCoRR, abs/2107.00630, 2021.\n\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword In EMNLP (Demonstration), pp. 66–71.\n\ntokenizer and detokenizer for neural text processing. Association for Computational Linguistics, 2018.\n\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto.\n\nDiffusion-lm improves controllable text generation. CoRR, abs/2205.14217, 2022.\n\nGautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with\n\ndiffusion models. In ISMIR, pp. 468–475, 2021.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 8162–8171. PMLR, 2021.\n\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and In ICML, volume 162 of Proceedings of Machine editing with text-guided diffusion models. Learning Research, pp. 16784–16804. PMLR, 2022.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022.\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer. High-\n\nresolution image synthesis with latent diffusion models. CoRR, abs/2112.10752, 2021.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. CoRR, abs/2205.11487, 2022.\n\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\n\npreprint arXiv:2202.00512, 2022. URL https://arxiv.org/abs/2202.00512.\n\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and A ̈aron van den Oord.\n\nStep-unrolled denoising autoencoders for text generation. In ICLR. OpenReview.net, 2022.\n\nStanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On accurate evaluation of gans for lan-\n\nguage generation. arXiv preprint arXiv:1806.04936, 2018.\n\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 2256–2265. JMLR.org, 2015.\n\nIlya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural net-\n\nworks. In ICML, 2011.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA MODEL ARCHITECTURE\n\nFor both the AR and the SED models, we use the same transformer (Vaswani et al., 2017) architecture, which are similar to those described in (Hoffmann et al., 2022), with relative positional encoding as described in (Dai et al., 2019) in the attention blocks, and with a 4x expansion and a Gelu (Hendrycks & Gimpel, 2016) non-linearity in the feed-forward blocks. The architecture hyper-parameters are detailed in table 9. Noised word embeddings, x ∈ RN ×D, are first passed through a linear projection that operates on each embedding independently to get a projected embedding whose feature dimension matches the width of the transformer, dmodel. At diffusion step t, we compute a time embedding as a sinusoidal position embedding (Vaswani et al., 2017) of size dmodel, which is then passed into a dmodel × dmodel linear layer and added to the projected embedding. We add a linear output projection layer E′ which takes the output of the transformer y ∈ RN ×dmodel and projects each element (yi)1≤i≤N back to the same size as the word embeddings. When using self-conditionning, we modify the input to the model by concatenating x and ˆx0 along the feature axis before passing them to the input projection layer.\n\nTable 9: Model hyper parameters.\n\nModel\n\nnumber of layers\n\nS L\n\n12 12\n\ndmodel\n\n896 1536\n\nnumber of heads\n\nhead size\n\n16 16\n\n64 128\n\nB FORWARD DIFFUSION PROCESS VISUALIZATION\n\nTo support the discussion on word embeddings dimension from Section 4.4, we present a visualization of the forward diffusion process. Given starting tokens x0, we project the noised tokens xt of the forward process at step t to their nearest neighbor among word embeddings E to obtain wt. We then store the 128 nearest neighbors N (w0) of starting tokens w0 = x0 and define the rank rt of wt at its index in N (w0). We display wt and highlight it in green if rt is close to zero (meaning wt is a close neighbor of w0) and in increasingly red colors otherwise. We present the first 16 nearest neighbors of w0 in Figure 2 and provide an illustration of the color code used for highlighting. Figure 3 shows an instance of the forward diffusion process while diffusing on embeddings of with a high dimension of 896 and Figure 4 shows diffusion on embeddings with a lower dimension of 32.\n\nWe observe that in high dimension, the noised token’s closest neighbour remains the original token up until the point where any token could be its closest neighbour. The diffusion random walk does not seem to pass through the neighbourhoods of semantically-related tokens. We hypothesize that the root cause of this issue is that the embedding space is mostly empty (with only 32000 points in R896, as dembed = 896); and that embeddings are potentially concentrating in a lower-dimensional space.\n\nIn contrast, in lower dimension we see meaningfully-related tokens appear as the corruption progresses (‘brown’ becomes ‘grey’, ‘quick’ becomes ‘swift’, ‘over’ becomes ‘underneath’ etc). We believe this more gradual information destruction is beneficial for the diffusion model.\n\nC ADDITIONAL SAMPLES\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Nearest neighbors of tokens from the sentence ”the quick brown fox jumps over the lazy dog”\n\nFigure 3: Visualization of the forward diffusion process up to 300 steps when diffusing on embeddings with dimension 896.\n\nFigure 4: Visualization of the forward diffusion process up to 300 steps when diffusing on embeddings with dimension 32.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Reverse diffusion process of SED-L with guidance scale 2.5.\n\nTable 10: Samples from SED-L.\n\nThis course is essential for the environment in which students choose the curriculum option study at JHD. Geographical Integration is at the heart of this department. Along with the urban infrastructural innovations of the mid-1990’s and social issues in the 21st Century. People within the department regularly exemplify the concept of real integration.\n\nThat did trigger some rewarding words or insights to begin preparing their kid for their future. Luckily, Mikaela has been nice enough to tolerate my questions about what parents can do to help, even during her summer vacation.\n\nI went to college at Boston University. After getting my degree, I decided to make a change! I enrolled in outdoor schools. After getting my degree, I loved jet skiing, offshore fishing, and Kitesurfing. The list became growing. More importantly, I started a career by fishing at sea. Now, I can’t get enough of the Pacific Ocean!\n\nThe beaver is an interesting animal that lives in rivers and waterfalls across Puerto Rico. It loves kayaking, fishing, and swimming. But, you want to know what are the animals behind the beaver?\n\nA year ago in Paris, I had the opportunity to take a field trip to La Rite-en-Laurences International de France where I met David Nigel Johnson, a professor of social studies. What a great trip and what a great day!\n\nA year ago in Paris, my friends and I went on a dirt road trip to the city. I remember walking through the church, its beautiful square, narrow streets lit with memorials and passing a terrible Catholic Bishop I am used to - what a sad day...\n\nThere was no evidence, only fleeting glimpses of the existence of cognitive disability. There was no luck and no solid science. It was all guesswork, the blinding prospect of pneumonia could spur imagination at the possibility of brain damage.\n\n14",
    "reference": "# Summary Of The Paper\n\nThis paper presents a method to train text based diffusion models which can generate text non-autoregressively modeling bidirectional context. Since diffusion models work in continuous domains only, the authors propose the following changes to the standard setup (1) represent a sequence of tokens as a sequence of pretrained embeddings (obtained from a BERT-based model) and compute a standard diffusion training loss (L2), (2) to recover the discrete tokens from predicted embeddings, add another loss to the diffusion training loss, which computes the cross entropy wrt the target token by projecting the predicted vector back to the vocabulary space using a learnable matrix, and (3) add self-conditioning to this framework following Chen at al 2022. Trained a large corpus (C4), the generated LM shows improvement over autoregressive LM on the computed quality and diversity metrics. Ablations have been conducted on the proposed changes and show the importance of each setup.\n\n# Strength And Weaknesses\n\nStrengths:\n1. This is the first work on diffusion LMs I have seen that show results on a simple unconstrained text generation baseline and seems to have positive results in terms of the metrics used. \n2. The writing is clear and the motivations seem sound. There are some clever ideas incorporated like padding in the middle of the sequence, self-conditioning which seem to give the results a boost. \n\nWeaknesses:\n1. There is a discussion in limitations on the inadequacy of automatic metrics but the authors do not evaluate a variety of them that prior work has done such as MAUVE, distinct-n, zipf score, repetition rate and many more. These metrics should be computed to a get better idea of the quality and diversity of the output. Unigram entropy is not what is usually reported but would provide a similar trend to dist-1 I would imagine. Please see these this paper for the description of these metrics: https://arxiv.org/abs/2202.00666.\n2. There are missing baselines such as Li et al 2022's diffusion LM which the authors describe as not being good for large datasets but this claim is not verified. The ablation in Table 5 with random embeddings is not the same loss function as far as I can tell as the diffusion LM paper.\n3. The model rely on a pretrained BERT token embeddings which just seems like an arbitrary choice. An ablation should be provided with different pretrained embeddings to understand the impact of this choice.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is clearly written and easy to follow. The ideas are mostly novel and seem reproducible.\n\n# Summary Of The Review\n\nThe paper presents a diffusion language model by applying diffusion on pretrained embeddings. The evaluation seems to suggest that the outputs from this model have better quality and diversity scores than autoregressive LMs, however some important metrics are missing so it's difficult to clearly say if the model is better than the baselines.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nAs with any text generation models, there are concerns that this model might generate toxic, offensive and discriminatory text. Hence, this paper should be accompanied by an ethics statement which it currently does not seem to have."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS\n\nDenny Zhou∗ Nathanael Sch ̈arli Le Hou Dale Schuurmans Claire Cui Olivier Bousquet Quoc Le Ed Chi\n\nJason Wei Nathan Scales Xuezhi Wang\n\nGoogle Research, Brain Team\n\nABSTRACT\n\nChain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.\n\n1\n\nINTRODUCTION\n\nDespite the great success of deep learning in the past decade, there still remain huge differences between human intelligence and machine learning: (1) Given a new task, humans usually can learn to accomplish it from only a few demonstration examples, while machine learning requires a large amount of labeled data for model training; (2) Humans can clearly explain the underlying rationale for their predictions or decisions, while machine learning is essentially a black box; (3) Humans can solve problems more difficult than any they have seen before, while for machine learning, examples in training and testing are typically at the same level of difficulty.\n\nThe recently proposed chain-of-thought prompting approach (Wei et al., 2022; Chowdhery et al., 2022) has taken a significant step for narrowing the gap between human intelligence and machine intelligence. It combines the idea of natural language rationales (Ling et al., 2017; Cobbe et al., 2021) with few-shot prompting (Brown et al., 2020). When further integrated with self-consistency decoding (Wang et al., 2022b) rather than using the typical greedy decoding, few-shot chain-of-thought prompting largely outperforms the state-of-the-art results in the literature on many challenging natural language processing tasks obtained from specially designed neural models trained with hundreds of times more annotated examples, while being fully interpretable.\n\nHowever, chain-of-thought prompting has a key limitation—it often performs poorly on tasks that require generalization of solving problems harder than the demonstration examples, such as compositional generalization (Lake & Baroni, 2018; Keysers et al., 2020). To tackle such easy-to-hard generalization issues, we propose least-to-most prompting. It consists of two stages: first decomposing a complex problem into a list of easier subproblems, and then sequentially solving these\n\n∗Corresponding to: dennyzhou@google.com\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nsubproblems, whereby solving a given subproblem is facilitated by the answers to previously solved subproblems. Both stages are implemented by few-shot prompting, so that there is no training or finetuning in either stage. An example usage of least-to-most prompting is illustrated in Figure 1.\n\nThe term least-to-most prompting is borrowed from educational psychology (Libby et al., 2008), where it is used to denote the technique of using a progressive sequence of prompts to help a student to learn a new skill. Here we apply this technique for teaching humans to teach language models. Empirical results on symbolic manipulation, compositional generalization, and math reasoning show that least-to-most prompting can indeed generalize to problems harder than those demonstrated.\n\nFigure 1: Least-to-most prompting solving a math word problem in two stages: (1) query the language model to decompose the problem into subproblems; (2) query the language model to sequentially solve the subproblems. The answer to the second subproblem is built on the answer to the first subproblem. The demonstration examples for each stage’s prompt are omitted in this illustration.\n\n2 LEAST-TO-MOST PROMPTING\n\nLeast-to-most prompting teaches language models how to solve a complex problem by decomposing it to a series of simpler subproblems. It consists of two sequential stages:\n\n1. Decomposition. The prompt in this stage contains constant examples that demonstrate the\n\ndecomposition, followed by the specific question to be decomposed.\n\n2. Subproblem solving. The prompt in this stage consists of three parts: (1) constant examples demonstrating how subproblems are solved; (2) a potentially empty list of previously answered subquestions and generated solutions, and (3) the question to be answered next.\n\nIn the example shown in Figure 1, the language model is first asked to decompose the original problem into subproblems. The prompt that is passed to the model consists of examples that illustrate how to decompose complex problems (which are not shown in the figure), followed by the specific problem to be decomposed (as shown in the figure). The language model figures out that the original problem can be solved via solving an intermediate problem “How long does each trip take?”.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nIn the next phase, we ask the language model to sequentially solve the subproblems from the problem decomposition stage. The original problem is appended as the final subproblem. The solving starts from passing to the language model a prompt that consists of examples that illustrate how problems are solved (not shown in the figure), followed by the first subproblem “How long does each trip take?”. We then take the answer generated by the language model (“... each trip takes 5 minutes.”) and construct the next prompt by appending the generated answer to the previous prompt, followed by the next subproblem, which happens to be the original problem in this example. The new prompt is then passed back to the language model, which returns the final answer.\n\nLeast-to-most prompting can be combined with other prompting techniques like chain-of-thought (Wei et al., 2022) and self-consistency (Wang et al., 2022b), but does not need to be. Also, for some tasks, the two stages in least-to-most prompting can be merged to form a single-pass prompt.\n\n3 RESULTS\n\nWe present least-to-most prompting results for symbolic manipulation, compositional generalization, and math reasoning tasks, and compare it with chain-of-thought prompting.\n\n3.1 SYMBOLIC MANIPULATION\n\nWe take the last-letter-concatenation task (Wei et al., 2022). In this task, each input is a list of words, and the corresponding output is the concatenation of the last letters of the words in the list. For example, “thinking, machine” outputs “ge”, since the last letter of “thinking” is “g” and the last letter of “machine” is “e”. Chain-of-thought prompting does a perfect job when the testing lists have the same length as the lists in the prompt exemplars. However, it performs poorly when the testing lists are much longer than the lists in the prompt exemplars. We show that least-to-most prompting overcomes this limitation and significantly outperforms chain-of-thought prompting on length generalization.\n\nQ: “think, machine, learning” A: “think”, “think, machine”, “think, machine, learning”\n\nTable 1: Least-to-most prompt context (decomposition) for the last-letter-concatenation task. It can decompose arbitrary long lists into sequential sublists with an accuracy of 100%.\n\nQ: “think, machine” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. Concatenating “k”, “e” leads to “ke”. So, “think, machine” outputs “ke”.\n\nQ: “think, machine, learning” A: “think, machine” outputs “ke”. The last letter of “learning” is “g”. Concatenating “ke”, “g” leads to “keg”. So, “think, machine, learning” outputs “keg”.\n\nTable 2: Least-to-most prompt context (solution) for the last-letter-concatenation task. The two exemplars in this prompt actually demonstrate a base case and a recursive step.\n\nLeast-to-most prompting. The least-to-most prompt contexts for the last-letter-concatenation task are shown in Tables 1 and 2. The exemplar in Table 1 demonstrates how to decompose a list into a sequence of sublists. The exemplar in Table 2 demonstrates how to map an input to the desired output. Given a new list, we first append it to the exemplar in Table 1 to construct the decomposition prompt, which is sent to the language model to obtain the list’s decomposition. Then, we construct for each sublist S a solution prompt, which consists of the exemplars in Table 2, followed by the previous sublist/response pairs (if any), followed by S. We sequentially issue these prompts to the language model and use the last response as the final solution.\n\nIt is worth a closer look at the exemplars in Table 2. Essentially, they teach language models how to build answers to new problems using the answers to previously solved problems: (1) the list in the\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nsecond exemplar (“think, machine, learning”) is an extension of the list in the first exemplar (“think, machine”) rather than an entirely independent one; (2) the response to “think, machine, learning” is built on the output of “think, machine” by starting with a sentence saying that “think, machine” outputs “ke”. The two exemplars together illustrate a base case and a recursive step.\n\nChain-of-thought prompting. the last-letterconcatenation task is listed in Table 3. It uses the same lists as the least-to-most prompt in Table 2. The only difference is that, in the chain-of-thought prompt, the response to the second list (“think, machine, learning”) is built from scratch, instead of using the output of the first list (“think, machine”).\n\nThe chain-of-thought prompt context\n\nfor\n\nQ: “think, machine” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. Concatenating “k”, “e” leads to “ke”. So, “think, machine” outputs “ke”.\n\nQ: “think, machine, learning” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. The last letter of “learning” is “g”. Concatenating “k”, “e”, “g” leads to “keg”. So, “think, machine, learning” outputs “keg”.\n\nTable 3: Chain-of-thought prompt context for the last-letter-concatenation task. Unlike the leastto-most prompt in Table 2, the exemplars in the chain-of-thought prompt are independent of each other.\n\nWe compare least-to-most prompting (Table 1 & 2) with chain-of-thought prompting (Table 3) and the standard few-shot prompting. The prompt for the standard few-shot prompting is constructed by removing the intermediate explanations in the chain-of-thought prompt. That is, it just consists of these two exemplars: (1) “think, machine” outputs “ke”; and (2) “think, machine, learning” outputs “keg”. We do not consider a training or finetuning baseline because a machine learning model based on two examples would generalize very poorly.\n\nResults. We randomly sample words in Wiktionary1 to construct testing lists with lengths varying from 4 to 12. For each given length, 500 lists are constructed. The accuracies of different methods with code-davinci-002 in GPT-3 are shown in Table 4. Standard prompting completely fails all test cases with an accuracy of 0. Chain-of-thought prompting significantly boosts the performance over standard prompting, but it still falls well behind least-to-most prompting, particularly when the lists are long. Moreover, the performance of chain-of-thought prompting drops much faster than least-to-most prompting as the length increases.\n\nL = 4 L = 6 L = 8 L = 10 L = 12\n\nStandard prompting Chain-of-Thought Least-to-Most\n\n0.0 84.2 94.0\n\n0.0 69.2 88.4\n\n0.0 50.2 83.0\n\n0.0 39.8 76.4\n\n0.0 31.8 74.0\n\nTable 4: Accuracies of different prompting methods on the last-letter-concatenation task. The length of testing lists increases from 4 to 12.\n\nIn Appendices 7.2 and 7.3, we present additional experiments with different chain-of-thought prompts and different language models. Note that in contrast to least-to-most prompting, the exemplars in a chain-of-thought prompt can be independent of each other. For the last-letter concatenation task, this means that we do not need to present exemplars that are sublists of other exemplars. In fact, a chain-of-thought prompt with independent lists tends to outperform one with dependent lists, as the former conveys more information. Furthermore, we can enhance chain-of-thought prompting by incorporating additional exemplars. This seems to be fair, as the least-to-most prompt contains more words due to its extra decomposition. As shown in Table 13 (Appendix 7.3), for lists with length 12, chain-of-thought prompting achieves an accuracy of 37.4% with 4 independent exemplars (Appendix 7.2.2), and 38.4% with 8 independent exemplars (Appendix 7.2.3). Although there\n\n1https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/\n\n1-10000\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nhave been notable advancements compared to an accuracy of 31.8% by the original prompt in Table 3, chain-of-thought prompting still lags behind least-to-most prompting, which boasts an accuracy of 74.0%.\n\nError analysis. While least-to-most prompting significantly outperforms chain-of-thought prompting, it is still far from achieving 100% accuracy for long lists. In Appendix 7.4, we present a detailed error analysis. We find that only very few of them are due to incorrect last letters, while most of them are concatenation errors (dropping or adding a letter). For example, given the list “gratified, contract, fortitude, blew”, the model drops the last letter in the concatenation of “dte” and “w”, and thus predicts the outcome to be “dte” instead of “dtew”. In another example “hollow, supplies, function, gorgeous”, the model somehow duplicates the last letter “s” in the concatenation of “wsn” and “s”, and thus the prediction becomes “wsnss” instead of “wsns”.\n\n3.2 COMPOSITIONAL GENERALIZATION\n\nSCAN (Lake & Baroni, 2018) is probably the most popular benchmark for evaluating compositional generalization. It requires mapping natural language commands to action sequences (Table 5). Sequence-to-sequence models perform poorly under length split where the action sequences in the training set (about 80% of the full set with over 20,000 examples) are shorter than the action sequences in the testing set. Many specialized neural-symbolic models have been proposed to solve SCAN (Chen et al., 2020; Liu et al., 2020; Nye et al., 2020; Shaw et al., 2021; Kim, 2021). We show that large language models with least-to-most prompting can solve SCAN using only a few demonstration examples. No training or finetuning is needed.\n\nCommand “look thrice after jump” “run left and walk” “look opposite right”\n\nAction Sequence JUMP LOOK LOOK LOOK TURN LEFT RUN WALK TURN RIGHT TURN RIGHT LOOK\n\nTable 5: Example commands in SCAN and their corresponding action sequences. An agent successfully executes a natural language command by performing its corresponding action sequence.\n\nLeast-to-most prompting. Like the last-letter-concatenation task in Section 3.1, least-to-most prompting for SCAN is based on two kinds of prompts: (1) a command decomposition prompt containing 8 exemplars to demonstrate how to decompose a long command to a list of short commands (see Table 6 for some of these exemplars); and (2) a command mapping prompt containing 14 exemplars to demonstrate how to map natural language commands to action sequences (see Table 7 for some of these exemplars). The full prompt contexts for both decomposition and mapping are listed in Appendix 8. The exemplars used in the command-mapping prompt are intended to fully cover the semantics of the SCAN commands.\n\nQ: “look opposite right thrice after walk” A: “look opposite right thrice” can be solved by: “look opposite right”, “look opposite right thrice”. “walk” can be solved by “walk”. So, “look opposite right thrice after walk” can be solved by: “look opposite right”, “look opposite right thrice”, “walk”.\n\nQ: “look around right thrice and walk” A: “look around right thrice” can be solved by: “look right”, “look around right”, “look around right thrice”. “walk” can be solved by “walk”. So, “look around right thrice and walk” can be solved by: “look right”, “look around right”, “look around right thrice”, “walk”.\n\nTable 6: Two exemplars taken from the least-to-most prompt for decomposing a long command to a sequence of short commands.\n\nWe use Python notation to make our prompts in least-to-most prompting and the baselines (stand few-shot prompting and chain-of-thought prompting) concise and meet the input size limit of language models (usually up to 2048 tokens). For example, in our prompt design, we map “look twice”\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nto “LOOK” * 2 rather than “LOOK LOOK”. We may have to point out that these Python expressions are just intermediate representations. After obtaining responses from language models, we run a postprocessing script to expand the Python expressions to generate the final results. However, it should not be surprising that we can also further teach language models to correctly process expressions. In Appendix 8.4, we show that with several demonstration exemplars, language models are able to expand Python expressions with a near-perfect accuracy of 99.7%.\n\nQ: “jump left” A: The output of “jump left” concatenates: the output of “turn left”, the output of “jump”. “turn left” outputs “TURN LEFT”. “jump” outputs “JUMP”. So concatenating the output of “turn left” and the output of “jump” leads to “TURN LEFT” + “JUMP”. So the output of “jump left” is “TURN LEFT” + “JUMP”.\n\nQ: “run and look twice” A: The output of “run and look twice” concatenates: the output of “run”, the output of “look twice”. “run” outputs “RUN”. “look twice” outputs “LOOK” * 2. So concatenating the output of “run” and the output of “look twice” leads to “RUN” + “LOOK” * 2. So the output of “run and look twice” is “RUN” + “LOOK” * 2.\n\nTable 7: Two exemplars taken from the least-to-most / chain-of-thought prompt for mapping commands to action sequences. Python expressions are used as intermediate representations.\n\nChain-of-thought prompting. The chain-of-thought prompt for SCAN uses the same commandmapping context as least-to-most prompting (see Table 7) but it does not use command decomposition, which is exclusively used for least-to-most prompting.\n\nResults. We compare least-to-most prompting with chain-of-thought prompting and standard fewshot prompting. The exemplars for standard few-shot prompting are derived from the chain-ofthought prompt by removing the intermediate explanations. The accuracies of different prompting methods with different language models are presented in Table 8. Example outputs can be found in Appendix 8.3. Using code-davinci-002, least-to-most prompting achieves an accuracy of 99.7% under length split. We also test least-to-most prompting on all other splits and even the full SCAN dataset. We find that its solving rate remains the same. In addition, it may be interesting to note that code-davinci-002 consistently outperforms text-davinci-002, regardless of the prompting method.\n\nMethod code-davinci-002 text-davinci-002 code-davinci-001\n\nStandard prompting Chain-of-Thought Least-to-Most\n\n16.7 6.0 0.4\n\n16.2 0.0 0.0\n\n99.7 76.0 60.7\n\nTable 8: Accuracies (%) of different prompting methods on the test set of SCAN under length split. The results of text-davinci-002 are based on a random subset of 100 commands.\n\nError analysis. In the test set of the length split, there are 13 failures in total from least-to-most prompting: 6 of them incorrectly interpret “twice” and “thrice” following “around”, and the rest incorrectly interpret “after” as “and”. Let us show a failed example for each category. In the example “walk opposite right twice after run around right thrice”, code-davinci-002 correctly translates the expression “run around right” to (“TURN RIGHT” + “RUN”) * 4. Then it makes a mistake when applying “thrice” to this expression and produces (“TURN RIGHT” + “RUN”) * 9 instead of (“TURN RIGHT” + “RUN”) * 4 * 3 or (“TURN RIGHT” + “RUN”) * 12. In the example “run opposite left thrice after run around left twice”, code-davinci-002 produces the correct translations for both sub-expressions that are connected by “after” but it combines them as if they were connected by “and”. This means that the model produces (“TURN LEFT” * 2 + “RUN”) * 3 + (“TURN LEFT” + “RUN”) * 4 * 2 instead of (“TURN LEFT” + “RUN”) * 4 * 2 + (“TURN LEFT” * 2 + “RUN”) * 3. A detailed error analysis can be found in Appendix 8.2.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n3.3 MATH REASONING\n\nIn this section, we apply least-to-most prompting to solve math word problems in GSM8K (Cobbe et al., 2021) and DROP (Dua et al., 2019). We are particularly interested at seeing if large language models combined with least-to-most prompting can solve problems more difficult than those seen in the prompts. Here, we simply measure the difficulty by the number of solving steps.\n\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do they have together?\n\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. 2. Elsa and Anna have 5 + 7 = 12 apples together.\n\nThe answer is: 12.\n\nTable 9: The least-to-most prompt for solving GSM8K. The demonstration problem is solved in only two steps, but the prompt can handle problems that require multiple steps to be solved.\n\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. So Elsa and Anna have 5 + 7 = 12 apples together.\n\nThe answer is: 12.\n\nTable 10: The chain-of-thought prompt for solving GSM8K. It is derived from the least-to-most prompt in Table 9 by removing the decomposition part.\n\nThe prompt that we design to solve GSM8K is shown in Table 9. The demonstration exemplar consists of two parts. The first part (starting from “Let’s break down this problem . . . ”) shows how the original problem can be decomposed into simpler subproblems, and the the second part shows how the subproblems are solved in sequence. Note that this prompt combines decomposition and subproblem solving into a single pass. One may instead design two different prompts respectively for decomposition and subproblem solving, as the least-to-most prompts in the previous sections, to further improve performance. Here, we focus on investigating how this simple least-to-most prompt generalizes from a simple 2-step problem to more complex multi-step problems.\n\nWe also construct a chain-of-thought prompt (Table 10) as our baseline. It is derived from the leastto-most prompt (Table 9) by removing the decomposition part. The results are shown in Table 11. Overall, least-to-most prompting only slightly improves chain-of-thought prompting: from 60.97% to 62.39%. However, least-to-most prompting essentially improves chain-of-thought prompting in solving problems which need at least 5 steps to be solved: from 39.07% to 45.23% (Table 12). We find that almost every problem in GSM8K that least-to-most prompting fails to solve can be eventually solved by using a manually crafted decomposition. This should not be surprising. For our humans, as long as we know how to decompose a complex problem into simpler subproblems, we actually have solved it. For the DROP benchmark, least-to-most prompting outperforms chainof-thought prompting by a large margin (Table 11). That is probably because most problems in DROP can be trivially decomposed.\n\nMethod Zero-Shot Standard prompting Chain-of-Thought Least-to-Most\n\nNon-football (DROP) 43.86 58.78 74.77 82.45\n\nFootball (DROP) GSM8K\n\n51.77 62.73 59.56 73.42\n\n16.38 17.06 60.87 62.39\n\nTable 11: Accuracies (%) of different prompting methods on GSM8K and DROP (only the subset containing numerical problems). The base language model is code-davinci-002.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nAccuracy by Steps (GSM8K) Least-to-Most Chain-of-Thought\n\nAll 62.39 60.87\n\n2 Steps 74.53 76.68\n\n3 Steps 68.91 67.29\n\n4 steps ≥ 5 steps 59.73 59.39\n\n45.23 39.07\n\nTable 12: Accuracies (%) of least-to-most prompting and chain-of-thought prompting, broken down by the number of reasoning steps required in the expected solution.\n\n4 RELATED WORK\n\nCompositional generalization. SCAN (Lake & Baroni, 2018) is a widely used benchmark to evaluate compositional generalization. Among all of its splits, the most challenging is the length split, which requires a model to generalize to test sequences longer than training ones. Prior work with good performance on SCAN mostly proposed neural-symbolic architectures (Chen et al., 2020; Liu et al., 2020) and grammar induction techniques (Nye et al., 2020; Shaw et al., 2021; Kim, 2021). Chen et al. (2020) proposed the neural-symbolic stack machine, which contains a neural network as the controller to generate an execution trace for a given input, and a symbolic stack machine to execute the trace and produce the output. The execution trace consists of domain-specific primitives for sequence manipulation, which allows the machine to break down the input sentence into different components, translate them separately, and compose them together. Liu et al. (2020) proposed a framework that cooperatively learns two neural modules, a composer and a solver, to jointly learn the input structure and the symbolic grammar rules. Both Nye et al. (2020) and Shaw et al. (2021) inferred the symbolic grammar rules of SCAN, while Kim (2021) proposed to learn a latent neural grammar. While approaches with symbolic components are able to achieve 100% accuracy on SCAN (Chen et al., 2020; Liu et al., 2020; Nye et al., 2020; Shaw et al., 2021), they require complicated model training and grammar inference algorithms to search in a large grammar space. Another line of work on SCAN designs data augmentation schemes (Andreas, 2020; Aky ̈urek et al., 2021; Lake, 2019). Both Andreas (2020) and Aky ̈urek et al. (2021) construct synthetic training samples by recombining fragments occurring in different training samples, and Aky ̈urek et al. (2021) further designs a sampling scheme that encourages the recombination model to produce rare samples. On the other hand, Lake (2019) proposed a meta training algorithm, which requires a meta-grammar space to construct training data, and the format of sampled grammars is similar to the SCAN grammar. While these data augmentation techniques improve the performance on several compositional generalization benchmarks, they fail to solve the length split of SCAN. Other prior works propose neural network architectures to improve compositional generalization, where they encourage the model to learn the word and span mapping (Russin et al., 2019; Li et al., 2019), the alignment of input and output as span trees (Herzig & Berant, 2021), and the permutation equivariance of input and output words (Gordon et al., 2020). Still, these end-to-end neural networks without symbolic components do not generalize to longer test inputs. Unlike the existing work, we demonstrate that without model architectures and symbolic components specially designed to improve compositional generalization, least-to-most prompting achieves 99.7% accuracy on any split (including length split) with only a handful of demonstration examples, and it does not require any training or finetuning.\n\nEasy-to-hard generalization. In addition to compositional generalization, there are many other tasks where the test cases require more reasoning steps to solve than the training examples, for example, the last-letter-concatenation task where the test lists are longer than the demonstration examples. Dong et al. (2019) propose Neural Logic Machines (NLMs) for both inductive learning and logic reasoning. NLMs trained on small-scale tasks (such as small size block worlds) can perfectly generalize to large-scale tasks (such as larger size block worlds). Schwarzschild et al. (2021) show that recurrent networks trained to solve simple problems with few recurrent steps (such as small size mazes or chess puzzles) can solve more complex problems (such as larger size mazes or chess puzzles) by performing additional recurrences during inference. In our method, we achieve easy-to-hard generalization by decomposing a complex problem into a series of easier problems.\n\nTask decomposition. Perez et al. (2020) decompose a multi-hop question into a number of independent single-hop subquestions, which are answered by an off-the-shelf question answering (QA) model. Then those answers are aggregated to form the final answer. Both question decomposition and answer aggregation are implemented by trained models. Wang et al. (2022a) conducts multi-hop QA by modeling prompts as continuous virtual tokens and progressively eliciting relevant knowl-\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nedge from language models via iterative prompting. Unlike these methods, our approach does not involve any training or finetuning. Moreover, the subquestions generated in least-to-most prompting are usually dependent and have to be sequentially solved in a specific order so that answers to some subquestions can be used as building blocks to solve other subquestions. Yang et al. (2022) translate natural language questions to SQL queries by decomposing a question into a sequence of slot-filling natural language prompts corresponding to SQL clauses via a rule-based system. Wu et al. (2022) propose chaining large language model steps such that the output of one step becomes the input for the next and develop an interactive system for users to construct and modify chains. Least-to-most prompting chains the processes of problem decomposition and subproblem solving.\n\n5 LIMITATIONS\n\nDecomposition prompts typically don’t generalize well across different domains. For instance, a prompt that demonstrates decomposing math word problems (as seen in Table 9) isn’t effective for teaching large language models to break down common sense reasoning problems, such as “Did Aristotle use a laptop?” (Geva et al., 2021). A new prompt must be designed to demonstrate decomposition for these types of problems in order to achieve optimal performance.\n\nGeneralizing decomposition can even be difficult within the same domain. We’ve observed that nearly all problems in GSM8K can be accurately solved if the large language models are provided with the correct decomposition of those challenging problems. This finding isn’t surprising and aligns with our experiences in solving math problems. Whenever we successfully break down a math problem into simpler subproblems we can solve, we’ve essentially solved the original problem. Exceptional results are achieved on the last-letter-concatenation task and the SCAN benchmark because decomposition in these tasks is relatively straightforward.\n\n6 CONCLUSION AND DISCUSSION\n\nWe introduced least-to-most prompting to enable language models to solve problems that are harder than those in the prompt. This approach entails a two-fold process: a top-down decomposition of the problem and a bottom-up resolution generation. Our empirical findings, which encompass symbolic manipulation, compositional generalization, and mathematical reasoning, reveal that least-to-most prompting significantly surpasses standard prompting and chain-of-thought prompting.\n\nIn general, prompting might not be the optimal method for teaching reasoning skills to large language models. Prompting can be viewed as a unidirectional communication form in which we instruct a language model without considering its feedback. A natural progression would be to evolve prompting into fully bidirectional conversations, enabling immediate feedback to language models, thereby facilitating more efficient and effective learning. The least-to-most prompting technique represents a stride towards instructing language models through such bidirectional interactions.\n\nACKNOWLEDGEMENT\n\nWe sincerely thank Xinyun Chen, Xinying Song, Jeff Dean, Zoubin Ghahramani, Fernando Pereira, Jacob Devlin, and Pete Shaw for sharing their valuable knowledge and advice during our discussions. Their expertise greatly improved the quality of our work. Additionally, we are grateful to the anonymous reviewers for their careful review and helpful suggestions, which helped shape our manuscript into its final form.\n\nREFERENCES\n\nEkin Aky ̈urek, Afra Feyza Aky ̈urek, and Jacob Andreas. Learning to recombine and resample data for compositional generalization. In International Conference on Learning Representations, 2021.\n\nJacob Andreas. Good-enough compositional data augmentation. In Annual Meeting of the Associa-\n\ntion for Computational Linguistics, 2020.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional generalization via neural-symbolic stack machines. Advances in Neural Information Processing Systems, 33:1690–1701, 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\nHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic\n\nmachines. In International Conference on Learning Representations, 2019.\n\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.\n\nJonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations, 2020.\n\nJonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generaliza-\n\ntion. In Annual Meeting of the Association for Computational Linguistics, 2021.\n\nDaniel Keysers, Nathanael Sch ̈arli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generalization: A comprehensive method on realistic data. International Conference on Learning Representations, 2020.\n\nYoon Kim. Sequence-to-sequence learning with latent neural grammars. Advances in Neural Infor-\n\nmation Processing Systems, 34, 2021.\n\nBrenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pp. 2873–2882. PMLR, 2018.\n\nBrenden M Lake. Compositional generalization through meta sequence-to-sequence learning. Ad-\n\nvances in neural information processing systems, 32, 2019.\n\nYuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hestness. Compositional generalization for primitive substitutions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4284–4293, 2019.\n\nMyrna E Libby, Julie S Weiss, Stacie Bancroft, and William H Ahearn. A comparison of most-toleast and least-to-most prompting on the acquisition of solitary play skills. Behavior analysis in practice, 1(1):37–43, 2008.\n\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nQian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. Advances in Neural Information Processing Systems, 33:11416–11427, 2020.\n\nMaxwell Nye, Armando Solar-Lezama, Josh Tenenbaum, and Brenden M Lake. Learning compositional rules via neural program synthesis. Advances in Neural Information Processing Systems, 33:10832–10842, 2020.\n\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8864–8880, 2020.\n\nJake Russin, Jason Jo, Randall C O’Reilly, and Yoshua Bengio. Compositional generalization in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708, 2019.\n\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 922–938, 2021.\n\nBoshi Wang, Xiang Deng, and Huan Sun. Shepherd pre-trained language models to develop a train\n\nof thought: An iterative prompting approach. arXiv preprint arXiv:2203.08383, 2022a.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Brian Ichter, Fei Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 2022.\n\nTongshuang Wu, Michael Terry, and Carrie Jun Cai. AI chains: Transparent and controllable humanAI interaction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems, pp. 1–22, 2022.\n\nJingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. Seqzero: Few-shot compositional semantic parsing with sequential prompts and zero-shot models. arXiv preprint arXiv:2205.07381, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nTable of Contents\n\n7 Last-letter-concatenation\n\n.\n\n.\n\n.\n\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n7.1 Prompt context for decomposing a word list into subproblems . .\n7.2 Prompt contexts with more and different examples . .\n. .\n. .\n.\n\n. .\n. 7.2.1 .\nStandard prompting, 4-shot .\n7.2.2 Chain-of-thought prompting, 4-shot . .\n7.2.3 Chain-of-thought prompting, 8-shot . 7.2.4 Chain-of-thought prompting, 2-shot, same examples as for least-to-most . .\n7.2.5 Least-to-most prompting, 4-shot .\n. .\n. .\n. .\n.\n\n. .\n. .\n7.5.1 .\n7.5.2 Chain-of-thought prompting: Success . 7.5.3 Chain-of-thought prompting: Failure . .\n7.5.4 Least-to-most prompting: Success .\n7.5.5 Least-to-most prompting: Failure .\n\n. 7.3 Data Generation and additional results .\n7.4 Error analysis: Least-to-most prompting . 7.5 Example outputs from code-davinci-002 . .\n\nStandard prompting: Failure\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n\n.\n\n.\n\n.\n\n8 SCAN\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n\n. .\n\n. .\n.\n\n. .\n. .\n\n8.1 Prompt contexts .\n\n. Standard prompting .\n\n. .\n. .\n. 8.1.1 .\n. 8.1.2 Least-to-most prompting . .\n. 8.1.3 Chain-of-thought prompting . 8.2 Error analysis: Least-to-most prompting . 8.3 Example outputs from code-davinci-002 .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. 8.3.1 Chain-of-thought prompting: Success . .\n8.3.2 Chain-of-thought prompting: Failure . .\n. 8.3.3 Least-to-most prompting: Success .\n. 8.3.4 Least-to-most prompting: Failure . 8.4 Expanding Python expressions using prompting .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n9 DROP\n\n.\n\n.\n\n.\n\n.\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n9.3 Football subset .\n\n9.1 Results with text-davinci-002 and LM-540B . .\n. .\n9.2 Non-football Subset .\n. .\n. .\n9.2.1 Zero-shot prompting . .\n9.2.2 .\nStandard prompting with 3 examples . .\n. 9.2.3 Chain-of-thought prompting with 3 examples . 9.2.4 Least-to-most prompting I: problem decomposition (5 examples) .\n9.2.5 Least-to-most prompting II: problem solving (3 examples) . .\n. .\n. .\n. .\n. .\n. .\n9.3.1 Zero-shot prompting . .\n. 9.3.2 .\nStandard prompting with 3 examples . 9.3.3 Chain-of-thought prompting with 3 examples . .\n. 9.3.4 Least-to-most prompting I: problem decomposition (6 examples) .\n9.3.5 Least-to-most prompting II: problem solving (3 examples) . .\n. .\n.\n\n. 9.4 Examples where least-to-most succeeded but chain-of-thought failed . .\n. .\n. .\n.\n\n9.4.1 Case 1 . 9.4.2 Case 2 . 9.4.3 Case 3 .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n.\n\n12\n\n14 14 14 14 14 15 15 15 16 17 18 18 19 21 22 25\n\n28 28 29 29 31 31 33 33 35 37 40 45\n\n46 46 46 46 47 47 48 48 49 49 49 49 50 51 52 52 52 53\n\nPublished as a conference paper at ICLR 2023\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n9.4.4 Case 4 . 9.4.5 Case 5 .\n\n. .\n. .\n9.5 Error analysis: Least-to-most prompting .\n\n. .\n. 9.5.1 Example of wrong problem decomposition . .\n9.5.2 Example of wrong problem solving . .\n. 9.5.3 Example of wrong given label .\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n.\n\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n\n10 GSM8K\n\n.\n\n10.3.1 Chain-of-Thought (1-shot) .\n10.3.2 Least-to-Most (1-shot)\n\n10.1 Experiment results: One-shot prompts 10.2 Experiment results: Engineered prompts .\n10.3 Prompt contexts: One-shot prompts . .\n. .\n. 10.4 Prompt contexts: Engineered prompts . .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n10.4.1 Zero-Shot .\n. 10.4.2 Standard prompting: 4 examples .\n. 10.4.3 Chain-of-Thought (best): 4 examples . 10.4.4 Least-to-Most (best) I - problem decomposition: 7 examples . .\n10.4.5 Least-to-Most (best) II - problem solving: 4 examples .\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n54 54 54 55 55 55\n\n56 56 56 57 58 58 58 58 58 59 59 60\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n7 LAST-LETTER-CONCATENATION\n\n7.1 PROMPT CONTEXT FOR DECOMPOSING A WORD LIST INTO SUBPROBLEMS\n\nIn Section 3.1 we mentioned that language model prompting can be used to decompose a word list such as “think, machine, learning, reasoning” into a sequence of subproblems “think, machine”, “think, machine, learning”, and “think, machine, learning, reasoning”.\n\nThe following prompt context achieves 100% accuracy on this task when using the text-davinci-002 model. Note that it achieves perfect accuracy on lists up to size 12 (which is the maximum that we tested for our experiment) even though it only contains one exemplar each for lists of sizes 2 and 3.\n\nQ: “machine, learning” A: creating sequential sublists of the list “machine, learning”: “machine” “machine, learning”\n\nQ: “machine, learning, artificial” A: creating sequential sublists of the list “machine, learning, artificial”: “machine” “machine, learning” “machine, learning, artificial”\n\n7.2 PROMPT CONTEXTS WITH MORE AND DIFFERENT EXAMPLES\n\nThe last-letter-concatenation experiments presented in Section 3.1 are based on prompt contexts that consists of 2 demonstration examples. To make sure that the accuracy gain achieved by leastto-most prompting is not caused by the slight increase in example length when compared to chainof-thought, we also performed experiments with more context examples so that we can compare least-to-most vs. chain-of-thought for different prompt sizes. Also, we perform experiments where we use for chain-of-thought prompting the same prompt examples that we use for least-to-most prompting (unlike the situation in Table ?? where we use different examples). All these prompts are shown below, and we present and discuss the corresponding accuracies in Section 7.3.\n\n7.2.1 STANDARD PROMPTING, 4-SHOT\n\nQ: “think, machine” A: “ke”\n\nQ: “learning, reasoning, generalization” A: “ggn”\n\nQ: “artificial, intelligence” A: “le”\n\nQ: “transformer, language, vision” A: “ren”\n\n7.2.2 CHAIN-OF-THOUGHT PROMPTING, 4-SHOT\n\nQ: “think, machine” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. Concatenating “k”, “e” leads to “ke”. So, “think, machine” outputs “ke”.\n\nQ: “learning, reasoning, generalization” A: The last letter of “learning” is “g”. The last letter of “reasoning” is “g”. The last letter of “generalization” is “n”. Concatenating “g”, “g”, “n” leads to “ggn”. So, “learning, reasoning, generalization” outputs “ggn”.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nQ: “artificial, intelligence” A: The last letter of “artificial” is “l”. The last letter of “intelligence” is “e”. Concatenating “l”, “e” leads to “le”. So, “artificial, intelligence” outputs “le”.\n\nQ: “transformer, language, vision” A: The last letter of “transformer” is “r”. The last letter of “language” is “e”. The last letter of “vision” is “n”. Concatenating “r”, “e”, “n” leads to “ren”. So, “transformer, language, vision” outputs “ren”.\n\n7.2.3 CHAIN-OF-THOUGHT PROMPTING, 8-SHOT\n\nQ: “think, machine” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. Concatenating “k”, “e” leads to “ke”. So, “think, machine” outputs “ke”.\n\nQ: “learning, reasoning, generalization” A: The last letter of “learning” is “g”. The last letter of “reasoning” is “g”. The last letter of “generalization” is “n”. Concatenating “g”, “g”, “n” leads to “ggn”. So, “learning, reasoning, generalization” outputs “ggn”.\n\nQ: “artificial, intelligence” A: The last letter of “artificial” is “l”. The last letter of “intelligence” is “e”. Concatenating “l”, “e” leads to “le”. So, “artificial, intelligence” outputs “le”.\n\nQ: “transformer, language, vision” A: The last letter of “transformer” is “r”. The last letter of “language” is “e”. The last letter of “vision” is “n”. Concatenating “r”, “e”, “n” leads to “ren”. So, “transformer, language, vision” outputs “ren”.\n\nQ: “school, teacher” A: The last letter of “school” is “l”. The last letter of “teacher” is “r”. Concatenating “l”, “r” leads to “lr”. So, “school, teacher” outputs “lr”.\n\nQ: “art, science, department” A: The last letter of “art” is “t”. The last letter of “science” is “e”. The last letter of “department” is “t”. Concatenating “t”, “e”, “t” leads to “tet”. So, “art, science, department” outputs “tet”.\n\nQ: “pen, laptop” A: The last letter of “pen” is “n”. The last letter of “laptop” is “p”. Concatenating: “n”, “p” leads to “np”. So, “pen, laptop” outputs “np”.\n\nQ: “book, campus, university” A: The last letter of “book” is “k”. The last letter of “campus” is “s”. The last letter of “university” is “y”. Concatenating: “k”, “s”, “y” leads to “ksy”. So, “book, campus, university” outputs “ksy”.\n\n7.2.4 CHAIN-OF-THOUGHT PROMPTING, 2-SHOT, SAME EXAMPLES AS FOR LEAST-TO-MOST\n\nQ: “think, machine” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. Concatenating “k”, “e” leads to “ke”. So, “think, machine” outputs “ke”.\n\nQ: “think, machine, learning” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. The last letter of “learning” is “g”. Concatenating “k”, “e”, “g” leads to “keg”. So, “think, machine, learning” outputs “keg”.\n\n7.2.5 LEAST-TO-MOST PROMPTING, 4-SHOT\n\nQ: “think, machine” A: The last letter of “think” is “k”. The last letter of “machine” is “e”. Concatenating “k”, “e” leads to\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n“ke”. So, “think, machine” outputs “ke”.\n\nQ: “think, machine, learning” A: “think, machine” outputs “ke”. The last letter of “learning” is “g”. Concatenating “ke”, “g” leads to “keg”. So, “think, machine, learning” outputs “keg”.\n\nQ: “transformer, language” A: The last letter of “transformer” is “r”. The last letter of “language” is “e”. Concatenating: “r”, “e” leads to “re”. So, “transformer, language” outputs “re”.\n\nQ: “transformer, language, vision” A: “transformer, language” outputs “re”. The last letter of “vision” is “n”. Concatenating: “re”, “n” leads to “ren”. So, “transformer, language, vision” outputs “ren”.\n\n7.3 DATA GENERATION AND ADDITIONAL RESULTS\n\nData generation. The last-letter-concatenation dataset is based on a list of the 10k most common English words (including proper nouns) used in books that are part of project Gutenberg, as collected in Wiktionary2. After eliminating profane words, we ended up with a list of 9694 words (all lowercase). For each of the desired list sizes 2, 4, 6, 8, 10, 12, we then generated 500 examples, each of which consists of a random sequence of these words (input) and the corresponding sequence of last letters (output). We will release the full dataset upon publication of this paper. Below are 10 random examples of list size 6:\n\n• IN: “narrative, celebrate, neighbouring, indebted, stove, calling” OUT: “eegdeg”\n\n• IN: “barley, silk, thankful, kiss, logs, silent” OUT: “yklsst”\n\n• IN: “knitting, conveyance, receives, represent, cow, shut” OUT: “gestwt”\n\n• IN: “olive, dark, limitation, airy, pocket, wondered” OUT: “eknytd”\n\n• IN: “apprehensive, exclamation, perspiration, trusting, destiny, tactics” OUT: “enngys”\n\n• IN: “qualified, envoy, disciple, exert, witnesses, plane” OUT: “dyetse”\n\n• IN: “decidedly, dome, france, chris, knowing, peaceful” OUT: “yeesgl”\n\n• IN: “deceit, refinement, tips, cord, princes, discovery” OUT: “ttsdsy”\n\n• IN: “drops, paste, defective, bohemia, requested, convenient” OUT: “seeadt”\n\n• IN: “diverse, christopher, homely, agreeable, fright, suspended” OUT: “eryetd”\n\nComplete results. Table 13 summarizes all the experiments we performed for the last-letterconcatenation task. In addition to the experiments where prompt contexts contain 2 demonstration examples presented in Section 3.1, this includes experiments where the prompts contain 4 and 8 demonstration examples (see above).\n\nWhile more prompt examples have no effect for standard prompting (the accuracy remains at 0), they increase the accuracy across the board for chain-of-thought and least-to-most prompting. However, least-to-most prompting consistently outperforms chain-of-thought prompting. In fact, even if we compare 2-shot least-to-most (prompt size 123 GPT3 tokens) to 8-shot chain-of-thought (prompt size 573 GPT3 tokens), the accuracy for least-to-most prompting is much higher than for chainof-thought prompting. The difference is especially pronounced for long sequences (e.g., for L = 12, we have least-to-most at 74.0% vs. chain-of-thought at 38.4%). This shows that least-to-most prompting is much more data-efficient than chain-of-thought prompting for this problem.\n\nComparing the first two rows for chain-of-thought prompting shows that chain-of-thought achieves higher accuracy if we use two independent examples (see prompt in Table ??) instead of the two dependent examples that we use for least-to-most prompting. This demonstrates that the accuracy advantage of least-to-most prompting over chain-of-thought prompting remains even if the use the same examples for both of them.\n\n2https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/\n\n1-10000\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nPrompting method Standard\n\nChain-of-Thought\n\nLeast-to-Most\n\n# Examples Any 2\n2 (L2M) 4\n8 4\n4 2\n4 4\n4\n\nModel Any code-002 code-002 code-002 code-002 text-002* code-001 code-002 code-002 text-002* code-001\n\nL = 4 L = 6 L = 8 L = 10 L = 12\n\n0.0 89.4 84.2 88.6 91.0 87.0 13.0 94.0 96.0 94.0 19.6\n\n0.0 75.0 69.2 77.0 79.8 64.0 1.8 88.4 92.0 90.0 8.4\n\n0.0 51.8 50.2 53.4 56.8 46.0 0.0 83.0 84.6 84.0 4.0\n\n0.0 39.8 39.8 44.0 46.8 25.0 0.0 76.4 80.2 72.0 1.0\n\n0.0 33.6 31.8 37.4 38.4 14.0 0.0 74.0 76.6 66.0 0.1\n\nTable 13: Accuracy of different prompting methods, prompt sizes, and GPT3 models on the lastletter-concatenation task with the length of lists increasing from 4 to 12. We use code-002 to denote the model code-davinci-002, text-002 to denote the model text-davinci-002, and code-001 to denote the model code-davinci-001. The results in the second row for chain-of-thought prompting correspond to the experiment where we use for chain-of-thought the same prompt examples that we use for least-to-most. The results of text-davinci-002 are based on a subset of 100 random examples (rather than the full set of 500 exammples).\n\ntwo additional GPT-3 models: The table also contains the results from running against text-davinci-002 and codex-davinci-001. While text-davinci-002 shows similar accuracy to code-davinci-002 on small list sizes, the accuracy drops off much faster when moving to larger list sizes, both for chain-of-thought prompting as well as for least-to-most prompting. This indicates that the code-davinci-002 model has an advantage when it comes to dealing with iteration and recursion.\n\nThe code-davinci-001 model performs much worse than code-davinci-002 across all dimensions. Even for the shortest list size (L = 4), the accuracy for least-to-most prompting is only 19.6% compared to 96% for code-davinci-002. This indicates that there is a large potential for improvement when using the exact same configuration with new model generations.\n\n7.4 ERROR ANALYSIS: LEAST-TO-MOST PROMPTING\n\nError type\n\nConcatenation error - Dropping a letter - Adding a letter - Wrong order Wrong template Incorrect last letter Copy error\n\n2 examples\n\n4 examples\n\nL = 4 L = 12 L = 4 L = 12\n\n13 8\n4 1\n7 2\n0\n\n19 12 7\n0 1\n1 0\n\n21 15 4\n2 0\n1 1\n\n20 15 3\n2 0\n2 0\n\nLeast-to-most prompting error\n\nTable 14: the code-davinci-002 model on list lengths 4 and 12 for prompt contexts consisting of 2 and 4 examples. Note that for some examples, the model made more than one type of error (e.g., dropping and adding a letter during concatenation).\n\nanalysis of 20 random failures of\n\nFor least-to-most prompting, we analyzed 20 random failures of the code-davinci-002 model on list lengths 4 and 12 for prompt contexts consisting of 2 and 4 examples. The results are shown in Table 14. Concatenation errors may either be due to dropping a letter, adding a letter or outputting the letters in the wrong order. Wrong template means that the model used the extension template instead of the base template to concatenate the last letter of the first two words of the list. Incorrect last letter means that the model got the last letter of a word wrong, and copy error means that the error was due to making a mistake when copying an intermediate result.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nWe observe that for the prompt consisting of 2 examples, the fraction of concatenation errors increases as we go from length 4 to length 12 while the fraction of wrong template errors go down. This makes sense because the number of concatenations grows with the length of the list, while the number of times the model needs to use the base template stays constant. Note that the template errors disappear when we move to the double prompt, which means that adding two more examples helps the model recognize which template to use. As a consequence, the double prompt has a similar distribution of errors for both list lengths.\n\nExamples of concatenation errors. In the example “gratified, contract, fortitude, blew”, the model drops the last letter in the concatenation of “dte” and “w”, which means that it predicts the last letter sequence to be “dte” instead of “dtew”.\n\nIn the example “hollow, supplies, function, gorgeous”, the model duplicates the last letter “s” in the concatenation of “wsn” and “s”, which means that it predicts the last letter sequence “wsnss” instead of “wsns”.\n\nIn the example “madly, vengeance, cowardice, monk”, the model drops the last letter “k” in the concatenation of “yee” and ”k” and instead adds the letter “g”. Consequently, the model predicts “yeeg” instead of “yeek”.\n\nIn the example “slender, lash, throng, scheme”, the model breaks the order of the letters “h” and “g” in the concatenation of “rh” and “g”, which means that it predicts the last letter sequence “rghe” instead of “rhge”.\n\nExample of incorrect last letter. In the example “modification, introducing, schools, lunch”, the model determines the last letter of the word “modification” to be “g”. Consequently, the predicted last letter sequence is “ggsh” instead of “ngsh”.\n\nExample of wrong template application. In the example “upper, unexpectedly, specifically, connection”, the model uses the extension template to determine the output of the first two words “upper, unexpectedly”. I.e., it produces:\n\n• “upper” outputs “er”. The last letter of “unexpectedly” is “y”. Concatenating “er”, “y”\n\nleads to “ery”. So, “upper, unexpectedly” outputs “ery”.\n\nwhen it should have produced:\n\n• The last letter of “upper” is “r”. The last letter of “unexpectedly” is “y”. Concatenating\n\n“r”, “y” leads to “ry”.\n\nAs a consequence, the model predicts the sequence “eryyn” instead of “ryyn”.\n\nExample of copy error. In the example “displeased, hawk, healing, anchor”, the model correctly computes the final concatenation “dkgr” but then makes a mistake when copying the result to the final output and drops the final “r”, which leads to the prediction “dkg” instead of “dkgr”:\n\n• “displeased, hawk, healing” outputs “dkg”. The last letter of “anchor” is “r”. Concatenating: “dkg”, “r” leads to “dkgr”. So, “displeased, hawk, healing, anchor” outputs “dkg”.\n\n7.5 EXAMPLE OUTPUTS FROM CODE-DAVINCI-002\n\nIn this section, we present 5 examples (one example for lengths 4, 6, 8, 10, 12) of successes and failures of the code-davinci-002 model for standard prompting, chain-of-thought prompting, and least-to-most prompting. They are all based on the prompt contexts containing 4 demonstration examples.\n\n7.5.1 STANDARD PROMPTING: FAILURE\n\n“witnesses, arkansas, remaining, privileged”\n\nGolden: “ssgd”\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nSolution: Q: “witnesses, arkansas, remaining, privileged” A: “n”\n\n“clan, noun, roy, nicholas, relish, liking”\n\nGolden: “nnyshg”\n\nSolution: Q: “clan, noun, roy, nicholas, relish, liking” A: “nicholas”\n\n“franklin, temporarily, high, origin, rapidly, diversion, firmament, promptly”\n\nGolden: “nyhnynty”\n\nSolution: Q: “franklin, temporarily, high, origin, rapidly, diversion, firmament, promptly” A: “n”\n\n“conquered, owed, enclosure, indeed, shattered, beads, increased, provided, weak, grind”\n\nGolden: “ddeddsddkd”\n\nSolution: Q: “conquered, owed, enclosure, indeed, shattered, beads, increased, provided, weak, grind” A: “d”\n\n“hope, alison, mirrors, benefits, altered, curve, keen, portuguese, distance, interfaces, proof, seamus”\n\nGolden: “enssdeneesfs”\n\nSolution: Q: “hope, alison, mirrors, benefits, altered, curve, keen, portuguese, distance, interfaces, proof, seamus” A: “hope”\n\n7.5.2 CHAIN-OF-THOUGHT PROMPTING: SUCCESS\n\n“geese, rogers, benefactor, version”\n\nGolden: “esrn”\n\nSolution: Q: “geese, rogers, benefactor, version” A: The last letter of “geese” is “e”. The last letter of “rogers” is “s”. The last letter of “benefactor” is “r”. The last letter of “version” is “n”. Concatenating “e”, “s”, “r”, “n” leads to “esrn”. So, “geese, rogers,\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nbenefactor, version” outputs “esrn”.\n\n“amply, bestow, maryland, edited, blowing, prosperous”\n\nGolden: “ywddgs”\n\nSolution: Q: “amply, bestow, maryland, edited, blowing, prosperous” A: The last letter of “amply” is “y”. The last letter of “bestow” is “w”. The last letter of “maryland” is “d”. The last letter of “edited” is “d”. The last letter of “blowing” is “g”. The last letter of “prosperous” is “s”. Concatenating “y”, “w”, “d”, “d”, “g”, “s” leads to “ywddgs”. So, “amply, bestow, maryland, edited, blowing, prosperous” outputs “ywddgs”.\n\n“needless, courts, reflected, curly, glowing, asp, chronicles, anthony”\n\nGolden: “ssdygpsy”\n\nSolution: Q: “needless, courts, reflected, curly, glowing, asp, chronicles, anthony” A: The last letter of “needless” is “s”. The last letter of “courts” is “s”. The last letter of “reflected” is “d”. The last letter of “curly” is “y”. The last letter of “glowing” is “g”. The last letter of “asp” is “p”. The last letter of “chronicles” is “s”. The last letter of “anthony” is “y”. Concatenating “s”, “s”, “d”, “y”, “g”, “p”, “s”, “y” leads to “ssdygpsy”. So, “needless, courts, reflected, curly, glowing, asp, chronicles, anthony” outputs “ssdygpsy”.\n\n“nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour”\n\nGolden: “latlrlnedr”\n\nSolution: Q: “nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour” A: The last letter of “nigel” is “l”. The last letter of “alaska” is “a”. The last letter of “rent” is “t”. The last letter of “hospital” is “l”. The last letter of “newspaper” is “r”. The last letter of “medical” is “l”. The last letter of “nation” is “n”. The last letter of “dave” is “e”. The last letter of “associated” is “d”. The last letter of “rumour” is “r”. Concatenating “l”, “a”, “t”, “l”, “r”, “l”, “n”, “e”, “d”, “r” leads to “latlrlnedr”. So, “nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour” outputs “latlrlnedr”.\n\n“brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert”\n\nGolden: “syedysspnytt”\n\nSolution: Q: “brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert” A: The last letter of “brightness” is “s”. The last letter of “preparatory” is “y”. The last letter of “intense” is “e”. The last letter of “accumulated” is “d”. The last letter of “reluctantly” is “y”. The last letter of “nets” is “s”. The last letter of “inquiries” is “s”. The last letter of “grip” is “p”. The last letter of “inquisition” is “n”. The last letter of “alley” is “y”. The last letter of “innocent” is “t”. The last letter of\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n“herbert” is “t”. Concatenating “s”, “y”, “e”, “d”, “y”, “s”, “s”, “p”, “n”, “y”, “t”, “t” leads to “syedysspnytt”. So, “brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert” outputs “syedysspnytt”.\n\n7.5.3 CHAIN-OF-THOUGHT PROMPTING: FAILURE\n\n“pattern, vote, afterwards, solemn”\n\nGolden: “nesn”\n\nSolution: Q: “pattern, vote, afterwards, solemn” A: The last letter of “pattern” is “n”. The last letter of “vote” is “e”. The last letter of “afterwards” is “s”. The last letter of “solemn” is “n”. Concatenating “n”, “e”, “s”, “n” leads to “ness”. So, “pattern, vote, afterwards, solemn” outputs “ness”.\n\n“disappointment, troublesome, respectfully, monstrous, worry, politics”\n\nGolden: “teysys”\n\nSolution: Q: “disappointment, troublesome, respectfully, monstrous, worry, politics” A: The last letter of “disappointment” is “t”. The last letter of “troublesome” is “e”. The last letter of “respectfully” is “y”. The last letter of “monstrous” is “s”. The last letter of “worry” is “y”. The last letter of “politics” is “s”. Concatenating “t”, “e”, “y”, “s”, “y”, “s” leads to “teyssy”. So, “disappointment, troublesome, respectfully, monstrous, worry, politics” outputs “teyssy”.\n\n“drawing, changes, purified, basic, others, send, chimney, vice”\n\nGolden: “gsdcsdye”\n\nSolution: Q: “drawing, changes, purified, basic, others, send, chimney, vice” A: The last letter of “drawing” is “g”. The last letter of “changes” is “s”. The last letter of “purified” is “d”. The last letter of “basic” is “c”. The last letter of “others” is “s”. The last letter of “send” is “d”. The last letter of “chimney” is “y”. The last letter of “vice” is “e”. Concatenating “g”, “s”, “d”, “c”, “s”, “d”, “y”, “e” leads to “gsdcsyde”. So, “drawing, changes, purified, basic, others, send, chimney, vice” outputs “gsdcsyde”.\n\n“sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch”\n\nGolden: “eyhtnsylgh”\n\nSolution: Q: “sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch” A: The last letter of “sensitive” is “e”. The last letter of “pray” is “y”. The last letter of “wreath” is “h”. The last letter of “betwixt” is “t”. The last letter of “golden” is “n”. The last letter of “regions” is “s”. The last letter of “loyalty” is “y”. The last letter of “interval” is “l”. The last letter of “hunting” is “g”. The last letter of “ranch” is “h”. Concatenating “e”, “y”, “h”, “t”, “n”, “s”, “y”, “l”, “g”, “h” leads to “eyhtsynslgh”. So, “sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch” outputs “eyhtsynslgh”.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n“freezing, arduous, pushing, vitality, hue, generosity, white, motto, matthew, preferred, hounds, breast”\n\nGolden: “gsgyeyeowdst”\n\nSolution: Q: “freezing, arduous, pushing, vitality, hue, generosity, white, motto, matthew, preferred, hounds, breast” A: The last letter of “freezing” is “g”. The last letter of “arduous” is “s”. The last letter of “pushing” is “g”. The last letter of “vitality” is “y”. The last letter of “hue” is “e”. The last letter of “generosity” is “y”. The last letter of “white” is “e”. The last letter of “motto” is “o”. The last letter of “matthew” is “w”. The last letter of “preferred” is “d”. The last letter of “hounds” is “s”. The last letter of “breast” is “t”. Concatenating “g”, “s”, “g”, “y”, “e”, “y”, “e”, “o”, “w”, “d”, “s”, “t” leads to “gsgyeyowdst”. So, “freezing, arduous, pushing, vitality, hue, generosity, white, motto, matthew, preferred, hounds, breast” outputs “gsgyeyowdst”.\n\n7.5.4 LEAST-TO-MOST PROMPTING: SUCCESS\n\n“worm, jackson, widow, car”\n\nGolden: “mnwr”\n\nSolution: Q: “worm, jackson” A: The last letter of “worm” is “m”. The last letter of “jackson” is “n”. Concatenating: “m”, “n” leads to “mn”. So, “worm, jackson” outputs “mn”.\n\nQ: “worm, jackson, widow” A: “worm, jackson” outputs “mn”. The last letter of “widow” is “w”. Concatenating: “mn”, “w” leads to “mnw”. So, “worm, jackson, widow” outputs “mnw”.\n\nQ: “worm, jackson, widow, car” A: “worm, jackson, widow” outputs “mnw”. The last letter of “car” is “r”. Concatenating: “mnw”, “r” leads to “mnwr”. So, “worm, jackson, widow, car” outputs “mnwr”.\n\n“twinkle, blushed, pale, fast, irony, tearing”\n\nGolden: “edetyg”\n\nSolution: Q: “twinkle, blushed” A: The last letter of “twinkle” is “e”. The last letter of “blushed” is “d”. Concatenating: “e”, “d” leads to “ed”. So, “twinkle, blushed” outputs “ed”.\n\nQ: “twinkle, blushed, pale” A: “twinkle, blushed” outputs “ed”. The last letter of “pale” is “e”. Concatenating: “ed”, “e” leads to “ede”. So, “twinkle, blushed, pale” outputs “ede”.\n\nQ: “twinkle, blushed, pale, fast” A: “twinkle, blushed, pale” outputs “ede”. The last letter of “fast” is “t”. Concatenating: “ede”, “t” leads to “edet”. So, “twinkle, blushed, pale, fast” outputs “edet”.\n\nQ: “twinkle, blushed, pale, fast, irony”\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nA: “twinkle, blushed, pale, fast” outputs “edet”. The last letter of “irony” is “y”. Concatenating: “edet”, “y” leads to “edety”. So, “twinkle, blushed, pale, fast, irony” outputs “edety”.\n\nQ: “twinkle, blushed, pale, fast, irony, tearing” A: “twinkle, blushed, pale, fast, irony” outputs “edety”. The last letter of “tearing” is “g”. Concatenating: “edety”, “g” leads to “edetyg”. So, “twinkle, blushed, pale, fast, irony, tearing” outputs “edetyg”.\n\n“colonists, empty, fowl, intend, diligent, reluctantly, seed, gentry”\n\nGolden: “syldtydy”\n\nSolution: Q: “colonists, empty” A: The last letter of “colonists” is “s”. The last letter of “empty” is “y”. Concatenating: “s”, “y” leads to “sy”. So, “colonists, empty” outputs “sy”.\n\nQ: “colonists, empty, fowl” A: “colonists, empty” outputs “sy”. The last letter of “fowl” is “l”. Concatenating: “sy”, “l” leads to “syl”. So, “colonists, empty, fowl” outputs “syl”.\n\nQ: “colonists, empty, fowl, intend” A: “colonists, empty, fowl” outputs “syl”. The last letter of “intend” is “d”. Concatenating: “syl”, “d” leads to “syld”. So, “colonists, empty, fowl, intend” outputs “syld”.\n\nQ: “colonists, empty, fowl, intend, diligent” A: “colonists, empty, fowl, intend” outputs “syld”. The last letter of “diligent” is “t”. Concatenating: “syld”, “t” leads to “syldt”. So, “colonists, empty, fowl, intend, diligent” outputs “syldt”.\n\nQ: “colonists, empty, fowl, intend, diligent, reluctantly” A: “colonists, empty, fowl, intend, diligent” outputs “syldt”. The last letter of “reluctantly” is “y”. Concatenating: “syldt”, “y” leads to “syldty”. So, “colonists, empty, fowl, intend, diligent, reluctantly” outputs “syldty”.\n\nQ: “colonists, empty, fowl, intend, diligent, reluctantly, seed” A: “colonists, empty, fowl, intend, diligent, reluctantly” outputs “syldty”. The last letter of “seed” is “d”. Concatenating: “syldty”, “d” leads to “syldtyd”. So, “colonists, empty, fowl, intend, diligent, reluctantly, seed” outputs “syldtyd”.\n\nQ: “colonists, empty, fowl, intend, diligent, reluctantly, seed, gentry” A: “colonists, empty, fowl, intend, diligent, reluctantly, seed” outputs “syldtyd”. The last letter of “gentry” is “y”. Concatenating: “syldtyd”, “y” leads to “syldtydy”. So, “colonists, empty, fowl, intend, diligent, reluctantly, seed, gentry” outputs “syldtydy”.\n\n“leo, observations, power, thou, dart, emergency, notorious, indulging, consume, provoke”\n\nGolden: “osrutysgee”\n\nSolution: Q: “leo, observations” A: The last letter of “leo” is “o”. The last letter of “observations” is “s”. Concatenating: “o”, “s” leads to “os”. So, “leo, observations” outputs “os”.\n\nQ: “leo, observations, power” A: “leo, observations” outputs “os”. The last letter of “power” is “r”. Concatenating: “os”, “r” leads to “osr”. So, “leo, observations, power” outputs “osr”.\n\nQ: “leo, observations, power, thou” A: “leo, observations, power” outputs “osr”. The last letter of “thou” is “u”. Concatenating: “osr”, “u”\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nleads to “osru”. So, “leo, observations, power, thou” outputs “osru”.\n\nQ: “leo, observations, power, thou, dart” A: “leo, observations, power, thou” outputs “osru”. The last letter of “dart” is “t”. Concatenating: “osru”, “t” leads to “osrut”. So, “leo, observations, power, thou, dart” outputs “osrut”.\n\nQ: “leo, observations, power, thou, dart, emergency” A: “leo, observations, power, thou, dart” outputs “osrut”. The last letter of “emergency” is “y”. Concatenating: “osrut”, “y” leads to “osruty”. So, “leo, observations, power, thou, dart, emergency” outputs “osruty”.\n\nQ: “leo, observations, power, thou, dart, emergency, notorious” A: “leo, observations, power, thou, dart, emergency” outputs “osruty”. The last letter of “notorious” is “s”. Concatenating: “osruty”, “s” leads to “osrutys”. So, “leo, observations, power, thou, dart, emergency, notorious” outputs “osrutys”.\n\nQ: “leo, observations, power, thou, dart, emergency, notorious, indulging” A: “leo, observations, power, thou, dart, emergency, notorious” outputs “osrutys”. The last letter of “indulging” is “g”. Concatenating: “osrutys”, “g” leads to “osrutysg”. So, “leo, observations, power, thou, dart, emergency, notorious, indulging” outputs “osrutysg”.\n\nQ: “leo, observations, power, thou, dart, emergency, notorious, indulging, consume” A: “leo, observations, power, thou, dart, emergency, notorious, indulging” outputs “osrutysg”. The last letter of “consume” is “e”. Concatenating: “osrutysg”, “e” leads to “osrutysge”. So, “leo, observations, power, thou, dart, emergency, notorious, indulging, consume” outputs “osrutysge”.\n\nQ: “leo, observations, power, thou, dart, emergency, notorious, indulging, consume, provoke” A: “leo, observations, power, thou, dart, emergency, notorious, indulging, consume” outputs “osrutysge”. The last letter of “provoke” is “e”. Concatenating: “osrutysge”, “e” leads to “osrutysgee”. So, “leo, observations, power, thou, dart, emergency, notorious, indulging, consume, provoke” outputs “osrutysgee”.\n\n“niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate, wet, howard”\n\nGolden: “eygnetncoetd”\n\nSolution: Q: “niece, greedy” A: The last letter of “niece” is “e”. The last letter of “greedy” is “y”. Concatenating: “e”, “y” leads to “ey”. So, “niece, greedy” outputs “ey”.\n\nQ: “niece, greedy, anything” A: “niece, greedy” outputs “ey”. The last letter of “anything” is “g”. Concatenating: “ey”, “g” leads to “eyg”. So, “niece, greedy, anything” outputs “eyg”.\n\nQ: “niece, greedy, anything, retain” A: “niece, greedy, anything” outputs “eyg”. The last letter of “retain” is “n”. Concatenating: “eyg”, “n” leads to “eygn”. So, “niece, greedy, anything, retain” outputs “eygn”.\n\nQ: “niece, greedy, anything, retain, moisture” A: “niece, greedy, anything, retain” outputs “eygn”. The last letter of “moisture” is “e”. Concatenating: “eygn”, “e” leads to “eygne”. So, “niece, greedy, anything, retain, moisture” outputs “eygne”.\n\nQ: “niece, greedy, anything, retain, moisture, jest” A: “niece, greedy, anything, retain, moisture” outputs “eygne”. The last letter of “jest” is “t”. Concatenating: “eygne”, “t” leads to “eygnet”. So, “niece, greedy, anything, retain, moisture, jest” outputs “eygnet”.\n\nQ: “niece, greedy, anything, retain, moisture, jest, adrian”\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nA: “niece, greedy, anything, retain, moisture, jest” outputs “eygnet”. The last letter of “adrian” is “n”. Concatenating: “eygnet”, “n” leads to “eygnetn”. So, “niece, greedy, anything, retain, moisture, jest, adrian” outputs “eygnetn”.\n\nQ: “niece, greedy, anything, retain, moisture, jest, adrian, attic” A: “niece, greedy, anything, retain, moisture, jest, adrian” outputs “eygnetn”. The last letter of “attic” is “c”. Concatenating: “eygnetn”, “c” leads to “eygnetnc”. So, “niece, greedy, anything, retain, moisture, jest, adrian, attic” outputs “eygnetnc”.\n\nQ: “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo” A: “niece, greedy, anything, retain, moisture, jest, adrian, attic” outputs “eygnetnc”. The last letter of “hugo” is “o”. Concatenating: “eygnetnc”, “o” leads to “eygnetnco”. So, “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo” outputs “eygnetnco”.\n\nQ: “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate” A: “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo” outputs “eygnetnco”. The last letter of “accurate” is “e”. Concatenating: “eygnetnco”, “e” leads to “eygnetncoe”. So, “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate” outputs “eygnetncoe”.\n\nQ: “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate, wet” A: “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate” outputs “eygnetncoe”. The last letter of “wet” is “t”. Concatenating: “eygnetncoe”, “t” leads to “eygnetncoet”. So, “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate, wet” outputs “eygnetncoet”.\n\nQ: “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate, wet, howard” A: “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate, wet” outputs “eygnetncoet”. The last letter of “howard” is “d”. Concatenating: “eygnetncoet”, “d” leads to “eygnetncoetd”. So, “niece, greedy, anything, retain, moisture, jest, adrian, attic, hugo, accurate, wet, howard” outputs “eygnetncoetd”.\n\n7.5.5 LEAST-TO-MOST PROMPTING: FAILURE\n\n“supper, procession, region, ruby”\n\nGolden: “rnny”\n\nSolution: Q: “supper, procession” A: The last letter of “supper” is “r”. The last letter of “procession” is “n”. Concatenating: “r”, “n” leads to “rn”. So, “supper, procession” outputs “rn”.\n\nQ: “supper, procession, region” A: “supper, procession” outputs “rn”. The last letter of “region” is “n”. Concatenating: “rn”, “n” leads to “rnn”. So, “supper, procession, region” outputs “rnn”.\n\nQ: “supper, procession, region, ruby” A: “supper, procession, region” outputs “rnn”. The last letter of “ruby” is “y”. Concatenating: “rnn”, “y” leads to “rnnn”. So, “supper, procession, region, ruby” outputs “rnnn”.\n\n“release, appeared, endure, field, render, monk”\n\nGolden: “ededrk”\n\nSolution: Q: “release, appeared” A: The last letter of “release” is “e”. The last letter of “appeared” is “d”. Concatenating: “e”, “d” leads\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nto “ed”. So, “release, appeared” outputs “ed”.\n\nQ: “release, appeared, endure” A: “release, appeared” outputs “ed”. The last letter of “endure” is “e”. Concatenating: “ed”, “e” leads to “ede”. So, “release, appeared, endure” outputs “ede”.\n\nQ: “release, appeared, endure, field” A: “release, appeared, endure” outputs “ede”. The last letter of “field” is “d”. Concatenating: “ede”, “d” leads to “eded”. So, “release, appeared, endure, field” outputs “eded”.\n\nQ: “release, appeared, endure, field, render” A: “release, appeared, endure, field” outputs “eded”. The last letter of “render” is “r”. Concatenating: “eded”, “r” leads to “ededer”. So, “release, appeared, endure, field, render” outputs “ededer”.\n\nQ: “release, appeared, endure, field, render, monk” A: “release, appeared, endure, field, render” outputs “ededer”. The last letter of “monk” is “k”. Concatenating: “ededer”, “k” leads to “edederk”. So, “release, appeared, endure, field, render, monk” outputs “edederk”.\n\n“mentally, uncomfortable, amount, straight, whatsoever, deference, channel, integrity”\n\nGolden: “yettrely”\n\nSolution: Q: “mentally, uncomfortable” A: The last letter of “mentally” is “y”. The last letter of “uncomfortable” is “e”. Concatenating: “y”, “e” leads to “ye”. So, “mentally, uncomfortable” outputs “ye”.\n\nQ: “mentally, uncomfortable, amount” A: “mentally, uncomfortable” outputs “ye”. The last letter of “amount” is “t”. Concatenating: “ye”, “t” leads to “yet”. So, “mentally, uncomfortable, amount” outputs “yet”.\n\nQ: “mentally, uncomfortable, amount, straight” A: “mentally, uncomfortable, amount” outputs “yet”. The last letter of “straight” is “t”. Concatenating: “yet”, “t” leads to “yett”. So, “mentally, uncomfortable, amount, straight” outputs “yett”.\n\nQ: “mentally, uncomfortable, amount, straight, whatsoever” A: “mentally, uncomfortable, amount, straight” outputs “yett”. The last letter of “whatsoever” is “r”. Concatenating: “yett”, “r” leads to “yettar”. So, “mentally, uncomfortable, amount, straight, whatsoever” outputs “yettar”.\n\nQ: “mentally, uncomfortable, amount, straight, whatsoever, deference” A: “mentally, uncomfortable, amount, straight, whatsoever” outputs “yettar”. The last letter of “deference” is “e”. Concatenating: “yettar”, “e” leads to “yettare”. So, “mentally, uncomfortable, amount, straight, whatsoever, deference” outputs “yettare”.\n\nQ: “mentally, uncomfortable, amount, straight, whatsoever, deference, channel” A: “mentally, uncomfortable, amount, straight, whatsoever, deference” outputs “yettare”. The last letter of “channel” is “l”. Concatenating: “yettare”, “l” leads to “yettarel”. So, “mentally, uncomfortable, amount, straight, whatsoever, deference, channel” outputs “yettarel”.\n\nQ: “mentally, uncomfortable, amount, straight, whatsoever, deference, channel, integrity” A: “mentally, uncomfortable, amount, straight, whatsoever, deference, channel” outputs “yettarel”. The last letter of “integrity” is “y”. Concatenating: “yettarel”, “y” leads to “yettarely”. So, “mentally, uncomfortable, amount, straight, whatsoever, deference, channel, integrity” outputs “yettarely”.\n\n“complexion, criminal, assume, seven, driver, hero, horse, purpose, zeus, joint”\n\nGolden:\n\n26\n\nPublished as a conference paper at ICLR 2023\n\n“nlenroeest”\n\nSolution: Q: “complexion, criminal” A: The last letter of “complexion” is “n”. The last letter of “criminal” is “l”. Concatenating: “n”, “l” leads to “nl”. So, “complexion, criminal” outputs “nl”.\n\nQ: “complexion, criminal, assume” A: “complexion, criminal” outputs “nl”. The last letter of “assume” is “e”. Concatenating: “nl”, “e” leads to “nle”. So, “complexion, criminal, assume” outputs “nle”.\n\nQ: “complexion, criminal, assume, seven” A: “complexion, criminal, assume” outputs “nle”. The last letter of “seven” is “n”. Concatenating: “nle”, “n” leads to “nlen”. So, “complexion, criminal, assume, seven” outputs “nlen”.\n\nQ: “complexion, criminal, assume, seven, driver” A: “complexion, criminal, assume, seven” outputs “nlen”. The last letter of “driver” is “r”. Concatenating: “nlen”, “r” leads to “nlenr”. So, “complexion, criminal, assume, seven, driver” outputs “nlenr”.\n\nQ: “complexion, criminal, assume, seven, driver, hero” A: “complexion, criminal, assume, seven, driver” outputs “nlenr”. The last letter of “hero” is “o”. Concatenating: “nlenr”, “o” leads to “nlenro”. So, “complexion, criminal, assume, seven, driver, hero” outputs “nlenro”.\n\nQ: “complexion, criminal, assume, seven, driver, hero, horse” A: “complexion, criminal, assume, seven, driver, hero” outputs “nlenro”. The last letter of “horse” is “e”. Concatenating: “nlenro”, “e” leads to “nlenroe”. So, “complexion, criminal, assume, seven, driver, hero, horse” outputs “nlenroe”.\n\nQ: “complexion, criminal, assume, seven, driver, hero, horse, purpose” A: “complexion, criminal, assume, seven, driver, hero, horse” outputs “nlenroe”. The last letter of “purpose” is “e”. Concatenating: “nlenroe”, “e” leads to “nlenroee”. So, “complexion, criminal, assume, seven, driver, hero, horse, purpose” outputs “nlenroee”.\n\nQ: “complexion, criminal, assume, seven, driver, hero, horse, purpose, zeus” A: “complexion, criminal, assume, seven, driver, hero, horse, purpose” outputs “nlenroee”. The last letter of “zeus” is “s”. Concatenating: “nlenroee”, “s” leads to “nlenroees”. So, “complexion, criminal, assume, seven, driver, hero, horse, purpose, zeus” outputs “nlenroees”.\n\nQ: “complexion, criminal, assume, seven, driver, hero, horse, purpose, zeus, joint” A: “complexion, criminal, assume, seven, driver, hero, horse, purpose, zeus” outputs “nlenroees”. The last letter of “joint” is “t”. Concatenating: “nlenroees”, “t” leads to “nlenroeast”. So, “complexion, criminal, assume, seven, driver, hero, horse, purpose, zeus, joint” outputs “nlenroeast”.\n\n“provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies, dost, treasury”\n\nGolden: “darlneyddsty”\n\nSolution: Q: “provoked, columbia” A: The last letter of “provoked” is “d”. The last letter of “columbia” is “a”. Concatenating: “d”, “a” leads to “da”. So, “provoked, columbia” outputs “da”.\n\nQ: “provoked, columbia, sir” A: “provoked, columbia” outputs “da”. The last letter of “sir” is “r”. Concatenating: “da”, “r” leads to “dar”. So, “provoked, columbia, sir” outputs “dar”.\n\nQ: “provoked, columbia, sir, principal” A: “provoked, columbia, sir” outputs “dar”. The last letter of “principal” is “l”. Concatenating: “dar”, “l”\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nleads to “darl”. So, “provoked, columbia, sir, principal” outputs “darl”.\n\nQ: “provoked, columbia, sir, principal, seven” A: “provoked, columbia, sir, principal” outputs “darl”. The last letter of “seven” is “n”. Concatenating: “darl”, “n” leads to “darln”. So, “provoked, columbia, sir, principal, seven” outputs “darln”.\n\nQ: “provoked, columbia, sir, principal, seven, extreme” A: “provoked, columbia, sir, principal, seven” outputs “darln”. The last letter of “extreme” is “e”. Concatenating: “darln”, “e” leads to “darle”. So, “provoked, columbia, sir, principal, seven, extreme” outputs “darle”.\n\nQ: “provoked, columbia, sir, principal, seven, extreme, timothy” A: “provoked, columbia, sir, principal, seven, extreme” outputs “darle”. The last letter of “timothy” is “y”. Concatenating: “darle”, “y” leads to “darley”. So, “provoked, columbia, sir, principal, seven, extreme, timothy” outputs “darley”.\n\nQ: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted” A: “provoked, columbia, sir, principal, seven, extreme, timothy” outputs “darley”. The last letter of “submitted” is “d”. Concatenating: “darley”, “d” leads to “darleyd”. So, “provoked, columbia, sir, principal, seven, extreme, timothy, submitted” outputs “darleyd”.\n\nQ: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered” A: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted” outputs “darleyd”. The last letter of “considered” is “d”. Concatenating: “darleyd”, “d” leads to “darleydd”. So, “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered” outputs “darleydd”.\n\nQ: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies” timothy, submitted, considered” outputs A: “provoked, columbia, sir, principal, seven, extreme, “darleydd”. The last letter of “spies” is “s”. Concatenating: “darleydd”, “s” leads to “darleydds”. So, “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies” outputs “darleydds”.\n\nQ: “provoked, columbia, sir, principal, seven, extreme, dost” A: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies” outputs “darleydds”. The last letter of “dost” is “t”. Concatenating: “darleydds”, “t” leads to “darleyddst”. So, “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies, dost” outputs “darleyddst”.\n\ntimothy, submitted, considered, spies,\n\nQ: “provoked, columbia, sir, principal, seven, extreme, dost, treasury” A: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies, dost” outputs “darleyddst”. The last letter of “treasury” is “y”. Concatenating: “darleyddst”, “y” leads to “darleyddsty”. So, “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies, dost, treasury” outputs “darleyddsty”.\n\ntimothy, submitted, considered, spies,\n\n8 SCAN\n\n8.1 PROMPT CONTEXTS\n\nIn this section we present the prompt contexts used for the SCAN benchmark in Section 3.2. It includes one context for each of standard prompting, least-to-most prompting, and chain-of-thought prompting.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\n8.1.1 STANDARD PROMPTING\n\nThe context for standard prompting consist of a set of commands together with the corresponding action sequences.\n\nQ: “turn left” A: “TURN LEFT”\n\nQ: “turn right” A: “TURN RIGHT”\n\nQ: “jump left” A: “TURN LEFT” + “JUMP”\n\nQ: “run right” A: “TURN RIGHT” + “RUN”\n\nQ: “look twice” A: “LOOK” * 2\n\nQ: “run and look twice” A: “RUN” + “LOOK” * 2\n\nQ: “jump right thrice” A: (“TURN RIGHT” + “JUMP”) * 3\n\nQ: “walk after run” A: “RUN” + “WALK”\n\nQ: “turn opposite left” A: “TURN LEFT” * 2\n\nQ: “turn around left” A: “TURN LEFT” * 4\n\nQ: “turn opposite right” A: “TURN RIGHT” * 2\n\nQ: “turn around right” A: “TURN RIGHT” * 4\n\nQ: “walk opposite left” A: “TURN LEFT” * 2 + “WALK”\n\nQ: “walk around left” A: (“TURN LEFT” + “WALK”) * 4\n\n8.1.2 LEAST-TO-MOST PROMPTING\n\nFor least-to-most prompting, we need 2 different prompt contexts: one to decompose commands into a sequence of components and one to map commands to actions.\n\nCommand mapping. The command mapping prompt context consists of a set of commands together with the corresponding action sequences and explanations of how they were obtained.\n\nQ: “turn left” A: “turn left” outputs “TURN LEFT”.\n\nQ: “turn right” A: “turn right” outputs “TURN RIGHT”.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nQ: “jump left” A: The output of “jump left” concatenates: the output of “turn left”, the output of “jump”. “turn left” outputs “TURN LEFT”. “jump” outputs “JUMP”. So concatenating the output of “turn left” and the output of “jump” leads to “TURN LEFT” + “JUMP”. So the output of “jump left” is “TURN LEFT” + “JUMP”.\n\nQ: “run right” A: The output of “run right” concatenates: the output of “turn right”, the output of “run”. “turn right” outputs “TURN RIGHT”. “run” outputs “RUN”. So concatenating the output of “turn right” and the output of “run” leads to “TURN RIGHT” + “RUN”. So the output of “run right” is “TURN RIGHT” + “RUN”.\n\nQ: “look twice” A: The output of “look twice” concatenates: the output of “look”, the output of “look”. “look” outputs “LOOK”. So repeating the output of “look” two times leads to “LOOK” * 2. So the output of “look twice” is “LOOK” * 2.\n\nQ: “run and look twice” A: The output of “run and look twice” concatenates: the output of “run”, the output of “look twice”. “run” outputs “RUN”. “look twice” outputs “LOOK” * 2. So concatenating the output of “run” and the output of “look twice” leads to “RUN” + “LOOK” * 2. So the output of “run and look twice” is “RUN” + “LOOK” * 2.\n\nQ: “jump right thrice” A: The output of “jump right thrice” concatenates: the output of “jump right”, the output of “jump right”, the output of “jump right”. “jump right” outputs “TURN RIGHT” + “JUMP”. So repeating the output of “jump right” three times leads to (“TURN RIGHT” + “JUMP”) * 3. So the output of “jump right thrice” is (“TURN RIGHT” + “JUMP”) * 3.\n\nQ: “walk after run” A: The output of “walk after run” concatenates: the output of “run”, the output of “walk”. “run” outputs “RUN”. “walk” outputs “WALK”. So concatenating the output of “run” and the output of “walk” leads to “RUN” + “WALK”. So the output of “walk after run” is “RUN” + “WALK”.\n\nQ: “turn opposite left” A: The output of “turn opposite left” concatenates: the output of “turn left”, the output of “turn left”. “turn left” outputs “TURN LEFT”. So repeating the output of “turn left” twice leads to “TURN LEFT” * 2. So the output of “turn opposite left” is “TURN LEFT” * 2.\n\nQ: “turn around left” A: The output of “turn around left” concatenates: the output of “turn left”, the output of “turn left”, the output of “turn left”, the output of “turn left”. “turn left” outputs “TURN LEFT”. So repeating the output of “turn left” four times leads to “TURN LEFT” * 4. So the output of “turn around left” is “TURN LEFT” * 4.\n\nQ: “turn opposite right” A: The output of “turn opposite right” concatenates: right”. “TURN RIGHT” * 2. So the output of “turn opposite right” is “TURN RIGHT” * 2.\n\nthe output of “turn right”, the output of “turn “turn right” outputs “TURN RIGHT”. So repeating the output of “turn right” twice leads to\n\nQ: “turn around right” A: The output of “turn around right” concatenates: the output of “turn right”, the output of “turn right”, the output of “turn right”, the output of “turn right”. “turn right” outputs “TURN RIGHT”. So repeating the output of “turn right” four times leads to “TURN RIGHT” * 4. So the output of “turn around right” is “TURN RIGHT” * 4.\n\nQ: “walk opposite left” A: The output of “walk opposite left” concatenates: the output of “turn opposite left”, the output of “walk”. “turn opposite left” outputs “TURN LEFT” * 2. “walk” outputs “WALK”. So concatenating the output of “turn opposite left” and the output of “walk” leads to “TURN LEFT” * 2 + “WALK”. So the\n\n30\n\nPublished as a conference paper at ICLR 2023\n\noutput of “walk opposite left” is “TURN LEFT” * 2 + “WALK”.\n\nQ: “walk around left” A: The output of “walk around left” concatenates: the output of “walk left”, the output of “walk left”, “walk left” outputs “TURN LEFT” + “WALK”. So the output of “walk left”, the output of “walk left”. repeating the output of “walk around left” four times leads to (“TURN LEFT” + “WALK”) * 4. So the output of “walk around left” is (“TURN LEFT” + “WALK”) * 4.\n\nCommand decomposition. The command decomposition prompt context consists of a set of commands together with the corresponding sequence of decomposed commands.\n\nQ: “look right after look twice” A: “look right after look twice” can be solved by: “look right”, “look twice”.\n\nQ: “jump opposite right thrice and walk” A: “jump opposite right thrice” can be solved by: “jump opposite right”, “jump opposite right thrice”. “walk” can be solved by: “walk”. So, “jump opposite right thrice and walk” can be solved by: “jump opposite right”, “jump opposite right thrice”, “walk”.\n\nQ: “run left twice and run right” A: “run left twice” can be solved by: “run left”, “run left twice”. “run right” can be solved by “run right”. So, “run left twice and run right” can.be solved by: “run left”, “run left twice”, “run right”.\n\nQ: “run opposite right” A: “run opposite right” can be solved by “run opposite right”.\n\nQ: “look opposite right thrice after walk” A: “look opposite right thrice” can be solved by: “look opposite right”, “look opposite right thrice”. “walk” can be solved by “walk”. So, “look opposite right thrice after walk” can be solved by: “look opposite right”, “look opposite right thrice”, “walk”.\n\nQ: “jump around right” A: “jump around right” can be solved by: “jump right”, “jump around right”. So, “jump around right” can be solved by: “jump right”, “jump around right”.\n\nQ: “look around right thrice and walk” A: “look around right thrice” can be solved by: “look right”, “look around right”, “look around right thrice”. “walk” can be solved by “walk”. So, “look around right thrice and walk” can be solved by: “look right”, “look around right”, “look around right thrice”, “walk”.\n\nQ: “turn right after run right thrice” “run right thrice” can be solved by: “run right”, “run A: “turn right” can be solved by: “turn right”. right thrice”. So, “turn right after run right thrice” can be solved by: “turn right”, “run right”, “run right thrice”.\n\n8.1.3 CHAIN-OF-THOUGHT PROMPTING\n\nWe reuse the command mapping prompt context from least-to-most prompting shown above.\n\n8.2 ERROR ANALYSIS: LEAST-TO-MOST PROMPTING\n\n20\n\nanalyzed\n\nleast-to-most\n\nprompting, we\n\nFor the models code-davinci-001 and text-davinci-002, and we analyzed all 13 errors for the model code-davinci-002. The results are shown in Table 15. Errors may either occur during command decomposition or during command translation. The translation errors are further split into the following types. Incorrect interpretation of “twice” and “thrice” means that the model made an error when applying “twice” and “thrice” to an expression. “After” interpreted as “and” means that the model translated an expression containing “after” as if it instead contained “and”. Copy error means that the model made a mistake when copying an intermediate result.\n\nrandom failures\n\nfor\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nError type Decomposition error Incorrect interpretation of “twice” and “thrice”\n\n- Following “around” - Following “opposite” - Other\n\n“after” interpreted as “and” Incorrect interpretation of “left” and ”right” Copy error\n\ncode-002 0\n6 6\n0 0\n7 0\n0\n\ncode-001 7\n10 3\n3 4\n4 0\n4\n\ntext-002 1\n16 15 1\n\n0 4\n0\n\nLeast-to-most prompting error analysis of 20 random failures for\n\nthe modTable 15: els code-davinci-001 and text-davinci-002 and all 13 errors for the model code-davinci-002. Note that for some examples, the model made more than one type of error.\n\nWe observe that the best model text-davinci-002 made only 2 types of mistakes: it sometimes makes a mistake when applying “twice” and “thrice” to an expression containing “around”, and it sometimes interprets “after” as “and”. For text-davinci-001, which is the older version of the same model, the errors are spread across all types. In particular, it’s worth noting that text-davinci-001 makes a significant number of decomposition errors and copy errors that were completely eliminated by its successor.\n\nThe model text-davinci-002 made most of its mistakes when interpreting “twice” and “thrice” following “around”. In addition, it sometimes made a mistake when translating “left” and “right”, which is something we did not observe with the other models. In some cases, it dropped the command entirely, whereas in other cases it invented a new action such as “LOOK LEFT” (see examples below).\n\nExamples of decomposition errors. In the example “run around left twice after jump around right thrice”, the code-davinci-001 model does not properly decompose the sub-expression “run around left twice”. Instead of decomposing it to the sequence [“run left”, “run around left”, “run around left twice”], it skips “run around left” and decomposes it to [“run left”, “run around left twice”]. Consequently, the model translates this sub-expression to (“TURN LEFT” + “RUN”) * 2 instead of (“TURN LEFT” + “RUN”) * 4 * 2.\n\nIn the example “look around right twice after jump around left twice”, the code-davinci-001 model does not properly decompose the sub-expression “jump around left twice”. Instead of decomposing it to the sequence [“jump left”, “jump around left”, “jump around left twice”], it skips “jump around left” and decomposes it to [“jump left”, “jump around left twice”]. Interestingly, the model is able to recover from this mistake and correctly translates the sub-expression to (“TURN LEFT” + “JUMP”) * 4 * 2, but still produces the wrong final action sequence because it interprets “after” like “and”.\n\nExamples of incorrect interpretation of “twice” and “thrice”. In the example “walk opposite right twice after run around right thrice”, the code-davinci-002 model correctly translates the expression “run around right” to (“TURN RIGHT” + “RUN”) * 4. Then it makes a mistake when applying “thrice” to this expression and produces (“TURN RIGHT” + “RUN”) * 9 instead of (“TURN RIGHT” + “RUN”) * 4 * 3 or (“TURN RIGHT” + “RUN”) * 12.\n\nexample\n\n“jump opposite\n\nthe twice In the code-davinci-002 model correctly translates the expression “jump around right” to (“TURN RIGHT” + “JUMP”) * 4. Then it makes a mistake when applying “thrice” to this expression and produces (“TURN RIGHT” + “JUMP”) * 8 instead of (“TURN RIGHT” + “JUMP”) * 4 * 3 or (“TURN RIGHT” + “JUMP”) * 12.\n\nand jump around right\n\nthrice”,\n\nright\n\nIn the example “walk around left thrice after run opposite left thrice”, the code-davinci-001 model correctly translates the expression “run opposite left” to “TURN LEFT” * 2 + “RUN”. Then it makes a mistake when applying “thrice” to this expression and produces “TURN LEFT” * 2 + “RUN” * 3 instead of (“TURN LEFT” * 2 + “RUN”) * 3.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nIn the example “walk around left thrice after look right twice”, the code-davinci-001 model correctly translates the expression “look right” to “TURN RIGHT” + “LOOK”. Then it makes a mistake when applying “twice” to this expression and produces “TURN RIGHT” + “LOOK” * 2 rather than (“TURN RIGHT” + “LOOK”) * 2.\n\nIn the example “walk left and run around right thrice”, the code-davinci-001 model interprets “thrice” as ‘twice”. This means that it produces “TURN LEFT” + “WALK” + (“TURN RIGHT” + “RUN”) * 4 * 2 instead of “TURN LEFT” + “WALK” + (“TURN RIGHT” + “RUN”) * 4 * 3.\n\nIn the example “jump right twice and look around left thrice”, the text-davinci-002 model correctly translates the sub-expression “look around left” to (“TURN LEFT” + “LOOK”) * 4. But when applying “thrice”, it produces the incorrect translation (“TURN LEFT” + “LOOK”) * 3 instead of (“TURN LEFT” + “LOOK”) * 4 * 3.\n\nExample of interpreting “after” as “and”. In the example “run opposite left thrice after run around left twice”, the code-davinci-002 model produces the correct translations for both sub-expressions that are connected by “after”, but it combines them as if they were connected by “and”. This means that the model produces (“TURN LEFT” * 2 + “RUN”) * 3 + (“TURN LEFT” + “RUN”) * 4 * 2 instead of (“TURN LEFT” + “RUN”) * 4 * 2 + (“TURN LEFT” * 2 + “RUN”) * 3.\n\nIn the example “walk around left thrice after walk twice”, the code-davinci-002 model produces the correct translations for both sub-expressions that are connected by “after”, but it combines them as if they were connected by “and”. This means that the model produces (“TURN LEFT” + “WALK”) * 4 * 3 + “WALK” * 2 instead of “WALK” * 2 + (“TURN LEFT” + “WALK”) * 4 * 3.\n\nIn the example “look around right twice after jump around left twice”, the code-davinci-001 model produces the correct translations for both sub-expressions that are connected by “after”, but it combines them as if they were connected by “and”. This means that the model produces (“TURN RIGHT” + “LOOK”) * 4 * 2 + (“TURN LEFT” + “JUMP”) * 4 * 2 instead of (“TURN LEFT” + “JUMP”) * 4 * 2 + (“TURN RIGHT” + “LOOK”) * 4 * 2.\n\nExamples of incorrect interpretation of “left” and “right”. In the example “look opposite right thrice after look around left thrice”, the text-davinci-002 model translates the component “look left” to “LOOK” instead of “TURN LEFT LOOK”. As a consequence, the whole command is translated to “LOOK” * 4 * 3 + (“TURN RIGHT” * 2 + “LOOK”) * 3 instead of (“TURN LEFT” + “LOOK”) * 4 * 3 + (“TURN RIGHT” * 2 + “LOOK”) * 3.\n\nIn the example “turn around right thrice after look around left twice”, the text-davinci-002 model makes up the new action “LOOK LEFT” as the translation of the component “look left”. As a consequence, it translates the whole command to (“LOOK LEFT” * 4) * 2 + (“TURN RIGHT” * 4) * 3 instead of (“TURN LEFT” + “LOOK”) * 4 * 2 + (“TURN RIGHT” * 4) * 3.\n\nExample of copy error. In the example “walk opposite right twice after look around left thrice”, the code-davinci-001 model produces the correct translations for both sub-expressions that are connected with “after”. In particular, it correctly translates “walk opposite right twice” to (“TURN RIGHT” * 2 + “WALK”) * 2. Then it makes two mistakes when combining them: it treats “after” like an “and”, and it omits the parenthesis when copying the expression (“TURN RIGHT” * 2 + “WALK”) * 2. This means that the model produces the translation “TURN RIGHT” * 2 + “WALK” * 2 + (“TURN LEFT” + “LOOK”) * 4 * 3” instead of (“TURN LEFT” + “LOOK”) * 4 * 3 + (“TURN RIGHT” * 2 + “WALK”) * 2.\n\n8.3 EXAMPLE OUTPUTS FROM CODE-DAVINCI-002\n\nIn this section, we present 5 examples of successes and failures of the code-davinci-002 model for chain-of-thought prompting and least-to-most prompting.\n\n8.3.1 CHAIN-OF-THOUGHT PROMPTING: SUCCESS\n\n“jump around left twice after walk opposite left thrice”\n\nGolden: TURN LEFT TURN LEFT WALK TURN LEFT TURN LEFT WALK TURN LEFT TURN LEFT WALK\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nTURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP\n\nSolution: Q: “jump around left twice after walk opposite left thrice” A: The output of “jump around left twice after walk opposite left thrice” concatenates: the output of “walk opposite left thrice”, the output of “jump around left twice”. “walk opposite left thrice” outputs (“TURN LEFT” * 2 + “WALK”) * 3. “jump around left twice” outputs (“TURN LEFT” + “JUMP”) * 4 * 2. So concatenating the output of “walk opposite left thrice” and the output of “jump around left twice” leads to (“TURN LEFT” * 2 + “WALK”) * 3 + (“TURN LEFT” + “JUMP”) * 4 * 2. So the output of “jump around left twice after walk opposite left thrice” is (“TURN LEFT” * 2 + “WALK”) * 3 + (“TURN LEFT” + “JUMP”) * 4 * 2.\n\n“walk around left twice and turn around right thrice”\n\nGolden: TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN RIGHT TURN RIGHT TURN RIGHT TURN RIGHT TURN RIGHT TURN RIGHT TURN RIGHT TURN RIGHT\n\nTURN RIGHT\n\nTURN RIGHT\n\nTURN RIGHT\n\nTURN RIGHT\n\nSolution: Q: “walk around left twice and turn around right thrice” A: The output of “walk around left twice and turn around right thrice” concatenates: the output of “walk around left”, the output of “walk around left”, the output of “turn around right”, the output of “turn around right”, the output of “turn around right”. “walk around left” outputs (“TURN LEFT” + “WALK”) * 4. “turn around right” outputs “TURN RIGHT” * 4. So concatenating the output of “walk around left”, the output of “walk around left”, the output of “turn around right”, the output of “turn around right”, the output of “turn around right” leads to (“TURN LEFT” + “WALK”) * 4 + (“TURN LEFT” + “WALK”) * 4 + “TURN RIGHT” * 4 + “TURN RIGHT” * 4 + “TURN RIGHT” * 4. So the output of “walk around left twice and turn around right thrice” is (“TURN LEFT” + “WALK”) * 4 + (“TURN LEFT” + “WALK”) * 4 + “TURN RIGHT” * 4 + “TURN RIGHT” * 4 + “TURN RIGHT” * 4.\n\n“jump around right thrice after run left thrice”\n\nGolden: TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP\n\nSolution: Q: “jump around right thrice after run left thrice” A: The output of “jump around right thrice after run left thrice” concatenates: the output of “run left thrice”, the output of “jump around right thrice”. “run left thrice” outputs (“TURN LEFT” + “RUN”) * 3. “jump around right thrice” outputs (“TURN RIGHT” + “JUMP”) * 12. So concatenating the output of “run left thrice” and the output of “jump around right thrice” leads to (“TURN LEFT” + “RUN”) * 3 + (“TURN RIGHT” + “JUMP”) * 12. So the output of “jump around right thrice after run left thrice” is (“TURN LEFT” + “RUN”) * 3 + (“TURN RIGHT” + “JUMP”) * 12.\n\n“run around left twice and walk around left thrice”\n\nGolden: TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nSolution: Q: “run around left twice and walk around left thrice” the output of A: The output of “run around left twice and walk around left thrice” concatenates: “run around left twice”, the output of “walk around left thrice”. “run around left twice” outputs (“TURN LEFT” + “RUN”) * 4 * 2. “walk around left thrice” outputs (“TURN LEFT” + “WALK”) * 4 * 3. So concatenating the output of “run around left twice” and the output of “walk around left thrice” leads to (“TURN LEFT” + “RUN”) * 4 * 2 + (“TURN LEFT” + “WALK”) * 4 * 3. So the output of “run around left twice and walk around left thrice” is (“TURN LEFT” + “RUN”) * 4 * 2 + (“TURN LEFT” + “WALK”) * 4 * 3.\n\n“jump around left thrice after turn around left thrice”\n\nGolden: TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP\n\nSolution: Q: “jump around left thrice after turn around left thrice” A: The output of “jump around left thrice after turn around left thrice” concatenates: the output of “turn around left thrice”, the output of “jump around left thrice”. “turn around left thrice” outputs “TURN LEFT” * 4 * 3. “jump around left thrice” outputs (“TURN LEFT” + “JUMP”) * 4 * 3. So concatenating the output of “turn around left thrice” and the output of “jump around left thrice” leads to “TURN LEFT” * 4 * 3 + (“TURN LEFT” + “JUMP”) * 4 * 3. So the output of “jump around left thrice after turn around left thrice” is “TURN LEFT” * 4 * 3 + (“TURN LEFT” + “JUMP”) * 4 * 3.\n\n8.3.2 CHAIN-OF-THOUGHT PROMPTING: FAILURE\n\n“look around right and look around right thrice”\n\nGolden: TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK TURN RIGHT LOOK\n\nSolution: Q: “look around right and look around right thrice” A: The output of “look around right and look around right thrice” concatenates: the output of “look around right”, the output of “look around right thrice”. “look around right” outputs “TURN RIGHT” * 4 + “LOOK”. “look around right thrice” outputs (“TURN RIGHT” * 4 + “LOOK”) * 3. So concatenating the output of “look around right” and the output of “look around right thrice” leads to “TURN RIGHT” * 4 + “LOOK” + (“TURN RIGHT” * 4 + “LOOK”) * 3. So the output of “look around right and look around right thrice” is “TURN RIGHT” * 4 + “LOOK” + (“TURN RIGHT” * 4 + “LOOK”) * 3.\n\n“run around left thrice and run left twice”\n\nGolden: TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN\n\nSolution: Q: “run around left thrice and run left twice” A: The output of “run around left thrice and run left twice” concatenates: the output of “run around\n\n35\n\nPublished as a conference paper at ICLR 2023\n\nleft thrice”, the output of “run left twice”. “run around left thrice” outputs (“TURN LEFT” + “RUN”) * 3. “run left twice” outputs “TURN LEFT” + “RUN” * 2. So concatenating the output of “run around left thrice” and the output of “run left twice” leads to (“TURN LEFT” + “RUN”) * 3 + “TURN LEFT” + “RUN” * 2. So the output of “run around left thrice and run left twice” is (“TURN LEFT” + “RUN”) * 3 + “TURN LEFT” + “RUN” * 2.\n\n“walk and walk around left thrice”\n\nGolden: WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK\n\nSolution: Q: “walk and walk around left thrice” A: The output of “walk and walk around left thrice” concatenates: the output of “walk”, the output of “walk around left thrice”. “walk” outputs “WALK”. “walk around left thrice” outputs (“TURN LEFT” + “WALK”) * 4. So concatenating the output of “walk” and the output of “walk around left thrice” leads to “WALK” + (“TURN LEFT” + “WALK”) * 4. So the output of “walk and walk around left thrice” is “WALK” + (“TURN LEFT” + “WALK”) * 4.\n\n“jump around right thrice after turn opposite left twice”\n\nGolden: TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP\n\nSolution: Q: “jump around right thrice after turn opposite left twice” the output A: The output of “jump around right thrice after turn opposite left twice” concatenates: of “turn opposite left twice”, the output of “jump around right thrice”. “turn opposite left twice” outputs “TURN LEFT” * 2. “jump around right thrice” outputs (“TURN RIGHT” + “JUMP”) * 4. So concatenating the output of “turn opposite left twice” and the output of “jump around right thrice” leads to “TURN LEFT” * 2 + (“TURN RIGHT” + “JUMP”) * 4. So the output of “jump around right thrice after turn opposite left twice” is “TURN LEFT” * 2 + (“TURN RIGHT” + “JUMP”) * 4.\n\n“run opposite right after walk around right thrice”\n\nGolden: TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT TURN RIGHT RUN\n\nSolution: Q: “run opposite right after walk around right thrice” the output of A: The output of “run opposite right after walk around right thrice” concatenates: “walk around right thrice” outputs “walk around right thrice”, the output of “run opposite right”. “run opposite right” outputs “TURN RIGHT” * 2 + “RUN”. So (“TURN RIGHT” + “WALK”) * 3. concatenating the output of “walk around right thrice” and the output of “run opposite right” leads to (“TURN RIGHT” + “WALK”) * 3 + “TURN RIGHT” * 2 + “RUN”. So the output of “run opposite right after walk around right thrice” is (“TURN RIGHT” + “WALK”) * 3 + “TURN RIGHT” * 2 + “RUN”.\n\n36\n\nPublished as a conference paper at ICLR 2023\n\n8.3.3 LEAST-TO-MOST PROMPTING: SUCCESS\n\n“jump around left thrice and walk opposite right”\n\nGolden: TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN RIGHT TURN RIGHT WALK\n\nDecomposition: Q: “jump around left thrice and walk opposite right” A: “jump around left thrice” can be solved by: “jump left”, “jump around left”, “jump around left thrice”. “walk opposite right” can be solved by: “walk opposite right”. So, “jump around left thrice and walk opposite right” can be solved by: “jump left”, “jump around left”, “jump around left thrice”, “walk opposite right”.\n\nSolution: Q: “jump left” A: The output of “jump left” concatenates: the output of “turn left”, the output of “jump”. “turn left” outputs “TURN LEFT”. “jump” outputs “JUMP”. So concatenating the output of “turn left” and the output of “jump” leads to “TURN LEFT” + “JUMP”. So the output of “jump left” is “TURN LEFT” + “JUMP”.\n\nQ: “jump around left” A: The output of “jump around left” concatenates: the output of “jump left”, the output of “jump left”, the output of “jump left”, the output of “jump left”. “jump left” outputs “TURN LEFT” + “JUMP”. So repeating the output of “jump around left” four times leads to (“TURN LEFT” + “JUMP”) * 4. So the output of “jump around left” is (“TURN LEFT” + “JUMP”) * 4.\n\nQ: “jump around left thrice” A: The output of “jump around left thrice” concatenates: the output of “jump around left”, the output of “jump around left”, the output of “jump around left”. “jump around left” outputs (“TURN LEFT” + “JUMP”) * 4. So repeating the output of “jump around left” three times leads to (“TURN LEFT” + “JUMP”) * 4 * 3. So the output of “jump around left thrice” is (“TURN LEFT” + “JUMP”) * 4 * 3.\n\nQ: “walk opposite right” A: The output of “walk opposite right” concatenates: the output of “turn opposite right”, the output of “walk”. “turn opposite right” outputs “TURN RIGHT” * 2. “walk” outputs “WALK”. So concatenating the output of “turn opposite right” and the output of “walk” leads to “TURN RIGHT” * 2 + “WALK”. So the output of “walk opposite right” is “TURN RIGHT” * 2 + “WALK”.\n\nQ: “jump around left thrice and walk opposite right” the output of A: The output of “jump around left thrice and walk opposite right” concatenates: “jump around left thrice”, the output of “walk opposite right”. “jump around left thrice” outputs (“TURN LEFT” + “JUMP”) * 4 * 3. “walk opposite right” outputs “TURN RIGHT” * 2 + “WALK”. So concatenating the output of “jump around left thrice” and the output of “walk opposite right” leads to (“TURN LEFT” + “JUMP”) * 4 * 3 + “TURN RIGHT” * 2 + “WALK”. So the output of “jump around left thrice and walk opposite right” is (“TURN LEFT” + “JUMP”) * 4 * 3 + “TURN RIGHT” * 2 + “WALK”.\n\n“turn around left thrice after run around right thrice”\n\nGolden: TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT\n\nDecomposition: Q: “turn around left thrice after run around right thrice” A: “turn around left thrice” can be solved by: “turn left”, “turn around left”, “turn around left thrice”. “run around right thrice” can be solved by: “run right”, “run around right”, “run around right thrice”. So, “turn around left thrice after run around right thrice” can be solved by: “turn left”, “turn around left”, “turn around left thrice”, “run right”, “run around right”, “run around right thrice”.\n\n37\n\nPublished as a conference paper at ICLR 2023\n\nSolution: Q: “turn around left” A: The output of “turn around left” concatenates: the output of “turn left”, the output of “turn left”, the output of “turn left”, the output of “turn left”. “turn left” outputs “TURN LEFT”. So repeating the output of “turn left” four times leads to “TURN LEFT” * 4. So the output of “turn around left” is “TURN LEFT” * 4.\n\nQ: “turn around left thrice” A: The output of “turn around left thrice” concatenates: the output of “turn around left”, the output of “turn around left”, the output of “turn around left”. “turn around left” outputs “TURN LEFT” * 4. So repeating the output of “turn around left” three times leads to (“TURN LEFT” * 4) * 3. So the output of “turn around left thrice” is (“TURN LEFT” * 4) * 3.\n\nQ: “run right” A: The output of “run right” concatenates: the output of “turn right”, the output of “run”. “turn right” outputs “TURN RIGHT”. “run” outputs “RUN”. So concatenating the output of “turn right” and the output of “run” leads to “TURN RIGHT” + “RUN”. So the output of “run right” is “TURN RIGHT” + “RUN”.\n\nQ: “run around right” A: The output of “run around right” concatenates: the output of “run right”, the output of “run right”, the output of “run right”, the output of “run right”. “run right” outputs “TURN RIGHT” + “RUN”. So repeating the output of “run right” four times leads to (“TURN RIGHT” + “RUN”) * 4. So the output of “run around right” is (“TURN RIGHT” + “RUN”) * 4.\n\nQ: “run around right thrice” A: The output of “run around right thrice” concatenates: the output of “run around right”, the output of “run around right”, the output of “run around right”. “run around right” outputs (“TURN RIGHT” + “RUN”) * 4. So repeating the output of “run around right” three times leads to ((“TURN RIGHT” + “RUN”) * 4) * 3. So the output of “run around right thrice” is ((“TURN RIGHT” + “RUN”) * 4) * 3.\n\nQ: “turn around left thrice after run around right thrice” the output of A: The output of “turn around left thrice after run around right thrice” concatenates: “run around right thrice” outputs “run around right thrice”, the output of “turn around left thrice”. ((“TURN RIGHT” + “RUN”) * 4) * 3. “turn around left thrice” outputs (“TURN LEFT” * 4) * 3. So concatenating the output of “run around right thrice” and the output of “turn around left thrice” leads to ((“TURN RIGHT” + “RUN”) * 4) * 3 + (“TURN LEFT” * 4) * 3. So the output of “turn around left thrice after run around right thrice” is ((“TURN RIGHT” + “RUN”) * 4) * 3 + (“TURN LEFT” * 4) * 3.\n\n“turn left after look around left thrice”\n\nGolden: TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT LOOK TURN LEFT\n\nDecomposition: Q: “turn left after look around left thrice” A: “turn left” can be solved by: “turn left”. “look around left thrice” can be solved by: “look left”, “look around left”, “look around left thrice”. So, “turn left after look around left thrice” can be solved by: “turn left”, “look left”, “look around left”, “look around left thrice”.\n\nSolution: Q: “look left” A: The output of “look left” concatenates: “turn left” outputs “TURN LEFT”. “look” outputs “LOOK”. So concatenating the output of “turn left” and the output of “look” leads to “TURN LEFT” + “LOOK”. So the output of “look left” is “TURN LEFT” + “LOOK”.\n\nthe output of “turn left”, the output of “look”.\n\nQ: “look around left” A: The output of “look around left” concatenates: the output of “look left”, the output of “look left”, the\n\n38\n\nPublished as a conference paper at ICLR 2023\n\noutput of “look left”, the output of “look left”. “look left” outputs “TURN LEFT” + “LOOK”. So repeating the output of “look left” four times leads to (“TURN LEFT” + “LOOK”) * 4. So the output of “look around left” is (“TURN LEFT” + “LOOK”) * 4.\n\nQ: “look around left thrice” A: The output of “look around left thrice” concatenates: the output of “look around left”, the output of “look around left”, the output of “look around left”. “look around left” outputs (“TURN LEFT” + “LOOK”) * 4. So repeating the output of “look around left” three times leads to ((“TURN LEFT” + “LOOK”) * 4) * 3. So the output of “look around left thrice” is ((“TURN LEFT” + “LOOK”) * 4) * 3.\n\nQ: “turn left after look around left thrice” A: The output of “turn left after look around left thrice” concatenates: the output of “look around left thrice”, the output of “turn left”. “look around left thrice” outputs ((“TURN LEFT” + “LOOK”) * 4) * 3. “turn left” outputs “TURN LEFT”. So concatenating the output of “look around left thrice” and the output of “turn left” leads to ((“TURN LEFT” + “LOOK”) * 4) * 3 + “TURN LEFT”. So the output of “turn left after look around left thrice” is ((“TURN LEFT” + “LOOK”) * 4) * 3 + “TURN LEFT”.\n\n“turn around left thrice and jump around left thrice”\n\nGolden: TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP TURN LEFT JUMP\n\nDecomposition: Q: “turn around left thrice and jump around left thrice” A: “turn around left thrice” can be solved by: “turn left”, “turn around left”, “turn around left thrice”. “jump around left thrice” can be solved by: “jump left”, “jump around left”, “jump around left thrice”. So, “turn around left thrice and jump around left thrice” can be solved by: “turn left”, “turn around left”, “turn around left thrice”, “jump left”, “jump around left”, “jump around left thrice”.\n\nSolution: Q: “turn around left” A: The output of “turn around left” concatenates: the output of “turn left”, the output of “turn left”, the output of “turn left”, the output of “turn left”. “turn left” outputs “TURN LEFT”. So repeating the output of “turn left” four times leads to “TURN LEFT” * 4. So the output of “turn around left” is “TURN LEFT” * 4.\n\nQ: “turn around left thrice” A: The output of “turn around left thrice” concatenates: the output of “turn around left”, the output of “turn around left”, the output of “turn around left”. “turn around left” outputs “TURN LEFT” * 4. So repeating the output of “turn around left” three times leads to (“TURN LEFT” * 4) * 3. So the output of “turn around left thrice” is (“TURN LEFT” * 4) * 3.\n\nQ: “jump left” A: The output of “jump left” concatenates: the output of “turn left”, the output of “jump”. “turn left” outputs “TURN LEFT”. “jump” outputs “JUMP”. So concatenating the output of “turn left” and the output of “jump” leads to “TURN LEFT” + “JUMP”. So the output of “jump left” is “TURN LEFT” + “JUMP”.\n\nQ: “jump around left” A: The output of “jump around left” concatenates: the output of “jump left”, the output of “jump left”, the output of “jump left”, the output of “jump left”. “jump left” outputs “TURN LEFT” + “JUMP”. So repeating the output of “jump left” four times leads to (“TURN LEFT” + “JUMP”) * 4. So the output of “jump around left” is (“TURN LEFT” + “JUMP”) * 4.\n\nQ: “jump around left thrice” A: The output of “jump around left thrice” concatenates: the output of “jump around left”, the output of “jump around left”, the output of “jump around left”. “jump around left” outputs (“TURN LEFT” + “JUMP”) * 4. So repeating the output of “jump around left” three times leads to ((“TURN LEFT” + “JUMP”) * 4) * 3. So the output of “jump around left thrice” is ((“TURN LEFT” + “JUMP”) * 4) * 3.\n\n39\n\nPublished as a conference paper at ICLR 2023\n\nQ: “turn around left thrice and jump around left thrice” the output of A: The output of “turn around left thrice and jump around left thrice” concatenates: “turn around left thrice” outputs “turn around left thrice”, the output of “jump around left thrice”. “jump around left thrice” outputs ((“TURN LEFT” + “JUMP”) * 4) * 3. So (“TURN LEFT” * 4) * 3. concatenating the output of “turn around left thrice” and the output of “jump around left thrice” leads to (“TURN LEFT” * 4) * 3 + ((“TURN LEFT” + “JUMP”) * 4) * 3. So the output of “turn around left thrice and jump around left thrice” is (“TURN LEFT” * 4) * 3 + ((“TURN LEFT” + “JUMP”) * 4) * 3.\n\n“walk around right twice after walk around right twice”\n\nGolden: TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK\n\nDecomposition: Q: “walk around right twice after walk around right twice” A: “walk around right twice” can be solved by: “walk right”, “walk around right”, “walk around right twice”. “walk around right twice” can be solved by: “walk right”, “walk around right”, “walk around right twice”. So, “walk around right twice after walk around right twice” can be solved by: “walk right”, “walk around right”, “walk around right twice”, “walk right”, “walk around right”, “walk around right twice”.\n\nSolution: Q: “walk right” A: The output of “walk right” concatenates: the output of “turn right”, the output of “walk”. “turn right” outputs “TURN RIGHT”. “walk” outputs “WALK”. So concatenating the output of “turn right” and the output of “walk” leads to “TURN RIGHT” + “WALK”. So the output of “walk right” is “TURN RIGHT” + “WALK”.\n\nQ: “walk around right” the output of “walk right”, the output of “walk A: The output of “walk around right” concatenates: “walk right” outputs “TURN RIGHT” right”, the output of “walk right”, the output of “walk right”. + “WALK”. So repeating the output of “walk around right” four times leads to (“TURN RIGHT” + “WALK”) * 4. So the output of “walk around right” is (“TURN RIGHT” + “WALK”) * 4.\n\nQ: “walk around right twice” A: The output of “walk around right twice” concatenates: the output of “walk around right”, the output of “walk around right”. “walk around right” outputs (“TURN RIGHT” + “WALK”) * 4. So repeating the output of “walk around right” twice leads to (“TURN RIGHT” + “WALK”) * 4 * 2. So the output of “walk around right twice” is (“TURN RIGHT” + “WALK”) * 4 * 2.\n\nQ: “walk around right twice after walk around right twice” A: The output of “walk around right twice after walk around right twice” concatenates: the output of “walk around right twice”, the output of “walk around right twice”. “walk around right twice” outputs (“TURN RIGHT” + “WALK”) * 4 * 2. So repeating the output of “walk around right twice” twice leads to (“TURN RIGHT” + “WALK”) * 4 * 2 * 2. So the output of “walk around right twice after walk around right twice” is (“TURN RIGHT” + “WALK”) * 4 * 2 * 2.\n\n8.3.4 LEAST-TO-MOST PROMPTING: FAILURE\n\n“jump opposite right twice and jump around right thrice”\n\nGolden: TURN RIGHT TURN RIGHT JUMP TURN RIGHT TURN RIGHT JUMP TURN RIGHT JUMP\n\n40\n\nPublished as a conference paper at ICLR 2023\n\nTURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP\n\nDecomposition: Q: “jump opposite right twice and jump around right thrice” A: “jump opposite right twice” can be solved by: “jump opposite right”, “jump opposite right twice”. “jump around right thrice” can be solved by: “jump right”, “jump around right”, “jump around right thrice”. So, “jump opposite right twice and jump around right thrice” can be solved by: “jump opposite right”, “jump opposite right twice”, “jump right”, “jump around right”, “jump around right thrice”.\n\nSolution: Q: “jump opposite right” A: The output of “jump opposite right” concatenates: the output of “turn opposite right”, the output of “jump”. “turn opposite right” outputs “TURN RIGHT” * 2. “jump” outputs “JUMP”. So concatenating the output of “turn opposite right” and the output of “jump” leads to “TURN RIGHT” * 2 + “JUMP”. So the output of “jump opposite right” is “TURN RIGHT” * 2 + “JUMP”.\n\nQ: “jump opposite right twice” A: The output of “jump opposite right twice” concatenates: the output of “jump opposite right”, the output of “jump opposite right”. “jump opposite right” outputs “TURN RIGHT” * 2 + “JUMP”. So repeating the output of “jump opposite right” twice leads to (“TURN RIGHT” * 2 + “JUMP”) * 2. So the output of “jump opposite right twice” is (“TURN RIGHT” * 2 + “JUMP”) * 2.\n\nQ: “jump right” A: The output of “jump right” concatenates: the output of “turn right”, the output of “jump”. “turn right” outputs “TURN RIGHT”. “jump” outputs “JUMP”. So concatenating the output of “turn right” and the output of “jump” leads to “TURN RIGHT” + “JUMP”. So the output of “jump right” is “TURN RIGHT” + “JUMP”.\n\nQ: “jump around right” A: The output of “jump around right” concatenates: the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”. “jump right” outputs “TURN RIGHT” + “JUMP”. So repeating the output of “jump around right” four times leads to (“TURN RIGHT” + “JUMP”) * 4. So the output of “jump around right” is (“TURN RIGHT” + “JUMP”) * 4.\n\nQ: “jump around right thrice” A: The output of “jump around right thrice” concatenates: the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”, the “jump right” outputs output of “jump right”, the output of “jump right”, the output of “jump right”. “TURN RIGHT” + “JUMP”. So repeating the output of “jump around right” eight times leads to (“TURN RIGHT” + “JUMP”) * 8. So the output of “jump around right thrice” is (“TURN RIGHT” + “JUMP”) * 8.\n\nQ: “jump opposite right twice and jump around right thrice” A: The output of “jump opposite right twice and jump around right thrice” concatenates: the output of “jump opposite right twice”, the output of “jump around right thrice”. “jump opposite right twice” outputs (“TURN RIGHT” * 2 + “JUMP”) * 2. “jump around right thrice” outputs (“TURN RIGHT” + “JUMP”) * 8. So concatenating the output of “jump opposite right twice” and the output of “jump around right thrice” leads to (“TURN RIGHT” * 2 + “JUMP”) * 2 + (“TURN RIGHT” + “JUMP”) * 8. So the output of “jump opposite right twice and jump around right thrice” is (“TURN RIGHT” * 2 + “JUMP”) * 2 + (“TURN RIGHT” + “JUMP”) * 8.\n\n“walk around left thrice after walk around right twice”\n\nGolden: TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN RIGHT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK\n\n41\n\nPublished as a conference paper at ICLR 2023\n\nDecomposition: Q: “walk around left thrice after walk around right twice” A: “walk around left thrice” can be solved by: “walk left”, “walk around left”, “walk around left thrice”. “walk around right twice” can be solved by: “walk right”, “walk around right”, “walk around right twice”. So, “walk around left thrice after walk around right twice” can be solved by: “walk left”, “walk around left”, “walk around left thrice”, “walk right”, “walk around right”, “walk around right twice”.\n\nSolution: Q: “walk left” A: The output of “walk left” concatenates: the output of “turn left”, the output of “walk”. “turn left” outputs “TURN LEFT”. “walk” outputs “WALK”. So concatenating the output of “turn left” and the output of “walk” leads to “TURN LEFT” + “WALK”. So the output of “walk left” is “TURN LEFT” + “WALK”.\n\nQ: “walk around left” A: The output of “walk around left” concatenates: the output of “walk left”, the output of “walk left”, the output of “walk left”, the output of “walk left”. “walk left” outputs “TURN LEFT” + “WALK”. So repeating the output of “walk around left” four times leads to (“TURN LEFT” + “WALK”) * 4. So the output of “walk around left” is (“TURN LEFT” + “WALK”) * 4.\n\nQ: “walk around left thrice” A: The output of “walk around left thrice” concatenates: the output of “walk around left”, the output of “walk around left”, the output of “walk around left”. “walk around left” outputs (“TURN LEFT” + “WALK”) * 4. So repeating the output of “walk around left” three times leads to (“TURN LEFT” + “WALK”) * 4 * 3. So the output of “walk around left thrice” is (“TURN LEFT” + “WALK”) * 4 * 3.\n\nQ: “walk right” A: The output of “walk right” concatenates: the output of “turn right”, the output of “walk”. “turn right” outputs “TURN RIGHT”. “walk” outputs “WALK”. So concatenating the output of “turn right” and the output of “walk” leads to “TURN RIGHT” + “WALK”. So the output of “walk right” is “TURN RIGHT” + “WALK”.\n\nQ: “walk around right” the output of “walk right”, the output of “walk A: The output of “walk around right” concatenates: “walk right” outputs “TURN RIGHT” right”, the output of “walk right”, the output of “walk right”. + “WALK”. So repeating the output of “walk around right” four times leads to (“TURN RIGHT” + “WALK”) * 4. So the output of “walk around right” is (“TURN RIGHT” + “WALK”) * 4.\n\nQ: “walk around right twice” A: The output of “walk around right twice” concatenates: the output of “walk around right”, the output of “walk around right”. “walk around right” outputs (“TURN RIGHT” + “WALK”) * 4. So repeating the output of “walk around right” twice leads to (“TURN RIGHT” + “WALK”) * 4 * 2. So the output of “walk around right twice” is (“TURN RIGHT” + “WALK”) * 4 * 2.\n\nQ: “walk around left thrice after walk around right twice” A: The output of “walk around left thrice after walk around right twice” concatenates: the output of “walk around left thrice”, the output of “walk around right twice”. “walk around left thrice” outputs (“TURN LEFT” + “WALK”) * 4 * 3. “walk around right twice” outputs (“TURN RIGHT” + “WALK”) * 4 * 2. So concatenating the output of “walk around left thrice” and the output of “walk around right twice” leads to (“TURN LEFT” + “WALK”) * 4 * 3 + (“TURN RIGHT” + “WALK”) * 4 * 2. So the output of “walk around left thrice after walk around right twice” is (“TURN LEFT” + “WALK”) * 4 * 3 + (“TURN RIGHT” + “WALK”) * 4 * 2.\n\n“run around left thrice after run around right twice”\n\nGolden: TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN RIGHT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN TURN LEFT RUN\n\nDecomposition: Q: “run around left thrice after run around right twice”\n\n42\n\nPublished as a conference paper at ICLR 2023\n\nA: “run around left thrice” can be solved by: “run left”, “run around left”, “run around left thrice”. “run around right twice” can be solved by: “run right”, “run around right”, “run around right twice”. So, “run around left thrice after run around right twice” can be solved by: “run left”, “run around left”, “run around left thrice”, “run right”, “run around right”, “run around right twice”.\n\nSolution: Q: “run left” A: The output of “run left” concatenates: the output of “turn left”, the output of “run”. “turn left” outputs “TURN LEFT”. “run” outputs “RUN”. So concatenating the output of “turn left” and the output of “run” leads to “TURN LEFT” + “RUN”. So the output of “run left” is “TURN LEFT” + “RUN”.\n\nQ: “run around left” A: The output of “run around left” concatenates: the output of “run left”, the output of “run left”, the output of “run left”, the output of “run left”. “run left” outputs “TURN LEFT” + “RUN”. So repeating the output of “run around left” four times leads to (“TURN LEFT” + “RUN”) * 4. So the output of “run around left” is (“TURN LEFT” + “RUN”) * 4.\n\nQ: “run around left thrice” A: The output of “run around left thrice” concatenates: the output of “run around left”, the output of “run around left”, the output of “run around left”. “run around left” outputs (“TURN LEFT” + “RUN”) * 4. So repeating the output of “run around left” three times leads to (“TURN LEFT” + “RUN”) * 4 * 3. So the output of “run around left thrice” is (“TURN LEFT” + “RUN”) * 4 * 3.\n\nQ: “run right” A: The output of “run right” concatenates: the output of “turn right”, the output of “run”. “turn right” outputs “TURN RIGHT”. “run” outputs “RUN”. So concatenating the output of “turn right” and the output of “run” leads to “TURN RIGHT” + “RUN”. So the output of “run right” is “TURN RIGHT” + “RUN”.\n\nQ: “run around right” A: The output of “run around right” concatenates: the output of “run right”, the output of “run right”, the output of “run right”, the output of “run right”. “run right” outputs “TURN RIGHT” + “RUN”. So repeating the output of “run around right” four times leads to (“TURN RIGHT” + “RUN”) * 4. So the output of “run around right” is (“TURN RIGHT” + “RUN”) * 4.\n\nQ: “run around right twice” A: The output of “run around right twice” concatenates: the output of “run around right”, the output of “run around right”. “run around right” outputs (“TURN RIGHT” + “RUN”) * 4. So repeating the output of “run around right” twice leads to (“TURN RIGHT” + “RUN”) * 4 * 2. So the output of “run around right twice” is (“TURN RIGHT” + “RUN”) * 4 * 2.\n\nQ: “run around left thrice after run around right twice” the output of A: The output of “run around left thrice after run around right twice” concatenates: “run around left thrice”, the output of “run around right twice”. “run around left thrice” outputs (“TURN LEFT” + “RUN”) * 4 * 3. “run around right twice” outputs (“TURN RIGHT” + “RUN”) * 4 * 2. So concatenating the output of “run around left thrice” and the output of “run around right twice” leads to (“TURN LEFT” + “RUN”) * 4 * 3 + (“TURN RIGHT” + “RUN”) * 4 * 2. So the output of “run around left thrice after run around right twice” is (“TURN LEFT” + “RUN”) * 4 * 3 + (“TURN RIGHT” + “RUN”) * 4 * 2.\n\n“jump opposite right twice after jump around right thrice”\n\nGolden: TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT JUMP TURN RIGHT TURN RIGHT JUMP TURN RIGHT TURN RIGHT JUMP\n\nDecomposition: Q: “jump opposite right twice after jump around right thrice” A: “jump opposite right twice” can be solved by: “jump opposite right”, “jump opposite right twice”. “jump around right thrice” can be solved by: “jump right”, “jump around right”, “jump around right\n\n43\n\nPublished as a conference paper at ICLR 2023\n\nthrice”. So, “jump opposite right twice after jump around right thrice” can be solved by: “jump opposite right”, “jump opposite right twice”, “jump right”, “jump around right”, “jump around right thrice”.\n\nSolution: Q: “jump opposite right” A: The output of “jump opposite right” concatenates: the output of “turn opposite right”, the output of “jump”. “turn opposite right” outputs “TURN RIGHT” * 2. “jump” outputs “JUMP”. So concatenating the output of “turn opposite right” and the output of “jump” leads to “TURN RIGHT” * 2 + “JUMP”. So the output of “jump opposite right” is “TURN RIGHT” * 2 + “JUMP”.\n\nQ: “jump opposite right twice” A: The output of “jump opposite right twice” concatenates: the output of “jump opposite right”, the output of “jump opposite right”. “jump opposite right” outputs “TURN RIGHT” * 2 + “JUMP”. So repeating the output of “jump opposite right” twice leads to (“TURN RIGHT” * 2 + “JUMP”) * 2. So the output of “jump opposite right twice” is (“TURN RIGHT” * 2 + “JUMP”) * 2.\n\nQ: “jump right” A: The output of “jump right” concatenates: the output of “turn right”, the output of “jump”. “turn right” outputs “TURN RIGHT”. “jump” outputs “JUMP”. So concatenating the output of “turn right” and the output of “jump” leads to “TURN RIGHT” + “JUMP”. So the output of “jump right” is “TURN RIGHT” + “JUMP”.\n\nQ: “jump around right” A: The output of “jump around right” concatenates: the output of “jump right”, the output of “jump “jump right” outputs “TURN RIGHT” right”, the output of “jump right”, the output of “jump right”. + “JUMP”. So repeating the output of “jump around right” four times leads to (“TURN RIGHT” + “JUMP”) * 4. So the output of “jump around right” is (“TURN RIGHT” + “JUMP”) * 4.\n\nQ: “jump around right thrice” A: The output of “jump around right thrice” concatenates: the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”, the output of “jump right”. “jump right” outputs “TURN RIGHT” + “JUMP”. So repeating the output of “jump around right” eight times leads to (“TURN RIGHT” + “JUMP”) * 8. So the output of “jump around right thrice” is (“TURN RIGHT” + “JUMP”) * 8.\n\nQ: “jump opposite right twice after jump around right thrice” A: The output of “jump opposite right twice after jump around right thrice” concatenates: the output “jump around right thrice” of “jump around right thrice”, the output of “jump opposite right twice”. outputs (“TURN RIGHT” + “JUMP”) * 8. “jump opposite right twice” outputs (“TURN RIGHT” * 2 + “JUMP”) * 2. So concatenating the output of “jump around right thrice” and the output of “jump opposite right twice” leads to (“TURN RIGHT” + “JUMP”) * 8 + (“TURN RIGHT” * 2 + “JUMP”) * 2. So the output of “jump opposite right twice after jump around right thrice” is (“TURN RIGHT” + “JUMP”) * 8 + (“TURN RIGHT” * 2 + “JUMP”) * 2.\n\n“walk around left thrice after walk right twice”\n\nGolden: TURN RIGHT WALK TURN RIGHT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK TURN LEFT WALK\n\nDecomposition: Q: “walk around left thrice after walk right twice” A: “walk around left thrice” can be solved by: “walk left”, “walk around left”, “walk around left thrice”. “walk right twice” can be solved by: “walk right”, “walk right twice”. So, “walk around left thrice after walk right twice” can be solved by: “walk left”, “walk around left”, “walk around left thrice”, “walk right”, “walk right twice”.\n\nSolution:\n\n44\n\nPublished as a conference paper at ICLR 2023\n\nQ: “walk left” A: The output of “walk left” concatenates: the output of “turn left”, the output of “walk”. “turn left” outputs “TURN LEFT”. “walk” outputs “WALK”. So concatenating the output of “turn left” and the output of “walk” leads to “TURN LEFT” + “WALK”. So the output of “walk left” is “TURN LEFT” + “WALK”.\n\nQ: “walk around left” A: The output of “walk around left” concatenates: the output of “walk left”, the output of “walk left”, the output of “walk left”, the output of “walk left”. “walk left” outputs “TURN LEFT” + “WALK”. So repeating the output of “walk around left” four times leads to (“TURN LEFT” + “WALK”) * 4. So the output of “walk around left” is (“TURN LEFT” + “WALK”) * 4.\n\nQ: “walk around left thrice” A: The output of “walk around left thrice” concatenates: the output of “walk around left”, the output of “walk around left”, the output of “walk around left”. “walk around left” outputs (“TURN LEFT” + “WALK”) * 4. So repeating the output of “walk around left” three times leads to (“TURN LEFT” + “WALK”) * 4 * 3. So the output of “walk around left thrice” is (“TURN LEFT” + “WALK”) * 4 * 3.\n\nQ: “walk right” A: The output of “walk right” concatenates: the output of “turn right”, the output of “walk”. “turn right” outputs “TURN RIGHT”. “walk” outputs “WALK”. So concatenating the output of “turn right” and the output of “walk” leads to “TURN RIGHT” + “WALK”. So the output of “walk right” is “TURN RIGHT” + “WALK”.\n\nQ: “walk right twice” A: The output of “walk right twice” concatenates: the output of “walk right”, the output of “walk right”. “walk right” outputs “TURN RIGHT” + “WALK”. So repeating the output of “walk right” twice leads to (“TURN RIGHT” + “WALK”) * 2. So the output of “walk right twice” is (“TURN RIGHT” + “WALK”) * 2.\n\nQ: “walk around left thrice after walk right twice” A: The output of “walk around left thrice after walk right twice” concatenates: the output of “walk around left thrice”, the output of “walk right twice”. “walk around left thrice” outputs (“TURN LEFT” “walk right twice” outputs (“TURN RIGHT” + “WALK”) * 2. So concatenating + “WALK”) * 4 * 3. the output of “walk around left thrice” and the output of “walk right twice” leads to (“TURN LEFT” + “WALK”) * 4 * 3 + (“TURN RIGHT” + “WALK”) * 2. So the output of “walk around left thrice after walk right twice” is (“TURN LEFT” + “WALK”) * 4 * 3 + (“TURN RIGHT” + “WALK”) * 2.\n\n8.4 EXPANDING PYTHON EXPRESSIONS USING PROMPTING\n\nIn Section 3.2, we mention that expanding the Python expressions that we use as an intermediate representation can be done either with a simple postprocessing script or by prompting a language model. In the following, we present a prompt that achieves 99.7% accuracy on a random sample of 1000 Python expressions that are outputted by our solution (using the code-davinci-002 model). This demonstrates that the L2M method can solve SCAN with a combined accuracy more than 99% (99.7% accuracy for generating the intermediate Python expressions and 99.7% for expanding these expressions), even if we do not use the Python executor and instead perform the expansion of the intermediate representation via prompting.\n\nQ: “JUMP” * 3 Rewrite: “JUMP” * 3 A: 1 JUMP 2 JUMP 3 JUMP\n\nQ: “RUN” * 4 * 2 Rewrite: “RUN” * 8 A: 1 RUN 2 RUN 3 RUN 4 RUN 5 RUN 6 RUN 7 RUN 8 RUN\n\nQ: “TURN RIGHT” + “WALK” Rewrite: “TURN RIGHT” + “WALK” A: TURN RIGHT WALK\n\n45\n\nPublished as a conference paper at ICLR 2023\n\nQ: (“TURN LEFT” + “LOOK”) * 2 + “TURN LEFT” + “LOOK” Rewrite: (“TURN LEFT” + “LOOK”) * 2 + “TURN LEFT” + “LOOK” A: 1 (TURN LEFT LOOK) 2 (TURN LEFT LOOK) TURN LEFT LOOK\n\nQ: (“TURN RIGHT” * 2 + “JUMP”) * 4 Rewrite: (“TURN RIGHT” * 2 + “JUMP”) * 4 A: 1 (1 TURN RIGHT 2 TURN RIGHT JUMP) 2 (1 TURN RIGHT 2 TURN RIGHT JUMP) 3 (1 TURN RIGHT 2 TURN RIGHT JUMP) 4 (1 TURN RIGHT 2 TURN RIGHT JUMP)\n\nQ: “TURN LEFT” * 2 + (“TURN RIGHT” + “WALK”) * 4 * 2 Rewrite: “TURN LEFT” * 2 + (“TURN RIGHT” + “WALK”) * 8 A: 1 TURN LEFT 2 TURN LEFT 1 (TURN RIGHT WALK) 2 (TURN RIGHT WALK) 3 (TURN RIGHT WALK) 4 (TURN RIGHT WALK) 5 (TURN RIGHT WALK) 6 (TURN RIGHT WALK) 7 (TURN RIGHT WALK) 8 (TURN RIGHT WALK)\n\nDiscussion. The prompt consists of 6 examples, each of which illustrates part of the knowledge needed for this task. Note that we add numbers and parentheses when we unfold multiplication to make it easier for the model to keep track of the repetitions.\n\n1. Multiplication\n\n2. Sequential multiplication\n\n3. Addition\n\n4. Avoid commutativity / associativity in addition\n\n5. Nested multiplication\n\n6. Addition of two multiplications\n\n9 DROP\n\n9.1 RESULTS WITH T E X T-D A V I N C I-002 AND LM-540B\n\nWe reported the results using code-davinci-002. Here, we report results using the text-davinci-002 model and a language model with 540 billion parameters (LM-540B).\n\nPrompting method Zero-Shot Standard prompting Chain-of-Thought Least-to-Most\n\ntext-davinci-002 Non-football (500 cases) 27.00 49.40 60.80 74.20\n\nFootball (500 cases) 31.60 54.40 57.40 63.40\n\nAccuracies\n\nand prompting methods with the GPT-3 Table 16: text-davinci-002 model on the numerical reasoning subset of DROP. We evaluate on 500 randomly sampled non-football/football examples. Compared to Table ??, we observe that text-davinci-002 is consistently worse than code-davinci-002.\n\nzero-shot\n\n(%) of\n\n9.2 NON-FOOTBALL SUBSET\n\n9.2.1 ZERO-SHOT PROMPTING\n\nFor zero-shot, the prompt format is as follows:\n\nQ: {question} A: The answer is\n\nNotice that we add “The answer is” at the beginning of the answer section.\n\n46\n\nPublished as a conference paper at ICLR 2023\n\nPrompting method Zero-Shot Standard prompting Chain-of-Thought Least-to-Most\n\nlm-540b Non-football (3988 cases) 48.42 56.54 63.84 79.24\n\nFootball (1862 cases) 44.95 60.47 67.35 69.98\n\nTable 17: Accuracies (%) of zero-shot and prompting methods with a pretrained language model with 540 billion parameters (lm-540).\n\n9.2.2 STANDARD PROMPTING WITH 3 EXAMPLES\n\nQ: Since the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states. How many years did the European Coal and Steel Community exist? A: The answer is 7.\n\nQ: In the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older. How many more percent are under the age of 18 compared to the 18 to 24 group? A: The answer is 14.8.\n\nQ: Playing in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Stafford’s five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving. How many TD passes did Stafford throw other than to Johnson? A: The answer is 2.\n\n9.2.3 CHAIN-OF-THOUGHT PROMPTING WITH 3 EXAMPLES\n\nQ: Since the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states. How many years did the European Coal and Steel Community exist? A: According to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7.\n\nQ: In the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older. How many more percent are under the age of 18 compared to the 18 to 24 group? A: According to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8.\n\nQ: Playing in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Stafford’s five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving. How many TD passes did Stafford throw other than to Jhonson?\n\n47\n\nPublished as a conference paper at ICLR 2023\n\nA: According to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2.\n\n9.2.4 LEAST-TO-MOST PROMPTING I: PROBLEM DECOMPOSITION (5 EXAMPLES)\n\nQ: The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older. Which age group is larger: under the age of 18 or 18 and 24? A: To answer the question “Which age group is larger: under the age of 18 or 18 and 24?”, we need to know: “How many percent were under the age of 18?”, “How many percent were between the ages of 18 and 24?”.\n\nQ: Old age pensions were raised by 300 francs per month to 1,700 francs for a single person and to 3,700 francs for a couple, while health insurance benefits were made more widely available to unemployed persons and part-time employees. How many francs were the old age pensions for a single person before they were raised? A: To answer the question “How many francs were the old age pensions for a single person before they were raised?”, we need to know: “How many francs were the old age pensions for a single person?”, “How many francs were old age pensions raised for a single person?”.\n\nQ: In April 2011, the ECB raised interest rates for the first time since 2008 from 1% to 1.25%, with a further increase to 1.50% in July 2011. However, in 2012-2013 the ECB lowered interest rates to encourage economic growth, reaching the historically low 0.25% in November 2013. Soon after the rates were cut to 0.15%, then on 4 September 2014 the central bank reduced the rates from 0.15% to 0.05%, the lowest rates on record. How many percentage points did interest rates drop between April 2011 and September 2014? A: To answer the question “How many percentage points did interest rates drop between April 2011 and September 2014?”, we need to know: “What was the interest rate in April 2011?”, “What was the interest rate in September 2014?”.\n\nQ: Non-nationals make up more than half of the population of Bahrain. According to government statistics dated between 2005-2009 roughly 290,000 Indians, 125,000 Bangladeshis, 45,000 Pakistanis, 45,000 Filipinos, and 8,000 Indonesians. How many Pakistanis and Indonesians are in Bahrain? A: To answer the question “How many Pakistanis and Indonesians are in Bahrain?”, we need to know: “How many Pakistanis are in Bahrain?”, “How many Indonesians are in Bahrain?”.\n\nQ: The Census Bureaus 2006-2010 American Community Survey showed that (in 2010 inflation adjustment dollars) median household income was $52,056 and the median family income was $58,942. How many years did the Census Bureaus American Community Survey last? A: To answer the question “How many years did the Census Bureaus American Community Survey last?”, we need to know: “When did the Census Bureaus American Community Survey start?”, “When did the Census Bureaus American Community Survey end?”.\n\n9.2.5 LEAST-TO-MOST PROMPTING II: PROBLEM SOLVING (3 EXAMPLES)\n\nThe city of Bangkok has a population of 8,280,925. The census showed that it is home to 81,570 Japanese and 55,893 Chinese nationals, as well as 117,071 expatriates from other Asian countries, 48,341 from Europe, 23,418 from the Americas. Q: How many Chinese nationals are there? A: “The census showed that it is home to 81,570 Japanese and 55,893 Chinese nationals”. So the answer is 55,893 Chinese nationals.\n\nQ: How many European nationals are there? A: “48,341 from Europe”. So the answer is 48,341.\n\nQ: How many more Chinese nationals are there than European nationals? A: We know that there are 55,893 Chinese nationals. We also know that there are 48,341 European nationals. So there are 55,893 - 48,341 = 7552 more Chinese nationals than European nationals. So the answer is 7552.\n\nThe gender distribution of the population was 50.2% male and 49.8% female. Of the adult population, 29 people or 14.6% of the population are between 20 and 29 years old. 28 people or 14.1% are\n\n48\n\nPublished as a conference paper at ICLR 2023\n\n30 to 39, 36 people or 18.2% are 40 to 49, and 31 people or 15.7% are 50 to 59. Q: What is the percentage of people between 40 to 49?. A: “36 people or 18.2% are 40 to 49”. So the answer is 18.2\n\nQ: How many percent of people were not 40 to 49? A: We know that 18.2% are 40 to 49. So 100% - 18.2% = 81.8% are not 40 to 49. So the answer is 81.8%.\n\nDuring the 1970s the oil shocks (1973 oil crisis and 1979 energy crisis) hit Iceland hard. Inflation rose to 43% in 1974 and 59% in 1980, falling to 15% in 1987 but rising to 30% in 1988. Iceland experienced moderately strong GDP growth (3% on average) from 1995 to 2004. Growth slowed between 2000 and 2002, but the economy expanded by 4.3% in 2003 and grew by 6.2% in 2004. Growth in 2005 exceeded 6%. Q: When did the oil crisis occur? A: “1973 oil crisis and 1979 energy crisis”. So the answer is 1973.\n\nQ: When did the energy crisis occur? A: “1979 energy crisis”. So the answer is 1979.\n\nQ: How many years passed between the oil and energy crises? A: We know that the oil crisis occurred in 1973. We also know that the energy crisis occurred in 1979. So 1979 - 1973 = 6 years passed between the oil and energy crises. So the answer is 6.\n\n9.3 FOOTBALL SUBSET\n\n9.3.1 ZERO-SHOT PROMPTING\n\nFor zero-shot, the prompt format is as follows:\n\nQ: {question} A: The answer is\n\nNotice that we add “The answer is” at the beginning of the answer section.\n\n9.3.2 STANDARD PROMPTING WITH 3 EXAMPLES\n\nQ: The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. How many yards do the shortest touchdown run and the longest touchdown pass combine for? A: The answer is 21.\n\nQ: The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime. How many points did the Ravens have at halftime? A: The answer is 10.\n\nQ: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith. How many scoreless quarters were there? A: The answer is 2.\n\n9.3.3 CHAIN-OF-THOUGHT PROMPTING WITH 3 EXAMPLES\n\nQ: The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In\n\n49\n\nPublished as a conference paper at ICLR 2023\n\nthe third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. How many yards do the shortest touchdown run and the longest touchdown pass combine for? A: All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21 yards.\n\nQ: The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime. How many points did the Ravens have at halftime? A: The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10 points.\n\nQ: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith. How many scoreless quarters were there? A: The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.\n\n9.3.4 LEAST-TO-MOST PROMPTING I: PROBLEM DECOMPOSITION (6 EXAMPLES)\n\nQ: The Seahawks played the San Francisco 49ers. In the first quarter, the ’Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB T. J. Duckett made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. How many yards do the shortest touchdown run and the longest touchdown pass combine for? A: To answer the question “How many yards do the shortest touchdown run and the longest touchdown pass combine for?”, we need to know: “How many yards was the shortest touchdown run?”, “How many yards was the longest touchdown pass?”.\n\nQ: The Steelers went home for an AFC North duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown run from running back Rashard Mendenhall. The Ravens would make it even in the second quarter as running back Willis McGahee got a 9-yard touchdown run. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter and a 33-yard field goal in the third quarter. Game ended with a scoreless fourth quarter. How many points did the Ravens have at halftime? A: To answer the question “How many points did the Ravens have at halftime?”, we need to know: “What were all the scores the Ravens had at halftime?”.\n\nQ: In 1995, the Kings overcame a 3-4 start to win eight of their final nine games and finished with a record, the second-best in the AFC. Quarterback Neil ODonnell, who completed 246 out of 416 passes for 2,970 yards and 17 touchdowns, with only seven interceptions led their offense. The Kings finished their 1995 season having lost how many games difference to the number of games they had won? A: To answer the question “The Kings finished their 1995 season having lost how many games difference to the number of games they had won?”, we need to know: “How many games the Kings had lost in their 1995 season?”, “How many games the Kings had won in their 1995 season?”.\n\nQ: The Broncos traveled to Sun Life Stadium to face the Miami Dolphins. The Dolphins grabbed the lead in the second quarter, with field goals of 35 and 45 yards by kicker Dan Carpenter. In the final quarter, the Dolphins quarterback Matt Moore threw a 16-yard touchdown pass to tight end Anthony Fasano (with a failed two-point conversion attempt), followed by a 35-yard field goal by Carpenter, giving Miami a 15-0 lead. Finally, the Broncos answered with Alex Jake kicking a 48-yard field\n\n50\n\nPublished as a conference paper at ICLR 2023\n\ngoal, ending the game with 15-3. How many yards was Dan Carpenter’s longest field goal? A: To answer the question “How many yards was Dan Carpenter’s longest field goal?”, we need to know: “What were all Dan Carpenter’s field goals?”.\n\nQ: The Cardinals flew to Gillette Stadium for a duel with the New England Patriots. In the fourth quarter, New England ended the boring game with Gostkowski’s 30-yard field goal. Original starting quarterback Kurt Warner (6/18 for 30 yards) was pulled in the third quarter via coach’s decision. How many incomplete passes did Kurt Warner have? A: To answer the question “How many incomplete passes did Kurt Warner have?”, we need to know: “How many passes did Kurt Warner attempt?”, “How many passes did Kurt Warner complete?”.\n\nQ: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard touchdown pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard TD pass to wide receiver Steve Smith. How many scoreless quarters were there? A: To answer the question “How many scoreless quarters were there?”, we need to know: “What were all the scoreless quarters?”.\n\n9.3.5 LEAST-TO-MOST PROMPTING II: PROBLEM SOLVING (3 EXAMPLES)\n\nThe Seahawks played the San Francisco 49ers. In the first quarter, the ’Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. Q: How many yards was the shortest touchdown run? A: All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number between 27, 9, 11 is 9. The shortest touchdown run was 9 yards. The answer is 9.\n\nQ: How many yards was the longest touchdown pass? A: All the touchdown passes are: a 12-yard touchdown pass. The longest touchdown pass was 12 yards. The answer is 12.\n\nQ: How many yards do the shortest touchdown run and the longest touchdown pass combine for? A: We know that the shortest touchdown run was 9-yard. We also know that the longest touchdown pass was 12-yard. 9 + 12 = 21. The answer is 21.\n\nThe Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime. Q: What were all the scores the Ravens had at halftime? A: The answer is: a 9-yard touchdown run, a 45-yard field goal.\n\nQ: How many points did the Ravens have at halftime? A: We know that the Ravens had a touchdown and a field goal at halftime. We also know that a touchdown is worth 7 points and a field goal is worth 3 points. 7 + 3 = 10. The answer is 10.\n\nThe Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith. Q: What were all the scoreless quarters? A: The answer is: the first quarter and the third quarter.\n\nQ: How many scoreless quarters were there? A: We know that the first and third quarters were all the scoreless quarters. The answer is 2.\n\n51\n\nPublished as a conference paper at ICLR 2023\n\n9.4 EXAMPLES WHERE LEAST-TO-MOST SUCCEEDED BUT CHAIN-OF-THOUGHT FAILED\n\n9.4.1 CASE 1\n\nQ: The first two Buick automobiles were made in 1899 and 1900 at the “Buick Auto-Vim and Power Company” by chief-engineer Walter Lorenzo Marr, but company owner David Dunbar Buick was reluctant to begin making automobiles, being satisfied with stationary and marine engine production, so Marr left Buick in 1901 to found Marr (automobile) under his own name. His replacement was Eugene Richard, who applied for a patent in 1902 for Marrs valve-in-head (Overhead valve engine) engine, which patent, number 771,095, was awarded to Richard in the name of Buick in 1904. In 1903, the third Buick automobile was made, this time by Richard, but in 1904 Buick, whose company was now called “Buick Motor Company,” moved to Flint, Michigan, and Richard stayed behind. Marr was rehired in Flint as chief engineer, to begin making automobiles in production. That year, 37 Buick automobiles were made, production increasing to 750 in 1905, 1,400 in 1906, 4,641 in 1907, and 8,800 in 1908, taking the number one spot away from close competitors Oldsmobile, Ford Motor Company, and Maxwell automobile. How many Buick automobiles were built between 1904 and 1905?\n\nChain-of-thought by code-davinci-002 A: According to the passage, 37 Buick automobiles were made in 1904, and 750 were made in 1905. 750 - 37 = 713. So the answer is 713.\n\nLeast-to-most by code-davinci-002 Q: How many Buick automobiles were built in 1904? A: “In 1903, the third Buick automobile was made, this time by Richard, but in 1904 Buick, whose company was now called “Buick Motor Company,” moved to Flint, Michigan, and Richard stayed behind. Marr was rehired in Flint as chief engineer, to begin making automobiles in production. That year, 37 Buick automobiles were made”. So the answer is 37.\n\nQ: How many Buick automobiles were built in 1905? A: “production increasing to 750 in 1905”. So the answer is 750.\n\nQ: How many Buick automobiles were built between 1904 and 1905? A: We know that 37 Buick automobiles were built in 1904. We also know that 750 Buick automobiles were built in 1905. So 37 + 750 = 787 Buick automobiles were built between 1904 and 1905. So the answer is 787.\n\nAnalysis Chain-of-thought failed because it computed “750 - 37 = 713”, which is wrong. The correct answer is “37 + 750 = 787”.\n\n9.4.2 CASE 2\n\nQ: The Guatemalan authorities were unaware of ongoing contacts between the colonial authorities in Yucat ́an and the Itza. In the second half of 1695 President Barrios began planning another expedition against the Itza from Verapaz, following harsh criticism of Juan D ́ıaz de Velasco’s retreat from near Lake Pet ́en Itz ́a in April of that year. Barrios died in November, and oidor Jos ́e de Escals became acting head of the Guatemalan colonial administration. He appointed oidor Bartolom ́e de Am ́esqueta to lead the next expedition against the Itza. Am ́esqueta marched his men through torrential rain from Cahab ́on to Mop ́an, arriving on 25 February 1696; the Guatemalans were still unaware of the clashes between Garc ́ıa’s forces and the Itza. Am ́esqueta’s expedition was suffering from shortages of native labour and supplies. Twenty-five of his 150 men were sick, and his advance stalled as the force rested up at Mop ́an. Captain D ́ıaz de Velasco volunteered to lead a party of 25 men ahead to the lake; he was accompanied by Dominican friars Crist ́obal de Prada and Jacinto de Vargas. He was also accompanied by AjK’ixaw, the Itza nobleman who had been taken prisoner on D ́ıaz’s previous expedition, who served as a trusted guide, scout and translator. They set out from Mop ́an on 7 March and left some sick soldiers with some supplies after five days, they then caught up with an advance party of roadbuilders and their military escort at R ́ıo Chakal. The two forces combined, giving D ́ıaz a force of 49 soldiers and 34 porters and archers from Verapaz. Once they arrived at IxB’ol, near the shore of Lake Pet ́en Itz ́a, AjK’ixaw was sent ahead as an emissary to Nojpet ́en. How many months after Juan D ́ıaz de Velasco’s retreat did President Barrios die?\n\n52\n\nPublished as a conference paper at ICLR 2023\n\nChain-of-thought by code-davinci-002 A: According to the passage, President Barrios died in November, and the expedition against the Itza was led by Bartolom ́e de Am ́esqueta in February. So the answer is 3.\n\nLeast-to-most by code-davinci-002 Q: When did Juan D ́ıaz de Velasco’s retreat? A: “April of that year”. So the answer is April.\n\nQ: When did President Barrios die? A: “November”. So the answer is November.\n\nQ: How many months after Juan D ́ıaz de Velasco’s retreat did President Barrios die? A: We know that Juan D ́ıaz de Velasco’s retreat was in April. We also know that President Barrios died in November. So November - April = 7 months after Juan D ́ıaz de Velasco’s retreat did President Barrios die. So the answer is 7.\n\nAnalysis Chain-of-thought generated the wrong final answer because one fact it used, “the expedition against the Itza was led by Bartolom ́e de Am ́esqueta in February”, is unrelated.\n\n9.4.3 CASE 3\n\nQ: Peele was christened on 25 July 1556 at St James Garlickhythe in the City of London. His father, James Peele (d. Dec 30, 1585), who appears to have belonged to a Devonshire family, was clerk of Christs Hospital, a school which was then situated in central London, and wrote two treatises on bookkeeping, The Maner and Fourme How to Kepe a Perfecte Reconyng (1553) and The Pathe Waye to Perfectnes (1569). The latter depicts James Peele in a woodcut on the title page. No contemporary likeness of George is known, although he was said to be short-legged, dark complected, red haired, and squinting, although not necessarily from reliable sources. Georges mother, Anne, died July 1, 1580, and his father married Christian Widers (d. 1597 in St. Christopher le Stocks) on November 3, 1580. She became a nurse on the Hospital payroll, where she remained five years after James Peeles death, when she married Ralph Boswell. His siblings included Anne (d. Jan 10, 1568/9), Isabel, Judith (d. Apr. 16, 1582), and James (b. Jan 3, 1563/4). Anne married John Alford on May 14, 1565, and had one son, Robert (October 9, 1567- c. March 12, 1654/5). Judith married John Jackman on June 19, 1575, and had three children, Susan (b. June 3, 1576), William (April 30, 1577-July 1, 1577), and Sarah (d. May 24, 1578). On Feb. 5, 1568/9, Isabel married Mathew Shakespeare, with whom she had eight children. Many scholars believe that this was a cousin of William Shakespeare, but this has not been verified. James Peele also wrote the Ironmongers Pageants of 1566 and 1569, which may have led to Georges writing of two Lord Mayors pageants. How many years after her marriage to James Peele did Christian Widers die?\n\nChain-of-thought by code-davinci-002 A: According to the passage, Christian Widers died in 1597. James Peele died in 1585. 1597 - 1585 = 12. So the answer is 12.\n\nLeast-to-most by code-davinci-002 Q: When did Christian Widers marry James Peele? A: “She became a nurse on the Hospital payroll, where she remained five years after James Peeles death, when she married Ralph Boswell”. So the answer is 1580.\n\nQ: When did Christian Widers die? A: “She became a nurse on the Hospital payroll, where she remained five years after James Peeles death, when she married Ralph Boswell”. So the answer is 1597.\n\nQ: How many years after her marriage to James Peele did Christian Widers die? A: We know that Christian Widers married James Peele in 1580. We also know that Christian Widers died in 1597. So 1597 - 1580 = 17 years after her marriage to James Peele did Christian Widers die. So the answer is 17.\n\nAnalysis Chain-of-thought generated the wrong final answer because one fact it used, “James Peele died in 1585”, is unrelated.\n\n53\n\nPublished as a conference paper at ICLR 2023\n\n9.4.4 CASE 4\n\nQ: Among the other denominations there are approximately 93,000 Mormons in 253 congregations, 25,000 Jewish adherents in 21 synagogues, 12,000 Muslims in 39 masjids, 7,000 Buddhists in 34 temples, 7,000 Hindus in 17 temples, 2,500 Unitarian Universalism in 9 congregations, 2,000 Bahai in 17 temples, 5 Sikh temples, a Zoroastrian temple, a Jain temple and an uncounted number of neopagans. How many more Mormons were there compared to Muslims?\n\nChain-of-thought by code-davinci-002 A: According to the passage, there are 93,000 Mormons and 25,000 Muslims. 93,000 - 25,000 = 68,000. So the answer is 68,000.\n\nLeast-to-most by code-davinci-002 Q: How many Mormons were there? A: “93,000 Mormons”. So the answer is 93,000.\n\nQ: How many Muslims were there? A: “12,000 Muslims”. So the answer is 12,000.\n\nQ: How many more Mormons were there compared to Muslims? A: We know that there were 93,000 Mormons. We also know that there were 12,000 Muslims. So there were 93,000 - 12,000 = 81,000 more Mormons than Muslims. So the answer is 81,000.\n\nAnalysis Chain-of-thought failed because it got one intermediate fact (the number of Muslims) wrong.\n\n9.4.5 CASE 5\n\nQ: Killigrew lived in Hanworth in Middlesex and Falmouth in Cornwall. On 4 November 1566 Killigrew married in the church of St Peter Le Poer, London, Catherine, fourth daughter of Sir Anthony Cooke. He thus became Cecil’s brother-in-law. His wife died in 1583, and on 7 November 1590 he was married in the same church to Ja ́el de Peigne, a French Hugenot. She was naturalised in June 1601. After Henry’s death she remarried on 19 April 1617 George Downham, Bishop of Derry, and died c.1632. By his first wife, Killigrew had four daughters: How many years after here marriage to Killigrew did Catherine die?\n\nChain-of-thought by code-davinci-002 A: According to the passage, Catherine died in 1583, and Killigrew married Ja ́el de Peigne in 1590. 1590 - 1583 = 7. So the answer is 7.\n\nLeast-to-most by code-davinci-002 Q: When did Catherine marry Killigrew? A: “On 4 November 1566 Killigrew married in the church of St Peter Le Poer, London, Catherine”. So the answer is 4 November 1566.\n\nQ: When did Catherine die? A: “His wife died in 1583”. So the answer is 1583.\n\nQ: By his first wife, Killigrew had four daughters: How many years after here marriage to Killigrew did Catherine die? A: We know that Catherine married Killigrew in 1566. We also know that Catherine died in 1583. So 1583 - 1566 = 17 years after here marriage to Killigrew did Catherine die. So the answer is 17.\n\nAnalysis Chain-of-thought generated the wrong final answer because one fact it used, “Killigrew married Ja ́el de Peigne in 1590”, is unrelated.\n\n9.5 ERROR ANALYSIS: LEAST-TO-MOST PROMPTING\n\nWe randomly picked 20 failure cases, and found that out of those 20 failure cases:\n\n• 4 are due to wrong problem decomposition – the decomposed problems do not make sense,\n\nor there is no decomposition at all.\n\n• 13 are due to wrong problem solving – it gave the wrong answer for a decomposed problem.\n\n54\n\nPublished as a conference paper at ICLR 2023\n\n• 3 are due to wrong “ground truth” – the given label is wrong regardless whether the prediction is correct or not (although, the model predictions for the sampled 4 cases are all correct).\n\n9.5.1 EXAMPLE OF WRONG PROBLEM DECOMPOSITION\n\nIn the following example, the decomposed question is just a rephrase of the original question.\n\nQ: Then, in 1544, five French galleys under Polin, including the superb R ́eale, accompanied Barbarossa’s fleet, on a diplomatic mission to Suleiman. The French fleet accompanied Barbarossa during his attacks on the west coast of Italy on the way to Constantinople, as he laid waste to the cities of Porto Ercole, Giglio, Talamona, Lipari and took about 6,000 captives, but separated in Sicily from Barbarossa’s fleet to continue alone to the Ottoman capital. Jerˆome Maurand, a priest of Antibes who accompanied Polin and the Ottoman fleet in 1544, wrote a detailed account in Itin ́eraire d’Antibes `a Constantinople. They arrived in Constantinople on 10 August 1544 to meet with Suleiman and give him an account of the campaign. Polin was back to Toulon on 2 October 1544. On his way to Constantinople, how many cities did Polin laid waste to?\n\nA: To answer the question ”On his way to Constantinople, how many cities did Polin laid waste to?”, we need to know: ”How many cities did Polin laid waste to on his way to Constantinople?”.\n\n9.5.2 EXAMPLE OF WRONG PROBLEM SOLVING\n\nIn the following example, the answer to the decomposed question, “How many percent of people were from 2 or more races in 2000?” is wrong.\n\nAs of 2011 the 88,112 residents of Orem had a racial and ethnic composition of 89.3% White American, 0.9% black or African American, 0.9% Native Americans of the United States, 1.6% Asian American, 0.5% Pacific Islanders, 4% non-Hispanics reporting some other race, 2.9% two or more races reported and 14.8% Hispanic, as Orem has a large Mexican American community with other Latinos residing in the city. This contrasts with the census of 2000, which showed a racial makeup of 90.80% White (U.S. Census), 0.33% African American (U.S. Census), 0.73% Native American (U.S. Census), 1.45% Asian (U.S. Census), 0.86% Pacific Islander (U.S. Census), 3.64% from Race (United States Census), and 2.18% from two or more races. Hispanic (U.S. Census) or Latino (U.S. Census) of any race were 8.56% of the population. The 2000 Census counted 84,324 people, 23,382 households, and 19,079 families. The population density at that time was 4,572.6 people per square mile (1,765.6/km2). There were 24,166 housing units at an average density of 1,310.4 per square mile (506.0/km2). There were 23,382 households out of which 48.8% had children under the age of 18 living with them, 69.0% were Marriage living together, 9.5% had a female householder with no husband present, and 18.4% were non-families. 12.4% of all households were made up of individuals and 5.1% had someone living alone who was 65 years of age or older. The average household size was 3.57 and the average family size was 3.93.\n\nQ: How many percent of people were from 2 or more races in 2000? A: “3.64% from Race (United States Census), and 2.18% from two or more races”. So the answer is 3.64% + 2.18% = 5.82%.\n\nQ: How many percent of people were not from 2 or more races in 2000? A: We know that 5.82% were from 2 or more races. So 100% - 5.82% = 94.18% were not from 2 or more races. So the answer is 94.18%.\n\n9.5.3 EXAMPLE OF WRONG GIVEN LABEL\n\nIn the example below, the given label is 417, which is incorrect. The correct ground truth should be 418. The model gives the correct answer via least-to-most prompting.\n\nThere were 20,928 births in 2006. Of these, 19,757 (94.40% of the births, 95.19% of the population) were to Non-Hispanic Whites. There were 22 births to American Indians (0.11% of the births and 0.54% of the population), 177 births to Asians (0.85% of the births and 0.68% of the population), 219 births to Hispanics (1.05% of the births and 0.88% of the population) and 753 births to Blacks and others (3.60% of the births and 3.56% of the population).\n\n55\n\nPublished as a conference paper at ICLR 2023\n\nQ: How many births were there for Asians? A: “177 births to Asians”. So the answer is 177.\n\nQ: How many births were there for Native Americans? A: “22 births to American Indians”. So the answer is 22.\n\nQ: How many births were there for Hispanics? A: “219 births to Hispanics”. So the answer is 219.\n\nQ: How many total births were there for Asians, Native Americans and Hispanics? A: We know that there were 177 births for Asians. We also know that there were 22 births for Native Americans. We also know that there were 219 births for Hispanics. So 177 + 22 + 219 = 418 total births for Asians, Native Americans and Hispanics. So the answer is 418.\n\n10 GSM8K\n\n10.1 EXPERIMENT RESULTS: ONE-SHOT PROMPTS\n\nWe compare here the effectiveness on compositional generalization of least-to-most prompting vs. chain-of-thought prompting by constructing for each prompting method a simple prompt context that contains a single example that is solvable with just 2 reasoning steps. We then evaluate accuracy on examples that may involve larger numbers of reasoning steps. The same example is used for both prompting methods. For the least-to-most prompting prompt, we adopt a simplified approach in which the problem decomposition and solution stages are merged into a single pass, with just one follow-up request being made to the language model to solicit the final answer.\n\nThe accuracy (%) of the two prompting methods with the GPT-3 code-davinci-002 model, with breakdown by number of steps in the expected solution, are listed in Table 18.\n\nAccuracy numbers for all prompting methods are calculated after applying the same post-processing as described in Section 3.3 for DROP.\n\nWhile the least-to-most prompting accuracy is overall only moderately higher than that of chain-ofthought prompting, the accuracy breakdown by number of steps shows that least-to-most prompting significantly outperforms chain-of-thought prompting as the number of reasoning steps increases beyond what was illustrated in the prompt.\n\nAccuracy by Steps Least-to-Most (1-shot): aL Chain-of-Thought (1-shot): aC Accuracy change: (aL/aC) − 1\n\nAll 62.39 60.87 +2.49\n\n2 74.53 76.68 -2.80\n\n3 68.91 67.29 +2.40\n\n4 59.73 59.39 +0.58\n\n5+ 45.23 39.07 +15.77\n\nAccuracy (%) of a simple 1-shot\n\nleast-to-most prompt with the GPT-3 Table 18: code-davinci-002 model on GSM8K, compared to that of a corresponding chain-of-thought prompt, broken down by number of reasoning steps required in the expected solution. Examples with 3 or more reasoning steps would require generalizing to more steps than were shown in the demonstration example in the prompt (which contains just 2 steps).\n\n10.2 EXPERIMENT RESULTS: ENGINEERED PROMPTS\n\nWe compare here the overall accuracy of the above-reported “Chain-of-Thought (1-shot)” and “Least-to-Most (1-shot)” methods with alternative existing prompting methods, as well as with variants of chain-of-thought and least-to-most prompting in which the prompts were engineered using multiple in-domain examples taken from the GSM8K train set.\n\nThe evaluated prompting methods are as follows (see Appendices 10.3 and 10.4 for the exact prompt contexts):\n\n• Zero-Shot: Simple zero-shot prompting. • Standard prompting: Standard few-shot prompting, using the same 4 examples as in the\n\n“problem solving” prompt context of “Least-to-Most (best)”.\n\n56\n\nPublished as a conference paper at ICLR 2023\n\n• Chain-of-Thought (original): Chain-of-thought prompting, using the original 8-shot\n\nprompt context described in Wei et al. (2022).\n\n• Chain-of-Thought (1-shot): The simple 1-shot chain-of-thought prompting method de-\n\nscribed in Appendix 10.1 above.\n\n• Least-to-Most (1-shot): The simple 1-shot least-to-most prompting method described in\n\nAppendix 10.1 above.\n\n• Chain-of-Thought (best): Chain-of-thought prompting, using the same 4 examples as in the “problem solving” prompt context of “Least-to-Most (best)”, with the solutions adjusted to chain-of-thought format.\n\n• Least-to-Most (best): Least-to-most prompting using separate prompts for the “problem decomposition” and “problem solution” steps and with multiple examples selected from the GSM8K train set. The “problem decomposition” prompt context contains 7 examples, with hand-crafted problem decompositions. The “problem solution” prompt contains 4 examples, with hand-crafted solutions for each step.\n\nThe accuracies (%) of these prompting methods with the GPT-3 code-davinci-002 model are listed in Table 19.\n\nIt can be noted first that, although the “Chain-of-Thought (1-shot)” prompt context is considerably simpler than the 8-shot prompt context proposed in the original chain-of-thought paper, we find the overall accuracy achieved to be quite close (60.87% for the 1-shot prompt, compared to 61.18% for the original 8-shot prompt). This suggests that “Chain-of-Thought (1-shot)” is indeed a reasonable chain-of-thought baseline to analyze in comparison to “Least-to-Most (1-shot)”.\n\nFurther, while we find the proposed 1-shot prompt attractive due to its simplicity and lack of datasetspecific content, we do find that further improvements in overall accuracy of both chain-of-thought and least-to-most prompting can be achieved if additional prompt engineering is applied, using multiple in-domain examples of arbitrary complexity from the GSM8K train set, as seen in the accuracies of “Chain-of-Thought (best)” and “Least-to-Most (best)”. We do not observe improvement in overall accuracy from least-to-most prompting compared to chain-of-thought prompting in this setting, where most of the test questions do not require more steps more steps to solve than the demonstration examples.\n\nPrompting method Zero-Shot Standard prompting Chain-of-Thought (original) Chain-of-Thought (1-shot) Least-to-Most (1-shot) Chain-of-Thought (best) Least-to-Most (best)\n\nAccuracy 16.38 17.063 61.18 60.88 62.39 68.613 68.01\n\nTable 19: Accuracies (%) of various prompting methods with the GPT-3 code-davinci-002 model on GSM8K.\n\n10.3 PROMPT CONTEXTS: ONE-SHOT PROMPTS\n\nWe include here the prompt contexts used in the experiments reported in Appendix 10.1.\n\nIn this section and in the following one, the placeholder “{question}” indicates the place where the original question is to be inserted, in cases where the format would not be obvious (e.g., where\n\n3Note that in two of the prompt contexts used in an earlier pre-print of this paper, one of the examples had contained a mistake, which has been corrected in this version of the paper. Specifically, in the “Chain-ofThought (best)” context, the last example had mistakenly omitted the final step, such that it ended incorrectly with “The answer is 17.” rather than “which means that sandy will get $20 - $17 = $3 as change. The answer is 3.” Similarly, the “Standard prompting” context incorrectly stated the answer as “17” rather than “3” for this example. The earlier versions of the prompts yielded the following accuracies: Standard prompting = 18.65, Chain-of-Thought (best) = 62.77.\n\n57\n\nPublished as a conference paper at ICLR 2023\n\ninstead of simply ending the prompt with “A:”, we include some additional prompt text like “A: The answer is”).\n\nIn the case of “Least-to-Most (1-shot)”, the prompt prefix for the initial request ends in “Let’s break down this problem:”. We then append to that prompt the initial reply that was received from the language model, followed by a newline and the string “The answer is:”, which we then use as the prompt in a second request, whose reply we treat as the final answer.\n\n10.3.1 CHAIN-OF-THOUGHT (1-SHOT)\n\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12.\n\n10.3.2 LEAST-TO-MOST (1-SHOT)\n\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together? 1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. 2. Elsa and Anna have 5 + 7 = 12 apples together.\n\nQ: {question} A: Let’s break down this problem: —– The answer is:\n\n10.4 PROMPT CONTEXTS: ENGINEERED PROMPTS\n\nWe include here the additional prompt templates used in the experiments reported in Appendix 10.2, with the exception of “Chain-of-Thought (original)“, which is the same as the “Prompt for Math Word Problems” reported in Appendix G of Wei et al. (2022).\n\n10.4.1 ZERO-SHOT\n\nQ: {question} A: The answer is\n\n10.4.2 STANDARD PROMPTING: 4 EXAMPLES\n\nQ: Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody? A: The answer is 32.\n\nQ: Carla bought 2 bags of mini peanut butter cups on clearance. Each bag was $6.00 but was 75% off. How much did she spend on 2 bags of candy? A: The answer is 3.\n\nQ: If Pam is currently twice as young as Rena is, and in 10 years Rena will be 5 years older than her, how old is Pam now? A: The answer is 5.\n\nQ: Cappuccinos cost $2, iced teas cost $3, cafe lattes cost $1.5 and espressos cost $1 each. Sandy orders some drinks for herself and some friends. She orders three cappuccinos, two iced teas, two cafe lattes, and two espressos. How much change does she receive back for a twenty-dollar bill? A: The answer is 3.\n\n58\n\nPublished as a conference paper at ICLR 2023\n\n10.4.3 CHAIN-OF-THOUGHT (BEST): 4 EXAMPLES\n\nQ: Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody? A: We were told that Mohamed is currently twice 30 years old, so he is currently 30 * 2 = 60 years old. That means that four years ago he must have been 60 - 4 = 56 years old. Four years ago, Kody was half as old as Mohamed, so Kody must have been 56 / 2 = 28 years old then. Since Kody was 28 years old four years ago, she must now be 28 + 4 = 32 years old. The answer is 32.\n\nQ: Carla bought 2 bags of mini peanut butter cups on clearance. Each bag was $6.00 but was 75% off. How much did she spend on 2 bags of candy? A: Each bag was $6.00 but was 75% off. So each bag cost $6.00 * (1 - 0.75) = $6.00 * 0.25 = $1.50. Carla bought 2 bags. So she spent $1.50 * 2 = $3.00. The answer is 3.\n\nQ: If Pam is currently twice as young as Rena is, and in 10 years Rena will be 5 years older than her, how old is Pam now? A: Since Rena will be 5 years older than Pam in 10 years, she must be 5 years older than Pam now as well. If Pam is currently twice as young as Rena, that means that Rena is currently twice as old as Pam is. So if P stands for Pam’s age now and R stands for Rena’s age now, then we know that R = 2 * P And since Rena is 5 years older than Pam now, we know that R = P + 5. By substitution, we have P + 5 = 2 * P, which means that P = 5. The answer is 5.\n\nQ: Cappuccinos cost $2, iced teas cost $3, cafe lattes cost $1.5 and espressos cost $1 each. Sandy orders some drinks for herself and some friends. She orders three cappuccinos, two iced teas, two cafe lattes, and two espressos. How much change does she receive back for a twenty-dollar bill? A: Sandy ordered three cappuccinos, which cost $2 each, so she spent $2 * 3 = $6 on cappuccinos. She ordered two iced teas, which cost $3 each, so she spent $3 * 2 = $6 dollars on ice teas. She ordered two cafe lattes, which cost $1.5 each, so she spent $1.5 * 2 = $3 on cafe lattes. She ordered two espressos, which cost $1 each, so she spent $1 * 2 = $2 on espressos. So altogether, Sandy spent $6 + $6 + $3 + $2 = $17 on drinks, which means that sandy will get $20 - $17 = $3 as change. The answer is 3.\n\n10.4.4 LEAST-TO-MOST (BEST) I - PROBLEM DECOMPOSITION: 7 EXAMPLES\n\nQ: Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody? A: To answer the question “How old is Kody?”, we need to know: “How old is Mohamed?”, “How old was Mohamed four years ago?”, “How old was Kody four years ago?”.\n\nQ: If Pam is currently twice as young as Rena is, and in 10 years Rena will be 5 years older than her, how old is Pam now? A: To answer the question “How old is Pam now?”, we need to know: “How much older is Rena than Pam currently?”.\n\nQ: As a freelancer, Baylor is paid for every finished work of a client he does on a freelance marketplace. Currently, he has $4000 on his dashboard from previous work done. He is currently working for three clients, with the first client paying him half the amount of money he currently has on his dashboard once the job is done. The second client will pay him 2/5 times more money than the first client once Baylor finishes his work. The third client will pay him twice the amount of money the first and second clients pay him together once he finishes the job. How much money will Baylor have in his dashboard after all the clients pay him for his work? A: To answer the question “How much money will Baylor have in his dashboard after all the clients pay him for his work?”, we need to know: “How much will Baylor’s first client pay him for his work?”, “How much more will Baylor’s second client pay him for his work compared to the first client?”, “How much will Baylor’s second client pay him for his work?”, “How much will the first and second clients pay him together once he finishes the job?”, “How much will Baylor’s third client pay him for his work?”, “How much money will all the clients pay Baylor for his work?”.\n\n59\n\nPublished as a conference paper at ICLR 2023\n\nQ: Cappuccinos cost $2, iced teas cost $3, cafe lattes cost $1.5 and espressos cost $1 each. Sandy orders some drinks for herself and some friends. She orders three cappuccinos, two iced teas, two cafe lattes, and two espressos. How much change does she receive back for a twenty-dollar bill? A: To answer the question “How much change does she receive back for a twenty-dollar bill?”, we need to know: “How much did the cappuccinos cost in total?”, “How much did the iced teas cost in total?”, “How much did the cafe lattes cost in total?”, “How much did the espressos cost in total?”, “How much did the drinks cost in total?”.\n\nQ: Betty & Paige are raising money for their kids’ little league team by hosting a bake sale. Betty has baked 4 dozen chocolate chip cookies, 6 dozen oatmeal raisin cookies and 2 dozen regular brownies. Paige baked 6 dozen sugar cookies, 3 dozen blondies and 5 dozen cream cheese swirled brownies. If they sell the cookies for $1.00 apiece and the blondies/brownies at $2.00 apiece, how much money will they raise? A: To answer the question “How much money will they raise?”, we need to know: “How many dozen cookies did they bake (not including blondies/brownies)?”, “How many cookies did they bake (not including blondies/brownies)?”, “How many dozen blondies/brownies did they bake (not including cookies)?”, “How many blondies/brownies did they bake (not including cookies)?”, “How much money will they raise from the cookies (not including blondies/brownies)?”, “How much money will they raise from the blondies/brownies (not including cookies)?”.\n\nQ: On a moonless night, three fireflies danced in the evening breeze. They were joined by four less than a dozen more fireflies, before two of the fireflies flew away. How many fireflies remained? A: To answer the question “How many fireflies remained?”, we need to know: “How many fireflies joined?”.\n\nQ: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity? A: To answer the question “How many popsicle sticks can they use for their Art class activity?”, we need to know: “How many popsicle sticks does Sid have?”, “How many popsicle sticks does Sam have?”.\n\n10.4.5 LEAST-TO-MOST (BEST) II - PROBLEM SOLVING: 4 EXAMPLES\n\nFour years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody?\n\nQ: How old was Mohamed four years ago? A: We were told that Mohamed is currently twice 30 years old, so he is currently 30 * 2 = 60 years old. That means that four years ago he must have been 60 - 4 = 56 years old. The answer is 56.\n\nQ: How old is Kody? A: Four years ago, Kody was half as old as Mohamed, so Kody must have been 56 / 2 = 28 years old then. Since Kody was 28 years old four years ago, she must now be 28 + 4 = 32 years old. The answer is 32.\n\nCarla bought 2 bags of mini peanut butter cups on clearance. Each bag was $6.00 but was 75% off. How much did she spend on 2 bags of candy?\n\nQ: How much did she spend on 2 bags of candy? A: Each bag was $6.00 but was 75% off. So each bag cost $6.00 * (1 - 0.75) = $6.00 * 0.25 = $1.50. Carla bought 2 bags. So she spent $1.50 * 2 = $3.00. The answer is 3.\n\nIf Pam is currently twice as young as Rena is, and in 10 years Rena will be 5 years older than her, how old is Pam now?\n\n60\n\nPublished as a conference paper at ICLR 2023\n\nQ: How much older is Rena than Pam currently? A: Since Rena will be 5 years older than Pam in 10 years, she must be 5 years older than Pam now as well. The answer is 5.\n\nQ: How old is Pam now? A: If Pam is currently twice as young as Rena, that means that Rena is currently twice as old as Pam is. So if P stands for Pam’s age now and R stands for Rena’s age now, then we know that R = 2 * P And since Rena is 5 years older than Pam now, we know that R = P + 5. By substitution, we have P + 5 = 2 * P, which means that P = 5. The answer is 5.\n\nCappuccinos cost $2, iced teas cost $3, cafe lattes cost $1.5 and espressos cost $1 each. Sandy orders some drinks for herself and some friends. She orders three cappuccinos, two iced teas, two cafe lattes, and two espressos. How much change does she receive back for a twenty-dollar bill?\n\nQ: How much did Sandy spend on drinks? A: Sandy ordered three cappuccinos, which cost $2 each, so she spent $2 * 3 = $6 on cappuccinos. She ordered two iced teas, which cost $3 each, so she spent $3 * 2 = $6 dollars on ice teas. She ordered two cafe lattes, which cost $1.5 each, so she spent $1.5 * 2 = $3 on cafe lattes. She ordered two espressos, which cost $1 each, so she spent $1 * 2 = $2 on espressos. So altogether, Sandy spent $6 + $6 + $3 + $2 = $17 on drinks. The answer is 17.\n\n61",
    "reference": "# Summary Of The Paper\n\nThe authors propose a new prompting method, called \"Least To Most\" prompting. Building upon the success of the \"chain-of-thought\" prompting method, the authors propose to split the prompting process into two stages: problem reduction, and problem solving. The former focusing on decomposing the task at hand into its constituent parts, the latter corresponding to solving each subproblem and combining the results into the final answer. The authors show that their model outperforms \"chain of thought\" prompting on a number of tasks, including both simulated tasks like  last letter concatenation and the SCAN dataset and on more realistic mathematical reasoning problems (Drop and GSM8k).\n\n# Strength And Weaknesses\n\nStrengths:\n- The problem of teaching language models new skills \"on the fly\" is extremely relevant\n- The paper has numerous illustrations, which helps to understand the approach and model behavior\n- The approach is novel and original\n\nWeaknesses:\n- There are a number of issues with experimental design. I detail those in the \"quality\" section of the review.\n- In terms of reproducibility, some elements of the approach are not very clearly reported, specifically, in the first two experiments, the authors say that either a script or simple prompting can be used (for decomposition in experiment 1, and for expanding python expressions), and it's not specified what was actually used.\n\nHere is one of the relevant quotes: \n\"To generate the final results, we either run a postprocessing script or prompt language models to expand the Python expressions. It is straightforward to teach language models to expand Python expressions via a few demonstrations.\nHowever, I can not find a more detailed description nor the results on the performance of these straightforward expansions. While straightforward from the prompting standpoint, it might still result in less than perfect accuracy & providing exact prompts is crucial for replication.\n\n- While thorough, the abundance of tables and model input/output printouts is, at times, getting in the way of readability.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n** Clarity **\n\nThe paper is generally clearly written and is a pleasure to read. There are some minor issues (which I list below), but they did not strongly affect my overall assessment of the paper.\n\nFirst, the phrase at the end of page 5 (\"We find that the ...) is continued amid a number of tables on page 6 (\"... solving rate remains the same\"), and then almost immediately followed by another table. It'd be better to move text or tables around so that it's a bit easier to read.\n\nAppendices are extremely thorough (which is a plus), but they also feature a lot of material that is difficult to navigate. It may, potentially, be better to move some of the additional results and demonstrations into supplementary materials.\n\nThe abundance of model input/output printouts in the main body of the paper is helpful, but I think that some of them may be condensed and explained verbally in text.\n\nOverall, the clarity is up to the standards of the ICLR conference.\n\n** Quality **\n\nThe experiments cover a range of substantially different tasks, both simulated and realistic. There are a few issues with experiment design, however, some of which I find substantial.\n\nExperiment 1 (symbolic manipulation):\n- The prompts differ not only in the recursive part ('So, “think, machine” outputs\n“ke”'), but also in the examples themselves. The chain-of-thought prompts in table 5 use completely different sets of words for length 2 and length 3 prompts. Because of that, it's impossible to tell whether it's the selection of words or the inclusion of the \"reduction\"/\"recursive\" phrase that is key for the method to work.\n- \"Such a trivial reduction can be done straightforwardly by a manually written script or prompting language models with several\ndemonstrations.\" regarding breaking a multi-word problem in to smaller constituents. However, if done with a script, the comparison with the regular \"chain-of-thought\" method is unfair. At the same time, if done using prompting, while straightforward, it can still break down on longer prompts. As far as I can tell, no detail is given in the paper on the \"straightforward\" prompting, and so the generalization performance of such prompting on longer sequences can not be assessed. Therefore, it may be that the improvements we see are due to better decomposition that may be delegated to an external script in the least-to-most prompting case and that has to be done internally by the language model in the case of \"chain-of-thought\" prompting.\n\nBecause of these concerns, the results from the first experiment can not be fully interpreted.\n\nExperiment 2 (SCAN):\nThe description of the experimental procedure is, again, partially unclear:\n\"To generate the final results, we either run a postprocessing script or prompt language models to expand the Python expressions. It is straightforward to teach language models to expand Python expressions via a few demonstrations.\"\nIt's unclear which method was used and whether there was a difference in performance. Again, while prompting may be straightforward,\nlanguage models are known to fail in unexpected ways on longer sequences. If scripts were used as part of Python expression expansion,\nit makes the comparison between the models unfair.\n\nWhat makes me especially worried is that in the SCAN experiment, it's not clear, why having Python notation such as \" * 2\" would be better than having verbal notation (such as \"twice\"), so it makes me worried that the benefit may be in reducing the errors during the decoding stage.\n\nExperiment 3:\nThis issue applies to different experiments too, but is most evident in experiment 3.\nI believe that for a fair comparison between prompting methods it is reasonable to normalize by the amount of information given. Least-to-most prompting essentially gives more prompts than the \"chain of thought\" one. They are partially redundant, but, nevertheless, in a way, the least-to-most prompting can be benefitting from the sheer number of examples (a solution to each subproblem may be seen as an example). \n\nFrom the practical standpoint, it is also reasonable to normalize by the amount of instruction provided to a language model, and I don't think it is 100% fair to say that \"least-to-most prompting\" and \"chain-of-thought\" prompting receive equivalent amount of instruction because both are two-shot, when each \"least-to-most\" prompting \"shot\" takes twice or more space in terms of volume of text.\n\nWhile not all of these issues are crucial, I believe that overall, they create an inconsistent impression, and because of that, I feel that the quality of the contribution is marginally below the requirements of the ICLR conference.\n\n** Novelty **\n\nThe work is substantially novel and original, and expand upon the \"chain-of-thought\" prompting in a meaningful way.\n\n** Reproducibility **\n\nI have mentioned some of the reproducibility issues in the \"quality\" section. The authors also do not provide the code, which could have helped to mitigate the issue.\n\n** Other suggestions **\n\nRegarding the last sentence on page 4 -- \"These look somewhat reminiscent of careless mistakes by humans\". I find that the paper will benefit if this sentence is removed.\n\nFirst, I disagree that even a careless human would end up with \"wsgss\" when concatenating \"wsg\" and \"s\". Second, the subtle implication that the model is somehow humanlike in its behaviour is very ambitious. When it's based on such loose grounds, it gives a very speculative impression, which overall damages the perceived credibility and rigour of the paper.\n\nThis is a minor suggestion that did not affect my overall assessment.\n\n# Summary Of The Review\n\nOverall, I enjoyed reading the paper, and I believe that it has a great potential. I am very torn when it comes to its assessment, since while many of the issues I've mentioned are minor, they accumulate to give the paper an inconsistent impression. The experimental section issues are the most important. Experiment 3 is, overall, the most convincing, but given that it also has some methodological concerns, and, given the issues with experiment 1 and 2, I lean towards a weak rejection. I will, however, stay open to revise my assessment based on other reviews and authors' rebuttal.\n\n** UPD **\nMany of my concerns were properly addressed, therefore I increase the score, pushing it above the acceptance threshold. I would have given it a 7, but the interface does not allow it.\n\nThat being said, I believe that additional results and clarifications dramatically improve the quality of the paper, and that it is now above the acceptance threshold and is of interest to a large portion of the ICLR community. In fact, I hope that the paper is accepted.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nThe authors reference additional experiments on a 540 billion parameter language model in the appendix. It's not central to the contribution, and the added value of this comparison is dubious.\n\nThe reason I mention it is that I believe that it might place additional pressure on reviewers, since it is well known that there is only one such model (PaLM) and only one research group with access to it (Google Research)."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMIXPRO: DATA AUGMENTATION WITH MASKMIX AND PROGRESSIVE ATTENTION LABELING FOR VISION TRANSFORMER\n\nQihao Zhao1, Yangyu Huang2 , Wei Hu1∗, Fan Zhang1†, Jun Liu3 1Beijing University of Chemical Technology, China 2Microsoft Research Asia, China 3Singapore University of Technology and Design, Singapore\n\nABSTRACT\n\nThe recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for vision transformer. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViTbased models at scales on ImageNet classification (73.8% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks.\n\n1\n\nINTRODUCTION\n\nTransformers (Vaswani et al., 2017) have revolutionized the natural language processing (NLP) field and have recently inspired the emergence of transformer-style architectures in the computer vision (CV) field, such as Vision Transformer (ViT) (Dosovitskiy et al., 2020). These methods design with competitive results in numerous CV tasks like image classification (Touvron et al., 2021a; Yuan et al., 2021; Wang et al., 2021; Liu et al., 2021; Touvron et al., 2021b; Ali et al., 2021), object detection (Fang et al., 2021; Dai et al., 2021; Carion et al., 2020; Zhu et al., 2020) and image segmentation (Strudel et al., 2021; Wang et al., 2021; Liu et al., 2021). Previous research has discovered that ViT-based networks are difficult to optimize and can easily overfit to training data with many images (Russakovsky et al., 2015), resulting in a significant generalization gap in the test data. To improve the generalization and robustness of the model, the recent works (Dosovitskiy et al., 2020; Touvron et al., 2021a; Yuan et al., 2021; Wang et al., 2021; Liu et al., 2021; Touvron et al., 2021b; Ali et al., 2021) employ data augmentation (Zhang et al., 2017) and regularization techniques (Szegedy et al., 2016) during training. Among them, the mixup-base methods such as Mixup (Zhang et al., 2017), CutMix (Yun et al., 2019) and TransMix (Chen et al., 2021) are implemented to improve the generalization and robustness of ViT-based networks. For the CutMix, patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. Furthermore, TransMix based on CutMix considers that not all pixels are created equal.\n\n∗Corresponding author. huwei@buct.edu.cn †Fan Zhang is with the College of Information Science and Technology and the Interdisciplinary Research\n\nCenter for Artificial Intelligence, Beijing University of Chemical Technology, Beijing 100029.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Comparison between TransMix (Chen et al., 2021) (a) and our proposed MixPro (b). 1) For image space, TransMix shares the same cropped region with CutMix (Yun et al., 2019), which results in patches containing different regions from the two images (patches colored red). Differently, as shown on the right of the figure, MixPro mixes patches using a patch-like mask. The size of the mask patches is the multiple of the image patches. This enables each patch of the mixed image to come from only one image (patches colored yellow and blue). 2) For label space, TransMix computes λ by λattn. In contrast, we propose progressive attention labeling that dynamically re-weights λarea and λattn using a progressive factor (α).\n\nThen it exploits ViT’s attention map to re-assign the weight to the mixed label rather than applying the proportion of the cropped image area.\n\nNevertheless, TransMix (Chen et al., 2021) has the following drawbacks for vision transformer: (1) TransMix and CutMix share the same region-level cropped images as input. However, ViT-based models naturally have global receptive fields from self-attention (Dosovitskiy et al., 2020), so the region-based mixed images may provide insufficient image contents. (More details are in Sec. A.1.) (2) As shown in Fig. 1 (a)TransMix provides cropped patches with sharp rectangular borders that are clearly distinguishable from the background (viewed as red patches). The ViT-based models may naturally be curious about the cropped patch and then pay attention to that patch, resulting in a basic weight of attention regardless of whether the patch contains useful information (Chen et al., 2021). (3) TransMix employs attention map to re-weight the confidence for the mixed targets. However, attention maps may not always be reliable during the training process. For example, at the beginning of the training, the model has no representation capability, and the attention maps gained are unreliable. In addition, it is possible to obtain difficult samples using massive data augmentation strategies, and the attention map is also unreliable. At this point, reassigning the mixed labels utilizing a low-confidence attention map will generate noisy mixed labels.\n\nTo this end, we propose a novel data augmentation method, MixPro, to tackle the above issues from the perspective of image space and label space, respectively. Our approach is presented in Fig. 1 (b). In detail, from the perspective of image space, we designed MaskMix, which is inspired by the mask strategy of MAE (He et al., 2022). Our MaskMix replaces the masked patches of one image with visible patches of another image to create a mixed image. In particular, the scale of each mask patch is adjustable and is a multiple of the image patch size. In this way, each image patch comes from only one image (viewed as yellow and blue patches in the figure). In addition, the mask decomposition of pictures can take into account both region and global contents. From the perspective of label space, we designed Progressive Attention Labeling (PAL), which utilizes a progressive factor (α) to dynamically re-weight the attention weight of the mixed attention label. The progressive factor (α) provides an indirect measure of the confidence of the attention map for the mixed sample, to trade-off the attention proportional weight (λattn) and the area proportional weight (λarea). In conclusion, we combine MaskMix and Progressive Attention Labeling, namely MixPro, as our data augmentation strategy to improve the generalization and robustness of ViT-based models.\n\nIn experiments, we demonstrate extensive evaluations of MixPro on various ViT-based models and tasks. MixPro exhibits greater performance gains than TransMix for all listed ViT-based models. Notably, MixPro can further bring an improvement of 0.9% for DeiT-S. Moreover, we demonstrate that if the model is first pretrained with MixPro on ImageNet, the superiority can be further transferred onto downstream tasks including object detection, instance segmentation, and semantic segmentation.\n\n2\n\nImage SpaceLabel Space(a) TransMix(b) MixProλ= λAttnλ=α·λAttn+ ( 1 -α) ·λareaInputPublished as a conference paper at ICLR 2023\n\nIn terms of robustness, compared to TransMix, MixPro also shows stronger robustness on three different benchmarks.\n\nOverall, we summarize our contributions as follows:\n\n• We propose a new data augmentation method, MixPro, to address the shortcomings of\n\nTransMix from the perspective of image space and label space, respectively.\n\n• From the perspective of image space, MixPro ensures that each image patch comes from only one image and contains more global contents. From the perspective of label space, MixPro utilizes a progressive factor to dynamically re-weight the attention weight of the mixed attention label.\n\n• In experiments, we demonstrate extensive evaluations of MixPro on various ViT-based models and downstream tasks. It boosts Deit-T achieving 73.8% and Deit-S achieving 81.3% on ImageNet-1K. Furthermore, compared to TransMix, MixPro also shows stronger robustness on three different benches.\n\n2 RELATED WORK\n\nVision Transformers (ViTs). Transformers were initially proposed for sequence models such as machine translation (Vaswani et al., 2017). Inspired by the success of transformers in NLP tasks, Vision Transformer (ViT) (Dosovitskiy et al., 2020) attempted to apply transformers for image classification by treating an image as a sequence of patches with excellent results compared to even state-of-the-art convolutional networks. DeiT (Touvron et al., 2021a) further extended ViT using a novel distillation approach and a powerful training recipe. Based on the success of ViT, numerous ViT-based models have emerged (Yuan et al., 2021; Wang et al., 2021; Liu et al., 2021; Touvron et al., 2021b; Ali et al., 2021; Yang et al., 2021). PVT (Wang et al., 2021) developed a progressive shrinking pyramid and a spatial-reduction attention layer to obtain high-resolution and multi-scale feature maps under limited computation/memory resources. XCIT (Ali et al., 2021) built their models with cross-covariance attention as its core component and demonstrate the effectiveness and generality of our models on various computer vision tasks. Swin (Liu et al., 2021) proposed the shifted window-based self-attention and showed the model was effective and efficient on vision problems. These ViT-based models have achieved good results not only for classification tasks (Deng et al., 2009), but also for object detection (Fang et al., 2021; Dai et al., 2021; Carion et al., 2020; Zhu et al., 2020) and image segmentation (Strudel et al., 2021; Wang et al., 2021; Liu et al., 2021) tasks. However, these ViT-based models rely on MixUp-based data augmentation to enhance generalization in case of insufficient data.\n\nMixUp-based data augmentation. Mixup (Zhang et al., 2017) was the first work to propose interpolation of two images and their labels to augment the training data. Subsequent variants of the Mixup appeared (Yun et al., 2019; Chen et al., 2021; Verma et al., 2019; Baek et al., 2021; Cascante-Bonilla et al., 2021; Kim et al., 2020; Walawalkar et al., 2020; Uddin et al., 2020). They can be mainly divided into two groups: global image mixture (e.g. Manifold Mixup (Verma et al., 2019), GridMix (Baek et al., 2021), TokenMix (Liu et al., 2022), and PatchMix (Cascante-Bonilla et al., 2021)), and local/region image mixture (e.g.CutMix (Zhang et al., 2017), Puzzle-Mix (Kim et al., 2020), Attentive-CutMix (Walawalkar et al., 2020), SaliencyMix (Uddin et al., 2020), and MixToken (Jiang et al., 2021)). Manifold Mixup (Verma et al., 2019) proposed to train neural networks on interpolations of hidden representations. GridMix (Baek et al., 2021) was composed of two local constraints: local data augmentation by grid-based image mixing and local patch mapping constrained by patch-level labels. In CutMix (Yun et al., 2019), patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. AttentiveCutMix (Walawalkar et al., 2020) enhanced CutMix by choosing the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. MixToken (Jiang et al., 2021) is a modified version of CutMix operating on the tokens after patch embedding. Among them, MixUp and CutMix are most successful in improving the performance of ViTs (Touvron et al., 2021a). Furthermore, TransMix (Chen et al., 2021) based on CutMix continues to improve the generalization and robustness of ViT-based models by reassigning the ground truth labels with transformer attention guidance. TokenMix (Liu et al., 2022) utilizes a pre-trained teacher model to generate attention maps for guiding the mixed labels.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nAll in all, our proposed MixPro differs significantly from the above MixUp-based methods. First of all, CutMix, MixToken and TransMix focus on region contents, whereas MixPro mixes images with adjustable mask patches and may contain more global contents. Secondly, TransMix mixes image patches may generate noisy attention maps, while MixPro generates clean attention maps for mixing labels. The last and most important one is that, from the viewpoint of label space, TransMix neglects the fact that the model attention map during training is not always reliable. MixPro solves this issue by using progressive factors to dynamically recalculate the attention weight of the mixed attention label without a pre-trained model to generate it.\n\n3 METHOD\n\n3.1 BACKGROUND\n\nMulti-head self-Attention. Multi-head self-Attention as introduced by ViTs (Dosovitskiy et al., 2020), firstly divides and embeds an image x ∈ RW ×H×3 to patch tokens xpatches ∈ RN ×d, where N is the number of tokens, each of dimensionality d. It aggregates the global information by a class token xcls ∈ R1×d. Then ViTs operate on the patch embedding z = [xcls, xpatches] ∈ R(1+N )×d. Given a Transformer with h attention heads and input patch embedding z, the class attention for each head can be formulated as:\n\nq = xcls · Wq,\n\nk = z · Wk, A′ = Sof tmax(q · k/(cid:112)d/h),\n\nA = {A′\n\n0,i|i ∈ [1, N ]},\n\n(1)\n\n(2)\n\n(3)\n\n(4)\n\nwhere W are weight matrices for q and k . A ∈ [0, 1]N is the attention map from the class token to the image patch tokens. It is summarized as which patches are most valuable for the final classifier. When there are multiple heads in the attention, we obtain them by simply averaging over all heads of the attention.\n\nTransMix. TransMix (Chen et al., 2021) calculates λattn (the proportion for mixing two labels) with the attention map A and a binary mask M ∈ {0, 1}W ×H from CutMix (Yun et al., 2019):\n\nλattn = A· ↓ (M),\n\n(5)\n\nwhere ↓ (· ) denotes the nearest-neighbor interpolation downsampling that can transform the original M from H × W into N pixels.\n\n3.2 MIXPRO\n\nOur approach, MixPro, consists of MaskMix and progressive attention labeling to improve the performance and robustness of ViT-based models from the viewpoint of image space and label space, respectively. Next, we will describe these two methods in detail. MaskMix. Let x ∈ RW ×H×C denote a training image and let y be its corresponding label. ViT (Dosovitskiy et al., 2020) regularly partitioned a high-resolution image x into N patches of a fixed size of Pimage × Pimage pixels (N = W/Pimage × H/Pimage) and then embeds these to tokens. In our method, we first divide the grid mask M ∈ {0, 1}W ×H into S = W/Pmask × H/Pmask patches, resulting in each mask patch region of size Pmask × Pmask. In particular, to make each image patch come from only one image, we ensure that the size of mask patches (Pmask) is a multiple of the image patch size (Pimage). Then we select ⌊S · τ ⌋ regions to mask, where\n\nand the value in the selected mask region is set to 1. In all our experiments, we follow CutMix (Yun et al., 2019) set β to 1, the τ is sampled from the uniform beta distribution Beta(1,1). With the mask\n\nτ = Beta(β, β),\n\n(6)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nM, for samples xi, xj and their corresponding labels yi, yj, we utilize the mask M ∈ {0, 1}W ×H to generate their mixing samples ((cid:101)x, (cid:101)y) :\n\n(cid:101)x = M ⊙ xi + (1 − M) ⊙ xj, (cid:101)y = λarea ⊙ yi + (1 − λarea) ⊙ yj,\n\n(7)\n\nwhere λarea = (cid:80)W i=1 M(i, j)/(W × H), binary mask filled with ones is represent as 1 and ⊙ is element-wise multiplication. In this way, as illustrated in Fig.1 (b), image patches are aligned with mask patches when mixing images to ensure that each image patch comes from only one image.\n\n(cid:80)H\n\ni=1\n\nProgressive Attention Labeling. However, the attention map A may not always be accurate for each mixed sample. At the early stage of the training, models have poor representational ability, and the attention map A may not be reliable for some difficult mixed samples. At this point, the mixed labels calculated with λattn would limit the performance of ViT.\n\nSince blindly using attention maps to reassign mixed labels is not reliable. The focus is on designing a progressive factor (α), which could provide an indirect measure of the confidence of the attention map for the mixed sample, to trade-off the attention proportional weight (λattn) and the area proportional weight (λarea). For neural networks with ground-truth distributions, cross-entropy measures the epistemic uncertainty of the model (Kendall & Gal, 2017). The more certain the model is, the more reliable the attention map is obtained. There, we employ the cosine similarity of the model output to the ground-truth labels as a proxy of cross-entropy to measure whether a mixed sample can obtain a high-confidence attention map. We calculate the cosine similarity, i.e.,\n\nd(p, (cid:101)y) =\n\np · (cid:101)y⊤ ||p|| · ||(cid:101)y||\n\n,\n\n(8)\n\nwhere p is the model’s output softmax probability. Since both p and (cid:101)y are non-negative vectors, the range of d is ∈ [0, 1]. Ideally, the cosine similarity d closer to 1, the samples better the network fits the mixed sample and it can also produce a high-confidence attention map. Therefore, we use the cosine similarity as our progressive factor α,\n\nand the λ written as:\n\nλ = α · λattn + (1 − α) · λarea,\n\n(9)\n\nwhere λ is the proportion to instead of λarea in Eq. (7) for mixing two labels. So far, we propose Progressive Attention Labeling (PAL), which employs the progressive factor (α) to dynamically re-weight the attention weight of the mixed attention label.\n\n3.3 DISCUSSION\n\nFigure 2: Visualization of progressive factor (α) and generalization analysis of our MixPro and TransMix. We employ Mixpro and TransMix based on Deit-S on ImageNet-1k for 300 epochs. Figures (a) and (b), demonstrate the visualization of the progressive factor (α). Figures (c) and (d) depict their loss and top-1 validation error on ImageNet-1k. MixPro provides improved generalization compared to TransMix.\n\nVisualization of progressive factor (α). Fig. 2 (a) indicates the trend of the mean progressive factor (α) of all samples with the epoch. We can observe that since the model will gradually fit the training samples, the progressive factor will grow slowly with epoch. Fig. 2 (b) indicates the value of the progressive factor in a mini-batch at epoch 300. We can find that since the model learns to different degrees for different samples, the progressive factors obtained by the model will be different, so\n\n5\n\nEpochEpoch(d) Top-1 Error(c) Lossαα(a) Trends of αEpochIndex(b) αin a mini-batchLossTop-1 Error(%)Published as a conference paper at ICLR 2023\n\nour method can flexibly adjust the weight of the attention map in the mixed labels according to the progressive factors.\n\nGeneralization analysis. We compare and analyze the generalization of MixPro and TransMix. Fig. 2 (c) and (d) depict their loss and top-1 validation error on ImageNet-1k for 300 epochs based on Deit-S. We can notice that although MixPro has a larger loss in training compared to TransMix, it has a smaller top-1 error in verification. This demonstrates that Deit-S trained with MixPro can achieve better generalizability.\n\n4 EXPERIMENT\n\nIn this section, we primarily evaluate MixPro for its effectiveness, transferability, and robustness. We first study the effectiveness of MixPro on ImageNet-1k classification in Sec. 4.1. Next, we show the transfer ability of a MixPro pre-trained model when it is fine-tuned for downstream tasks (. subTran). In the Sec.4.3, we also show that Mixpro can improve the model’s robustness compared to TransMix.\n\n4.1\n\nIMAGENET CLASSIFICATION\n\nTable 1: Compared to TransMix, MixPro provides better performance on a wide range of model variants, e.g. DeiT, PVT, CaiT, XCiT , Swin on ImageNet-1k classification. All the baselines are reported in TransMix (Chen et al., 2021).\n\nModels\n\nParams\n\n#FLOPs\n\nTop-1 Acc(%)\n\nTop-1 Acc(%) +TransMix\n\nTop-1 Acc(%) +MixPro\n\nDeiT-T (Touvron et al., 2021a) PVT-T (Wang et al., 2021) XCiT-T (Ali et al., 2021) CA-Swin-T (Liu et al., 2021)\n\nCaiT-XXS DeiT-S (Touvron et al., 2021a) PVT-S (Wang et al., 2021) XCiT-S (Ali et al., 2021) CA-Swin-S (Liu et al., 2021)\n\nPVT-M (Wang et al., 2021) PVT-L (Wang et al., 2021) XCiT-M (Ali et al., 2021) DeiT-B (Touvron et al., 2021a)\n\nXCiT-L (Ali et al., 2021)\n\n5.7M 13.2M 12M 28.3M\n\n17.3M 22.1M 24.5M 26M 49.6M\n\n44.2M 61.4M 84M 86.6M\n\n189M\n\n1.6G 1.9G 2.3G 4.2G\n\n3.8G 4.7G 3.8G 4.8G 8.5G\n\n6.7G 9.8G 16.2G 17.6G\n\n36.1G\n\n72.2 75.1 79.4 81.6\n\n79.1 79.8 79.8 82.0 82.8\n\n81.2 81.7 82.7 81.8\n\n82.9\n\n72.6 75.5 80.1 81.8\n\n79.8 80.7 80.5 82.3 83.2\n\n82.1 82.4 83.4 82.4\n\n83.8\n\n73.8+(+1.2) 76.7+(+1.2) 81.2+(+1.1) 82.8+(+1.0)\n\n80.6+(+0.8) 81.3+(+0.6) 81.2+(+0.7) 82.9+(+0.6) 83.7+(+0.5)\n\n82.7+(+0.6) 82.9+(+0.5) 84.1+(+0.7) 82.9+(+0.5)\n\n84.7+(+0.9)\n\nImplementation Details. For image classification, we evaluate on ImageNet-1K (Deng et al., 2009), which contains 1.28M training images and 50K validation images from 1,000 classes. We examined various baseline vision Transformer models using in TranMix (Chen et al., 2021) including DeiT (Touvron et al., 2021a), PVT (Wang et al., 2021), CaiT (Touvron et al., 2021b), XCiT (Ali et al., 2021), CA-Swin (Liu et al., 2021; Chen et al., 2021). Specifically, CA-Swin replaces the last Swin (Liu et al., 2021) block with a classification attention (CA) block without parameter overhead, which makes it possible to generalize TransMix and MixPro onto Swin. The training schemes will be slightly adjusted to the official papers’ implementation. These experiments’ settings follow TransMix(Chen et al., 2021). We employ an AdamW optimizer for 300 epochs expect that XCiT (Ali et al., 2021) and CaiT (Touvron et al., 2021b) reported 400 epochs. Following the TransMix, we use a cosine decay learning rate scheduler and 20 epochs of linear warm-up. Similarly, a batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. All baselines have already contained the carefully tuned regularization methods reported in TransMix (Chen et al., 2021) except for repeated augmentation (Hoffer et al., 2020) and EMA (Polyak & Juditsky, 1992) which do not enhance performance. Following TransMix(Chen et al., 2021), the attention map A in Eq. 5 can be obtained as an intermediate output from the multi-head self-attention layer of the last transformer block.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4.1.1 COMPARISON WITH TRANSMIX ON A WIDE RANGE OF VIT-BASED MODEL.\n\nAs shown in Table 1, MixPro can enhance the top-1 accuracy on ImageNet for all the listed ViT-based models. Compared to TransMix, MixPro achieves better performance on all models. In particular, MixPro has better performance on models with fewer parameters. For example, MixPro reaches 73.8% on Deit-T, which is 1.2% higher than TransMix.\n\n4.1.2 COMPARISON WITH SOTA MIXUP VARIANTS\n\nTable 2: Top1-accuracy, training speed (im/sec) and number of parameters comparison with state-ofthe-art Mixup variants on ImageNet-1k for 300 epochs. For a fair comparison, all listed models are built upon DeiT-S training recipe. Training speed (im/sec) takes account of data mixup, and model forward and backward in train-time. The main results are reported in TransMix (Chen et al., 2021) and TokenMix (Liu et al., 2022).\n\nMethod\n\nBackbone\n\n#Params\n\nSpeed (im/sec)\n\ntop-1 Acc (%)\n\nBaseline GridMix CutMix Attentive-CutMix SaliencyMix Puzzle-Mix TransMix TokenMix\n\nMixPro(Ours)\n\nDeiT-S\n\n22M 22M 22M 46M 22M 22M 22M 22M\n\n22M\n\n322 322 322 239 314 139 322 322\n\n322\n\n78.6 79.5 (+0.9) 79.8 (+1.2) 77.5 (-1.1) 79.2 (+0.6) 79.8 (+1.2) 80.7 (+2.1) 80.8 (+2.2)\n\n81.3 (+2.7)\n\nWe compare many SOTA Mixup-based methods on ImageNet-1k (Deng et al., 2009) in this section. Following TransMix, we also train based on DeiT-S for a fair comparison. MixPro is measured in image per second (im/sec), training speed (i.e., training throughput), and takes into account data mixup (Zhang et al., 2017), model forward and backward in train-time in an average of five runs for images at resolution 224x224 under 128 batch size with a TeslaV100 graphic card. Table 2 shows that MixPro outperforms all other Mixup-based methods.\n\n4.2 TRANSFER TO DOWNSTREAM TASKS\n\nWe demonstrate the transferability of our MixPro-based pre-trained models to the downstream tasks, including semantic segmentation, object detection, and instance segmentation. We observe the enhancements over the vanilla pre-trained baselines and TransMix.\n\n4.2.1 SEMANTIC SEGMENTATION\n\npretrained Backbone Decoder\n\nmIoU +MS\n\nResNet101 ResNet101 Deeplabv3+ 47.3\n\n48.5\n\nDeiT-S +TransMix +MixPro\n\nDeiT-S +TransMix +MixPro\n\nDeiT-S\n\nLinear\n\nDeiT-S\n\nSegmenter\n\n49.1 49.7 50.3\n\n49.7 50.6 51.1\n\n49.6 50.3 50.9\n\n50.5 51.2 51.6\n\nTable 3: Overhead-free impact of MixPro on transferring to a downstream semantic segmentation task on the Pascal Context (Mottaghi et al., 2014) dataset. (MS) denotes multiscale testing. The best results are in bold.\n\nBackbone\n\n#Params\n\nObject detection\n\nAP b AP b\n\n50 AP b\n\nInstance segmentation 50 AP m\n\n75 APm AP m\n\n75\n\nResNet50 ResNet101\n\nPVT-S TransMix-PVT-S MixPro-PVT-S\n\n44.2M 63.2M\n\n44.1M 44.1M 44.1M\n\n38.0 40.4\n\n40.4 40.9 41.4\n\n58.6 61.1\n\n62.9 63.8 64.2\n\n41.4 44.2\n\n43.8 44.0 44.4\n\n34.4 36.4\n\n37.8 38.4 38.9\n\n57.1 57.7\n\n60.1 60.7 61.1\n\n36.7 38.8\n\n40.3 41.3 41.7\n\nTable 4: Following TransMix (Chen et al., 2021), Overheadfree impact of MixPro on transferring to downstream object detection and instance segmentation using Mask R-CNN (He et al., 2017) with PVT (Wang et al., 2021) backbone on COCO val2017.AP b denotes bounding box AP for object detection, and AP mk denotes mask AP for instance segmentation.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nSettings. We decode the sequence of patch encoding zpatches ∈ Rp×d to a segmentation map s ∈ RH×W ×K in our experiments, where K is the number of semantic classes. Following TransMix (Chen et al., 2021), we also employ two convolution-free decoders. The first one is the linear decoder, which is a point-wise linear layer on DeiT. Patch encoding zpatches ∈ Rp×d is also employed to generate patch-level logits, which are reshaped and bilinearly upsampled to the segmentation map s. The second one is the segmenter decoder (Strudel et al., 2021), which is a transformer-based decoder, namely the Mask Transformer. We also train and evaluate the models on the Pascal Context (Mottaghi et al., 2014) dataset, which contains 4998 images with 59 semantic classes and a background class. In addition, the reported mean Intersection over Union (mIoU) is averaged over all classes as the main metric. The experiments are carried out using MMSegmentation (Contributors, 2020). All comparative experimental results come from TransMix (Chen et al., 2021).\n\nResults. Table 3 shows that the MixPro pre-trained DeiT-SLinear and DeiT-S-Segmenter outperform the TransMix pre-trained baselines of 0.6% and 0.5% mIoU, respectively. For multi-scale testing. MixPro also outperforms the TransMix pre-trained baselines of 0.6% and 0.4% mIoU.\n\n4.2.2 OBJECT DETECTION AND INSTANCE SEGMENTATION\n\nSettings. Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118K images and evaluates 5K validation images. Following TransMix, we employ our method on PVT (Wang et al., 2021) as the detection backbone since its pyramid features make it beneficial to object detection. Following TransMix (Chen et al., 2021), we adopt 1x training schedule (i.e., 12 epochs) to train the detector based on mmDetection (Chen et al., 2019) framework.\n\nResults. As indicated in Table 4, we observe that on two downstream tasks, the results of MixPro’s pre-trained backbone once again outperformed TransMix’s pre-trained backbone, which enhanced 0.5% box AP and 0.5% mask AP, respectively.\n\n4.3 ROBUSTNESS ANALYSIS\n\nWe also compare MixPro with TransMix in terms of robustness and out-of-distribution performance.\n\nFigure 3: Robustness against occlusion. The figure shows the robustness of DeiT-S against occlusion with different information loss ratios.\n\n4.3.1 ROBUSTNESS TO OCCLUSION\n\nNaseer et al. (Naseer et al., 2021) investigate whether ViTs perform robustly in the presence of missing partial or substantial image content. In particular, ViTs divide an image into N =196 patches on a 14x14 spatial grid. For example, an image of size 224×224×3 is split into 196 patches, each of size 16×16×3. In the experiments, \"patch dropping\" means replacing original image patches with 0-value patches. Following TransMix (Chen et al., 2021), we showcase the classification accuracy on an ImageNet-1k validation set with three dropping settings. (1) Random Patch Dropping, which selects and drops a subset of M patches at random. (2) Salient (foreground) Patch Dropping, which studies the robustness of ViTs against occlusions of highly salient regions. Naseer et al. (Naseer et al., 2021) thresholds on DINO’s attention map to obtain salient patches, which are dropped by ratios. (3) Non-salient (background) Patch Dropping, in which the least salient regions of an image are selected and dropped following the above approach.\n\n8\n\nTop-1 AccInformation loss(a) Random patch droppingInformation loss(b) Salience patch droppingTop-1 Acc(c) Non-Salience patch droppingInformation lossTop-1 AccPublished as a conference paper at ICLR 2023\n\nResults. As demonstrated in Fig. 3, DeiT-S with MixPro is superior to TransMix and vanilla DeiT-S on all occlusion levels.\n\n4.3.2 NATURAL ADVERSARIAL EXAMPLE\n\nTable 5: The robustness of DeiT-S against natural adversarial examples on ImageNet-A and out-ofdistribution examples on ImageNet-O.\n\nModels\n\nNat. Adversarial Example Top1-Acc Calib-Error AURRAC\n\nOut-of-Dist AUPRC\n\nDeiT-S TransMix-DeiT-S MixPro-DeiT-S\n\n19.1% 21.1% 22.4%\n\n32.0 % 31.2 % 30.3 %\n\n23.8 % 28.8 % 32.4 %\n\n20.9 % 21.9 % 23.1 %\n\nFor these experiments, we use the ImageNet-A dataset (Hendrycks et al., 2021), which adversarially collects 7,500 unmodified, natural but \"hard\" real-world images. They are drawn from some challenging scenarios ,such as fog scenes and occlusion. For evaluating our method, following settings in TransMix (Chen et al., 2021), we evaluate methods on the top-1 accuracy, Calibration Error (CalibError) (Hendrycks et al., 2021) which judges how classifiers can reliably forecast their accuracy, and the Area Under the Response Rate Accuracy Curve (AURRAC) which is an uncertainty estimation metric.\n\nResults. As demonstrated in Table 5, MixPro-trained Deit-S outperforms TransMix-trained DeiT-S and vanilla DeiT-S on all metrics. MixPro lifts Top1-Acc on ImageNet-adversarial by 1.3%. For AURRAC, MixPro-DeiT-S achieves 32.4%, 3.6% higher than TransMix-DeiT-S.\n\n4.3.3 OUT-OF-DISTRIBUTION DETECTION\n\nThe ImageNet-O (Hendrycks et al., 2021) is an adversarial out-of-distribution detection dataset. It adversarially collects 2000 images from outside ImageNet-1K. The anomalies of unforeseen classes should result in low-confidence predictions. The metric is the area under the precision-recall curve (AUPRC). Table 5 shows that MixPro-trained DeiT-S outperforms TransMix-trained DeiT-S by 1.2% AUPRC and outperforms DeiT-S by 2.2% AUPRC.\n\n5 CONCLUSION\n\nIn this paper, we propose a new data augmentation method, MixPro. MixPro addresses the shortcomings of the current SOTA data augmentation method, TransMix, from the perspective of image and label space for the vision transformer. From the perspective of image space, we propose MaskMix which is a random mask strategy with adjustable scale. MaskMix ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we propose progressive attention labeling, which utilizes a progressive factor (α) to dynamically reweight the attention weight of the mixed attention label. Experimental results show that compared with TransMix, our method brings an improvement of 1.2%, 0.6%, and 0.5% for DeiT-T, DeiT-S, and Deit-B on the imagenet, respectively. After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to three downstream tasks such as semantic segmentation, object detection, and instance segmentation . In addition, compared to TransMix, MixPro also shows stronger robustness on three different benchmarks. In the ablation study, we detail the effect of each proposed module, the effect of different scales of mask patches, the different strategies of the progressive factor, and so on.\n\nAcknowledgments and Disclosure of Funding\n\nThis work was supported in part by the National Natural Science Foundation of China under Grant 62271034, and in part by the Fundamental Research Funds for the Central Universities under Grant XK2020-03.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. Advances in neural information processing systems, 34, 2021.\n\nKyungjune Baek, Duhyeon Bang, and Hyunjung Shim. Gridmix: Strong regularization through local\n\ncontext mapping. Pattern Recognition, 109:107594, 2021.\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213–229. Springer, 2020.\n\nPaola Cascante-Bonilla, Arshdeep Sekhon, Yanjun Qi, and Vicente Ordonez. Evolving image\n\ncompositions for feature representation learning. arXiv preprint arXiv:2106.09011, 2021.\n\nJie-Neng Chen, Shuyang Sun, Ju He, Philip Torr, Alan Yuille, and Song Bai. Transmix: Attend to\n\nmix for vision transformers. arXiv preprint arXiv:2111.09833, 2021.\n\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n\nMMSegmentation Contributors. Mmsegmentation: Openmmlab semantic segmentation toolbox and\n\nbenchmark, 2020.\n\nXiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-to-end object detection with dynamic attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2988–2997, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\n\nwith cutout. arXiv preprint arXiv:1708.04552, 2017.\n\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, and N. Houlsby. An image is worth 16x16\n\nwords: Transformers for image recognition at scale. 2020.\n\nYuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and Wenyu Liu. You only look at one sequence: Rethinking transformer in vision through object detection. Advances in Neural Information Processing Systems, 34, 2021.\n\nKaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n\nIEEE international conference on computer vision, pp. 2961–2969, 2017.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15262–15271, 2021.\n\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129–8138, 2020.\n\nZi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter: Token labeling for training better vision transformers. Advances in Neural Information Processing Systems, 34:18590–18602, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nA. Kendall and Y. Gal. What uncertainties do we need in bayesian deep learning for computer vision?\n\n2017.\n\nJang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In International Conference on Machine Learning, pp. 5275–5285. PMLR, 2020.\n\nJihao Liu, Boxiao Liu, Hang Zhou, Hongsheng Li, and Yu Liu. Tokenmix: Rethinking image mixing\n\nfor data augmentation in vision transformers. arXiv preprint arXiv:2207.08409, 2022.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10012–10022, October 2021.\n\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 891–898, 2014.\n\nMuhammad Muzammal Naseer, Kanchana Ranasinghe, Salman H Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. Advances in Neural Information Processing Systems, 34, 2021.\n\nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM\n\njournal on control and optimization, 30(4):838–855, 1992.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7262–7272, 2021.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021a.\n\nHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 32–42, 2021b.\n\nAFM Uddin, Mst Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae, et al. Saliencymix: A saliency guided data augmentation strategy for better regularization. arXiv preprint arXiv:2006.01791, 2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In International Conference on Machine Learning, pp. 6438–6447. PMLR, 2019.\n\nDevesh Walawalkar, Zhiqiang Shen, Zechun Liu, and Marios Savvides. Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification. arXiv preprint arXiv:2003.13048, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 568–578, October 2021.\n\nJianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 558–567, October 2021.\n\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6023–6032, 2019.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n\nA APPENDIX\n\nA.1 ABLATION STUDY\n\nThe effect of our proposed module. Table 6 demonstrated the effect of removing different components to provide insights into what makes MixPro successful. The experimental results are based on DeiT-S training for 300 epochs and Mixup (Zhang et al., 2017) is default used. We can observe that combining MaskMix and PAL, our method MixPro can enhance 1.5% compared to the standard setup (using Mixup and CutMix together), reaching 81.3% top-1 accuracy.\n\nTable 6: Top1-accuracy is on imagenet-1K based on DeiT-S. We study the effect of removing different components to provide insights into what makes MixPro successful.\n\nCutMix TransMix MaskMix\n\nPAL top-1 Acc (%)\n\n(cid:33) (cid:33) (cid:37) (cid:33) (cid:37)\n\n(cid:37) (cid:33) (cid:33) (cid:33) (cid:37)\n\n(cid:37) (cid:37) (cid:33) (cid:37) (cid:33)\n\n(cid:37) (cid:37) (cid:37) (cid:33) (cid:33)\n\n79.8 80.7 (+0.9) 81.0 (+1.2) 81.1 (+1.3) 81.3 (+1.5)\n\nFigure 4: Illustration of different mask strategies.\n\n12\n\n(a) region(b) block(c) random(d) random with adjustable scale (Ours) Published as a conference paper at ICLR 2023\n\nThe effect of different mask strategies. Figure 4 illustrates the different mask strategies. The region-based strategy is widely used in previous works (Yun et al., 2019; Uddin et al., 2020; DeVries & Taylor, 2017). Block and random based strategies mainly used in self-supervised learning (He et al., 2022; Bao et al., 2021). Our MixPro boosts random strategy on an adjustable scale. To better inspect the impact of mask strategies alone, in table 7, we directly use λarea to generate mixed labels instead of PAL. Table 7 shows the different performances of the mask strategies. Block and random strategy results come from TokenMix (Liu et al., 2022). We can observe that the block and random mask strategy improves significantly compared to the region mask strategy. This also indicates that images with more mixed global content are more effective for the vision transformer. Furthermore, our proposed adjustable scale is necessary for the random mask strategy. Compared with block and random strategies, the mask patches with a 4x scale can improve the accuracy of Deit-T by 0.5% and Deit-S by 0.2%.\n\nTable 7: Ablation of mask strategy.\n\nmodel\n\nregion\n\nblock\n\nrandom random(4x scale)\n\nDeit-T Deit-S\n\n72.2 79.8\n\n72.7 80.6\n\n72.7 80.6\n\n73.2 80.8\n\nThe effect of different scales of mask patches. The scale of mask patches Pmask is multiple of the scale of image patches. There are several optional scales ∈ {1×, 2×, 4×, 7×}. The Fig. 5 (a) indicates that the evaluate results of MixPro with different scales based on DeiT-S on Imagenet-1K top-1 error. For all scales considered, MixPro improves upon the baseline (20.2%) significantly. Moreover, it performs best when the scale is multiplied by 4×. We still recommend adjusting the scale size more finely for different models to get better results.\n\nFigure 5: Effect of different scales of mask patch and β on Imagenet-1K top-1 error with DeiT-S.\n\nThe effect of different β. In Fig. 5 (b), we demonstrate the top-1 error of MixPro under different Beta distribution, such as β ∈ {0.5, 0.8, 1, 2}, on Imagenet-1K with DeiT-S. We can observe that β is equal to 1 when our model achieves the best performance.\n\nThe different strategies of progressive factor α. To facilitate the understanding of our proposed progressive attention labeling (PAL), we explore several different strategies to generate the progressive factor α evaluating on Imagenet-1K with DeiT-T. We employ several types of strategies: 1) Progressrelevant strategies adjust α with the number of training epochs, e.g., parabolic decay, etc. 2) Progress-irrelevant strategies include the equal weight. 3) A learnable parameter.\n\nHow the boundary inside the patch influences ViTs? TransMix introduces sharp rectangular borders even within an image patch (see Fig. 1, red colored) that are clearly distinguishable from the background. These borders inside the patch may draw excessive attention to the model, because for vision transformers, a patch would form a semantic unit, and the computation of the attention map of the whole image is obtained by summing the attention of each image patch.\n\n13\n\n1818.51919.5201x2x4x7xScaleTop-1 Error(%)1818.218.418.618.8190.50.812βTop-1 Error(%)(a) The different scales of mask patches.(b) Effect of different β.Published as a conference paper at ICLR 2023\n\nOur MaskMix is able to eliminate the influence of the internal borders of patches. By aligning the borders of the mask patches with the image patches, there thus will not be sharp rectangular borders within image patches. Here also design three approaches to explore the performance of our MaskMix: A. Aligning mask borders to patch borders for TransMix. B. Misaligning the borders of mask patches with image patches for our MaskMix. C. Reducing the scale of each mask unit for our MaskMix to introduce more noises. Experimental results are below:\n\nTable 8: Three approaches to exploring the performance of our MaskMix.\n\nmethod\n\nTransMix\n\nA\n\nB\n\nC MaskMix(Ours)\n\nTop-1 Acc (%)\n\n72.6\n\n73.1\n\n73.1\n\n73.4\n\n73.8\n\nWe can observe that our MaskMix introduces more noisy patterns (such as B and C) lead to worse results. The TransMix eliminates the internal borders of the patches using the approach A and lifting Top-1 Acc by 0.5%. This shows the improvements brought by the design in our MaskMix.\n\nWhether introducing more random patterns will further improve the performance? We test three random patterns to evaluate the DeiT-T on ImageNet. The patterns and results are below:\n\nTable 9: Evaluated on more noisy patterns.\n\nmodel\n\nBaseline Random shuffling on the cropped patches Random shuffling on the patches of the canvas image Both\n\nresults\n\n73.2 69.8 69.7 67.3\n\nWe can observe that damaging the original geometry of the image with these schemes leads to worse results, i.e., simply introducing random patterns (e.g., via simple random shuffling) does not bring performance improvement.\n\nTable 10: Ablation studies of different progressive factor strategies on Imagenet-1K with on DeiT-T.\n\nstrategy\n\nEqual weight Linear increment\n\nParabolic increment Learnable parameter\n\nα\n\n0.5 T\nTmax T\nTmax -\n\n2\n\nPAL (Ours)\n\ncosine distance\n\nTop-1 Error (%)\n\n27.3 27.1\n\n26.9 27.0\n\n26.2\n\nAs illustrated in Table 10, the progress-relevant strategies (i.e., linear increment, and parabolic increment) for generating α can yield better results than the progress-irrelevant strategy. The effects of progress-relevant strategies and learnable parameter strategy are equally. These observations prove our motivation that the model gets a low-confident attention map during the early training process, so employing the low-confident attention map for mixed labels may not be a good choice. Among these strategies, the one with the best performance for generating α is our proposed PAL.\n\nA.2 VISUALIZATION\n\nAs shown in Fig. 6, we visualize images generated by our method and its corresponding attention map. We can observe that mixed images produced by MaskMix consist of multiple scattered and continuous areas in the image, which thus can capture broader parts of the object content of the original images from a global perspective. Furthermore, the attention maps of these mixed images also make sense, capturing the feature of objects in mixed images.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Visualization. Top: Original images. Medium: Mixed images generated by our MaskMix. Bottom: Attention maps of mixed images.\n\nA.3 TRAINING DETAILS ON IMAGENET-1K\n\nOur training receipt follows previous works (Touvron et al., 2021a; Chen et al., 2021). The default setting is in Table 11.\n\nTable 11: Training settings on ImageNet-1K.\n\nconfig\n\nvalue\n\noptimizer learning rate weight decay batch size learning rate schedule warm-up epochs training epochs augmentation label smoothing drop path MixUp CutMix MixPro\n\nAdamW 0.001 0.05 1024 cosine decay 20 300 RandAug(9, 0.5) 0.1 0.1 0.8 1\n1\n\nA.4 PSEUDO-CODE\n\nAlgorithm 1 provides the pseudo-code of MixPro in a pytorch-like style. It demonstrates that simply few lines of code can boost the performance in the plug-and-play manner.\n\n15\n\nOriginal imagesMixed imagesAttention maps(a)(b)(c)(d)(e)chickadeetenchlemonpomegranatelemonscabbardscabbardwater bottlechameleonrock crabPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Pseudo-code of MixPro in a PyTorch-like style. # H, W: the height and width of the input image. # h, w: the height and width of the attention map. # M: 0/1 mask of MaskMix with shape (H,W). # downsample: downsample from (H,W) to (h,w).\n\nfor (x, y) in dataloader: # load a mini-batch\n\nτ = Beta(β,β) # Eq. (6) M = mask_generate(τ , Pmask,Pimage) λarea = sum(M) (cid:101)x = x * M + x.flip(0) * (1-M) # Eq. (7) logits, A = model((cid:101)x)\n\n(cid:101)y = λarea * y + (1-λarea) * y.flip(0) # Eq. (7) α = cos_similarity(logits, (cid:101)y) # Eq. (8)\n\nM’ = downsample(M) λattn = matmul(A, M’) # Eq. (5) λ = α * λattn + (1-α) * λarea # Eq. (9) (cid:101)y = λ * y + (1-λ) * y.flip(0)\n\nCrossEntropyLoss(logits, (cid:101)y).backward()\n\n16",
    "reference": "# Summary Of The Paper\n\nIn this submission, the author proposed a data augmentation method for ViT. Specifically, two samples will be mixed in a global level, where patches of an image will be replaced with the patches from another images at corresponding locations. Then, the labels are generated using both the attention map generated by the ViT and the binary mask to avoid the negative effect using only attention map in the beginning of training process. The experimental results have illustrated the effectiveness of the proposed method. The approach is further demonstrated in the downstream applications.\n\n# Strength And Weaknesses\n\nPros:\n1. The paper is well written and easy to follow.\n2. The experiments are thorough and the results are pretty good.\n3. The proposed method can be generalized to different visual tasks and boost performance.\nCons:\n1. In the downstream tasks, seems like the proposed method is only employed for pretraining. Can it be used for directly finetuning for downstream applications?\n2. Minor: in Generalization analysis: \"Fig2(c) depicts their loss and top-1.....\" should be Fig2(c) and (d) depicts their loss and top-1....\"\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written and easy to follow. The authors have conducted plenty of experiments to demonstrate the effectiveness of the proposed method and the proposed approach can be reproduced based on the information given in the paper.\n\n# Summary Of The Review\n\nTo sum up, the paper is well written and easy to follow. It proposed a data augmentation method for ViT which can further boost the performance of ViT on various tasks. Experiments on different tasks and applications were conducted and the results were well analyzed. I have some questions which can be referred to in Strength And Weaknesses.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nOUT-OF-DISTRIBUTION DETECTION AND SELECTIVE GENERATION FOR CONDITIONAL LANGUAGE MODELS\n\nJiaming Luo1 Yao Zhao1 Kundan Krishna2\n\nJie Ren1∗ Mohammad Saleh1 Balaji Lakshminarayanan1 1Google Research ∗Correspondence to: {jjren, peterjliu}@google.com\n\nPeter J Liu1∗\n\n2Carnegie Mellon University, work done while at Google Research\n\nABSTRACT\n\nMachine learning algorithms typically assume independent and identically distributed samples in training and at test time. Much work has shown that highperforming ML classifiers can degrade significantly and provide overly-confident, wrong classification predictions, particularly for out-of-distribution (OOD) inputs. Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output. We present a highly accurate and lightweight OOD detection method for CLMs, and demonstrate its effectiveness on abstractive summarization and translation. We also show how our method can be used under the common and realistic setting of distribution shift for selective generation (analogous to selective prediction for classification) of high-quality outputs, while automatically abstaining from low-quality ones, enabling safer deployment of generative language models.\n\n1\n\nINTRODUCTION\n\nRecent progress in generative language models (Wu et al., 2016a; Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020; Zhang et al., 2020) has led to quality approaching human-performance on research datasets and has opened up the possibility of their wide deployment beyond the academic setting. In realistic user-facing scenarios such as text summarization and translation, it should be expected that user provided inputs can significantly deviate from the training data distribution. This violates the independent, identically-distributed (IID) assumption commonly used in evaluating machine learning models.\n\nMany have shown that performance of machine learning models can degrade significantly and in surprising ways on OOD inputs (Nguyen et al., 2014; Goodfellow et al., 2014; Ovadia et al., 2019). For example an image classifier may detect cows in images with very high accuracy on its IID test set but confidently fails to detect a cow when paired with an unseen background (Murphy, 2023; Nagarajan et al., 2020). This has led to active research on OOD detection for a variety of domains, including vision and text but focused primarily on classification. Salehi et al. (2021); Bulusu et al. (2020); Ruff et al. (2021) provide comprehensive reviews on this topic.\n\nConditional language models are typically trained given input sequence x = x1 . . . xL to autoregressively generate the next token in a sequence y = y1 . . . yT as a classification over the token-vocabulary V , pθ(y|x) = (cid:81)T t=1 pθ(yt|y<t, x), yt ∈ V . Consequently, the perils of out-ofdistribution are arguably more severe as (a) errors propagate and magnify through auto-regression, and (b) the space of low-quality outputs is greatly increased as arbitrary text sequences can be generated. Common errors from text generation models include disfluencies (Holtzman et al., 2020) and factual inaccuracies (Goodrich et al., 2019; Maynez et al., 2020). A common failure case we observed in abstractive summarization is for the model to output “All images are copyrighted” as the summary for news articles from a publisher (CNN) different than what it was trained on (BBC) (see Figure A.7).\n\nIn this work, we propose OOD detection methods for CLMs using abstractive summarization and translation as case studies. Similar to classification, we show in Section 2.1 that CLMs have untrustworthy likelihood estimation on OOD examples, making perplexity a poor choice for OOD\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ndetection. In Section 2.2, we propose a highly-accurate, simple, and lightweight OOD score based on the model’s input and output representations (or embeddings) to detect OOD examples, requiring negligible additional compute beyond the model itself.\n\nWhile accurate OOD detection enables the conservative option of abstaining from generation on OOD examples, it may be desirable to generate on known near-domain data, e.g. generate summaries for articles from news publishers that differ from our fine-tuning set. Thus the ability to selectively generate where the model is more likely to produce higher-quality outputs, enables safer deployment of conditional language models. We call this procedure selective generation, analogous to the commonly used term selective prediction in classification (Chow, 1957; Bartlett & Wegkamp, 2008; Geifman & El-Yaniv, 2017). In Section 4, we show that while model perplexity is a reasonable choice for performing selective generation with in-domain examples, combining with our OOD score works much better when the input distribution is shifted.\n\nIn summary, our contributions are:\n\n• Propose lightweight and accurate scores derived from a CLM’s embeddings for OOD detection, significantly outperforming baselines on abstractive summarization and translation tasks, without the need for a separate detection model.\n\n• Show that model perplexity can be an unreliable signal for quality estimation on OOD examples, but combined with our OOD scores can be used effectively to selectively generate higher-quality outputs while abstaining on lower ones.\n\n• Propose an evaluation framework for OOD detection and selective generation for CLMs,\n\nincluding human quality ratings for summarization.\n\n2 OOD DETECTION IN CONDITIONAL LANGUAGE MODELS\n\nThe maximum softmax probability (MSP), p(y|x), y = arg maxk=1,...,K p(k|x) is a simple, commonly used OOD score for K-class classification problem (Hendrycks & Gimpel, 2016; Lakshminarayanan et al., 2017). For CLMs, the perplexity, which is monotonically related to the negative log-likelihood of the output sequence averaged over tokens − 1 t=1 log p(yt|y<t, x) is a natural OOD score to consider, and analogous to the negative MSP in classification because both are based on softmax probabilities. We first study how well the perplexity performs for OOD detection tasks.\n\n(cid:80)T\n\nT\n\n2.1 PERPLEXITY IS ILL-SUITED FOR OOD DETECTION (a) Summarization\n\n(b) Translation\n\nFigure 1: Perplexity scores density of a CLM trained on (a) xsum for summarization, and (b) WMT for translation, evaluated on other datasets/domains. Perplexity is not well suited for OOD detection due to significant overlap between in-domain and OOD scores.\n\nIn Figure 1, we compare the distribution of perplexity of (a) a summarization model and (b) a translation model trained on in-domain dataset and evaluated on multiple OOD datasets, respectively. For summarization, a model is trained on xsum and evaluated on other news datasets including cnn dailymail and newsroom as near-OOD datasets, and forum (forumsum) and dialogue (samsum and reddit tifu) datasets as far-OOD (see Section 3 for details). The perplexity distributions overlap significantly with each other even though the input documents are significantly different. Furthermore, perplexity assigns cnn dailymail even lower scores than the in-domain xsum.\n\nFor translation, the model is trained on WMT15 dataset and evaluated on other WMT test splits (Bojar et al., 2015), OPUS100 (Aulamo & Tiedemann, 2019), and MTNT (Michel & Neubig, 2018). The in-domain and OOD datasets perplexity densities overlap even more. Overall, these results suggest that perplexity is not well suited for OOD detection.\n\n2\n\n0.00.51.01.52.02.5perplexity0.00.51.01.52.02.53.0Densityxsumcnn_dailymailnewsroom_allreddit_tifuforumsumsamsum012345perplexity0.00.20.40.60.81.01.2Densityholdoutndt2015KoranITsubtitlesmtntPublished as a conference paper at ICLR 2023\n\n2.2 DETECTING OOD USING CLM’S EMBEDDINGS\n\nGiven a trained conditional language model, we propose using the input and output representations/embeddings computed as part of the inference/generation process to detect OOD examples. In this work, we use Transformer encoder-decoder models and obtain the input embedding z by averaging the encoder’s final-layer hidden state vectors hi ∈ Rd (d is the hidden dimension) corresponding to the input sequence token xi. To obtain the output embedding w we average the decoder’s final-layer hidden state vectors gi ∈ Rd corresponding to the output token yi. Thus\n\nz :=\n\n1 L\n\nL (cid:88)\n\ni=1\n\nhi\n\nw :=\n\n1 T\n\nT (cid:88)\n\ni=1\n\ngi,\n\nz, w ∈ Rd\n\nwhere L and T are the input and output sequence lengths respectively. Figure 2 illustrates the idea.\n\nFigure 2: The proposed OOD detector based on input and output embeddings.\n\nIntuitively, if the embedding of a test input or output is far from the embedding distribution of the training data, it is more likely to be OOD. One way of measuring this distance is to fit a Gaussian, N (μ, Σ), μ ∈ Rd, Σ ∈ Rd×d, to the training embeddings and use the Mahalanobis distance (MD):\n\nMD(x; μ, Σ) := (x − μ)T Σ−1(x − μ),\n\nThis has been used for OOD detection using the representations from classification models (Lee et al., 2018) and computing the distances to class-conditional Gaussians.\n\nUnlike classification, which has class labels, in conditional language modeling we have paired input and output text sequences. We fit one Gaussian on the training input embeddings, N (μz, Σz), and a second Gaussian on the embeddings of the training ground-truth outputs, N (μw, Σw).\n\nFor a test input and output embedding pair (ztest, wtest), the input MD is computed as\n\nMDinput(ztest) := MD(ztest; μz, Σz)\n\n(Input MD OOD score)\n\nThe output MD is computed similarly:\n\nMDoutput(wtest) := MD(wtest; μw, Σw)\n\n(Output MD OOD score)\n\nMahalanobis distance is equivalent to computing a negative log-likelihood of the Gaussian distribution (up to a constant and a scalar), i.e. − log p(z) = d 2 (z − μ)T Σ−1(z − μ) = const. + 1 2 MD(z). Ren et al. (2019) showed that normalizing the likelihood with the likelihood of a background model works better for OOD detection. In a similar vein, Ren et al. (2021) proposed an analogous Relative Mahalanobis Distance (RMD) for classification: using the relative distance between the class-conditional Gaussians and a single background Gaussian using data from all classes. That method cannot be directly applied for CLMs because outputs are not just class labels. Thus in this work, we extend the RMD idea to conditional language models,\n\n2 log(2π) + 1\n\n2 log |Σ| + 1\n\nRMDinput(ztest) := MDinput(ztest) − MD0(ztest), 0 , Σz\n\nwhere MD0(ztest) := MD(ztest; μz 0 ), fit using a large, broad dataset to approximately represent all domains. In practice, we use C4, a large Common Crawl-based English dataset (Raffel et al., 2020)1 and ParaCrawl’s English-French dataset\n\n0 ) is the MD to a background Gaussian N (μz\n\n0 , Σz\n\n(Input RMD OOD score)\n\n1https://www.tensorflow.org/datasets/catalog/c4\n\n3\n\nUnsupported DomainSupported Domain“Sorry we do not support function for this input”.Input TextOutput TextIn-domainEncoderDecoderPerplexityConditional Language ModelInput OOD DetectorInput EmbeddingInput OOD scoreOutput OOD DetectorOutput EmbeddingOutput OOD scoreHigh OOD score?YesNoPublished as a conference paper at ICLR 2023\n\n(Ba ̃n ́on et al., 2020)2, as the data for fitting the background distributions for summarization and translation in our experiments, respectively.\n\nWhile we use the ground-truth outputs to fit N (μw, Σw), we decode outputs from the trained CLMs and use those output embeddings to fit the background output Gaussian, N (μw\n\nδ , Σw\n\nδ ).\n\nRMDoutput(wtest) := MDoutput(wtest) − MDδ(wtest),\n\n(Output RMD OOD score)\n\nδ , Σw\n\nwhere MDδ(wtest) := MD(wtest; μw δ ) is the MD to the decoded output background distribution N (μw δ ). See Algorithm 1 and 2 for the detailed steps. Using decoded outputs serves two purposes: (1) We do not require supervised data (e.g. document-summary pairs) to fit the background Gaussian. (2) Decoded outputs may exhibit increased deficiencies that result from running the model on out-of-distribution data, which provides greater contrast with the in-domain ground-truth labels.\n\nδ , Σw\n\nThe RMD score can be regarded as a background contrastive score that indicates how close the test example is to the training domain compared to the background domains. A negative score suggests the example is relatively in-domain, while a positive score suggests the example is OOD. A higher score indicates greater OOD-ness.\n\nBinary classifier for OOD detection Since we have explicitly defined two classes, in-domain and background/general domain, another option is to train a binary classifier to discriminate embeddings from the two classes. We train a logistic regression model and use the un-normalized logit for the background as an OOD score. The Input Binary logits OOD score uses the input embeddings as features, whereas the Output Binary logits OOD score uses the decoded output embeddings as features. A higher score suggests higher likelihood of OOD. The preferred use of the logits over probability was also recommended by previous OOD studies for classification problems (Hendrycks et al., 2019). Though RMD is a generative-model based approach and the binary classifier is a discriminative model, we show that RMD is a generalized version of binary logistic regression and can be reduced to a binary classification model under certain conditions (see Section A.5 for details).\n\n3 EXPERIMENTS: OOD DETECTION\n\n3.1 EXPERIMENT SETUP\n\nWe run our experiments using Transformer (Vaswani et al., 2017) encoder-decoder models trained for abstractive summarization and translation. Below we specify the dataset used for training/finetuning (i.e. in-domain) and the OOD datasets.\n\nIn the case of summarization, OOD datasets can be intuitively categorized as near or far OOD based on the nature of the documents. For example, news articles from different publishers may be considered as sourced from different distributions, but are closer than news articles are to dialogue transcripts. We also quantitatively showed that using n-gram overalp analysis in Table A.10. In contrast, the translation datasets we use consist of English-French sentence pairs with less variation between datasets due to the shorter length of sentences.\n\nSummarization model We fine-tuned PEGASUSLARGE (Zhang et al., 2020) on the xsum (Narayan et al., 2018) dataset, consisting of BBC News articles with short, abstractive summaries.\n\nSummarization datasets We use 10,000 examples from xsum and C4 training split to fit indomain/foreground and background Gaussian distributions, respectively. For test datasets, we have cnn dailymail (Hermann et al., 2015; See et al., 2017), news articles and summaries from CNN and DailyMail; newsroom (Grusky et al., 2018), article-summary pairs from 38 major news publications; reddit tifu (Kim et al., 2018), informal stories from sub-reddit TIFU with author written summaries of very diverse styles; samsum (Gliwa et al., 2019) and forumsum (Khalman et al., 2021), high-quality summaries of casual dialogues.\n\nTranslation model We train a Transformer base model (Vaswani et al., 2017) with embedding size 512 on WMT15 English-French (Bojar et al., 2015). The model is trained with Adafactor optimizer (Shazeer & Stern, 2018) for 2M steps with 0.1 dropout and 1024 batch size. Decoding is done using beam search with 10 beam size and α = 0.6 length normalization (Wu et al., 2016b). The best checkpoint scores 39.9 BLEU on newstest2014.\n\n2https://www.tensorflow.org/datasets/catalog/para crawl\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: AUROCs for OOD detection. For summarization task (a), cnn dailymail and newsroom are considered as near shift OOD since it shares news topics as xsum, and reddit tifu, forumsum, and samsum are far shift OOD. For translation (b), WMT dataset contains various test WMT datasets collected from different years, OPUS contains five different domains (the degree of shift varies), and MTNT contains noisy data from Reddit.\n\n(a) Summarization\n\nMeasure\n\ncnn dailymail\n\nnewsroom reddit tifu\n\nforumsum samsum\n\nNear Shift OOD\n\nFar Shift OOD\n\nMD RMD Binary logits\n\nPerplexity (baseline) NLI score (baseline) MD RMD Binary logits\n\n0.651 0.828 0.997\n\n0.424 0.440 0.944 0.958 0.989\n\nINPUT OOD 0.799 0.930 0.959\n\nOUTPUT OOD 0.665 0.469 0.933 0.962 0.982\n\n(b) Translation\n\n0.974 0.998 1.000\n\n0.909 0.709 0.985 0.998 1.000\n\n0.977 0.997 0.999\n\n0.800 0.638 0.973 0.993 0.998\n\n0.995 0.999 0.998\n\n0.851 0.743 0.985 0.998 0.997\n\nMeasure\n\nnt2014\n\nndd2015\n\nndt2015\n\nlaw\n\nmedical Koran\n\nIT\n\nsub\n\nWMT\n\nOPUS\n\nMD RMD Binary logits\n\nPerplexity (baseline) COMET (baseline) Prism (baseline) MD RMD Binary logits\n\n0.534 0.798 0.864\n\n0.570 0.484 0.445 0.609 0.786 0.822\n\n0.671 0.866 0.904\n\n0.496 0.514 0.504 0.733 0.858 0.860\n\nINPUT OOD\n\n0.670 0.863 0.904\n\n0.511 0.389 0.485\n\nOUTPUT OOD 0.392 0.435 0.459 0.482 0.355 0.507\n\n0.494 0.525 0.505 0.739 0.861 0.865\n\n0.704 0.840 0.813\n\n0.363 0.543 0.565 0.784 0.845 0.783\n\n0.737 0.957 0.963\n\n0.657 0.632 0.716 0.838 0.939 0.942\n\n0.828 0.959 0.928\n\n0.343 0.619 0.604 0.900 0.951 0.890\n\n0.900 0.969 0.950\n\n0.359 0.518 0.577 0.935 0.959 0.910\n\nMTNT\n\n0.668 0.943 0.963\n\n0.633 0.724 0.699 0.794 0.922 0.931\n\nTranslation datasets We use 100,000 examples from WMT15 En-Fr and the same number of examples from ParaCrawl En-Fr to fit the foreground and background Gaussians, respectively. For test, we use newstest2014 (nt14), newsdiscussdev2015 (ndd15), and newsdiscusstest2015 (ndt15) from WMT15 (Bojar et al., 2015) and the law, Koran, medical, IT, and subtitles (sub) subsets from OPUS (Tiedemann, 2012; Aulamo & Tiedemann, 2019). We also use the English-French test set of MTNT (Michel & Neubig, 2018), consisting of noisy comments from Reddit.\n\nEvaluation metric We use the area under the ROC curve (AUROC) between the in-domain test data as negative and the OOD test data as positive sets to evaluate and compare the OOD detection performance. AUROC 1.0 means a perfect separation, and 0.5 means the two are not distinguishable.\n\nBaseline methods We compare our proposed OOD scores with various baseline methods, including (1) the model perplexity score, (2) the embedding-based Mahalanobis distance. In addition, we also compare with (3) Natural Language Inference (NLI) score (Honovich et al., 2022) for summarization, and (4) COMET (Rei et al., 2020) and (5) Prism (Thompson & Post, 2020) for translation. NLI score measures the factual consistency by treating the input document as a premise and the generated summary as a hypothesis. Both COMET and Prism are quality estimation metrics designed to measure translation quality without access to a human reference. More specifically, COMET finetunes the large XLM-R model (Conneau et al., 2020) on human evaluation data, and Prism is the perplexity score from a multilingual NMT model trained on 99.8M sentence pairs in 39 languages.\n\n3.2 RESULTS\n\nRMD and Binary classifier are better at OOD detection than baselines Table 1 shows the AUROCs for OOD detection on the (a) summarization and (b) translation datasets. Overall, our proposed OOD scores RMD and Binary logits outperform the baselines with high AUROCs (above 0.8). The commonly used output metrics, perplexity, NLI, COMET and Prism, have generally low AUROC scores (many have values around 0.5-0.6), suggesting they are not suited for OOD detection. Interestingly, we noticed that the output OOD scores perform better for summarization, while the input OOD scores perform better for translation. One possible reason is that when summariza-\n\n5\n\nPublished as a conference paper at ICLR 2023\n\ntion outputs are low-quality (e.g. producing repeated text or irrelevant summaries) they look very different than reference summaries, making OOD output score more sensitive to the contrast.\n\nThough RMD and Binary logits OOD scores both perform well at OOD detection, RMD OOD score is better at distinguishing near-OOD from far-OOD. This can be seen in Figure 3 where near-OOD datasets have scores distributed in between in-domain and far-OOD. In the summarization task, near-OOD (news articles) datasets cnn dailymail and newsroom have their RMD scores distributed in the middle of xsum and reddit tifu, forumsum and samsum. In contrast, under the binary logits score, the near-OOD and far-OOD datasets have largely overlapping score distributions making it hard to distinguish between the two. In practice, RMD OOD score may be better suited for selective generation where domain shifts are expected. We explore this in more detail in Section 4.\n\nFigure 3: Density of RMD (left) and Binary logits (right) OOD scores evaluated on summarization datasets. RMD is better at distinguishing near-OOD from far-OOD.\n\nFor the translation task, we additionally note that all methods have small AUROC for law dataset, suggesting that none of the methods are detecting the dataset as OOD. To better understand the special characteristics of the law dataset, we conducted an n-gram overlap analysis between the various test sets including law and the in-domain training data. We observed that law has the highest unigram overlap rate (48.8%) and the second highest overall overlap with the in-domain data (Table A.9).3 This shows that law is close to in-domain data in terms of surface features, which might contribute to the low AUROC scores for all tested methods.\n\nWe use ParaCrawl instead of C4 for translation because our translation model is trained on the sentence level, unlike the summarization model that takes the document as input. To further explore the effect of the background data on the performance, we split C4 documents into sentences and use that as the background data to compute the scores. The OOD detection performance using C4 sentences is very similar to that using ParaCrawl, as shown in Table A.3, suggesting that our method is not particularly sensitive to the choice of background data.\n\n4 USING OOD SCORES FOR SELECTIVE GENERATION\n\nThe most conservative option for deployment of a conditional language model is to completely abstain from generating on inputs that are detected as out-of-distribution, for which we have shown in Section 3 our OOD scores are fairly accurate. However, it is often desirable to expand the use of models beyond strictly in-distribution examples, if the quality of outputs is sufficiently high. In classification, this has been framed as determining when to trust a classifier, or selective prediction (Geifman & El-Yaniv, 2017; Lakshminarayanan et al., 2017; Tran et al., 2022). In this section, we seek to predict the quality of generation given an example, which may be out-of-distribution and abstain if the predicted quality is low. We call this selective generation. In practice, abstaining may correspond to hiding the model’s generated text, or turning off a summarization/translation feature.\n\n4.1 EXPERIMENT SETUP\n\nWe use the same models and datasets described in Section 3.1 but instead of simply detecting outof-distribution examples, our focus now is to predict the quality of generation for examples possibly outside the training distribution.\n\n3We define overlap rate as the percentage of unique n-grams in the test set that are also present in the indomain data. The overall overlap is defined as the geometric mean of all the n-gram overlap rates up to n = 4. All domains/splits including the in-domain data are subsampled to 1K for this analysis.\n\n6\n\n20000200040006000RMD Output OOD Score0.00000.00020.00040.00060.0008Densityxsumcnn_dailymailnewsroom_allreddit_tifuforumsumsamsum2001000100200300Binary Logits Output OOD Score0.0000.0020.0040.0060.0080.010DensityPublished as a conference paper at ICLR 2023\n\nMeasuring Translation quality We use BLEURT (Pu et al., 2021) as the main metric to measure translation quality. Previous work has demonstrated that neural metrics such as BLEURT are much better correlated with human evaluation, on both the system level and the sentence level (Freitag et al., 2021). BLEURT scores range from 0 to 1, with higher scores indicating better quality.\n\nMeasuring Summarization quality In general, it is unclear how to automatically measure the quality of summaries generated by a model on out-of-distribution examples (in this case, examples from different datasets). The reason is summarization datasets have dataset-specific summary styles that may be difficult to compare. For example, xsum summaries are typically single-sentence whereas cnn dailymail summaries consist of multiple sentences. Thus we report ROUGE-1 score as an automatic measure but primarily use human evaluation to assess the quality. Amazon Mechanical Turk workers were asked to evaluate summaries generated by the xsum model on a scale of 1-5 (bad-good) using 100 examples from xsum, cnn dailymail, reddit tifu, and samsum. We collected 3 ratings per example and computed the median. See Section A.3 for more details.\n\n4.2 PERPLEXITY HAS DIMINISHING CAPABILITY IN PREDICTING QUALITY ON OOD DATA\n\nSince the models are trained using negative log-likelihood as the loss, perplexity (which is monotonically related) is a good predictor of output quality for in-domain data. In fact, the Kendall rank correlation coefficient τ between perplexity and human judged quality score is 0.256 (See Table 2) for in-domain xsum for summarization. However, when including shifted datasets to test, we found that the perplexity score is worse at predicting quality on OOD data. For example the Kendall’s τ decreases to 0.068 for OOD dataset samsum (see Table A.4). We observed similar trend in translation, although less severe, as data shifted from in-domain to OOD, the Kendall’s τ between perplexity and BLEURT decreases (see Table A.5). Figure 4 further shows the correlation between perplexity and the quality score (ROUGE-1, human rating, and BLEURT, respectively) as a function of OOD score. It is clear to see the correlation decreasing as OOD score increases and the trend is consistent for both summarization and translation.\n\n(a) Summarization, ROUGE-1 (b) Summarization, human rating\n\n(c) Translation, BLEURT\n\nFigure 4: The Kendall’s τ correlation between perplexity and (a) ROUGE-1, (b) human evaluation median rating, and (c) BLEURT decreases as OOD score increases respectively. Note that we use output RMD OOD score for summarization and input RMD OOD score for translation.\n\n4.3 COMBINING OOD SCORES AND PERPLEXITY\n\nWhile model perplexity for quality estimation is worse for OOD examples, we observed that our OOD scores and perplexity are complementary in quality prediction. Figure A.1 shows a 2-D plot between the OOD score and perplexity regarding quality. We can see that neither perplexity nor OOD score can perfectly separate good and bad examples, and the combination of the two can work much better. Our observation echos work in uncertainty estimation in classification models (Mukhoti et al., 2021): perplexity based on softmax predictive distribution is regarded as an estimation for aleatoric uncertainty (caused by inherent noise or ambiguity in data), and the OOD distance based on representation estimates the epistemic uncertainty (caused by a lack of training data), and combining the two provides a comprehensive estimation of uncertainty.\n\nWe propose two simple methods to combine perplexity and OOD scores. (1) A simple linear regression, trained on a random 10% data split using ROUGE-1 or BLEURT as the quality score, and evaluated on the test split and human evaluation split. (2) the sum of the percentile ranks (PR) of the scores, i.e. PRsum= PRperplexity + PROOD. We sum PRs instead of their raw values because the two scores are in different ranges, PR(x) = R(x)\n\nN × 100, where R(x) is x’s rank in the list of size N .\n\nTable 2 shows the Kendall’s τ correlation coefficient between the various single and combined scores and the quality metric with only in-domain and all examples from all datasets. When all datasets\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Kendall’s τ correlation (p-value < 0.05 are grayed out) between various measures and human evaluation for summarization and BLEURT for translation. The “All” column shows the correlation when both in-domain and OOD examples are merged. Note for negatively correlated scores (e.g. perplexity (ppx), RMD), we take the negative value of the score for easier comparison.\n\n(a) Summarization\n\nMeasure\n\nIn-domain\n\nAll\n\nSingle Score\n\nPerplexity (baseline) NLI score (baseline) Input RMD Output RMD\n\nCombined Score\n\nPRsum(ppx, input RMD) PRsum(ppx, output RMD) Linear Reg. (ppx, input & output)\n\n0.256 0.337 0.015 0.053\n\n0.186 0.250 0.235\n\n0.300 0.381 0.336 0.385\n\n0.358 0.415 0.422\n\n(b) Translation\n\nMeasure\n\nIn-domain\n\nAll\n\nSingle Score\n\nPerplexity (baseline) COMET (baseline) Prism (baseline) Input RMD Output RMD\n\nCombined Score\n\nPRsum(ppx, input RMD) PRsum(ppx, output RMD) Linear Reg. (ppx, input & output)\n\n0.309 0.184 0.184 0.147 0.086\n\n0.321 0.323 0.318\n\n0.286 0.336 0.301 0.195 0.170\n\n0.361 0.356 0.352\n\nare merged, the combined scores significantly improve the correlation over perplexity by up to 12% (absolute) for summarization and 8% for translation, while the gains over the best external model-based (and much more expensive) baselines are 4% and 3%. The two combination methods perform similarly. See Tables A.4 and A.5 for an expanded table of scores.\n\n4.4 SELECTIVE GENERATION USING THE COMBINED SCORE\n\nIn selective generation, our goal is to generate when the model is more likely to produce highquality output, and abstain otherwise, enabling safer deployment of generative language models. To evaluate that, we propose using the Quality vs Abstention Curve (QA), analogous to accuracy versus rejection curve used for selective prediction in the classification (Chow, 1957; Bartlett & Wegkamp, 2008; Geifman & El-Yaniv, 2017). Similar concepts were proposed also in Malinin & Gales (2020); Xiao et al. (2020), but they only use automatic quality metrics for the analysis while we consider human evaluation to assess the quality as well. Specifically, at a given abstention rate α, the highest α-fraction scoring examples are removed and the average quality of remaining examples is computed. We want to maximize the quality of what is selectively generated and a better curve is one that tends to the upper-left which corresponds to removing bad examples earlier than good ones.\n\nFigure 5 shows the QA curves for various methods on summarization and translation. Quality is measured by human evaluation for summarization (see Figure A.4 for similar ROUGE-1 plot), and BLEURT for translation. The combined scores have the highest quality score at almost all abstention rates for both summarization and translation, while linear regression and PRsum perform similarly. For single scores, the OOD score performs better than perplexity and NLI scores at almost all abstention rates for summarization. For translation, the OOD score is better than perplexity when abstention rate α > 0.65 and worse than perplexity when α < 0.65. In other words, OOD score is better at abstaining slightly far-OOD while perplexity is better at abstaining near-OOD examples. Interestingly, our combined score is even marginally better than COMET that requires a separate neural network trained on human evaluation data. Prism is better than single scores, but much worse than our combined score. Area under the QA curves are shown in Tables A.6 and A.8 for reference.\n\nFigures 5 (b, d) are the corresponding survival curves showing how many examples per dataset are selected for generation as a function of abstention rate, based on the PRsum score. For summarization, the samples from far-OOD datasets reddit tifu and samsum are eliminated first with their sample count decreasing rapidly. The near-OOD dataset cnn dailymail and in-domain xsum are kept intact until α > 0.3, and in-domain xsum examples survive the longest. Similarly for translation, the out-of-domain and worst-quality (as seen in Table A.1b) Koran, MTNT, and subtitles examples are eliminated first, and the best-performing law and in-domain datasets are abstained last. The order in which datasets are eliminated corresponds to the aggregate quality by dataset, which we report in Table A.1. Besides the quantitative results, we show a few real examples in Section A.14 to better demonstrate how our predicted quality score helps selective generation.\n\n5 RELATED WORK\n\nOOD detection problem was first proposed and studied in vision classification problems (Hendrycks & Gimpel, 2016; Liang et al., 2017; Lakshminarayanan et al., 2017; Lee et al., 2018; Hendrycks et al., 2018; 2019), and later in text classification problems such as sentiment analysis (Hendrycks\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(c)\n\n(b)\n\n(d)\n\nFigure 5: (a) The Quality (human eval) vs Abstention curve for summarization. Combined scores have the highest quality at almost all abstention rates. (b) Survival count of each dataset as a function of abstention rate, using PRsum (we use output/input RMD for summarization/translation to pair with perplexity). OOD data is abstained earlier than in-domain. (c, d) The same as (a, b) for translation.\n\net al., 2020), natural language inference (Arora et al., 2021), intent prediction (Liu et al., 2020a; Tran et al., 2022), and topic prediction (Rawat et al., 2021). The widely used OOD methods can be characterized roughly into two categories (1) softmax probability or logits-based scores (Hendrycks & Gimpel, 2016; Liang et al., 2017; Hendrycks et al., 2019; Liu et al., 2020b), (2) embedding-based methods that measure the distance to the training distribution in the embedding space (Lee et al., 2018; Ren et al., 2021; Sun et al., 2022), (3) contrastive learning based methods which incorporate the contrastive loss into the classification cross-entropy loss to improve representation learning and consequently improve OOD detection (Winkens et al., 2020; Zhou et al., 2021). Though it is not straightforward to extend those classifier-based scores to CLMs especially for input OOD detection, we extend three of them based on our understanding as baselines for comparison with our methods. See Section A.6 for details. The results in Table A.2 show that those methods are in general not competitive with our proposed methods RMD and Binary logits, especially on near-OOD datasets.\n\nOOD detection problem is less studied in CLMs. A few studies explored OOD detection in semantic parsing (Lukovnikov et al., 2021; Lin et al., 2022), speech recognition (Malinin & Gales, 2020), and machine translation (Malinin et al., 2021; Xiao et al., 2020), but many of them focus on ensemblebased methods like Monte Carlo dropout or deep ensemble which use the averaged perplexity after sampling multiple output sequences.The ensembling method costs N times of the inference time, which is not feasible in practice. In this work, we focus on developing scores that can be readily derived from the generative model itself, without much increase in computation. We include an ensemble-based baseline in Section A.6 and show that its performance is worse than our methods.\n\n6 CONCLUSION AND FUTURE WORK\n\nWe have proposed lightweight and accurate scores to detect out-of-distribution examples for conditional language generation tasks. For real-world deployment, we have also shown how our OOD scores can be combined with language model perplexity to selectively generate high-quality outputs while abstaining from low-quality ones in the setting of input distribution shift.\n\nAlthough our experiments focus on summarization and translation, our methods do not make any assumptions about the task modality, and we believe our method is widely applicable to other tasks where the model output is a sequence, e.g. image captioning. While our analysis was restricted to conditional language modeling with encoder-decoder Transformers, we expect our method to also work with decoder-only (Liu et al., 2018) architectures, used by some large language models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022).\n\nFinally, analyzing why certain examples are OOD could lead to insights in how to make models more robust. Section A.13 presents one possible way to attribute OOD scores to sentences.\n\n9\n\n0.00.10.20.30.40.50.60.7Abstention Rate0.550.600.650.700.75Human Eval Median RatingPerplexity (Base)NLI (Base)Output RMDLinear Reg.PR sum0.00.20.40.60.81.0Abstention Rate020406080100Survival Countxsumcnn_dailymailreddit_tifusamsum0.00.20.40.60.8Abstention Rate0.6500.6750.7000.7250.7500.7750.800BLEURTPerplexity (Base)COMET (Base)Prism (Base)Input RMDLinear Reg.PR sum0.00.20.40.60.81.0Abstention Rate02004006008001000Survival Countholdoutndt2015lawKoransubtitlesmtntPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nThe authors would like to thank Jeremiah Zhe Liu, Sharat Chikkerur, and the anonymous reviewers for their helpful feedback on the manuscript. The authors would also like to thank Colin Cherry, George Foster, and Polina Zablotskaia for their feedback throughout the project.\n\nREFERENCES\n\nUdit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect them.\n\narXiv preprint arXiv:2109.06827, 2021.\n\nMikko Aulamo and J ̈org Tiedemann. The OPUS resource repository: An open package for creating parallel corpora and machine translation services. In Proceedings of the 22nd Nordic Conference on Computational Linguistics, pp. 389–394, Turku, Finland, September–October 2019. Link ̈oping University Electronic Press. URL https://aclanthology.org/W19-6146.\n\nMarta Ba ̃n ́on, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl`a-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ram ́ırez-S ́anchez, Elsa Sarr ́ıas, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. ParaCrawl: Web-scale acquisition of parallel corIn Proceedings of the 58th Annual Meeting of the Association for Computational Linpora. guistics, pp. 4555–4567, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.417. URL https://aclanthology.org/2020.acl-main.417.\n\nPeter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss.\n\nJournal of Machine Learning Research, 9(8), 2008.\n\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 1–46, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3001. URL https://aclanthology.org/W15-3001.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nSaikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. Anomalous\n\nexample detection in deep learning: A survey. IEEE Access, 8:132330–132347, 2020.\n\nChi-Keung Chow. An optimum character recognition system using decision functions. IRE Trans-\n\nactions on Electronic Computers, (4):247–254, 1957.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm ́an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. UnIn Proceedings of the 58th Annual supervised cross-lingual representation learning at scale. Meeting of the Association for Computational Linguistics, pp. 8440–8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https: //aclanthology.org/2020.acl-main.747.\n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondˇrej Bojar. Results of the WMT21 metrics shared task: Evaluating metrics with expertIn Proceedings of the Sixth Conference based human evaluations on TED and news domain. on Machine Translation, pp. 733–774, Online, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.wmt-1.73.\n\nYonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. Advances in\n\nneural information processing systems, 30, 2017.\n\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70–79, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. arXiv preprint arXiv:1412.6572, 2014.\n\nBen Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’19, pp. 166–175, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330955. URL https: //doi.org/10.1145/3292500.3330955.\n\nMax Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018. doi: 10.18653/v1/n18-1065. URL http://dx.doi.org/10.18653/v1/n18-1065.\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\n\nexamples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\n\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier\n\nexposure. arXiv preprint arXiv:1812.04606, 2018.\n\nDan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. arXiv preprint arXiv:1911.11132, 2019.\n\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100, 2020.\n\nKarl Moritz Hermann, Tom ́as Kocisk ́y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, pp. 1693– 1701, 2015. URL http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH.\n\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. TRUE: Re-evaluating factual consistency evaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3905–3920, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.287. URL https://aclanthology.org/2022.naacl-main.287.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMisha Khalman, Yao Zhao, and Mohammad Saleh. Forumsum: A multi-speaker conversation summarization dataset. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4592–4599, 2021.\n\nByeongchang Kim, Hyunwoo Kim, and Gunhee Kim. Abstractive summarization of reddit posts\n\nwith multi-level memory networks, 2018.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting\n\nout-of-distribution samples and adversarial attacks. NeurIPS, 2018.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence preIn Proceedings of training for natural language generation, translation, and comprehension. the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\n\nShiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detec-\n\ntion in neural networks. arXiv preprint arXiv:1706.02690, 2017.\n\nZi Lin, Jeremiah Zhe Liu, and Jingbo Shang. Towards collaborative neural-symbolic graph semantic parsing via uncertainty. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 4160–4173, 2022.\n\nJeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in Neural Information Processing Systems, 33:7498–7512, 2020a.\n\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\n\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detec-\n\ntion. Advances in Neural Information Processing Systems, 33:21464–21475, 2020b.\n\nDenis Lukovnikov, Sina Daubener, and Asja Fischer. Detecting compositionally out-of-distribution In Findings of the Association for Computational Linguistics:\n\nexamples in semantic parsing. EMNLP 2021, pp. 591–598, 2021.\n\nAndrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction.\n\narXiv preprint arXiv:2002.07650, 2020.\n\nAndrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. Shifts: A dataset of real distributional shift across multiple large-scale tasks. arXiv preprint arXiv:2107.07455, 2021.\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1906–1919, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.173. URL https://aclanthology.org/2020.acl-main.173.\n\nPaul Michel and Graham Neubig. MTNT: A testbed for machine translation of noisy text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 543–553, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1050. URL https://aclanthology.org/D18-1050.\n\nJishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deter-\n\nministic uncertainty: A simple baseline. arXiv e-prints, pp. arXiv–2102, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nKevin P. Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023. URL\n\nprobml.ai.\n\nVaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure\n\nmodes of out-of-distribution generalization. arXiv preprint arXiv:2010.15775, 2020.\n\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the sumIn Proceedings mary! topic-aware convolutional neural networks for extreme summarization. of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206.\n\nA Nguyen, J Yosinski, and J Clune. Deep neural networks are easily fooled: high confidence\n\npredictions for unrecognizable images. arxiv. arXiv preprint arXiv:1412.1897, 2014.\n\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019.\n\nAmy Pu, Hyung Won Chung, Ankur Parikh, Sebastian Gehrmann, and Thibault Sellam. Learning compact metrics for MT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 751–762, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.58. URL https://aclanthology.org/2021.emnlp-main.58.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified textto-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.\n\nMrinal Rawat, Ramya Hebbalaguppe, and Lovekesh Vig. Pnpood: Out-of-distribution detection for text classification via plug andplay data augmentation. arXiv preprint arXiv:2111.00506, 2021.\n\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. Comet: A neural framework for mt\n\nevaluation. arXiv preprint arXiv:2009.09025, 2020.\n\nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A DePristo, Joshua V Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. NeurIPS, 2019.\n\nJie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022, 2021.\n\nLukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gr ́egoire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert M ̈uller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756–795, 2021.\n\nMohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, and Mohammad Sabokrou. A unified survey on anomaly, novelty, open-set, and out-ofdistribution detection: Solutions and future challenges. arXiv preprint arXiv:2110.14051, 2021.\n\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with In Proceedings of the 55th Annual Meeting of the Association pointer-generator networks. for Computational Linguistics (Volume 1: Long Papers), pp. 1073–1083, Vancouver, Canada, doi: 10.18653/v1/P17-1099. URL July 2017. Association for Computational Linguistics. https://aclanthology.org/P17-1099.\n\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\n\nIn International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nYiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest\n\nneighbors. arXiv preprint arXiv:2204.06507, 2022.\n\nBrian Thompson and Matt Post. Automatic machine translation evaluation in many languages via\n\nzero-shot paraphrasing. arXiv preprint arXiv:2004.14564, 2020.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.\n\nJ ̈org Tiedemann. Parallel data, tools and interfaces in OPUS.\n\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pp. 2214–2218, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). URL http: //www.lrec-conf.org/proceedings/lrec2012/pdf/463 Paper.pdf.\n\nDustin Tran, Jeremiah Liu, Michael W Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, et al. Plex: Towards reliability using pretrained large model extensions. arXiv preprint arXiv:2207.07411, 2022.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam, Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016a. URL http://arxiv.org/abs/1609.08144.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine transarXiv preprint lation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016b.\n\nTim Z Xiao, Aidan N Gomez, and Yarin Gal. Wat zei je? detecting out-of-distribution translations\n\nwith variational transformers. arXiv preprint arXiv:2006.08344, 2020.\n\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML’20. JMLR.org, 2020.\n\nWenxuan Zhou, Fangyu Liu, and Muhao Chen. Contrastive out-of-distribution detection for pre-\n\ntrained transformers. arXiv preprint arXiv:2104.08812, 2021.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 THE OUTPUT QUALITY FOR SUMMARIZATION AND TRANSLATION DATASETS.\n\nTable A.1: The output quality for summarization and translation datasets. (a) Summarization quality (higher is better) for xsum model. ROUGE-1 is based on all samples in the test split per dataset, while human evaluation is based on 100 samples. The raw human evaluation rating ranges from 1 to 5. We normalized the score by dividing 5.0, and toke the median of the ratings over 3 raters to reduce inter-rater noise. The standard deviation among 3 ratings are reported in brackets. (b) Translation quality for different datasets (higher is better). All datasets are sub-sampled to 1000 sentence pairs.\n\n(a) Summarization\n\nDataset\n\nROUGE-1\n\nHuman evaluation\n\nxsum cnn dailymail reddit tifu samsum\n\n0.474 0.226 0.140 0.210\n\n0.698 (0.182) 0.624 (0.145) 0.450 (0.152) 0.376 (0.147)\n\n(b) Translation\n\nDataset\n\nBLEURT\n\nBLEU\n\nlaw nt2014 holdout ndt2015 ndd2015 medical IT MTNT sub Koran\n\n0.781 0.731 0.674 0.671 0.664 0.643 0.588 0.565 0.552 0.491\n\n53.8 39.8 41.8 37.9 30.9 34.2 28.3 32.0 22.8 12.9\n\nA.2 OOD SCORE AND PERPLEXITY ARE COMPLEMENTARY FOR PREDICTING OUTPUT\n\nQUALITY.\n\n(a) Summarization, ROUGE-1\n\n(b) Summarization, human rating\n\n(c) Translation, BLEURT\n\nFigure A.1: 2D plot between OOD and perplexity. The two scores are self-normalized by its percentile rank respectively. Each square corresponds to a subset of samples whose OOD and perplexity scores are within the percentile bin. The size of the square represents the size of the bin where the color indicates the quality of the model’s output. The OOD score and perplexity capture different properties of model outputs, and combining both scores can be beneficial for quality prediction.\n\n15\n\n102030405060708090100Percentile rank of perplexity10 20 30 40 50 60 70 80 90 100Percentile rank of output RMD0204060count0.00.20.40.6rouge1102030405060708090100Percentile rank of perplexity10 20 30 40 50 60 70 80 90 100Percentile rank of output RMD02468count10120.00.20.40.60.81.0human rating10 20 30 40 50 60 70 80 90 100Percentile rank of input RMD102030405060708090100Percentile rank of perplexity0.40.50.60.70.80.9bleurt050100150200countPublished as a conference paper at ICLR 2023\n\nA.3 AMAZON MECHANICAL TURK ASSESSMENT OF SUMMARY QUALITY\n\nA PEGASUSLARGE model fine-tuned on xsum was run on a random sample of 100 examples from the test split of four datasets: xsum, cnn dailymail, reddit tifu, samsum. Each example was rated for general summarization quality on a rating of 1-5 by 3 AMT workers using the template shown in Figure A.2. Workers were required to be Masters located in the US with greater than 95% HIT Approval Rate, with at least 1000 HITs approved and were paid $0.80 per rating.\n\nFigure A.2: AMT template for summarization human evaluation.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA.4 ALGORITHM FOR RMD OOD SCORES\n\nAlgorithm 1 Fitting Gaussians for input and output embeddings\n\n1: Input: CLM M with encoder fe and decoder fd trained on in-domain train set Din\n\n{(x, y)}. A large and background dataset such as C4 or ParaCrawl Dbg ˆy = M (x).\n\ntrain = train = {(x, ˆy)}, where\n\n2: Generate the input embeddings S in\n\ntrain = {z|fe(x), x ∈ Din\n\ntrain} and S bg\n\ntrain = {z|fe(x), x ∈\n\nDbg\n\ntrain}.\n\nusing S bg\n\ntrain.\n\n3: Fit a Gaussian distribution N (μz, Σz) using S in\n\ntrain, and a background Gaussian N (μz\n\n0 , Σz 0 )\n\n4: Similarly, generate output embeddings E in train}.\n\n{w|fd( ˆy), ˆy ∈ Dbg\n\n5: Fit a Gaussian distribution N (μw, Σw) using E in\n\ntrain = {w|fd(y), y ∈ Din\n\ntrain}, and E bg\n\ntrain =\n\ntrain and a background Gaussian N (μw\n\nδ , Σw δ )\n\nusing E bg\n\ntrain.\n\nAlgorithm 2 OOD score inference\n\n1: Input: In-domain test set Din\n\ntest = {(x, ˆy)}. OOD test set Dood\n\ntest = {(x, ˆy)}, where ˆy = M (x).\n\n2: Generate input embeddings S in 3: Compute input OOD score RMDinput(z) for z ∈ S in\n\ntest = {z|fe(x), x ∈ Din\n\ntest} and S ood\n\ntest = {z|fe(x), x ∈ Dood test , respectively. Compute AUROC\n\ntest}.\n\ntest and S ood\n\nbased on the input OOD scores.\n\n4: Similarly, generate output embeddings E in\n\ntest = {w|fd( ˆy), ˆy ∈ Din\n\n{w|fd( ˆy), ˆy ∈ Dood respectively. Compute AUROC based on the output OOD scores.\n\ntest}. Compute output OOD score RMDoutput(w) for w ∈ E in\n\ntest} and E ood\n\ntest = test and E ood test ,\n\nA.5 THE CONNECTION BETWEEN RMD AND BINARY CLASSIFIER\n\nRMD is a generative model based approach which assumes the distributions of the two classes are Gaussian, while the binary classifier is a discriminative model which learns the decision boundary between two classes. Though they have different settings, under certain condition, the Gaussian generative model can be reduced to a binary classifier. To see the connection, let us assume the label y = 0 if the sample is from in-domain, and y = 1 if the sample is from the general domain. Let us also assume the two classes have balanced sample size without loss of generality p(y = 1) = p(y = 0). Since the log-probability of log p(y = 1|z) can be rewritten using the Bayes rule log p(y = 1|z) = log p(z|y = 1) + log p(y = 1) − log p(z), the logit (log odds) can be written as,\n\nlogit = log\n\n(cid:18) p(y = 1|z) p(y = 0|z)\n\n(cid:19)\n\n= log p(y = 1|z) − log p(y = 0|z)\n\n= log p(z|y = 1) − log p(z|y = 0)\n\n= −\n\n1 2\n\n(MD(z; μy=1, Σy=1) − MD(z; μy=0, Σy=0)) + const.\n\nWhen Σ = Σy=1 = Σy=0, the equation can be further simplified as\n\nlogit = Σ−1(μy=1 − μy=0)T z −\n\n1 2\n\n= β1z + β0.\n\n(cid:0)μT\n\ny=1Σ−1μy=1 − μT\n\ny=0Σ−1μy=0\n\n(cid:1) + const.\n\nTherefore, when assuming the covariance matrices are identical for the two Gaussian distributions, the Gaussian generative model can be reduced to a binary classification model. However, our RMD does not assume the same covariance matrix in both distributions. We estimate the covariance matrix individually for each class. So our RMD is different from binary classifier, and it has higher model capacity than the binary classifier.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable A.2: AUROCs for OOD detection for comparing our proposed method with more baseline\n\nMeasure\n\ncnn dailymail\n\nnewsroom reddit tifu\n\nforumsum samsum\n\nNear Shift OOD\n\nFar Shift OOD\n\nKNN (α=100%, k=1000) MD RMD Binary logits\n\nNLI score Perplexity Mean(MSP) Energy score Ensemble using MC dropout (N =5) Ensemble using MC dropout (N =10) KNN (α=100%, k=1000) MD RMD Binary logits\n\nINPUT OOD\n\n0.743 0.799 0.930 0.959\n\nOUTPUT OOD\n\n0.469 0.665 0.616 0.592 0.768 0.774 0.791 0.933 0.962 0.982\n\n0.887 0.651 0.828 0.997\n\n0.440 0.424 0.343 0.460 0.496 0.497 0.860 0.944 0.958 0.989\n\n0.944 0.974 0.998 1.000\n\n0.709 0.909 0.877 0.960 0.970 0.976 0.948 0.985 0.998 1.000\n\n0.961 0.977 0.997 0.999\n\n0.638 0.800 0.715 0.899 0.937 0.947 0.926 0.973 0.993 0.998\n\n0.955 0.995 0.999 0.998\n\n0.743 0.851 0.826 0.981 0.944 0.956 0.968 0.985 0.998 0.997\n\nA.6 COMPARISON WITH MORE BASELINE METHODS\n\nAs we discussed in the related works, OOD detection problem was mainly studied in classification problems, and less studied in CLMs. Though it is not straight forward to extend classifier-based scores to CLMs especially for the input OOD detection, we would like to include as many possible methods as we can to present a comprehensive comparison for different methods.\n\nFor those methods which rely on classification head derived logits, MSP (Hendrycks & Gimpel, 2016), max-logit (Hendrycks et al., 2019), and energy score (Liu et al., 2020b), we simply consider the output decoding process as a sequence of classifications over tokens, and take the average of the corresponding score over the generated output tokens y1, . . . , yT as the output OOD scores. Therefore we added the following scores for CLMs,\n\n• Mean(MSP) − 1\n\n(cid:80)T\n\nT (cid:80)T\n\nt=1 p(yt|y<t, x). t=1 E(x, ft), where E(x, ft) = −τ log (cid:80)\n\nT\n\n• Energy score 1\n\nv∈V ef (yt=v|y<t,x)/τ , f (yt = v|y<t, x) is the logit corresponding to the v-th token at the t-th decoding step, V is the token-vocabulary, and τ is the temperature parameter. We set τ = 1 since the original paper (Liu et al., 2020b) suggested the energy score can be used parameter-free by simply setting τ = 1.\n\n• Ensemble estimation of the output perplexity from multiple Monte-Carlo dropout samples. Malinin & Gales (2020); Xiao et al. (2020) propose to turn on the MC dropout layer at the inference time and sample multiple times (N ) using different random seeds as a way to approximate the Bayesian neural networks. We follow their idea and generate multiple output sequences and use the averaged perplexity as the uncertainty score. Note that the inference time for ensemble based method is N times of that for the single model based score.\n\n• KNN-based OOD score. Sun et al. (2022) propose to use the distance to the k-th nearest neighbour in the training set in the embedding space as an OOD score. There are two hyper-parameters in the KNN-based method, α and k. α is the proportion of training data sampled for nearest neighbor calculation, and k refers to the k-th nearest neighbor. We use the optimal k = 1000 and α = 100 as suggested by the paper. We also normalize the embedding features since the paper showed the feature normalization is critical for good performance.\n\nMean(MSP), energy score, and ensembled perplexity score, are all derived from the logits of the tokens in output sequences, so they are output OOD scores. The KNN-based method can be applied for both input sequence embeddings and output sequence embeddings.\n\nTable A.2 shows the AUROCs for OOD detection for the above newly added baselines, as a comparison to our methods. First, the logits based output OOD scores, perplexity, mean(MSP), energy score, even the ensembled perplexity score which costs N times of the inference time, are in general not competitive with our proposed method RMD and Binary logits. Though the energy score\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable A.3: Comparison of the OOD detection performance using two different background data, ParaCrawl and C4 sentence.\n\nMeasure\n\nnt2014\n\nndd2015\n\nndt2015\n\nlaw\n\nmedical Koran\n\nIT\n\nsub\n\nWMT\n\nOPUS\n\nRMD (ParaCrawl) RMD (C4 sent)\n\nBinary logits (ParaCrawl) Binary logits (C4 sent)\n\nRMD (ParaCrawl) RMD (C4 sent)\n\nBinary logits (ParaCrawl) Binary logits (C4 sent)\n\nInput MD Output MD Perplexity COMET Prism\n\n0.798 0.833\n\n0.864 0.848\n\n0.786 0.818\n\n0.822 0.853\n\n0.534 0.609 0.570 0.484 0.445\n\nINPUT OOD\n\n0.863 0.911\n\n0.904 0.916\n\nOUTPUT OOD\n\n0.861 0.898\n\n0.865 0.919\n\n0.866 0.916\n\n0.904 0.916\n\n0.858 0.901\n\n0.860 0.925\n\nOTHER BASELINES\n\n0.671 0.733 0.496 0.514 0.504\n\n0.670 0.739 0.494 0.525 0.505\n\n0.389 0.269\n\n0.485 0.285\n\n0.355 0.259\n\n0.507 0.294\n\n0.511 0.482 0.392 0.435 0.459\n\n0.840 0.811\n\n0.813 0.808\n\n0.845 0.845\n\n0.783 0.809\n\n0.704 0.784 0.363 0.543 0.565\n\n0.957 0.954\n\n0.963 0.944\n\n0.939 0.953\n\n0.942 0.964\n\n0.737 0.838 0.657 0.632 0.716\n\n0.959 0.924\n\n0.928 0.918\n\n0.951 0.947\n\n0.890 0.901\n\n0.828 0.900 0.343 0.619 0.604\n\n0.969 0.985\n\n0.950 0.987\n\n0.959 0.979\n\n0.910 0.981\n\n0.900 0.935 0.359 0.518 0.577\n\nMTNT\n\n0.943 0.953\n\n0.963 0.976\n\n0.922 0.947\n\n0.931 0.975\n\n0.668 0.794 0.633 0.724 0.699\n\nis a bit better than perplexity and mean(MSP), and ensembled score is better than energy score, the performance gap between those methods and our proposed method is still big, especially for the near-OOD datasets. Second, KNN-based methods are not as good as MD and RMD either. Though it is possible that the optimal hyper-paramaters suggested by the paper may not be the optimal ones for our problem, searching for the optimal hyper-parameters requires a separate validation set. In contrast, our proposed methods have no hyperparameters.\n\nA.7 EFFECT OF THE CHOICE OF THE BACKGROUND DATASET\n\nOur principle for choosing the background data is to make it as general as possible. For summarization we use the C4 dataset, which contains a large amount of web crawl documents, to represent a broad range of topics. Similarly for translation, we use ParaCrawl dataset, which is also a large web crawl of sentences, because our translation model is a sentence to sentence model, unlike the summarization model that takes the document as the input. To further explore the effect of the background data on the performance, we split C4 documents into sentences and use that as the background data to compute the scores, and compare that with the version using ParaCrawl dataset. The OOD detection performance using C4 sentences is very similar to that using ParaCrawl, as shown in Table A.3. For example, ParaCrawl-based input OOD score has slightly better performance on medial, Koran, IT datasets, while C4 based input score is slightly better at the other datasets. Both are significantly better than the baseline methods, and both give the same ranking of datasets on their OOD-ness, so our conclusion remains. Those results verify that our method is robust to the choice of background data.\n\nA.8 ROC PLOTS FOR THE CORRESPONDING AUROC SCORES FOR OOD DETECTION\n\nTo better visualize the OOD detection performance, we present Figure A.3 to show the ROC plots for the corresponding AUROC scores for OOD detection in Table 1. Each of the OOD measures is used for separating the in-domain test data as negative and the OOD test data as positive sets. The AUROC is defined as the area under the ROC curves. The closer an ROC curve is to the upper left corner, the larger the AUROC value is. AUROC 1.0 means a perfect separation, and 0.5 means the two are not distinguishable. AUROC is independent of the choice of threshold, so it can be used for fair comparisons among methods.\n\nA.9 CORRELATION BETWEEN DIFFERENT SCORES AND THE QUALITY METRICS\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n(a) Input MD\n\n(b) Input RMD\n\n(c) Input Binary logits\n\n(d) Output MD\n\n(e) Output RMD\n\n(f) Output Binary logits\n\n(g) Perplexity\n\n(h) Mean(MSP)\n\n(i) NLI score\n\nFigure A.3: ROC plots for the corresponding AUROC scores in Table 1 for OOD detection in summarization\n\nTable A.4: Kendall’s τ correlation (p-value < 0.05 are greyed out) between various measures with human-judged quality of a PEGASUS xsum model decoded on summarization datasets. The “All” column shows the correlation when examples from all datasets are included. Note for negatively correlated scores (e.g. perplexity, OOD score), we take the negative value of the score for easier comparison. A few intra-dataset correlations have p-value < 0.05 due to the small sample size (only 100 examples per dataset were sent for human evaluation).\n\nMeasure\n\nMD RMD Binary Logits\n\nPerplexity (baseline) NLI score (baseline) MD RMD Binary logits\n\nPR sum (perplexity, input RMD) PR sum (perplexity, output RMD) PR sum (perplexity, input & output RMD) PR sum (perplexity, input binary logits) PR sum (perplexity, output binary logits) PR sum (perplexity, input & output binary logits) Lineare regression (perplexity, input & output)\n\nIn-domain Near Shift OOD cnn dailymail\n\nxsum\n\nFar Shift OOD\n\nreddit tifu\n\nsamsum\n\nAll\n\n-0.018 -0.033 -0.061\n\n0.186 0.308 -0.055 0.177 -0.100\n\n0.134 0.350 0.242 0.079 0.086 0.003 0.402\n\nSingle Score\n\nINPUT OOD 0.044 0.015 -0.022\n\nOUTPUT OOD 0.256 0.337 0.106 0.053 0.199\n\nCombined Score\n\n0.186 0.250 0.171 0.214 0.347 0.277 0.235\n\n20\n\n-0.017 0.017 0.028\n\n0.081 0.226 0.202 0.214 0.091\n\n0.082 0.168 0.158 0.126 0.114 0.127 0.170\n\n0.133 0.133 0.106\n\n0.068 0.132 0.352 0.314 0.026\n\n0.109 0.237 0.250 0.090 0.052 0.096 0.250\n\n0.328 0.336 0.233\n\n0.300 0.381 0.384 0.385 0.213\n\n0.358 0.415 0.401 0.322 0.330 0.307 0.422\n\n0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratecnn_dailymailnewsroom_allreddit_tifuforumsumsamsumPublished as a conference paper at ICLR 2023\n\nTable A.5: Kendall τ correlation (p-value < 0.05 are grayed out) between various measures and quality measured by BLEURT on translation datasets. For easier comparison, we negate the signs of the coefficients for measures that are expected to have negative correlation with BLEURT (e.g., OOD score). Within the same dataset, perplexity shows good correlation, but it deteriorates (with the exception of MTNT) as we move to more OOD datasets such as Koran.\n\nMeasure\n\nMD RMD Binary logits\n\nPerplexity (baseline) COMET (baseline) Prism (baseline) MD RMD Binary logits\n\nRR sum (perplexity, input RMD) PR sum(perplexity, output RMD) PR sum(perplexity, input & output RMD) PR sum(perplexity, input binary logits) PR sum(perplexity, output binary logits) PR sum(perplexity, input & output binary logits) Linear regression (perplexity, input & output)\n\nWMT\n\nOPUS\n\nholdout\n\nnt2014\n\nndd2015\n\nndt2015\n\nlaw medical Koran\n\nIT\n\nsub\n\nMTNT\n\nAll\n\nSingle Score\n\nINPUT OOD\n\n-0.131 0.091 0.116\n\n-0.129 0.049 0.141\n\nOUTPUT OOD\n\n0.337 0.397 0.329 -0.066 0.049 0.058\n\n0.352 0.402 0.337 -0.064 0.044 0.075\n\nCombined Score\n\n0.361 0.357 0.284 0.352 0.302 0.262 0.370\n\n0.351 0.359 0.264 0.372 0.314 0.288 0.355\n\n-0.117 0.115 0.162\n\n0.375 0.443 0.342 -0.048 0.095 0.114\n\n0.410 0.414 0.329 0.384 0.350 0.309 0.414\n\n-0.081 0.147 0.144\n\n0.309 0.184 0.184 -0.029 0.086 0.106\n\n0.321 0.323 0.291 0.323 0.318 0.300 0.318\n\n-0.171 0.197 0.124\n\n0.389 0.324 0.179 -0.096 0.135 0.094\n\n0.382 0.371 0.346 0.391 0.356 0.340 0.383\n\n0.041 0.013 -0.003\n\n0.224 0.253 0.188 0.032 -0.026 -0.036\n\n0.230 0.200 0.119 0.195 0.168 0.125 0.243\n\n-0.147 -0.071 0.025\n\n-0.093 -0.060 -0.071\n\n0.012 0.098 0.104\n\n0.222 0.359 0.192 -0.105 -0.077 -0.013\n\n0.227 0.225 0.297 0.174 0.286 0.151 0.041 -0.057 -0.056 0.061 -0.059 -0.012\n\n0.161 0.152 0.082 0.211 0.162 0.145 0.180\n\n0.154 0.164 0.084 0.111 0.127 0.053 0.119\n\n0.261 0.240 0.231 0.234 0.156 0.163 0.268\n\n-0.117 0.083 0.161\n\n0.341 0.414 0.370 -0.020 0.077 0.075\n\n0.354 0.350 0.290 0.359 0.293 0.287 0.367\n\n0.007 0.195 0.202\n\n0.286 0.336 0.301 0.083 0.170 0.151\n\n0.361 0.356 0.311 0.335 0.299 0.288 0.352\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nA.10 SELECTIVE GENERATION AND OUTPUT QUALITY PREDICTION\n\n(a) ROUGE-1 vs Abstention Rate\n\n(b) Survival count per dataset\n\nFigure A.4: (a) The summarization quality ROUGE-1 vs abstention curve for single scores, including input and output RMD OOD scores, output perplexity score, and NLI score, and combined scores, including linear regression machine learning model, percentile sum of RMD OOD scores and perplexity score. The corresponding area under the curve is in Table A.7. (b) The survival count of each dataset as the joint dataset is abstained. Each dataset is sub-sampled to 400 examples for this analysis.\n\nFigure A.5: The translation survival count of each dataset as the joint dataset is abstained. Complete results for Figure 5 (d).\n\n22\n\n0.00.10.20.30.40.50.60.7Abstention Rate0.2500.2750.3000.3250.3500.3750.4000.425rouge1Perplexity (Base)NLI (Base)Output RMDLinear Reg.PR sum0.00.20.40.60.81.0Abstention Rate050100150200250300350400Survival Countxsumcnn_dailymailnewsroom_allreddit_tifuforumsumsamsum0.00.20.40.60.81.0Abstention Rate02004006008001000Survival Countholdoutnt2014ndd2015ndt2015lawmedicalKoranITsubtitlesmtntPublished as a conference paper at ICLR 2023\n\nTable A.6: Area under the quality (human eval) vs abstention curve for summarization for various single scores and the proposed combined scores.\n\nMeasure\n\nArea under the quality (human eval) vs abstention curve\n\nMD RMD Binary logits\n\nPerplexity (baseline) NLI score (baseline) MD RMD Binary logits\n\nSingle Score\n\nInput OOD\n\nOutput OOD\n\nCombined Score\n\nPRsum(perplexity, input RMD) PRsum(perplexity, output RMD) PRsum(perplexity, input & output RMD) PRsum(perplexity, input binary logits) PRsum(perplexity, output binary logits) PRsum(perplexity, input & output binary logits) Linear regression (perplexity, input & output RMD)\n\n0.464 0.466 0.445\n\n0.458 0.469 0.469 0.474 0.441\n\n0.468 0.478 0.476 0.461 0.461 0.456 0.481\n\nTable A.7: Area under the quality (ROUGE-1) vs abstention curve for summarization for various single scores and the proposed combined scores.\n\nMeasure\n\nArea under the quality (rouge1) vs abstention curve\n\nMD RMD Binary logits\n\nPerplexity (baseline) NLI score (baseline) MD RMD Binary logits\n\nSingle Score\n\nInput OOD\n\nOutput OOD\n\nCombined Score\n\nPRsum(perplexity, input RMD) PRsum(perplexity, output RMD) PRsum(perplexity, input & output RMD) PRsum(perplexity, input binary logits) PRsum(perplexity, output binary logits) PRsum(perplexity, input & output binary logits) Linear regression (perplexity, input & output RMD)\n\n0.208 0.214 0.217\n\n0.221 0.207 0.219 0.221 0.207\n\n0.222 0.228 0.224 0.225 0.221 0.220 0.229\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nTable A.8: Area under the quality (BLEURT) vs abstention curve for translation using various single scores and the proposed combined scores.\n\nNames\n\nArea under the quality vs abstention curve\n\nSingle Score\n\nInput OOD\n\nOutput OOD\n\nMD RMD Binary logits\n\nPerplexity (baseline) Comet (baseline) Prism (baseline) MD RMD Binary logits\n\nCombined Score\n\nPRsum(perplexity, input RMD) PRsum(perplexity, output RMD) PRsum(perplexity, input & output RMD) PRsum(perplexity, input binary logits) PRsum(perplexity, output binary logits) PRsum(perplexity, input & output binary logits) Linear regression (ppx, input & output)\n\n0.583 0.623 0.621\n\n0.627 0.644 0.638 0.601 0.618 0.608\n\n0.647 0.646 0.641 0.639 0.632 0.633 0.645\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nA.11\n\nINVESTIGATION OF THE N-GRAM OVERLAP BETWEEN LAW DATASET AND IN-DOMAIN DATASETS\n\ndomain/split\n\noverall average\n\nn-gram overlap\n\nn = 1\n\nn = 2\n\nn = 3\n\nn = 4\n\nholdout nt2014 ndd2015 ndt2015 law medical Koran IT sub MTNT\n\n8.3 4.9 5.1 4.6 7.7 4.3 2.8 4.0 2.8 2.5\n\n45.4 39.0 40.7 39.0 48.8 33.5 32.6 35.9 38.6 31.4\n\n16.8 12.3 12.9 12.8 16.1 10.7 8.7 10.6 10.9 8.4\n\n4.8 2.7 2.7 2.6 4.2 2.4 1.4 2.2 1.4 1.2\n\n1.3 0.5 0.5 0.3 1.1 0.4 0.2 0.3 0.1 0.1\n\nTable A.9: n-gram overlap analysis between the various test sets including law and the in-domain training data, we observe that law has the highest unigram overlap rate (48.8%) and the second highest overall overlap (defined as the geometric mean) with the in-domain data.\n\nA.12 QUANTITATIVE ANALYSIS USING N-GRAM OVERLAP TO DETERMINE NEAR- AND\n\nFAR-OOD DATASETS IN SUMMARIZATION\n\nTo support our claim that the news related test datasets, cnn dailymail and newsroom are closer to the in-domain xsum than the other dialogue datasets reddit tifu, samsum, and forumsum, we compute the n-gram overlap between each of the test datasets and the in-domain dataset. We use Jaccard similarity score, J(A, B) = |A∩B| |A∪B| , where A and B are the set of n-gram in dataset A and dataset B, to measure the similarity between two datasets. Table A.10 shows the similarity scores based on 1 − 4 grams. It is clear to see that cnn dailymail and newsroom have significantly higher similarity with the in-domain xsum data than other three datasets. Therefore, we call the news-related datasets near-OOD and the other dialogue based datasets far- OOD.\n\ndomain/split\n\noverall average\n\nn-gram overlap\n\nxsum cnn dailymail newsroom reddit tifu forumsum samsum\n\n7.3 6.2 5.3 2.8 2.7 1.2\n\nn = 1\n\nn = 2\n\nn = 3\n\nn = 4\n\n32.4 31.1 28.8 17.2 18.0 10.4\n\n13.3 12.7 11.1 6.9 6.5 3.1\n\n4.6 4.0 3.3 1.8 1.6 0.7\n\n1.4 0.9 0.7 0.3 0.3 0.1\n\nTable A.10: Jaccard similarity based on n-gram overlap between the various test sets and the indomain xsum training data. We observe that the news-related datasets cnn dailymail and newsroom have significantly higher similarity scores with the in-domain xsum data than the other three OOD datasets reddit tifu, forumsum, and samsum.\n\nA.13 VISUALIZATION OF OOD SCORE ON SHIFTED DATASET\n\nWe explore how individual parts of an input text contribute to the OOD score, which can help us visualize which parts of the text are OOD. We define the OOD score of each sentence in the text using a leave-one-out strategy: For any given sentence, we compute the OOD score of the article with and without that sentence in it. The negative of the change in the OOD score after removing the sentence denotes the OOD score of that sentence. Intuitively, if removing the sentence decreases the overall OOD score, that sentence is assigned a positive OOD score and vice-versa. Figure A.6 illustrates an example where an article contains noise in the form of tweets with emojis, and the OOD scoring mechanism described above assigns positive OOD scores to those tweets and negative scores to the main text.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nFigure A.6: OOD score can be attributed to individual sentences to highlight the out-of-domain noisy parts of text (red denotes out-of-domain and blue denotes in-domain text), e.g. tweets present in articles scraped from internet. Example taken from Newsroom dataset.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nA.14 SUMMARIZATION EXAMPLES WITH LOW/ HIGH PREDICTED QUALITY SCORES\n\nBesides the quantitative results, here we show a few real examples to better demonstrate how well our predicted quality score helps for selective generation on out-of-distribution examples. The model here was fine-tuned on xsum but inference was run on examples from cnn dailymail.\n\nFigure A.7, A.8, and A.9 show 3 examples in cnn dailymail that have the highest PRsum(perplexity, output RMD) scores that predict for low quality summaries.\n\nFigure A.10, A.11, and A.12 show 3 examples in cnn dailymail PRsum(perplexity, output RMD) scores that predict for high quality summaries.\n\nthat have the lowest\n\nDocument: A man trying to elude police jumped into a Missouri creek overnight wearing only his underwear – but his daring gambit did not pay off. Responding officers and firefighters followed the fugitive into the murky waters of Brush Creek in Kansas City and fished him out early Friday morning. The 38year-old suspect has been taken to an area hospital to be treated for injuries to his arm and leg. He may face charges in connection to a hit-and-run crash. Escape by water: A 38-year-old man stripped down to his skivvies and jumped into Brush Creek in Kansas City, Missouri, after being stopped by police. Up Brush Creek without a paddle: The suspect reached the middle of the creek and spent 10-15 minutes swimming back and forth. According to a Kansas City Police Department’s arrest report, officers were called to a gas station in the 4600 block of Prospect at around 2am after receiving complaints from neighbors about a car blasting loud music. The report states that when police approached the car, a grey 2007 Infinity, and asked to see the driver’s license, the man smiled, said, ‘I’m out!’ and took off from the scene. The Infinity promptly smashed into the north side of the Brush Creek bridge, after which the driver got out of the mangled car and jumped into the water. Police say the 38-year-old suspect stripped down to his underwear and spent 10-15 minutes swimming in chest-deep water, with officers waiting for him on north and south sides of the creek. Surrounded: When firefighters tried to pull him out, he threatened them with a log. Fish out of water: Police officers armed with a BB gun went after the nighttime bather and apprehended him. The bather was complaining of a broken leg, according to Fox4KC, so the Kansas City Fire Department’s water rescue crew were sent in to fish him out. But the half-naked man in the water was not going to go quietly. ‘The suspect picked up a large log and started swinging it at the firemen so they backed off as to not escalate the situation,’ the arrest report states. That is when uniformed police officers armed with a BB gun followed the man into the creek, got him in a choke hold and pulled him out of the creek. Police suspect the man may have been under the influence of drugs or alcohol. Prelude: Before he jumped in the water, the 38-year-old driver fled from police and smashed his 2007 Infinity into a bridge. Police suspect the man may have been under the influence of drugs or alcohol at the time. As of Friday morning, the 38-year-old has not been formally charged with any crime. Reference Summary: The 38-year-old suspect was questioned by Kansas City police after neighbors complained he was blasting music in his 2007 Infinity. Instead of handing over his ID, driver smiled, said ’I’m out!’ and took off. After crashing into bridge, the man stripped down to his underwear and jumped into Brush Creek. It took cops armed with a BB gun 15 minutes to fish out the fugitive. Model Summary: All images are copyrighted. Human rating score (↑ means high quality): 0.2 PRsum(perplexity, output RMD) (↓ means high quality): 0.67\n\nFigure A.7: Examples in cnn dailymail that have the highest PRsum(perplexity, output RMD) scores that predict for low quality summaries.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nDocument: A crisp fan who gets through 42 bags in a week has discovered a skull-shaped deep-fried potato snack in one of his packets. Barry Selby, 54, who lives with his dog in Poole, Dorset, was eating a bag of cheese and onion crisps when he made the bizarre discovery, which appears to be a profile of a human skull. The floor-fitter has decided to keep the two inches tall by two-and-a-half inches wide snack as he believes it is far more impressive than other oddly-shaped examples he has seen on the internet. Scroll down for video. Spooky find: Barry Selby was eating a bag of Tesco cheese and onion crisps when he found the ’skull’ snack. Mr Selby said: ’I was shocked when I found it. I was just eating a bag of cheese and onion crisps from Tesco and when I pulled it out it did take me back a bit. ’I thought it was worth keeping as I don’t think I will ever find one like it again. It must have been a very weird-shaped potato. ’It’s about two inches tall and two-and-a-half inches wide and it’s in perfect detail, it even has an eye socket. ’I sometimes give my dog, Max, crisps in a bowl, so it’s lucky he didn’t have this packet or I wouldn’t have found it. Weird snack: Mr Selby has decided to keep the unusual find, which appears to show a jaw, nose and eye. Comparison: The 54-year-old said he was ’shocked’ to make the discovery, although it is not his first. In the 1990s he came across a 3D heart-shaped crisp, which he kept until it broke. And it’s not the first odd-shaped snack he has come across - in the 1990s he found a crisp shaped like a 3D heart, which he kept for several years until it broke. But he says this find was different: ’This one was a big one. I just thought ”wow” and wanted to share it. ’I’ve been keeping it on top of my computer in the front room, but it should be in a protective box really. ’I’m going to keep it forever, it’s just so spooky. I looked on the internet for other funny-shaped crisps but this is a one-off.’ Reference Summary: Barry Selby from Dorset was eating bag of Tesco cheese and onion crisps. The 54-year-old discovered a snack shaped like profile of the human skull. He said he was ’shocked’ with the find and has decided to ’keep it forever’ It’s not his first weird food find - he once discovered a heartshaped crisp. Model Summary: All images are copyrighted. Human rating score (↑ means high quality): 0.2 PRsum(perplexity, output RMD) (↓ means high quality): 0.66\n\nFigure A.8: Examples in cnn dailymail that have the highest PRsum(perplexity, output RMD) scores that predict for low quality summaries.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nDocument: Last week she was barely showing – but Demelza Poldark is now the proud mother to the show’s latest addition. Within ten minutes of tomorrow night’s episode, fans will see Aidan Turner’s dashing Ross Poldark gaze lovingly at his new baby daughter. As Sunday night’s latest heartthrob, women across the country have voiced their longing to settle down with the brooding Cornish gentleman – but unfortunately it seems as if his heart is well and truly off the market. Scroll down for video. Last week she was barely showing – but Demelza Poldark is now the proud mother to the show’s latest addition. He may have married his red-headed kitchen maid out of duty, but as he tells her that she makes him a better man, audiences can have little doubt about his feelings. What is rather less convincing, however, is the timeline of the pregnancy. With the climax of the previous episode being the announcement of the pregnancy, it is quite a jump to the start of tomorrow’s instalment where Demelza, played by Eleanor Tomlinson, talks about being eight months pregnant. Just minutes after – once again without any nod to the passing of time – she is giving birth, with the last month of her pregnancy passing in less than the blink of an eye. With the climax of the previous episode being the announcement of the pregnancy, it is quite a jump to the start of tomorrow’s instalment where Demelza, played by Eleanor Tomlinson, talks about being eight months pregnant. As Sunday night’s latest heartthrob, women across the country have voiced their longing to settle down with Poldark – but unfortunately it seems as if his heart is well and truly off the market. Their fast relationship didn’t go unnoticed by fans. One posted on Twitter: ‘If you are pregnant in Poldark times expect to have it in the next 10 minutes’ It is reminiscent of the show’s previous pregnancy that saw Elizabeth, another contender for Ross’s affection, go to full term in the gap between two episodes. This didn’t go unnoticed by fans, who posted on Twitter: ‘Poldark is rather good, would watch the next one now. Though if you are pregnant in Poldark times expect to have it in the next 10 minutes.’ Reference Summary: SPOILER ALERT: Maid gives birth to baby on Sunday’s episode. Only announced she was pregnant with Poldark’s baby last week. Model Summary: It’s all change in the world of Poldark. Human rating score (↑ means high quality): 0.4 PRsum(perplexity, output RMD) (↓ means high quality): 0.62\n\nFigure A.9: Examples in cnn dailymail that have the highest PRsum(perplexity, output RMD) scores that predict for low quality summaries.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nDocument: Rangers boss Stuart McCall says he is already working on a dossier of signing targets for next season - even though he may not be around to parade them. The interim Ibrox manager still does not know if he will be in charge beyond the current campaign after being lured back to his old club to kickstart their faltering promotion bid. So far, everything is going to plan with Gers second in the Scottish Championship table and destined for a semi-final play-off slot. Stuart McCall says he is already looking at transfer targets for next season, though he may not be at Rangers. But with 12 players out of contract, McCall knows the Light Blues will need to strengthen if they have any chance of keeping pace with rivals Celtic next season - if they go up - and is already piecing together a wish list of potential new arrivals. He said: ’I’ve been speaking to a lot of agents and putting things in place for if and when... Even if I’m not here, if I’m getting players put to me who would like to come to Rangers regardless of the manager, then we build a little portfolio of positions that we will be needing next year. ’It’s not a case of us standing still and then thinking come June 1, ’Oh we need to get into action’. ’No, there are a lot of agents who come to us and we build a little dossier of players that as a staff, we think will be good for next season, regardless of what league we are in. ’It would be slightly naive [if we were not doing that]. If I’m in charge or not, I still want the club to do well and I will put my view across to the board on who I think should be coming into the club and who should be here.’ McCall is compiling a dossier on targets as he looks to put the club in the best possible position. Rangers have operated a haphazard transfer policy since re-emerging from the embers of liquidation. The club’s team of scouts were jettisoned under the disastrous Craig Whyte regime and former boss Ally McCoist was largely forced to turn to a list of former Ibrox servants he had personal knowledge of when trying to bolster his squad. But McCall revealed the club’s new board are now starting the process of re-establishing their spying network - albeit on a smaller level than before. ’I think there has been discussions behind the scenes with different people,’ said the former Motherwell boss. ’I don’t think we are at the stage where we were 10 or 15 years ago where we were aiming to get into the Champions League and bringing players in for three and four million yet. ’I don’t think Rangers will be at the stage yet next year where we need international scouts everywhere. Rangers have expanded their scouting network after a haphazard system over the past few years. ’But certainly a scouting network needs to be put in place. ’Having said that, I spoke to Craig Levein at Hearts and they do a lot of their scouting with [online service] Wyscout. When I brought Henrik Ojamaa in at Motherwell, that was after I’d seen a clip of him on YouTube. I sold him for £350,000 after signing him for nothing. That was great. ’So you can still do your own background work. Personally I would always like to see the player myself. I’ve only ever signed one player without watching him first and slightly regretted it. ’So yeah we need a scouting network but at this moment where Rangers are, not to the extent where we have scouts all over Europe.’ McCall admitted he still does not know if he will rejoin Gordon Strachan’s Scotland staff for the June 13 Euro 2016 qualifier with Ireland in Dublin. And he also confessed to uncertainties ahead of Saturday’s match with Falkirk. McCall’s side are still in line for promotion, sitting in the play-off positions in the Scottish Championship. Peter Houston’s Bairns - five points behind fourth-placed Queen of the South with two games to play - need an unlikely series of results to make the play-offs but McCall says that raises more questions than answers. He said: ’Housty is a wily old fox who has done terrifically well in his career so I don’t know what to expect. ’It will take a difficult set of results for them to get into the play-offs so I don’t know if they will come here and think the pressure is off and play care free. ’They don’t lose many goals so we may have to be patient through the 90 minutes. We have had a couple of decent results against them but they have capable players and we will need to be at our best.’ Reference Summary: Rangers are currently second in the Scottish Championship. Stuart McCall’s side are in pole position to go up via the play-offs. But McCall is still not certain of his future at the club next season. Rangers boss says he is still trying to build the squad for next year. Rangers have begun to expand their scouting after several poor years. Model Summary: Stuart McCall says he is already looking at transfer targets for next season, though he may not be at Rangers. Human rating score (↑ means high quality): 0.8 PRsum(perplexity, output RMD) (↓ means high quality): 0.10\n\nFigure A.10: Examples in cnn dailymail that have the lowest PRsum(perplexity, output RMD) scores that predict for high quality summary.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nDocument: An Alberta student who’d accidentally left his headlights on all day was greeted by what may have been the world’s friendliest note from a stranger when he returned to his car. But Derek Murray, a University of Alberta law student, found more than just the note that cold November day in Edmonton–he also found an extension cord and battery charger left by the stranger to bring his dead Acura back to life. Now that Murray’s life-affirming tale has now gone viral, he says ’It just shows you how such a pure act of kindness from one person can just spread through everyone and help make everyone’s day a little brighter.’ Good Samaritan: A friendly stranger left this unbelievably friendly letter to Alberta law student Derek Murray in order to help him get his car started after he left the headlights on all day. At first, though, he assumed the letter was from an angry fellow motorist, he told the National Post. ’When I first saw the note, I was expecting it to be an angry letter from someone telling me not to park there. Instead, I got someone just totally brightening my day. My day could have been ruined but, because of this guy, it was the highlight of my day.’ The note reads, in part:. I noticed you left your lights on. The battery will probably not have enough charge to start your vehicle. I left a blue extension cord on the fence and a battery charger beside the fence in the cardboard box. If you know how to hook it up, use it to start your car. What followed was a detailed explanation of how to use the equipment. ’Sure enough,’ Derek recalled to the National Post, ’I looked over at the house my car was parked beside, and there was a blue extension cord plugged into an outlet behind the guy’s house with a battery charger right there beside it.’ Derek was able to get his car started, but when he rang the good Samaritan’s doorbell, there was no answer. So, Derek left his own note as a thank you for the kind gesture. He later snapped a photo of the stranger’s friendly note to post to Facebook, where it has now gone viral. The note has been viewed millions of times and even Edmonton Mayor Don Iveson retweeted the photo. Derek snapped a photo of the note for Facebook and it has since gone viral. e ’It just shows you how such a pure act of kindness from one person can just spread through everyone and help make everyone’s day a little brighter,’ Derek said. Reference Summary: Derek Murray, a University of Alberta law student, could have had his day ruined by the mistake by a stranger’s kindness brightened it up. Murray posted his story and the note online and the random act of kindness has now gone viral. Model Summary: A Canadian student who accidentally left his headlights on all day was greeted by what may have been the world’s friendliest note from a stranger when he returned to his car. Human rating score (↑ means high quality): 0.8 PRsum(perplexity, output RMD) (↓ means high quality): 0.11\n\nFigure A.11: Examples in cnn dailymail that have the lowest PRsum(perplexity, output RMD) scores that predict for high quality summary.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nDocument: Bayern Munich had to make do without FOUR important first-team stars as Pep Guardiola’s side attempted to overturn a 3-1 deficit against Porto on Tuesday night. Injured quartet Franck Ribery, Mehdi Benatia, David Alaba and Arjen Robben were forced to watch on from the sidelines as the German giants bid to reach the Champions League semi-finals. However, the absence of Robben and Co appeared to make no difference as Bayern raced into a 5-0 lead at half-time before claiming a 6-1 victory to win the tie 7-4 on aggregate. Injured trio Franck Ribery, Mehdi Benatia and David Alaba chat ahead of Bayern’s clash with Porto. Injured Ribery acknowledges a steward before taking a seat at the Allianz Arena on Tuesday night. Ribery looks on as former Roma defender Benatia chats with the France international in the dugout. While Ribery, Benatia and Alaba chatted in the home dugout ahead of kick-off, Holland international Arjen Robben was in front of the mic doing some punditry alongside Bayern goalkeeping legend Oliver Kahn. Ribery missed the game after failing to recover from a recent ankle injury while former Roma defender Benatia faces another two weeks out with a groin problem. Robben was unavailable for the encounter with an abdominal injury. David Alaba, meanwhile, is set for a month on the sidelines having partially ruptured knee ligaments playing for Austria at the start of April. Bayern had just 14 fit players to choose from against Porto in the first leg but tore the Portuguese giants apart at the Allianz Arena to progress. Holland international Arjen Robben was pictured doing punditry alongside Bayern legend Oliver Kahn (right) Bayern Munich wideman Robben was unavailable for the Champions League clash with an abdominal injury. Reference Summary: Bayern Munich beat Porto 6-1 at the Allianz Arena on Tuesday night. German giants were without Franck Ribery, David Alaba and Mehdi Benatia. Arjen Robben was also sidelined and did some punditry for the tie. Model Summary: Arjen Robben, Mehdi Benatia, Franck Ribery and David Alaba all missed Bayern Munich’s Champions League quarter-final second leg against Porto. Holland international Arjen Robben was pictured doing punditry alongside Bayern legend Oliver Kahn (right) Bayern Munich wideman Robben was unavailable for the Champions League clash with an abdominal injury. Human rating score (↑ means high quality): 0.8 PRsum(perplexity, output RMD) (↓ means high quality): 0.11\n\nFigure A.12: Examples in cnn dailymail that have the lowest PRsum(perplexity, output RMD) scores that predict for high quality summary.\n\n32",
    "reference": "# Summary Of The Paper\n\nConditional language models often perform poorly on out-of-domain (OOD) data. This might be a reason why they shouldn't be tasked with generating anything for OOD examples at all: generations might be of poor quality and even more unpredictable than normal outputs. In order to predict which examples a conditional language model can be safely applied to, the authors propose a score to detect OOD examples. The score they introduce is Mahalanobis distance, either using the input or the output embeddings. They further show that the latter can be combined with perplexity to obtain an even stronger score.\n\nThe authors perform two types of experiments (on summarization and translation). First, they show that their score can detect OOD data more reliably than baselines. Second, they show that their score can also indicate low-quality generations.\n\n# Strength And Weaknesses\n\nStrengths:\n- The paper has a clear motivation and succeeds at what it's trying to do.\n- The experiments are well designed.\n\nWeaknesses:\n- Nothing major.\n\nThere are a couple of typos.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very clear and its quality is high. It's also novel, though inspired by a similar score for classification tasks. \nThe experiments seem to be reproducible.\n\n# Summary Of The Review\n\nI believe this paper will be useful for the community. It should get accepted.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nNEURAL DESIGN FOR GENETIC PERTURBATION EXPERIMENTS\n\nAldo Pacchiano Microsoft Research NYC apacchiano@microsoft.com\n\nDrausin Wulsin & Robert A. Barton & Luis Voloch Immunai {drausin,robert.barton,luis}@immunai.com\n\nABSTRACT\n\nThe problem of how to genetically modify cells in order to maximize a certain cellular phenotype has taken center stage in drug development over the last few years (with, for example, genetically edited CAR-T, CAR-NK, and CAR-NKT cells entering cancer clinical trials). Exhausting the search space for all possible genetic edits (perturbations) or combinations thereof is infeasible due to cost and experimental limitations. This work provides a theoretically sound framework for iteratively exploring the space of perturbations in pooled batches in order to maximize a target phenotype under an experimental budget. Inspired by this application domain, we study the problem of batch query bandit optimization and introduce the Optimistic Arm Elimination (OAE) principle designed to find an almost optimal arm under different functional relationships between the queries (arms) and the outputs (rewards). We analyze the convergence properties of OAE by relating it to the Eluder dimension of the algorithm’s function class and validate that OAE outperforms other strategies in finding optimal actions in experiments on simulated problems, public datasets well-studied in bandit contexts, and in genetic perturbation datasets when the regression model is a deep neural network. OAE also outperforms the benchmark algorithms in 3 of 4 datasets in the GeneDisco experimental planning challenge.\n\n1\n\nINTRODUCTION\n\nWe are inspired by the problem of finding the genetic perturbations that maximize a given function of a cell (a particular biological pathway or mechanism, for example the proliferation or exhaustion of particular immune cells) while performing the least number of perturbations required. In particular, we are interested in prioritizing the set of genetic knockouts (via shRNA or CRISPR) to perform on cells that would optimize a particular scalar cellular phenotype. Since the space of possible perturbations is very large (with roughly 20K human protein-coding genes) and each knockout is expensive, we would like to order the perturbations strategically so that we find one that optimizes the particular phenotype of interest in fewer total perturbations than, say, just brute-force applying all possible knockouts. In this work we consider only single-gene knockout perturbations since they are the most common, but multi-gene perturbations are also possible (though considerably more technically complex to perform at scale). While a multi-gene perturbation may be trivially represented as a distinct (combined) perturbation in our framework, we leave for future work the more interesting extension of embedding, predicting, and planning these multi-gene perturbations using previously observed single-gene perturbations.\n\nWith this objective in mind we propose a simple method for improving a cellular phenotype under a limited budget of genetic perturbation experiments. Although this work is inspired by this concrete biological problem, our results and algorithms are applicable in much more generality to the setting of experimental design with neural network models. We develop and evaluate a family of algorithms for the zero noise batch query bandit problem based on the Optimistic Arm Elimination principle (OAE). We focus on developing tractable versions of these algorithms compatible with neural network function approximation.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nDuring each time-step OAE fits a reward model on the observed responses seen so far while at the same time maximizing the reward on all the arms yet to be pulled. The algorithm then queries the batch of arms whose predicted reward is maximal among the arms that have not been tried out.\n\nWe conduct a series of experiments on synthetic and public data from the UCI Dua & Graff (2017) database and show that OAE is able to find the optimal “arm\" using fewer batch queries than other algorithms such as greedy and random sampling. Our experimental evaluation covers both neurally realizable and not neurally realizable function landscapes. The performance of OAE against benchmarks is comparable in both settings, demonstrating that although our presentation of the OAE algorithm assumes realizability for the sake of clarity, it is an assumption that is not required in practice. In the setting where the function class is realizable i.e. the function class F used by OAE contains the function generating the rewards, and the evaluation is noiseless we show two query lower bounds for the class of linear and 1−Lipshitz functions.\n\nWe validate OAE on the public CMAP dataset Subramanian et al. (2017), which contains tens of thousands of genetic shRNA knockout perturbations, and show that it always outperforms a baseline and almost always outperforms a simpler greedy algorithm in both convergence speed to an optimal perturbation and the associated phenotypic rewards. These results illustrate how perturbational embeddings learned from one biological context can still be quite useful in a different biological context, even when the reward functions of these two contexts are different. Finally we also benchmark our methods in the GeneDisco dataset and algorithm suite (see Mehrjou et al. (2021)) and show OAE to be competitive against benchmark algorithms in the task of maximizing HitRatios.\n\n2 RELATED WORK\n\nBayesian Optimization The field of Bayesian optimization has long studied the problem of optimizing functions severely limited by time or cost Jones et al. (1998). For example, Srinivas et al. (2009) introduce the GP-UCB algorithm for optimizing unknown functions. Other approaches based on adaptive basis function regression have also been used to model the payoff function as in Snoek et al. (2015). These algorithms have been used in the drug discovery context. Mueller et al. (2017) applied Bayesian optimization to the problem of optimizing biological phenotypes. Very recently, GeneDisco was released as a benchmark suite for evaluating active learning algorithms for experiment design in drug discovery Mehrjou et al. (2021). Perhaps the most relevant to our setting are the many works that study the batch acquisition setting in Bayesian active learning and optimization such as Kirsch et al. (2019); Kathuria et al. (2016) and the GP − BUCB algorithm of Desautels et al. (2014). In this work we move beyond the typical parametric and Bayesian assumptions from these works towards algorithms that work in conjunction with neural network models. We provide guarantees for the no noise setting we study based on the Eluder dimension Russo & Van Roy (2013).\n\nParallel Bandits Despite its wide applicability in many scientific applications, batch learning has been studied relatively seldom in the bandit literature. Despite this, recent work (Chan et al., 2021) show that in the setting of contextual linear bandits (Abbasi-Yadkori et al., 2011), the finite sample complexity of parallel learning matches that of sequential learning irrespective of the batch size provided the number of batches is large enough. Unfortunately, this is rarely the regime that matters in many practical applications such as drug development where the size of the experiment batch may be large but each experiment may be very time consuming, thus limiting their number. In this work we specifically address this setting in our experimental evaluation in Section E.\n\nStructure Learning Prior work in experiment design tries to identify causal structures with a fixed budget of experiments Ghassami et al. (2018). Scherrer et al Scherrer et al. (2021) proposes a mechanism to select intervention targets to enable more efficient causal structure learning. Sussex et al. (2021) extend the amount of information contained in each experiment by simultaneously intervening on multiple variables. Causal matching, where an experimenter can perform a set of interventions aimed to transform the system to a desired state, is studied in Zhang et al. (2021).\n\nNeural Bandits Methods such as Neural UCB and Shallow Neural UCB Zhou et al. (2020); Xu et al. (2020) are designed to add an optimistic bonus to model predictions of a nature that can be analytically computed as is extremely reminiscent of the one used in linear bandits (Auer, 2002; Dani et al., 2008), thus their theoretical validity depends on the ‘linearizing’ conditions to hold. More\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nrecently (Pacchiano et al., 2021b) have proposed the use of pseudo-label optimisim for the Bank Loan problem where they propose an algorithm that adds optimism to neural network predictions through the addition of fake data and is only analyzed in the classification setting. Our algorithms instead add optimism to their predictions. The later is achieved via two methods, either by explicitly encouraging it to fit a model whose predictions are large in unseen data, or by computing uncertainties.\n\nActive Learning Active learning is relatively well studied problem Settles (2009); Dasgupta (2011); Hanneke et al. (2014) particularly in the context of supervised learning. See for example Balcan et al. (2009), Dasgupta et al. (2007), Settles (2009), Hanneke et al. (2014). There is a vast amount of research on active learning for classification (see for example Agarwal (2013), Dekel et al. (2010) and Cesa-Bianchi et al. (2009) where the objective is to learn a linearly parameterized response model P(y|x). Broadly speaking there are two main sample construction approaches, diversity Sener & Savarese (2017); Geifman & El-Yaniv (2017); Gissin & Shalev-Shwartz (2019) and uncertainty sampling Tong & Koller (2001); Schohn & Cohn (2000); Balcan et al. (2009); Settles et al. (2007), successful in the large Guo & Schuurmans (2007); Wang & Ye (2015); Chen & Krause (2013); Wei et al. (2015); Kirsch et al. (2019) and small batch sizes regimes respectively. Diversity sampling methods produce spread out samples to better cover the space while uncertainty-based methods estimate model uncertainty to select what points to label. Hybrid approaches are common as well. A common objective in the active learning literature is to collect enough samples to produce a model that minimizes the population loss over the data distribution. This is in contrast with the objective we study in this work, which is to find a point in the dataset with a large response. There is a rich literature dedicated to the development of active learning algorithms for deep learning applications both in the batch and single sample settings Settles et al. (2007); Ducoffe & Precioso (2018); Beluch et al. (2018); Ash et al. (2021).\n\n3 PROBLEM DEFINITION\n\nLet y⋆ : A → R be a response function over A ⊂ Rd. We assume access to a function class F ⊂ Fun(A, R) where Fun(A, R) denotes the set of functions from A to R. Following the typical online learning terminology we call A the set of arms. In this work we allow A to be infinite, although we only consider finite A in practice.\n\nIn our setting the experiment designer (henceforth called the learner) interacts with y⋆ and A in a sequential manner. During the t−th round of this interaction, aided by F and historical query and response information the learner is required to query a batch of b ∈ N arms {at,i}b i=1 ⊂ A and observe noiseless responses {yt,i = y⋆(at,i)}b i=1 after which these response values are added to the historical dataset Dt+1 = Dt ∪ {(at,i, yt,i)}b\n\ni=1.\n\nIn this work we do not assume that y⋆ ∈ F. Instead we allow the learner access to a function class F to aid her in producing informative queries. This is a common situation in the setting of neural experiment design, where we may want to use a DNN model to fit the historical responses and generate new query points without prior knowledge of whether it accurately captures y⋆. Our objective is to develop a procedure that can recover an ‘almost optimal’ arm a ∈ A in the least number of arm pulls possible. We consider the following objective,\n\nτ −quantile optimality. Find an arm aτ ∈ A belonging to the top τ −quantile1 of {y⋆(a)}a∈A.\n\nAlthough ε−optimality (find an arm aε ∈ A such that y⋆(aε) + ε ≥ maxa∈A y⋆(a) for ε ≥ 0) is the most common criterion considered in the optimization literature, for it to be meaningful it requires knowledge of the scale of maxa∈A y⋆(a). In some scenarios this may be hard to know in advance. Thus in our experiments we focus on the setting of τ −quantile optimality as a more relevant practical performance measure. This type of objective has been considered by many works in the bandit literature (see for example Szorenyi et al. (2015); Zhang & Ong (2021)). Moreover, it is a measure of optimality better related to practical objectives used in experiment design evaluation, such as hit ratio in the GeneDisco benchmark library Mehrjou et al. (2021). We show in Section E that our algorithms are successful at producing almost optimal arms under this criterion after a small number of queries. The main challenge we are required to overcome in this problem is designing a smart choice of batch\n\n1In the case of an infinite set A quantile optimality is defined with respect to a measure over A.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nqueries {at,i}B and zooming into others that have shown promising rewards.\n\ni=1 that balances the competing objectives of exploring new regions of the arm space\n\nIn this work we focus on the case where the observed response values y⋆(a) of any arm a ∈ A are noiseless. In the setting of neural perturbation experiments the responses are the average of many expression values across a population of cells, and thus it is safe to assume the observed response is almost noiseless. In contrast with the noisy setting, when the response is noiseless, querying the same arm twice is never necessary. We leave the question on how to design algorithms for noisy responses in the function approximation regime for future work. although note it can be reduced to our setting if we set the exploitation round per data point sufficiently large.\n\nEvaluation. After the queries ∪t i=1 the learner will output a candidate approximate optimal arm ˆat among all the arms whose labels she has queried (all arms in Dt+1) by considering (ˆat, ˆyt) = arg max(a,y)∈Dt+1 y, the point with the maximal observed reward so far. Given a quantile value τ we measure the performance of our algorithms by considering the first timestep tτ first where a τ −quantile optimal point ˆat was proposed.\n\nl=1{al,i}b\n\n4 OPTIMISTIC ARM ELIMINATION\n\nWith these objectives in mind, we introduce a family of algorithms based on the Optimistic Arm Elimination Algorithm (OAE) principle. We call Ut to the subset of arms yet to be queried by our algorithm. At time t any OAE algorithm produces a batch of query points of size b from2 Ut. Our algorithms start round t by fitting an appropriate response predictor (cid:101)ft : Ut → R based on the historical query points Dt and their observed responses so far. Instead of only fitting the historical responses with a square loss and produce a prediction function (cid:101)ft, we encourage the predictions of (cid:101)ft to be optimistic on the yet-to-be-queried points of Ut.\n\nWe propose two tractable ways of achieving this. First by fitting a model (or an ensemble of models) t to the data in Dt and explicitly computing a measure of uncertainty ̃ut : Ut → R of its predictions (cid:101)f o on Ut. We define the optimistic response predictor (cid:101)ft(a) = (cid:101)f o t (a) + ̃ut(a). Second, we achieve this by defining (cid:101)ft to be the approximate solution of a constrained objective,\n\n(cid:101)ft ∈ arg max f ∈F\n\nA(f, Ut)\n\ns.t.\n\n(cid:88)\n\n(a,y)∈Dt\n\n(f (a) − y)2 ≤ γt.\n\n(1)\n\nwhere γt a possibly time-dependent parameter satisfying γt ≥ 0 and A(f, U) is an acquisition objective tailored to produce an informative arm (or batch of arms) from Ut. We consider a couple a∈U (max(0, f (a)))p of acquisition objectives Aavg(f, U) = 1 for some p > 0 and Asoftmax(f, U) = log (cid:0)(cid:80) a∈U f (a). An important acquisition functions of theoretical interest, although hard to optimize in practice are Amax(f, U) = maxa∈U f (a) and its batch version Amax,b(f, U) = maxB⊂U ,|B|=b a∈B f (a). Regardless of whether (cid:101)ft was computed via Equation 1 or it is an uncertainty aware objective of the form (cid:101)ft(a) = (cid:101)f o\n\na∈U exp(f (a))(cid:1) and Asum(f, U) = (cid:80) (cid:80)\n\nt (a) + (cid:101)ut(a), our algorithm then produces a query batch Bt by solving\n\na∈U f (a), Ahinge(f, U) = 1\n\n(cid:80)\n\n(cid:80)\n\n|U |\n\n|U |\n\nBt ∈ arg max B⊂Ut,|B|=b\n\nAavg( (cid:101)ft, B).\n\n(2)\n\nThe principle of Optimism in the Face of Uncertainty (OFU) allows OAE algorithms to efficiently explore new regions of the space by acting greedily with respect to a model that fits the rewards of the arms in Dt as accurately as possible but induces large responses from the arms she has not tried. If y⋆ ∈ F, and (cid:101)ft is computed by solving Equation 1, it can be shown the optimistic model overestimates the true response values i.e. (cid:80) y⋆(a) where B⋆,t = arg maxB⊂Ut,|B|=b a∈B y⋆(a). Consult Appendix D.2 for a proof and an explanation of the relevance of this observation.\n\na∈Bt (cid:101)ft(a) ≥ (cid:80)\n\na∈B⋆,t\n\n(cid:80)\n\nActing greedily based on an optimistic model means the learner tries out the arms that may achieve the highest reward according to the current model plausibility set. After pulling these arms, the learner can successfully update the model plausibility set and repeat this procedure.\n\n2The batch equals Ut when |Ut| ≤ b.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Optimistic Arm Elimination Principle (OAE) Input Action set A ⊂ Rd, num batches N , batch size b Initialize Unpulled arms U1 = A. Observed points and labels dataset D1 = ∅ for t = 1, · · · , N do If t = 1 then: · Sample uniformly a size b batch Bt ∼ U1. Else: · Solve for (cid:101)ft and compute Bt ∈ arg maxB⊂Ut||B|=b Observe batch rewards Yt = {y∗(a) for a ∈ Bt} Update Dt+1 = Dt ∪ {(Bt, Yt)} and Ut+1 = Ut\\Bt .\n\nA( (cid:101)ft, B).\n\n4.1 TRACTABLE IMPLEMENTATIONS OF OAE\n\nIn this section we go over the algorithmic details behind the approximations that we have used when implementing the different OAE methods we have introduced in Section 4.\n\n4.1.1 OPTIMISTIC REGULARIZATION\n\nIn order to produce a tractable implementation of the constrained problem 1 we approximate it with the optimism regularized objective,\n\n(cid:101)ft ∈ arg min\n\nf ∈F\n\n1 |Dt|\n\n(cid:88)\n\n(a,y)∈Dt\n\n(f (a) − y)2 − λregA(f, Ut) (cid:125)\n\n(cid:123)(cid:122) Optimism Regularizer\n\n(cid:124)\n\n.\n\n(3)\n\nAnd define Bt following Equation 2. Problem 3 is compatible with DNN function approximation. In our experiments we set the acquisition function to Aavg, Ahinge with p = 4 and Asoftmax. The resulting methods are MeanOpt, HingePNorm and MaxOpt. Throughout this work RandomBatch corresponds to uniform arm selection and Greedy to setting the optimism regularizer to 0.\n\n4.1.2 ENSEMBLE METHODS\n\nWe consider two distinct methods (Ensemble and EnsembleNoiseY) to produce uncertainty estimations based on ensemble predictions. In both we fit M models { (cid:98)f i\n\ni=1 to Dt and define\n\nt }M\n\n(cid:101)f o t (a) =\n\nmaxi=1,··· ,M (cid:98)f i\n\nt (a) + mini=1,··· ,M (cid:98)f i\n\nt (a)\n\n2\n\n,\n\n ̃ut(a) = max\n\ni=1,··· ,M\n\nt (a) − f o (cid:98)f i\n\nt (a).\n\nt + ̃ut. We explore two distinct methods to produce M different models { (cid:98)f i\n\nSo that (cid:101)ft = (cid:101)f o i=1 fit to Dt that differ in the origin of the model noise. Ensemble produces M models resulting of independent random initialization of their model parameters.\n\nt }M\n\nThe EnsembleNoiseY method injects ‘label noise’ into the dataset responses. For all i ∈ {1, · · · , M } we build a dataset Di t = {(a, y + ξ) for (a, y) ∈ Dt, ξ ∼ N (0, 1)} where ξ is an i.i.d. zero mean Gaussian random sample. The functions { (cid:98)f i\n\ni=1 are defined as,\n\nt }M\n\n(cid:98)f i t ∈ arg min\n\nf ∈F\n\n(cid:88)\n\n(f (a) − y)2.\n\n(a,y)∈Di t\n\nIn this case the uncertainty of the ensemble predictions is the result of both the random parameter initialization of the { (cid:98)f i i=1 and the ‘label noise’. This noise injection procedure draws its inspiration from methods such as RLSVI and NARL Russo (2019) and Pacchiano et al. (2021a).\n\nt }M\n\n4.2 DIVERSITY SEEKING VERSIONS OF OAE\n\nIn the case b > 1, the explore / exploit trade-off is not the sole consideration in selecting the arms that make up Bt. In this case, we should also be concerned about selecting sufficiently diverse points within the batch Bt to maximize the information gathering ability of the batch. In Section 4.2 we show how to extend Algorithm 1 (henceforth referred to as vanilla OAE) to effectively induce\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nquery diversity. We introduce two versions OAE − DvD and OAE − Seq which we discuss in more detail below. A detailed description of tractable implementations of these algorithms can be found in Appendix C.\n\n4.2.1 DIVERSITY VIA DETERMINANTS\n\nInspired by diversity-seeking methods in the Determinantal Point Processes (DPPs) literature Kulesza & Taskar (2012), we introduce the OAE − DvD algorithm. Inspired by the DvD algorithm ParkerHolder et al. (2020) we propose to augment the vanilla OAE objective with a diversity regularizer.\n\nBt ∈ arg max B⊆Ut,|B|=b\n\nAsum( (cid:101)ft, B) + Div( (cid:101)ft, B).\n\n(4)\n\nOAE − DvD’s Div regularizer is inspired by the theory of Determinantal Point Processes and equals a weighted log-determinant objective. OAE − DvD has access to a kernel function K : Rd ×Rd → R and at the start of every time step t ∈ N it builds a kernel matrix Kt ∈ R|Ut|×|Ut|,\n\nFor any subset B ⊆ Ut we define the OAE − DvD diversity-aware score as,\n\nKt[i, j] = K(ai, aj),\n\n∀i, j ∈ Ut.\n\nDiv(f, B) = λdiv log (Det(Kt[B, B]))\n\n(5)\n\nWhere Kt[B, B] corresponds to the submatrix of Kt with columns (and rows) indexed by B and λdiv is a diversity regularizer. Since the resulting optimization problem in Equation 6 may prove to be extremely hard to solve, we design a greedy maximization algorithm to produce a surrogate solution. The details can be found in Appendix B.1. OAE − DvD induces diversity leveraging the geometry of the action space.\n\n4.2.2 SEQUENTIAL BATCH SELECTION RULES\n\nIn this section we introduce OAE − Seq a generalization of the OAE algorithm designed to produce in batch diversity. OAE − Seq produces a query batch by solving a sequence of b optimization problems. The first element at,1 of batch Bt is chosen as the arm in Ut achieving the most optimistic prediction over plausible models (following objective 1 and any of the tractable implementations defined in Section C, either optimistic regularization or ensemble methods). To produce the second point at,2 in the batch (provided b > 1) we temporarily add the pair (at,1, ̃yt,1) to the data buffer Dt, where ̃yt,1 is a virtual reward estimator for at,1. Using this ’fake labels’ augmented data-set we select at,2 following the same optimistic selection method used for at,1. Although other choices are possible in practice we set ̃yt,1 as a mid-point between an optimistic and pessimistic prediction of the value of y∗(at,1). The name OAE − Seq derives from the ’sequential’ way in which the batch is produced. If OAE − Seq selected arm a to be in the batch, and this arm has a non-optimistic virtual reward ̃y(a) that is low relative to the optimistic values of other arms, then OAE − Seq will not select too many arms close to a in the same batch. OAE − Seq induces diversity not through the geometry of the arm space but in a way that is intimately related to the plausible arm values in the function class. A similar technique of adding hallucinated values to induce diversity has been proposed before, for example in the GP − BUCB algorithm of Desautels et al. (2014). Ours is the first time this general idea has been tested in conjunction with scalable neural network function approximation algorithms. A detailed discussion of this algorithm can be found in Section B.1.1.\n\n4.3 THE STATISTICAL COMPLEXITY OF ZERO NOISE BATCH LEARNING\n\nIn this section we present our main theoretical results regarding OAE with function approximation. In our results we use the Eluder dimension Russo & Van Roy (2013) to characterize the complexity of the function class F. This is appropriate because our algorithms make use of the optimism principle to produce their queries Bt. We show two novel results. First, we characterize the sample complexity of zero noise parallel optimistic learning with Eluder classes with dimension d. Perhaps surprisingly the regret of Vanilla OAE with batch size b has the same regret profile the case b = 1 up to a constant burn in factor of order bd. Second, our results holds under model misspecification, that is when y⋆ ̸∈ F at the cost of a linear dependence in the misspecification error. Although our results are for the noiseless setting (the subject of this work), we have laid the most important part of the groundwork\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nto extend them to the case of noisy evaluation. We explain why in Appendix D.2. We relegate the formal definition of the Eluder to Appendix D.2.\n\nIn this section we measure the misspecification of y⋆ via the ∥ · ∥∞ norm. We assume y⋆ satisfies minf ∈F ∥f − y⋆∥∞ ≤ ω where ∥f − y⋆∥∞ = maxa∈A |f (a) − y⋆(a)|. Let (cid:101)y⋆ = arg minf ∈F ∥y⋆ − f ∥∞ be the ∥ · ∥∞ projection of y⋆ onto F. We analyze OAE where (cid:101)ft is computed by solving 1 with γt ≥ (t − 1)bω2 with acquisition objective Amax,b. We will measure the performance of OAE via its regret defined as,\n\nRegF ,b(T ) =\n\nT (cid:88)\n\n\n\n\n\n(cid:88)\n\ny⋆(a) −\n\nl=1\n\na∈B⋆,t\n\n\n\ny⋆(a)\n\n .\n\n(cid:88)\n\na∈Bt\n\n(cid:80)\n\nWhere B⋆,t = maxB⊂Ut,|B|=b Theorem 4.1. The regret of OAE with Amax,b acquisition function satisfies, (cid:16) (cid:17) dimE(F, αT )b + ω(cid:112)dimE(F, αT )T b\n\na∈B y⋆(a). The main result in this section is,\n\nRegF ,b(T ) = O\n\n.\n\nWith αt = max (cid:0) 1\n\nt2 , inf{∥f1 − f2∥∞ : f1, f2 ∈ F, f1 ̸= f2}(cid:1) and C = maxf ∈F ,a,a′∈A |f (a) − f (a′)|.\n\nThe proof of theorem 4.1 can be found in Appendix D.2. This result implies the regret is bounded by a quantity that grows linearly with ω, the amount of misspecification but otherwise only with the scale of dimE(F, αT )b. The misspecification part of the regret scales as the same rate as a sequential algorithm running T b batches of size 1. When ω = 0, the regret is upper bounded by O (dimE(F, αT )d). For example, in the case of linear models, the authors of Russo & Van Roy (2013) show dimE(F, ε) = O (d log(1/ε)). This shows that for example sequential OAE when b = 1 achieves the lower bound of Lemma D.1 up to logarithmic factors. In the setting of linear models, the b dependence in the rate above is unimprovable by vanilla OAE without diversity aware sample selection. This is because an optimistic algorithm may choose to use all samples in each batch to explore a single unexplored one dimensional direction. Theoretical analysis for OAE − DvD and OAE − Seq is left for future work. In Appendix D.1 we also show lower bounds for the query complexity for linear and Lipshitz classes.\n\n5 TRANSFER LEARNING ACROSS GENETIC PERTURBATION DATASETS\n\nIn order to show the effectiveness of OAE in the large batch - small number of iterations regime we consider genetic perturbations from the CMAP dataset Subramanian et al. (2017), which contains a 978-gene bulk expression readout from thousands of single-gene shRNA knockout perturbations3 across a number of cell lines.\n\nRegression mean Figure 1: squared error loss for models that predict cell-line specific phenotypic rewards from VCAP-derived perturbational features.\n\nWe consider the setting in which we have observed the effect of knockouts in one biological context (i.e., cell line) and would like to use it to plan a series of knockout experiments in another. Related applications may have different biological contexts, from different cell types or experimental conditions. We use the level 5 CMAP observations, each of which contains of 978-gene transcriptional readout from an shRNA knockout of a particular gene in a particular cell line. In our experiments, we choose to optimize a cellular proliferation phenotype, defined as a function on the 978-gene expression space. See Appendix F for details.\n\nWe use the 4 cells lines with the most number genetic perturbations in common: VCAP (prostate cancer, n = 14602 ), HA1E (kidney epithelium, n = 10487), MCF7 (breast cancer, n = 6638), and A375 (melanoma, n = 10033). We first learn a 100-dimensional action (perturbation) embedding ai for each perturbation in VCAP with an autoencoder. The autoencoder has a 100-dimension\n\n3The shRNA perturbations are just a subset of the 1M+ total perturbations across different perturbation\n\nclasses.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Linear. τ -quantile batch convergence of MeanOpt on genetic perturbation datasets.\n\nbottleneck layer and two intermediate layers of 1500 and 300 ReLU units with dropout and batch normalization and is trained using the Adam optimizer on mean squared reconstruction loss. We use these 100-dimensional perturbations embeddings as the features to train the (cid:101)ft functions for each of the other cell types. According to our OAE algorithm, we train a fresh feed-forward neural network with two intermediate layers (of 100 and 10 units) for after observing the phenotypic rewards for each batch of 50 gene (knockout) perturbations. Figure 6 summarizes this approach.\n\nFigure 1 shows the mean squared error loss of models trained to predict the cell-line specific phenotypic reward from the 100-dimensional VCAPderived perturbational features. These models are trained on successive batches of perturbations sampled via RandomBatch and using the same NN 1500-300 hidden layer architecture of the decoder. Not surprisingly, the loss for the VCAP reward is one of the lowest, but that of two other cell lines (HA1E and MCF7) are also quite similar, showing the NN 1500300 neural net function class is flexible to learn the reward function in one context from the perturbational embedding in another.\n\nIn all of our experiments we consider either a linear or a neural network function class with ReLU activations. In all of them we consider a batch size b = 50 and a number of batches N = 20. Figure 2 shows the convergence and reward results for the 4 cell lines when the neural network architecture equals NN 100-10. Since the perturbation action features were learned on the VCAP dataset (though agnostic to any phenotypic reward), the optimal VCAP perturbations are found quite quickly by all versions of MeanOpt including Greedy. Interestingly, MeanOpt still outperforms RandomBatch in the HA1E and MCF7 cell lines but not on A375. When the neural network architecture equals NN 10-5, MeanOpt is only competitive with RandomBatch on the VCAP and MCF7 datasets (see figure 37 in Appendix G.4). Moreover, when F is a class of linear functions, MeanOpt can beat RandomBatch only in VCAP. This can be explained by looking at the regression fit plots of figure 4. The baseline loss value is the highest for A375 even for NN 100-10, thus indicating this function class is too far from the true responses values for A375. The loss curves for NN 10-5 lie well above those for NN 100-10 for all datasets thus explaining the degradation in performance when switching from a NN 100-10 to a smaller capacity\n\nFigure 2: NN 100-10 τ -quantile batch convergence on genetic perturbation datasets of MeanOpt (left) and Ensemble, EnsembleNoiseY (right).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nof NN 10-5. Finally, the linear fit achieves a very small loss for VCAP explaining why MeanOpt still outperforms RandomBatch in VCAP with linear models. In all other datasets the linear fit is subpar to the NN 100-10, explaining why MeanOpt in NN 100-10 works better than in linear ones. We note that EnsembleOptimism is competitive with RandomBatch in both NN 10-5 and NN 100-10 architectures in 7 our of the 8 experiments we conducted. In Appendix G.4 the reader can find results for MaxOpt and HingeOpt. Both methods underperform in comparison with MeanOpt. In Appendix E.2.1 and E.2.2 the reader will find experiments using tractable versions of OAE − DvD and OAE − Seq. In Appendix E and G, we present extensive additional experiments and discussion of our findings over different network architectures (including linear), and over a variety of synthetic and public datasets from the UCI database Dua & Graff (2017).\n\nFigure 4: Training set regression fit curves evolution over training for the suite of gene perturbation datasets. Each training batch contains 10 datapoints.\n\n5.0.1 GENEDISCO EXPERIMENTAL PLANNING BENCHMARK\n\nWe test a Bayesian OAE algorithm against the GeneDisco benchmark Mehrjou et al. (2021), which assess the \"Hit Rate\" of different experimental planning algorithms over a number of pooled CRISPR experiments. We assess our performance against the other acquisition functions provided in the public implementation4 that select batches based solely on uncertainty considerations. We use the public implementation of GeneDisco and do not change the neural network architecture provided corresponding to a Bayesian Neural Network with a hidden layer of size 32. We use the 808 dimensional Achilles treatment descriptors and built (cid:101)ft by adding the BNN’s uncertainty to the model base predictions. We test our algorithm in the Schmidt et al. 2021 (IFNg), Schmidt et al. 2021 (IL2), Zhuang et al. 2019 (NK), and Zhu et al. 2021 (SarsCov2) datasets and tested for performance using the Hit Ratio metric after collecting 50 batches of size 16. This is defined as the ratio of arm pulls lying in the top .05 quantile of genes with the largest absolute value. Our results are in Table 5. OAE outperforms the other algorithms by a substantial margin in 3 out of the 4 datasets that we tested.\n\nDataset Schmidt et al. 2021 (IFNg) Schmidt et al. 2021 (IL2) Zhuang et al. 2019 (NK) Zhu et al. 2021 (SarsCov2)\n\nTopUncertain 0.057 0.083 0.035 0.035\n\nSoftUncertain OAE-Bayesian\n\n0.046 0.081 0.047 0.049\n\n0.062 0.107 0.085 0.0411\n\nFigure 5: Hit Ratio Results after 50 batches of size 16. BNN model trained with Achilles treatment descriptors. Final Hit Ratio average of 5 independent runs. TopUncertain selects the 16 points with the largest uncertainty values and SoftUncertain samples them using a softmax distribution.\n\n6 CONCLUSION\n\nIn this work we introduce a variety of algorithms inspired in the OAE principle for noiseless batch bandit optimization. We also show lower bounds for the query complexity for linear and Lipshitz classes as well as a novel regret upper bound in terms of the Eluder dimension of the query class. Our theoretical results hold under misspecification. Through a variety of experiments in synthetic, public and biological data we show that the different incarnations of OAE we propose in this work can quickly search through a space of actions for the almost optimal ones. This work focused in the case where the responses are noiseless. Extending our methods and experimental evaluation to the case where the responses are noisy is an exciting avenue for future work.\n\n4https://github.com/genedisco/genedisco-starter\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nYasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic\n\nbandits. Advances in neural information processing systems, 24:2312–2320, 2011.\n\nAlekh Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In Interna-\n\ntional Conference on Machine Learning, pp. 1220–1228. PMLR, 2013.\n\nJordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade. Gone fishing: Neural active learning with fisher embeddings. Advances in Neural Information Processing Systems, 34:8927– 8939, 2021.\n\nP. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine\n\nLearning Research, 3(Nov):397–422, 2002.\n\nMaria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. Journal of\n\nComputer and System Sciences, 75(1):78–89, 2009.\n\nWilliam H Beluch, Tim Genewein, Andreas Nürnberger, and Jan M Köhler. The power of ensembles for active learning in image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9368–9377, 2018.\n\nErdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh. Batch active learning using determi-\n\nnantal point processes. arXiv preprint arXiv:1906.07975, 2019.\n\nNicolo Cesa-Bianchi, Claudio Gentile, and Francesco Orabona. Robust bounds for classification via selective sampling. In Proceedings of the 26th annual international conference on machine learning, pp. 121–128, 2009.\n\nJeffrey Chan, Aldo Pacchiano, Nilesh Tripuraneni, Yun S Song, Peter Bartlett, and Michael I Jordan.\n\nParallelizing contextual linear bandits. arXiv preprint arXiv:2105.10590, 2021.\n\nYuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular optimization. In International Conference on Machine Learning, pp. 160–168. PMLR, 2013.\n\nV. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In\n\nCOLT, pp. 355–366. Omnipress, 2008.\n\nSanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767–1781,\n\n2011.\n\nSanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm.\n\nAdvances in neural information processing systems, 20, 2007.\n\nOfer Dekel, Claudio Gentile, and Karthik Sridharan. Robust selective sampling from single and\n\nmultiple teachers. In COLT, pp. 346–358, 2010.\n\nThomas Desautels, Andreas Krause, and Joel W. Burdick. Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization. Journal of Machine Learning Research, 15(119): 4053–4103, 2014. URL http://jmlr.org/papers/v15/desautels14a.html.\n\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.\n\nics.uci.edu/ml.\n\nMelanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin\n\nbased approach. arXiv preprint arXiv:1802.09841, 2018.\n\nYonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint\n\narXiv:1711.00941, 2017.\n\nAmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Elias Bareinboim. Budgeted experiment design for causal structure learning. In International Conference on Machine Learning, pp. 1724–1733. PMLR, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nDaniel Gissin and Shai Shalev-Shwartz. Discriminative active learning.\n\narXiv preprint\n\narXiv:1907.06347, 2019.\n\nYuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. Advances in neural\n\ninformation processing systems, 20, 2007.\n\nInsu Han, Prabhanjan Kambadur, Kyoungsoo Park, and Jinwoo Shin. Faster greedy map inference for determinantal point processes. In International Conference on Machine Learning, pp. 1384–1393. PMLR, 2017.\n\nRuth E Hanna and John G Doench. Design and analysis of CRISPR-Cas experiments. Nat. Biotechnol.,\n\n38(7):813–823, July 2020.\n\nSteve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends® in\n\nMachine Learning, 7(2-3):131–309, 2014.\n\nDonald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive\n\nblack-box functions. Journal of Global optimization, 13(4):455–492, 1998.\n\nTarun Kathuria, Amit Deshpande, and Pushmeet Kohli. Batched gaussian process bandit optimization via determinantal point processes. Advances in Neural Information Processing Systems, 29: 4206–4214, 2016.\n\nAndreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. Advances in neural information processing systems, 32:7026–7037, 2019.\n\nAlex Kulesza and Ben Taskar. Determinantal point processes for machine learning. arXiv preprint\n\narXiv:1207.6083, 2012.\n\nArash Mehrjou, Ashkan Soleymani, Andrew Jesson, Pascal Notin, Yarin Gal, Stefan Bauer, and Patrick Schwab. Genedisco: A benchmark for experimental design in drug discovery. arXiv preprint arXiv:2110.11875, 2021.\n\nSérgio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of bank\n\ntelemarketing. Decision Support Systems, 62:22–31, 2014.\n\nJonas Mueller, David Reshef, George Du, and Tommi Jaakkola. Learning optimal interventions. In\n\nArtificial Intelligence and Statistics, pp. 1039–1047. PMLR, 2017.\n\nGeorge L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions—i. Mathematical programming, 14(1):265–294, 1978.\n\nAldo Pacchiano, Philip Ball, Jack Parker-Holder, Krzysztof Choromanski, and Stephen Roberts. Towards tractable optimism in model-based reinforcement learning. In Uncertainty in Artificial Intelligence, pp. 1413–1423. PMLR, 2021a.\n\nAldo Pacchiano, Shaun Singh, Edward Chou, Alex Berg, and Jakob Foerster. Neural pseudo-label optimism for the bank loan problem. Advances in Neural Information Processing Systems, 34, 2021b.\n\nJack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33:18050–18062, 2020.\n\nDaniel Russo. Worst-case regret bounds for exploration via randomized value functions. Advances in\n\nNeural Information Processing Systems, 32, 2019.\n\nDaniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic\n\nexploration. Advances in Neural Information Processing Systems, 26, 2013.\n\nNino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard Schölkopf, Michael C Mozer, Yoshua Bengio, Stefan Bauer, and Nan Rosemary Ke. Learning neural causal models with active interventions. arXiv preprint arXiv:2109.02429, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nGreg Schohn and David Cohn. Less is more: Active learning with support vector machines. In ICML,\n\nvolume 2, pp. 6. Citeseer, 2000.\n\nOzan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set\n\napproach. arXiv preprint arXiv:1708.00489, 2017.\n\nBurr Settles. 2009.\n\nBurr Settles, Mark Craven, and Soumya Ray. Multiple-instance active learning. Advances in neural\n\ninformation processing systems, 20, 2007.\n\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural networks. In International conference on machine learning, pp. 2171–2180. PMLR, 2015.\n\nNiranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.\n\nAravind Subramanian, Rajiv Narayan, Steven M Corsello, David D Peck, Ted E Natoli, Xiaodong Lu, Joshua Gould, John F Davis, Andrew A Tubelli, Jacob K Asiedu, David L Lahr, Jodi E Hirschman, Zihan Liu, Melanie Donahue, Bina Julian, Mariya Khan, David Wadden, Ian C Smith, Daniel Lam, Arthur Liberzon, Courtney Toder, Mukta Bagul, Marek Orzechowski, Oana M Enache, Federica Piccioni, Sarah A Johnson, Nicholas J Lyons, Alice H Berger, Alykhan F Shamji, Angela N Brooks, Anita Vrcic, Corey Flynn, Jacqueline Rosains, David Y Takeda, Roger Hu, Desiree Davison, Justin Lamb, Kristin Ardlie, Larson Hogstrom, Peyton Greenside, Nathanael S Gray, Paul A Clemons, Serena Silver, Xiaoyun Wu, Wen-Ning Zhao, Willis Read-Button, Xiaohua Wu, Stephen J Haggarty, Lucienne V Ronco, Jesse S Boehm, Stuart L Schreiber, John G Doench, Joshua A Bittker, David E Root, Bang Wong, and Todd R Golub. A next generation connectivity map: L1000 platform and the first 1,000,000 profiles. Cell, 171(6):1437–1452.e17, November 2017.\n\nScott Sussex, Andreas Krause, and Caroline Uhler. Near-optimal multi-perturbation experimental\n\ndesign for causal structure learning. arXiv preprint arXiv:2105.14024, 2021.\n\nBalazs Szorenyi, Róbert Busa-Fekete, Paul Weng, and Eyke Hüllermeier. Qualitative multi-armed In International Conference on Machine Learning, pp.\n\nbandits: A quantile-based approach. 1660–1668. PMLR, 2015.\n\nSimon Tong and Daphne Koller. Support vector machine active learning with applications to text\n\nclassification. Journal of machine learning research, 2(Nov):45–66, 2001.\n\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-\n\nbridge University Press, 2019.\n\nZheng Wang and Jieping Ye. Querying discriminative and representative samples for batch mode active learning. ACM Transactions on Knowledge Discovery from Data (TKDD), 9(3):1–23, 2015.\n\nKai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning.\n\nIn International conference on machine learning, pp. 1954–1963. PMLR, 2015.\n\nPan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. Neural contextual bandits with deep\n\nrepresentation and shallow exploration. arXiv preprint arXiv:2012.01780, 2020.\n\nMengyan Zhang and Cheng Soon Ong. Quantile bandits for best arms identification. In International\n\nConference on Machine Learning, pp. 12513–12523. PMLR, 2021.\n\nVicky Zhang, Chandler Squires, and Caroline Uhler. Matching a desired causal state via shift\n\ninterventions. Advances in Neural Information Processing Systems, 34, 2021.\n\nDongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration,\n\n2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nCONTENTS\n\n1 Introduction\n\n2 Related Work\n\n3 Problem Definition\n\n4 Optimistic Arm Elimination\n\n4.1 Tractable Implementations of OAE . . . .\n\n. . . . . . .\n\n. . .\n\n. . . . . . . . . . . .\n\n4.1.1 Optimistic Regularization . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4.1.2 Ensemble Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4.2 Diversity seeking versions of OAE . . . .\n\n. . . . . . .\n\n. . .\n\n. . . . . . . . . . . .\n\n4.2.1 Diversity via Determinants . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4.2.2\n\nSequential batch selection rules\n\n. . . . . . . . . . . . . . . . . . . . . . .\n\n4.3 The Statistical Complexity of Zero Noise Batch Learning . . . . . . . . . . . . . .\n\n5 Transfer Learning Across Genetic Perturbation Datasets\n\n5.0.1 GeneDisco Experimental Planning Benchmark . . . . . . . . . . . . . . .\n\n6 Conclusion\n\nA Transfer Learning Across Genetic Perturbation Datasets\n\nB Complementary description of the methods\n\nB.1 Appendix Diversity via Determinants\n\n. . . . . . . . . . . . . . . . . . . . . . . .\n\nB.1.1 Sequential batch selection rules\n\n. . . . . . . . . . . . . . . . . . . . . . .\n\nC Tractable Implementations of OAE − DvD and OAE − Seq\n\nC.0.1 Diversity via Determinants . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nC.0.2 Sequential Batch Selection Rules\n\n. . . . . . . . . . . . . . . . . . . . . .\n\nD Theoretical Results\n\nD.1 Quantifying the Query Complexity of F . . . .\n\n. . . . . . .\n\n. . .\n\n. . . . . . . . .\n\nD.2 Optimism and its properties\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.3 Noisy responses .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nE Experiments\n\nE.1 Vanilla OAE .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . .\n\n. . .\n\n. . . . . . . . . . . . . . . . . . . . . . .\n\nE.1.1 Synthetic One Dimensional Datasets . . . . . . . . . . . . . . . . . . . . .\n\nE.1.2 Public Supervised Learning Datasets . . . . . . . . . . . . . . . . . . . . .\n\nE.2 Experiments Diversity Seeking Objectives . . . . . . . . . . . . . . . . . . . . . .\n\nE.2.1 OAE − DvD .\n\n.\n\n. . . . . . .\n\n. . .\n\n. . . . .\n\n. . . . . . . . . . . . . . . .\n\n13\n\n1\n\n2\n\n3\n\n4\n\n5\n\n5\n\n5\n\n5\n\n6\n\n6\n\n6\n\n7\n\n9\n\n9\n\n14\n\n15\n\n15\n\n16\n\n17\n\n17\n\n17\n\n18\n\n18\n\n21\n\n25\n\n25\n\n26\n\n26\n\n29\n\n30\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nE.2.2 OAE − Seq .\n\n.\n\n.\n\n. . . . .\n\n. . . . .\n\n. . .\n\n. . . . . . . . . . . . . . . . . .\n\n32\n\nF Cellular proliferation phenotype\n\nG Further Experiments\n\nG.1 Synthetic Datasets .\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nG.2 Regression Fitted Datasets\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nG.3 UCI public datasets .\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nG.4 Transfer Learning Biological Datasets . . . . . . . . . . . . . . . . . . . . . . . .\n\n34\n\n35\n\n35\n\n37\n\n37\n\n38\n\nA TRANSFER LEARNING ACROSS GENETIC PERTURBATION DATASETS\n\nIn this section we have placed a diagrammatic version of the data pipeline described in section 5.\n\nFigure 6: Neural design for genetic perturbation experiments. (a) Learn a perturbation action embedding space by training an autoencoder on the gene expression resulting from a large set of observed genetic perturbations in a particular biological context (e.g., shRNA gene knockouts for a particular cell line in CMAP). (b) Select an initial batch of B perturbation actions to perform in parallel within a related (but different) biological context. Selection can be random (uniform) or influenced by prior information about the relationship between genes and the phenotype to be optimized. (c) Perform the current batch of experimental perturbations in vitro and observed their corresponding phenotypic rewards. (d) Concatenate the latest batch’s features and observed rewards to those of previous batches to update the perturbation reward training set. (e) Train a new perturbation reward regression (with some degree of pre-defined optimism) network on the observed perturbation rewards. (f) Use this regressor to predict the optimistic rewards for the currently unobserved perturbations. (g) Select the next batch from these unobserved perturbations with the highest optimistic reward.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nB COMPLEMENTARY DESCRIPTION OF THE METHODS\n\nB.1 APPENDIX DIVERSITY VIA DETERMINANTS\n\nInspired by diversity-seeking methods in the Determinantal Point Processes (DPPs) literature Kulesza & Taskar (2012), we introduce the OAE − DvD algorithm. Inspired by the DvD algorithm ?? we propose the use\n\nDPPs can be used to produces diverse subsets by sampling proportionally to the determinant of the kernel matrix of points within the subset. From a geometric perspective, the determinant of the kernel matrix represents the volume of a parallelepiped spanned by feature maps corresponding to the kernel choice. We seek to maximize this volume, effectively “filling” the feature space. Using a determinant score to induce diversity has been proposed as a strategy in other domains, most notably in the form of the Diversity via Determinants (DvD) algorithm from Parker-Holder et al. (2020) for Population Based Reinforcement Learning. It is from this work that we take inspiration to name our algorithm OAE − DvD. The idea of using DPPs for diversity guided active learning has been explored by Bıyık et al. (2019). In the active learning setting the objective function used to build the batch is purely driven by the diversity objective. The method works as follows. At time t, OAE − DvD constructs a regression estimator (cid:101)ft using the arms and responses in Dt (for example by solving problem 1). Instead of using Equation 2, our algorithm selects batch Bt by optimizing a diversity aware objective of the form,\n\nBt ∈ arg max B⊆Ut,|B|=b\n\nAsum( (cid:101)ft, B) + Div( (cid:101)ft, B).\n\n(6)\n\nOAE − DvD’s Div regularizer is inspired by the theory of Determinantal Point Processes and equals a weighted log-determinant objective. OAE − DvD has access to a kernel function K : Rd ×Rd → R and at the start of every time step t ∈ N it builds a kernel matrix Kt ∈ R|Ut|×|Ut|,\n\nFor any subset B ⊆ Ut we define the OAE − DvD diversity-aware score as,\n\nKt[i, j] = K(ai, aj),\n\n∀i, j ∈ Ut.\n\nDiv(f, B) = λdiv log (Det(Kt[B, B])) (7) Where Kt[B, B] corresponds to the submatrix of Kt with columns (and rows) indexed by B and λdiv is a diversity regularizer. Since the resulting optimization problem in Equation 6 may prove to be extremely hard to solve, we design a greedy maximization algorithm to produce a surrogate solution. We build the batch Bt greedily. The first point at,1 in the batch is selected to be the point in Ut that maximizes the response (cid:101)ft. For all i ≥ 2 the point at,i in Ut is selected from Ut\\{at,j}i−1 j=1 such that, Asum( (cid:101)ft, {a} ∪ {at,j}i−1\n\nj=1) + Div( (cid:101)ft, {a} ∪ {at,j}i−1 j=1)\n\nat,i =\n\nmax a∈Ut\\{at,j }i−1\n\nj=1\n\nAlgorithm 2 Optimistic Arm Elimination - DvD (OAE − DvD) Input Action set A ⊂ Rd, num batches N , batch size b, λdiv Initialize Unpulled arms U1 = A. Observed points and labels dataset D1 = ∅ for t = 1, · · · , N do if t = 1 then\n\nSample uniformly a size b batch Bt ∼ U1.\n\nelse\n\nCompute Bt using the greedy procedure described above.\n\nObserve batch rewards Yt = {y∗(a) for a ∈ Bt} Update Dt+1 = Dt ∪ {(Bt, Yt)}. Update Ut+1 = Ut\\Bt .\n\nWe define a reward augmented kernel matrix Kaug t\n(cid:19)\n\n(cid:18)(cid:113)\n\nsatisfies Kaug\n\nt\n\n[B, B] = diag\n\n(i, j) = Kt(i, j)\n\nexp\n\n(cid:114)\n\n(cid:16) (cid:101)ft(i)+ (cid:101)ft(j) λdet\n\n(cid:17)\n\n. This matrix\n\n(cid:18)(cid:113)\n\n(cid:19)\n\nexp( (cid:101)ft/λdet)\n\n[B, B]\n\nexp( (cid:101)ft/λdet)\n\n[B, B] · Kt[B, B] · diag\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nfor all B ⊆ Ut. Since the determinant of a product of matrices is the product of their determinants it follows that Det(Kaug · Det (Kt[B, B]). Thus for all B ⊆ Ut, equation 6 with diversity score 7 can be rewritten as\n\na∈B (cid:101)ft(a)/λdet\n\n[B, B]) = exp\n\n(cid:16)(cid:80)\n\n(cid:17)\n\nt\n\nBt ∈ arg max B⊆Ut,|B|=b\n\nlog (Det(Kaug\n\nt\n\n[B, B]))\n\n(cid:80)\n\na∈B (cid:101)ft(a)\n\nt\n\n[B, B])) =\n\nThis is because log (Det(Kaug + log (Det(Kt[B, B])) for all B ⊂ Ut. It is well known the log-determinant set function for a positive semidefinite matrix is submodular (see for example section 2.2 of Han et al. (2017)). It has long been established that the greedy algorithm achieves an approximation ratio of (1 − 1 e ) for the constrained submodular optimization problem (see Nemhauser et al. (1978)). This justifies the choices we have made behind the greedy algorithm we use to select Bt.\n\nλdiv\n\nB.1.1 SEQUENTIAL BATCH SELECTION RULES\n\nIn this section we introduce OAE − Seq a generalization of the OAE algorithm designed to produce in batch diversity. OAE − Seq produces a query batch by solving a sequence of b optimization problems. OAE − Seq uses first (cid:101)Dt,0 = Dt the set of arms pulled so far as well as (cid:101)Ut,0 = Ut (the set of arms yet to be pulled) to produce a function (cid:101)ft,1 that determines the initial arm in the batch via the greedy choice at,1 = arg maxa∈Ut (cid:101)ft,1(a). The function (cid:101)ft,1 is computed using the same method as any vanilla OAE procedure. Having chosen this arm, in the case when b > 1, a virtual reward (cid:101)yt,1 (possibly different from (cid:101)ft(at,1)) is assigned to the query arm at,1, and datasets (cid:101)Dt,1 = Dt ∪ {(at,1, (cid:101)yt,1)} and (cid:101)Ut,1 = Ut\\{at,1} are defined. The same optimization procedure that produced (cid:101)ft,1 is used to output (cid:101)ft,2 now with (cid:101)Dt,1 and (cid:101)Ut,1 as inputs. Arm at,2 is defined as the greedy choice at,2 = arg maxa∈ (cid:101)Ut,1 (cid:101)ft,2(a). The remaining batch elements (if any) are determined by successive repetition of this process so that (cid:101)Dt,i = Dt ∪{(at,j, (cid:101)yt,j)}i j=1. The trace of this procedure leaves behind a sequence of functions and datasets {( (cid:101)ft,i, (cid:101)Ut,i)}b i=1 such that at,i ∈ arg max\n\nj=1 and (cid:101)Ut,i = Ut\\{at,j}i\n\n(cid:101)Ut,i−1 (cid:101)ft,i(a).\n\nAlgorithm 3 Optimistic Arm Elimination - Batch Sequential (OAE − Seq) Input Action set A ⊂ Rd, number of batches N , batch size b, pessimism-optimism balancing parameter α. Initialize Unpulled arms U1 = A. Observed points and labels dataset D1 = ∅ for t = 1, · · · , N do if t = 1 then\n\nSample uniformly a size b batch Bt ∼ U1.\n\nelse\n\nSolve for {( (cid:101)ft,i, (cid:101)Ut,i)}b Compute\n\ni=1 via the procedure described in the text.\n\nBt = {at,i ∈ arg max a∈ (cid:101)Ut,i−1\n\n(cid:101)ft,i(a)}b\n\ni=1.\n\n(8)\n\nObserve batch rewards Yt = {f∗(a) for a ∈ Bt} Update Dt+1 = Dt ∪ {(Bt, Yt)} ∈ D. Update Ut+1 = Ut\\Bt .\n\nTo determine the value of the virtual rewards (cid:101)yt,i, we consider a variety of options. We start by discussing the case when the fake reward (cid:101)yt,i = (cid:101)ft(at,i) and the acquisition function equals Asum(f, U).\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nWhen γt = 0, the fake reward satisfies (cid:101)yt,i = (cid:101)ft(at,i) and y⋆ ∈ F it follows that (cid:101)ft,i = (cid:101)ft independent of i ∈ [B] is a valid choice for the function ensemble { (cid:101)ft,i}b i=1. In this case the query batch Bt can be computed by solving for (cid:101)ft,\n\n(cid:101)ft ∈ arg max f ∈F\n\n(cid:88)\n\na∈Ut\n\nf (a)\n\ns.t.\n\n(cid:88)\n\n(f (a) − y)2 = 0.\n\n(9)\n\n(a,y)∈ (cid:101)Dt,i\n\nAnd defining the batch Bt as\n\nBt = arg max\n\nB⊆Ut s.t. |B|=b\n\n(cid:88)\n\na∈B\n\n(cid:101)ft(a).\n\nThe equivalence between this definition of Bt and the sequential batch selection rule follows by noting the equality constraint from 9 ensures that (cid:101)ft,i = (cid:101)ft is a valid solution for each of the intermediate problems defining the sequence {( (cid:101)ft,i, (cid:101)Ut,i)}B\n\ni=1.\n\nOAE − Seq is designed with a more general batch selection rule that may yield distinct intermediate arm selection functions { (cid:101)ft,i}. In our experiments we compute the virtual reward (cid:101)yt,i as an average of (cid:101)f optimistic\n\n, optimistic and a pessimistic estimators of the responses in Ut,i−1.\n\nand (cid:101)f pessimistic\n\nt,i\n\nt,i\n\nWe consider two mechanisms for computing (cid:101)f optimistic of (cid:101)ft,i is based on producing an uncertainty function ̃ut,i and a base model (cid:101)f o t,i + ut,i and (cid:101)f pessimistic (cid:101)f o of the constrained objectives\n\nt,i − ̃ut,i. Second, we define (cid:101)f optimistic\n\nand (cid:101)f pessimistic\n\n= (cid:101)f o\n\nt,i\n\nt,i\n\nt,i\n\nt,i\n\n. First when the computation t,i, we define (cid:101)f optimistic\n\n=\n\nt,i\n\nand (cid:101)f pessimistic\n\nt,i\n\nas the solutions\n\n(cid:101)f optimistic\n\nt,i\n\n= arg max f ∈Ft,i\n\nA(f, Ut)\n\nand\n\n(cid:101)f pessimistic\n\nt,i\n\n= arg min f ∈Ft,i\n\nA(f, Ut)\n\n(a,y)∈Dt,i−1\n\nWhere Ft,i = {f ∈ F s.t. (cid:80) rewards as a weighted average of the pessimistic and optimistic predictors (cid:101)yt,i = α (cid:101)f optimistic α) (cid:101)f pessimistic to define what points are part of the batch as (cid:101)ft,i = (cid:101)f optimistic . The principle of adding hallucinated values to induce diversity has been proposed before for example in the GP − BUCB algorithm of Desautels et al. (2014).\n\n(f (a) − y)2 ≤ γt}. In both cases we define the fictitious + (1 −\n\nwhere α ∈ [0, 1] is an optimism weighting parameter while we keep the functions used\n\nt,i\n\nt,i\n\nt,i\n\nC TRACTABLE IMPLEMENTATIONS OF OAE − DvD AND OAE − Seq\n\nIn this section we go over the algorithmic details behind the approximations that we have used when implementing the different OAE methods we have introduced in Section 4.\n\nC.0.1 DIVERSITY VIA DETERMINANTS\n\nThe OAE − DvD algorithm differs from OAE only in the way in which the query batch Bt is computed. OAE − DvD uses equation 6 instead of equation 2. Solving for Bt is done via the greedy algorithm described in Section 4.2.1. The function (cid:101)ft can be computed via a regularized optimization objective or using an ensemble. In our experimental evaluation we opt for defining (cid:101)ft via the regularization route as the result of solving problem 3. More experimental details including the type of kernel used are explained in Section E.2. In our experimental evaluation we use the MeanOpt objective to produce (cid:101)ft and we refer to the resulting method as DetD.\n\nC.0.2 SEQUENTIAL BATCH SELECTION RULES\n\nWe explore different ways of defining the functions { (cid:101)ft,i}b i=1. Depending on what procedure we use to optimize and produce these functions we will obtain different versions of OAE − Seq. We use the name SeqB to denote the OAE − Seq method that fits the functions { (cid:101)f optimistic i=1 and\n\n}b\n\nt,i\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n{ (cid:101)f pessimistic\n\nt,i\n\n}b\n\ni=1 by solving the regularized objectives,\n\n(cid:101)f optimistic\n\nt,i\n\n∈ arg min\n\nf ∈F\n\n(cid:101)f pessimistic\n\nt,i\n\n∈ arg min\n\nf ∈F\n\n1 |Dt,i−1|\n\n1 |Dt,i−1|\n\n(cid:88)\n\n(a,y)∈Dt,i−1\n\n(cid:88)\n\n(a,y)∈Dt,i−1\n\n(f (a) − y)2−λregA(f, Ut,i−1),\n\n(f (a) − y)2+λregA(f, Ut,i−1).\n\nt,i\n\n+ (1 − α) (cid:101)f pessimistic\n\ni=1 are defined as (cid:101)ft,i = (cid:101)f optimistic\n\nIn our experiments we set the acquisition function to Aavg. For some value5 of λreg > 0. The functions { (cid:101)ft,i}b and the virtual rewards as (cid:101)yt,i = α (cid:101)f optimistic where α ∈ [0, 1] is an optimism-pessimism weighting parameter. In our experimental evaluation we use α = 1 2 . More experimental details are presented in Section E.2. The functions { (cid:101)f optimistic Borrowing the definitions of Section 4.1.2\n\ni=1 can be defined with the use of an ensemble.\n\ni=1 and { (cid:101)f pessimistic }b\n\n}b\n\nt,i\n\nt,i\n\nt,i\n\nt,i\n\n(cid:101)f optimistic\n\nt,i\n\n= (cid:101)f o\n\nt,i+ ̃ut,i,\n\n(cid:101)f pessimistic\n\nt,i\n\n= (cid:101)f o\n\nt,i− ̃ut,i.\n\nt,i and ̃ut,i are computed by first fitting an ensemble of models { (cid:98)f j\n\nWhere (cid:101)f o j=1 using dataset Dt,i−1. In our experimental evaluation we explore the use of Ensemble and EnsembleNoiseY optimization styles to fit the models { (cid:98)f j j=1. In our experiments we use the names Ensemble − SeqB and Ensemble − SeqB − NoiseY to denote the resulting sequential batch selection methods. More details of our implementation can be found in Section E.2.\n\nt,i}M\n\nt,i}M\n\nD THEORETICAL RESULTS\n\nD.1 QUANTIFYING THE QUERY COMPLEXITY OF F\n\nLet ε ≥ 0 and define tε opt(A, f ) to be the first time-step when an ε−optimal point ˆat is proposed by a learner (possibly randomized) when interacting with arm set A ⊂ Rd and the pseudo rewards are noiseless evaluations {f (a)}a∈A with f ∈ F. We define the query complexity of the A, F pair as,\n\nTε(A, F) = min\n\nAlg\n\nmax f ∈F\n\nE (cid:2)tε\n\nopt(A, f )(cid:3)\n\nWhere the minimum iterates over all possible learning algorithms. We can lower bound of the problem complexity for several simple problem classes, Lemma D.1. When A = {∥x∥ ≤ 1 for x ∈ Rd} and\n\n1. If F is the class of linear functions defined by vectors in the unit ball F = {x → θ⊤x : d and Tε(A, F) ≥ ⌈d − εd⌉ otherwise.\n\n∥θ∥ ≤ 1 for θ ∈ Rd} then Tε(A, F) ≥ d when ε < 1\n\n2. If F is the class of 1-Lipschitz functions functions then Tε(A, F) ≥ (cid:0) 1\n\n4ε\n\n(cid:1)d\n\n.\n\nProof. As a consequence of Yao’s principle, we can restrict ourselves to deterministic algorithms. Indeed,\n\nmin Alg\n\nmax f ∈F\n\nE (cid:2)tε\n\nopt(A, f )(cid:3) = max\n\nmin DetAlg\n\nDF\n\nEf ∼DF\n\n(cid:2)tε\n\nopt(A, f )(cid:3)\n\nThus, to prove the lower bound we are after it is enough to exhibit a distribution DF over instances f ∈ F and show a lower bound for the expected tε opt(A, f ) where the expectation is taken using DF .\n\nWith the objective of proving item 1 let DF be the uniform distribution over the sphere Unif d(1). By symmetry it is easy to see that\n\nEθ∼Unif d(r) [θi] = 0,\n\n∀i ∈ d and ∀r ≥ 0.\n\n5As we have pointed out in Section 4.2.2 setting λreg = 0 reduces OAE − Seq to vanilla OAE.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nThus,\n\nVarθ∼Unif d(r) (θi) = Eθ∼Unif d(r)[θ2 i ]\n\n(i) =\n\nr2 d\n\n.\n\nEquality (i) follows because\n\nd (cid:88)\n\ni=1\n\nEθ∼Unif d(r)[θ2\n\ni ] = Eθ∼Unif d(r)[∥θ∥2] = r2\n\nand because by symmetry for all i, j ∈ [d] the second moments agree,\n\nEθ∼Unif d(r)[θ2\n\ni ] = Eθ∼Unif d(r)[θ2 j ]\n\nFinally,\n\nEθ∼Unif d(r)[|θi|] ≤\n\n(cid:113)\n\nEθ∼Unif d(r)[θ2\n\ni ] =\n\nr √\n\nd\n\n.\n\n(10)\n\n(11)\n\nLet DetAlg be the optimal deterministic algorithm for DF and a1 be its first action. Since DF is the unofrm distribution over the sphere, inequality 11 expected scale of the reward reward experienced is upper bounded by 1√ , and furthermore, since ∥a1∥ = 1, the expected second moment of the reward experienced (where expectations are taken over DF ) equals 1 d .\n\nd\n\nWe now employ a conditional argument, if DetAlg has played a1 and observed a reward r1,\n\nWe assume that up to time m algorithm DetAlg has played actions a1, · · · , am and received rewards r1, · · · , rm.\n\nGiven these outcomes, DetAlg can recover the component of θ lying in span(a1, · · · , am). Let am+1 be DetAlg’s action at time m+1. By assumption this is a deterministic function of a1, · · · , am and r1, · · · , rm. Since θ is drawn from Unif d(1), the expected squared dot product between the component of a⊥\n\nm+1 = Proj(am+1, span(a1, · · · , am)⊥) satisfies,\n\nE\n\nθ∼DF |{a⊤\n\n1 θ=ri}m\n\ni=1\n\n(cid:104)(cid:0)θ⊤a⊥\n\nm+1\n\n(cid:1)2(cid:105)\n\n=\n\n=\n\nm∥2\n\n1 − ∥θ0 d − m 1 − ∥θ0 d − m\n\nm∥2\n\n(cid:0)1 − ∥a0\n\nm+1∥2(cid:1)\n\n.\n\n(12)\n\nwhere θ0 m = Proj(θ, span(a1, · · · , am)). The last inequality follows because the conditional distribution of Proj(θ, span(a1, · · · , am)⊥) given a1, · · · , am and r1, · · · , rm is a uniform distribution over the d − m dimensional sphere of radius (cid:112)1 − ∥θ0 m+1∥2 and we have assumed the ∥a0 m+1 with Proj(θ, span(a1, · · · , am)⊥) satisfies Equation 10.\n\nm+1∥ = 0. Thus, the agreement of a⊥\n\nm∥2, the scale of a⊥\n\n1 − ∥a0\n\nm+1 is\n\n(cid:113)\n\nWe consider the expected square norm of the recovered θ up to time m. This is the random variable ∥θ0\n\nt = Proj(at, span(a1, · · · , at−1)⊥). Thus,\n\nm∥2 = (cid:80)m\n\nwhere a⊥\n\n(cid:0)θ⊤a⊥\n\n(cid:1)2\n\nt\n\nt=1\n\nEθ∼DF\n\n(cid:2)∥θ0\n\nm∥2(cid:3) = Eθ∼DF\n\n= Eθ∼DF\n\n(i)\n\n= Eθ∼DF\n\n(cid:34) m (cid:88)\n\nt=1 (cid:34) m (cid:88)\n\nt=1 (cid:34) m (cid:88)\n\nt=1\n\n(cid:35)\n\n(cid:0)θ⊤a⊥\n\nt\n\n(cid:1)2\n\nE\n\nθ∼DF |{a⊤\n\n1 θ=ri}t−1\n\ni=1\n\n(cid:35)\n\n(cid:104)(cid:0)θ⊤a⊥\n\nt\n\n(cid:1)2(cid:105)\n\n1 − ∥θ0\n\nt−1∥2\n\n(cid:35)\n\nd − m\n\nEquality (i) holds because of 12. Recall that by Equation 10,\n\nEθ∼DF\n\n(cid:2)∥θ0\n\n1∥2(cid:3) =\n\n1 d\n\n.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nThus by the above equalities,\n\nEθ∼DF\n\n(cid:2)∥θ0\n\n2∥2(cid:3) =\n\n1 d\n\n+\n\n1 − 1 d\nd − 1\n\n=\n\n2 d\n\n.\n\nUnrolling these equalities further we conclude that (cid:2)∥θ0\n\nEθ∼DF\n\nm∥2(cid:3) =\n\nm d\n\n.\n\nThis implies the expected square agreement between the learner’s virtual guess ˆat is upper bounded by m d , the expected number of queries required is at least d. When ε > d, the expected number of queries instead satisfies a lower bound of ⌈d − εd⌉.\n\nd . Thus, when ε < 1\n\nWe now show shift our attention to Lipschitz functions. First we introduce the following simple construction of a 1−Lipschitz function over a small ball of radius ε. We use this construction throughout our proof. Let x ∈ Rd be an arbitrary vector, define B(x, ε) as the ball centered around x of radius 2ε under the ∥ · ∥2 norm and S(x, 2ε) as the sphere (the surface of B(x, 2ε)) centered around x\n\nDefine the function f ε\n\nx : Rd → R as,\n\nf ε\n\nx(z) =\n\n(cid:26)minz′∈S(x,2ε) ∥z − z′∥2\n\n0\n\nif z ∈ B(x, 2ε) o.w.\n\nIt is easy to see that f ε\n\nx is 1−Lipschitz. We consider three different cases,\n\n1. If z1, z2 ∈ B(x, 2ε)c then |f ε\n\nx(z1) − f ε\n\nx(z2)| = 0 ≤ ∥z1 − z2∥. The result follows.\n\n2. If z1 ∈ B(x, 2ε) but z2 ∈ B(x, 2ε)c. Let z3 be the intersection point in the line going x(z2)| = minz′∈S(x,2ε) ∥z1 − z′∥2 ≤\n\nx(z1) − f ε\n\nfrom z1 to z2 lying on S(x, 2ε). Then |f ε ∥z1 − z3∥2 ≤ ∥z1 − z2∥2.\n\n3. If z1, z2 ∈ B(x, 2ε). It is easy to see that |f ε\n\nx(z2)| = |∥z1 − x∥2 − ∥z2 − x∥2|. And therefore by the triangle inequality applied to x, z1, z2, that |∥z1 − x∥2 − ∥z2 − x∥2| ≤ ∥z1 − z2∥2. The result follows.\n\nx(z1) − f ε\n\nLet N (B(0, 1), 2ε) be a 2ε−packing of the unit ball. For simplicity we’ll use the notation N2ε = |N (B(0, 1), 2ε)|. Define the set of functions F ε = {f ε x for all x ∈ N (B(0, 1), 2ε)} and define DF as the uniform distribution over F ε ⊂ F. Similar to the case when F is the set of linear functions, we make use of Yao’s principle. Let DetAlg be an optimal deterministic algorithm for DF .\n\nLet ai be DetAlg’s i−th query point and ri be the i−th reward it receives. If the ground truth was f ε x\nand the algorithm does not sample a query point from inside B(x, 2ε), it will receive a reward of 0 and thus would not have found an ε−optimal point. Thus topt(f ε x) ≥ first time to pull an arm in B(x, 1). As a consequence of this fact,\n\nEf ε\n\nx∼DF [1(a1 ∈ B(x, 2ε))] ≤\n\n1 N2ε\n\n.\n\nHence Ef ε\n\nx∼DF [1(a1 ̸∈ B(x, 2ε))] ≥ N2ε−1\n\nN2ε\n\n. Therefore,\n\nE (cid:2)tε\n\nopt\n\n(cid:3) ≥\n\nN2ε(cid:88)\n\nl=1\n\nN2ε − l N2ε − l + 1\n\nN2ε − l N2ε − l + 1\n\nN2ε/2 (cid:88)\n\nl=1 1\n4\n\nN2ε.\n\n≥\n\n≥\n\n(i) ≥ Covering(B(0, 1), 4ε)\n\nSince N2ε and inequality (ii) from Lema 5.7 in Wainwright (2019).\n\n≥ (cid:0) 1\n\n(cid:1)d\n\n(ii)\n\n4ε\n\nwhere inequality (i) is a consequence of Lemma 5.5\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTranslating to Quantile Optimality . The results of Lemma D.1 can be interpreted in the langauge of quantile optimality by imposing a uniform measure over the sphere. In this case ε−optimality is equivalent (approximately) to a 1 − 2−d(1−ε)2\n\nquantile.\n\nThe results of Lemma D.1 hold regardless of the batch size B. It is thus impossible to design an algorithm that can single out an ε-optimal arm in less than Tε(A, F) queries for all problems defined by the pair A, F simultaneously.\n\nD.2 OPTIMISM AND ITS PROPERTIES\n\nThe objective of this section is to prove Theorem 4.1 which we restart for the reader’s convenience. Theorem 4.1. The regret of OAE with Amax,b acquisition function satisfies, (cid:17) (cid:16) dimE(F, αT )b + ω(cid:112)dimE(F, αT )T b\n\nRegF ,b(T ) = O\n\n.\n\nWith αt = max (cid:0) 1\n\nt2 , inf{∥f1 − f2∥∞ : f1, f2 ∈ F, f1 ̸= f2}(cid:1) and C = maxf ∈F ,a,a′∈A |f (a) − f (a′)|.\n\nLet’s start by defining the ε−Eluder dimension, a complexity measure introduced by Russo & Van Roy (2013) to analyze optimistic algorithms. Throughout this section we’ll use the notation ∥g∥A = (cid:112)(cid:80) Definition D.2. Let ε ≥ 0 and Z = {ai}n\n\na∈A g2(a) to denote the data norm of function g : A → R. i=1 ⊂ A be a sequence of arms.\n\n1. An action a is ε−dependent on Z with respect to F if any f, f ′ ∈ F satisfying\n\n(cid:112)(cid:80)n\n\ni=1(f (ai) − f ′(ai))2 ≤ ε also satisfies |f (a) − f ′(a)| ≤ ε.\n\n2. An action a is ε−independent of Z with respect to F if a is not ε−dependent on Z.\n\n3. The ε−eluder dimension dimE(F, ε) of a function class F is the length of the longest sequence of elements in A such that for some ε′ ≥ ε, every element is ε′−independent of its predecessors.\n\nLemma D.3. Let’s assume y⋆ satisfies minf ∈F ∥y⋆ − f ∥∞ ≤ ω where ∥g∥∞ = maxa∈Z |g(a)|. Let (cid:101)y⋆ = arg minf ∈F ∥y⋆ − f ∥∞ be the ∥ · ∥∞ projection of y⋆ onto F. If (cid:101)ft is computed by solving 1 with γt ≥ (t − 1)ω2b with acquisition objective Amax,b the response predictions of (cid:101)ft over values Bt satisfy,\n\n(cid:88)\n\ny⋆(a) ≤ bω +\n\n(cid:88)\n\n(cid:101)y⋆(a) ≤ bω +\n\n(cid:88)\n\n(cid:101)ft(a).\n\nwhere B⋆,t = maxB⊂Ut,|B|=b\n\n(cid:80)\n\na∈B y⋆(a).\n\na∈B∗,t\n\na∈B∗,t\n\na∈Bt\n\nProof. Let Ft be the subset of F satisfying f ∈ Ft if (cid:80) definition (cid:101)y⋆ ∈ Ft. Substituting the definition of Amax,b into Equation 1,\n\n(x,y)∈Dt\n\n(f (x) − y)2 ≤ (t − 1)ω2 ≤ γt. By\n\n(cid:101)ft ∈ arg max f ∈Ft\n\nmax B⊂Ut,|B|=b\n\nf (a).\n\n(cid:88)\n\na∈B\n\n(cid:88)\n\na∈ (cid:101)B∗,t\n\nSince (cid:101)y⋆ ∈ Ft,\n\nmax f ∈Ft,B⊂Ut,|B|=b\n\nf (b) =\n\n(cid:88)\n\na∈B\n\n(cid:88)\n\na∈Bt\n\n(cid:101)ft(a) ≥\n\n(cid:101)y⋆(a) ≥\n\n(cid:88)\n\na∈B∗,t\n\n(cid:101)y⋆(a).\n\n(cid:80) Where (cid:101)B⋆,t = maxB⊂Ut,|B|=b a ∈ A, we have (cid:101)y⋆(a) + ω ≥ y⋆(a). This finalizes the proof. m\n\na∈B (cid:101)y⋆(a) and B⋆,t = maxB⊂Ut,|B|=b\n\n(cid:80)\n\na∈B y⋆(a). Finally, for all\n\nSince (cid:101)y⋆(a) + ω ≥ y⋆(a). for all a ∈ A, Lemma D.3 implies,\n\nt (cid:88)\n\n\n\n\n\n(cid:88)\n\ny⋆(a) −\n\nl=1\n\na∈B⋆,t\n\n(cid:88)\n\na∈Bt\n\n\n\ny⋆(a)\n\n ≤ 2btω +\n\nt (cid:88)\n\n(cid:88)\n\nl=1\n\na∈Bt\n\n(cid:101)ft(a) − (cid:101)y⋆(a)\n\n(13)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nWe define the width of a subset (cid:101)F ⊆ F at an action a ∈ A by,\n\n(cid:101)F (a) = sup r\nf,f ′∈ (cid:101)F\n\n|f (a) − f ′(a)|.\n\nAnd use the shorthand notation rt(a) to denote rFt(a) where Ft = {f ∈ F s.t. (cid:80) y)2 ≤ γt}. Equation 13 implies,\n\n(a,y)∈Dt\n\n(f (a) −\n\nt (cid:88)\n\n\n\n\n\n(cid:88)\n\ny⋆(a) −\n\nl=1\n\na∈B⋆,t\n\n(cid:88)\n\na∈Bt\n\n\n\ny⋆(a)\n\n ≤ 2btω +\n\nt (cid:88)\n\n(cid:88)\n\nl=1\n\na∈Bt\n\nrt(a)\n\n(14)\n\nIn order to bound the contribution of the sum (cid:80)t rt(a) we use a similar technique as in Russo & Van Roy (2013). First we prove a generalization of Proposition 3 of Russo & Van Roy (2013) to the case of parallel feedback. Proposition D.4. If {γt ≥ 0|t ∈ N} is a nondecreasing sequence and Ft = {f ∈ F : (cid:80)\n\na∈Bt\n\n(cid:80)\n\nl=1\n\n(a,y)∈Dt\n\n(f (a) − y)2 ≤ γt} then,\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\na∈Bt\n\n1(rt(a) > ε) ≤ O\n\nWhere d = dimE(F, ε).\n\n(cid:18) γtd\n\nε2 + bd\n\n(cid:19)\n\n.\n\nProof. We will start by upper bounding the number of disjoint sequences in Dt−1 that an action a ∈ Bt can be ε−dependent on when rt(a) > ε. If rt(a) > ε there exist ̄f , f ∈ Ft such that ̄f (a) − f (a) > ε. By definition if a ∈ Bt is ε−dependent on a sequence S ⊆ ∪t−1 S ≤ ε2 would imply a to be ε−independent of Dt). Thus if a is ε−dependent on K disjoint sequences S1, · · · , SK ⊆ Dt, then ∥ ̄f − f ∥2\n\nS > ε2 (since otherwise ∥ ̄f − f ∥2\n\nl=1Bl = Dt then ∥ ̄f − f ∥2\n\n≥ Kε2. By the triangle inequality,\n\nDt\n\n∥ ̄f − f ∥Dt ≤ ∥ ̄f − (cid:101)y⋆∥Dt + ∥f − (cid:101)y⋆∥Dt ≤ 2\n\n√\n\nγt.\n\nThus it follows that ε\n\n√\n\n√\n\nK < 2\n\nγt and therefore\n\nK ≤\n\n4γt ε2\n\n(15)\n\nNext we prove a lower bound for K. In order to do so we prove a slightly more general statement. Consider a batched sequence of arms { ̄al,i}i∈[b],l∈[τ ] where for the sake of the argument ̄al,i is not necessarily meant to be al,i. We use the notation ̄Bl = { ̄al,i}i∈[b] to denote the l−th batch in { ̄al,i}i∈[b],l∈[τ ] and ̄Dt = ∪t−1\n\n ̄Bl.\n\nl=1\n\nLet τ ∈ N and define (cid:101)K as the largest integer such that (cid:101)Kd + b ≤ τ b. We show there is a batch number l′ ≤ τ and in-batch index i′ such that al′,i′ is ε−dependent on a subset of disjoint sequences of size at least\n\n2 out of a set of (cid:101)K disjoint sequences ̄S1, · · · , ̄S ̃K ⊆ ̄Dl−1.\n\n ̃K\n\nFirst let’s start building the ̄Si sequences by setting ̄Si =the i−th element in { ̄al,i}i∈[b],l∈[τ ] ordered lexicographically. This will involve elements of up to batch ̄B⌈ ̃K/b⌉.\n\nSince we are going to apply the same argument recursively in our proof, let’s denote the ‘current’ batch index in the construction of ̄S1, · · · , ̄S (cid:101)K ⊆ ̄D ̄l. At the start of the argument ̄l = ⌈ ̃K/b⌉.\n\n(cid:101)K as ̄l, this is, we set ̄S1, · · · , ̄S\n\nIf there is an arm a ∈ ̄B ̄l+1 such that a is ε−dependent on at least (cid:101)K/2 of the { ̄Si} (cid:101)K i=1 sequences, the result would follow. Otherwise, it must be the case that for all a ∈ ̄B ̄l+1 there are at least (cid:101)K sequences in { ̄Si} (cid:101)K\n\ni=1 on which a is ε−independent.\n\n2\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nLet’s consider a bipartite graph with edge sets ̄B ̄l+1 and { ̄Si} (cid:101)K a ∈ ̄B ̄l+1 and ̄Sj ∈ { ̄Si} (cid:101)K sequences in { ̄Si} (cid:101)K of size at least min( (cid:101)K\n\ni=1. We draw an edge between i=1 if a is ε−independent on ̄Sj. If for all a ∈ ̄B ̄l+1 there are at least (cid:101)K i=1 on which a is ε−independent, Lemma D.5 implies the existence of a matching\n\n8 , b) between elements in ̄B ̄l+1 and the sequences in { ̄Si} (cid:101)K\n\ni=1.\n\n2\n\nThe case min( (cid:101)K\n\n8 , b) = (cid:101)K\n\n8 can only occur when (τ −1)b\n\n8d ≤ b and therefore when τ b ≤ 8bd + b.\n\n2d\n\n2 = (τ −1)b\n\nsubsequences. If τ b ≥ 8γτ d\n\n8 , b) = b. If we reach ̄l = τ it must be the case that at least (τ − 1)b points could be In case min( (cid:101)K accommodated into the (cid:101)K sequences. By definition of (cid:101)K it must be the case that each sub-sequence Si satisfies |Si| ≥ d. Since each element of subsequence Si is ε−independent of its predecesors, |Si| = d. In this case we would conclude there is an element in ̄Bτ that is ε−dependent on (cid:101)K and therefore at least (cid:101)K\n\n2 ≥ 4γτ Combining the results of the two last paragraphs we conclude that if τ b ≥ 8γτ d ε2 + 2d + 2b + 8bd then there is a batch index l′ ∈ [τ ] such that there is an arm ̄al′,i ∈ ̄Bl′ that is ε−dependent of at least (cid:101)K Let’s apply the previous result to the sequence of at,i for i ∈ [b] and t ∈ R such that rt(at,i) > ε. An immediate consequence of the previous results is that if (cid:80)t ε2 + 2d + 2b+8bd there must exist an arm in Bt such that it is ε−dependent on at least 4γt ε2 +1 disjoint sequences (cid:80) of Dt. Nonetheless, Equation 15 implies this is impossible. Thus, (cid:80)t 1(rl(a) > ε) ≤ ε2 + 2d + 2b + 8bd ≤ 8γtd\n\nε2 + 1 disjoint sequences contained in ̄Dl′.\n\nε2 + 12bd. The result follows.\n\nε2 + 2d + b then (cid:101)K\n\n1(rl(a) > ε) ≥ 8γtd\n\n2 ≥ 4γτ\n\nε2 + 1.\n\na∈Bl\n\na∈Bl\n\n8γtd\n\n(cid:80)\n\nl=1\n\nl=1\n\nFinally, the RHS of equation 14 can be upper bounded using a modified version of Lemma 2 of Russo & Van Roy (2013) (which can be found in Appendix D.2 ) yielding,\n\nT (cid:88)\n\n\n\n\n\n(cid:88)\n\ny⋆(a) −\n\nl=1\n\na∈B⋆,t\n\n\n\ny⋆(a)\n\n ≤\n\n(cid:16)\n\n+O\n\n1 T\n\n(cid:88)\n\na∈Bt\n\nmin (dimE(F, αT )b, T ) C + (cid:112)dimE(F, αT )ωT\n\n(cid:17)\n\n.\n\nWhere αt = max (cid:0) 1 t2 , inf{∥f1 − f2∥∞ : f1, f2 ∈ F, f1 ̸= f2}(cid:1) and C = maxf ∈F ,a,a′∈A |f (a) − f (a′)|. The quantity in the left is known as regret. This result implies the regret is bounded by a quantity that grows linearly with ω, the amount of misspecification but otherwise only with the scale of dimE(F, αT )b. Our result is not equivalent to splitting the datapoints in b parts and adding b independent upper bounds. The resulting upper bound in the later case will have in a term of the form\n\n(cid:16) b(cid:112)dimE(F, αT )ωT\n\n(cid:17)\n\nO\n\nwhereas in our analysis this term does not depend on b. When ω = 0,\n\nthe regret is upper bounded by O (dimE(F, αT )d). For example, in the case of linear models, the authors of Russo & Van Roy (2013) show dimE(F, αT ) = O (d log(1/ε)). This shows that for example sequential OAE when b = 1 achieves the lower bound of Lemma D.1 up to logarithmic factors. In the setting of linear models, the b dependence in the rate above is unimprovable by vanilla OAE without diversity aware sample selection. This is because an optimistic algorithm may choose to use all samples in each batch to explore explore a single unexplored one dimensional direction. Theoretical analysis for OAE − DvD and OAE − Seq is left for future work. Lemma D.5. Let G be a bipartite graph with node set A ∪ B such that |A| = ̃K and |B| = b. If for all nodes v ∈ B it follows that N (v) ≥ (cid:101)K\n\n2 then,\n\n1. If (cid:101)K\n\n2 ≥ b there exists a perfect matching between the nodes in B and the ones in A.\n\n2. If instead (cid:101)K\n\n2 < b there exists a subset of (cid:101)K\n\n8 nodes in A with a perfect matching to B.\n\nProof. The first item follows immediately from Hall’s marriage theorem. Notice that in this case the neighboring set of any subset of nodes in B has a cardinality of at least (cid:101)K 2 and therefore it is at least the size of B. The conditions of Hall’s theorem are satisfied thus implying the existence of a perfect matching between the nodes in B to the nodes in A.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nIn the second scenario, let’s first prove there exists a subset A′ of A of size at least (cid:101)K element in A′ has at least b There are at least b (cid:101)K greater or equal to b\n\n4 such that every 4 neighbors in B. We prove this condition by the way of contradiction. 2 edges in the graph. Suppose there were at most L vertices in A with degree 4 . This value of L must satisfy the inequality,\n\nLb + ( (cid:101)K − L)\n\n(cid:19)\n\n− 1\n\n≥\n\n(cid:18) b 4\n\nb (cid:101)K 2\n\n.\n\nThis is because the maximum number of edges a vertex in A can have equals b. Thus, L ≥ (cid:101)K\n\n3 − 1.\n\nAll nodes in L have degree at least b in this scenario (cid:101)K (cid:101)L to B. The result follows.\n\n8 < b\n\n8 and since 4 , Hall’s stable marriage theorem implies there is a perfect matching between\n\n4 . If we restrict ourselves to a subset (cid:101)L of L of size (cid:101)K\n\nLemma 2 of Russo & Van Roy (2013) adapted to notation of OAE.\n\nLemma D.6. F s.t. (cid:80)\n\n(a,y)∈Dt\n\nIf {γt ≥ 0|t ∈ N} is a nondecreasing sequence and Ft = {f ∈\n\n(f (a) − y)2 ≤ γt} then with probability 1 for all T ∈ N,\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\na∈Bt\n\nrt(a) ≤ O\n\n(cid:16)\n\n(cid:17) dimE(F, αT )b + ω(cid:112)dimE(F, αT )T b\n\n.\n\nWhere αt = max (cid:0) 1 f (a′)|.\n\ntb , 1\n\n2 inf{∥f1 − f2∥∞ : f1, f2 ∈ F, f1 ̸= f2}(cid:1) and C = maxf ∈F ,a,a′∈A |f (a) −\n\nProof. The proof of lemma D.6 follows the proof template as that of Lemma 2 in Russo & Van Roy (2013). We reproduce it here for completeness.\n\nFor ease of notation we use d = dimE(F, αT ). We first {r1(a1,1), r1(a1,2), · · · , r1(a1,b), · · · , rT (aT,b)) as ri1 ≤ · · · ≤ riT b. We have,\n\nre-order\n\nthe sequence\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\na∈Bt\n\nrt(a) =\n\nT b (cid:88)\n\nj=1\n\nrij =\n\nT b (cid:88)\n\nj=1\n\n1(rij ≤ αT )rij +\n\nT b (cid:88)\n\nj=1\n\n1(rij > αT )rij\n\n(i) ≤ 1 +\n\nT b (cid:88)\n\nj=1\n\n1(rij > αT )rij\n\nWhere inequality (i) holds by definition of αT noting that\n\nT b (cid:88)\n\nj=1\n\n1(rij ≤ αT )rij = 0,\n\nif αT =\n\n1 2\n\ninf{∥f1 − f2∥∞ : f1, f2 ∈ F, f1 ̸= f2}.\n\nand\n\nT b (cid:88)\n\nj=1\n\n1(rij ≤ αT )rij ≤ 1,\n\nif αT =\n\n1 T b\n\n.\n\nProposition D.4 (since d ≥ dimE(F, ε) for all ε ≥ αT ) implies that for all ij with rij > αT , we can bound j as\n\nj ≤\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\na∈Bt\n\n1(rt(a) > rij ) ≤ O\n\n(cid:32)\n\nγT d r2 ij\n\n(cid:33)\n\n+ bd\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 Noisy Batch Selection Principle (OAE) Input Action set A ⊂ Rd, num batches N , batch size b Initialize Observed points and labels dataset D1 = ∅ for t = 1, · · · , N do If t = 1 then: · Sample uniformly a size b batch Bt ∼ U1. Else: · Solve for (cid:101)ft and compute Bt ∈ arg maxB⊂Ut||B|=b Observe batch rewards Yt = {y∗(a) for a ∈ Bt} Update Dt+1 = Dt ∪ {(Bt, Yt)} and Ut+1 = Ut\\Bt .\n\nA( (cid:101)ft, B).\n\nSo there is a constant c > 0 such that rij ≤ O O (bd) since the radii rij are all of constant size. We conclude that,\n\n(j−cbd)+\n\n(cid:16)(cid:113) γT d\n\n(cid:17)\n\n. For all j ≤ cbd notice that (cid:80)cbd\n\nj=1 rij =\n\n\n\nrij ≤ O\n\nbd +\n\nT b (cid:88)\n\nj=1\n\n(cid:115)\n\nT b (cid:88)\n\nj=cbd+1\n\nγT d (j − cbd)+\n\n\n\n\n\n(cid:33)\n\nkdk\n\n(cid:32)\n\nbd + (cid:112)γT d\n\n(cid:90) T b−cbd\n\n√\n\n(cid:16)\n\nj=1 (cid:17) bd + (cid:112)γT dT b\n\n.\n\n≤ O\n\n= O\n\nSubstituting γT = ω2T b we conclude,\n\nrij = O\n\n(cid:16)\n\nbd + ω\n\n√\n\n(cid:17)\n\ndT b\n\n.\n\nT b (cid:88)\n\nj=1\n\nThus finalizing the result.\n\nCombining the result of Lemma D.6 and Equation 14 finalizes the proof of Theorem 4.1.\n\nD.3 NOISY RESPONSES\n\nIn this section we describe how our results imply improved bounds for the setting with noisy responses. In this case we assume that yt,i = y⋆(at,i) + ξt,i where ξt,i is conditionally zero mean.\n\nWe consider the following optimistic least squares algorithm,\n\nE EXPERIMENTS\n\nWe demonstrate the effectiveness of our OAE algorithm in several problem settings across public and synthetic datasets. We evaluate the algorithmic implementations described in Section 4.1 by setting the acquisition function to Aavg(f, U) and the batch selection rule as in Equation 2 for the vanilla OAE methods and as Equations 6 and 8 for OAE’s diversity inducing versions OAE − DvD and OAE − Seq respectively. All neural network architectures use ReLU activations. All ensemble methods use an ensemble size of M = 10 and xavier parameter initialization.\n\nSmall vs Large Batch Regimes. Oftentimes the large batch - small number of iterations regime is the most interesting scenario from a practical perspective Hanna & Doench (2020). In scientific settings like pooled genetic perturbations, each experiment may take a long time (many weeks or months) to conclude, but it is possible to conduct a batch of experiments in parallel together. We study this regime in Section 5.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: (top row) Synthetic one dimensional datasets (bottom) Evolution of MeanOpt in the MultiValleyOneDimHole dataset using λreg = 0.001. From left to right: Iterations 5, 15, 25, 45.\n\nE.1 VANILLA OAE\n\nWe test MeanOpt, HingePNorm, MaxOpt, Ensemble and EnsembleNoiseY’s performance (see Section 4.1 for a detailed description of each of these algorithms) over different values of the regularization parameter λreg, including the ‘greedy’ choice of λreg = 0, henceforth referred to as Greedy. We compare these algorithms to the baseline method that selects points Bt by selecting a uniformly random batch of size B from the set Ut, henceforth referred to as RandomBatch and against each other.\n\nWe conduct experiments on three kind of datasets. First in Section E.1.1 we capture the behavior of OAE in a set of synthetic one dimensional datasets specifically designed to showcase different landscapes for y⋆ ranging from uni-modal to multi-modal with missing values. In Section E.1.2 we conduct similar experiments on public datasets from the UCI database Dua & Graff (2017). In both sections E.1.1 and E.1.2 all of our experiments have a batch size of 3, a time horizon of N = 150 and over two types of network architectures. In Section 5 we consider the setting in which we have observed the effect of knockouts in one biological context (i.e., cell line) and would like to use it to plan a series of knockout experiments in another. We test OAE in this context by showing the effectiveness of the MeanOpt, HingePNorm, MaxOpt, Ensemble and EnsembleNoiseY methods in successfully leveraging the learned features from a source cell line in the optimization of a particular cellular proliferation phenotype for several target cell lines.\n\nAll of our vanilla OAE methods show that better expressivity of the underlying model class F allows for better performance (as measured by the number of trials it requires to find a response within a particular response quantile from the optimum). Low capacity (in our experiments ReLU neural networks with two layers of sizes of 10 and 5) models have a harder time competing against RandomBatch than larger ones (ReLU neural networks with two layers of sizes 100 and 10). We also present results for linear and ‘very high’ capacity models (two layer of sizes 300 and 100).\n\nE.1.1 SYNTHETIC ONE DIMENSIONAL DATASETS\n\nFigure 7 shows different one dimensional synthetic datasets that used to validate our methods. The leftmost, the OneHillValleyOneDim dataset consists of 1000 arms uniformly sampled from the interval [−10, 10]. The responses y are unimodal. The learner’s goal is to find the arm with x−coordinate value equals to 3 as it is the one achieving the largest response. We use the dataset OneHillValleyOneDimHole to test for OAE’s ability to find the maximum when the surrounding points are not present in the dataset.\n\nThe remaining two datasets MultiValleyOneDim and MultiValleyOneDimHole are built with the problem of multimodal optimization in mind. Each of these datasets have 4 local maxima. We use MultiValleyOneDim to test the OAE’s ability to avoid getting stuck in local optima. The second dataset mimics the construction of the OneHillValleyOneDimHole dataset and on top of testing the algorithm’s ability to escape local optima, it also is meant to test what happens when the global optimum’s neighborhood isn’t present in the dataset. Since one of the algorithms we test is the greedy algorithm (corresponding to λ = 0), the ’Hole’ datasets are meant to present a challenging situation for this class of algorithms.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: NN 10-5. τ -quantile batch convergence (left) and corresponding rewards over batches (right) on the OneHillValleyOneDim (easier) and MultiValleyOneDim (harder), OneHillValleyOneDimHole and MultiValleyOneDimHole synthetic datasets show how the OAE algorithm can achieve higher reward faster than RandomBatch or Greedy. The neural network architecture has two hidden layers of sizes 10 and 5.\n\nFigure 9: NN 100-10. τ -quantile batch convergence (left) and corresponding rewards over batches (right) on the suite of synthetic datasets when the neural network architecture has two hidden layers of sizes 100 and 10.\n\nWe test several neural network architectures across all experiments. In all these cases we use ReLU activation functions trained for 5000 steps via the Adam optimizer using batches of size 10. In our tests we use a batch size B = 3, a number of batches N = 150 and repeat each experiment a total of 25 times, reporting average results with standard error bars at each time step.\n\nFirst we compare the MeanOpt version of OAE with a simple RandomBatch strategy that selects a random batch of size B from the dataset points that have not been queried yet and a Greedy algorithm corresponding to MeanOpt with λreg = 0. Figures 8, 9 and 10 show representative results for MeanOpt with (λreg = 0.001, 0.01, 0.1) accross three different neural network architectures, NN 10-5, NN 100-10, and NN 300-100. In all cases, the high optimism versions of MeanOpt perform substantially better than RandomBatch. In both multimodal datasets Greedy underperforms with respect to the versions of MeanOpt with λreg > 0. This points to the usefulness of optimism when facing multimodal optimization landscapes. We also note that for example NN 10-5 is the best performing architecture for MeanOpt with λreg = 0.001 despite the regression loss of fitting the MultiValleyOneDim’s responses with a NN 10-5 architecture not reaching zero (see Figure 13). This indicates the function class F need not contain y⋆ for MeanOpt to achieve good performance.\n\nFigure 10: NN 300-100. τ -quantile batch convergence on the suite of synthetic datasets when the network architecture has two hidden layers of sizes 300 and 100.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nMoreover, it also indicates the use of higher capacity models, despite being able to achieve better regression fit may not perform better than lower capacity ones in the task of finding a good performing arm. We leave the task of designing smart strategies to select the optimal network architecture for future work. It suffices to note all of the architectures used in our experimental evaluation performed better than more naive strategies such as RandomBatch.\n\nSecond, in Figures 11 and 12 we evaluate MeanOpt vs ensemble implementations of OAE across the two neural network architectures NN 10-5 and NN 100-10. We observe that Ensemble performs competitively with all other methods in the one dimensional datasets and outperforms all in the OneHillValleyOneDimHole. In the multi dimensional datasets, MeanOpt performs better than Ensemble, EnsembleNoiseY and Greedy. In this case the most optimistic version of MeanOpt (λreg = .1) is the best performing of all. This may indicate that in multi modal environments the optimism injected by the random initialization of the ensemble models in Ensemble or the reward noise in EnsembleNoiseY do not induce an exploration strategy as effective as the explicit optimistic fit of MeanOpt. In unimodal datasets, the opposite is true, MeanOpt with a large regularizer (λreg = 0.1) underperforms Ensemble, EnsembleNoiseY and Greedy. In Appendix G.1 the reader may find similar results for MaxOpt and HingeOpt. Similar results hold in that case.\n\nFigure 11: NN 10-5. τ -quantile batch convergence of MeanOptimism vs EnsembleOptimism vs EnsembleNoiseY.\n\nFigure 12: NN 100-10. τ -quantile batch convergence of MeanOptimism vs EnsembleOptimism vs EnsembleNoiseY.\n\nFigure 13: Training set regression fit curves evolution over training for the suite of synthetic datasets. Each training batch contains 10 datapoints.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nFigure 15: NN 10-5. τ -quantile batch convergence on the Adult and BikeSharingDay datasets with regression-fitted response values.\n\nE.1.2 PUBLIC SUPERVISED LEARNING DATASETS\n\nthe versions of\n\nWe test our methods on public binary classification (Adult, Bank) and regression (BikeSharingDay, BikeSharingHour, BlogFeedback) datasets from the UCI repository (Dua In our implemen- & Graff, 2017). tation, the UCI datasets we use have the following characteristics. Due to our internal data processing that splits the data into train, test and validation sets the number of datapoints we consider may be different from the size of their public versions. Our code converts the datasets categorical attributes into numerical ones using one hot encodings. That explains the discrepancy between the number of attributes listed in the public description of these datasets and ours https://archive.ics. (see uci.edu/ml/index.php). The BikeSharingDay dataset consists of 658 datapoints each with 13 attributes. The BikesharingHour dataset consists of 15642 datapoints each with 13 attributes. The BlogFeedback dataset consists of 52396 datapoints each with 280 attributes. The Adult dataset consists of 32561 datapoints each with 119 attributes. The Bank dataset (Moro et al., 2014) consists of 32950 datapoints each with 60 attributes. To evaluate our algorithm we assume the response (regression target or binary classification label) values are noiseless. We consider each observation i in a dataset to represent a discrete action, each of which has features ai and reward y∗(ai) from the response. In all of our experiments we use a batch size of 3 and evaluate over 25 independent runs each with 150 batches.\n\nFigure 14: NN 100-10. τ -quantile batch convergence (left) BikeSharingDay, BikeSharingHour and BlogFeedback datasets with original response values.\n\nWe first use all 5 public datasets to test OAE in the setting when the true function class F is known (in this case, a neural network) is known contain the function OAE learns over the course of the batches. We train a neural network under a simple mean squared error regression fit to the binary responses (for the binary classification datasets) or real-valued responses (for the regression datasets). This regression neural network consists of a neural network with two hidden layers. In Figure 15 we present results where the neural network layers have sizes 10 and 5 and the responses are fit to the Adult classification dataset and the regression dataset BikeSharingDay. In Appendix G.2 and Figure 32 we present results where we fit a two layer neural network model of dimensions 100 and 10 to the responses of the Bank classification dataset and the BikeSharingHour regression dataset. In each case we train the regression fitted responses on the provided datasets using 5, 000 batch gradient\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nFigure 16: Training set regression fit curves evolution over training for the UCI datasets. Each training batch contains 10 datapoints.\n\nFigure 17: Linear. τ -quantile batch convergence on the BikeSharingDay, BikeSharingHour and BlogFeedback datasets.\n\nsteps (with a batch size of 10). During test time we use the real-valued response predicted by our regression model as the reward for the corresponding action. This ensures the dataset’s true reward response model has the same architecture as the reward model used by OAE.\n\nWe use the same experimental parameters and comparison algorithms as in the synthetic dataset experiments. Figure 15 shows the results on the binary classification Adult and regression BikeSharingDay datasets using these fitted responses. We observe the MeanOpt algorithm handily outperforms RandomBatch on both datasets. Appendix G.2 shows similar results for BikeSharingHour and Bank. We also compare the performance of MeanOpt, Ensemble and EnsembleNoiseY in the Adult and BikeSharingDay datasets. In both cases, ensemble methods achieved better performance than MeanOpt. It remains an exciting open question to verify whether these observations translates into a general advantage for ensemble methods in the case when y⋆ ∈ F.\n\nGiven OAE’s strong performance when the true and learned reward functions are members of the same function class F, we next explore the performance when they are not necessarily in the same class by revising the problem on the regression datasets to use their original, real-world responses. Figure 14 shows results for the BikeSharingDay, BikeSharingHour and BlogFeedback datasets. In this case Ensemble outperforms both RandomBatch in τ -quantile convergence time. In all of these plots we observe that high optimism approaches underperform compared with low optimism ones. Ensemble and Greedy (the degenerate version λreg = 0 of MeanOpt) achieve the best performance across all three datasets. We observe the same phenomenon take place even when F is a class of linear functions (see figure 18). Just as we observed in the case of the suite of synthetic datasets, setting F to be a class of linear models MeanOpt still achieves substantial performance gains w.r.t RandomBatch (see figure 17) despite its regression fit loss never reaching absolute zero (see figure 16). In Appendix G.3, figure 33 we compare the performance of MaxOpt and HingeOpt with RandomBatch when F is a class of neural networks with hidden layers of sizes 100 and 10. We observe the performance of MaxOpt, although beats RandomBatch is suboptimal in comparison with MeanOpt. In contrast HingeOpt has a similar performance to MeanOpt.\n\nE.2 EXPERIMENTS DIVERSITY SEEKING OBJECTIVES\n\nIn this section we explore how diversity inducing objectives can sometimes result in better performing variants of OAE. We implement and test DetD, SeqB, Ensemble − SeqB and\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nFigure 18: Linear. τ -quantile batch convergence on the BikeSharingDay, BikeSharingHour and BlogFeedback datasets.\n\nFigure 20: NN 10-5.τ -quantile batch convergence performance of DetD vs RandomBatch in the suite of synthetic datasets.\n\nEnsemble − SeqB − NoiseY. In all the experiments we have conducted we kept the batch size b and the number of batches N for each dataset equal to the settings used in Section E.1. The neural network architectures are the same we have considered before, two layer networks with ReLU activations.\n\nE.2.1 OAE − DvD\n\nFigure 19: NN 100-10. τ -quantile batch convergence performance of DetD vs RandomBatch in the BikeSharingDay dataset.\n\nWe implemented and tested the DetD algorithm described in Section C.0.1. In our experiments we set λdiv = 1 and set (cid:101)ft to be the result of solving the MeanOpt objective (see problem 3) for different values of λreg. We set K to satisfy,\n\n(cid:32)\n\nK(a1, a2) = exp\n\n−\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\na1 ∥a1∥\n\n−\n\na2 ∥a2∥\n\n2(cid:33)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nWe see that across the suite of synthetic datasets and architectures (NN 10-5 and NN 100-10) the performance of MeanOpt degrades when diversity is enforced (see figures 20 and 21 and compare with figures 8 and 9). A similar phenomenon is observed in the suite of UCI datasets (see figure 19 for DetD results on the BikeSharing dataset\n\nand figure 14 for comparison).\n\nIn contrast, we note that DetD beats the performance of RandomBatch in the A373, MCF7 and HA1E datasets and over the two neural architectures NN 10-5 and NN 100-10. Nonetheless, the DetD is not able to beat RandomBatch in the VCAP dataset. These results indicate that a diversity objective that relies only on the geometry of the arm space and does not take into account the response values may be beneficial when y⋆ ̸∈ F but could lead to a deterioration in performance when y⋆ ∈ F. The algorithm designer should be careful when balancing diversity objectives and purely optimism driven exploration strategies, since the optimal combination depends on the nature of the dataset. It remains an interesting avenue for future research to design strategies that diagnose in advance the appropriate diversity/optimism balance for OAE − DvD.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nFigure 21: NN 100-10. τ -quantile batch convergence performance of DetD vs RandomBatch in the suite of synthetic datasets.\n\nFigure 22: NN 10-5. τ -quantile batch convergence performance of DetD vs RandomBatch in the suite of genetic perturbation datasets.\n\nE.2.2 OAE − Seq\n\nIn this section we present our experimental evaluation of the three tractable implementations of OAE − Seq we described in Section C.0.2. In our experiments we set the optimism-pessimism weighting parameter α = 1/2 and the acquisition function to Aavg. We are primarily concerned with answering whether ‘augmenting’ the MeanOpt, Ensemble and EnsembleNoiseY methods with a sequential in batch selection mechanism leads to an improved performance for OAE. We answer this question in the affirmative. In our experimental results we show that across datasets and neural network architectures adding in batch sequential optimism either improves or leads to no substantial degradation in the number of batches OAE requires to arrive at good arm.\n\nNN 100-10.\n\nFigure 24: vs SeqB, Ensemble − SeqB − NoiseY on the suite of synthetic datasets.\n\nτ -quantile batch convergence performance of (left) MeanOpt (center) Ensemble vs Ensemble − SeqB and (right) Ensemble − NoiseY vs\n\nFigure 23: NN 100-10. τ -quantile batch convergence performance of DetD vs RandomBatch in the suite of genetic perturbation datasets.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nFigure 26: NN 10-5. τ -quantile batch convergence performance of (left) MeanOpt vs SeqB, (center) Ensemble vs Ensemble − SeqB and (right) Ensemble − NoiseY vs Ensemble − SeqB − NoiseY on the suite of synthetic datasets.\n\nFigure 25: NN 100-10. τ -quantile batch convergence performance of MeanOpt vs SeqB on the gene perturbation datasets.\n\nMore precisely in figures 26 and 24 we show that in the set of synthetic datasets adding a sequential in batch selection rule improves the performance of OAE across the board for (almost) all datasets and all methods MeanOpt, Ensemble and EnsembleNoiseY and two neural architectures NN 10-5 and NN 100-10. Similar gains are observed when incorporating a sequential batch selection rule to OAE in the BikeSharingDay and BikeSharingHour datasets when F corresponds to the class NN 100-10 (see figure 27). Finally, we observe the performance of OAE − Seq either did not degrade or slightly improved that of MeanOpt, Ensemble and EnsembleNoiseY in the BlogFeedback and the genetic perturbation datasets (see figures 34, 25 and 38). We conclude that incorporating a sequential batch decision rule, although it may be computationally expensive, is a desirable strategy to adopt. It\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nNN 100-10.\n\nFigure 27: vs SeqB, Ensemble − SeqB − NoiseY on the BikeSharingDay and BikeSharingHour datasets.\n\nτ -quantile batch convergence performance of (left) MeanOpt (center) Ensemble vs Ensemble − SeqB and (right) Ensemble − NoiseY vs\n\nis interesting to note that in contrast with DetD the diversity induced by SeqB did not alleviate the subpar performance of MeanOpt in the set of genetic perturbation datasets. This can be explained by SeqB induces query diversity by fitting fake responses and it therefore may be limited by the expressiveness of F. The DetD instead injects diversity by using the geometry of the arm space and may bypass the limitations of exploration strategies induced by F. Unfortunately DetD may result in suboptimal performance when y⋆ ∈ F.\n\nF CELLULAR PROLIFERATION PHENOTYPE\n\nLet G be the list of genes present in CMAP also associated with proliferation phenotype according to the Seurat cell cycle signature, and let xi,g represent the level 5 gene expression of perturbation ai for gene g ∈ G. We define the proliferation reward for perturbation ai as the average expression of the genes in G,\n\nf prolif ∗\n\n(ai) =\n\n1 |G|\n\n(cid:88)\n\ng∈G\n\nxi,g\n\nFor convenience, G = {AURKB, BIRC5, CCNB2, CCNE2, CDC20, CDC45, CDK1, CENPE, GMNN, KIF2C, LBR, NCAPD2, NUSAP1, PCNA, PSRC1, RFC2, RPA2, SMC4, STMN1, TOP2A, UBE2C, UBR7, USP1}.\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nG FURTHER EXPERIMENTS\n\nG.1 SYNTHETIC DATASETS\n\nFigure 28: NN 10-5. MaxOptimism comparison vs RandomBatch over the suite of synthetic datasets.\n\nFigure 29: NN 100-10. MaxOptimism comparison vs RandomBatch over the suite of synthetic datasets.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\nFigure 30: NN 10-5. HingePNorm comparison vs RandomBatch over the suite of synthetic datasets.\n\nFigure 31: NN 100-10. HingePNorm comparison vs RandomBatch over the suite of synthetic datasets.\n\n36\n\nPublished as a conference paper at ICLR 2023\n\nG.2 REGRESSION FITTED DATASETS\n\nFigure 32: NN 100-10. BikeSharingHour and Bank Regression Fitted Datasets. Our algorithms perform substantially better than RandomBatch. Optimistic approaches are better than Greedy in the Bank dataset.\n\nG.3 UCI PUBLIC DATASETS\n\nFigure 33: RandomBatch in the BikeSharingDay, BikeSharingHour and BlogFeedback datasets.\n\nNN 100-10. MaxOptimism and HingePNormOptimism comparison vs\n\n37\n\nPublished as a conference paper at ICLR 2023\n\nNN 100-10.\n\nFigure 34: vs SeqB, Ensemble − SeqB − NoiseY on the BlogFeedback dataset.\n\nτ -quantile batch convergence performance of (left) MeanOpt (center) Ensemble vs Ensemble − SeqB and (right) Ensemble − NoiseY vs\n\nG.4 TRANSFER LEARNING BIOLOGICAL DATASETS\n\nFigure 35: NN 10-5. MaxOptimism and HingePNormOptimism comparison vs RandomBatch.\n\n38\n\nPublished as a conference paper at ICLR 2023\n\nFigure 36: NN 100-10. RandomBatch.\n\nMaxOptimism and HingePNormOptimism comparison vs\n\n39\n\nPublished as a conference paper at ICLR 2023\n\nFigure 37: EnsembleNoiseY (right) on genetic perturbation datasets.\n\nNN 10-5.\n\nτ -quantile batch convergence of MeanOpt (left), Ensemble and\n\n40\n\nPublished as a conference paper at ICLR 2023\n\nFigure 38: NN 10-5. τ -quantile batch convergence performance of (left) MeanOpt vs SeqB, (center) Ensemble vs Ensemble − SeqB and (right) Ensemble − NoiseY vs Ensemble − SeqB − NoiseY on the genetic perturbation datasets.\n\n41",
    "reference": "# Summary Of The Paper\n\nMotivated by the genetic perturbations problem in biology, the authors reformulate this problem as a batched regret minimization problem with a large action space.  Specifically, they assume the feedback is noiseless and prior-known a potential function class that outputs the score of a given action (a perturbation) but this function class can be misspecified. \n\nBased on this setting, they propose the Optimistic Arm Elimination (OAE) framework, which can be adapted to various optimistic predictors. Under this framework, they give a theoretical regret guarantee in terms of the Eluder dimension of the function, the misspecification level, and the batch size. Then they test their framework on various experiments by choosing the regression model as deep neural net and get SOTA in 3 of 4 datasets.\n\n# Strength And Weaknesses\n\nStrong: This is a well-motivated real-world problem. The author gives a meaningful formulation of this problem and shows promising experimental results.\n\nWeaknesses:\n1\\ The proposed framework in the main paper, although high-level and flexible, seems just a very standard optimism-based framework that incorporates various existing heuristic methods  (Section 4.1, 4.2)\n\n2\\ Under the noiseless assumption, the proposed predictor in Eqn.(1) and the corresponding Theorem 4.1 seems not surprising to me -- If there is no misspecification then this is just a standard passive learning problem. If there is some misspecification, then it is linear in misspecification. Also this predictor is not parameter-free because you need to choose tolerance $\\gamma$ based on misspecifcation.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIt is well-written. But I cannot find the reference for citations in 5.0.1 (those datasets)\n\n# Summary Of The Review\n\nWhile I believe it is practical and meaningful based on those experimental results. It is a bit unclear to me whether its novelty on theoretical contributions or algorithm design contributions. I am not experienced in doing experiments so it is hard to judge the overall contributions.\n\nAlso, given this is an ml for a science paper. I am willing to raise my score if people familiar with this area endorse it.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  }
]