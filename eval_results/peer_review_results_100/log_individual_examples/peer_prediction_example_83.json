{
  "example_idx": 83,
  "reference": "RHINO: DEEP CAUSAL TEMPORAL RELATIONSHIP LEARNING WITH HISTORY-DEPENDENT NOISE\n\nWenbo Gong, Joel Jennings, Cheng Zhang & Nick Pawlowski Microsoft Research Cambridge, UK {wenbogong, joeljennings, cheng.zhang, nick.pawlowski} @microsoft.com\n\nABSTRACT\n\nDiscovering causal relationships between different variables from time series data has been a long-standing challenge for many domains such as climate science, finance, and healthcare. Given the complexity of real-world relationships and the nature of observations in discrete time, causal discovery methods need to consider non-linear relations between variables, instantaneous effects and historydependent noise (the change of noise distribution due to past actions). However, previous works do not offer a solution addressing all these problems together. In this paper, we propose a novel causal relationship learning framework for timeseries data, called Rhino, which combines vector auto-regression, deep learning and variational inference to model non-linear relationships with instantaneous effects while allowing the noise distribution to be modulated by historical observations. Theoretically, we prove the structural identifiability of Rhino. Our empirical results from extensive synthetic experiments and two real-world benchmarks demonstrate better discovery performance compared to relevant baselines, with ablation studies revealing its robustness under model misspecification.\n\n1\n\nINTRODUCTION\n\nTime series data is a collection of data points recorded at different timestamps describing a pattern of chronological change. Identifying the causal relations between different variables and their interactions through time (Spirtes et al., 2000; Berzuini et al., 2012; Guo et al., 2020; Peters et al., 2017) is essential for many applications e.g. climate science, health care, etc. Randomized control trials are the gold standard for discovering such relationships, but may be unavailable due to cost and ethical constraints. Therefore, causal discovery with just observational data is important and fundamental to many real-world applications (L ̈owe et al., 2022; Bussmann et al., 2021; Moraffah et al., 2021; Wu et al., 2020; Runge, 2018; Tank et al., 2018; Hyv ̈arinen et al., 2010; Pamfil et al., 2020).\n\nThe task of temporal causal discovery can be challenging for several reasons: (1) relations between variables can be non-linear in the real world; (2) with a slow sampling interval, everything happens in between will be aggregated into the same timestamp, i.e. instantaneous effect; (3) the noise may be non-stationary (its distribution depends on the past observations), i.e. history-dependent noise. For example, in stock markets, the announcements of some decisions from a leading company after the market closes may have complex effects (i.e. non-linearity) on its stock price immediately after the market opening (i.e. slow sampling interval and instantaneous effect) and its price volatility may also be changed (i.e. history-dependent noise). Similarly, in education, students that recently earned good marks on algebra tests should also score well on an upcoming algebra exam with little variation (i.e. history-dependent noise).\n\nTo the best of our knowledge, existing frameworks’ performances suffer in many real-world scenarios as they cannot address these aspects in a satisfactory way. Especially, history-dependent noise has been rarely considered in past. A large category of the preceding works, called Granger causality (Granger, 1969), is based on the fact that cause-effect relationships can never go against time. Despite many recent advances (Wu et al., 2020; Shojaie & Michailidis, 2010; Siggiridou &\n\n1\n\nKugiumtzis, 2015; Amornbunchornvej et al., 2019; L ̈owe et al., 2022; Tank et al., 2018; Bussmann et al., 2021; Dang et al., 2018; Xu et al., 2019), they all rely on the absence of instantaneous effects with a fixed noise distribution. Constraint-based methods have also been extended for time series causal discovery (Runge, 2018; 2020), which is commonly applied by folding the time-series. This introduced new assumptions and translated the aforementioned requirements to challenges in conditional independence testing (Shah & Peters, 2020).Additionally, they require a stronger faithfulness assumption and can only identify the causal graph up to a Markov equivalence class without detailed functional relationships.\n\nAn alternative line of research leverages the development of causal discovery with functional causal models (Hyv ̈arinen et al., 2010; Pamfil et al., 2020; Peters et al., 2013). They can model both instantaneous and lagged effects as long as they have theoretically guaranteed structural identifiability. Unfortunately, they do not consider history-dependent noise. One central challenge of modelling this dependency is that noise depending on the lagged parents may break the model structural identifiability. For static data, Khemakhem et al. (2021) proves the structural identifiability only when this dependency is restricted to a simple functional form. Thus, the key research question is whether the identifiability can be preserved with complex historical dependencies in the temporal setting.\n\nMotivated by these requirements, we propose a novel temporal discovery framework called Rhino (deep causal temporal relationship learning with history dependent noise), which can model nonlinear lagged and instantaneous effects with flexible history-dependent noise. Our contributions are:\n\n• A novel causal discovery framework called Rhino, Revision(Q2)-Reviewer hiTaconsisting of a novel functional form of its SEMs and variational training framework, where the proposed form of its SEM combines vector auto-regression and deep learning to model non-linear lagged and instantaneous effects with history-dependent noise.\n\n• We prove that Rhino SEMs with the proposed form are structurally identifiable. To achieve this, we provide general conditions for structural identifiability with history-dependent noise, of which the form of Rhino SEMs is a special case. Furthermore, we clarify relations to several previous works.\n\n• We conduct extensive synthetic experiments with ablation studies to demonstrate the advantages of Rhino and its robustness across different settings. Additionally, we compare its performance to a wide range of baselines in two real-world discovery benchmarks.\n\n2 BACKGROUND\n\nIn this section, we briefly introduce necessary prerequisite knowledge. In particular, we focus on structural equation models, Granger causality (Granger, 1969) and vector auto-regression. For review of more recent related work, please refer to Section 5.\n\nStructural Equation Models (SEMs) Consider X ∈ RD with D variables, SEM describes the causal relationships between them given a causal graph G: G, εi) G are the parents of node i and εi are mutually independent noise variables. Under the i∈V where V is a set of nodes with size D, the\n\nwhere Pai context of multivariate time series, Xt = (cid:0)X i corresponding SEM given a temporal causal graph G is\n\nX i = fi(Pai\n\n(1)\n\n(cid:1)\n\nt\n\nt),\n\nX i\n\nG(t), εi\n\nt = fi,t(Pai\n\nG(< t), Pai G(< t) contains the parent values specified by G in previous time (lagged parents); Pai\n\nwhere Pai G(t) are the parents at the current time t (instantaneous parents). The above SEM induces a joint distribution over the stationary time series {Xt}T t=0 (see Assumption 1 in Appendix B for the definition). However, functional causal models with the above general form cannot be directly used for causal discovery due to the structural unidentifiability (Lemma 1, Zhang et al. (2015) One way to solve this is sacrificing the flexibility by restricting the functional class. For example, additive noise models (ANM), (Hoyer et al., 2008)\n\nX i = fi(PaG(X i)) + εi, which have recently been used for causal reasoning with non-temporal data (Geffner et al., 2022).\n\n(2)\n\n(3)\n\n2\n\n<t, does not help the prediction of X i\n\nGranger Causality Granger causality (Granger, 1969) has been extensively used for temporal causal discovery. It is based on the idea that the series X j does not Granger cause X i if the history, X j t for some t given the past of all other time series X k for k ̸= j, i. Definition 2.1 (Granger Causality (Tank et al., 2018; L ̈owe et al., 2022)). Given a multivariate stationary time series {Xt}T\n\nt=0 and a SEM fi,t defined as\n\nt = fi,t(Pai X j Granger causes X i if ∃l ∈ [1, t] such that X j\n\nX i\n\nG(< t)) + εi t, t−l ∈ Pai\n\nG(< t) and fi,t depends on X j\n\nt−l.\n\n(4)\n\nGranger causality is equivalent to causal relations for directed acyclic graph (DAG) if there are no latent confounders and instantaneous effects (Peters et al., 2013; 2017). Apart from the lack of instantaneous effects, it also ignore the history-dependent noise with independent εi t.\n\nVector Auto-regressive Model Another line of research focuses on directly fitting the identifiable SEM to the observational data with instantaneous effects. One commonly-used approach is called vector auto-regression (Hyv ̈arinen et al., 2010; Pamfil et al., 2020):\n\nX i\n\nt = βi +\n\nK (cid:88)\n\nD (cid:88)\n\nτ =0\n\nj=1\n\nBτ,jiX j\n\nt−τ + εi\n\nt\n\n(5)\n\nwhere βi is the offset, τ is the model lag, Bτ ∈ RD×D is the weighted adjacency matrix specifying the connections at time t − τ (i.e. if Bτ,ji = 0 means no connection from X j t is the independent noise. Under these assumptions, the above linear SEM is structurally identifiable, which is a necessary condition for recovering the ground truth graph (Hyv ̈arinen et al., 2010; Peters et al., 2013; Pamfil et al., 2020). However, the above linear SEM with independent noise variables is too restrictive to fulfil the requirements described in Section 1. Therefore, the research question is how to design a structurally identifiable non-linear SEM with flexible history-dependent noise.\n\nt−τ to X i\n\nt ) and εi\n\n3 RHINO: RELATIONSHIP LEARNING WITH HISTORY DEPENDENT NOISE\n\nThis section introduces Rhino: Section 3.1 describes the novel functional form for Rhino SEMs, allowing for history-dependent noise. Section 3.2 details the associated variational inference framework for causal discovery.\n\n3.1 MODEL FORMULATION\n\nFor a multivariate stationary time series {Xt}T t=0, we assume that their causal relations follow a temporal adjacency matrix G0:K with maximum lag K where Gτ ∈[1,K] specifies the lagged effects t−τ → X j between Xt−τ and Xt, G0 specifies the instantaneous parents. We define Gτ,ij = 1 if X i t\nand 0 otherwise. 1 We propose a novel functional form for Rhino’s SEM that incorporates non-linear relations, instantaneous effects, and flexible history-dependent noise:\n\nX i\n\nG(t)) + gi(Pai\n\nt = fi(Pai\n\nG(< t), Pai\n\n(6) where fi is a general differentiable non-linear function, and gi is a differentiable transform s.t. the transformed noise has a proper density. Despite of an additive structure, our formulation offers much more flexibility in both functional relations and noise distributions compared to previous works (Pamfil et al., 2020; Peters et al., 2013). By placing few restrictions on fi, gi, it can capture functional non-linearity through fi and transform εi t through a flexible function gi, depending on Pai\n\nG(< t), to capture the history dependency of the additive noise.\n\nG(< t), εi t)\n\nNext, we propose flexible functional designs for fi, gi, which must respect the relations encapsulated in G. Namely, if X j G(t), then ∂fi/∂X j t−τ = 0 and similarly for gi. We design \n\nG(< t) ∪ Pai\n\nt−τ /∈ Pai\n\n\n\nfi(Pai\n\nG(< t), Pai\n\nG(t)) = ζi\n\n\n\nK (cid:88)\n\nD (cid:88)\n\nGτ,jilτ j\n\n(cid:16)\n\nX j\n\nt−τ\n\n(cid:17)\n\n\n\n(7)\n\n1In the following, we interchange the usage of the notation G and G0:K for brevity.\n\nτ =0\n\nj=1\n\n3\n\nwhere ζi and lτ i (i ∈ [1, D] and τ ∈ [0, K]) are neural networks. For efficient computation, we use weight sharing across nodes and lags: ζi(·) = ζ(·, u0,i) and lτ j(·) = l(·, uτ,j), where uτ,i is the trainable embedding for node i at time t − τ .\n\nThe design of gi needs to properly balance the flexibility and tractability of the transformed noise density. We choose a conditional normalizing flow, called conditional spline flow (Trippe & Turner, 2018; Durkan et al., 2019; Pawlowski et al., 2020), with a fixed Gaussian noise εi t for all t, i. The spline parameters are predicted using a hyper-network with a similar form to Eq. (7) to incorporate history dependency. The only difference is now τ is summed over [1, K] to remove the instantaneous parents. Due to the invertibility of gi, the noise likelihood conditioned on lagged parents is\n\npgi(gi(εi\n\nt)|Pai\n\nG(< t)) = pε(εi t)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂g−1 i\n∂εi t\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\n(8)\n\n3.2 VARIATIONAL INFERENCE FOR CAUSAL DISCOVERY\n\nRhino adopts a Bayesian view of causal discovery (Heckerman et al., 2006), which aims to learn a graph posterior distribution instead of inferring a single graph. For N observed multivariate time series X (1)\n\n0:T , the joint likelihood with model parameter θ is\n\n0:T , . . . , X (N )\n\np(X (1)\n\n0:T , . . . , X (N )\n\n0:T , G) = p(G)\n\nN (cid:89)\n\nn=1\n\npθ(X (n)\n\n0:T |G).\n\n(9)\n\nGraph Prior When designing the graph prior, we combine three components: (1) DAG constraint; (2) graph sparseness prior; (3) domain-specific prior knowledge (optional). Inspired by the NOTEARS (Zheng et al., 2018; Geffner et al., 2022; Morales-Alvarez et al., 2021), we propose the following unnormalised prior\n\np(G) ∝ exp (cid:0)−λs∥G0:K∥2\n\nF − ρh2(G0) − αh(G0) − λp∥G0:K − Gp\n\n0:K∥2\n\nF\n\n(cid:1)\n\n(10)\n\nwhere h(G) = tr(eG⊙G) − D is the DAG penalty proposed in (Zheng et al., 2018) and is 0 if and only if G is a DAG; ⊙ is the Hadamard product; Gp is an optional domain-specific prior graph, which can be used when partial domain knowledge is available; λs, λp specify the strength of the graph sparseness and domain-specific prior terms respectively; α, ρ characterize the strength of the DAG penalty. Since the lagged connections specified in G1:K can only follow the direction of time, only the instantaneous part, G0, can contain cycles. Thus, the DAG penalty is only applied to G0.\n\nVariational Objective Unfortunately, the exact graph posterior p(G|X (1) 0:T ) is intractable due to the large combinatorial space of DAGs. To overcome this challenge, we adopt variational inference (Blei et al., 2017; Zhang et al., 2018), which uses a variational distribution qφ(G) to approximate the true posterior. We choose qφ(G) to be a product of independent Bernoulli distributions (refer to Appendix E for details). The corresponding evidence lower bound (ELBO) is\n\n0:T , . . . , X (N )\n\n(cid:16)\n\nlog pθ\n\nX (1)\n\n0:T , . . . , X (N )\n\n0:T\n\n(cid:17)\n\n≥ Eqφ(G)\n\n(cid:124)\n\n(cid:34) N\n\n(cid:88)\n\nn=1\n\nlog pθ(X (n)\n\n0:T |G) + log p(G)\n\n+ H(qφ(G))\n\n(11)\n\n(cid:35)\n\n(cid:123)(cid:122) ELBO(θ,φ)\n\n(cid:125)\n\nwhere H(qφ(G)) is the entropy of qφ(G). From the causal Markov assumption and auto-regressive nature, we can further simplify\n\nlog pθ(X (n)\n\n0:T |G) =\n\nT (cid:88)\n\nD (cid:88)\n\nt=0\n\ni=1\n\nlog pθ(X i,(n)\n\nt\n\n|Pai\n\nG(< t), Pai\n\nG(t))\n\nand from Rhino’s functional form (Eq. (6)) proposed in Section 3.1\n\nlog pθ(X i,(n)\n\nt\n\n|Pai\n\nG(< t), Pai\n\nG(t)) = log pgi\n\n(cid:16)\n\nzi,(n)\n\nt\n\n|Pai\n\nG(< t)\n\n(cid:17)\n\n(12)\n\n(13)\n\nwhere zi,(n) G(t)) and pgi is defined in Eq. (8) (Appendix A for details). The parameters θ, φ are learned by maximizing the ELBO, where the Gumbel-softmax gradient\n\nt −fi(Pai\n\nG(< t), Pai\n\n= X i,(n)\n\nt\n\n4\n\nestimator is used (Jang et al., 2016; Maddison et al., 2016). We also leverage augmented Lagrangian training (Hestenes, 1969; Andreani et al., 2008), similar as Geffner et al. (2022), to anneal α, ρ to make sure Rhino only produces DAGs (refer to Appendix B.1 in Geffner et al. (2022)). Once Rhino is trained, the temporal causal graph can be inferred by G ∼ qφ(G).\n\nTreatment effect estimation As Rhino learns the causal graph and the functional relationship simultaneously, it can be extended for causal inference tasks such as treatment effect estimation (Geffner et al., 2022). See Appendix D for details.\n\n4 THEORETICAL CONSIDERATIONS\n\nHere, we show the theoretical guarantees of Rhino including (1) the structural identifiability of Rhino SEMs and (2) soundness of the proposed variational inference framework. Together, they guarantee the validity of Rhino as a causal discovery method. In the end, we clarify relations to existing works.\n\n4.1 STRUCTURAL IDENTIFIABILITY\n\nOne of the key challenges for causal discovery with a flexible functional relationship is to show the structural identifiability. Namely, we cannot find two different graphs that induce the same joint likelihood from the proposed functional causal model. In the following, we present a theorem for Rhino SEMs that summarizes our main theoretical contribution. Theorem 1 (Identifiability of Rhino SEMs). Assuming Rhino SEMs satisfy the causal Markov property, minimality, sufficiency, DAGness and the induced joint likelihood has a proper density (see Appendix B for details), and (1) all functions and induced distributions are third-order differentiable; (2) function fi is non-linear and not invertible w.r.t. any nodes in Pai G(t); (3) the double derivative (log pgi (gi(εi t is zero at most at some discrete points, then the SEM with the form defined in Eq. (6) is structural identifiable for both bivariate and multivariate case.\n\nG(< t)))′′ w.r.t εi\n\nt)|Pai\n\nSketch of proof. This theorem is a summary of a collection of theorems proved in Appendix B. The strategy is instead of directly proving the identifiability of the model, we provide identifiability conditions for a general temporal SEMs, followed by showing a generalization of Rhino SEMs satisfies these conditions. The identifiability of Rhino SEMs directly follows from it.\n\nProve bivariate identifiability conditions for general temporal SEMs The first step is to prove the bivariate identifiability conditions that a general temporal SEMs (Eq. (2)) should satisfy (refer to Theorem 3 in Appendix B.1). Inspired by the techniques from Peters et al. (2013), we proved that temporal SEMs are bivariate identifiable if (1) the model for initial conditions is identifiable; (2) the model is identifiable w.r.t. instantaneous parents. Compared to Peters et al. (2013), we relaxed the identifiable model class condition so that it can be applied to history-dependent noise. In particular, (2) implies we only need to pay attention to instantaneous parents, rather than the entire parents (Peters et al., 2013), and opens the door for flexible lagged dependency.\n\nIdentifiability of history-dependent post non-linear model Next, we propose a novel generalization of Rhino SEMs, called history-dependent PNL. Theorem 4 and Corollary 4.1 in Appendix B.2 prove it is bivariate identifiable w.r.t. instantaneous parents (i.e. satisfy the conditions in Theorem 3) with additional assumptions (1), (2) and (3) in Theorem 1. The functional form of history-dependent PNL is defined as\n\nX i\n\n(cid:0)fit\n\nt = νit\n\nG(t)(cid:1) + git where ν is invertible w.r.t. the first argument. Due to its similarity to PNL (Zhang & Hyvarinen, 2012) and ANM (Hoyer et al., 2008), we combine their techniques to prove our results. The bivariate identifiability of Rhino SEMs directly follows from this with ν being the identity mapping.\n\nG(< t), Pai\n\nG(< t)(cid:1) ,\n\nG(< t), εit\n\n(cid:1) , Pai\n\n(cid:0)Pai\n\n(cid:0)Pai\n\nGeneralization to multivariate case In the end, we prove the above bivariate identifiability can be generalized to the multivariate case by adapting the techniques from Peters et al. (2012) and combining it with the proof strategy from step 1. Refer to Theorem 5 in Appendix B.3 for details.\n\n5\n\n4.2 VALIDITY OF VARIATIONAL OBJECTIVE AND RELATIONS TO OTHER METHODS\n\nNext, we show the validity of the variational objective (Eq. (11)) in the sense that optimizing it can lead to the ground truth graph. Theorem 1 in Geffner et al. (2022) justifies the validity of the variational objective under the same set of assumptions as Rhino. Theorem 2 (Validity of variational objective (Geffner et al., 2022)). Assuming the conditions in Theorem 1 are satisfied, and further assume no model misspecification (see Definition B.1), then the solution (θ′, q′ φ(G) = δ(G = G′), where G′ is a unique graph. In particular, G′ = G∗ and pθ′(X0:T ; G′) = p(X0:T ; G∗), where G∗ is the ground truth graph and p(X0:T ; G∗) is the true data generating distribution.\n\nφ(G)) from optimizing Eq. (11) with infinite data satisfies q′\n\nWe emphasize that Theorem 2 guarantees the correctness with the global optimum, rather than characterizing the convergence during optimizing Eq. (11). We assume the Rhino only searches in the DAG space. In practice, with augmented Lagrangian (Wei et al., 2020; Ng et al., 2022), we can only obtain an approximate posterior over DAGs, converging to a local optimum.\n\nRelation to other methods Many previous works of using functional causal model for causal time series discovery (Hyv ̈arinen et al., 2010; Pamfil et al., 2020; Tank et al., 2018; Peters et al., 2013) are closely related to Rhino. Since Rhino SEMs incorporate history-dependent noise with flexible function non-linearity, it is the most flexible member of this family. Refer to Appendix C for details.\n\n5 RELATED WORK\n\nAssaad et al. (2022) provides a comprehensive overview of causal discovery method for time series. The first is Granger causality, which can be further split into (1) vector auto-regression (Wu et al., 2020; Shojaie & Michailidis, 2010; Siggiridou & Kugiumtzis, 2015; Amornbunchornvej et al., 2019) and (2) deep learning (L ̈owe et al., 2022; Tank et al., 2018; Bussmann et al., 2021; Dang et al., 2018; Xu et al., 2019). Granger causality methods cannot handle instantaneous effects, which can be observed in a slow-sampling system. Additionally, they also assume a fixed noise distribution.\n\nUsing funcitonal causal models can mitigate the aforementioned two problems. VARLiNGaM (Hyv ̈arinen et al., 2010) extends the identifiability theory of linear non-Gaussian ANM (Shimizu et al., 2006) to vector auto-regression for modelling time series data. DYNOTEARS (Pamfil et al., 2020) leverages the recently proposed NOTEARS framework (Zheng et al., 2018) to continuously relax the DAG constraints for fully differentiable DAG structure learning. However, the above approach is still limited to linear functional forms. TiMINo (Peters et al., 2013) provides a general theoretical framework for temporal causal discovery with functional causal models. Our theory leverages some of their proof techniques. Unfortunately, all the aforementioned methods assume no history dependency for the noise. On the other hand, Rhino can model (1) non-linear function relations; (2) instantaneous effect; (3) and history-dependent noise at the same time.\n\nThe third category is constraint-based approaches. Due to its non-parametric nature, it can handle history-dependent noise. PCMCI (Runge et al., 2019) combines PC (Spirtes et al., 2000) and conditional independence test for discovery from time series. PCMCI+ (Runge, 2018; 2020) further extends PCMCI to infer both lagged and instantaneous effects. CD-NOD (Huang et al., 2020) has recently been proposed to handle non-stationary heterogeneous data, where the data distribution can shift across time. Despite their generality, they can only infer MECs; cannot learn the explicit functional forms between variables; and require a stronger assumption than minimality (i.e. faithfulness).\n\n6 EXPERIMENTS\n\nWe release the code of Rhino for reproducing the following experiments.2.\n\n6.1 SYNTHETIC DATA\n\nWe evaluate our method on a large set of synthetically generated datasets with known causal graphs. We use the main body of this paper to present the overall performance of our method compared to rel-\n\n2https://github.com/microsoft/causica/tree/v0.0.0\n\n6\n\nevant baselines and one ablation study on the robustness to lag mismatch. In Appendix F.3, we conduct extensive analysis, including (1) on different graph type; (2) ablation on history-dependency; (3) ablation study on instantaneous effect. This set of datasets are generated by various settings (e.g. type of graphs, instantaneous/no instantaneous effect, etc.). 5 datasets are generated for each combination of settings with different seeds, yielding 160 datasets in total. In order to comprehensively test Rhino’s robustness, we deliberately generated 75% of the datasets that mismatch the Rhino configurations. Details of the data generation can be found in Appendix F.1.\n\nWe compare Rhino to a wide range of baselines, including VARLiNGaM (Hyv ̈arinen et al., 2010), PCMCI+(Runge, 2020) and DYNOTEARS (Pamfil et al., 2020). PCMCI only outputs Markov equivalence classes (MECs). We resolve this by enumerating all DAGs in the MEC. For details on the methods, see Appendix F.2. Additionally, we include two variants of Rhino: (1) Rhino+g, where an independent Gaussian noise is used; (2) Rhino+s, where Gaussian εi is transformed by an independent spline.\n\nFigure 1 presents the F1 score of the lagged, instantaneous and temporal adjacency matrix of all methods aggregated over all datasets3, denoted as ’Lag’, ’Inst.’ and ’Temporal’, respectively. Rhino achieves overall competitive or the best performance in terms of the full temporal adjacency matrix across all possible datasets, especially for lower dimensions. Comparing Rhino’s lagged discovery to its two variants, the better score indicates the history-dependent noise is useful to the lagged graph discovery, contributing to the better overall F1 performance (Appendix F.3 for ablation).\n\nDespite of the strong performance from PCMCI+, it can only identify the graph up to MECs without explicit functional relations. Computationally, PCMCI+ exceeds the maximum training time of 1 week on 40 nodes (see Appendix F.3), suggesting its computation bottleneck in high dimensions.\n\nDYNOTEARS achieves inferior results in general due to limited modelling power from the linear nature. This is much clearer in high dimensions due to the increasing difficulty of the problem.\n\nFigure 1: F1-scores of Rhino (light yellow) compared to all baseline methods. The different subplots show the performance for dataset with different number of nodes. ‘L=2’ refers to models with lag 2.\n\nWe explore the behaviour of Rhino with different lag parameters other than the ground truth lag 2. From Table 1, worse training log-likelihoods suggest that Rhino with insufficient history (lag = 1) is unable to correctly model the data and this leads to a decrease in F1 scores. Interestingly, Rhino is also robust with longer lags. Despite of the slightly better likelihood (lag = 3), it achieves comparable performance to the model with the correct lag. Also, from their similar F1 Lag score, it suggests the extra adjacency matrix is nearly empty.\n\n6.2 DREAM3 GENE NETWORK\n\nIn this section, we evaluate Rhino performance with a real-world biology benchmark called DREAM3 (Prill et al., 2010; Marbach et al., 2009). These datasets are often used to evaluate Granger causality (Khanna & Tan, 2019; Tank et al., 2018; Nauta et al., 2019; Bussmann et al., 2021) but\n\n3We note that we run each method on 40 different dataset settings for all possible numbers of nodes.\n\n7\n\n0.00.51.0F1Dim = 5Dim = 10Inst.LagTemporal0.00.51.0F1Dim = 20Inst.LagTemporalDim = 40Model NameRhino (L=2)Rhino+g (L=2)Rhino+s (L=2)DYNOTEARSVAR-LiNGaMPCMCI+Dim\n\n5\n\n10\n\n20\n\n40\n\nF1 Inst. F1 Lag F1 Temporal LL\n\nF1 Inst. F1 Lag F1 Temporal LL\n\nF1 Inst. F1 Lag F1 Temporal LL\n\nF1 Inst. F1 Lag F1 Temporal LL\n\nRhino (L=1)\n\nRhino (L=2)\n\nRhino (L=3)\n\n0.11 ± 0.17 0.28 ± 0.13 0.34 ± 0.12 −4.14 ± 1.63\n\n0.13 ± 0.17 0.26 ± 0.08 0.28 ± 0.11 −7.97 ± 2.09\n\n0.20 ± 0.22 0.59 ± 0.22 0.59 ± 0.20 −3.83 ± 1.62\n\n0.19 ± 0.23 0.51 ± 0.17 0.49 ± 0.18 −7.21 ± 2.22\n\n0.21 ± 0.23 0.57 ± 0.24 0.56 ± 0.22 −3.75 ± 1.64\n\n0.19 ± 0.22 0.48 ± 0.19 0.45 ± 0.20 −7.01 ± 1.91\n\n0.15 ± 0.16 0.24 ± 0.12 0.25 ± 0.13\n\n0.18 ± 0.19 0.40 ± 0.21 0.37 ± 0.21 −15.62 ± 3.16 −14.70 ± 2.87 −14.72 ± 2.82\n\n0.16 ± 0.17 0.42 ± 0.22 0.39 ± 0.22\n\n0.13 ± 0.16 0.20 ± 0.18 0.20 ± 0.18\n\n0.18 ± 0.21 0.34 ± 0.30 0.32 ± 0.29 −31.44 ± 5.16 −30.10 ± 4.71 −30.20 ± 4.74\n\n0.22 ± 0.23 0.40 ± 0.31 0.37 ± 0.30\n\nTable 1: Comparison of the causal discovery performance of Rhino with different lag-parameters (L ∈ [1, 3]). Apart from the 3 F1 scores, LL shows the log-likelihood of the training data.\n\nrecently adopted for SEM-based method (Pamfil et al., 2020). The dataset consists in silico measurements of gene expression levels for 5 different networks. Each network contains d = 100 genes. Each time series represents a perturbation trajectory with time length T = 21. For each network, 46 perturbation trajectories are recorded. The goal is to infer the causal structure of each network. We use the area under the ROC curve (AUROC) as the performance metric. We consider the same baselines as in the synthetic experiments (i.e. DYNOTEARS and PCMCI+) without VARLiNGaM since its default implementation fails when the number of variables (d = 100) is greater than the series length (T = 21). Additionally, we also consider relevant Granger causality methods, including cMLP, cLSTM (Tank et al., 2018); TCDF(Nauta et al., 2019); SRU and eSRU (Khanna & Tan, 2019)). Their corresponding results are directly cited from Khanna & Tan (2019). Appendix G.1 specifies Rhino hyperparameters. Since the ground truth graph is a summary graph (see Definition G.1 in Appendix G.2), Appendix G.2 details about the post-processing step on aggregating temporal graph to summary graph for Rhino, DYNOTEARS and PCMCI+.\n\nMethod\n\ncMLP cLSTM TCDF SRU eSRU DYNO. PCMCI+ Rhino+g Rhino\n\nE.Coli 1\n\nE.Coli 2\n\nYeast 1\n\nYeast 2\n\nYeast 3\n\n0.644 0.629 0.614 0.657 0.66 0.590 0.530 ± 0.002 0.673±0.013 0.685±0.003\n\n0.568 0.609 0.647 0.666 0.629 0.547 0.519 ± 0.002 0.665±0.009 0.680±0.007\n\n0.585 0.579 0.581 0.617 0.627 0.527 0.530 ± 0.003 0.659±0.005 0.664±0.006\n\n0.506 0.519 0.556 0.575 0.557 0.526 0.510 ± 0.001 0.598±0.004 0.585±0.004\n\n0.528 0.555 0.557 0.55 0.55 0.510 0.512 ± 0 0.588±0.005 0.567±0.003\n\nTable 2: The AUROC of the aggregated adjacency matrix for 5 DREAM3 datasets without selfconnections. DYNO. means DYNOTEARS. For Rhino and PCMCI+, the results are reported by averaging over 5 runs. Khanna & Tan (2019) only reported the single-run results for baselines.\n\nTable 2 demonstrates the AUROC of the summary graph inferred after training. It is clear that Rhino and its variant outperform all other methods. Although Rhino is not formulated to solve the summary graph discovery, it shows a clear advantage compared to the state-of-the-art Granger causality. Thus, Rhino can be used to infer either temporal or summary graph depending on users’ needs.\n\n8\n\nBy inspecting the hyperparameters of Rhino in Appendix G.1, instantaneous effects seem to provide no obvious help for in these datasets. It suggests the recording intervals are fast enough to avoid any aggregation effect. This explains why the Granger causality can also perform reasonably well.\n\nUnlike the strong performances of DYNOTEARS and PCMCI+ in synthetic experiments, they perform poorly in DREAM3. The linear nature of DYNOTEARS seems to harm its performance drastically. PCMCI+ suffers from the low independence test power under small training data.\n\nAnother interesting ablation is to compare with Rhino+g, which performs on par with Rhino and achieves better scores on 2 out of 5 datasets. Although we have no access to the true noise mechanism, we suspect that the added noise is not history-dependent and highly likely to be Gaussian. Despite the model mismatch, Rhino is still one of the best methods for this problem. This further strengthens our belief in the robustness of our model under different setups.\n\n6.3 NETSIM BRAIN CONNECTIVITY\n\nIn this section, we evaluate Rhino using fMRI imaging data, which has also been used as a benchmark for temporal causal discovery (L ̈owe et al., 2022; Khanna & Tan, 2019; Assaad et al., 2022). Each time series represents the signal simulated for a human subject, which describes d = 15 different regions in the brain with T = 200 timestamps. The goal is to infer the connectivity between different brain regions. We assume that different human subjects share the same connectivity. We only use the data from human subject 2 − 6 in Sim-3.mat from https://www.fmrib.ox.ac. uk/datasets/netsim/index.html with self-connections. We use the same set of baselines as DREAM3 (Section 6.2) plus VARLiNGaM. Appendix G.4 describes hyperparameter settings.\n\nTable 3 shows the AUROCs for different methods. Remarkably, the proposed Rhino and its variants achieve significantly better AUROC compared to the baselines. Especially, Rhino obtains nearly optimal AUROC, demonstrating its robustness to the small dataset and good balances between true and false positive rates (see Appendix H). By comparing Rhino and Rhino+NoInst., we conclude that modelling instantaneous effects is important in real application, indicating the sampling interval is not frequent enough to explain everything as lagged effects. This can be double confirmed by comparing Rhino+NoInst with Granger causality, where it performs on par with the state-ofthe-art baseline when disabling the instantaneous effect. Last but not least, by comparing Rhino+g with Rhino, we find that historydependent noise is also helpful in this dataset.\n\n7 CONCLUSION\n\nMethod\n\ncMLP cLSTM TCDF SRU eSRU DYNO. PCMCI+ VARLiNGaM\n\nAUROC\n\n0.93 0.83 0.91 0.80 0.88 0.90 0.83 ± 0 0.84 ± 0\n\nRhino+g Rhino+NoInst. Rhino\n\n0.974 ± 0.002 0.93 ± 0.006 0.99 ± 0.001\n\nTable 3: AUROCs of the summary graph. Rhino+NoInst is Rhino without instantaneous effects. For Rhino, VARLiNGaM, PCMCI+, results are obtained by averaging over 5 different runs.\n\nInferring temporal causal graphs from observational time series is an important task in many scientific fields. Especially, some applications (e.g. education, climate science, etc.) require the modelling of non-linear relationships; instantaneous effects and history-dependent noise distributions at the same time. Previous works fail to offer an appropriate solution for all three requirements. Motivated by this, we propose Rhino, which combines vector auto-regression with deep learning and variational inference to perform causal temporal relationship learning with all three requirements. Theoretically, we prove the structural identifiability of Rhino with flexible history-dependent noise, and clarify its relations to existing works. Empirical evaluations demonstrate its superior performance and robustness when Rhino is misspecified, and the advantages of history-dependent noise mechanisms. This opens an exciting route of extending Rhino to handle non-stationary time-series and unobserved confounders in future work.\n\n9\n\n8 REPRODUCIBILITY STATEMENT\n\nTheoretical Contributions The main theoretical contribution is summarized in Theorem 1. This theorem is the result from a collection of theorems proved in Appendix B. In Appendix B, we detailed the fundamental assumptions (Assumption 1-Assumption 5) required for the all theorems. The theorem-specific assumptions are mentioned in the statement of the theorem. To ease the understanding of the proof, we also provide the skecth of proof in Theorem 1. Since Theorem 2 is directly cited from (Geffner et al., 2022) without major modification, the proof can be found in Appendix A in Geffner et al. (2022).\n\nEmpirical Evaluations For synthetic, DREAM3 and Netsim experiments, we listed the hyperparameters in Appendix F.2, Appendix G.1 and Appendix G.4, respectively. Appendix F.1 explains the synthetic data generation. For DREAM3 and Netsim, the dataset can be found in the public github repo https://github.com/sakhanna/SRU_for_GCI/tree/master/data. The post processing steps for DREAM3 and Netsim evaluations are described in Appendix G.2.\n\nREFERENCES\n\nChainarong Amornbunchornvej, Elena Zheleva, and Tanya Y Berger-Wolf. Variable-lag granger causality for time series analysis. In 2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pp. 21–30. IEEE, 2019.\n\nRoberto Andreani, Ernesto G Birgin, Jos ́e Mario Mart ́ınez, and Mar ́ıa Laura Schuverdt. On augmented lagrangian methods with general lower-level constraints. SIAM Journal on Optimization, 18(4):1286–1309, 2008.\n\nCharles K Assaad, Emilie Devijver, and Eric Gaussier. Survey and evaluation of causal discovery\n\nmethods for time series. Journal of Artificial Intelligence Research, 73:767–819, 2022.\n\nCarlo Berzuini, Philip Dawid, and Luisa Bernardinell. Causality: Statistical perspectives and ap-\n\nplications. John Wiley & Sons, 2012.\n\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-\n\ncians. Journal of the American statistical Association, 112(518):859–877, 2017.\n\nBart Bussmann, Jannes Nys, and Steven Latr ́e. Neural additive vector autoregression models for causal discovery in time series. In International Conference on Discovery Science, pp. 446–460. Springer, 2021.\n\nXuan-Hong Dang, Syed Yousaf Shah, and Petros Zerfos. seq2graph: Discovering dynamic dependencies from multivariate time series with multi-level attention. arXiv preprint arXiv:1812.04448, 2018.\n\nHadi Mohaghegh Dolatabadi, Sarah Erfani, and Christopher Leckie. Invertible generative modeling using linear rational splines. In International Conference on Artificial Intelligence and Statistics, pp. 4236–4246. PMLR, 2020.\n\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Ad-\n\nvances in neural information processing systems, 32, 2019.\n\nTomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, et al. Deep end-to-end causal inference. arXiv preprint arXiv:2202.02195, 2022.\n\nClive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.\n\nEconometrica: journal of the Econometric Society, pp. 424–438, 1969.\n\nRuocheng Guo, Lu Cheng, Jundong Li, P Richard Hahn, and Huan Liu. A survey of learning causality with data: Problems and methods. ACM Computing Surveys (CSUR), 53(4):1–37, 2020.\n\nDavid Heckerman, Christopher Meek, and Gregory Cooper. A bayesian approach to causal discov-\n\nery. In Innovations in Machine Learning, pp. 1–28. Springer, 2006.\n\n10\n\nMagnus R Hestenes. Multiplier and gradient methods. Journal of optimization theory and applica-\n\ntions, 4(5):303–320, 1969.\n\nPatrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Sch ̈olkopf. Nonlinear causal discovery with additive noise models. Advances in neural information processing systems, 21, 2008.\n\nBiwei Huang, Kun Zhang, Jiji Zhang, Joseph D Ramsey, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Sch ̈olkopf. Causal discovery from heterogeneous/nonstationary data. J. Mach. Learn. Res., 21(89):1–53, 2020.\n\nAapo Hyv ̈arinen, Kun Zhang, Shohei Shimizu, and Patrik O Hoyer. Estimation of a structural vector autoregression model using non-gaussianity. Journal of Machine Learning Research, 11(5), 2010.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\n\npreprint arXiv:1611.01144, 2016.\n\nSaurabh Khanna and Vincent YF Tan. Economy statistical recurrent units for inferring nonlinear\n\ngranger causality. arXiv preprint arXiv:1911.09879, 2019.\n\nIlyes Khemakhem, Ricardo Monti, Robert Leech, and Aapo Hyvarinen. Causal autoregressive flows. In International conference on artificial intelligence and statistics, pp. 3520–3528. PMLR, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nSteffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.\n\nSindy L ̈owe, David Madras, Richard Zemel, and Max Welling. Amortized causal discovery: Learning to infer causal graphs from time-series data. In Conference on Causal Learning and Reasoning, pp. 509–525. PMLR, 2022.\n\nChao Ma and Cheng Zhang. Identifiable generative models for missing not at random data imputa-\n\ntion. Advances in Neural Information Processing Systems, 34:27645–27658, 2021.\n\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\n\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\n\nDaniel Marbach, Thomas Schaffter, Claudio Mattiussi, and Dario Floreano. Generating realistic in silico gene networks for performance assessment of reverse engineering methods. Journal of computational biology, 16(2):229–239, 2009.\n\nRaha Moraffah, Paras Sheth, Mansooreh Karami, Anchit Bhattacharya, Qianru Wang, Anique Tahir, Adrienne Raglin, and Huan Liu. Causal inference for time series analysis: Problems, methods and evaluation. Knowledge and Information Systems, pp. 1–45, 2021.\n\nPablo Morales-Alvarez, Angus Lamb, Simon Woodhead, Simon Peyton Jones, Miltiadis Allamanis, and Cheng Zhang. Vicause: Simultaneous missing value imputation and causal discovery with groups. arXiv preprint arXiv:2110.08223, 2021.\n\nMeike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolu-\n\ntional neural networks. Machine Learning and Knowledge Extraction, 1(1):19, 2019.\n\nIgnavier Ng, S ́ebastien Lachapelle, Nan Rosemary Ke, Simon Lacoste-Julien, and Kun Zhang. On the convergence of continuous constrained optimization for structure learning. In International Conference on Artificial Intelligence and Statistics, pp. 8176–8198. PMLR, 2022.\n\nRoxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Georgatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data. In International Conference on Artificial Intelligence and Statistics, pp. 1595–1605. PMLR, 2020.\n\nNick Pawlowski, Daniel Coelho de Castro, and Ben Glocker. Deep structural causal models for tractable counterfactual inference. Advances in Neural Information Processing Systems, 33:857– 869, 2020.\n\n11\n\nJonas Peters, Joris Mooij, Dominik Janzing, and Bernhard Sch ̈olkopf. Identifiability of causal graphs\n\nusing functional models. arXiv preprint arXiv:1202.3757, 2012.\n\nJonas Peters, Dominik Janzing, and Bernhard Sch ̈olkopf. Causal inference on time series using restricted structural equation models. Advances in Neural Information Processing Systems, 26, 2013.\n\nJonas Peters, Dominik Janzing, and Bernhard Sch ̈olkopf. Elements of causal inference: foundations\n\nand learning algorithms. The MIT Press, 2017.\n\nRobert J Prill, Daniel Marbach, Julio Saez-Rodriguez, Peter K Sorger, Leonidas G Alexopoulos, Xiaowei Xue, Neil D Clarke, Gregoire Altan-Bonnet, and Gustavo Stolovitzky. Towards a rigorous assessment of systems biology models: the dream3 challenges. PloS one, 5(2):e9202, 2010.\n\nJakob Runge. Causal network reconstruction from time series: From theoretical assumptions to practical estimation. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(7):075310, 2018.\n\nJakob Runge. Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear In Conference on Uncertainty in Artificial Intelligence, pp. 1388–1397.\n\ntime series datasets. PMLR, 2020.\n\nJakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detecting and quantifying causal associations in large nonlinear time series datasets. Science advances, 5 (11):eaau4996, 2019.\n\nRajen D Shah and Jonas Peters. The hardness of conditional independence testing and the gener-\n\nalised covariance measure. The Annals of Statistics, 48(3):1514–1538, 2020.\n\nShohei Shimizu, Patrik O Hoyer, Aapo Hyv ̈arinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.\n\nAli Shojaie and George Michailidis. Discovering graphical granger causality using the truncating\n\nlasso penalty. Bioinformatics, 26(18):i517–i523, 2010.\n\nElsa Siggiridou and Dimitris Kugiumtzis. Granger causality in multivariate time series using a timeordered restricted vector autoregressive model. IEEE Transactions on Signal Processing, 64(7): 1759–1773, 2015.\n\nPeter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction,\n\nand search. MIT press, 2000.\n\nAlex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily Fox. Neural granger causality for\n\nnonlinear time series. stat, 1050:16, 2018.\n\nBrian L Trippe and Richard E Turner. Conditional density estimation with bayesian normalising\n\nflows. arXiv preprint arXiv:1802.04908, 2018.\n\nDennis Wei, Tian Gao, and Yue Yu. Dags with no fears: A closer look at continuous optimization for learning bayesian networks. Advances in Neural Information Processing Systems, 33:3895–3906, 2020.\n\nTailin Wu, Thomas Breuel, Michael Skuhersky, and Jan Kautz. Discovering nonlinear relations with\n\nminimum predictive information regularization. arXiv preprint arXiv:2001.01885, 2020.\n\nChenxiao Xu, Hao Huang, and Shinjae Yoo. Scalable causal graph learning through a deep neural network. In Proceedings of the 28th ACM international conference on information and knowledge management, pp. 1853–1862, 2019.\n\nCheng Zhang, Judith B ̈utepage, Hedvig Kjellstr ̈om, and Stephan Mandt. Advances in variational IEEE transactions on pattern analysis and machine intelligence, 41(8):2008–2026,\n\ninference. 2018.\n\n12\n\nKun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. arXiv\n\npreprint arXiv:1205.2599, 2012.\n\nKun Zhang, Zhikun Wang, Jiji Zhang, and Bernhard Sch ̈olkopf. On estimation of functional causal models: general results and application to the post-nonlinear causal model. ACM Transactions on Intelligent Systems and Technology (TIST), 7(2):1–22, 2015.\n\nXun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous optimization for structure learning. Advances in Neural Information Processing Systems, 31, 2018.\n\n13\n\nA ELBO AND LIKELIHOOD DERIVATION\n\nThe goal is to derive a lower bound for the joint likelihood pθ(X (1)\n\n0:T , . . . , X (N )\n\n0:T ).\n\npθ(X (1) (cid:90)\n\n0:T , . . . , X (N ) 0:T ) X (1)\n\n(cid:16)\n\npθ\n\n= log\n\n0:T , . . . , X (N )\n\n0:T |G\n\n(cid:17)\n\np(G)dG\n\n= log\n\n(cid:90) qφ(G) qφ(G)\n\n(cid:16)\n\npθ\n\nX (1)\n\n0:T , . . . , X (N )\n\n0:T |G\n\n(cid:17)\n\np(G)dG\n\n(cid:90)\n\n≥\n\nqφ(G) log pθ\n\n(cid:16)\n\nX (1)\n\n0:T , . . . , X (N )\n\n0:T |G\n\n(cid:17)\n\np(G)dG + H(qφ(G))\n\n(14)\n\n=Eqφ(G)\n\n(cid:34) N\n\n(cid:88)\n\nn=1\n\nlog pθ(X (n)\n\n0:T |G) + log p(G)\n\n+ H(qφ(G))\n\n(cid:35)\n\nwhere Eq. (14) is obtained by using Jensen’s inequality.\n\nWe can further simplify the likelihood pθ(X (n)\n\n0:T |G):\n\nlog pθ(X (n)\n\n0:T |G) = log\n\nT (cid:89)\n\nt=0\n\npθ(X (n)\n\nt\n\n|X (n)\n\n<t , G)\n\n=\n\n=\n\n(cid:16)\n\nlog pθ\n\nX (n)\n\nt\n\n|X (n)\n\n<t , G\n\n(cid:17)\n\nT (cid:88)\n\nt=0\n\nT (cid:88)\n\nD (cid:88)\n\nt=0\n\ni=1\n\nlog pθ\n\n(cid:16)\n\nX i,(n)\n\nt\n\n|Pai\n\nG(< t), Pai\n\nG(t)\n\n(cid:17)\n\n(15)\n\nwhere Eq. (15) is obtained through Markov factorization (Lauritzen, 1996).\n\nB STRUCTURAL IDENTIFIABILITY\n\nIn this section, we will focus on proving the structural identifiability of Rhino SEMs. Before diving into the details, let us clarify the required assumptions. i (Pai, εi) Definition B.1 (Correctly specified model). For a true data generating mechanism X i = f ∗ for i = 1, . . . , D, we say a model with functional space F is correctly specified if there exists a function fi ∈ F s.t. fi = f ∗\n\ni for all i = 1, . . . , D.\n\nHere, we emphasize that the above definition of model specification does not require identifiability in parameters space, but in function space instead. Namely, it allows multiple sets of parameters that correspond to the same function. Our definition is more general than some of the previous work, which enforces parameter identifiability (e.g. Def 2.1 in Ma & Zhang (2021))\n\nAssumption 1 (Causal Stationarity (Runge, 2018)). The time series process Xt with a graph G is called causally stationary over a time index set T if and only if for all links X i t in the graph\n\nt−τ → X j\n\nX i\n\nt−τ ̸⊥⊥ X j\n\nt |X<t\\{X i\n\nt−τ } holds for all t ∈ T\n\nThis characterizes the nature of the time-series data generating mechanism, which validates the choice of the auto-regressive model.\n\nAssumption 2 (Causal Markov Property (Peters et al., 2017)). Given a DAG G and a joint distribution p, this distribution is said to satisfy causal Markov property w.r.t. the DAG G if each variable is independent of its non-descendants given its parents.\n\nThis is a common assumptions for the distribution induced by an SEM. With this assumption, one can deduce conditional independence between variables from the graph.\n\n14\n\nAssumption 3 (Causal Minimality). Consider a distribution p and a DAG G, we say this distribution satisfies causal minimality w.r.t. G if it is Markovian w.r.t. G but not to any proper subgraph of G.\n\nMinimality is also a common assumption for SEMs (Hoyer et al., 2008; Zhang & Hyvarinen, 2012; Peters et al., 2012), which can be regarded as a weaker version of faithfulness (Peters et al., 2017).\n\nAssumption 4 (Causal Sufficiency). A set of observed variables V is causally sufficient for a process Xt if and only if in the process every common cause of any two or more variables in V is in V or has the same value for all units in the population.\n\nThis assumption implies there are no latent confounders present in the time-series data.\n\nAssumption 5 (Well-defined Density). We assume the joint likelihood induced by the Rhino SEM (Eq. (6)) is absolutely continuous w.r.t. a Lebesgue or counting measure and | log p(X0:T ; G)| < ∞ for all possible G.\n\nThis assumption is to make sure the induced distribution has a well-defined probability density function. It is also required for the equivalence of the global, local Markov property and Markov factorization property (Theorem 6.22 from Peters et al. (2017)).\n\nAssumption 6 (Rhino in DAG space). We assume the Rhino framework can only return the solutions from DAG space. Namely, the posterior distribution from Rhino can only put weights on DAGs.\n\nThis assumption regularizes the search space of Rhino to be DAG space, which aligns with assumption 2 and 3 on causal graphs.\n\nIn the following, we will structure the entire proof into three steps:\n\n1. Prove a general conditions that the bivariate time series model needs to satisfy for structural\n\nidentifiability. This adapts from the theorem 1 in Peters et al. (2013).\n\n2. Prove that a generalized form of SEM, modified from the post non-linear (PNL) model (Zhang & Hyvarinen, 2012), satisfies the conditions mentioned in step 1. The proposed Rhino (Eq. (6)) is a special case of the above SEM.\n\n3. In the end, we generalize the above indentifiability to the multivariate case.\n\nB.1 GENERAL IDENTIFIABILITY CONDITIONS\n\nFirst, we derive the conditions required for identifiability for a general bivariate time series SEM, defined as\n\nX i\n\nt = fi,t\n\n(cid:0)Pai\n\nG(< t), Pai\n\nG(t), εi\n\nt\n\n(cid:1) .\n\nWe call the above SEM transition model, since it only defines the transition behavior rather than the initial conditions. We also need to incorporate a source model, which characterizes the initial conditions:\n\n(16)\n\n(17)\n\nX i\n\ns = fi,s(Pai\n\nG, εi s)\n\nfor s ∈ [0, S], where S is the length for the initial conditions and Pai i. We define ps(X0:S ) as the induced joint distribution for the initial conditions.\n\nG contains the parents for node\n\nNow, we prove the following theorem.\n\nTheorem 3 (Identifiability conditions for bivariate time series). Assuming Assumption 1-5 are satisfied, given a bivariate temporal process X0:T and Y0:T that are governed by the above SEM (Eq. (16)) with source model (Eq. (17)), then the above SEM for the bivariate temporal process is structural identifiable if the following conditions are true:\n\n1. Source model fi,s is structural identifiable for all i = 1, . . . , D and s ∈ [0, S].\n\n2. The transition model (Eq. (16)) is bivariate identifiable w.r.t the instantaneous parents. G (< t)), then ∄G′ G′ (< t)) = p for all t ∈\n\nNamely, if graph G induced conditional distributions p(Xt, Yt|PaX,Y such that G ̸= G′ and the induced conditional ̄p(Xt, Yt|Pa [S + 1, T ].\n\nX,Y\n\n15\n\nwhere PaX,Y union of parents under G′.\n\nG (< t) is the union of the lagged parents of Xt and Yt under G, and Pa\n\nX,Y\n\nG′ (< t) is the\n\nProof. We prove this by contradiction. Assume we have an induced joint distribution p(X0:T , Y0:T ) under G, and corresponding ̄p under G′. We further assume the above two conditions in the theorem are met and p = ̄p but G ̸= G′.\n\nThus, we have DKL[p∥ ̄p] = 0. Due to the temporal nature of the model, we can further decompose it as the following:\n\nDKL[p∥ ̄p] (cid:90)\n\np(X0:T , Y0:T ) log\n\n=\n\np(X0:T , Y0:T ) ̄p(X0:T , Y0:T )\n\ndX0:T dY0:T\n\n=DKL[p(X0:S , Y0:S ) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) ps\n\n∥ ̄p(X0:S , Y0:S ) (cid:123)(cid:122) (cid:125) ̄ps\n\n(cid:124)\n\n] +\n\n(cid:90)\n\np(X0:S , Y0:S )DKL[p(XS+1:T , YS+1:T |X0:S , Y0:S )∥\n\n ̄p(XS+1:T , YS+1:T |X0:S , Y0:S )]dX0:S dY0:S\n\n=DKL[ps∥ ̄ps] +\n\nT (cid:88)\n\nt=S+1\n\n=0.\n\nEp(X0:t−1,Y0:t−1) [DKL [p(Xt, Yt|X0:t−1, Y0:t−1)∥ ̄p(Xt, Yt|X0:t−1, Y0:t−1)]]\n\nThis means we have DKL[ps∥ ̄ps] = 0 and DKL [p(Xt, Yt|X0:t−1, Y0:t−1)∥ ̄p(Xt, Yt|X0:t−1, Y0:t−1)] = 0 almost everywhere. Inspired by the strategy used in (Peters et al., 2013), We consider the following three scenarios:\n\nDisagree on initial conditions We assume G and G′ disagree on the initial conditions. From the condition 1, we know the source model fi,s is identifiable. Namely, we cannot find G ̸= G′ with disagreement on initial conditions such that DKL[ps∥ ̄ps] = 0. This is a contradiction, meaning that G and G′ must agree on the connections between initial set of nodes.\n\nDisagree on lagged parents only This means for all t ∈ [S + 1, T ], the instantaneous connections at t for G and G′ are the same, and ∃t ∈ [S + 1, T ] such that PaX,Y G′ (< t). We can use a similar argument as the theorem 1 in Peters et al. (2013). W.l.o.g., we assume under G, we have Xt−τ → Yt and there is no connections between them under G′. Thus, from Markov conditions, we have\n\nG (< t) ̸= Pa\n\nX,Y\n\nYt ⊥⊥ Xt−τ |X0:t−1 ∪ Y0:t−1 ∪ NDY\n\nt \\{Yt, Xt−τ }\n\nunder G′, where NDY minimality and proposition 6.16 in Peters et al. (2017), we have\n\nt are the non-descendants of node Yt at some time t. However, from the causal\n\nYt ̸⊥⊥ Xt−τ |X0:t−1 ∪ Y0:t−1 ∪ NDY\n\nt \\{Yt, Xt−τ }\n\nunder G. This means under this case, DKL [p(Xt, Yt|X0:t−1, Y0:t−1)∥ ̄p(Xt, Yt|X0:t−1, Y0:t−1)] ̸= 0, which is a contradiction.\n\nDisagree also on instantaneous parents This scenarior means ∃t ∈ [S + 1, T ] such that they disagree on instantaneous parents. W.l.o.g. we assume Xt → Yt under G and Yt → Xt under G′.\n\nLet’s define X0:t−1 ∪ Y0:t−1 = h, hY h contains the parent values under G′, and hX distributions from SEM (Eq. (16)) with G, G′ are\n\nG ⊆ h contains the values of PaY\n\nG′ ⊆ G′ accordingly. Thus, the induced conditional\n\nG(< t) under G, ̄hY\n\nG , ̄hX\n\np(Xt, Yt|hX\n\nG ∪ hY\n\nG)\n\nand\n\n ̄p(Xt, Yt| ̄hX\n\nG′ ∪ ̄hY\n\nG′)\n\nFrom the Markov conditions, we have\n\np(Xt, Yt|X0:t−1, Y0:t−1) = p(Xt, Yt|PaX,Y\n\nG (< t))\n\n16\n\nTherefore, we have\n\nDKL [p(Xt, Yt|h)∥ ̄p(Xt, Yt|h)]\n\n=0 =DKL[p(Xt, Yt|hX\n\nG ∪ hY\n\nG)∥ ̄p(Xt, Yt| ̄hX\n\nG′ ∪ ̄hY\n\nG′)]\n\nfor arbitrary h, which contradicts the strucutral identifiability w.r.t. the instantaneous parents.\n\nIn summary, with the two conditions, we cannot find G ̸= G′ such that the induced joint p(X0:T , Y0:T ) = ̄p(X0:T , Y0:T ), meaning that the SEMs defined as Eq. (16) and Eq. (17) are identifiable w.r.t. bivariate time series.\n\nSince one can use any identifiable static models to characterize the initial behavior of the time series, we will focus on condition 2 for the transition model. In the following, we will show that a generalization of PNL, called history-dependent PNL, satisfies condition 2 under assumptions.\n\nB.2\n\nIDENTIFIABILITY OF HISTORY-DEPENDENT PNL\n\nFirst, we propose a generalization of PNL (Zhang & Hyvarinen, 2012) so that it can be historydependent. For a multivariate temporal process X0:T , we propose history-dependent PNL as\n\nX i\n\nt = νit\n\n(cid:0)fit\n\n(cid:0)Pai\n\nG(< t), Pai\n\nG(t)(cid:1) + git\n\n(cid:0)Pai\n\nG(< t), εit\n\n(cid:1) , Pai\n\nG(< t)(cid:1)\n\n(18)\n\nwhere νit is an invertible transformation w.r.t. the first argument. The main differences of the above SEM compared to typical PNL are (1) the invertible transformation νit can be history dependent; (2) the inner noise distribution can also be history-dependent.\n\nNext, we show the main theorem about its bivariate identifiability w.r.t. its instantaneous parents.\n\nTheorem 4 (History-dependent PNL Bivariate Identifiability). Assume Assumption 1-5 are satisfied, all transformations in Eq. (18) and corresponding induced distributions are 3rd-order differentiable. Given a bivariate temporal process X0:T , Y0:T , then the history-dependent PNL defined as Eq. (18) is bivariate identifiable w.r.t its instantaneous parents (i.e. satisfy condition 2 in Theorem 3), except for some special cases.\n\nProof. W.l.o.g. at time t ∈ [S + 1, T ], we assume Xt → Yt for instantaneous connection under G and Yt → Xt under G′. We fix a value h for their entire history X0:t−1 ∪ Y0:t−1 = h. With h, we further define their lagged parents as PaX G ⊆ h under G and G′(< t) = ̄hY G′ ⊆ h, Pa Pa\n\nG (< t) = hX G′ under G′.\n\nG′(< t) = ̄hX\n\nG(< t) = hY\n\nG ⊆ h, PaY\n\nX\n\nY\n\nTherefore, the SEM at time t can be written as\n\nYt = ν (cid:0)f (hY\n\nG, Xt) + g(hY\n\nG, εY ), hY\n\nG\n\n(cid:1)\n\n(19)\n\nand\n\nG′, Yt) + ̄g( ̄hX under G and G′, respectively. Let’s assume that their induced conditional distributions at time t are equal (i.e. violating the identifiable condition (2) in Theorem 3):\n\nG′, εX ), ̄hX\n\nXt = ̄ν (cid:0) ̄f ( ̄hX\n\n(20)\n\nG′\n\n(cid:1)\n\nlog p(Xt, Yt|hX (cid:124)\n\n(cid:123)(cid:122) under G\n\nG ∪ hY\n\n= log ̄p(Xt, Yt| ̄hX\n\nG′ ∪ ̄hY\n\nG) (cid:125)\n\n(cid:124)\n\nG′) (cid:125)\n\n(cid:123)(cid:122) under G′\n\nFrom the Markov properties, the above equation is equivalent to\n\nlog p(Xt, Yt|h) = log ̄p(Xt, Yt|h)\n\nwith a fixed value h of the entire history.\n\nNow, let’s define\n\nαt = ̄ν−1(Xt)\n\nand\n\nβt = ν−1(Yt)\n\n17\n\nwhere we omits the dependence of ̄ν−1 to ̄hX G. It is easy to observe that we have an invertible mapping between (Xt, Yt) and (αt, βt). Thus, from the change of variable formula, we have\n\nG′ and ν−1 to hY\n\nlog p(Xt, Yt|h) = log pα,β(αt, βt|h) + log |J |\n\nand\n\nlog ̄p(Xt, Yt|h) = log ̄pα,β(αt, βt|h) + log |J | where J is the Jacobian matrix of the transformation. Thus, the equivalence of log p and log ̄p in the (Xt, Yt) space can be translated to (αt, βt) space.\n\nThus, from Eq. (19), we have\n\nβt = Φ(αt) + g(hY\n\nG, εY )\n\n(21)\n\nunder G. And from Eq. (20), we have\n\n(22) under G′. This forms an additive noise model between αt, βt with history-dependent noise. Next, we can use a similar proof techniques as in Hoyer et al. (2008). Here, Φ(·) = f (hY G′, ·) and Ψ(·) = ̄f ( ̄hX\n\nG, ·) ◦ ̄ν( ̄hX\n\nG, ·). We further define\n\nG′, εX )\n\nαt = Ψ(βt) + ̄g( ̄hX\n\nG′, ·) ◦ ν(hY η1(αt) = log p(αt|h) η2(g(hY\n\nG, εY )) = log pg(g(hY\n\nG, εY )|h)\n\n ̄η1(βt) = log ̄p(βt|h) ̄η2( ̄g( ̄hX\n\nG′, εX )) = log ̄pg( ̄g( ̄hX\n\nG′, εX )|h)\n\nThus, under G (i.e. Eq. (21)), we have\n\nlog p(αt, βt|h) = log p(βt|αt, h) + log p(αt|h)\n\n=η2(βt − Φ(αt)) + η1(αt)\n\nSimilarly, under G′ (i.e. Eq. (22)), we have\n\nlog ̄p(αt, βt) = ̄η2(αt − Ψ(βt)) + ̄η1(βt)\n\n(23)\n\n(24)\n\nBased on Eq. (24), we have\n\n∂2 log ̄p ∂αt∂βt\n\n= − ̄η′′\n\n2 Ψ′\n\nand\n\n∂2 log ̄p ∂αt\n\n2 = ̄η′′\n\n2\n\nThus, we have\n\n(cid:18) ∂2 log ̄p/∂αt∂βt ∂2 log ̄p/∂αt Due to the equivalence of log ̄p and log p, we apply the above operations to Eq. (23). After some algebraic manipulation, we obtained the following differential equations for η′′\n\n∂ ∂αt\n\n= 0\n\n(cid:19)\n\n2\n\nη′′′\n\n1 −\n\nη′′ 1 Φ′′ Φ′ =\n\n(cid:18) η′\n\n2η′′′ 2\nη′′ 2\n\n(cid:19)\n\n− 2η′′\n\n2\n\nΦ′′Φ′ −\n\nη′′′ 2\nη′′ 2\n\nΦ′η′′\n\n1 + η′\n\n2\n\n(cid:18)\n\nΦ′′′ −\n\n2 Φ′ ̸= 0: (cid:19) (Φ′′)2 Φ′\n\n.\n\n(25)\n\nInterestingly, this is exactly equivalent to Eq.(4) in Zhang & Hyvarinen (2012). The main difference is the definition of variables and transformations in here are all history-dependent.\n\nFurther, we can also observe that\n\nβt ⊥⊥ ̄g(hY G, εY ) and ̄g( ̄hX\n\nG, εY )|X0:t−1 ∪ Y0:t−1 = h. Since βt = Φ(αt) + g(hY G′, εX ) = αt − Ψ(βt), it is trivial to show the determinant of the Jacobian of the transformation (αt, g) to (βt, ̄g) is 1. Thus, by a similar argument in theorem 1 from Zhang & Hyvarinen (2012), we can derive 1 + η′′ η′′\n\n2Φ′′\n\n1\n\nΨ′ =\n\n2 (Φ′)2 − η′ η′′ 2 Φ′\n\nfor η′′\n\n2 Φ′ ̸= 0.\n\nThus, the above two differential equations has the same form as theorem 1 in Zhang & Hyvarinen (2012) where the main difference is that all distributions and transformations involved in our case depend on history h.\n\nTherefore, we can directly cite the theorem 8 from Zhang & Hyvarinen (2012), which proves that the above differential equations hold true only for 5 types of special cases. One can refer to Table 1 in Zhang & Hyvarinen (2012) for details.\n\n18\n\nCorollary 10 from Zhang & Hyvarinen (2012) validates the choice of using nueral network for the transformation f . For completeness, we include it here with slight modification:\n\nCorollary 4.1 (Identifiability with neural netowrk f ). Assuming the assumptions in Theorem 4 are true, and the double derivative (log pg(g(PaY G(< t), εY )|X0:t−1 ∪ Y0:t−1))′′ w.r.t εY is zero at most at some discrete points. If function f is not invertible w.r.t. the instantaneous parents, then, the history-dependent PNL defined as Eq. (18) is bivariate identifiable w.r.t. the instantaneous parents (i.e. satisfy condition 2 in Theorem 3).\n\nIt is clear to see that the form of Rhino SEMs (Eq. (6)) is a special case of the history-dependent PNL (Eq. (18)), where the outer history-dependent invertible transformation ν is the identity mapping. Thus, we can directly leverage Theorem 3 together with Theorem 4 to show Rhino SEMs are identifiable w.r.t bivariate time series, and Corollary 4.1 to validate our design choice (Eq. (7)).\n\nB.3 GENERALIZING TO MULTIVARIATE TIME SERIES\n\nPreviously, we prove the identifiability conditions for bivariate time series. In this section, we will generalize it to the multivariate case.\n\nTheorem 5 (Generalization to multivariate time series). Assuming the assumptions in Theorem 4 are satisfied, we further assume that the multivariate SEM defined in Eq. (18) satisfies: for each pair of node i, j ∈ V , the SEM\n\n\n\n\n\nX i\n\nt = νit\n\n fit\n\n\n\nPai\n\nG(< t), Pai\n\nG(t)\\{X j\n\nt },\n\n\n\n  + git\n\n· (cid:124)(cid:123)(cid:122)(cid:125) X j\n\nt\n\n(cid:0)Pai\n\nG(< t), εit\n\n(cid:1) , Pai\n\nG(< t)\n\n\n\n \n\nis bivariate identifiable w.r.t. the input, and an identifiable source model is adopted. Then, the history-dependent PNL is identifiable except for some special cases.\n\nProof. For this proof, we can follow the strategy used in Theorem 3 and Peters et al. (2013). We categorize the difference of the graph G and G′ into three types. Following the same analysis of the KL divergence of the two induced joint distributions, we can see that (1) DKL[ps∥ ̄ps] = 0 and DKL[p(Xt|X0:t−1)∥ ̄p(Xt|X0:t−1)] = 0.\n\nDisagree on initial conditions Since we assume that the source model is identifiable, this contradicts DKL[ps∥ ̄ps] = 0.\n\nDisagree on lagged parents only We notice that the analysis used in Theorem 3 for this disagreement can be directly translated to multivariate case. The only difference is that the notation Yt, Xt is changed accordingly.\n\nDisagree also on instantaneous parents For this case, with a fixed history value h = X0:t−1, the aim is to compare the conditionals DKL[p(Xt|X0:t−1 = h)∥ ̄p(Xt|X0:t−1 = h)]. Thus, the problem becomes to how to generalize the bivariate identifiability for instantaneous parents to the multivariate case. We leverage the theorem 2 from Peters et al. (2012), which proves the multivariate identifiability for any models that belongs to IFMOC. It is easy to see that if the assumptions in Theorem 5 are met, the history-dependent PNL belongs to IFMOC w.r.t. the instantaneous parents. It should be noted that the entire history-dependent PNL DOES NOT belong to IFMOC, but this does not affect our results since we only care about the instantaneous parents under this case.\n\nC RELATION TO OTHER METHODS\n\nVARLiNGaM (Hyv ̈arinen et al., 2010) VARLiNGaM (Hyv ̈arinen et al., 2010) is a causal discovery method for time series data based on the linear vector auto-regression, which can model both lagged and instantaneous effects. Its SEM is defined as Eq. (5), where the noise εi t is an independent non-Gaussian noise. It is easy to observe that this is a special case of Rhino (Eq. (6)) by setting fi as the matrix multiplication of the weighted adjacency G0:K with the nodes, and gi as the identity mapping. For the training objective, VARLiNGaM adopted a two stage training to sidestep the\n\n19\n\ndifficulty of directly optimizing the log likelihood. From the Theorem 2 for Rhino, we note that the solution from optimizing the variational objective is equivalent to maximizing the log likelihood under infinite data limit. Therefore, by setting large enough DAGness penalty coefficient α, ρ, the inferred graph from both methods should be equivalent.\n\nDYNOTEARS (Pamfil et al., 2020) The formulation of DYNOTEARS is the same as VARLiNGaM, which is based on linear vector auto-regression. The main novelty is the usage of the DAGness penalty h(G), which continuously relaxes the DAG constraint. The training objective is the mean square error with augmented Lagrange scheme for DAGness penalty. Thus, it is obvious that DYNOTEARS is a special case of Rhino with linear transformations and identity gi. Similarly, Theorem 2 shows the connections between the variational objective and maximum likelihood, which is equivalent to mean square error if the noise distribution is Gaussian with equal variances.\n\ncMLP cMLP (Tank et al., 2018) combines Granger causality with deep neural networks. The model formulation is\n\nX i\n\nt = fi(X 1\n\n0:t−1, . . . , X D\n\n0:t−1) + εi\n\nt\n\nwhere fi is a function based on MLP. Although the input is the entire history, the one that matters is the node that has the connection to X i t (i.e. lagged parents). Therefore, it is easy to see they are closely related to Rhino without instantaneous parents Pai G(t) and history-dependent noise. Since the training objective of cMLP is based on the mean square error with sparseness constraint, by the same argument as before, the variational objective is equivalent to mean square error with equal variance Gaussian noise and large training data.\n\nTiMINo (Peters et al., 2013) TiMINo is most similar to our work among all the aforementioned methods in terms of model formulation. TiMINo proposed a very general formulation based on IFMOC (Peters et al., 2012) and showed the conditions for structural identifiability. Rhino generalizes the TiMINO in a way such that noise history dependency can be incorporated. Thus, Rhino only belongs to IFMOC w.r.t. the instantaneous parents. Therefore, Rhino without the history-dependent noise is a TiMINo model. The training objective of TiMINo is based on the dependence minimization between the noise residuals and causes, and can only infer summary graph instead of temporal causal graph. Zhang et al. (2015) proved the equivalence of the mutual information minimization to maximum likelihood, which is equivalent to our variational objective under infinite data.\n\nD TREATMENT EFFECT ESTIMATION\n\nWe now show how to leverage the fitted Rhino for estimating the conditional average treatment effect (CATE). For simplicity, we only consider a special case of CATE defined as\n\nCATE(a, b) = Eqφ(G)\n\np(X Y\n\nt+τ |X<t,do(X I\n\nt =a),G)[X Y\n\nt+τ ] − E\n\np(X Y\n\nt+τ |X<t,do(X I\n\n(cid:104)\n\nE\n\nt =b),G)[X Y\n\n(cid:105) t+τ ]\n\n(26) We assume the conditioning variable can only be X<t (i.e. the entire history before t), and the intervention and target variable can only be either at current time t or sometime in the future t+τ . We emphasize that this formulation is for simplicity, and Rhino can be easily generalized to more cases as Geffner et al. (2022). Once fitted, the idea is to draw target samples X Y t+τ from the interventional distribution p(X Y t ), G) for each graph sample G ∼ qφ(G). Then, unbiased Monte Carlo estimation can be used to compute CATE. For sampling from the interventional distribution, we can use the ”multilated” graph Gdo(X I t are removed. The intervention samples can be obtained by simulating the Rhino with history X<t, X I\n\nt ) to replace G, where all incoming edges to X I\n\nt+τ |X<t, do(X I\n\nt = a or b and Gdo(X I\n\nt ).\n\nD.1 CAUSAL INFERENCE RESULTS\n\nHere, we provide the preliminary results for CATE performance of Rhino by calculating the RMSEs of the estimated CATEs comparing to the true CATE from the interventional samples (lower is better). We present boxplots of the performance in Fig. 2. All Rhino-based method perform similarly. Surprisingly, the CATE performance seems to have little correlation to the causal discovery performance and warrants further study in the future.\n\n20\n\nFigure 2: Comparison of the RMSE of the average treatment effects (CATEs) of the different instantiations of Rhino depending on the dimensionality. E[CATE] refers to RMSE of the expected CATE over the posterior graph distribution (i.e. G ∼ qφ(G)). ML ATE uses the most likely graph to calculate the ATE. These results are obtained by averaging 160 datasets, similar to the discovery setup.\n\nE VARIATIONAL DISTRIBUTION FORMULATION\n\nHere we provide the detailed formulation of the independent Bernoulli distribution qφ(G). Since this distribution is responsible for modelling the temporal adjacency matrix G0:K, we use Σk to represents the edge probability in Gk. We further split the edge probability matrices into the instantaneous part Σ0 and lagged parts Σ1:K.\n\nTo avoid the constrained optimization of Σ1:K (i.e. the value needs to be within [0, 1]), we adopt the following formulation:\n\nσk,ij =\n\nexp(uk,ij) exp(uk,ij) + exp(vk,ij)\n\n(27)\n\nwhere uk,ij ∈ Uk, vk,ij ∈ Vk and Uk, Vk ∈ RD×D for all k = 1, . . . , K. Since we do not require lagged adjacency matrix to be a DAG, Uk, Vk has no constraints during optimization.\n\nOn the other hand, G0 needs to be a DAG for instantaneous effect. By smart formulation, we can get rid of the length-1 cycles. The intuition is that for a pair of node i, j, only three mutually exclusive possibilities can exist: (1) i → j; (2) j → i; (3) no edge between them. Thus, instead of using a full probability matrix Σ0, we use three lower triangular matrices U0, V0 and E0 to characterise the above three scenarios. For node i > j,\n\np(i → j) =\n\np(j → i) =\n\np(no edge) =\n\nexp(uij) exp(uij) + exp(vij) + exp(eij) exp(vij) exp(uij) + exp(vij) + exp(eij) exp(eij) exp(uij) + exp(vij) + exp(eij)\n\n.\n\nThus, by this formulation, the corresponding instantaneous adjacency matrix will not contain length1 cycles.\n\n21\n\n02RMSEDim = 5Dim = 10E[ATE]ML ATE02RMSEDim = 20E[ATE]ML ATEDim = 40Model NameRhino (L=2)Rhino+g (L=2)Rhino+s (L=2)F SYNTHETIC EXPERIMENTS\n\nF.1 DATA GENERATION\n\nWe create the synthetic datasets in a four step process: 1) generate random Erd ̈os–R ́enyi (ER) or scale-free (SF) graphs that specify the lagged and instantaneous causal relationships; 2) drawing random MLPs for the functional relationships as well as a random conditional spline transformation to modulate the scale of the Gaussian noise variables ε; 3) sample initial starting conditions and follow Eq. (2) with the additive noise to simulate the temporal progression; 4) removing the burn-in period and return stable timeseries. We consider four different axes of variation for the data generation: number of nodes Nnodes ∈ [5, 10, 20, 40]; ER or SF graphs; instantaneous or no instantaneous effects; and history-dependent or history-independent noise (i.e. Gaussian noise). All combinations are generated with 5 different seeds, yielding 160 different datasets. Datasets with instantaneous effects have 4 × Nnodes edges in the instantaneous adjacency matrix. All datasets have 2 × Nnodes connections in the lagged adjacency matrices. The MLPs for the functional relationships are fullyconnected with two hidden layers,64 units and ReLU activation. In case of history-independent noise, we are using Gaussian as the base distribution. The history dependency is modelled as a product of a scale variable obtained by the transformation of the averaged lagged parental values through a random-sampled quadratic spline, and Gaussian noise variable.\n\nThe datasets with 40 nodes are generated with a series length of 400 steps, a burn-in period of 100 steps, and 100 training series. All other datasets are generated with a time-series length of 200, burnin period of 50 steps and 50 training series. We generate random interventions for all the datasets by setting the treatment variable to 10 for intervention and -10 for reference. 5000 ground-truth intervention samples are used to estimate the true treatment effect.\n\nF.2 METHODS\n\nAll benchmarks for the synthetic experiments are run by using publicly available libraries: VARLiNGaM Hyv ̈arinen et al. (2010) is implemenented in the lingam4 python package. PCMCI+(Runge, 2020) is implemented in Tigramite5. We use the implementation in causalnex6 to run DYNOTEARS(Pamfil et al., 2020). We use the default parameters for all these baselines. For PCMCI+, we enumerate all graphs in the Markov equivalence class to evaluate the causal discovery performance (see Appendix G.2 for details).\n\nFor Rhino and its variants, we use the same set of hyper-parameters for all 160 datasets to demonstrates our robustness. By default, we allow Rhino and its variants to model instantaneous effect; set the model lag to be the ground truth 2 except for ablation study; the qφ(G) is initialized to favour sparse graphs (edge probability< 0.5); quadratic spline flow is used to for history-dependent noise. For the model formulation, we use 2 layer fully connected MLPs with 64 (5 and 10 nodes), 80 (10 nodes) and 160 (40 nodes) for all neural networks in Rhino-based methods. We also apply layer normalization and residual connections to each layer of the MLPs. For the gradient estimator, we use the Gumbel softmax method with a hard forward pass and a soft backward pass with temperature of 0.25. All spline flows uses 8 bins. The embedding sizes for transformation (i.e. Eq. (7) and conditional spline flow) is equal to the node number.\n\nFor the sparseness penalty λs in Eq. (10), we use 9 for Rhino and Rhino+s, and 5 for Rhino+g. We set ρ = 1 and α = 0 for all Rhino-based methods. For optimization, we use Adam (Kingma & Ba, 2014) with learning rate 0.01. The training procedure follows from Appendix B.1 in Geffner et al. (2022).\n\nF.3 ADDITIONAL CAUSAL DISCOVERY RESULTS\n\nAblation: different type of graphs The first study is to test our model robustness to different types of graphs. Fig. 3 shows the discovery performance over ER or SF graph averaged over all other possible data setting combinations. Most methods perform better on ER graphs than on SF graphs,\n\n4see https://lingam.readthedocs.io 5see https://jakobrunge.github.io/tigramite/ 6see https://causalnex.readthedocs.io/en/latest/\n\n22\n\nFigure 3: Comparison of the F1 score of the different baseline methods as well as Rhino (light yellow) depending on the dimensionality and the graph type. Inst. refers to the performance on the instantaneous adjacency matrix, Lag refers to the lagged adjancency matrices and temporal considers the full temporal matrix.\n\nwith only DYNOTEARS (Pamfil et al., 2020) as an exception. We note that the PCMCI+ runs on SF graphs with 40 nodes exceed our maximum run time of 1 week, showing its computational limitation in high dimensions. Nevertheless, Rhino achieves consistent performance throughout all graph settings.\n\nAblation: history dependency Figure 4 explores the performance difference of all methods on data generated with/without history-dependent noise. Interestingly, most methods perform better on the history-dependent datasets than the history-independent ones. The possible reasons are (1) the difficulty of the discovery also depends on the randomly sampled functions; (2) the default hyperparameters of all methods are initially chosen to favor the datasets with history-dependent noise and instantaneous effects. We find that PCMCI+ is the most robust across both settings, followed by Rhino and DYNOTEARS. On the other hand, the two variants of Rhino seems to be less robust. When the Rhino is correctly specified, it achieves the best performance. In summary, Rhino demonstrates reasonable robustness to history-dependency mismatch and achieves the best when correctly specified.\n\nAblation: instantaneous effect We investigate the impact of instantaneous effects in the data. Figure 5 shows the F1 score averaged over all possible setting combinations other than instantaneous effect. All methods seem to be robust across both settings with PCMCI+ and Rhino performing the best. The score of the instantaneous adjacency matrix when instantaneous effects are disabled is not defined and therefore not plotted.\n\n23\n\n0.00.51.0F1Dim = 5 | Type = ERDim = 5 | Type = SF0.00.51.0F1Dim = 10 | Type = ERDim = 10 | Type = SF0.00.51.0F1Dim = 20 | Type = ERDim = 20 | Type = SFInst.LagTemporal0.00.51.0F1Dim = 40 | Type = ERInst.LagTemporalDim = 40 | Type = SFModel NameRhino (L=2)Rhino+g (L=2)Rhino+s (L=2)DYNOTEARSVAR-LiNGaMPCMCI+Figure 4: Comparison of the F1 score of the different baseline methods as well as Rhino (light yellow) depending on the dimensionality and whether the data is generated with history-depence or not. Inst. refers to the performance on the instantaneous adjacency matrix, Lag refers to the lagged adjancency matrices and temporal considers the full temporal matrix.\n\nG REAL-WORLD EXPERIMENT DETAILS\n\nG.1 DREAM3 HYPERPARAMETER SETTING\n\nFor tuning the hyper-parameters of Rhino, its variants and DYNOTEARS, we split each of the 5 datasets into 80%/20% training/validation. We tune Rhino and its variants based on the validation likelihoods, and DYNOTEARS based on the validation RMSE error. For PCMCI+, we use the default settings recommended in the Tigramite package (https://github.com/jakobrunge/ tigramite). For other Granger causality baselines, refer to Table 7-11 in Khanna & Tan (2019).\n\nOther than the hyper-parameters reported in Table 4, we use 1-layer MLPs with 10 hidden units for both lτ,j, ζi in Eq. (7) and the hyper-network for conditional spline flow (8 bins). All the MLPs use residual connections and layer-norm at every hidden layer. We use linear conditional spline flow (Dolatabadi et al., 2020) instead of the original quadratic version (Durkan et al., 2019) for better training stability. We also initialise the Bernoulli probability qφ(G) to favour dense graphs (i.e. edge probability > 0.5). For prior p(G), we set the initial value ρ = 1 and α = 0. For the gradient estimator, we use the Gumbel softmax method with a hard forward pass and a soft backward pass with temperature of 0.25. We use batch size 64, learning rate 0.001 with Adam optimizer (Kingma & Ba, 2014). The training procedure follows from Appendix B.1 in Geffner et al. (2022).\n\nTable 5 contains the hyper-parameters setup for DYNOTEARS. We set the maximum training iterations to be 1000 with DAGness tolerance 10−8. The threshold value for the weighted adjacency matrix is 0.05. For PCMCI+, the maximum lag is set to 2. The conditional independence test is set to parcorr, which is based on linear ordinary least square (OLS). A more powerful choice can be\n\n24\n\n0.00.51.0F1Dim = 5 | Hist.-Dep. = FalseDim = 5 | Hist.-Dep. = True0.00.51.0F1Dim = 10 | Hist.-Dep. = FalseDim = 10 | Hist.-Dep. = True0.00.51.0F1Dim = 20 | Hist.-Dep. = FalseDim = 20 | Hist.-Dep. = TrueInst.LagTemporal0.00.51.0F1Dim = 40 | Hist.-Dep. = FalseInst.LagTemporalDim = 40 | Hist.-Dep. = TrueModel NameRhino (L=2)Rhino+g (L=2)Rhino+s (L=2)DYNOTEARSVAR-LiNGaMPCMCI+Figure 5: Comparison of the F1 score of the different baseline methods as well as Rhino (light yellow) depending on the dimensionality and whether the data is generated with instantaneous effects or not. Inst. refers to the performance on the instantaneous adjacency matrix, Lag refers to the lagged adjancency matrices and temporal considers the full temporal matrix.\n\nHyperparams\n\nNode Embedding\n\nInstantaneous eff. Node Embed. (flow)\n\nlag\n\nλs Auglag\n\nRhino (Ecoli1) Rhino (Ecoli2) Rhino (Yeast1) Rhino (Yeast2) Rhino (Yeast3) Rhino+g (Ecoli1) Rhino+g (Ecoli2) Rhino+g (Yeast1) Rhino+g (Yeast2) Rhino+g (Yeast3)\n\n16 16 32 32 32 100 100 100 100 100\n\nFalse False False False False False False False False False\n\n16 100 100 100 16 N/A N/A N/A N/A N/A\n\n2 2\n2 2\n2 2\n2 2\n2 2\n\n19 25 25 25 25 15 25 15 19 9\n\n30 80 10 80 5\n60 25 5\n125 10\n\nTable 4: The hyperparameter setup for Rhino. Node embedding is the dimensionality of uτ,i below Eq. (7); Instantaneous eff. specifies whether it models the instantaneous effect or not; Node Embed. (flow) represents the dimensionality of the node embedding for the hypernetwork used for conditional spline flow gi since the hyper-network shares the similar structure as Eq. (7); lag defines the model lag order; and λs is the sparseness penalty in the prior (Eq. (10)); Auglag is the number of augmented Lagrangian steps, each step consists of 2000 training iterations.\n\na nonlinear independence test based on GP, called GPDC. However, PCMCI+ with GP DC is too slow to finish the training.\n\n25\n\n0.00.51.0F1Dim = 5 | Inst. Effect = FalseDim = 5 | Inst. Effect = True0.00.51.0F1Dim = 10 | Inst. Effect = FalseDim = 10 | Inst. Effect = True0.00.51.0F1Dim = 20 | Inst. Effect = FalseDim = 20 | Inst. Effect = TrueInst.LagTemporal0.00.51.0F1Dim = 40 | Inst. Effect = FalseInst.LagTemporalDim = 40 | Inst. Effect = TrueModel NameRhino (L=2)Rhino+g (L=2)Rhino+s (L=2)DYNOTEARSVAR-LiNGaMPCMCI+Hyperparams\n\nlag\n\nλa\n\nEcoli1 Ecoli2 Yeast1 Yeast2 Yeast3\n\n2 2\n2 3\n2\n\n0.01 0.1 0.005 0.01 0.01\n\nλw\n\n0.5 0.01 0.1 0.01 0.005\n\nTable 5: The hyperparameter setup for DYNOTEARS.\n\nMetrics Rhino Rhino+g Dynotears PCMCI\n\nEcoli1 0.183± 0.012 0.169± 0.010 0.120 0.051±0.006\n\nEcoli2 0.214± 0.009 0.211±0.010 0.066 0.049± 0.003\n\nYeast1 0.261± 0.006 0.254± 0.006 0.059 0.068±0.005\n\nYeast2 0.136± 0.005 0.148± 0.007 0.092 0.046± 0.002\n\nYeast3 0.113± 0.005 0.126± 0.003 0.045 0.060±0\n\nTable 6: Orientation F1 score of DREAM3 datasets.\n\nG.2 POST-PROCESSING TEMPORAL ADJACENCY MATRIX\n\nThe ground truth graphs for DREAM3 and Netsim datasets are summary graph, which is essentially the temporal graph aggregated over time. We provide a formal definition of summary graph:\n\nDefinition G.1 (Causal summary graph (Assaad et al., 2022)). Let Xt be a multivariate temporal process, and G = (V , E) be a summary graph. The edge p → q exists if and only if there exists some time t and some lag τ such that X p t at time t with a lag 0 ≤ i for p ̸= q and with a time lag of 0 < i for p = q.\n\nt−τ causes X q\n\nUnlike the some of the Granger causality baselines, Rhino (and its variants), DYNOTEARS, VARLiNGaM produces the temporal adjacency matrix after training. For DREAM3 and Netsim datasets, this creates the incompatibility during evaluation. Thus, we need to aggregate the temporal graph into a summary graph before comparing to the ground truth. For binary adjacency matrix, we sum over the time steps followed by a step function, i.e. step((cid:80) k Gk). Thus, there will be an edge i → j in summary graph as long as there is a connection from i to j at any timestamp. For the Bernoulli probability matrix from Rhino and its variants, we take a max(·) over the timestamp to generate the probability matrix for the summary graph.\n\nAn exception is PCMCI+, which can only produce MECs for the instantaneous adjacency matrix. In such case, we will enumerate up to 10000 possible instantaneous DAGs from the MECs. Together with the lagged adjacency matrix, we will perform the above post-processing step to generate the corresponding aggregated adjacency matrix. We also estimate the corresponding edge probabilities by taking the average over all possible DAGs.\n\nFor DREAM3 experiments, we ignore the self-connections by setting the diagonal of the aggregated adjacency matrix to be 0.\n\nFor Netsim, self-connections are not ignored, following the same settings as Khanna & Tan (2019).\n\nG.3 ADDITIONAL DREAM3 RESULTS\n\nHere, Fig. 6 shows the additional ROC curve plots for all 5 datasets in DREAM3. For the visualization purpose, we only select a single run for Rhino and this will not affect the curve much due to small standard error in Table 2.\n\nIn addition, we provides the additional metrics (Orientation F1 and SHD) for DREAM3 datasets in Table 6 and Table 7. These results are obtained by using the same hyperparameters mentioned in Appendix G.1. In particular, we tune the threshold for rounding the continuous-valued adjacency matrix to binary adjacency matrix for both Rhino and baselines. It can be observed that the F1 and SHD agree with the trend of AUROC, where Rhino and Rhino+g achieve the best performance compared to baselines. This further supports the advantages of our proposed methods.\n\n26\n\nFigure 6: The ROC curve plots of Rhino and other baselines for DREAM3 datasets. For illustration purpose, we only select a single run of Rhino, Rhino+g, DYNOTEARS and PCMCI+ to plot ROC curve. Since the standard error reported in Table 2 is relatively small, the plot should not vary much for other runs. The ROC curve of other baselines are directly taken from figure 7 in Khanna & Tan (2019).\n\n27\n\n0.00.10.20.30.40.50.60.7FPR0.00.20.40.60.81.0TPREcoli1_1000.00.10.20.30.40.50.60.7FPR0.00.20.40.60.81.0TPREcoli2_1000.00.10.20.30.40.50.60.7FPR0.00.20.40.60.81.0TPRYeast1_1000.00.10.20.30.40.50.60.7FPR0.00.20.40.60.81.0TPRYeast2_1000.00.10.20.30.40.50.60.7FPR0.00.20.40.60.81.0TPRYeast3_100eSRUSRUTCDFcLSTMcMLPRhinoDYNOTEARSpcmci+Rhino+gMetrics Rhino Rhino+g Dynotears PCMCI\n\nEcoli1 157.8±3.23 162.8±3.13 372 266.6±2.01\n\nEcoli2 120.4± 1.19 122±2.87 422 273.2±1.48\n\nYeast1 161.8±1.86 182.4±1.43 397 288.6±2.24\n\nYeast2 399.8±6.77 401.4±3.85 630 507.4±0.92\n\nYeast3 627.6±5.47 760.6±6.18 838 704±0\n\nTable 7: SHD of DREAM3 datasets.\n\nG.4 NETSIM HYPERPARAMETER SETTING\n\nFor the Netsim experiment, we extract subject 2-6 in Sim-3.mat to form the training data and use subject 7-8 as validation dataset. Following the same settings as DREAM3 (Appendix G.1), we tune the hyperparameters of Rhino and its variants based on the validation log likelihood; DYNOTEARS with MSE on validation dataset; and use default settings of PCMCI+ from Tigramite package.\n\nIt is worth noting that unlike DREAM3 experiment, where the results and hyperparameters of Granger causality baselines can be directly taken from Khanna & Tan (2019). Their setup of Netsim experiment is different from ours, where they train the baselines using a single subject and compute the corresponding AUROC, followed by averaging over subjects 2-6. Our setup is to train all methods using the entire data from subject 2-6 before computing AUROC. Thus, the hyperparameters for Granger causality are slightly different, and the AUROC increases for the baselines compared to those reported in Khanna & Tan (2019).\n\nRhino The hyperparameters are the same as DREAM3, except for the following: we initialise the Bernoulli probability of qφ(G) to have no preference (i.e. edge probability= 0.5); the λs = 25; we use 2 layer MLPs with 64 hidden units for both functional model (Eq. (7)) and hyper-network with embedding size 15; the augmented Lagrangian step is 5. For Rhino variants, we use the above settings as well.\n\nDYNOTEARS, PCMCI+ and VARLiNGaM For DYNOTEARS, we set lag to be 2, λa = 0.5 and λw = 0.5. For PCMCI+, we use parcorr independence test with lag 3. For VARLiNGaM, we use lag 2 with default settings as https://lingam.readthedocs.io/en/latest/.\n\nGranger Causality For computing AUROC, we follow the same method as Khanna & Tan (2019); Tank et al. (2018) by sweeping through a range of hyperparameters. Specifically, we use the same hyperparameters for SRU and eSRU as (Khanna & Tan, 2019). For cMLP, we choose the ridge penalty as 0.43 and sweep through the group sparse penalty in range [0.1, 1]. For cLSTM, we set the ridge penalty to be 0.045, and sweep the group sparse penalty in range [0.1, 1].For TCDF, we sweep through the threshold in range [−1, 2] for the attention scores. Other than the above hyperparameters, everything else follows the setup as in Khanna & Tan (2019).\n\nG.5 ADDITIONAL NETSIM RESULTS\n\nFigure 7 shows the ROC curve plot for Rhino and other baselines. It is clear that Rhino achieves significantly better TPR-FPR trade-offs compared to others. Table 8 shows additional discovery metrics of Rhino and baselines for Netsim dataset. We can observe that F1 score and SHD in general agree with AUROC reported in Table 3, where Rhino-based methods outperform the baselines, apart from Rhino+NoInst. This again confirms the necessity of modelling instantaneous effect for realworld challenges. Rhino outperforms Rhino+g on two out of three metrics (including AUROC), which shows the advantage of history-dependent noise.\n\nH AUROC METRIC\n\nAUROC metric is a one of the standard metrics for evaluating the causal discovery, which measures the trade-off between the true positive rate (TPR) and false positive rate (FPR). However, during the experiments, we found out that AUROC does not necessarily correlate well with other discovery metrics. From Fig. 8, it is clear that the F1 score continues to increase whereas AUROC and validation likelihood starts to decrease after few steps. Since the dataset of Netsim is relatively small,\n\n28\n\nMethod DYNO. PCMCI+ VarLiNGaM Rhino+g Rhino+NoInst Rhino\n\nOri. F1 0.341 0.41 0.44 0.539 ± 0.036 0.212 ± 0.014 0.551 ± 0.048\n\nSHD 17 18 18 10.4 ± 1.08 29 ± 1.5 13.8 ± 1.5\n\nTable 8: Orientation F1 and SHD of Rhino and baselines for Netsim dataset.\n\nFigure 7: The ROC curve plots of Rhino and other baselines for Netsim dataset. Similar to Fig. 6, we only select 1 run out of 5 for Rhino, Rhino+g, DYNOTEARS, PCMCI+ for illustration purpose.\n\n29\n\n0.00.20.40.60.81.0FPR0.00.20.40.60.81.0TPRNetsim Roc CurveRhinoRhino+gDYNOTEARSpcmci+VARLiNGaMcMLPcLSTMTCDFeSRUSRUFigure 8: The curves of orientation F1, AUROC and validation likelihood during training. Each curve is obtained by averaging over 5 random seeds. The validation curve agrees well with the AUROC curve, but shows an opposite trends as F1 curve. This potentially indicates model overfitting in the later stage of training.\n\nthis indicates the possible overfitting. This disagreement originates from the different aspects these metrics care about. For AUROC, it cares about the trade-off between TPR and FPR with various decision thresholds, and it penalizes the wrong decisions with certainty harshly. On the other hand, F1 score cares about the final inferred binary adjacency matrix with a fixed decision threshold. For example, if we multiply the Bernoulli probability matrix by a small factor (e.g. 10−5), the AUROC score will remain the same but the F1 score will tends to 0 with the default decision threshold 0.5.\n\nThus, model overfitting tends to drive the edge probabilities towards 1 or 0, which may help the F1 score but these extreme decisions can result in a large decrease in the AUROC score. Thus, for small dataset, we believe AUROC is a better metric than F1, which also agrees with validation likelihood.\n\nIn addition, the Bayesian setup of Rhino may also help with better AUROC for small dataset. From the same figure, even the large decrease of validation likelihood suggests potential model overfitting, the AUROC still maintains a reasonable value. This may be due to the Bayesian view of the causal graph, where the posterior edge probability does not converge to extreme values.\n\n30\n\n010203040Steps0.20.40.60.81.0F1/AUROC ScoreAUROC/val_likelihood v.s. F1 ScoreAUROCOrientation F1Val_likelihood807060504030val_likelihood",
  "translations": [
    "# Summary Of The Paper\n\nThe paper is trying to deal with non-linear relationships in time series data and proposes a structural equation model called Rhino, which combines vector auto-regression, deep learning as well as variational inference techniques.\n\n# Strength And Weaknesses\n\nStrength\n1. The problem causal discovery the paper is trying to tackle is quite important and interesting.\n2. The method is supported by the structural identifiability theory. \n\nWeakness\n1. I do not understand why related works is not Section 2, why Section 5. This is quite confusing.\n2. In the experiments section, I was wondering if we should only use AUC as the evaluation metrics since one major goal seems to be discover the causal relationships.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work's quality needs some improvement but the novelty of the paper is good.\n\n# Summary Of The Review\n\nOverall, I think the paper is of good quality, with some improvements required on the experiments and writing. But the overall the methodology and theory part is good.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper introduces RHINO, a novel framework designed for causal discovery in time series data, which effectively addresses challenges such as non-linear relationships, instantaneous effects, and history-dependent noise. RHINO combines vector auto-regression, deep learning, and variational inference, proving the structural identifiability of its proposed models. Empirical results demonstrate that RHINO outperforms existing baselines on both synthetic and real-world datasets, showcasing robustness even under model misspecifications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of various methodologies to tackle complex causal relationships in time series. The proof of structural identifiability is a significant theoretical contribution that enhances the credibility of the proposed model. Additionally, the extensive empirical validation across diverse datasets effectively illustrates RHINO's capabilities. However, a potential weakness is that while the paper claims robustness under model misspecifications, the specific scenarios tested could be expanded to further validate this claim. The complexity of the model may also limit its accessibility for practitioners not well-versed in deep learning techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations, contributions, and methodologies of RHINO. The quality of the writing is high, making complex concepts understandable. The novelty of combining deep learning with traditional causal modeling is noteworthy, contributing meaningfully to the field. The reproducibility statement is comprehensive, providing sufficient details for readers to replicate the experiments, which enhances the overall contribution of the work.\n\n# Summary Of The Review\nOverall, RHINO represents a significant advancement in the field of causal discovery for time series data, effectively addressing key limitations of existing methods. The theoretical foundations are robust, and the empirical validations convincingly support the proposed framework. However, the complexity of the model may pose a barrier to its broader adoption.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces RHINO, a novel framework designed for discovering causal relationships in time series data. It addresses challenges associated with modeling non-linear relationships, instantaneous effects, and history-dependent noise through a combination of Structural Equation Models (SEMs), vector auto-regression, deep learning, and variational inference. The methodology includes a robust model formulation that incorporates history-dependent noise, proven theoretical guarantees for structural identifiability, and extensive experimental evaluations demonstrating RHINO's superior performance over baseline methods on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe strengths of RHINO lie in its robust performance across a variety of datasets, showcasing its effectiveness in real-world applications. The theoretical foundations presented provide confidence in the model's identifiability and flexibility in capturing complex causal structures. However, the paper also has weaknesses, notably its computational complexity, especially in high-dimensional settings, and its reliance on specific assumptions like the causal Markov property, which may not always hold true. Additionally, while the results are promising, the generalizability of RHINO to other domains or data distributions remains to be validated further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the framework's objectives, methodology, and findings. The quality of the theoretical analysis is commendable, and the empirical evaluations are thorough, providing a comprehensive understanding of RHINO's capabilities. The novelty of the approach, particularly the consideration of history-dependent noise, is significant, as it offers a new perspective in causal discovery. However, the reproducibility may be challenging due to the computational demands and the specific conditions under which the theoretical guarantees hold.\n\n# Summary Of The Review\nOverall, RHINO presents a significant advancement in the field of causal discovery from time series data, effectively addressing the complexities of non-linear relationships and noise dependencies. While the framework demonstrates strong empirical performance and theoretical support, considerations regarding computational efficiency and broader applicability warrant further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces RHINO, a framework designed for causal relationship learning from time series data, addressing challenges such as non-linear relationships, instantaneous effects, and history-dependent noise. RHINO employs a novel structural equation model (SEM) formulation, utilizing neural networks for non-linear function approximation and variational inference for causal discovery. The authors provide theoretical proofs of structural identifiability and demonstrate RHINO's superiority through empirical evaluations on both synthetic datasets and real-world benchmarks, such as gene networks and fMRI data.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive approach to causal learning that simultaneously addresses multiple critical aspects often overlooked by existing methods, such as Granger causality and vector auto-regression. The theoretical contributions, including the proofs of identifiability, are well-articulated and contribute to the field's understanding of causal inference. However, a potential weakness is the reliance on complex neural network architectures, which may limit interpretability and introduce challenges in understanding the causal mechanisms inferred by the model. Additionally, the empirical validation, while robust, could benefit from further diversity in the types of datasets employed for testing.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, theoretical contributions, and experimental results. The quality of writing is high, making it accessible to readers with varying levels of expertise in causal inference. The novelty of RHINO lies in its ability to integrate history-dependent noise and instantaneous effects into the causal discovery process, which is a significant advancement over traditional methods. The authors provide a reproducibility statement, including detailed descriptions of their methods and publicly available code, enhancing the paper's credibility and facilitating further research.\n\n# Summary Of The Review\nOverall, RHINO presents a significant advancement in causal relationship learning from time series data, effectively addressing critical challenges through an innovative modeling framework and strong theoretical backing. The empirical results support its claims of improved performance over existing methods, although there are areas where further validation could enhance the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces Rhino, a novel framework designed for modeling complex time series data. Rhino addresses non-linear relationships, instantaneous effects, and history-dependent noise, showcasing its capabilities through both theoretical guarantees and empirical validation. The authors prove the structural identifiability of the model, which is critical for establishing valid causal inferences. Extensive experiments indicate that Rhino outperforms existing methods, demonstrating robustness to model misspecification and the potential for application in causal inference tasks. The code for Rhino is made publicly available to promote transparency and facilitate further research.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its comprehensive framework, theoretical guarantees, and empirical validation. The introduction of Rhino as a method that integrates deep learning techniques is promising, allowing for flexible modeling of complex relationships. However, the model's complexity may hinder interpretability for practitioners, and the assumptions underlying identifiability could limit its applicability in diverse real-world scenarios. While the empirical results are compelling, they may not fully generalize across various types of time series data. Additionally, while the paper presents robust findings in synthetic environments, the effectiveness of Rhino in real-world settings with unobserved confounders remains uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, effectively communicating its contributions and methodologies. The novelty of the approach is significant, particularly in the context of causal discovery in time series analysis. The inclusion of ablation studies adds quality to the empirical results, though the sensitivity of these findings to specific datasets warrants caution. The open-source availability of the code enhances reproducibility, yet the complexity of the implementation may pose challenges for users unfamiliar with the underlying theory.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of time series analysis and causal inference through the introduction of the Rhino framework. While the empirical validation and theoretical guarantees are commendable, there are notable concerns regarding interpretability and generalizability that should be addressed in future work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"RHINO: Deep Causal Temporal Relationship Learning with History-Dependent Noise\" presents a novel framework for causal discovery in time series data. The authors introduce Rhino, which combines vector auto-regression, neural networks, and variational inference to model complex temporal causal relationships, particularly focusing on non-linear interactions and historical noise distributions. Key contributions include a unique structural equation model (SEM) that accommodates history-dependent noise, theoretical proofs of structural identifiability, extensive empirical validation against traditional methods, and comprehensive ablation studies demonstrating the importance of history-dependent noise in enhancing model accuracy.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of deep learning with traditional causal modeling, providing a significant advancement in the field. The theoretical contributions regarding structural identifiability add a robust foundation for the proposed methodology, and the empirical results demonstrate superior performance across various datasets, enhancing the credibility of the findings. However, weaknesses include potential interpretability issues associated with deep learning methods, which may concern practitioners in critical fields like healthcare and finance. Additionally, the paper lacks a thorough discussion on computational efficiency in large-scale applications, which could impact its practicality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections that effectively convey the methodology and findings. The quality of the writing is high, making complex concepts accessible. The novelty of Rhino is significant, given its innovative approach to causal discovery and the incorporation of history-dependent noise. However, reproducibility could be a concern due to the reliance on deep learning components, which may require specific configurations and tuning that are not extensively detailed in the manuscript.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to causal discovery in time series data, successfully addressing challenges related to non-linear relationships and dynamic noise structures. The strong theoretical insights coupled with extensive empirical validation position Rhino as a valuable contribution to the field of causal inference, although potential issues related to interpretability and computational efficiency should be addressed.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel adversarial training framework called \"Rhino,\" designed to enhance the robustness of machine learning models against adversarial attacks. The methodology integrates vector auto-regression and deep learning techniques to model the complex relationships between input features while accounting for history-dependent noise in adversarial environments. Findings from extensive experiments on both synthetic and real-world datasets demonstrate that Rhino outperforms existing adversarial training methods in terms of both robustness and accuracy, especially in scenarios with dynamic adversarial strategies.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative framework that addresses the limitations of traditional adversarial training methods by incorporating non-linear modeling and history-dependent dynamics. The theoretical guarantees on structural identifiability provide a solid foundation for the proposed methodology. Additionally, the empirical validation through comprehensive experiments on diverse datasets reinforces the claims of improved performance. However, weaknesses include a potential lack of clarity in the presentation of the variational inference approach and the need for more extensive discussions on the implications of the results in real-world applications. Furthermore, while the framework is promising, its complexity may pose challenges for practical implementations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but some technical details could be more clearly articulated, particularly regarding the variational inference methods employed. The quality of the empirical results is strong, showing significant improvements over baseline methods. In terms of novelty, the integration of history-dependent features into adversarial training is a notable contribution. However, the reproducibility of the results may be affected by the complexity of the proposed framework, as the implementation details are not exhaustively discussed.\n\n# Summary Of The Review\nOverall, the Rhino framework presents a significant advancement in adversarial training methods, effectively addressing the challenges posed by evolving adversarial attacks. While the theoretical contributions and empirical results are compelling, some aspects of clarity and reproducibility require attention to ensure broader applicability in practical scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces \"RHINO,\" a new framework for causal discovery from time series data, which combines vector auto-regression, deep learning techniques, and variational inference. The authors claim that RHINO effectively addresses challenges related to non-linear relationships and history-dependent noise in causal modeling, which they argue have not been adequately managed by existing methods. The paper presents a structural identifiability proof for RHINO's Structural Equation Models (SEMs) and includes empirical evaluations demonstrating its performance on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious attempt to integrate various established methodologies into a cohesive framework for causal discovery. The structural identifiability proof is a noteworthy theoretical contribution, albeit one that aligns with standard practices in causal modeling. However, the paper suffers from several weaknesses, including an exaggeration of RHINO's improvements over existing methods and a tendency to dismiss prior work in the field. The empirical results, while extensive, do not convincingly demonstrate a groundbreaking advancement, and the complexity introduced by RHINO may not translate to significant practical benefits.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology and findings clearly; however, the claims regarding the novelty and effectiveness of RHINO lack sufficient empirical backing, creating a potential disconnect between the presented evidence and bold assertions. The reproducibility of the results is not addressed in detail, leaving questions about the practical implementation of RHINO's framework. Although the methodology involves familiar concepts such as variational inference, the paper does not sufficiently clarify how these are innovatively combined to warrant the claims of novelty.\n\n# Summary Of The Review\nOverall, while RHINO presents a compelling approach to causal discovery, the paper's claims about its uniqueness and transformative impact on the field appear overstated. The contributions, while valuable, do not sufficiently differentiate RHINO from existing methodologies. The paper would benefit from a more balanced discussion of previous work and a clearer demonstration of RHINO's comparative advantages.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces RHINO, a novel framework for learning causal relationships from time series data that effectively addresses challenges such as non-linear relationships, instantaneous effects, and history-dependent noise. The authors present a new structural equation model (SEM) that proves structural identifiability under certain conditions. Empirical evaluations demonstrate RHINO's superiority over existing methods across various datasets, achieving improved F1 scores and AUROC metrics, notably in synthetic data and real-world applications like gene networks and brain connectivity.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to causal discovery, particularly its ability to handle history-dependent noise and instantaneous effects which are often inadequately addressed by traditional methods. The empirical results are robust and clearly demonstrate RHINO's advantages over several baseline models, which enhances its credibility. However, the paper could benefit from a more detailed discussion on the theoretical implications of the identified structural identifiability and potential limitations of the proposed framework, such as its scalability to larger datasets or its performance under different types of noise.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of writing is high, making complex concepts accessible without oversimplification. The novelty of the RHINO framework is significant, as it represents a meaningful advancement in the field of causal discovery. Additionally, the authors provide sufficient details in the reproducibility statement, including synthetic data generation processes and hyperparameter settings, which enhances the paper's reliability and facilitates replication of results.\n\n# Summary Of The Review\nOverall, RHINO presents a compelling advancement in causal relationship learning, showcasing significant improvements in performance metrics compared to traditional methods. The framework's innovative handling of history-dependent noise and non-linear relationships positions it as a valuable tool for various scientific applications. However, further exploration of its theoretical foundations and limitations would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper \"RHINO: Deep Causal Temporal Relationship Learning with History-Dependent Noise\" introduces a novel framework for causal discovery from observational time series data, leveraging structural equation models (SEMs) to uncover causal relationships. The authors claim to enhance traditional models by incorporating history-dependent noise, allowing for a more nuanced understanding of how past observations influence current outcomes. Empirical validation is conducted using synthetic data and real-world benchmarks, asserting the model's robustness and superiority over existing methods like Granger causality.\n\n# Strength And Weaknesses\nThe paper presents several significant contributions, particularly the introduction of history-dependent noise and the application of SEMs to causal discovery in time series. However, there are notable weaknesses regarding the foundational assumptions made. The reliance on strict conditions for identifiability, stationarity in data, and the effectiveness of functional forms raises concerns about the model's generalizability to complex real-world scenarios. Additionally, the empirical results, while promising, are primarily based on specific datasets, which may not adequately represent broader contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally strong, with a well-structured presentation of methodologies and findings. However, the assumptions underlying the model could be articulated more thoroughly to enhance understanding. The novelty is present in the integration of history-dependent noise, although the overall significance may be mitigated by the restrictive assumptions made. Regarding reproducibility, while the empirical section provides some insights, the reliance on specific datasets and assumptions about noise and stationarity could hinder broader application and verification of results.\n\n# Summary Of The Review\nOverall, the paper makes an interesting contribution to causal discovery in time series analysis, particularly with its novel incorporation of history-dependent noise. However, several foundational assumptions limit the applicability and generalizability of the proposed model, raising questions about its robustness in real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents RHINO, a novel framework aimed at discovering causal relationships in time series data while addressing common challenges such as non-linear relationships, instantaneous effects, and history-dependent noise. The authors introduce a new structural equation model (SEM) that incorporates these complexities and utilize a variational inference framework for causal discovery. The theoretical foundations of RHINO are established, demonstrating its structural identifiability, and extensive experiments show that RHINO outperforms existing methods in various scenarios, both synthetic and real-world.\n\n# Strength And Weaknesses\nOne of the main strengths of RHINO is its ability to effectively model history-dependent noise and instantaneous effects, which are often inadequately addressed by traditional methods. The theoretical contributions are well-articulated, establishing a solid foundation for the framework's reliability. However, a potential weakness is the complexity of the proposed model, which might hinder its applicability in certain practical situations where simpler models could suffice. Additionally, while the experiments demonstrate RHINO's efficacy, further exploration into its performance with non-stationary time series and unobserved confounders could enhance its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the methodology and findings accessible to readers. The quality of the theoretical foundations and empirical evaluations is commendable. The novelty of RHINO lies in its integration of multiple complexities in causal discovery, setting it apart from existing approaches. The reproducibility statement is robust, providing sufficient information regarding the theoretical contributions, empirical evaluations, and datasets, which facilitates the replication of results.\n\n# Summary Of The Review\nOverall, RHINO presents a significant advancement in the field of causal discovery from time series data by effectively addressing critical challenges through a novel framework. Its theoretical and empirical contributions are substantial, although further exploration of its applicability in complex scenarios could enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel algorithm for enhancing the performance of reinforcement learning (RL) agents in partially observable environments. The proposed method, called POMDP-Actor-Critic (PAC), integrates a predictive model of the environment's dynamics with a traditional actor-critic framework. The authors demonstrate that PAC significantly outperforms existing methods on several benchmark tasks, particularly in scenarios with high levels of uncertainty and partial observability. Additionally, they provide a theoretical analysis of the convergence properties of their approach, establishing its soundness.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Framework:** The integration of predictive modeling with actor-critic methods is a fresh perspective that could lead to significant advancements in RL.\n2. **Strong Theoretical Basis:** The theoretical analysis of convergence adds credibility to the proposed method and provides useful insights for future research.\n3. **Thorough Empirical Evaluation:** The experiments conducted are comprehensive, showcasing the robustness of PAC across various challenging environments.\n4. **Reproducibility:** The authors have made their code publicly available, facilitating reproducibility and allowing for further exploration by other researchers.\n\n**Weaknesses:**\n1. **Limited Comparison to State-of-the-Art:** While the performance improvements are notable, the paper could benefit from a more extensive comparison with a broader range of existing methods in the literature.\n2. **Hyperparameter Sensitivity:** The impact of hyperparameter choices on performance is not sufficiently addressed, leaving questions about the method's robustness.\n3. **Generalization Concerns:** The paper does not fully explore how well the method generalizes to unseen environments, which is critical for real-world applications.\n4. **Presentation Issues:** Some sections are dense and could benefit from clearer explanations and more structured formatting to enhance overall readability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but certain technical sections could be clearer, particularly the descriptions of the algorithm and the experimental setup. The novelty of the approach is significant, as it brings a new perspective to RL in partially observable environments. The authors have taken steps to ensure reproducibility by providing code and detailed descriptions of their methodology.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to improving RL in challenging environments, supported by strong theoretical foundations and comprehensive empirical results. However, it would benefit from clearer comparisons with existing methods and a more thorough discussion of its generalization capabilities. Addressing these points would enhance the overall impact of the research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces \"RHINO,\" a novel framework designed for causal relationship discovery in time series data, addressing the complexities of non-linear relationships and history-dependent noise. The methodology combines vector auto-regression, deep learning, and variational inference to create a model that can effectively modulate noise distributions based on historical observations. The findings demonstrate that RHINO outperforms existing baselines in causal discovery tasks, achieving robust performance even under model misspecification.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating multiple methodologies to address specific challenges in causal discovery, particularly the management of history-dependent noise. This integration is theoretically proven to be structurally identifiable, which is a significant contribution to the field. However, one potential weakness is the limited discussion on the scalability of RHINO and its applicability to real-world datasets that may exhibit non-stationarity or unobserved confounders. The authors acknowledge future work in these areas but could provide more insight into current limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents its objectives, methodology, and findings. The clarity of the explanations regarding the integration of various techniques is commendable, making the complex concepts accessible. In terms of novelty, RHINO offers a fresh perspective on causal discovery by addressing previously neglected aspects, particularly history-dependent noise. Reproducibility is supported by the empirical validation of the proposed method against existing baselines, though further details on implementation and dataset specifications would enhance reproducibility efforts.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of causal discovery in time series analysis with its novel framework, RHINO. The integration of deep learning and variational inference with traditional methods marks a significant advancement, despite some limitations in discussing scalability and real-world applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"Rhino,\" a novel framework for deep causal temporal relationship learning that effectively addresses challenges in causal discovery from time series data, including non-linear relationships, instantaneous effects, and history-dependent noise. The authors introduce a new functional form for Structural Equation Models (SEMs) that incorporates history-dependent noise, combined with vector auto-regression and deep learning techniques. Through extensive empirical evaluations on both synthetic and real-world datasets, the paper demonstrates that Rhino outperforms existing methods in terms of key performance metrics, such as F1 scores, AUROC, and SHD.\n\n# Strength And Weaknesses\nStrengths of the paper include its theoretical contributions, specifically the proof of structural identifiability for SEMs with history-dependent noise, which adds significant value to the field of causal discovery. The empirical validation across a variety of datasets, including synthetic data with known causal structures and real-world applications, showcases the versatility and robustness of the proposed model. However, the paper could be improved by providing more detailed discussions on the limitations of the proposed approach, particularly concerning non-stationary time series and latent confounders, which are only briefly mentioned.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings, making it accessible to readers with varying levels of expertise in causal inference and deep learning. The novelty of Rhino lies in its integration of history-dependent noise into causal models, an area that has not been comprehensively addressed in prior work. The authors have made their code and datasets publicly available, which enhances the reproducibility of their results and supports the research community in building upon their findings.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of causal discovery by introducing a robust framework that effectively handles complex temporal relationships in time series data. While the theoretical and empirical results are compelling, a more thorough exploration of the method's limitations would strengthen the paper. \n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"RHINO: Deep Causal Temporal Relationship Learning with History-Dependent Noise\" presents a novel framework designed to address the challenges of causal discovery in time series data. The authors propose RHINO, which integrates vector auto-regression, deep learning, and variational inference to effectively model non-linear relationships, instantaneous effects, and history-dependent noise. The methodology includes a new functional form for Structural Equation Models (SEMs), a variational inference framework, and theoretical proofs of structural identifiability. Empirical results demonstrate that RHINO outperforms existing methods across synthetic datasets and real-world applications, such as gene networks and brain connectivity analysis.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous methodology, which combines established concepts with innovative approaches to address critical limitations in current causal discovery methods. The theoretical proof of structural identifiability adds credibility to the proposed framework. Empirical evaluations show strong performance metrics, indicating the robustness of RHINO. However, a notable weakness is the lack of a comprehensive discussion on potential limitations and scenarios where RHINO may underperform. Additionally, comparative analyses with more recent methodologies could enhance the paper's relevance and impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and findings. The methodology is explained in detail, facilitating understanding and reproducibility. The novelty of RHINO lies in its integration of different approaches to tackle causal discovery, particularly through the consideration of history-dependent noise. The experimental validation is thorough, although the authors could strengthen clarity by including more explicit comparisons with state-of-the-art methods.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of causal discovery in time series data by addressing key challenges that previous methods have overlooked. Although the methodology is robust and the empirical results are compelling, the paper would benefit from a more thorough exploration of limitations and a broader comparison with current state-of-the-art techniques.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces \"Rhino,\" a novel framework for deep causal temporal relationship learning that addresses the challenges of non-linear interdependencies, instantaneous causal effects, and history-dependent noise in multivariate temporal data. The methodology combines vector auto-regression, deep learning, and variational inference, offering a theoretical proof of structural identifiability. Empirical evaluation using both synthetic datasets and real-world benchmarks demonstrates Rhino's superior performance relative to existing causal discovery methodologies, supported by robust ablation studies.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive treatment of complex causal relationships that are often overlooked by traditional methods, particularly in handling non-stationary noise and instantaneous effects. The theoretical contributions are solid, providing a clear framework for understanding structural identifiability and the validity of the proposed variational inference approach. However, a potential weakness is the reliance on synthetic datasets in the experimental validation; while the paper includes real-world benchmarks, further real-world application across diverse domains could strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, theoretical foundations, and empirical results. The clarity of the mathematical formulations and the logical progression of the arguments contribute to the overall quality of the paper. The novelty of the approach is significant, as it integrates multiple advanced techniques to tackle a critical problem in causal inference. Reproducibility appears to be supported through detailed descriptions of the methodologies and experiments, though the absence of shared code or data could be a hindrance for some researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel approach to causal discovery in temporal data, successfully addressing key challenges and demonstrating superior performance. While the theoretical and empirical contributions are robust, further validation in diverse real-world contexts would enhance the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a framework named Rhino, aimed at addressing challenges in causal discovery specifically within time series data. The authors claim that Rhino incorporates history-dependent noise, a factor they argue has been overlooked in previous methodologies. However, the methodology lacks clarity, and the empirical findings are drawn primarily from extensive synthetic datasets, which raises concerns about their applicability to real-world scenarios. The paper concludes with claims of improved performance over baseline methods, though the statistical significance of these improvements remains questionable.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to tackle the complex problem of causal discovery in time series data by incorporating history-dependent noise. However, the paper's weaknesses are pronounced. The introduction fails to provide a detailed critique of existing frameworks, leaving the reader uncertain about the specific contributions of Rhino. The theoretical aspects, particularly the proof of structural identifiability, are overly complex and may limit the framework's applicability. The empirical validations are not robust, as they rely heavily on synthetic datasets without proper statistical analysis, leading to doubts about the external validity of the results. Furthermore, comparisons with existing methods lack depth, failing to offer a fair assessment of Rhino's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is lacking, particularly in the dense section on variational inference, which may hinder reproducibility and practical implementation. The novelty of the approach is overstated, as the authors do not sufficiently differentiate Rhino from existing methods. Overall, the quality of the presentation falls short of expectations for a scientific publication, with many claims appearing more like marketing statements than rigorous findings.\n\n# Summary Of The Review\nOverall, the paper presents an ambitious framework for causal discovery in time series data, but it falls short in terms of theoretical justification and empirical validation. The claims made regarding the novelty and effectiveness of Rhino are not sufficiently substantiated, raising concerns about its practical applicability and robustness.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces RHINO, a novel framework designed for causal discovery from time series data by integrating vector auto-regression, deep learning, and variational inference. This approach addresses significant challenges in the field by incorporating history-dependent noise, which enhances its adaptability to real-world, dynamic data. Empirical results demonstrate RHINO's superior performance over traditional baselines in both synthetic experiments and real-world applications, showcasing its robustness and applicability across various domains such as healthcare, finance, and climate science.\n\n# Strength And Weaknesses\nThe strengths of RHINO lie in its innovative methodology, which combines established techniques to tackle complex causal relationships effectively. Its theoretical foundation of structural identifiability ensures that the causal relationships it uncovers are reliable, fostering confidence in its application. Additionally, the framework's robustness to model misspecifications is a significant advantage in practical scenarios. However, one potential weakness is that while the paper indicates future extensions to handle non-stationary data and unobserved confounders, it does not provide detailed exploration or preliminary results for these extensions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of the writing is high, making complex concepts accessible. The novelty of RHINO is evident in its unique combination of techniques and focus on history-dependent noise. The release of the code enhances reproducibility, allowing other researchers to validate the results and explore further applications, which is crucial for advancing the field of causal inference.\n\n# Summary Of The Review\nOverall, RHINO represents a significant advancement in causal discovery methodologies for time series data, demonstrating both theoretical rigor and practical applicability. The framework's innovative integration of various techniques and its proven performance make it a valuable contribution to the field. Future extensions could further enhance its utility, and the open-source release encourages collaborative research.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces RHINO, a novel framework for discovering causal relationships in time series data, integrating structural equation models (SEMs), vector auto-regression, and deep learning. Its main contributions include establishing the structural identifiability of RHINO SEMs, incorporating history-dependent noise into causal modeling, and employing variational inference for estimating causal graphs. The findings suggest that RHINO outperforms existing methods like Granger causality by effectively addressing the limitations in handling instantaneous effects and varying noise, thereby enhancing the accuracy of causal inference in complex temporal contexts.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical advancements, particularly the proof of structural identifiability and the innovative incorporation of history-dependent noise, which extends the applicability of causal models. The variational inference methodology is a robust choice that aligns well with the complexities of causal graph inference. However, the paper could benefit from a more extensive empirical evaluation to validate its theoretical claims. While the theoretical framework is sound, the lack of real-world applications and case studies may limit the practical implications of RHINO.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its theoretical contributions clearly, making complex concepts accessible. The quality of the theoretical work is high, and the novelty of integrating history-dependent noise into causal models is significant. However, the reproducibility of the results may be hindered due to the absence of detailed experimental setups or datasets, which are crucial for enabling other researchers to validate and build upon the findings.\n\n# Summary Of The Review\nOverall, RHINO represents a substantial theoretical advancement in causal discovery methods, particularly in the context of time series data. Its novel approach to incorporating history-dependent noise and ensuring structural identifiability sets a strong foundation for future research. However, further empirical validation is needed to fully assess its practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the RHINO framework for deep causal relationship learning in time series data, combining elements of vector auto-regression, deep learning, and variational inference. RHINO presents a novel functional form for Structural Equation Models (SEMs) that accommodates non-linear relationships and history-dependent noise through the use of differentiable functions and neural networks for parameterization. Empirical results demonstrate RHINO's superior performance in causal discovery on synthetic and real-world datasets compared to existing baseline methods such as VARLiNGaM, PCMCI+, and DYNOTEARS, with detailed experimental setups and robust evaluation metrics.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of deep learning techniques with causal inference, particularly in handling non-linear relationships and history-dependent noise, which are often challenging in traditional models. The extensive experimentation with both synthetic and real-world datasets, along with a thorough comparison to baseline methods, adds credibility to its findings. However, the paper falls short in discussing the broader implications of its results, such as potential real-world applications or theoretical advancements in causal inference. Additionally, while the experiments are comprehensive, the reliance on specific hyperparameters may limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible for readers familiar with causal inference and deep learning. The experimental details are adequately documented, with code availability facilitating reproducibility. The novelty of the proposed model and its application to causal discovery is significant, yet the paper could benefit from a more extensive discussion on the implications of its contributions.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of causal inference by introducing RHINO, a framework that effectively combines deep learning with causal discovery methodologies. Its empirical results support the claims of improved performance, although a deeper exploration of the broader implications and generalizability would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Rhino, a novel framework for causal discovery that purportedly addresses the limitations of existing methods such as Granger causality by accommodating non-linear relationships, instantaneous effects, and history-dependent noise. The authors present theoretical proofs for the structural identifiability of Rhino structural equation models (SEMs) and demonstrate superior performance in synthetic experiments compared to baselines like VARLiNGaM and DYNOTEARS. Additionally, the paper reports promising results on real-world datasets, such as DREAM3 and Netsim, while emphasizing Rhino's robustness under model misspecification.\n\n# Strength And Weaknesses\nWhile the paper claims significant advancements over traditional methods, there are several weaknesses in its comparative framework. The authors do not adequately acknowledge the historical significance and practical applications of Granger causality, nor do they sufficiently contextualize the strengths of alternative methods in scenarios where their assumptions hold. The claims of structural identifiability and robustness are not sufficiently contrasted with existing literature, which limits the perceived novelty of these contributions. Furthermore, while the synthetic experiments are extensive, they may overshadow the performance of older models that have been validated in real-world contexts. The methodological innovations, particularly around deep learning and variational inference, lack necessary critiques of their prior applications in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, presenting its methodology and findings clearly. However, the novelty of the proposed methods is somewhat diminished by the lack of comprehensive discussions regarding existing techniques and their effectiveness. The reproducibility of results could be enhanced by providing more details on experimental setups and comparative analyses, particularly regarding the limitations of the datasets used.\n\n# Summary Of The Review\nOverall, while the paper presents Rhino as a significant advancement in causal discovery, it often fails to adequately consider the complexities and successes of established methodologies. This leads to an overly favorable portrayal of Rhino’s contributions, which may not fully capture the nuanced landscape of causal inference research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach for Deep Causal Temporal Relationship Learning with History-Dependent Noise, addressing critical issues in modeling causal relationships over time. The authors propose a new framework that incorporates history-dependent noise into causal inference, utilizing functional causal models. The methodology is rigorously validated through experiments on synthetic and real-world datasets, demonstrating improved performance over baseline models in terms of predictive accuracy and robustness. The findings indicate that the proposed approach significantly enhances the understanding of temporal relationships in various domains, such as climate science and healthcare.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to incorporating history-dependent noise into causal inference, which is a significant advancement in the field. The empirical validation is thorough, showcasing the method's effectiveness across multiple datasets. However, the paper has some weaknesses, particularly in clarity and presentation. There are inconsistencies in notation and some typographical errors that detract from the overall readability. Additionally, while the contributions are substantial, the theoretical underpinnings could benefit from deeper exploration and clearer exposition.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hindered by inconsistent notation and formatting issues, which may confuse readers. The quality of the methodology is strong, but minor typographical errors and unclear variable definitions need addressing to enhance comprehension. The novelty of the approach is significant, as it introduces a fresh perspective on causal relationship modeling. While the authors provide sufficient detail for reproducibility, the inconsistencies in notation and some missing clarifications on hyperparameters may pose challenges for others attempting to replicate the work.\n\n# Summary Of The Review\nOverall, while the paper makes valuable contributions to causal inference with a novel methodology, it suffers from clarity issues and minor inconsistencies that need to be resolved. Addressing these issues would greatly enhance the paper's impact and accessibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework, referred to as Rhino, for causal discovery that accounts for history-dependent noise in time series data. The methodology involves a robust statistical approach to infer causal relationships while addressing the complexities introduced by historical noise. The findings indicate that Rhino outperforms existing causal discovery methods under certain conditions, particularly in scenarios characterized by temporal dependencies.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to integrating history-dependent noise into causal inference, which is a significant advancement in the field. The authors provide empirical evidence supporting the efficacy of Rhino, although the experiments are somewhat limited in scale, which raises concerns about the generalizability of the findings. Additionally, the paper would benefit from a broader comparison against other causal discovery methods, particularly those designed to handle non-linear relationships. There are also several areas for future exploration, such as the application of Rhino to non-stationary time series and the integration of latent confounders.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. However, certain sections could benefit from more detailed discussions, particularly regarding the interpretability of the causal relationships identified and the potential implications of varying hyperparameters. The authors do not discuss computational complexity, which is critical for assessing the practical application of Rhino. Reproducibility might be enhanced by providing additional metrics for evaluation, such as robustness to noise and sensitivity to model assumptions.\n\n# Summary Of The Review\nOverall, the paper offers a compelling contribution to the field of causal inference by introducing Rhino, which accounts for history-dependent noise. Although the methodology is promising, the limited scope of experiments and lack of discussion on certain critical aspects could hinder its broader applicability and understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces \"RHINO,\" a novel framework for discovering causal relationships in time series data, specifically addressing challenges such as non-linear relationships, instantaneous effects, and history-dependent noise. RHINO employs Structural Equation Models (SEMs) to model causal relationships and uses Granger causality as a foundational concept, focusing on observational data rather than randomized control trials. The authors provide a theoretical framework demonstrating the structural identifiability of their model under specific conditions and utilize variational inference for approximating the posterior distribution. Empirical evaluations show that RHINO outperforms several baseline methods on both synthetic datasets and real-world benchmarks, demonstrating robustness even in the presence of model misspecifications.\n\n# Strength And Weaknesses\nThe strengths of RHINO lie in its comprehensive theoretical foundation and empirical validation. The structural identifiability proof enhances the credibility of the model, and the use of both synthetic and real-world datasets provides a well-rounded evaluation of its performance. The ablation studies further illustrate the model's robustness to variations in specifications. However, the paper could benefit from a more detailed discussion on the implications of the assumptions required for identifiability and the potential limitations in scenarios with non-stationary time-series data or unobserved confounders.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making complex concepts accessible to the reader. The quality of writing is high, with thorough explanations of the methodologies and results. The novelty of the proposed framework is significant, as it addresses specific limitations in existing causal discovery methods. However, reproducibility may be impacted by the complexity of the Bayesian framework and the need for specific hyperparameter tuning, which could be better detailed in the paper.\n\n# Summary Of The Review\nOverall, RHINO presents a significant advancement in causal discovery for time series data, backed by strong theoretical and empirical evidence. The framework not only addresses existing challenges but also provides valuable insights into the robustness of causal inference in complex settings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework, Rhino, designed for causal discovery in time-series data. Rhino integrates various aspects of causal inference, including handling history-dependent noise and structural identifiability conditions. The methodology is evaluated against existing baselines, demonstrating superior performance in specific contexts. However, the authors express concerns about the generalizability of the framework to diverse real-world datasets.\n\n# Strength And Weaknesses\nThe main strength of Rhino lies in its innovative approach to incorporating history-dependent noise and establishing structural identifiability conditions, which contribute to its performance in causal discovery tasks. However, several weaknesses are noted, including a lack of generalizability to non-stationary datasets, potential computational bottlenecks in high-dimensional scenarios, and the reliance on hyperparameter tuning that may hinder practical implementation. Additionally, the evaluation metrics are limited, and there is insufficient validation of Rhino's applicability in treatment effect estimation and causal inference tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. However, the complexity of the model may challenge reproducibility, particularly in high-dimensional settings or when specific hyperparameters are required. The novelty of Rhino is significant, especially in its attempt to address issues like history-dependent noise; however, the assumptions made regarding noise distribution and the temporal structure of data may limit its practical utility in dynamic environments.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting framework for causal discovery that addresses some critical issues in the field. Nonetheless, significant limitations regarding generalizability, computational complexity, and the robustness of assumptions necessitate further exploration and validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"RHINO - Deep Causal Temporal Relationship Learning with History-Dependent Noise\" presents a new framework for causal discovery in time series data. The authors propose the RHINO framework, which aims to address the challenges of identifying causal relationships in observational data characterized by non-linear relationships, slow sampling intervals, and history-dependent noise. The methodology involves a combination of deep learning and variational inference to derive structural identifiability conditions and to validate the framework through extensive ablation studies and synthetic experiments, ultimately demonstrating its robustness and superiority over existing methods.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its attempt to tackle the complex problem of causal discovery with a novel framework that includes theoretical proofs of structural identifiability. The extensive empirical evaluation, including synthetic and real-world benchmarks, adds credibility to the findings. However, the paper also suffers from a lack of originality in its conceptualization, as many of the challenges it addresses have been well documented in the literature. The complexity of the model and the use of convoluted terminology may also hinder accessibility and understanding for practitioners.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by its density and the use of technical jargon, which may alienate readers not deeply familiar with the domain. The quality of the writing is generally high, but the novelty is mixed; while the framework is presented as innovative, it builds upon existing concepts without a significant paradigm shift. The reproducibility of the findings is supported by the inclusion of ablation studies and benchmarks, although the complexity of the methodology may pose challenges for others attempting to replicate it.\n\n# Summary Of The Review\nOverall, the paper presents a well-structured approach to a recognized problem in causal discovery, but it lacks a significant degree of novelty and may come across as more of a sophisticated rehash of known ideas. While the empirical results are promising, the paper does not sufficiently advance the field in a groundbreaking way.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel framework named Rhino, which incorporates history-dependent noise for enhanced causal discovery in time series data. The authors utilize deep learning techniques to model non-linear relationships and adopt a Bayesian approach for causal inference. Significant findings include the demonstration of Rhino's effectiveness on biological networks and brain connectivity, as well as a comprehensive synthetic data generation process that facilitates rigorous testing of causal inference algorithms.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative integration of history-dependent noise, which addresses a critical gap in causal discovery methodologies. The framework's application of deep learning to capture non-linear relationships is promising and could lead to improved performance in real-world scenarios. However, the complexity of the models may hinder interpretability, which is a notable weakness. While the paper conducts ablation studies, comparisons against a broader spectrum of causal discovery methods are recommended to fully assess Rhino's robustness and performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with clear explanations of the methodology and findings. The novelty of the approach is significant, particularly in the context of incorporating history-dependent noise and using a Bayesian framework. However, reproducibility could be a concern due to the complexity of the implementations and the need for more interpretable outputs. Future work should aim to enhance interpretability without compromising the model's performance.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to causal discovery in time series using the Rhino framework, which shows promise in addressing historical noise and non-linear relationships. While the contributions are noteworthy, there is a need for improved interpretability and broader benchmarking against existing methods. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Rhino, a novel method for causal discovery in time series data, particularly focusing on the challenges posed by non-linear relationships, instantaneous effects, and history-dependent noise. The methodology is validated through extensive benchmarking on both synthetic datasets and real-world datasets, such as DREAM3 and Netsim, achieving competitive or superior performance compared to existing baselines like VARLiNGaM, DYNOTEARS, and PCMCI+. Key findings include Rhino's robustness to model misspecification, its ability to maintain high AUROC scores across various settings, and the positive contribution of history-dependent noise modeling to its overall performance.\n\n# Strength And Weaknesses\nStrengths of the paper include its strong empirical results across multiple datasets, demonstrating Rhino’s effectiveness in causal discovery. The method's robustness to model misspecification and the insightful ablation studies add depth to the analysis, highlighting the importance of specific components within the model. However, a notable weakness is the discrepancy observed between AUROC and F1 scores, particularly in smaller datasets, which raises concerns about potential overfitting and the need for cautious interpretation of results. Additionally, while the comparative analysis is thorough, further exploration of computational efficiency across varying dimensions could enhance the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers. The quality of the empirical evaluations is high, with comprehensive benchmarks that lend credibility to the results. In terms of novelty, Rhino introduces significant advancements in handling complex causal relationships, although the extent of its contributions relative to existing methods could be more explicitly articulated. Reproducibility is addressed to an extent, but additional details on the implementation and parameter settings would be beneficial for practitioners looking to replicate the results.\n\n# Summary Of The Review\nOverall, the paper effectively showcases Rhino as a robust method for causal discovery in time series data, with strong empirical performance across diverse datasets. While the findings are promising, attention to discrepancies in performance metrics and a clearer presentation of implementation details could enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for causal discovery that addresses the challenges posed by history-dependent noise and instantaneous effects in time-series data. The authors propose a new methodology that integrates structural equation modeling (SEM) with directed acyclic graphs (DAGs) to identify causal relationships more effectively. Empirical evaluations on synthetic and real-world datasets demonstrate that the proposed method outperforms existing approaches in terms of accuracy and robustness, revealing significant insights into the underlying causal structures.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to integrating SEM and DAGs, which is a notable contribution to the field of causal inference. The empirical results are compelling, showcasing the method's effectiveness across various scenarios. However, the paper suffers from clarity issues, particularly in the abstract and terminologies where complex concepts are not sufficiently defined. Additionally, while the methodology is promising, the reliance on appendices for detailed explanations may hinder accessibility for some readers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by dense language and complex sentence structures that could be simplified for better readability. The quality of the research is high, but the presentation detracts from its overall impact. The novelty of the proposed methodology is significant, providing fresh insights into causal discovery. However, the reproducibility could be improved by providing clearer guidance on the implementation of the methodology, including detailed descriptions of the datasets used and the experimental setup.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative framework for causal discovery that demonstrates strong empirical performance. However, improvements in clarity and accessibility are necessary to maximize its impact and ensure reproducibility in future research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.417472091841623,
    -1.6118730603795433,
    -1.68895232437526,
    -1.6498145386199314,
    -1.8235304121752445,
    -1.694475651812872,
    -1.7015460312356123,
    -1.9226922462008957,
    -1.701737935358283,
    -1.8052307253639637,
    -1.612635427230168,
    -1.3666563571555774,
    -1.801229347538872,
    -1.6580190941667146,
    -1.678794588900804,
    -1.7819737973638952,
    -1.911932068901509,
    -1.7096634089660574,
    -1.7204114814416362,
    -1.7222931000241088,
    -2.0684056255849423,
    -1.6365562788048096,
    -1.800132826331369,
    -1.6594165369073557,
    -1.8852323644527473,
    -1.8482888349108182,
    -1.8491964306740662,
    -1.7794896881098792,
    -1.5765268786926068
  ],
  "logp_cond": [
    [
      0.0,
      -1.937751323368477,
      -1.9391499058029837,
      -1.991108044339399,
      -1.995831575451694,
      -1.9432750801270735,
      -2.0098232703112164,
      -1.9249866889941432,
      -1.9949285174151195,
      -2.0253025106103486,
      -1.9688403120391376,
      -2.117990645785681,
      -1.9315251944143035,
      -1.9575124661795371,
      -1.9297147590184938,
      -1.9482851136928352,
      -1.9927657775082246,
      -1.942745722300911,
      -1.9883420086009342,
      -1.9747355977012957,
      -1.9521993384364171,
      -2.099354154400197,
      -2.0388951905731676,
      -1.9937335042881443,
      -2.0279542779401494,
      -1.9805876839190384,
      -2.02368479588168,
      -2.0051493036683943,
      -2.061002504415139
    ],
    [
      -1.1911180488029873,
      0.0,
      -1.1243737509778906,
      -1.0885930597124491,
      -1.1110151625341755,
      -1.0998107826678274,
      -1.1672916863459661,
      -1.0630193260092673,
      -1.1085884256517249,
      -1.1669584308484806,
      -1.090674230600041,
      -1.3459107485675343,
      -1.0708133806437912,
      -1.157407166791381,
      -1.099055524917771,
      -1.1293837299171048,
      -1.1084667091602987,
      -1.1574070619405927,
      -1.0897886244579986,
      -1.1697553084846424,
      -1.088897597538499,
      -1.282772560326965,
      -1.213713712359867,
      -1.0724573299658524,
      -1.21494676392638,
      -1.1730682536920418,
      -1.1758952881886557,
      -1.167160304062326,
      -1.2315700953869753
    ],
    [
      -1.3169866318313788,
      -1.1762785184300428,
      0.0,
      -1.23528933067903,
      -1.217467135729099,
      -1.1889053598806623,
      -1.2538574468292367,
      -1.2234815144869862,
      -1.2367203466668661,
      -1.2055703111316658,
      -1.2032879188229795,
      -1.4611771609515831,
      -1.1971922011073541,
      -1.1638380753481707,
      -1.1596968645830192,
      -1.2429148704627724,
      -1.2342795855198077,
      -1.1991358841227255,
      -1.2090589933540294,
      -1.233721912762436,
      -1.2630832837578136,
      -1.345976226444251,
      -1.298215849961148,
      -1.2172710564060651,
      -1.2726865130813112,
      -1.2588797863467065,
      -1.2522924917951346,
      -1.234702948476844,
      -1.3368027709864556
    ],
    [
      -1.3201922394973218,
      -1.164775652317927,
      -1.200488267962846,
      0.0,
      -1.192662421647365,
      -1.1893914157536787,
      -1.2103423842941263,
      -1.2041515768898547,
      -1.1381246513431145,
      -1.2011719999889294,
      -1.2035020241585512,
      -1.3361227551714958,
      -1.2216809628503298,
      -1.2034231253206036,
      -1.1673150932552547,
      -1.2106384296023456,
      -1.216962814755222,
      -1.232453656002612,
      -1.188215840900346,
      -1.243662145288238,
      -1.1851393722182737,
      -1.2679964139080935,
      -1.265652148920537,
      -1.217697487857063,
      -1.254712652847346,
      -1.2150702063679848,
      -1.2655330368154383,
      -1.2189408736758376,
      -1.3144094091516239
    ],
    [
      -1.489836526134144,
      -1.386188216462508,
      -1.397111447914301,
      -1.3944803710058071,
      0.0,
      -1.4417968882306105,
      -1.4194185467237175,
      -1.4345732889803844,
      -1.4588937007109957,
      -1.4291877787352203,
      -1.4151388099308768,
      -1.5467800669937368,
      -1.3999256775943385,
      -1.4325512725473148,
      -1.4531278565789176,
      -1.4478240179362394,
      -1.441013147080527,
      -1.442065825075911,
      -1.4432021343101207,
      -1.4370140990349671,
      -1.4127064967738328,
      -1.528868892648727,
      -1.4706354134302873,
      -1.3759481581560982,
      -1.4383061826272636,
      -1.441731068528135,
      -1.4221958852437797,
      -1.398444300501916,
      -1.5193507809684192
    ],
    [
      -1.3488531106329142,
      -1.2765860826561304,
      -1.2553551331476431,
      -1.290959352123488,
      -1.3262042350824979,
      0.0,
      -1.3165402915764775,
      -1.2404213450505914,
      -1.3345478152461263,
      -1.2818927567722835,
      -1.3017769190044473,
      -1.4272171687805473,
      -1.2979048308190402,
      -1.227974848828283,
      -1.19484622577666,
      -1.2753936548847173,
      -1.309253994434627,
      -1.2823851438302232,
      -1.2966504260710836,
      -1.2762589751248226,
      -1.2845228748161506,
      -1.3667291050265429,
      -1.375150694375923,
      -1.2735128034010634,
      -1.3095964857723263,
      -1.22971465822434,
      -1.3484689748216132,
      -1.2859013640834827,
      -1.4029673443179957
    ],
    [
      -1.3340099649921435,
      -1.2042168159055608,
      -1.2242004315961097,
      -1.223937041911733,
      -1.240149804208243,
      -1.2217252986159455,
      0.0,
      -1.2143569228294349,
      -1.3178489162197624,
      -1.3097405570044625,
      -1.291187500439794,
      -1.382489427363166,
      -1.2093254210758377,
      -1.2732100386709972,
      -1.1982589176634293,
      -1.2544378566672687,
      -1.2748395829367443,
      -1.2336103559522313,
      -1.2251733220797312,
      -1.2797918769535293,
      -1.2928476275295748,
      -1.358216562648973,
      -1.3391545703638976,
      -1.2916568690608285,
      -1.314903765405178,
      -1.3060079478362927,
      -1.2941636667712109,
      -1.3077310004250278,
      -1.3959925369158594
    ],
    [
      -1.555717383258433,
      -1.4485428288030557,
      -1.4527768093563638,
      -1.5215222372094737,
      -1.5468932493193106,
      -1.4739662146184525,
      -1.543767719963552,
      0.0,
      -1.489883735545129,
      -1.53551957784408,
      -1.4804647634803143,
      -1.6874873227598446,
      -1.4742975066468607,
      -1.4491098657341661,
      -1.4273744322194766,
      -1.511727810939484,
      -1.4821819641082508,
      -1.4630258293756344,
      -1.474967256551761,
      -1.498057023641015,
      -1.4664139861744148,
      -1.6150719399785967,
      -1.5880836630083126,
      -1.4640917403765321,
      -1.5546129393770252,
      -1.518220359660497,
      -1.5267794042278329,
      -1.5332818350224073,
      -1.5710742591702256
    ],
    [
      -1.3453305348044338,
      -1.1739483211679458,
      -1.2296636542597166,
      -1.1398811941720615,
      -1.3000554695009263,
      -1.2085273254684157,
      -1.3000764056547214,
      -1.203953782154956,
      0.0,
      -1.2270047636464774,
      -1.1859248183032092,
      -1.4214250552732044,
      -1.2292934466112921,
      -1.1814077151481852,
      -1.1291187725637089,
      -1.256759606063832,
      -1.2345894142582374,
      -1.2382333091791333,
      -1.2180221688245592,
      -1.2210923747473672,
      -1.212896993472529,
      -1.323949434942078,
      -1.2891159729289783,
      -1.2379435323284436,
      -1.2934244712117176,
      -1.2482206007170327,
      -1.2601625435917034,
      -1.2381408250062103,
      -1.3160859997232985
    ],
    [
      -1.4781664248487494,
      -1.444736135587702,
      -1.4447017073754602,
      -1.4446489237536433,
      -1.4646019089116706,
      -1.3982910772158574,
      -1.4934886376870553,
      -1.4582193470814797,
      -1.4603740462332875,
      0.0,
      -1.4192935850716981,
      -1.5606591755320178,
      -1.4176762341989024,
      -1.3846987309283623,
      -1.4062622143298362,
      -1.4218514481777569,
      -1.4487838282006713,
      -1.4232967581914804,
      -1.419031483710883,
      -1.4262200411279684,
      -1.408899060117036,
      -1.4573275332795432,
      -1.4617665631529397,
      -1.4201377700909292,
      -1.4168365366309887,
      -1.3841953958078286,
      -1.4693040295141147,
      -1.4475308287043538,
      -1.4973826608691743
    ],
    [
      -1.2649788393284178,
      -1.1239597173616687,
      -1.1378523884088527,
      -1.112900061780569,
      -1.1894707837159604,
      -1.1772609979122006,
      -1.2241988468584908,
      -1.1641721663850813,
      -1.0563149534351324,
      -1.1562556096814964,
      0.0,
      -1.33405747693834,
      -1.1284682598403053,
      -1.1298591824489925,
      -1.1378912570063893,
      -1.1800692386710834,
      -1.1722949528516315,
      -1.166860180192774,
      -1.142391108444039,
      -1.2462388644395133,
      -1.1329501161118913,
      -1.259209126340133,
      -1.1750387876617239,
      -1.1080758419047436,
      -1.1939599225821693,
      -1.1681405290094182,
      -1.2195931946288103,
      -1.1580128950071589,
      -1.218361269815532
    ],
    [
      -1.1590916222994483,
      -1.1456253733063255,
      -1.1481843784547665,
      -1.1289217498996482,
      -1.117060626935487,
      -1.1279862291588443,
      -1.0984364578545949,
      -1.1116231126614795,
      -1.145893725787785,
      -1.1425370629059892,
      -1.1467364952076768,
      0.0,
      -1.1629402954597041,
      -1.1358671487505902,
      -1.130576096878334,
      -1.1513891841563504,
      -1.1293732838227868,
      -1.145454932388174,
      -1.141346420763249,
      -1.1316036329654588,
      -1.1095457723427362,
      -1.072473359166122,
      -1.1281457281377845,
      -1.1428504483597424,
      -1.151904108362608,
      -1.133187196140915,
      -1.1480867743404346,
      -1.1333551535619246,
      -1.0915211899615491
    ],
    [
      -1.3547492901741647,
      -1.222571918145295,
      -1.2585779817805827,
      -1.3398764991482386,
      -1.3173922014827553,
      -1.268895205609835,
      -1.334327920053621,
      -1.235930687060409,
      -1.3253598121640928,
      -1.3597776958020638,
      -1.2634587094316663,
      -1.5073471965694376,
      0.0,
      -1.2975795380772257,
      -1.2244402234803058,
      -1.2713681242265857,
      -1.3529825895358565,
      -1.2577495319158791,
      -1.3058293528314828,
      -1.335835378451813,
      -1.2885938995171948,
      -1.4582168868368834,
      -1.3513099547701886,
      -1.2290691194898067,
      -1.3611131510428256,
      -1.2943096621780101,
      -1.39679423666746,
      -1.3265409469315135,
      -1.4164063567779273
    ],
    [
      -1.274269267054645,
      -1.1286098114266387,
      -1.137130935530281,
      -1.1610080786048893,
      -1.1918376384350788,
      -1.1339529535832502,
      -1.2393112328571054,
      -1.1270966286277275,
      -1.1562577627302688,
      -1.172888581525958,
      -1.177323441474205,
      -1.3546914303836848,
      -1.1669506763633033,
      0.0,
      -1.0343931977784386,
      -1.1077749346954167,
      -1.2101604978105407,
      -1.1832307517222806,
      -1.1747109609865218,
      -1.1590500937007655,
      -1.165845092551723,
      -1.2689413507760314,
      -1.2251836455948129,
      -1.1697567268018794,
      -1.2371392127393863,
      -1.1891932633536895,
      -1.2330667396332502,
      -1.1920168075977906,
      -1.2825533279495667
    ],
    [
      -1.2958902410972644,
      -1.1710181770938737,
      -1.1458766151293178,
      -1.1987140915457821,
      -1.279569401055292,
      -1.1775030687567614,
      -1.293370514953511,
      -1.1865982253455944,
      -1.1959394944750497,
      -1.2455446046808964,
      -1.2204683167291028,
      -1.4039072991325277,
      -1.1991623997827907,
      -1.1139131694157134,
      0.0,
      -1.1715529931032918,
      -1.278983349838943,
      -1.2229825093134254,
      -1.2344574274590947,
      -1.203698141085241,
      -1.210997686427424,
      -1.320005738696447,
      -1.3215797232323243,
      -1.2282290473724098,
      -1.3055193091789286,
      -1.2174098710105248,
      -1.3042045098107526,
      -1.2379820437666758,
      -1.3383858439680336
    ],
    [
      -1.3543208879622208,
      -1.259482984462515,
      -1.2680465791988837,
      -1.3012376089741846,
      -1.3675030485916664,
      -1.252822414046351,
      -1.3607823293958992,
      -1.2744915639534014,
      -1.326154814044986,
      -1.2719692936495721,
      -1.290771530883009,
      -1.5086795785070177,
      -1.254493549105527,
      -1.2390763695552371,
      -1.214867267779495,
      0.0,
      -1.2943296955204329,
      -1.3162005464775919,
      -1.2269717223076568,
      -1.299717922090703,
      -1.2662761883869194,
      -1.370165629952284,
      -1.329284754744923,
      -1.2633159846851805,
      -1.3181833605931226,
      -1.271201362531428,
      -1.3488326829381556,
      -1.3220464566435823,
      -1.3869027319625185
    ],
    [
      -1.5772877051067935,
      -1.4808743609852004,
      -1.4468499299671356,
      -1.533677010071389,
      -1.511985288822641,
      -1.4401213454401258,
      -1.5494256400910604,
      -1.4252522803781449,
      -1.4969505058505728,
      -1.5199344200064853,
      -1.4422118602568388,
      -1.6398780419225534,
      -1.4999018352677458,
      -1.4630830304788707,
      -1.454588313673131,
      -1.4995761058158386,
      0.0,
      -1.47662014066318,
      -1.4805012793150913,
      -1.5269284987187384,
      -1.4749262622714832,
      -1.6031289644933595,
      -1.518467277591133,
      -1.4693175583973466,
      -1.4921204453642822,
      -1.46991031493803,
      -1.4583052185891914,
      -1.486634356428778,
      -1.5684166593794644
    ],
    [
      -1.3006578025651483,
      -1.1554305001612641,
      -1.2280752148800063,
      -1.2800295948670242,
      -1.2489259654289826,
      -1.2348933124496029,
      -1.2777718081943183,
      -1.2041656588073848,
      -1.2473934745693673,
      -1.3214276495993953,
      -1.271278906254041,
      -1.4326377114177797,
      -1.1582962605636196,
      -1.2196089040078892,
      -1.1796429880125963,
      -1.2221949560559109,
      -1.2289148683312536,
      0.0,
      -1.2396695977949876,
      -1.2674635665372356,
      -1.2441287763835256,
      -1.3494341726693342,
      -1.3218389280334972,
      -1.2176556170079742,
      -1.3056127390968304,
      -1.2535763448875867,
      -1.2928984097527052,
      -1.3072812024144933,
      -1.355775149730246
    ],
    [
      -1.3381396554659342,
      -1.262010760458312,
      -1.2546438648867966,
      -1.3029398697553323,
      -1.3374489536355134,
      -1.3155122592975796,
      -1.3267571369783502,
      -1.2524470022770646,
      -1.308891547701842,
      -1.2843396738961803,
      -1.306945848865376,
      -1.4722168206997475,
      -1.2856923288231399,
      -1.2865674280554737,
      -1.2791419531764159,
      -1.3054254468280224,
      -1.3124773210545833,
      -1.2585753246710591,
      0.0,
      -1.3028952394251465,
      -1.262585420959169,
      -1.423810752379654,
      -1.3585259449841511,
      -1.2516339569611497,
      -1.3322615014104808,
      -1.3251636165958973,
      -1.3361854460365257,
      -1.3155580624282976,
      -1.3782176263144659
    ],
    [
      -1.3743115000530475,
      -1.2628315052340016,
      -1.2625245581469204,
      -1.2558066309634086,
      -1.3325722726399458,
      -1.2450966540859731,
      -1.3508944076222242,
      -1.242626453370017,
      -1.2751341442323911,
      -1.2776658712813345,
      -1.2924219643607011,
      -1.4580319705585267,
      -1.2689548477068364,
      -1.2322866151266958,
      -1.2034069209500289,
      -1.266282627588078,
      -1.3148453437089687,
      -1.287166788448317,
      -1.294225050736692,
      0.0,
      -1.24043386134109,
      -1.371009875108244,
      -1.3318640581275403,
      -1.3006465403359675,
      -1.3040791586910374,
      -1.3154596013647655,
      -1.3105331025199867,
      -1.2823714496100338,
      -1.346682496968259
    ],
    [
      -1.7207560724800788,
      -1.5948232344287072,
      -1.6313661549800862,
      -1.644414516673716,
      -1.6534414719002177,
      -1.6468838151960394,
      -1.734240499885487,
      -1.5785843934159254,
      -1.6340886313653469,
      -1.6759398404566648,
      -1.6238285946310198,
      -1.855148089250035,
      -1.6297708632632695,
      -1.6338818045181773,
      -1.6254293283353713,
      -1.6453039290746083,
      -1.6855578060843501,
      -1.6359194640476529,
      -1.6174021383058028,
      -1.6029010238847132,
      0.0,
      -1.8021795208230547,
      -1.7509103730920859,
      -1.6237948411480876,
      -1.7022909393934311,
      -1.6763526883630946,
      -1.6733373877181388,
      -1.5448073922614145,
      -1.7651868384147755
    ],
    [
      -1.3934596219980586,
      -1.3196517481686336,
      -1.3606619918853904,
      -1.297036388404002,
      -1.314470230532153,
      -1.2939502573488202,
      -1.3086151283752347,
      -1.352051256017434,
      -1.3270256738879298,
      -1.275234435875827,
      -1.3319253815517362,
      -1.3325736456386255,
      -1.354208531849102,
      -1.2760807383106803,
      -1.2662476130495093,
      -1.3114119440308978,
      -1.363688706333529,
      -1.332603705148121,
      -1.3690685727499956,
      -1.3265510057592194,
      -1.3364019989070368,
      0.0,
      -1.347196671015313,
      -1.3565938409905078,
      -1.3544780533109544,
      -1.2831080679179878,
      -1.3450994515128114,
      -1.2935736372454458,
      -1.3211063573647228
    ],
    [
      -1.469688797807861,
      -1.4204772276698585,
      -1.4217610613701654,
      -1.427192934609493,
      -1.4351342852811098,
      -1.3723880698804851,
      -1.4285542522732246,
      -1.3959385772951933,
      -1.4146163919492885,
      -1.3616075501015323,
      -1.3835338557092893,
      -1.480653478617546,
      -1.3945225570861637,
      -1.3703019015978566,
      -1.3965006854414919,
      -1.4294256276659476,
      -1.3590262501572998,
      -1.3855079149004368,
      -1.4006393548016498,
      -1.3798525167946007,
      -1.395901849965654,
      -1.3980594848315244,
      0.0,
      -1.4070593645911875,
      -1.35934826560314,
      -1.3865985037107347,
      -1.372681087920927,
      -1.396162264736716,
      -1.4175464097525996
    ],
    [
      -1.3076277785522692,
      -1.1299926196354242,
      -1.1712189201661685,
      -1.2122755496750943,
      -1.2144075051815746,
      -1.1882291734454884,
      -1.2840552686198892,
      -1.1938377360699015,
      -1.1887688711743867,
      -1.2052011549405943,
      -1.2100500928466873,
      -1.3988354970522736,
      -1.1675940314465398,
      -1.1934221779778262,
      -1.2163193225124984,
      -1.1845088547261815,
      -1.2304604960903418,
      -1.2358899455243637,
      -1.187232874335773,
      -1.26411931972585,
      -1.175461409145159,
      -1.3278336833993558,
      -1.2914356764731487,
      0.0,
      -1.2444040898794324,
      -1.221605198742135,
      -1.2862340355114799,
      -1.2071674155834937,
      -1.255519686969652
    ],
    [
      -1.5270621459484592,
      -1.4716982121601916,
      -1.4293978090230945,
      -1.500167965573908,
      -1.4788084519660143,
      -1.433212133348741,
      -1.5023492054960255,
      -1.445424255590335,
      -1.4797760166715799,
      -1.4501155389452778,
      -1.4724790095278268,
      -1.6008469960084344,
      -1.4220082512806878,
      -1.4734952533892023,
      -1.4415872967375547,
      -1.4750019467892346,
      -1.3985205066009827,
      -1.4563008168863483,
      -1.459701695737007,
      -1.4695344936414874,
      -1.4323648237098672,
      -1.5330986060607519,
      -1.4556151081927362,
      -1.4595369550621187,
      0.0,
      -1.4316149352064318,
      -1.4499410817277516,
      -1.4490806127077887,
      -1.5366492200140707
    ],
    [
      -1.5433901940904562,
      -1.422251752614658,
      -1.4288780983580174,
      -1.4597362858369476,
      -1.4684956292410287,
      -1.3750212488869735,
      -1.5218431370106427,
      -1.4009728767115963,
      -1.4677976057144486,
      -1.4128577706160792,
      -1.4611079959457707,
      -1.5970395122452106,
      -1.4426868691866774,
      -1.4560303342994727,
      -1.3771442097214581,
      -1.412788700907865,
      -1.4592701864215256,
      -1.4635826953465605,
      -1.4839749273925358,
      -1.4750440351703022,
      -1.4374251901052502,
      -1.528253117177685,
      -1.5128885607591431,
      -1.4213975264098773,
      -1.4719872914636687,
      0.0,
      -1.4858964111383297,
      -1.485412819427102,
      -1.5324202600400727
    ],
    [
      -1.520565827953994,
      -1.4597367740165281,
      -1.4351847152166326,
      -1.456145913070834,
      -1.4309264848614884,
      -1.4462330646244834,
      -1.501419921540963,
      -1.4657909651425123,
      -1.463002894288975,
      -1.4071267440886208,
      -1.4634761867783694,
      -1.6220081960742434,
      -1.4848890948974132,
      -1.4560255740551928,
      -1.4629112385508143,
      -1.4740346357431822,
      -1.389408129110364,
      -1.4506115577408034,
      -1.4495883682694546,
      -1.4811939469161006,
      -1.4764276697776575,
      -1.5500563187449077,
      -1.465624030002069,
      -1.4650397179041643,
      -1.466423300726584,
      -1.436296679615545,
      0.0,
      -1.4445492050749884,
      -1.510298823809123
    ],
    [
      -1.4589870770528703,
      -1.3773854867102426,
      -1.3510454680892667,
      -1.3984142753505129,
      -1.4126721723284674,
      -1.3882689901800154,
      -1.4282533080032296,
      -1.356767872055194,
      -1.3674765369890591,
      -1.3927582396732043,
      -1.368777955484312,
      -1.5382759909962531,
      -1.3834493141495632,
      -1.3933338797585801,
      -1.3857571835796574,
      -1.4062761024194332,
      -1.405428482617541,
      -1.4452042644573142,
      -1.3929717750236235,
      -1.3786703087091834,
      -1.2767466654384274,
      -1.4917926362808038,
      -1.4277166341614775,
      -1.3420156509818009,
      -1.3984177512606422,
      -1.4033358362834338,
      -1.3776871306365535,
      0.0,
      -1.4324082798223112
    ],
    [
      -1.2527895308363233,
      -1.1465449411265085,
      -1.1624466767726116,
      -1.1833865135994541,
      -1.1792257585441137,
      -1.1759088097672823,
      -1.202407051673926,
      -1.1631224016636583,
      -1.1384117971730703,
      -1.1728818683452817,
      -1.1513370880391371,
      -1.2264543561695482,
      -1.181591147382422,
      -1.1747809246862775,
      -1.1734011890429525,
      -1.1869466471898116,
      -1.142697826832139,
      -1.1727010522684784,
      -1.1716996573397014,
      -1.1733139406800308,
      -1.1601420952873343,
      -1.1057647803322912,
      -1.1550622146458271,
      -1.1509936765693438,
      -1.1532298336853877,
      -1.1672682392163087,
      -1.1797690808831123,
      -1.1163188223450677,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.4797207684731457,
      0.4783221860386391,
      0.4263640475022239,
      0.42164051638992883,
      0.4741970117145493,
      0.4076488215304064,
      0.49248540284747966,
      0.42254357442650337,
      0.3921695812312742,
      0.44863177980248525,
      0.2994814460559416,
      0.48594689742731934,
      0.4599596256620857,
      0.48775733282312905,
      0.4691869781487876,
      0.4247063143333982,
      0.4747263695407118,
      0.4291300832406886,
      0.4427364941403271,
      0.4652727534052057,
      0.3181179374414258,
      0.3785769012684552,
      0.4237385875534785,
      0.38951781390147344,
      0.43688440792258443,
      0.3937872959599429,
      0.41232278817322854,
      0.3564695874264836
    ],
    [
      0.4207550115765559,
      0.0,
      0.48749930940165265,
      0.5232800006670941,
      0.5008578978453677,
      0.5120622777117159,
      0.4445813740335771,
      0.548853734370276,
      0.5032846347278184,
      0.44491462953106264,
      0.5211988297795023,
      0.2659623118120089,
      0.5410596797357521,
      0.45446589358816225,
      0.5128175354617723,
      0.48248933046243847,
      0.5034063512192446,
      0.4544659984389505,
      0.5220844359215446,
      0.4421177518949009,
      0.5229754628410443,
      0.3291005000525782,
      0.39815934801967634,
      0.5394157304136908,
      0.39692629645316324,
      0.43880480668750144,
      0.4359777721908875,
      0.4447127563172173,
      0.380302964992568
    ],
    [
      0.37196569254388123,
      0.5126738059452172,
      0.0,
      0.45366299369623,
      0.47148518864616107,
      0.5000469644945977,
      0.4350948775460233,
      0.46547080988827383,
      0.45223197770839385,
      0.4833820132435942,
      0.4856644055522805,
      0.22777516342367687,
      0.49176012326790586,
      0.5251142490270893,
      0.5292554597922408,
      0.4460374539124876,
      0.45467273885545234,
      0.48981644025253446,
      0.47989333102123055,
      0.45523041161282407,
      0.42586904061744635,
      0.342976097931009,
      0.39073647441411197,
      0.47168126796919485,
      0.4162658112939488,
      0.4300725380285535,
      0.4366598325801254,
      0.4542493758984161,
      0.3521495533888044
    ],
    [
      0.32962229912260965,
      0.48503888630200453,
      0.4493262706570855,
      0.0,
      0.4571521169725665,
      0.46042312286625275,
      0.4394721543258051,
      0.4456629617300767,
      0.511689887276817,
      0.448642538631002,
      0.4463125144613802,
      0.3136917834484356,
      0.42813357576960165,
      0.44639141329932785,
      0.4824994453646767,
      0.43917610901758586,
      0.4328517238647094,
      0.4173608826173194,
      0.4615986977195854,
      0.4061523933316935,
      0.46467516640165774,
      0.38181812471183796,
      0.3841623896993944,
      0.43211705076286844,
      0.3951018857725854,
      0.43474433225194664,
      0.38428150180449316,
      0.43087366494409385,
      0.3354051294683076
    ],
    [
      0.3336938860411005,
      0.4373421957127366,
      0.4264189642609435,
      0.42905004116943735,
      0.0,
      0.381733523944634,
      0.404111865451527,
      0.3889571231948601,
      0.3646367114642488,
      0.3943426334400242,
      0.40839160224436766,
      0.27675034518150765,
      0.423604734580906,
      0.39097913962792963,
      0.37040255559632684,
      0.3757063942390051,
      0.3825172650947175,
      0.3814645870993334,
      0.3803282778651238,
      0.38651631314027735,
      0.4108239154014117,
      0.2946615195265174,
      0.3528949987449572,
      0.4475822540191463,
      0.38522422954798086,
      0.38179934364710943,
      0.40133452693146476,
      0.42508611167332844,
      0.30417963120682523
    ],
    [
      0.34562254117995783,
      0.4178895691567417,
      0.43912051866522894,
      0.4035162996893842,
      0.3682714167303742,
      0.0,
      0.3779353602363946,
      0.45405430676228065,
      0.35992783656674576,
      0.41258289504058854,
      0.3926987328084248,
      0.2672584830323248,
      0.3965708209938319,
      0.4665008029845892,
      0.49962942603621197,
      0.41908199692815473,
      0.385221657378245,
      0.4120905079826489,
      0.39782522574178847,
      0.4182166766880495,
      0.4099527769967215,
      0.3277465467863292,
      0.319324957436949,
      0.42096284841180864,
      0.38487916604054573,
      0.4647609935885322,
      0.34600667699125887,
      0.4085742877293894,
      0.29150830749487633
    ],
    [
      0.3675360662434688,
      0.4973292153300515,
      0.4773455996395026,
      0.4776089893238793,
      0.46139622702736927,
      0.47982073261966685,
      0.0,
      0.48718910840617746,
      0.3836971150158499,
      0.39180547423114986,
      0.4103585307958184,
      0.3190566038724463,
      0.4922206101597746,
      0.4283359925646151,
      0.503287113572183,
      0.4471081745683436,
      0.42670644829886806,
      0.467935675283381,
      0.47637270915588115,
      0.42175415428208307,
      0.4086984037060375,
      0.34332946858663926,
      0.3623914608717147,
      0.40988916217478377,
      0.38664226583043426,
      0.3955380833993196,
      0.40738236446440146,
      0.39381503081058455,
      0.30555349431975287
    ],
    [
      0.36697486294246273,
      0.47414941739784,
      0.46991543684453196,
      0.4011700089914221,
      0.37579899688158513,
      0.4487260315824433,
      0.37892452623734374,
      0.0,
      0.43280851065576664,
      0.38717266835681574,
      0.44222748272058143,
      0.23520492344105115,
      0.4483947395540351,
      0.4735823804667296,
      0.49531781398141916,
      0.4109644352614117,
      0.440510282092645,
      0.4596664168252613,
      0.44772498964913465,
      0.42463522255988084,
      0.456278260026481,
      0.3076203062222991,
      0.33460858319258313,
      0.4586005058243636,
      0.3680793068238706,
      0.4044718865403987,
      0.39591284197306287,
      0.3894104111784884,
      0.35161798703067015
    ],
    [
      0.35640740055384934,
      0.5277896141903373,
      0.47207428109856653,
      0.5618567411862216,
      0.4016824658573568,
      0.4932106098898674,
      0.4016615297035617,
      0.49778415320332714,
      0.0,
      0.47473317171180573,
      0.5158131170550739,
      0.2803128800850787,
      0.47244448874699096,
      0.5203302202100979,
      0.5726191627945743,
      0.44497832929445114,
      0.4671485211000457,
      0.4635046261791498,
      0.48371576653372395,
      0.4806455606109159,
      0.488840941885754,
      0.37778850041620515,
      0.4126219624293048,
      0.46379440302983954,
      0.40831346414656555,
      0.45351733464125044,
      0.4415753917665797,
      0.46359711035207285,
      0.38565193563498457
    ],
    [
      0.3270643005152143,
      0.3604945897762617,
      0.36052901798850345,
      0.3605818016103204,
      0.34062881645229304,
      0.40693964814810624,
      0.31174208767690836,
      0.34701137828248396,
      0.3448566791306762,
      0.0,
      0.38593714029226556,
      0.24457154983194584,
      0.38755449116506124,
      0.4205319944356014,
      0.39896851103412745,
      0.3833792771862068,
      0.35644689716329236,
      0.3819339671724833,
      0.3861992416530806,
      0.37901068423599527,
      0.39633166524692776,
      0.34790319208442044,
      0.34346416221102394,
      0.38509295527303444,
      0.38839418873297493,
      0.4210353295561351,
      0.335926695849849,
      0.3576998966596099,
      0.3078480644947894
    ],
    [
      0.34765658790175014,
      0.4886757098684993,
      0.47478303882131523,
      0.49973536544959885,
      0.4231646435142076,
      0.4353744293179673,
      0.38843658037167716,
      0.4484632608450867,
      0.5563204737950356,
      0.4563798175486715,
      0.0,
      0.27857795029182797,
      0.48416716738986265,
      0.4827762447811754,
      0.4747441702237787,
      0.4325661885590846,
      0.4403404743785364,
      0.4457752470373939,
      0.47024431878612893,
      0.3663965627906547,
      0.4796853111182766,
      0.3534263008900349,
      0.4375966395684441,
      0.5045595853254243,
      0.41867550464799863,
      0.4444948982207497,
      0.3930422326013576,
      0.4546225322230091,
      0.394274157414636
    ],
    [
      0.20756473485612914,
      0.2210309838492519,
      0.21847197870081092,
      0.2377346072559292,
      0.2495957302200904,
      0.23867012799673315,
      0.26821989930098256,
      0.2550332444940979,
      0.2207626313677924,
      0.2241192942495882,
      0.21991986194790059,
      0.0,
      0.2037160616958733,
      0.23078920840498718,
      0.23608026027724338,
      0.21526717299922704,
      0.23728307333279064,
      0.22120142476740345,
      0.22530993639232832,
      0.23505272419011858,
      0.2571105848128412,
      0.29418299798945546,
      0.23851062901779296,
      0.22380590879583506,
      0.2147522487929694,
      0.23346916101466242,
      0.21856958281514283,
      0.2333012035936528,
      0.2751351671940283
    ],
    [
      0.4464800573647074,
      0.5786574293935771,
      0.5426513657582894,
      0.4613528483906335,
      0.48383714605611683,
      0.532334141929037,
      0.466901427485251,
      0.5652986604784631,
      0.4758695353747793,
      0.44145165173680834,
      0.5377706381072058,
      0.2938821509694345,
      0.0,
      0.5036498094616464,
      0.5767891240585663,
      0.5298612233122864,
      0.4482467580030156,
      0.543479815622993,
      0.49539999470738927,
      0.465393969087059,
      0.5126354480216773,
      0.34301246070198865,
      0.44991939276868353,
      0.5721602280490654,
      0.4401161964960465,
      0.506919685360862,
      0.40443511087141215,
      0.4746884006073586,
      0.38482299076094484
    ],
    [
      0.3837498271120696,
      0.5294092827400758,
      0.5208881586364336,
      0.49701101556182525,
      0.4661814557316357,
      0.5240661405834643,
      0.41870786130960913,
      0.530922465538987,
      0.5017613314364457,
      0.4851305126407566,
      0.48069565269250947,
      0.3033276637830298,
      0.49106841780341126,
      0.0,
      0.6236258963882759,
      0.5502441594712979,
      0.44785859635617387,
      0.4747883424444339,
      0.4833081331801927,
      0.49896900046594905,
      0.4921740016149916,
      0.38907774339068313,
      0.4328354485719017,
      0.4882623673648352,
      0.42087988142732824,
      0.4688258308130251,
      0.42495235453346436,
      0.46600228656892395,
      0.3754657662171479
    ],
    [
      0.38290434780353966,
      0.5077764118069303,
      0.5329179737714862,
      0.4800804973550219,
      0.399225187845512,
      0.5012915201440427,
      0.38542407394729294,
      0.49219636355520957,
      0.48285509442575436,
      0.43324998421990757,
      0.4583262721717012,
      0.27488728976827637,
      0.4796321891180133,
      0.5648814194850906,
      0.0,
      0.5072415957975123,
      0.3998112390618611,
      0.45581207958737857,
      0.4443371614417093,
      0.47509644781556304,
      0.4677969024733801,
      0.358788850204357,
      0.3572148656684797,
      0.45056554152839423,
      0.37327527972187546,
      0.46138471789027924,
      0.3745900790900514,
      0.44081254513412826,
      0.3404087449327704
    ],
    [
      0.42765290940167433,
      0.5224908129013801,
      0.5139272181650114,
      0.48073618838971055,
      0.4144707487722288,
      0.5291513833175441,
      0.421191467967996,
      0.5074822334104938,
      0.45581898331890924,
      0.510004503714323,
      0.49120226648088616,
      0.27329421885687744,
      0.5274802482583683,
      0.542897427808658,
      0.5671065295844002,
      0.0,
      0.4876441018434623,
      0.4657732508863033,
      0.5550020750562383,
      0.48225587527319225,
      0.5156976089769758,
      0.4118081674116112,
      0.45268904261897225,
      0.5186578126787147,
      0.4637904367707726,
      0.5107724348324671,
      0.43314111442573955,
      0.4599273407203128,
      0.3950710654013767
    ],
    [
      0.3346443637947154,
      0.4310577079163085,
      0.46508213893437333,
      0.3782550588301199,
      0.39994678007886786,
      0.4718107234613831,
      0.3625064288104485,
      0.486679788523364,
      0.41498156305093614,
      0.39199764889502364,
      0.4697202086446701,
      0.27205402697895553,
      0.4120302336337631,
      0.44884903842263824,
      0.4573437552283779,
      0.41235596308567035,
      0.0,
      0.43531192823832887,
      0.43143078958641756,
      0.38500357018277054,
      0.43700580663002575,
      0.3088031044081494,
      0.3934647913103759,
      0.44261451050416234,
      0.4198116235372267,
      0.442021753963479,
      0.45362685031231753,
      0.42529771247273085,
      0.3435154095220445
    ],
    [
      0.4090056064009091,
      0.5542329088047933,
      0.48158819408605114,
      0.42963381409903323,
      0.4607374435370748,
      0.4747700965164545,
      0.43189160077173905,
      0.5054977501586726,
      0.46226993439669006,
      0.3882357593666621,
      0.43838450271201634,
      0.27702569754827766,
      0.5513671484024378,
      0.49005450495816816,
      0.5300204209534611,
      0.4874684529101465,
      0.48074854063480377,
      0.0,
      0.46999381117106975,
      0.4421998424288218,
      0.4655346325825318,
      0.3602292362967232,
      0.38782448093256017,
      0.4920077919580832,
      0.404050669869227,
      0.45608706407847066,
      0.4167649992133522,
      0.4023822065515641,
      0.35388825923581146
    ],
    [
      0.3822718259757021,
      0.4584007209833243,
      0.46576761655483967,
      0.417471611686304,
      0.38296252780612283,
      0.4048992221440566,
      0.39365434446328607,
      0.4679644791645716,
      0.41151993373979434,
      0.43607180754545594,
      0.41346563257626023,
      0.24819466074188878,
      0.43471915261849636,
      0.4338440533861625,
      0.44126952826522037,
      0.41498603461361383,
      0.407934160387053,
      0.4618361567705771,
      0.0,
      0.41751624201648974,
      0.4578260604824673,
      0.2966007290619823,
      0.3618855364574851,
      0.4687775244804866,
      0.3881499800311554,
      0.3952478648457389,
      0.3842260354051106,
      0.4048534190133386,
      0.3421938551271704
    ],
    [
      0.3479815999710614,
      0.4594615947901073,
      0.4597685418771884,
      0.46648646906070024,
      0.389720827384163,
      0.4771964459381357,
      0.3713986924018846,
      0.47966664665409176,
      0.4471589557917177,
      0.4446272287427744,
      0.42987113566340773,
      0.2642611294655821,
      0.4533382523172724,
      0.49000648489741305,
      0.51888617907408,
      0.45601047243603077,
      0.4074477563151402,
      0.43512631157579196,
      0.42806804928741693,
      0.0,
      0.48185923868301894,
      0.35128322491586483,
      0.3904290418965686,
      0.42164655968814135,
      0.41821394133307144,
      0.4068334986593434,
      0.4117599975041222,
      0.43992165041407505,
      0.3756106030558499
    ],
    [
      0.34764955310486356,
      0.4735823911562351,
      0.43703947060485615,
      0.42399110891122627,
      0.4149641536847246,
      0.4215218103889029,
      0.33416512569945533,
      0.48982123216901696,
      0.43431699421959546,
      0.39246578512827757,
      0.44457703095392254,
      0.2132575363349074,
      0.43863476232167287,
      0.434523821066765,
      0.442976297249571,
      0.423101696510334,
      0.3828478195005922,
      0.43248616153728947,
      0.4510034872791395,
      0.4655046017002291,
      0.0,
      0.2662261047618877,
      0.3174952524928565,
      0.44461078443685476,
      0.3661146861915112,
      0.39205293722184775,
      0.39506823786680356,
      0.5235982333235278,
      0.3032187871701668
    ],
    [
      0.24309665680675097,
      0.31690453063617596,
      0.2758942869194192,
      0.33951989040080766,
      0.32208604827265663,
      0.3426060214559894,
      0.32794115042957483,
      0.28450502278737555,
      0.3095306049168798,
      0.3613218429289826,
      0.30463089725307335,
      0.3039826331661841,
      0.28234774695570763,
      0.36047554049412933,
      0.3703086657553003,
      0.3251443347739118,
      0.2728675724712806,
      0.3039525736566886,
      0.267487706054814,
      0.3100052730455902,
      0.30015427989777277,
      0.0,
      0.28935960778949665,
      0.27996243781430175,
      0.2820782254938552,
      0.35344821088682177,
      0.2914568272919982,
      0.34298264155936375,
      0.31544992144008677
    ],
    [
      0.33044402852350796,
      0.37965559866151044,
      0.3783717649612035,
      0.372939891721876,
      0.3649985410502592,
      0.42774475645088383,
      0.3715785740581443,
      0.4041942490361756,
      0.3855164343820805,
      0.4385252762298366,
      0.4165989706220796,
      0.3194793477138229,
      0.40561026924520527,
      0.42983092473351237,
      0.4036321408898771,
      0.37070719866542134,
      0.4411065761740691,
      0.41462491143093216,
      0.39949347152971915,
      0.4202803095367682,
      0.40423097636571503,
      0.4020733414998445,
      0.0,
      0.39307346174018143,
      0.440784560728229,
      0.4135343226206343,
      0.42745173841044193,
      0.40397056159465294,
      0.38258641657876935
    ],
    [
      0.3517887583550865,
      0.5294239172719315,
      0.4881976167411872,
      0.44714098723226137,
      0.4450090317257811,
      0.4711873634618673,
      0.3753612682874665,
      0.4655788008374542,
      0.470647665732969,
      0.45421538196676137,
      0.4493664440606684,
      0.2605810398550821,
      0.49182250546081585,
      0.46599435892952945,
      0.4430972143948573,
      0.4749076821811742,
      0.4289560408170139,
      0.423526591382992,
      0.47218366257158273,
      0.3952972171815057,
      0.48395512776219674,
      0.3315828535079999,
      0.36798086043420697,
      0.0,
      0.4150124470279233,
      0.43781133816522066,
      0.37318250139587583,
      0.45224912132386197,
      0.4038968499377036
    ],
    [
      0.3581702185042881,
      0.41353415229255575,
      0.4558345554296528,
      0.38506439887883936,
      0.40642391248673304,
      0.45202023110400624,
      0.38288315895672187,
      0.4398081088624124,
      0.40545634778116746,
      0.43511682550746955,
      0.4127533549249205,
      0.28438536844431295,
      0.4632241131720596,
      0.41173711106354505,
      0.44364506771519263,
      0.41023041766351276,
      0.48671185785176463,
      0.42893154756639906,
      0.4255306687157403,
      0.41569787081126,
      0.45286754074288016,
      0.35213375839199546,
      0.4296172562600111,
      0.42569540939062867,
      0.0,
      0.45361742924631554,
      0.43529128272499573,
      0.4361517517449587,
      0.34858314443867666
    ],
    [
      0.304898640820362,
      0.4260370822961601,
      0.4194107365528008,
      0.3885525490738706,
      0.37979320566978947,
      0.47326758602384467,
      0.3264456979001755,
      0.4473159581992219,
      0.3804912291963696,
      0.435431064294739,
      0.3871808389650475,
      0.2512493226656076,
      0.40560196572414076,
      0.3922585006113455,
      0.47114462518936007,
      0.4355001340029532,
      0.3890186484892926,
      0.3847061395642577,
      0.3643139075182824,
      0.37324479974051594,
      0.41086364480556803,
      0.3200357177331332,
      0.33540027415167506,
      0.4268913085009409,
      0.3763015434471495,
      0.0,
      0.3623924237724885,
      0.3628760154837163,
      0.3158685748707455
    ],
    [
      0.3286306027200723,
      0.3894596566575381,
      0.41401171545743365,
      0.3930505176032322,
      0.4182699458125778,
      0.4029633660495828,
      0.3477765091331033,
      0.38340546553155397,
      0.38619353638509124,
      0.44206968658544543,
      0.3857202438956968,
      0.22718823459982285,
      0.3643073357766531,
      0.3931708566188734,
      0.38628519212325196,
      0.3751617949308841,
      0.4597883015637023,
      0.39858487293326283,
      0.39960806240461166,
      0.3680024837579656,
      0.37276876089640876,
      0.29914011192915857,
      0.38357240067199716,
      0.38415671276990193,
      0.38277312994748236,
      0.4128997510585213,
      0.0,
      0.40464722559907784,
      0.3388976068649432
    ],
    [
      0.32050261105700883,
      0.40210420139963654,
      0.42844422002061244,
      0.3810754127593663,
      0.36681751578141175,
      0.3912206979298638,
      0.3512363801066496,
      0.4227218160546853,
      0.41201315112082004,
      0.38673144843667484,
      0.41071173262556715,
      0.24121369711362606,
      0.396040373960316,
      0.38615580835129903,
      0.3937325045302218,
      0.37321358569044594,
      0.37406120549233823,
      0.334285423652565,
      0.3865179130862557,
      0.4008193794006958,
      0.5027430226714518,
      0.28769705182907535,
      0.35177305394840164,
      0.4374740371280783,
      0.381071936849237,
      0.37615385182644534,
      0.4018025574733257,
      0.0,
      0.34708140828756795
    ],
    [
      0.3237373478562835,
      0.42998193756609826,
      0.41408020191999517,
      0.39314036509315264,
      0.39730112014849306,
      0.4006180689253245,
      0.37411982701868074,
      0.41340447702894845,
      0.4381150815195365,
      0.40364501034732503,
      0.4251897906534696,
      0.35007252252305854,
      0.39493573131018467,
      0.4017459540063293,
      0.40312568964965423,
      0.3895802315027952,
      0.4338290518604677,
      0.40382582642412834,
      0.4048272213529054,
      0.40321293801257596,
      0.4163847834052725,
      0.4707620983603156,
      0.4214646640467796,
      0.42553320212326295,
      0.4232970450072191,
      0.40925863947629804,
      0.3967577978094945,
      0.46020805634753903,
      0.0
    ]
  ],
  "row_avgs": [
    0.4282872608707607,
    0.46330473664813304,
    0.444710503305418,
    0.4265849293784187,
    0.3835905246445624,
    0.3931332727170848,
    0.4225037240912206,
    0.41001675840194923,
    0.45658620301098407,
    0.36314565085212835,
    0.438391264060078,
    0.23409501572591637,
    0.4813577736048321,
    0.47036391408496,
    0.43867088127734,
    0.47632633811587854,
    0.4116865456770587,
    0.4465676918062717,
    0.40694681129800553,
    0.4258585903497863,
    0.4038148522495369,
    0.3099821839769639,
    0.3979656648269769,
    0.4310698088572491,
    0.41611131645260774,
    0.3838032905451269,
    0.38008943143849455,
    0.3801934285208445,
    0.40793409576055667
  ],
  "col_avgs": [
    0.3499454406803422,
    0.4537251819027294,
    0.4445600849467005,
    0.4253594114853471,
    0.4065864145850518,
    0.4474955020039613,
    0.38217902375577456,
    0.4506224643576828,
    0.42240272889018515,
    0.42109057626795726,
    0.4297614146593994,
    0.27382071360732113,
    0.43741906559162264,
    0.44463688497669634,
    0.46558455778597685,
    0.4250591720758991,
    0.4178817847833562,
    0.42382121701681763,
    0.4299618974508473,
    0.4134630274976669,
    0.44146471851686214,
    0.3417116445372687,
    0.3741419456033878,
    0.4376189267753395,
    0.39101763449513655,
    0.42273080165873267,
    0.39204845085819867,
    0.420315512071865,
    0.346666263711018
  ],
  "combined_avgs": [
    0.38911635077555146,
    0.45851495927543123,
    0.4446352941260593,
    0.4259721704318829,
    0.3950884696148071,
    0.42031438736052307,
    0.40234137392349756,
    0.430319611379816,
    0.4394944659505846,
    0.3921181135600428,
    0.43407633935973866,
    0.25395786466661874,
    0.4593884195982274,
    0.45750039953082816,
    0.4521277195316584,
    0.4506927550958888,
    0.41478416523020745,
    0.4351944544115447,
    0.4184543543744264,
    0.4196608089237266,
    0.4226397853831995,
    0.3258469142571163,
    0.38605380521518234,
    0.43434436781629426,
    0.40356447547387214,
    0.40326704610192976,
    0.3860689411483466,
    0.40025447029635475,
    0.37730017973578733
  ],
  "gppm": [
    584.73917762113,
    545.8628230653724,
    547.7751412791704,
    555.3442781350692,
    560.4876000627448,
    545.5094389586005,
    573.098987058402,
    540.7223752895904,
    557.2110686947193,
    556.4685582931396,
    555.8185048551836,
    618.60187552301,
    550.2002336871011,
    547.9129690273322,
    538.8418523244926,
    555.637880123459,
    556.0382508035003,
    556.8971961025147,
    554.1328708412063,
    560.091023717794,
    542.7796946752962,
    591.9739339383503,
    577.6445157764351,
    550.470037319982,
    570.4547400714731,
    554.0388974130573,
    570.393609398435,
    555.1547327798909,
    592.3813388132914
  ],
  "gppm_normalized": [
    1.3674789160134004,
    1.24757485061356,
    1.248903182715674,
    1.273626071955349,
    1.2737600193969902,
    1.2427106984127883,
    1.3170015984315675,
    1.230925558857179,
    1.2668185266732606,
    1.2613183808696338,
    1.2580323361087882,
    1.4208899608073333,
    1.2516011591774163,
    1.246428904512239,
    1.228323178490942,
    1.272477728385254,
    1.2604451683153286,
    1.2725201758829858,
    1.2589693964108206,
    1.2723118315316022,
    1.232220260971048,
    1.3487597144848025,
    1.3119989546998603,
    1.2480379209101053,
    1.2914127561035398,
    1.2674744187480995,
    1.2918147242499298,
    1.2606840388184988,
    1.34211619548846
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401,
    668,
    451,
    488,
    442,
    477,
    399,
    415,
    391,
    477,
    467,
    410,
    552,
    421,
    402,
    423,
    411,
    434,
    392,
    381,
    432,
    411,
    388,
    434,
    406,
    404,
    403,
    408,
    482,
    364,
    515,
    394,
    445,
    456,
    590,
    399,
    441,
    475,
    430,
    427,
    416,
    502,
    397,
    443,
    381,
    435,
    391,
    431,
    382,
    457,
    431,
    379,
    342,
    407,
    378,
    371,
    372,
    478,
    412,
    676,
    441,
    444,
    451,
    462,
    493,
    380,
    407,
    445,
    468,
    464,
    604,
    461,
    488,
    449,
    498,
    445,
    422,
    409,
    421,
    392,
    428,
    402,
    455,
    441,
    429,
    408,
    494,
    372,
    307,
    405,
    426,
    444,
    444,
    447,
    430,
    442,
    415,
    412,
    410,
    547,
    406,
    431,
    431,
    401,
    422,
    414,
    409,
    427,
    431,
    407,
    386,
    438,
    364,
    436,
    366,
    452,
    371
  ],
  "response_lengths": [
    1498,
    2307,
    2522,
    2599,
    2576,
    2704,
    2572,
    2521,
    2472,
    2393,
    2375,
    3023,
    2340,
    2427,
    2494,
    2378,
    2454,
    2380,
    2339,
    2465,
    2524,
    2384,
    2265,
    2495,
    2127,
    2411,
    2130,
    2585,
    2144
  ]
}