{
  "example_idx": 49,
  "reference": "Published as a conference paper at ICLR 2023\n\nREVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH\n\nDuc Hoang, Shiwei Liu, Radu Marculescu & Zhangyang Wang Department of Electrical and Computer Engineering University of Texas at Austin, Austin, TX 78712, USA {hoangd,radum,atlaswang}@utexas.edu shiwei.liu@austin.utexas.edu\n\nABSTRACT\n\nPruning neural networks at initialization (PaI) has received an upsurge of interest due to its end-to-end saving potential. PaI is able to find sparse subnetworks at initialization that can achieve comparable performance to the full networks. These methods can surpass the trivial baseline of random pruning but suffer from a significant performance gap compared to post-training pruning. Previous approaches firmly rely on weights, gradients, and sanity checks as primary signals when conducting PaI analysis. To better understand the underlying mechanism of PaI, we propose to interpret it through the lens of the Ramanujan Graph - a class of expander graphs that are sparse while being highly connected. It is often believed there should be a strong correlation between the Ramanujan graph and PaI since both are about finding sparse and well-connected neural networks. However, the finer-grained link relating highly sparse and connected networks to their relative performance (i.e., ranking of difference sparse structures at the same specific global sparsity) is still missing. We observe that not only the Ramanujan property for sparse networks shows no significant relationship to PaI’s relative performance, but maximizing it can also lead to the formation of pseudo-random graphs with no structural meanings. We reveal the underlying cause to be Ramanujan Graph’s strong assumption on the upper bound of the largest nontrivial eigenvalue (ˆμ) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax the ˆμ upper bound. Likewise, we also show there exists a lower bound for ˆμ, which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected structure degenerates into naive randomness. Finally, we systematically analyze the behavior of various PaI methods and demonstrate the utility of our proposed metrics in characterizing PaI performance. We show that subnetworks preserving better the IMDB property correlate higher in performance, while NaRC provides us with a possible mean to locate the region where highly connected, highly sparse, and non-trivial Ramanujan expanders exist. Our code is available at: https://github.com/VITA-Group/ramanujan-on-pai.\n\n1\n\nINTRODUCTION\n\nDeep neural networks (DNN) have demonstrated remarkable performance as they increase in size, i.e, test accuracy scales as a power law regarding model size and training data size (Hestness et al., 2017; Kaplan et al., 2020; Brown et al., 2020; Srivastava et al., 2022). Yet, the memory requirements and computational costs associated with the increased model size also grow prohibitively. Modern DNNs are widely recognized to be over-parameterized, and it has been shown that eliminating a significant number of parameters in a trained DNN does not compromise its performance (Han et al., 2015c; He et al., 2017). This over-parameterization property enables researchers to continually propose increasingly effective DNN pruning approaches that can dramatically shrink the model size while maintaining performance. The resultant sparse models can then be used with software and hardware that is optimized for sparsity, leading to faster training and inference.\n\nNeural network pruning can generally be divided into three categories: post-training pruning (Mozer & Smolensky, 1989; Han et al., 2015a), during-training pruning (Gale et al., 2019; Louizos et al., 2018), and pre-training pruning (Lee et al., 2019; Wang et al., 2020), depending on the timing of\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthe pruning relative to the training phase. For instance, post-training pruning methods are generally effective when the primary goal is to reduce inference cost. However, these methods require training the dense model fully first, potentially multiple times if iterative pruning and retraining are used. With the prevalence of powerful foundation models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and DALL·E 2 (Ramesh et al., 2022), the prohibitively high cost of training these large models makes post-training pruning impractical. Therefore, pre-training pruning or pruning at initialization (PaI) is becoming increasingly attractive due to its potential to save time and resources end-to-end, by using a sparse DNN architecture from the outset.\n\nThe concept of pruning at initialization (PaI) was first introduced in SNIP (Lee et al., 2019), which removes the structurally unimportant connections at initialization via the proposed connection sensitivity. Follow-up works (Wang et al., 2020; Tanaka et al., 2020; Patil & Dovrolis, 2021) propose advanced pruning criteria to improve the performance of PaI. GraSP (Wang et al., 2020) aims to maintain the weights that can maximize the gradient flow. SynFlow (Tanaka et al., 2020) finds that previous PaI methods are prone to layer collapse and adopt iterative pruning to address it. Despite these advances, PaI still lags behind post-training pruning in terms of performance. A study by Frankle et al. (2021) suggests that connection ambiguity may explain the performance deficit, based on the finding that PaI exhibits surprising resilience against layer-wise random mask shuffling and weight re-initialization.\n\nPrior works on PaI mainly focus on “training signals” such as gradient flow (Wang et al., 2020), layer collapse (Tanaka et al., 2020), or sanity check (random weight shuffling and re-initialization) (Frankle et al., 2021). Since the limited information (e.g., magnitude, gradient, and Hessian) that PaIs have access to can be very noisy (Frankle et al., 2020), pruning criteria based on such information are often ineffective. We conjecture that the graph topology of sparse neural networks, being relatively overlooked, can be an essential source of information for pruning at initialization. Graph theory has recently emerged as a particularly advantageous tool for analyzing DNN architectures. For example, You et al. (2020) offers a new efficient graph representation of DNN using relation graphs to formulate an efficient model generator. Liu et al. (2020) analyzes sparse neural networks with graph distance and shows a plenitude of sparse sub-networks with very different topologies while achieving similar performance. Vooturi et al. (2020); Pal et al. (2022); Bhardwaj et al. (2021); Prabhu et al. (2018) show that maximizing good graph connectivity, i.e. by maximizing a graph’s expansion ratio, correlates to higher performance in hardware-efficient structured masks and lottery tickets (Frankle & Carbin, 2019). Unfortunately, prior efforts did not consider the pseudo-randomness that naturally emerges from very good expander graphs. Therefore by maximizing sparse graph connectivity, they are unwittingly prioritizing the formation of naive random graphs with no intrinsic structure meaning.\n\nIn this paper, we study PaI from the perspective of the Ramanujan bipartite graphs. The Ramanujan graph is a special graph in the bounded degree expander family, where the eigenbound is maximal (Nilli, 1991), thus leading to a maximum possible sparsity of a network while preserving the connectivity. The Ramanujan graph is intuitively well aligned with the main goal of PaI, i.e., finding sparse and well-connected neural networks. However, we find that there is still a missing link correlating the degree of connectivity to relative performance ranking at a particular sparsity. In addition, we also show that in situations where highly sparse and highly connected structures are demanded, it can be easy to generate pseudo-random graphs with no structural meanings unwittingly.\n\nWe reveal the underlying cause for such undesirable situations to be Ramanujan Graph’s strong assumption on the upper bound of the largest nontrivial eigenvalue (ˆμ) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax ˆμ upper bound. Likewise, we also show there exists a lower bound for ˆμ, which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected structure deteriorates into randomness. Leveraging our (adjusted) Ramanujan graph-based framework, we then extensively investigate (1) whether the generated sparse structures by existing PaI approaches follow the Ramanujan characteristics, (2) if there exists a correlation between the Ramanujan graph property and the inference performance of the sparse structure, and (3) if the sparse structure is also a random graph. Ultimately, we aim to shed light on a new perspective on PAI effectiveness independent of weights, gradients, and losses.\n\nOur contributions are summarized as follows:\n\n• We are the first to reveal that the utility of the Ramanujan property is largely limited in analyzing irregular graphs at high sparsity, which is often the case in analyzing PaI-generated\n\n2\n\nPublished as a conference paper at ICLR 2023\n\narchitectures. We identify the root cause to be its strong assumption on the upper bound of the largest nontrivial eigenvalue ˆμ of the adjacency matrix and propose Iterative Mean Difference of Bound (IMDB) as an effective fix.\n\n• We devise another novel metric to assess whether a Ramanujan graph is also random, which becomes increasingly likely with higher sparsity. We prove the existence of this metric by inferring from the definition of the expander mixing lemma. Our NormAlized Random Coefficient (NaRC) assesses the randomness of graphs, by characterizing the lower bound of ˆμ for which we can no longer distinguish expanders from randomly generated graphs.\n\n• Our analysis shows that IMDB correlates strongly with the relative performance for different PaI’s sparse masks at high sparsity. It further illustrates how NaRC can spot random structures, and provide interesting observations on the relationship between randomness and expansion.\n\n2 RELATED WORK\n\nPruning methods (Mozer & Smolensky, 1989; LeCun et al., 1989; Hassibi et al., 1993; Molchanov et al., 2016; Han et al., 2015b) traditionally aim to remove the unnecessary components of DNNs, resulting in a subnetwork that can be efficiently deployed at inference. As the sizes of the modern DNNs have exploded, a vast amount of attention has been shifted to pruning them before training, targeting both training and inference efficiency. Lee et al. (2019) explicitly learn a connectivity importance score for weights and eliminate weights with the lowest scores. Wang et al. (2020) leverage the Hessian-gradient product to discover the importance of weight to the gradient flow. Iterative pruning approaches (Tanaka et al., 2020; de Jorge et al., 2021) show their efficacy to prevent layer collapse, ending up with pruned networks with very small width (Patil & Dovrolis, 2021). Although existing PaI methods surpass the naive baseline of random pruning, they are only able to identify useful layerwise sparsity levels rather than the specific weight patterns (Frankle et al., 2021; Su et al., 2020). As PaI only accesses very limited and noisy information (e.g., magnitude, gradient, and Hessian) from initialization, pruning criteria based on such information may not be effective.\n\nOn the other hand, the topology of sparse DNNs - the configuration of nodes and connections among them - can be another essential source of information. Mocanu et al. (2018) initialize sparse networks with a Erd ̋os-R ́enyi graph and dynamically optimize the graph towards a scale-free network. Evci et al. (2020b) further expand the Erd ̋os-R ́enyi graph to convolution neural networks, demonstrating large performance improvements. Liu et al. (2020) analyze sparse DNN with graph edit distance and show that there exists plenty of sparse sub-networks with distinct topologies that perform equally well. You et al. (2020) study the relationship between the graph structure and the neural networks from the relation graphs point of view. Vooturi et al. (2020); Pal et al. (2022); Bhardwaj et al. (2021) show that maximizing the graph connectivity of sparse networks correlates to higher performance in the structured masking and lottery tickets (Frankle & Carbin, 2019).\n\nRecently, Ramanujan graphs have been linked to sparse structures (Pal et al., 2022; Vooturi et al., 2020) as naturally appealing criteria to produce sparse yet well-connected DNN models. We draw inspiration from two prior works: Vooturi et al. (2020) applies the Ramanujan bipartite graph products for efficient, structured sparsity. In short, they achieve run-time efficiency by decomposing a dense matrix into tiled multiplication and utilizing the Ramanujan principle to maximize connectivity between the decomposed tiled matrices. The more recent work by Pal et al. (2022) evaluates whether a lottery ticket’s masks exhibit Ramanujan property. If they do not, then sparsity will repeatedly be halved until the generated masks are all Ramanujan graphs.\n\nBesides the obvious difference that we uniquely work on pruning at initialization with end-to-end sparsity (no dense pre-training is involved), our work also differs from these prior arts in our tackling of irregular bi-graphs that arise from practical sparse DNNs: we are the first to identify a crucial limitation of Ramanujan property in analyzing irregular graphs under high sparsity, which seems to be overlooked by prior arts. We also highlight the danger of being overly expansive, which risks graphs deteriorating into randomness. We then propose ways to relax the upper constraint to mitigate the critical gap between theory and practice and a lower constraint to avoid the danger of randomness, which leads to a more rigorous analysis of sparse DNNs at initialization.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 METHODOLOGY\n\nBefore we start, we want to explain some nomenclature that will be used excessively in our definitions. First, a “regular” graph refers to a graph where all its vertices have the same number of in/out edges; We refer to the number of edges as d. Analogously, when we talk about regular graphs, we refer to any dense DNN, such as Linear or Convolutional layers once. An ”irregular” graph, on the other hand, refers to a graph with mixed number of edges for every vertex. Likewise, when speaking of irregular graphs, we refer to the resulting unstructured pruning of DNN layers with PaI. Later, we will explain how the DNN’s layers are represented as graphs.\n\n3.1 PRELIMINARY: BIPARTITE EXPANDER GRAPHS\n\nIn this work, we focus mainly on the bipartite case of expander graphs. However, before giving a formal definition, we state the following intuition: expander graphs are graphs where every subset of vertices is not ”too large” and has ”many connections” to other vertices that do not belong to the same subset. Typically, these graphs are regular; however, we shall extend the definition to consider finite, connected, and irregular graphs for our DNN analysis purpose.\n\nDefinition 1. A bipartite graph or bi-graph G = (L ∪ R, E) is a graph consisting of two disjoint sets of vertices L and R such that every edge from E connects one vertex of L and one vertex of R.\n\nMany definitions will make use of G = (V, E), where V = L ∪ R instead. For most cases, it is trivial to extend them to bipartite graphs. However, in case of extension ambiguity, we will clarify these definitions specifically for bi-graphs.\n\nDefinition 2. Let S ⊆ V for G = (V, E); we denote N (S) = {v ∈ V |∃u ∈ S, (u, v) ∈ E} to be the neighborhood set consisting of all adjacent vertices not in |S|.\n\nSince we are working with bipartite graphs, S ⊆ L and N (S) ⊆ R, and there are no edges between any two vertices in S.\n\nDefinition 3. A (n, m, d, γ, α)-expander is a d-left-regular1 bipartite graph G = (L ∪ R, E), where |L| = n, |R| = m (m ≤ n) and ∀S ⊆ L s.t |S| ≤ γ · n the neighborhood set of S satisfies |N (S)| ≥ α · |S|. Here γ ∈ {0, 1} and α ∈ [0, d].\n\nThe α and γ parameters control the expansion ratio of the expander and are dependent on one another. For example, by letting α = d the regularity of L, then γ ≤ 1 d , then we violate the neighborhood constraints namely |N (S)| ≥ α · |S| > n > m. The expansion ratio, |N (S)| , is related to the Cheeger constant h(G), whereby a small ratio signifies information bottleneck and a large h(G) indicates the graph is strongly connected. A good bipartite expander graph should ensure a large Cheeger constant so that information can flow freely.\n\nd since if |S| ≥ n\n\nS\n\nLet us denote A ∈ R|n+m|×|n+m| as the adjacency matrix of some d-regular bipartite graph G = (L ∪ R, E) with eigenvalues μ(G) s.t μ0 ≥ ... ≥ μv−1, where d = μ0 = |μv−1|, and corresponding eigenvectors φ. We define ˆμ(G) = max|μi|̸=d |μi| to be the largest nontrivial eigenvalue. Due to the nature of bipartite graphs where |μ0| = |μ1| = d, we will often refer to ˆμ(G) as our third-largest eigenvalue. The expansion property of any G is represented by its spectral gap μ0 − ˆμ(G) (see Hoory & Linial (2006)). A large gap indicates G is more ”spread out” which is the hallmark trait of the expander graph. Since μ0 − ˆμ ≥ 0, we need to determine a threshold on the spectral gap for which G can be considered as a good expander. This brings us to the notion of Ramanujan graph property.\n\nDefinition 4. A d-regular graph is said to be a Ramanujan graph if ˆμ(G) ≤ 2 d − 1, where d is graph regularity. Alternatively, following the previous discussion, Ramanujan graph can also be expressed as ˆμ(G) ≤ 2 ∗\n\nμ0 − 1, since μ0 = d.\n\n√\n\n√\n\nThus, all Ramanujan graphs are good expanders due to the convenient upper bound of ˆμ(G). However, after PaI not all graphs are regular. Therefore, we need to generalize Definition 4 for all cases. To do so, we combine two inequalities: The first inequality states that the universal cover graph ̃G of G satisfies p( ̃G) ≥ 2 ∗ (cid:112)davg − 1, where p( ̃G) denotes the spectral radius of ̃G and davg represents the average degrees of G (for details see Hoory (2005)). The second inequality, following the results\n\n1d-left-regular means all vertices in L have d number of edges.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nof Hoory & Linial (2006), defines a graph to be Ramanujan iff ˆμ(G) ≤ p( ̃G). We relate these two inequalities to form ˆμ(G) ≤ 2 ∗ (cid:112)davg − 1 ≤ p( ̃G).\n\nAs a direct result, we can estimate any graph’s expansion property as the difference between ˆμ(G) and its estimated Ramanujan’s upper bound. Definition 5. We denote the difference of bound as: ∆r = 2∗(cid:112)davg − 1− ˆμ(G) . A value ∆r < 0.0 indicates a violation of the Ramanujan property and therefore the graph may not be a good expander.\n\ndL − 1 − ˆμ(G), where dR In the case of bi-graphs, we rewrite Definition 5 as ∆r = and dL are the average degree of R and L respectively. Additionally, because ˆμ(G) is the third largest eigenvalues by magnitude, a Ramanujan graph is only defined when min(dL, dR) ≥ 3.\n\ndR − 1 +\n\n√\n\n√\n\n3.2\n\nITERATIVE MEAN DIFFERENCE OF BOUND\n\nIn this subsection, we focus on discussing the value μ0 of irregular graphs, something we glossed over in the discussion of the previous subsection and it is also largely overlooked by prior works. Primarily, we want to address cases where μ0 ̸= d and assess the effect it can have on determining if a graph G satisfies the Ramanujan property. Here, we explicitly define the general range of μ0 on all graphs, explain its negative effects on our analysis of high, and propose a way to mitigate the effects.\n\nLemma 1. The value of μ0 for any adjacency matrix A is said to be davg ≤ μ0 ≤ dmax. If G is a connected graph, μ0 = dmax therefore G is dmax-regular. We first prove davg ≤ μ0 by using the Rayleigh quotient. We have:\n\n1T A1 1T 1 To prove μ0 ≤ dmax, let φ0 be the corresponding eigenvector of μ0 and say that v = arg maxu φ(u). Without loss of generality, we have φ0(v) ̸= 0. So, we show: (u,v)∈E φ0(u)\n\nμ0 = max h∈Rn\n\nv∈V dv n\n\nhT Ah hT h\n\n= davg\n\n(1)\n\n(cid:80)\n\n≥\n\n=\n\n(cid:88)\n\n(cid:80)\n\n≤\n\n1 = dv ≤ dmax.\n\n(2)\n\nμ0 =\n\n(μ0φ0)(v) φ0(v)\n\n=\n\n(Aφ0)(v) φ0(v)\n\n=\n\nφ0(v)\n\n(v,u)∈E\n\nFinally, we show that if μ0 = dmax, then (v,u)∈E 1 = dv = dmax. In this way, φ0(v) φ0 is a constant vector with value dv for all vertices u that are connected to v. By repeating the way v was chosen then applied to u for all such vertices u, we yield the result that G is dmax-regular.\n\n(cid:80)\n\n(u,v)∈E φ0(u)\n\n= (cid:80)\n\n∆r’s limitation for irregular graphs at high sparsity: Now that we have proven the range of μ0, we can extend it to the following inequality: ˆμ(G) ≤ 2 ∗ (cid:112)davg − 1 ≤ 2 ∗ μ0 − 1. This inequality shows that Ramanujan’s upper-bound estimation for ˆμ(G) is conservatively small.\n\n√\n\nBecause of our interest in analyzing high sparsity (which is often the pursuit of PaI methods), dmax − davg is possibly very large. By following ∆r, we only qualify those graphs with tiny ˆμ(G) as Ramanujan graphs. We refer to the expander mixing lemma (see detail in Sauerwald & Sun (2011)), which relates to the relative degree of ˆμ(G) and the smaller ˆμ(G) (the more G appears to be random). Consequently, in the irregular graph case, we could dismiss valid expanders due to the overly limiting requirements while retaining only graphs with random sparse-graph structures when analyzing high sparsity.\n\nIMDB. We propose a new metric called Iterative Mean Difference of Bound (∆rimdb) as a way to relax the ∆r’s constraint. Let G be our irregular graph, we define a set K to be {Ki(V, E, di) ⊂ G|di ≥ 3} the set of d-regular subgraphs in G, ∆rimdb is formally defined as:\n\n∆rimdb =\n\n1 |K|\n\n|K| (cid:88)\n\ni\n\n(2 ∗\n\n(cid:112)\n\ndi − 1 − ˆμ(Ki))\n\n(3)\n\nSince every graph in K are regular, we do not have to estimate the upper bound of their ˆμ. Intuitively, we say an irregular graph G is a good expander if its regular subgraphs are good expanders. Finally, we extend Eq 3. for bi-graphs as:\n\n∆rimdb =\n\n1 |K|\n\n|K| (cid:88) (\n\ni\n\n(cid:112)\n\ndi − 1 +\n\n(cid:112)\n\ndR − 1 − ˆμ(Ki))\n\n(4)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwhere dR is the average degree of R since we only care about d-left-regular bi-graphs for our settings. Our ablation study shows ∆rimdb effectively correlates relative performance to relative connectivity for highly connected highly sparse structures.\n\n3.3 NORMALIZED RANDOM COEFFICIENT\n\nIn the previous section, we mentioned the expander mixing lemma and its relation to ˆμ(G), but did not go into much detail about it. In this section, we define this relation and show how it can be used to prove the existence of our NormAlized Random Coefficient (NaRC).\n\nDefinition 6. Given a d-regular G(V, E), the expander mixing lemma is given as:\n\n||E(S, T )| −\n\nd|S||T | n\n\n| ≤ μ1(G)(cid:112)|S||T |\n\n(5)\n\nwhere S, T ⊆ V , S ∩ T = {0}, n = |V |, μ1(G) is the second largest eigenvalue of G, and |E(S, T )| is the total number of edges between S and T . The intuition behind it is that the smaller μ1(G), the more G appears to be random. For more explanations, please refer to Sauerwald & Sun (2011).\n\nFor our irregular bi-graph case, we exchange μ1 and μ0 since they are equivalent, and replace d with dL, the average left degree. We can also exchange the definition of S to that of Definition 2, with S ⊆ L, likewise replace T with N (S) ⊆ R and n with m = |R|. The expression now reads as:\n\n||E(S, N (S))| −\n\ndL|S||N (S)| m\n\n| ≤ μ0(G)(cid:112)|S||N (S)|\n\n(6)\n\nFrom Lemma 1, we know that davg is the smallest possible value μ0 can be for G, thus the following inequality is true:\n\ndavg ≤\n\n||E(S, N (S))| − dL|S||N (S)| (cid:112)|S||N (S)|\n\nm\n\n|\n\n≤ μ0 ≤ dmax\n\n(7)\n\nEq 7 further highlights our earlier argument for IMDB, but we can now take it a step further. With this inequality, we can rewrite the Ramanujan upper bound to include the random expander graph subsets. This means:\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nˆμ(G) ≤ 2\n\n||E(S, N (S))| − dL|S||N (S)| (cid:112)|S||N (S)|\n\nm\n\n|\n\n− 1\n\n(8)\n\nBy subtracting the right-hand side, we arrive at our definition for NaRC:\n\nσ = (\n\nˆμ(G)2 4\n\n+ 1)(cid:112)|S| ∗ |N (S)| − ||E(S, N (S))| −\n\ndL |m|\n\n|S| ∗ |N (S)|| ≤ 0\n\n(9)\n\nand we say that G is a random expander if σ ≤ 0, and |L|∗|R| denote our normalized degree of randomness. From now on, when we refer σ, we will use its normalized version |L|∗|R| . From the above inequality, we can now clearly see how expander mixing lemma relates to ˆμ(G), and the smaller it is, the more random the graph becomes.\n\nσ\n\n|σ|\n\nOne reason we do not want our graphs to be reduced to randomness is that it conflicts with our desire for “specific connections” as stated by Frankle et al. (2021). Compared to trivial random sparsity, non-random and structurally meaningful masks are expected to provide an interpretation of where and how performance is derived. Our experiments show under the right circumstance, a network can learn to overcome randomness, and those with meaningful masks achieve significantly better performance on average.\n\n3.4 BIPARTITE EXPANDER GRAPHS ON PRUNING AT INITIALIZATION\n\nAll models can be viewed as a sequence of computing graphs. Let M denote the set of a l-layer model, we obtain M = {Gi, ..., Gl} graphs, where Gi = (L ∪ R, E) is the i-th layer’s graph representation with L and R indicates the input and output layer respectively. M initially starts as a set of complete bipartite graphs. Pruning is then a process of edge sparsification on M with the\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nresulting sub-graphs considered irregular expander graphs. The expander property appeals to network sparsification analysis because it can approximate a complete graph, as shown in Spielman (2018). However, approximating complete graphs using irregular graphs is still an open question. For our study, we consider only the convolution and linear layers of any given model.\n\nConvolution layer. A convolutional weight consists of four dimensions namely the input channels, output channels, kernel width, and kernel height. To represent the convolution layer as a bipartite graph, we can unfold the input and kernels dimension to get W ∈ R|L|×|R|, where |L| = Cin ∗ Kw ∗ Kh and |R| = Cout. The resulting weighted graph is written as G = (L ∪ R, E, W ). Note that the number of edges |E| = |W |.\n\nLinear Layer. A linear weight consists of only the input and output channels, and bipartite conversion is trivial. We directly express its weight as W ∈ R|L|×|R|, where |L| = Cin and |R| = Cout. The resulting weighted graphs is similarly written as G = (L ∪ R, E, W ). Prunning at initialization. For each network, let W denotes the set of weights {wl ∈ Rnl |∀l ∈ {1, ..., L}} where nl is the number of parameters at layer l. Pruning is the process of generating binary masks ml ∈ {0, 1}nl . A pruned subnetwork has weights wl ⊙ml, where ⊙ is the element-wise product. Most PaI methods have two stages: First, they issue scores zl ∈ Rnl to all weights. Second, they remove the score into mask ml with overall sparsity s. Pruning may occur iteratively or in one shot depending on the methods. We study the following representative PaI techniques:\n\n• Random (Liu et al., 2022) is the most basic PaI method that uniformly prunes every layer with the same pruning ratio assigned globally. Each parameter is randomly assigned a score based on the normal distribution. • ERK (Evci et al., 2020a; Mocanu et al., 2018) initializes sparse networks with a Erd ̋os-R ́enyi graph where small layers are usually allocated more weights. •SNIP (Lee et al., 2019) issues scores sl = |gl ⊙ wl| where gl and wl are gradients and weights respectively. The weights with the lowest scores after one iteration are pruned before training. •GraSP (Wang et al., 2020) removes weights that impeded gradient flows, by computing the Hessian-gradient product hl and issue scores sl = −w ⊙ hl. • SynFlow (Tanaka et al., 2020) iteratively prunes a model with its weights replaced with |wl|. Each time, it propagates an input of 1’s and computes the gradients based on the task’s loss function rl. It issues a score sl = |rl ⊙ wl| and removes the parameters with the smallest scores.\n\n4 EXPERIMENTS\n\nIn this section, we examine our earlier claims with empirical results and see how they fare; furthermore, with supporting evidence, we answer questions regarding relationships between Ramanujan to performance, randomness to performance, and Ramanujan to randomness. Finally, we point out intuitions and what they imply for PaI under the lens of the Ramanujan perspective.\n\nExperimental settings. We conduct our experiments with two different DNN architectures: Resnet34 (He et al., 2016) and Vgg-16 (Simonyan & Zisserman, 2014) on CIFAR-10 (Krizhevsky, 2009). We run our experiments with three random seeds and initialize all PaI methods identically with three different initial weights generated by each seed to ensure fairness. Table 1 summarizes our standardized training configurations. We include additional results on CIFAR-100 in the Appendix.\n\nTable 1: Summary of architectures and hyperparameters that we study in this paper.\n\nModel\n\nData\n\n#Epoch Batch Size Optimizer LR LR Decay, Epoch Weight Decay\n\nResnet-34 CIFAR-10\n\nVGG-16\n\nCIFAR-10\n\n250\n\n250\n\n256\n\n256\n\nSGD\n\nSGD\n\n0.1\n\n0.1\n\n10×, [160, 180]\n\n10×, [160, 180]\n\n0.0005\n\n0.0005\n\n4.1 OBSERVATIONS, INTUITIONS AND ABLATIONS\n\nRelationship between Ramanujan and performance: In Figure 1, we confirm that Ramanujan ∆r indeed correlates with performance as a function of density (denser equates wider in general). However, this observation is already known (Pal et al., 2022), so it is not very exciting. The left column of Figure 1, which visualizes Definition 5, can only show that connectivity correlates strongly with density, but it cannot claim whether strong connectivity is related to relative performance. We attribute this missing link to the strong upper bound of ˆμ(G). While ˆμ(G) ensures that graphs that satisfy the inequality are expanders, we see that it holds little correlation with the actual performance potential of the network. This brings us to our first contribution, which is the Iterative Mean Difference of Bound ∆rimdb.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: In these figures, solid lines always refer to the left y-axis, while dash lines refer to the right y-axis. Here, we illustrate the relationship between network density (x-axis), accuracy (left-y), and Ramanujan property (∆r (left column) and ∆rimdb (right column)) for Resnet-34 (top-row) and Vgg-16(bottom-row). First, we show in all cases Ramanujan graphs correlate strongly with performance (upward trend). Second, we show that ∆rimdb also strongly correlates to relative performance between different PaI methods at different sparsity. Note that for Vgg-16, we dropped GrASP due to its inability to generate a feasible mask.\n\nCorrelating relative performance with ∆rimdb: Figure 1 right column shows that ∆rimdb resolves the missing link stated early on. We demonstrate that ∆rimdb is able to correlate the degree of connectivity of individual sparse structures with their final ranking in performance at specific density levels. While its effectiveness lessens with increasing density, it is undeniable that ∆rimdb mirrors the performance trend of various sparse structures. Intuitively speaking, we can extend ∆rimdb to be a performance ranker for unstructured sparse masks. Ultimately, the results prove our intuition that an irregular graph G is a good expander if its regular subgraphs are good expanders too.\n\nOn the relationship between performance and σ: When we show that there exists a ratio σ that can provide a randomness estimation on the graph, we are naturally curious about its relationship with performance. But first, we need to perform a sanity check on σ to see if it works. We can confirm, using our two random methods (Rand and ERK) on Figure 3, that they always yield σ ≤ 0, no matter the settings. Now we observe from the left column of Figure 3 that there seems to be little correlation between performance and the randomness of the graph at first glance. The lack of correlation would support previous observations made by Frankle et al. (2021); Liu et al. (2022) on the sufficiency of random pruning given the right sparsity. However, Figure 2 quickly dispels these notions, which neatly brings us to our next point.\n\nFigure 2: The relationship between performance and σ for Resnet-34 with and without skip. We observe σ starts to correlate with performance without skipconnections.\n\nThe value of σ: We observe an interesting inverse relationship between σ and ∆rimdb for Resnet34 and Vgg-16. In Figure 3 right columns, we observe Resnet-34 to have a mutual correlation\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Identically formatted as Figure 1, on the left column, we illustrate the relationship between model’s performance and network’s randomness over global density for Resnet-34 and Vgg-16. On the right column, we try to correlate the Ramanujan characteristic (∆rimdb) with our Normalized Random Coefficient (NaRC) over performance. The observation is interesting because the relationship are perfectly inverted between our two models.\n\nbetween its degree of expansion and randomness that somehow yield increasingly better performance. Meanwhile, for Vgg-16, we see that a higher degree of expansion correlates to a lower degree of randomness, producing better performance. The second observation follows our specific intuitions, while the first observation seemingly contradicts them. How can randomness contribute to better performance? The answer turns out to be straightforward. In Figure 2, we compare the performance of Resnet-34 with and without skip connections at various densities. We observe that (1) as the model gets more sparse without skip connections, its masked structure becomes more specific (less random) to ensure gradient flows; (2) skip connections help carry information and act as a crutch to overcome the random nature of the model; (3) without skip connections, Resnet-34 becomes more specific; however, relying on gradients alone is not enough to recover the performance achieved by the model with skip connections. It all means that σ negatively affects performance for both models, and skip connections can help alleviate the symptoms while exacerbating the problem as they effectively hide randomness.\n\nOverall, we have shown a way to relax the strong constraint on ˆμ(G) such that the resulting graph’s connectivity correlates strongly with its final performance. Further checking the lower bound for ˆμ(G) could indicate whether a sparse structure deteriorates into randomness. Tying all together, we now have the necessary tool to locate the region where highly connected, highly sparse, and non-trivial expanders exist. While not our primary objective, we foresee future PaI works utilizing our metrics as “checkers” to guide the design of their new criteria in selecting sparse structures.\n\n5 CONCLUSION\n\nThis work delved into quantifying PaI from the perspective of Ramanujan graphs. Firstly, we introduced ∆rimdb as a novel way to relax the strong upper bound of ∆r in cases of highly sparse, highly connected networks. Secondly, we proved that there existed a random coefficient called NaRC that could reliably estimate the degree of randomness for any given irregular sparse graph, which served as a lower bound for ˆμ(G) beyond which a sparse structure deteriorated into randomness. Ultimately, we provided a new perspective on the effectiveness of PAI that is independent of weights, gradients, and losses. This work was purely scientific and no negative impact was anticipated.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors thank Peihao Wang for his valuable insights offered during the project discussions. Z. Wang is in part supported by NSF Scale-MoDL (award number: 2133861) and the NSF AI Institute for Foundations of Machine Learning (IFML).\n\nREFERENCES\n\nKartikeya Bhardwaj, Guihong Li, and Radu Marculescu. How does topology influence gradient propagation and model performance of deep networks with densenet-type skip connections? 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13493–13502, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nPau de Jorge, Amartya Sanyal, Harkirat Singh Behl, Philip H. S. Torr, Gr ́egory Rogez, and Puneet Kumar Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. ArXiv, abs/2006.09081, 2021.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:\n\nMaking all tickets winners. ArXiv, abs/1911.11134, 2020a.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020b.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=rJl-b3RcF7.\n\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pp. 3259–3269. PMLR, 2020.\n\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ig-VyQc-MLK.\n\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv\n\npreprint arXiv:1902.09574, 2019.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations, 2015a.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015b.\n\nSong Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for\n\nefficient neural network. ArXiv, abs/1506.02626, 2015c.\n\nBabak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network\n\npruning. In IEEE international conference on neural networks, pp. 293–299. IEEE, 1993.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pp. 1389–1397, 2017.\n\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n\nShlomo Hoory. A lower bound on the spectral radius of the universal cover of a graph. J. Comb.\n\nTheory, Ser. B, 93:33–43, 2005.\n\nShlomo Hoory and Nathan Linial. Expander graphs and their applications. Bulletin of the American\n\nMathematical Society, 43:439–561, 2006.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information\n\nprocessing systems, 2, 1989.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Single-shot network pruning based on connection sensitivity. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX.\n\nShiwei Liu, TT van der Lee, Anil Yaman, Zahra Atashgahi, D Ferrar, Ghada Sokar, Mykola Pechenizkiy, and DC Mocanu. Topological insights into sparse neural networks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, ECMLPKDD, 2020.\n\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. In International Conference on Learning Representations, 2022.\n\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\n\nl 0 regularization. International Conference on Learning Representations, 2018.\n\nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018.\n\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. International Conference on Learning Representations, 2016.\n\nMichael C Mozer and Paul Smolensky. Using relevance to reduce network size automatically.\n\nConnection Science, 1(1):3–16, 1989.\n\nAlon Nilli. On the second eigenvalue of a graph. Discrete Mathematics, 91(2):207–210, 1991.\n\nBithika Pal, Arindam Biswas, Sudeshna Kolay, Pabitra Mitra, and Biswajit Basu. A study on the\n\nramanujan graph property of winning lottery tickets. In ICML, 2022.\n\nShreyas Malakarjun Patil and Constantine Dovrolis. Phew : Constructing sparse networks that learn\n\nfast and generalize well without training data. In ICML, 2021.\n\nAmeya Prabhu, G. Varma, and Anoop M. Namboodiri. Deep expander networks: Efficient deep\n\nnetworks from graph theory. In ECCV, 2018.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nThomas Sauerwald and He Sun.\n\nExpander mixing lemma, October Lecture 3: 2011. URL https://resources.mpi-inf.mpg.de/departments/d1/teaching/ ws11/SGT/Lecture3.pdf.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556. ICLR., 2014.\n\nDaniel A. Spielman. Properties of expander graphs, October 2018. URL https://www.cs.\n\nyale.edu/homes/spielman/561/lect17-18.pdf.\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n\nJingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural Information Processing Systems. arXiv:2009.11094, 2020.\n\nHidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli. Pruning neural networks\n\nwithout any data by iteratively conserving synaptic flow. ArXiv, abs/2006.05467, 2020.\n\nDharma Teja Vooturi, G. Varma, and Kishore Kothapalli. Ramanujan bipartite graph products for\n\nefficient block sparse neural networks. ArXiv, abs/2006.13486, 2020.\n\nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkgsACVKPH.\n\nJiaxuan You, Jure Leskovec, Kaiming He, and Saining Xie. Graph structure of neural networks. In\n\nICML, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL EXPERIMENTS ON CIFAR-100\n\nFigure 4: On these figures, solid lines always refer to the left y-axis, while dash lines refer to the right y-axis. Here we are illustrating the relationship between network density (x-axis), accuracy (left-y), and Ramanujan property (∆r (left column) and ∆rimdb (right column)) for Resnet-34 (toprow) and Vgg-16(bottom-row). First, we show in all cases Ramanujan graphs correlate strongly with performance (upward trend). Second, we show ∆rimdb also strongly correlates to relative performance between different PaI methods at different sparsity.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Identically formatted as Figure 1, on the left column, we illustrate the relationship between model’s performance and network’s randomness over global density for Resnet-34 and Vgg-16. On the right column, we try to correlate the Ramanujan characteristic (∆rimdb) with our Normalized Random Coefficient (NaRC) over performance. The observation is interesting because the relationship are perfectly inverted between our two models.\n\n14",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a new method to prune neural networks at initialization. By analogy to previous work which used sparse random subnetworks to speed up training, this paper instead uses Ramanjuan graphs, which are well-studied and known to be favorable with respect to expansion. It relaxes the eigenvalue condition of Ramanujan graphs to a new property (IMDB) which empirically correlates to increased performance when the network is trained on image classification tasks.\n\n# Strength And Weaknesses\n\nGeneral comments\n-\n\n- Why use the third largest eigenvalue of the adjacency matrix? Usually the spectral gap is taken to be the difference between the first and second largest eigenvalues, which is guaranteed to be positive if the graph is connected.\n\nStrengths\n-\n\n- The idea of using expander graphs to guide pruning is interesting and unifies existing work.\n\nWeaknesses\n-\n\n- The decision to not use existing measures of spectral expansion is not entirely convincing. You mention that the Ramanujan condition is too pessimistic for non-regular graphs when $d_{max}$ is used, but why not use something like the second eigenvalue of the normalized Laplacian?\n- The decision to pass to all $K$-regular subgraphs seems somewhat arbitrary and possibly expensive to compute. \n- The pruning measure proposed does not leverage the weights of the network at initialization, since it does not sparsify the network based \n on sensitivity of the loss function.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper provides a new perspective on pruning using the theory of spectral expanders, which are more general than simply using random graphs. The writing in some parts is confusing. For example, Lemma 1 reads \"The value of $\\mu_0$ for any adjacency matrix is said to be $d_{avg} \\leq \\mu_0 \\leq d_{max}$.\" This looks like it's stating a definition even though it is part of the lemma statement, and there is no separation between the lemma statement and its proof.\n\n# Summary Of The Review\n\nThe idea of measuring performance of pruning in terms of spectral properties of the adjacency matrix is a good idea. However, the choices made in deciding upon the IMDB measure seem arbitrary in comparison to existing measures of spectral expansion. The choice of comparing to $\\delta r$ is not entirely fair as a baseline, since that particular bound was designed around regular graphs.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization through the Lens of Ramanujan Graph\" investigates the efficacy of pruning neural networks at initialization (PaI) by leveraging concepts from Ramanujan graphs. The authors introduce two novel metrics: the Iterative Mean Difference of Bound (IMDB), which assesses performance by relaxing upper bounds on eigenvalues for irregular graphs, and the Normalized Random Coefficient (NaRC), which evaluates randomness in sparse networks. Key findings include a lack of correlation between Ramanujan properties and PaI performance, while IMDB emerges as an effective predictor of performance in sparse subnetworks, and NaRC provides insights into the randomness of network structures. The research aims to enhance the understanding of network topology's impact on PaI methods.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to analyzing PaI through graph theory, particularly with the introduction of IMDB and NaRC, which offer new perspectives on the performance of sparse networks. The study is well-structured, with a clear methodology and comprehensive experiments conducted on established DNN architectures. However, a notable weakness is the limited scope of the relationship between Ramanujan properties and performance, as the findings suggest that while IMDB correlates with performance, the potential of NaRC remains less explored. Furthermore, the paper could benefit from additional empirical validation across a wider range of architectures and datasets to strengthen its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly articulated, presenting complex concepts in an accessible manner. The quality of the methodology is high, with adequate experimental setups and standardized configurations for training. The novelty lies in the integration of graph theory into the analysis of PaI, which is a relatively unexplored avenue in the literature. The reproducibility is supported by the availability of code on GitHub, allowing others to replicate the experiments and verify the findings.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of neural network pruning by framing the analysis in terms of graph theory. The introduction of IMDB and NaRC as novel metrics for understanding sparse network performance is commendable. However, the limited correlation between Ramanujan properties and performance suggests that further exploration is needed to fully exploit the potential of these metrics.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces the concept of Pruning at Initialization (PaI), which aims to identify sparse subnetworks that perform similarly to full networks right from the initialization phase. The authors analyze PaI through the framework of Ramanujan graphs and propose two new metrics: the Iterative Mean Difference of Bound (IMDB) and the Normalized Random Coefficient (NaRC). The study employs ResNet-34 and VGG-16 architectures on the CIFAR-10 dataset to evaluate the performance of various PaI methods, demonstrating a correlation between the proposed metrics and network performance while highlighting the nuanced relationship between randomness and performance.\n\n# Strength And Weaknesses\nStrengths of the paper lie in its innovative approach of applying graph theory to analyze pruning methodologies, which is a relatively unexplored area. The introduction of IMDB and NaRC metrics significantly enhances the analysis of network connectivity and randomness, providing useful tools for future research. However, the study's focus on only two specific architectures may limit the applicability of its findings to a broader range of DNNs. Additionally, the complexity of the observed relationships raises questions that warrant further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers with varying levels of expertise in the field. The quality of the experimental design is commendable, as it employs standardized configurations and multiple seeds to ensure reproducibility. The novelty of using graph theory in the context of PaI is a significant contribution, although further empirical validation across diverse architectures would strengthen its impact.\n\n# Summary Of The Review\nOverall, this paper offers valuable insights into pruning at initialization through a graph-theoretical lens, introducing new metrics that are likely to facilitate further research in this area. While the findings are promising, the limited scope of the architectures tested may restrict the generalizability of the results.\n\n# Correctness\nRating: 4/5\n\n# Technical Novelty And Significance\nRating: 5/5\n\n# Empirical Novelty And Significance\nRating: 4/5",
    "# Summary Of The Paper\nThe paper \"Revisiting Pruning at Initialization through the Lens of Ramanujan Graph\" presents a novel approach to Pruning at Initialization (PaI), aiming to identify sparse subnetworks that maintain comparable performance to fully connected networks. The authors introduce the concept of Ramanujan Graphs to explore the relationship between sparsity and connectivity in neural networks. Key contributions include the development of new metrics — Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC) — to better analyze the performance of sparse networks. The experimental results demonstrate that subnetworks with favorable IMDB properties correlate with improved performance, although the relationship between graph connectivity and performance is nuanced.\n\n# Strength And Weaknesses\nThe strength of the paper lies in its innovative application of graph theory to understand network pruning, particularly through the introduction of IMDB and NaRC metrics. The insights gained from analyzing the structure of subnetworks provide a fresh perspective on PaI methods, which typically rely on gradient information. However, the paper could benefit from more extensive empirical evaluation across diverse architectures and datasets to validate the proposed metrics comprehensively. Additionally, the complexity of the theoretical foundations may pose a barrier to understanding for readers unfamiliar with graph theory.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, though some sections, particularly those involving graph theory, may be challenging for a broader audience. The quality of the writing is high, and the figures effectively illustrate the relationships discussed. The novelty of the approach is significant, as it bridges concepts from graph theory and deep learning, yet the reproducibility is somewhat limited due to the reliance on specific architectures (ResNet-34 and VGG-16) and datasets (CIFAR-10) without exploring a wider range of scenarios.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of deep learning by integrating graph theoretical concepts into the analysis of pruning methods. The introduction of new metrics offers promising avenues for future research, although the empirical validation and clarity for a broader audience could be enhanced.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to pruning at initialization (PaI) in neural networks, leveraging the structural properties of Ramanujan graphs. The authors introduce the Iterative Mean Difference of Bound (IMDB) method to enhance the analysis of irregular graphs under high sparsity. Through the introduction of the Normalized Random Coefficient (NaRC) metric, the paper provides insights into the randomness of sparse structures and correlates the performance of subnetworks with improved IMDB properties. Empirical evaluations demonstrate the effectiveness of the proposed methods across selected architectures and datasets, although the practical implications of the findings and their generalizability remain open to further investigation.\n\n# Strength And Weaknesses\nThe paper makes several significant contributions, notably through the introduction of innovative metrics and methods that advance the understanding of neural network pruning. The application of Ramanujan graphs provides an enriching perspective, while the IMDB method effectively addresses key limitations in existing analyses of graph sparsity. Moreover, the empirical results bolster theoretical claims, showcasing the proposed metrics' relevance across various architectures. However, the study has notable limitations, including a reliance on specific architectures and datasets, which may hinder the generalizability of the conclusions. Additionally, while critiques of traditional PaI methods are insightful, the paper lacks comprehensive alternative strategies to address these shortcomings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the complex ideas accessible to readers. The quality of the writing is strong, supplemented by a thorough review of related work that situates the research within the broader field of neural network pruning. However, certain theoretical aspects would benefit from more rigorous mathematical proofs or simulations to enhance reliability. The novelty of the proposed methods is commendable, yet the reproducibility of the findings may be limited due to the narrow scope of experiments conducted.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of neural network pruning through innovative metrics and methods, supported by empirical validation. However, the generalizability of the findings is constrained by the specific architectures and datasets used, and further exploration of practical implications and alternative strategies is warranted.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper addresses the challenge of pruning neural networks at initialization (PaI), proposing a new framework that emphasizes the role of graph structures in assessing network connectivity and performance. The authors introduce the \"Rigorous Connectivity Index\" (RCI) as a novel metric that relaxes traditional eigenvalue constraints, along with the concept of \"Structured Random Coefficient\" (SRC) to identify lower bounds on eigenvalue assessments. Extensively validated through experiments on ResNet-34 and VGG-16 architectures using the CIFAR-10 dataset, the study finds a strong correlation between RCI and performance, indicating that subnetworks with superior RCI characteristics tend to perform better. The paper also explores the implications of irregular bipartite graphs, broadening the understanding of network pruning methodologies.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative contributions, specifically the introduction of RCI and SRC metrics, which represent a significant theoretical development in pruning at initialization. The empirical validation provides solid evidence for the practicality of these metrics, suggesting they can improve performance prediction in neural networks. However, the theoretical underpinnings of RCI and SRC could be further strengthened with more rigorous mathematical proofs, ensuring their robustness across various network architectures. Additionally, a deeper exploration of the metrics' implications in real-world applications could enhance the paper's relevance beyond academic benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the motivation behind the research and the proposed methodologies. The quality of the experimental design is commendable, and the results are presented in a clear and concise manner. The novelty of the contributions, particularly the shift towards a graph-structure-based analysis, is significant. However, the reproducibility of results could be improved with more detailed descriptions of the experimental setup and the datasets used, as well as the availability of code and data.\n\n# Summary Of The Review\nThe paper presents a significant advancement in the understanding of pruning at initialization by introducing novel metrics based on graph structures. While the empirical results support the theoretical claims, the paper would benefit from stronger mathematical foundations and a deeper exploration of real-world applicability. Overall, it represents a valuable contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Adversarial Training Through the Lens of Ramanujan Graph\" investigates adversarial training (AT) of neural networks using concepts from graph theory, specifically focusing on Ramanujan graphs. The authors propose that the structural properties of these graphs can enhance the understanding of adversarial training's effectiveness in improving the robustness of neural networks against adversarial attacks. Key contributions include the introduction of a graph-theoretic framework, two novel metrics (Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC)), and extensive empirical analysis demonstrating the correlation between the proposed metrics and the adversarial robustness of models trained on datasets like CIFAR-10 using architectures such as ResNet and VGG.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach that combines graph theory with adversarial training, providing fresh insights into model robustness. The empirical validation is robust, with clear demonstrations linking the proposed metrics to adversarial performance. The framework and metrics are well-defined, enhancing understanding of the relationship between network structure and adversarial training. However, the study's scope is somewhat limited due to its exclusive focus on Ramanujan graphs, leaving questions about the applicability of findings to other graph structures. Additionally, the complexity of the proposed metrics may necessitate further explanation and validation across diverse neural network architectures and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, with a structured approach to both the theoretical framework and empirical analysis. The novel integration of graph theory into adversarial training is a significant contribution, marking a departure from traditional methodologies. However, the complexity of the new metrics could hinder reproducibility, as they may require a deeper understanding of the underlying graph-theoretic concepts for full implementation.\n\n# Summary Of The Review\nOverall, this paper offers a novel perspective on adversarial training by utilizing properties of Ramanujan graphs to develop new metrics for assessing robustness. While the approach is innovative and empirically validated, the limitations in scope and potential complexity of the metrics may affect broader applicability and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" introduces significant contributions to the field of neural network pruning, focusing on the concept of pruning at initialization (PaI). The authors present a novel theoretical framework based on Ramanujan graphs, claiming that their Iterative Mean Difference of Bound (IMDB) can reshape the understanding of sparse subnetworks and their performance. Additionally, they introduce the Normalized Random Coefficient (NaRC) as a new metric for assessing randomness in graphs, asserting that their empirical analyses demonstrate a strong correlation between IMDB and network performance, potentially paving the way for improved neural network efficiency. However, the methodology relies on a limited number of architectures, raising concerns about the generalizability of their findings.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambitious claims and the introduction of new metrics like IMDB and NaRC, which could provide fresh insights into the analysis of neural networks. The critique of existing pruning techniques is also noteworthy, as it challenges the status quo and advocates for a new approach. However, the weaknesses include a lack of thorough empirical validation across diverse architectures, which limits the robustness of their conclusions. Additionally, the assertion that their work renders prior methods obsolete may be overly ambitious, as it does not sufficiently acknowledge the contributions of existing literature.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured but suffers from instances of exaggerated claims that may detract from the clarity of the contributions. While the introduction of novel metrics is a valuable addition, the paper does not sufficiently detail how these metrics can be reproducibly applied in future research, which could impact their utility. The novelty of the theoretical framework is significant, yet it might not be as groundbreaking as presented, given that some concepts are already explored in existing literature.\n\n# Summary Of The Review\nOverall, the paper presents interesting concepts that could advance the understanding of neural network pruning, particularly through the lens of graph theory. However, the contributions may not be as revolutionary as claimed, and the limited empirical validation raises questions about the practical applicability of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" presents a novel perspective on Pruning at Initialization (PaI) methodologies, highlighting their potential to achieve performance comparable to fully-trained networks while being more efficient. The authors introduce two key metrics: Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC), which provide insights into the structural properties of pruned networks. Through experiments on Resnet34 and VGG-16 architectures using CIFAR-10, the paper demonstrates that networks exhibiting favorable IMDB characteristics yield improved performance, while NaRC helps identify when sparse structures become effectively random.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to linking graph theory with pruning methods, offering a fresh perspective that could drive future research. The introduction of IMDB and NaRC as metrics to assess network structures and their correlation with performance is particularly insightful. However, a potential weakness is that the experimental validation is limited to a small selection of architectures and datasets, which may restrict the generalizability of the findings. The reliance on graph-theoretic properties, while interesting, may also raise questions regarding their practical applicability across diverse network types.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its hypotheses, contributions, and findings. The methodology is detailed, allowing for reproducibility; however, some sections could benefit from additional clarity regarding the implementation of the proposed metrics. The novelty of the approach is strong, as it integrates concepts from graph theory into the analysis of pruning techniques, which is relatively unexplored in the current literature. Overall, the quality of the writing and the presentation of the results is commendable.\n\n# Summary Of The Review\nThis paper presents a significant advancement in the understanding of Pruning at Initialization by applying graph-theoretic concepts, providing new metrics that correlate well with model performance. While the findings are promising, the scope of experiments may limit broader applicability, and further validation across different architectures is warranted.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the application of Ramanujan graph properties to the analysis of sparse neural networks derived from pruning at initialization (PaI). The authors challenge the assumption that these properties directly correlate with network performance, particularly questioning the relevance of eigenvalue bounds and the analysis of irregular graph structures. They propose new metrics to assess the randomness in graph configurations and suggest that certain characteristics, including topological connectivity and sparsity levels, significantly influence performance outcomes. The findings indicate that the relationship between graph properties and network performance is more complex than previously assumed.\n\n# Strength And Weaknesses\nThe paper presents a thoughtful critique of the utility of Ramanujan graphs in the context of neural network pruning, offering new perspectives that challenge established assumptions. However, it suffers from several weaknesses, including a narrow focus on specific graph characteristics and an inadequate exploration of the implications of weight-based metrics, which are critical to understanding network performance. Additionally, the generalizability of the findings across various architectures is questionable, and the assumptions regarding static pruning frameworks may limit the applicability of the conclusions presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and communicates its arguments effectively. The novelty of proposing metrics such as IMDB and NaRC is commendable, although the underlying assumptions may reduce their overall impact. The reproducibility of the findings could be enhanced by providing more comprehensive details about the experiments and the specific architectures explored, as well as by discussing the implications of dynamic pruning strategies.\n\n# Summary Of The Review\nWhile the paper presents valuable insights into the limitations of applying Ramanujan graph properties to neural network pruning, its conclusions are built on several unexamined assumptions that could undermine their validity. A broader exploration of network characteristics and a more nuanced discussion of pruning techniques would strengthen the contributions made in this work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the process of pruning neural networks at initialization (PaI) by leveraging concepts from Ramanujan graphs. Its main contributions include the introduction of two novel metrics—Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC)—which aim to assess the effectiveness of PaI methods more accurately than traditional weight and gradient signals. The findings demonstrate that these new metrics correlate well with performance across various deep neural network architectures, highlighting the connection between graph properties and network performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing the shortcomings of existing pruning techniques, especially the critique of reliance on weight and gradient signals. The introduction of IMDB and NaRC represents a meaningful advancement in the evaluation of pruning methods, providing a fresh perspective on the structuring of sparse networks. However, one weakness is the limited scope of experiments, which primarily focus on a few architectures, potentially restricting the generalizability of the findings. Additionally, while the theoretical underpinnings are compelling, further empirical validation across a broader range of models would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates the concepts clearly, making it accessible to readers with varying levels of familiarity with the topic. The methodology is described in sufficient detail, allowing for reproducibility of the proposed metrics. The novelty of the work is significant, as it challenges established norms in pruning approaches and introduces new evaluation metrics. However, the paper could benefit from more comprehensive experimental validation to substantiate its claims fully.\n\n# Summary Of The Review\nOverall, this paper presents a novel approach to pruning neural networks at initialization by introducing new metrics that better assess pruning effectiveness. While the contributions are theoretically sound and the methodology is clear, the empirical validation could be more extensive to enhance the generalizability of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel framework for enhancing the efficiency of deep neural networks through an adaptive pruning technique. The authors propose a methodology that combines gradient-based optimization with a reinforcement learning approach to dynamically adjust the pruning rates during training. The findings demonstrate that the proposed method significantly reduces computational overhead while maintaining model accuracy, outperforming several baseline pruning techniques on benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to model pruning, which offers a fresh perspective on optimizing neural network architectures. The integration of reinforcement learning allows for a more adaptable and potentially effective pruning strategy. However, the paper lacks a thorough discussion of the limitations of the proposed method, particularly in terms of its applicability to various architectures or its robustness against different types of data distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-written and structured, making it accessible to readers with a background in machine learning. The methodology is described in sufficient detail, although some aspects, particularly regarding the reinforcement learning algorithm, could benefit from clearer explanations. The novelty of the approach is evident, yet more explicit comparisons with existing methods could bolster its significance. The reproducibility of the results is aided by the inclusion of hyperparameters and experimental settings, though sharing code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nThis paper offers a significant contribution to the field of model pruning through an innovative adaptive technique that leverages reinforcement learning. While the results are promising, the paper would benefit from a more robust discussion of its limitations and a clearer exposition of certain methodologies.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" explores the burgeoning field of pruning neural networks at initialization (PaI). The authors propose to analyze PaI through the framework of Ramanujan graphs, introducing two new metrics: Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC). The findings reveal that subnetworks preserving the IMDB property correlate with higher performance, while NaRC provides insights into the randomness of the sparse structures. This work aims to deepen the understanding of how the topology of neural networks influences the effectiveness of PaI methods.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including the introduction of novel metrics (IMDB and NaRC) that enhance the understanding of pruning methods at initialization. By linking network topology to performance, the authors provide a fresh perspective on a well-studied area. However, a notable weakness is the lack of significant correlation found between the properties of Ramanujan graphs and the performance of PaI-generated subnetworks, which may limit the applicability of the theoretical framework introduced. Additionally, while the proposed metrics are interesting, their practical implementation and integration into existing pruning techniques require further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with the subject. The quality of writing is high, with a coherent flow of ideas from the introduction to the conclusion. The novelty of the approach, particularly the use of Ramanujan graphs and the proposed metrics, adds to the paper's significance. However, the reproducibility of results may be challenged by the complexity of the metrics introduced and their dependence on specific graph properties, which could require careful implementation in future work.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to understanding pruning at initialization through the lens of Ramanujan graphs, introducing valuable metrics that can enhance future research in this area. Despite some limitations in correlating graph properties with performance, the work has potential implications for optimizing sparse neural network structures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" presents a novel approach to pruning neural networks at initialization (PaI) by leveraging the structural properties of Ramanujan graphs. The authors introduce two new metrics—Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC)—to evaluate the performance of sparse subnetworks in comparison to fully connected networks. Through experiments involving ResNet34 and VGG-16 on the CIFAR-10 dataset, the authors demonstrate that the Ramanujan property is correlated with performance outcomes, while their proposed metrics effectively assess the efficacy of PaI methods, thereby providing valuable insights into model design and optimization.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative use of graph theory to analyze the pruning process, which is relatively unexplored in existing literature. The introduction of IMDB and NaRC metrics offers a fresh perspective on assessing pruning effectiveness, providing practitioners with new tools for evaluating sparsity in neural networks. However, the paper also has weaknesses, including a limited exploration of the implications of high sparsity scenarios where the Ramanujan property may not hold, which could leave unanswered questions about the robustness of the proposed methodology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers with varying levels of expertise in the field. The methodology is described in sufficient detail, allowing for reproducibility of the experiments. The novelty of combining graph theory with neural network pruning is significant, although the paper could further enhance clarity by providing more extensive discussions on the limitations of the proposed metrics and their applicability in diverse contexts.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of neural network pruning by introducing new metrics grounded in graph theory. While the clarity and quality of the writing are commendable, the exploration of high sparsity scenarios and their implications could be improved. The findings suggest promising avenues for future research in optimizing neural network architectures.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" explores the concept of pruning neural networks at initialization (PaI) and highlights the performance gap between PaI and traditional post-training pruning methods. The authors propose the use of Ramanujan graphs as a novel framework to analyze and enhance PaI, introducing new metrics such as IMDB and NaRC to evaluate the effectiveness of the pruning strategy. The experimental results, based on architectures like ResNet-34 and VGG-16 across datasets including CIFAR-10 and CIFAR-100, demonstrate a correlation between the properties of Ramanujan graphs and improved model performance, suggesting significant implications for future research in the area.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive review of related work, which effectively situates the research within the existing literature. The methodology is well-structured, with clear definitions and theoretical insights that inform the proposed metrics. The experimental design is robust, with detailed descriptions of architectures and datasets, supported by visual analysis of results. However, a potential weakness is the lack of discussion regarding the limitations of the proposed methods and their applicability in real-world scenarios. Additionally, while the paper introduces novel metrics, further validation of their effectiveness in various contexts would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical organization that facilitates reader comprehension. Technical terms are adequately defined, making complex concepts accessible. The quality of the presentation is high, with well-structured sections and coherent arguments. The novelty of the work lies in the introduction of Ramanujan graphs to the pruning at initialization framework, which presents significant potential for advancing research in model efficiency. The authors have made their code available, enhancing the reproducibility of their results, which is a commendable aspect of the submission.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in understanding the interplay between graph properties and neural network pruning at initialization. It offers both theoretical insights and practical implications for improving model efficiency, although it could benefit from a more in-depth exploration of limitations and real-world applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" by Duc Hoang et al. investigates the pruning at initialization (PaI) methodology for efficiently deriving sparse neural network architectures. The authors propose a novel framework that uses the properties of Ramanujan graphs to analyze and enhance the effectiveness of PaI techniques. They introduce two new metrics: the Iterative Mean Difference of Bound (IMDB) and the Normalized Random Coefficient (NaRC), which provide insights into the structural properties of sparse networks. The empirical findings demonstrate a correlation between these graph-theoretic properties and the performance of neural networks, suggesting that a deeper understanding of graph structures can lead to improved pruning strategies.\n\n# Strength And Weaknesses\nThe paper's major strength lies in its innovative approach of applying graph theory, specifically Ramanujan graphs, to the domain of pruning methodologies. This novel perspective enriches the existing literature and offers a significant theoretical underpinning for understanding PaI. The introduction of IMDB and NaRC as new metrics is a commendable contribution that addresses existing gaps in evaluating sparsity. However, a potential weakness is the relatively shallow exploration of the empirical results, which may not fully support the theoretical claims made regarding the effectiveness of the proposed metrics. The correlation findings, while promising, require further validation across diverse architectures and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making the theoretical concepts accessible even to readers who may not have a strong background in graph theory. The quality of writing is high, with appropriate use of mathematical notation and definitions. The novelty of the approach is clear, as it diverges from traditional metrics used in pruning methodologies. However, the reproducibility of the results could be improved by providing additional details on experimental setups and parameters used in the empirical evaluations.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new approach to understanding pruning at initialization through the lens of graph theory, contributing valuable new metrics to the field. While the theoretical foundations are robust and the analysis is insightful, the empirical validation could benefit from further depth and clarity.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a framework for neural network pruning based on Ramanujan graph theory, aiming to improve performance metrics related to pruning at initialization (PaI). The methodology involves evaluating the effectiveness of the proposed metrics, namely Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC), in assessing the utility of pruning techniques. However, the findings indicate a significant performance gap between PaI and post-training pruning, raising concerns about the practicality and competitiveness of the proposed methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical exploration of Ramanujan graphs and their potential application to neural network pruning. However, its weaknesses are pronounced, particularly in the limited applicability of the proposed framework to real-world scenarios, as highlighted by the performance gap noted. The reliance on specific metrics that may not generalize across diverse architectures undermines the robustness of the conclusions. Additionally, the lack of extensive experimental validation and the ambiguity surrounding the utility of the Ramanujan property further detract from the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by ambiguities in the assessment of key concepts, such as the \"utility\" of the Ramanujan property. While the quality of writing is generally acceptable, the lack of comprehensive experiments across various datasets and models limits the reproducibility of the findings. In terms of novelty, the paper appears to revisit familiar territory without introducing substantial innovations or breakthroughs, which raises questions about its overall significance in the field.\n\n# Summary Of The Review\nIn summary, the paper presents an intriguing theoretical framework for neural network pruning based on Ramanujan graph theory; however, it falls short in practical applicability, empirical validation, and innovative contributions. The findings and methodologies presented do not convincingly advance the state-of-the-art in neural network pruning.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" presents a novel approach to the pruning of neural networks at initialization (PaI) by integrating principles from graph theory, specifically Ramanujan graphs. The authors introduce two innovative metrics: the Iterative Mean Difference of Bound (IMDB) and the Normalized Random Coefficient (NaRC), which facilitate a deeper analysis of sparsity in deep neural networks (DNNs). The findings demonstrate that networks adhering to the IMDB property can achieve significant performance improvements, thereby establishing a new framework for evaluating the effectiveness of sparse neural networks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its groundbreaking contributions that blend graph theory with deep learning, offering practical metrics for analyzing and optimizing network sparsity. The introduction of IMDB and NaRC as novel evaluation tools is particularly noteworthy as they provide clear methods for assessing performance gains in pruned networks. However, the paper could benefit from more extensive empirical validation across a wider range of architectures beyond ResNet and VGG, which may limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making complex concepts accessible to a broader audience. The quality of the methodology is strong, with a clear rationale for the proposed metrics. The novelty of combining graph theory with neural network pruning is significant, marking a fresh perspective in the field. Reproducibility is supported by detailed descriptions of the metrics and methodologies; however, further datasets and architectures tested would enhance verification of results.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of neural network pruning by introducing innovative metrics and establishing a strong correlation between graph properties and network performance. While the findings are promising, additional empirical evaluation across diverse architectures would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to Pruning at Initialization (PaI) by leveraging the properties of Ramanujan graphs to enhance the performance of sparse neural networks. The authors introduce two key theoretical contributions: the Iterative Mean Difference of Bound (IMDB), which relaxes the stringent upper bounds on eigenvalues imposed by Ramanujan properties, and the Normalized Random Coefficient (NaRC), which establishes a lower bound for eigenvalues. By employing graph topology and the Expander Mixing Lemma, the work provides a theoretical framework that underscores the importance of structural integrity in neural network pruning, aiming to improve upon existing empirical methods that often overlook these theoretical aspects.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its rigorous theoretical contributions and the introduction of metrics that address previously overlooked aspects of network connectivity and performance. The framework provided has the potential to guide future research into effective pruning strategies. However, one notable weakness is the limited empirical validation of the proposed theoretical constructs, which may hinder the practical applicability of the findings. Additionally, the reliance on theoretical metrics might not fully capture the complexities of real-world neural network training and performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its arguments clearly, making complex theoretical concepts accessible. The quality of the writing is high, with a logical progression of ideas that supports the overall thesis. The novelty of the approach is significant, as it bridges theoretical insights with practical implications for neural network pruning. However, the reproducibility of the theoretical results may be challenging, given that the empirical validation is not extensively covered, thereby raising questions about how these theoretical insights can be implemented in practice.\n\n# Summary Of The Review\nOverall, this paper makes a compelling case for the integration of theoretical insights into the development of pruning strategies for neural networks. While the theoretical contributions are noteworthy and provide a fresh perspective on the topic, the lack of empirical evidence to support these claims may limit the immediate applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" presents new metrics for analyzing sparse neural networks, specifically focusing on Pruning at Initialization (PaI) techniques. The authors introduce the Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC) as innovative approaches to assess the impact of pruning on neural network structures. They conduct experiments using ResNet34 and VGG-16 architectures on the CIFAR-10 dataset, comparing various PaI methods such as Random, ERK, SNIP, GraSP, and SynFlow. The findings reveal a correlation between the proposed metrics and model performance, although the paper does not delve into the broader implications of these results.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of IMDB and NaRC, which provide a new lens through which to view the effects of pruning in neural networks. The detailed methodology and experimental setup enhance the reproducibility of the findings, with clear definitions of graph theory concepts that support the analysis. However, a significant weakness is the lack of discussion on the broader implications of the findings or future directions for research in this area. While the results are compelling, the paper could benefit from a more comprehensive exploration of how these metrics may influence the field of neural network pruning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a logical flow from the introduction of metrics to the experimental validation. The quality of the writing is high, making complex concepts accessible. The novelty of introducing IMDB and NaRC is notable, as is the application of graph theory to the analysis of DNNs. The reproducibility of the study is strengthened by the availability of code and detailed experimental configurations.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of neural network pruning by introducing new metrics and methodologies for analyzing sparse structures. While the findings are significant, the lack of broader implications and future directions limits its impact. Nonetheless, the clarity and quality of the work warrant consideration for publication.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper claims to enhance existing pruning at initialization (PaI) methods through the introduction of the Iterative Mean Difference of Bound (IMDB) and the Normalized Random Coefficient (NaRC) as novel metrics for assessing graph performance. The authors argue that these contributions provide a clearer understanding of graph randomness and its impact on neural network performance. However, the methodology primarily reiterates concepts from prior works without substantial innovation, and the experimental validation appears limited in scope.\n\n# Strength And Weaknesses\nThe paper attempts to build on prior research by addressing the limitations of Ramanujan graphs and proposing new metrics for evaluating graph performance. However, it largely revisits established ideas without delivering significant advancements or rigorous comparative analysis. The claims regarding NaRC's superiority over existing metrics lack empirical validation, and the experimental section is relatively narrow compared to previous studies. Overall, the paper does not convincingly demonstrate that its contributions provide a novel perspective on PaI effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally acceptable, but the novelty of the contributions is questionable as they often echo earlier findings without providing new insights. The quality of the analysis is undermined by a lack of rigorous comparisons with existing methodologies. Reproducibility is not clearly addressed, as the experimental setup appears limited, making it difficult to validate the findings against broader benchmarks in the field.\n\n# Summary Of The Review\nOverall, the paper does not present a significant advancement in the field of neural network pruning, as it primarily rehashes existing ideas without offering in-depth analysis or compelling empirical evidence. The contributions lack the necessary novelty and rigor to substantiate the authors' claims regarding their approach's effectiveness.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to pruning at initialization (PaI) in deep neural networks (DNN), focusing on identifying efficient sparse sub-networks that maintain performance while reducing model size. The authors propose new metrics to evaluate the effectiveness of their pruning strategy and demonstrate their methodology through extensive experiments on various datasets. The findings indicate that the proposed PaI method outperforms traditional pruning techniques, achieving a better trade-off between sparsity and accuracy.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative metrics for assessing pruning strategies, which are well-defined and grounded in theory. Additionally, the empirical results convincingly demonstrate the advantages of the proposed methodology over existing techniques. However, the paper suffers from issues related to clarity and consistency in presentation, including inconsistent formatting of terms and citations, which may hinder reader comprehension. Furthermore, some sections could benefit from clearer definitions and explanations of specialized terminology.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces novel concepts and approaches, the clarity is undermined by inconsistent terminology and formatting. The quality of the experiments is solid, but further details on hyperparameters would enhance reproducibility. The novelty of the proposed metrics and findings is significant, although the paper could improve in presenting these ideas in a more accessible manner.\n\n# Summary Of The Review\nOverall, the paper offers valuable contributions to the field of pruning at initialization in deep neural networks, showcasing both theoretical and empirical advancements. However, clarity and consistency issues need to be addressed to enhance the reader's experience and understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel pruning methodology termed Pruning at Initialization (PaI), which aims to reduce the computational burden of neural networks while maintaining performance. The authors conduct experiments primarily on CIFAR-10 and CIFAR-100 datasets, demonstrating that their approach can achieve competitive accuracy with significantly fewer parameters compared to traditional methods. The methodology focuses on pruning connections in a network at the initialization stage, and the findings suggest that this early intervention can lead to effective sparsity without compromising model performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of the PaI methodology, which shows promising results in parameter reduction while retaining accuracy. However, the paper has several weaknesses. It lacks a comprehensive exploration of different pruning techniques, such as post-training and during-training methods, which could provide a more nuanced understanding of pruning effectiveness. Additionally, the limited range of datasets used in the experiments restricts the generalizability of the proposed metrics. The authors also fail to analyze the trade-offs between sparsity and performance in real-world applications and do not sufficiently compare their method with state-of-the-art techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings clearly. However, the novelty of the proposed metrics is somewhat undermined by the lack of comparison with existing pruning methods. The reproducibility of the results could be improved by including more detailed descriptions of the experimental setup and parameters. Furthermore, the authors' discussion could benefit from a deeper exploration of theoretical foundations and implications for real-world applications, which would enhance both clarity and quality.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting pruning technique that demonstrates potential for reducing model size while maintaining performance. However, its contributions could be significantly strengthened by broader experimental validation, deeper theoretical analysis, and comparisons with existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Revisiting Pruning at Initialization through the Lens of Ramanujan Graph\" investigates the pruning of neural networks at initialization (PaI) with a focus on the structural properties of Ramanujan graphs. The authors introduce two novel metrics: the Iterative Mean Difference of Bound (IMDB) and the Normalized Random Coefficient (NaRC), which are used to evaluate the performance of various PaI methods. The findings indicate that the proposed metrics correlate strongly with performance, especially in irregular graphs, and suggest that traditional metrics may overlook critical aspects of graph theory in neural network pruning.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its introduction of novel metrics that provide a deeper understanding of the relationship between graph properties and neural network performance. The use of established architectures (ResNet-34 and VGG-16) on the CIFAR-10 dataset adds credibility to the empirical findings. However, a notable weakness is the lack of detailed exploration into the implications of the observed correlations, particularly the absence of a comprehensive explanation for the relative performance rankings. Additionally, while the statistical robustness is ensured through the use of multiple random seeds, the paper could benefit from further validation across a wider range of architectures and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers with varying levels of familiarity with the topic. The quality of the writing is high, with appropriate use of terminology and clear definitions of the proposed metrics. The novelty of the metrics IMDB and NaRC is significant, contributing fresh perspectives to the field of neural network pruning. The reproducibility of the study is supported by detailed descriptions of the experimental setup, including hyperparameters and random seeds used.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the understanding of pruning neural networks at initialization by introducing innovative metrics that leverage graph theory. While the findings are compelling and well-supported statistically, further exploration of the implications of the results would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the relationship between the properties of Ramanujan graphs and the performance of pruning at initialization (PaI) in neural networks. The authors introduce two novel metrics, the Iterative Mean Difference of Bound (IMDB) and the Normalized Random Coefficient (NaRC), aimed at evaluating the effectiveness of graph structures on pruning performance. However, their findings indicate a lack of significant correlation between the Ramanujan property and the performance of subnetworks, along with limitations in the generalizability and applicability of their proposed metrics.\n\n# Strength And Weaknesses\nWhile the paper makes a commendable effort to bridge the gap between graph theory and neural network pruning, several weaknesses undermine its contributions. The primary strength lies in the introduction of IMDB and NaRC as new metrics for analysis; however, these metrics do not directly address the causes of performance gaps in PaI. The reliance on specific assumptions about Ramanujan graphs limits the applicability of the findings, particularly as real-world neural networks often exhibit irregular structures. Furthermore, the empirical validation is constrained to only two architectures, which raises concerns regarding the robustness and generalizability of the results. The exploration of pseudo-random graphs and irregular graph structures is insufficient, indicating a need for a more comprehensive approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate, with a structured presentation of ideas, though the depth of analysis in certain areas could be improved. The novelty primarily stems from the proposed metrics; however, their contribution to advancing the field is limited by the lack of empirical validation across various architectures and datasets. The reproducibility of the results may be hindered by the narrow focus on specific models and datasets, without comprehensive assessments of broader applicability.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing exploration of graph properties in relation to neural network pruning but falls short in several critical areas, including the generalizability of metrics and empirical validation. The limitations in theoretical assumptions and the narrow scope of experiments suggest that further research is necessary to enhance the applicability and effectiveness of the proposed pruning methodologies.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph\" explores the emerging approach of pruning neural networks at initialization (PaI) and its potential advantages. The authors assert that leveraging the properties of Ramanujan graphs can enhance the performance of sparse networks. They introduce new metrics, IMDB and NaRC, to analyze the relationship between graph properties and network performance. The experimental results, conducted on standard architectures like ResNet-34 and VGG-16 using the CIFAR-10 dataset, demonstrate a correlation between connectivity and performance, suggesting that maximizing Ramanujan properties could yield effective pruning strategies.\n\n# Strength And Weaknesses\nThe paper attempts to provide a novel perspective on PaI by linking it with graph theory, specifically Ramanujan graphs. However, the contributions feel somewhat rehashed, as many concepts discussed are already established in the literature. While the introduction of new metrics is commendable, they do not significantly advance the field. The methodology, including the definitions of graph types, appears to add unnecessary complexity without offering substantial new insights. Overall, while the paper addresses an important topic, its contributions may not be as original or impactful as suggested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, making it accessible for readers familiar with deep learning and pruning techniques. However, the novelty of the contributions is questionable, as many findings seem to echo existing knowledge in the field. The proposed metrics and their implications for future work are not sufficiently justified, which may hinder reproducibility and practical application. The authors should provide clearer explanations of how their work diverges from established practices to strengthen their claims.\n\n# Summary Of The Review\nOverall, the paper offers a perspective on pruning at initialization that integrates Ramanujan graph properties, yet it lacks substantial novelty and depth. While it presents some interesting ideas and metrics, many contributions are overshadowed by their familiarity and the complexity introduced without clear justification. The paper would benefit from a more rigorous exploration of original insights.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper investigates the concept of pruning at initialization (PaI) within neural networks, proposing new metrics, Iterative Mean Difference of Bound (IMDB) and Normalized Random Coefficient (NaRC), to evaluate the efficacy of various pruning strategies. The authors conduct empirical evaluations using ResNet-34 and VGG-16 architectures, revealing a correlation between their proposed metrics and network performance. However, the paper highlights limitations in existing methods, particularly regarding the analysis of irregular graphs and the implications of randomness within graph structures.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its exploration of pruning techniques that challenge conventional post-training methods, emphasizing the importance of initialization strategies. The introduction of IMDB and NaRC provides novel metrics that contribute to the understanding of pruning efficacy. However, the paper fails to fully integrate graph neural networks (GNNs) and other advanced methodologies such as reinforcement learning or persistent homology, which could offer deeper insights into the relationships between network structures and performance. Additionally, the focus on specific architectures limits the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas with reasonable clarity, although certain concepts, particularly around the integration of GNNs and hybrid approaches, could be articulated more explicitly. The quality of the empirical results is solid, but the novelty is somewhat tempered by the lack of interdisciplinary approaches that could enhance the findings. The reproducibility of the results could be improved by providing additional details on experimental setups and broader architectural evaluations.\n\n# Summary Of The Review\nOverall, the paper presents an innovative approach to pruning at initialization, supported by new metrics and empirical results. Nonetheless, it would benefit from incorporating more interdisciplinary methods and broader experimental frameworks to strengthen its contributions and applicability across various neural network architectures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a comprehensive evaluation of pruning at initialization (PaI) techniques applied to two neural network architectures, ResNet-34 and VGG-16, using the CIFAR-10 dataset. Key contributions include the introduction of new metrics such as the Iterative Mean Difference of Bound (IMDB) and the Normalized Random Coefficient (NaRC), which are employed to analyze the correlation between network connectivity and performance at varying sparsity levels. The findings reveal that while Ramanujan graph properties correlate with performance, they do not guarantee it, and that structured sparsity can lead to enhanced model accuracy compared to random sparsity.\n\n# Strength And Weaknesses\nThe paper effectively highlights the importance of structured sparsity and introduces valuable metrics that provide deeper insights into the performance dynamics of pruning methods. The correlation established between IMDB and performance is particularly noteworthy, as is the finding regarding the influence of randomness on model outcomes. However, the paper could benefit from a more thorough discussion on the limitations of the proposed metrics and their applicability to a broader range of architectures beyond those tested. Furthermore, while the visualizations are informative, additional quantitative analysis could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The visualizations aid in understanding the relationships discussed, although more detailed explanations of the metrics would enhance clarity. The novelty of the proposed metrics is commendable, and their potential impact is significant. However, details regarding the reproducibility of the results, such as code availability or specific hyperparameter settings, are not extensively addressed, which could hinder others from replicating the study.\n\n# Summary Of The Review\nOverall, the paper offers significant contributions to the understanding of pruning techniques in neural networks, particularly through the introduction of new metrics that correlate with performance. While the findings are promising and provide valuable insights, there are areas for improvement regarding the discussion of limitations and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH\" explores a novel approach to model pruning at initialization using concepts derived from Ramanujan graphs. The authors propose a methodology that integrates these mathematical structures to enhance the efficiency of neural network training. The findings suggest that this approach not only improves the performance of pruned models but also provides a theoretical foundation for understanding the underlying mechanisms of pruning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative application of Ramanujan graphs to a well-established problem in neural network optimization, which adds a fresh perspective to the field of model pruning. The empirical results demonstrate significant improvements in model performance, providing strong evidence for the proposed methodology. However, the paper also has weaknesses, including a lengthy and dense abstract, inconsistent terminology, and areas of unclear phrasing that could hinder reader comprehension. Additionally, while the empirical results are promising, the organization within the results section could be improved for better clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe quality of the writing varies throughout the paper. While the core concepts are compelling, the clarity suffers from complex sentences, excessive use of parentheses, and jargon without sufficient definitions. The novelty of the approach is significant, as it introduces a unique mathematical framework to a conventional problem. However, reproducibility may be a concern due to the lack of detailed descriptions of experimental setups and insufficient clarity in presenting results. \n\n# Summary Of The Review\nOverall, the paper presents a novel approach to pruning at initialization that is both theoretically and empirically significant. However, improvements in clarity, organization, and terminology consistency are necessary to enhance the reader's understanding and make the findings more accessible.\n\n# Correctness\n4/5 - The methodology appears sound and the findings are supported by empirical evidence, though some aspects could benefit from clearer explanations.\n\n# Technical Novelty And Significance\n4/5 - The introduction of Ramanujan graphs to the pruning problem is a novel contribution that holds potential for advancing the field, though the application may require further exploration.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results are promising, they are not sufficiently novel in the context of existing literature on pruning, thus limiting their impact. Further comparison with established methods might strengthen this aspect."
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3659269127039524,
    -1.7087121066997348,
    -1.7189576681173482,
    -1.614946311103905,
    -1.968145509171306,
    -1.848638801176207,
    -1.5715583798322943,
    -1.8218230054581617,
    -1.7730065768868264,
    -2.1015858010314137,
    -1.687511996974046,
    -1.4737628388112844,
    -1.5925288205826331,
    -1.678504228042078,
    -1.6606339589464403,
    -1.6684461568309639,
    -1.8155259633412084,
    -1.7569089814231846,
    -1.8191129190560353,
    -1.6007107637687885,
    -2.0739860119654687,
    -1.7348967246477465,
    -1.5358643897185171,
    -1.6346732896220713,
    -1.8490448275454012,
    -1.8534540640792982,
    -1.8875614387135204,
    -1.7494430465773587,
    -1.690831204143579
  ],
  "logp_cond": [
    [
      0.0,
      -2.0956553735564656,
      -2.1155331322429602,
      -2.11113507668477,
      -2.116924473095676,
      -2.169542319171552,
      -2.1674383622657367,
      -2.1128897874219894,
      -2.1157572549372867,
      -2.1172466558724965,
      -2.1306689868321738,
      -2.236273142943977,
      -2.1115916155137797,
      -2.1063434376070345,
      -2.121564115345221,
      -2.109568268677793,
      -2.1233298233319626,
      -2.1083897790626094,
      -2.121943311660071,
      -2.1408569779595514,
      -2.127729489271073,
      -2.1934730337834965,
      -2.196519506685054,
      -2.1150797009552558,
      -2.121533604733537,
      -2.1152138775498694,
      -2.167477128421264,
      -2.1337113592016532,
      -2.1578321287333417
    ],
    [
      -1.3727616646805418,
      0.0,
      -1.180325650958783,
      -1.1816795621399503,
      -1.2187024992793831,
      -1.3437822309539447,
      -1.262486910647652,
      -1.1654679735694495,
      -1.174338582822762,
      -1.299372170185257,
      -1.2310804916043565,
      -1.476806268540707,
      -1.1109273806654592,
      -1.182556840693377,
      -1.252526087975971,
      -1.2056947994777123,
      -1.2653540800036132,
      -1.1844072662376242,
      -1.2349963680889164,
      -1.1899311202336802,
      -1.238786295462399,
      -1.4111831439291826,
      -1.404441515348162,
      -1.1719926590207876,
      -1.1843323969491841,
      -1.2173916138716736,
      -1.2595403110018273,
      -1.2041840660356855,
      -1.4254960143142463
    ],
    [
      -1.385572941528733,
      -1.1314748844246079,
      0.0,
      -1.1811266468814154,
      -1.234489976446187,
      -1.3478185421471718,
      -1.2876420212601802,
      -1.2188648331473,
      -1.1861840954271956,
      -1.3009171412678426,
      -1.2353420315237997,
      -1.4970893874316025,
      -1.242950392757618,
      -1.185475433426773,
      -1.2902327156449782,
      -1.2003083886648203,
      -1.2930898804786766,
      -1.2124314125786362,
      -1.3142368148604529,
      -1.1602884316985784,
      -1.2852850779550153,
      -1.413077018836475,
      -1.4601782550239648,
      -1.1726623217046301,
      -1.2767152997810562,
      -1.2970295316260498,
      -1.2048832588249696,
      -1.193124847000131,
      -1.4281789900190485
    ],
    [
      -1.3123047538270642,
      -1.0800912090810442,
      -1.055192203958859,
      0.0,
      -1.0585866857134625,
      -1.239486390577892,
      -1.1560019225119247,
      -1.1279389117897105,
      -1.0719264228376786,
      -1.219013789258231,
      -1.0913665956223009,
      -1.3002377989638507,
      -1.0759784691519367,
      -1.0378981989338938,
      -1.1400465966662172,
      -1.0965034274654888,
      -1.1450840707090109,
      -1.1074234600988428,
      -1.1480680518500688,
      -1.070968826811953,
      -1.12697975491888,
      -1.2972922179541635,
      -1.3236195050627593,
      -1.1038763322015077,
      -1.1442093209691495,
      -1.1359559521363642,
      -1.151864250909239,
      -1.0895200922495287,
      -1.2784741245517002
    ],
    [
      -1.6163923501338833,
      -1.4252186687156303,
      -1.436078379052079,
      -1.4172845102314628,
      0.0,
      -1.5957481734322554,
      -1.543176796090132,
      -1.4309257431555151,
      -1.4504633511389724,
      -1.4939991462809694,
      -1.4300101779952143,
      -1.6577028423409097,
      -1.4477804529666511,
      -1.419822393563492,
      -1.4991190399984657,
      -1.4444677171108364,
      -1.4460423244848908,
      -1.4783837916339093,
      -1.50912213904481,
      -1.4298422417057666,
      -1.4207075185937317,
      -1.6463095480684045,
      -1.6550816352726736,
      -1.4289147637008337,
      -1.422678293504275,
      -1.527200770582582,
      -1.4537417261116468,
      -1.4626035775077388,
      -1.6398313277833867
    ],
    [
      -1.5294477706321101,
      -1.412703351454937,
      -1.422718205675332,
      -1.4329572523213607,
      -1.4298883081184521,
      0.0,
      -1.507251777394029,
      -1.4246412513411408,
      -1.399343919018574,
      -1.444423467140701,
      -1.4567118755338389,
      -1.590915064053537,
      -1.4051960129519392,
      -1.421037692198329,
      -1.47278685252806,
      -1.4075079727911277,
      -1.518628838300098,
      -1.4473522631427027,
      -1.427736914540418,
      -1.4446653903638216,
      -1.5110888914815561,
      -1.486058855590236,
      -1.572757793965864,
      -1.4178593575774079,
      -1.4590819935644672,
      -1.4271876849320382,
      -1.4093220680728382,
      -1.390339021705869,
      -1.5616432962981825
    ],
    [
      -1.2798872426108951,
      -1.0648163111553484,
      -1.0677729432799217,
      -1.092567215144051,
      -1.0977579798265065,
      -1.2044870803475078,
      0.0,
      -1.0605145054245015,
      -1.0079193860754045,
      -1.1543383266853582,
      -1.1013243347447876,
      -1.3248145063517078,
      -1.095960229038642,
      -1.0348436959297167,
      -1.1647610892016749,
      -1.033022322764957,
      -1.120828319828509,
      -1.0485946845068854,
      -1.096068521053567,
      -1.1096906658831525,
      -1.1382094001622272,
      -1.2619691304810283,
      -1.2755349305206858,
      -1.0241074785902444,
      -1.0797317953755798,
      -1.1256257682209638,
      -1.1057746198778307,
      -1.1003549377345743,
      -1.3006870792693146
    ],
    [
      -1.523431379363017,
      -1.2760107893710873,
      -1.3166734411322527,
      -1.347814446437178,
      -1.3046328730190457,
      -1.4678386312983682,
      -1.3651397615416494,
      0.0,
      -1.2951763426997454,
      -1.3876980010772648,
      -1.351930845093946,
      -1.5588194560636175,
      -1.2723839557217789,
      -1.3330110550366232,
      -1.3792002888793657,
      -1.3043122241981568,
      -1.3442820303047813,
      -1.2842567288182993,
      -1.3549866743725112,
      -1.305572522394041,
      -1.3277495857588144,
      -1.501082272043053,
      -1.507359163181236,
      -1.324352723833257,
      -1.327947395141697,
      -1.3046186364169245,
      -1.3443492698668047,
      -1.325268524307965,
      -1.5190724781073361
    ],
    [
      -1.5013894062768283,
      -1.2219986307288242,
      -1.2296729119228635,
      -1.2928887349891776,
      -1.2655668128692839,
      -1.4219168121485375,
      -1.2841039969532067,
      -1.2281435445652884,
      0.0,
      -1.3495788719296278,
      -1.2926907249791522,
      -1.5069586879164965,
      -1.2021705182636324,
      -1.250174086426107,
      -1.3226562068629084,
      -1.2069557919430538,
      -1.3624457818207933,
      -1.2313162301319436,
      -1.3366419755313195,
      -1.2286052783833317,
      -1.3553541559876892,
      -1.4601546975477402,
      -1.4639055005440749,
      -1.2142386501802998,
      -1.3291951571999734,
      -1.2907906554779978,
      -1.3220683717231683,
      -1.2320045849538641,
      -1.504483953139052
    ],
    [
      -1.6978390196383744,
      -1.5082371340098417,
      -1.5619701112886701,
      -1.6276226927362196,
      -1.6066339706771882,
      -1.6817443720431564,
      -1.6403397723075424,
      -1.5393969747876526,
      -1.5926905267321319,
      0.0,
      -1.6239163711573725,
      -1.8357836450794225,
      -1.5610794141037723,
      -1.5828095681644074,
      -1.6210826536650582,
      -1.538886849764805,
      -1.59134820686917,
      -1.563004454468926,
      -1.5791096999363483,
      -1.6416831480150023,
      -1.5946768932024518,
      -1.7308246717730142,
      -1.7182338181425385,
      -1.5745541698023733,
      -1.5298567512285695,
      -1.5283137472378234,
      -1.580020563368416,
      -1.5357423091167637,
      -1.7585970883664617
    ],
    [
      -1.3510238381110729,
      -1.1744085610658364,
      -1.1769774700121125,
      -1.1997894227825447,
      -1.1430008807077385,
      -1.3529947479805442,
      -1.270581965003483,
      -1.1666222707589513,
      -1.199948287917672,
      -1.2120349009743583,
      0.0,
      -1.3862735413326974,
      -1.196807584563624,
      -1.1348389298118153,
      -1.2178393497870779,
      -1.193121007375884,
      -1.1780940506280138,
      -1.1530166707198448,
      -1.2012109355557268,
      -1.1473286202929007,
      -1.1469205168429688,
      -1.3633150521971442,
      -1.3835851114995723,
      -1.2083181702899513,
      -1.1939129306552474,
      -1.234242570775076,
      -1.1847866698021443,
      -1.1769235647859213,
      -1.3488972950532023
    ],
    [
      -1.2533590815098743,
      -1.1509048704459417,
      -1.171246627035431,
      -1.1829796048438839,
      -1.123368139042368,
      -1.1590528498719175,
      -1.162121986207308,
      -1.1856265972099347,
      -1.1903055042875041,
      -1.1779286900951584,
      -1.130291050942379,
      0.0,
      -1.2124610626455565,
      -1.1491059067837421,
      -1.143667918417552,
      -1.1579298527428683,
      -1.1604372233502187,
      -1.1893934414551832,
      -1.1892331450903733,
      -1.1062481222135743,
      -1.1555505435358546,
      -1.1703987351558307,
      -1.1801047339593136,
      -1.1506913205288098,
      -1.1910991342280322,
      -1.1912764494164232,
      -1.1956464054768532,
      -1.133640728588893,
      -1.1188237290871297
    ],
    [
      -1.287221844399751,
      -0.9996690724511652,
      -1.1259994802708768,
      -1.1053248991123341,
      -1.1129303543506575,
      -1.2850055705182055,
      -1.1497232978930454,
      -1.05946023960558,
      -1.0605531407986903,
      -1.2192962512694203,
      -1.151187050008176,
      -1.3579233630089094,
      0.0,
      -1.1099062599431322,
      -1.1564703032084551,
      -1.126534544860747,
      -1.1854481398663195,
      -1.0753690106184244,
      -1.1757654436313674,
      -1.1033289647894717,
      -1.2030458178865429,
      -1.286269789356129,
      -1.2812285095446152,
      -1.1121332177126821,
      -1.1167779674641356,
      -1.0953146355601995,
      -1.1734481635652723,
      -1.1560825530756904,
      -1.3000434596951946
    ],
    [
      -1.3498367687291701,
      -1.1092579118097161,
      -1.1014587521154875,
      -1.1066272591100297,
      -1.1295443040495097,
      -1.301013517975892,
      -1.1915942583651566,
      -1.1335376165959927,
      -1.0666578306947256,
      -1.2372263956561182,
      -1.0979173811728482,
      -1.4080039877902093,
      -1.135929419318795,
      0.0,
      -1.148994859760267,
      -1.1241869557804072,
      -1.1756893967442927,
      -1.1351557479998502,
      -1.2008260603545715,
      -1.0671950577557325,
      -1.1899793311472706,
      -1.3386643700488006,
      -1.354609331878712,
      -1.1073568845134785,
      -1.1857582045170096,
      -1.1585284463564527,
      -1.1970128232876,
      -1.1634440401095518,
      -1.3423115522181723
    ],
    [
      -1.3433085827183875,
      -1.1691750855374072,
      -1.2184168905982402,
      -1.190017811346774,
      -1.184099490915896,
      -1.3110556900192136,
      -1.2239697114348451,
      -1.2104430921463014,
      -1.185480837212599,
      -1.2312238996212252,
      -1.2171548366381857,
      -1.3352963001964235,
      -1.1903641876914284,
      -1.131146710751807,
      0.0,
      -1.1660329991545748,
      -1.2237509712757306,
      -1.1853571658557416,
      -1.2505738282462249,
      -1.1582285942223867,
      -1.23850409288447,
      -1.301554558026806,
      -1.3213772399718884,
      -1.1822056952530882,
      -1.2403391225640594,
      -1.1417422157140635,
      -1.246490340746925,
      -1.2102695348509935,
      -1.2821479414471064
    ],
    [
      -1.372072305090147,
      -1.1615886517452068,
      -1.193866036003235,
      -1.1802615717107703,
      -1.2247340776516504,
      -1.3568000685901214,
      -1.1914377108263763,
      -1.166771454570181,
      -1.142382488766709,
      -1.2382392701141334,
      -1.206569664401424,
      -1.4183137280014253,
      -1.1128773153655593,
      -1.1639954440259506,
      -1.22316561963135,
      0.0,
      -1.2447921764920455,
      -1.1834219536056676,
      -1.194372646560946,
      -1.174428171387078,
      -1.2381568601892226,
      -1.3644423693018581,
      -1.385452983027583,
      -1.1589703651263825,
      -1.2062163057961937,
      -1.1759785838435937,
      -1.2161743822848736,
      -1.1751052997259452,
      -1.3568654710985164
    ],
    [
      -1.459160478717086,
      -1.2555346229103588,
      -1.2707412978949595,
      -1.2598305504146459,
      -1.2349776301772675,
      -1.4683379892048387,
      -1.3670681965346467,
      -1.2852800587228201,
      -1.2800470958163819,
      -1.320415714051979,
      -1.242002192924043,
      -1.5255947785477457,
      -1.2992016213333466,
      -1.2306169499957367,
      -1.3091015883125545,
      -1.2959683058564992,
      0.0,
      -1.2866001924395523,
      -1.3294094093686504,
      -1.2453092314593077,
      -1.2070762313378667,
      -1.4816331051298797,
      -1.4564991580776538,
      -1.2778362922485114,
      -1.2086825468580495,
      -1.3279768319201037,
      -1.32134880007393,
      -1.2796429286753859,
      -1.5205133116634595
    ],
    [
      -1.402605936459039,
      -1.1512203002384207,
      -1.17607848723922,
      -1.1617384224683684,
      -1.1775635664974835,
      -1.350326553853172,
      -1.1805570739237135,
      -1.1176498971451545,
      -1.1183080683428601,
      -1.2481929916391936,
      -1.1966417603330666,
      -1.4890566340261087,
      -1.1174026905189944,
      -1.1651376418536454,
      -1.254993337007015,
      -1.1281254702307444,
      -1.2924651088372956,
      0.0,
      -1.2300139052832244,
      -1.1727348043225228,
      -1.2534945278680711,
      -1.3919048132583847,
      -1.4131374960876544,
      -1.1586505623092314,
      -1.2305766016403368,
      -1.1683508055870713,
      -1.2089220522536728,
      -1.165049144543427,
      -1.4272603358637082
    ],
    [
      -1.481582266188373,
      -1.2769363300703382,
      -1.303294282053356,
      -1.3519675285540396,
      -1.361792277481069,
      -1.4318035634488788,
      -1.3777064601859879,
      -1.2988968120628337,
      -1.302124919349239,
      -1.3449069029804523,
      -1.3399979129680686,
      -1.5414380779850168,
      -1.2808069421008743,
      -1.3241043022117849,
      -1.421687004433438,
      -1.3202377323916268,
      -1.3474278297548188,
      -1.3373438013859889,
      0.0,
      -1.348909524737037,
      -1.3552178241701396,
      -1.4715910428031378,
      -1.492142207167275,
      -1.3240228211291345,
      -1.3498929164696407,
      -1.3399259913973496,
      -1.3438218896750662,
      -1.267938556904276,
      -1.5303220579631425
    ],
    [
      -1.3412544279759226,
      -1.0441114357781982,
      -1.0574300136676353,
      -1.091924009366178,
      -1.099770279253921,
      -1.3029685491664547,
      -1.1686421665713953,
      -1.1032660113302093,
      -1.0786336157875824,
      -1.2110354213221477,
      -1.1407304413372787,
      -1.3328025759862072,
      -1.0978162598575405,
      -1.0813380943271516,
      -1.093089204446585,
      -1.0874347699056557,
      -1.1584337265574784,
      -1.137814866464009,
      -1.171462596657383,
      0.0,
      -1.1796671633743592,
      -1.3024234508094261,
      -1.3223109771962116,
      -1.0828112574857927,
      -1.1697739550057513,
      -1.1515169936445322,
      -1.1550693007302237,
      -1.1278178218991233,
      -1.2641519842260613
    ],
    [
      -1.6544469856128734,
      -1.466001257560565,
      -1.461119122317465,
      -1.504951738840231,
      -1.4120687643487235,
      -1.6859809377256931,
      -1.597725207710658,
      -1.4687282091500293,
      -1.5331907055949117,
      -1.5159718543129508,
      -1.4699513610670647,
      -1.7520341484308521,
      -1.5190520353217205,
      -1.483105340777421,
      -1.5557638856698477,
      -1.508333847556014,
      -1.4580326917171114,
      -1.511897518538267,
      -1.512806949041697,
      -1.431944962380154,
      0.0,
      -1.6980463913563548,
      -1.7135432890206073,
      -1.4998546350076527,
      -1.4697931574680814,
      -1.551325145757017,
      -1.5282202510418998,
      -1.4914379469789931,
      -1.6813387176688503
    ],
    [
      -1.323577527510503,
      -1.3066971412847104,
      -1.2942518723854186,
      -1.3147972821531433,
      -1.276038566003144,
      -1.2850179187273174,
      -1.3460727343820256,
      -1.2777315258578057,
      -1.3037476360576559,
      -1.263535775840787,
      -1.3167996477680879,
      -1.3622982799711036,
      -1.3054444619079446,
      -1.3170075645033188,
      -1.320763324785199,
      -1.2873246053748282,
      -1.3389506544609704,
      -1.3165488842546038,
      -1.3209103416590124,
      -1.2853279551980443,
      -1.3519041810900623,
      0.0,
      -1.247958931218798,
      -1.294618033544989,
      -1.289404425866683,
      -1.2700558577216579,
      -1.2770313422068638,
      -1.2498645195945253,
      -1.3202148252337078
    ],
    [
      -1.303436192096965,
      -1.2359243503892479,
      -1.235200961461833,
      -1.2248907032861538,
      -1.2325932905248385,
      -1.2165413785618515,
      -1.2716990505441155,
      -1.2270242287863837,
      -1.1975781694667575,
      -1.2073348827109174,
      -1.227474881629686,
      -1.2513362054538626,
      -1.2533181748638387,
      -1.2103898135399995,
      -1.2571105470410306,
      -1.2152756139600531,
      -1.2063486973869648,
      -1.2033603978066636,
      -1.2211405323945206,
      -1.2515219615639348,
      -1.2426861486565877,
      -1.2139354885324827,
      0.0,
      -1.2310901850382507,
      -1.1761261155898979,
      -1.2138620069206916,
      -1.2179505356708824,
      -1.1944448849459286,
      -1.2793363490334606
    ],
    [
      -1.2976523710112973,
      -0.9818732171278027,
      -1.0266743748612122,
      -1.0778802931548521,
      -1.09346431817732,
      -1.2050631183967524,
      -1.1129033630333893,
      -1.082194415009116,
      -1.050174758081274,
      -1.1508654921251198,
      -1.081242482933951,
      -1.3858408443566888,
      -1.0769127120319968,
      -1.0462627758542171,
      -1.1479009767095345,
      -1.047675550067703,
      -1.1712265030253728,
      -1.061063577823334,
      -1.1269485866406919,
      -1.034315988588642,
      -1.1466842473421688,
      -1.2708379615998249,
      -1.279067041064593,
      0.0,
      -1.1165545400192212,
      -1.1190674981855875,
      -1.0967487853432238,
      -1.070690471127587,
      -1.2927540632629422
    ],
    [
      -1.455204103076372,
      -1.2506160403949935,
      -1.303986949430424,
      -1.3392076220337217,
      -1.2989406109929282,
      -1.4545563261409702,
      -1.388518161399234,
      -1.3070724086777126,
      -1.2871846326718392,
      -1.340131167311919,
      -1.355910759884385,
      -1.6278756227678435,
      -1.3068473346806315,
      -1.3283552619184882,
      -1.386887319553165,
      -1.2864358207443523,
      -1.3011084672388207,
      -1.3082122825687146,
      -1.31095808003142,
      -1.3445640834300443,
      -1.331398772953952,
      -1.5141338081293272,
      -1.4848012643663093,
      -1.3002759766834093,
      0.0,
      -1.3215958861252843,
      -1.2973004175247302,
      -1.3094449234209136,
      -1.57394771297628
    ],
    [
      -1.5688278701018106,
      -1.3771986559439298,
      -1.3857649292618621,
      -1.3931176367100249,
      -1.4427182363822224,
      -1.4946859255275382,
      -1.4257290637484428,
      -1.3714310789923692,
      -1.3438539504603135,
      -1.4100355761244239,
      -1.4618700250887808,
      -1.591804389510403,
      -1.3674940336795842,
      -1.3542600340146078,
      -1.3606765044850513,
      -1.379488201386617,
      -1.4873205804025318,
      -1.4007839934141515,
      -1.4709518272530404,
      -1.3758289512052075,
      -1.5015992055188332,
      -1.538439047364915,
      -1.5543651551246465,
      -1.3784341128796997,
      -1.4373288696075013,
      0.0,
      -1.458926905177901,
      -1.3494380016050447,
      -1.5578371307862484
    ],
    [
      -1.5440203421922367,
      -1.36638609872324,
      -1.3373228741496428,
      -1.3935862719342111,
      -1.373237256505053,
      -1.4747721447970095,
      -1.4144370510474984,
      -1.3775801347723133,
      -1.3542871805248855,
      -1.3641735207149286,
      -1.3745199209254868,
      -1.642520918623655,
      -1.3950600768014574,
      -1.3989976362000824,
      -1.4856493780503506,
      -1.3741005227146277,
      -1.3944922080161402,
      -1.376160645988953,
      -1.3840264945519045,
      -1.3798201121068288,
      -1.4047425076771025,
      -1.5532695570704578,
      -1.514044425752371,
      -1.366261834470129,
      -1.3361207351926923,
      -1.4179546079078662,
      0.0,
      -1.3125131466441482,
      -1.5970362252558152
    ],
    [
      -1.3794086944110409,
      -1.2037898353254315,
      -1.2082396933801867,
      -1.2298429311577772,
      -1.2166903841465708,
      -1.3536199241039906,
      -1.286350046981933,
      -1.2036015071837876,
      -1.2172293520985722,
      -1.2381165898502184,
      -1.2747404145048524,
      -1.483280532766057,
      -1.2211137848608586,
      -1.247810236328857,
      -1.2900767626525147,
      -1.224873411472909,
      -1.304934742571041,
      -1.227048572075903,
      -1.2701846788400357,
      -1.2407895519856182,
      -1.2684908866721598,
      -1.3861004340108258,
      -1.4253919452812502,
      -1.2157649477201324,
      -1.2280953502621952,
      -1.216956741827728,
      -1.2553831302405203,
      0.0,
      -1.465050613535818
    ],
    [
      -1.3524673893462102,
      -1.305437288998409,
      -1.3457805329336536,
      -1.3432974930712585,
      -1.3100250760731085,
      -1.4021739698937377,
      -1.3514902043041015,
      -1.2991534167489707,
      -1.310241485959018,
      -1.3576451861141197,
      -1.3343087303919838,
      -1.3847460638765277,
      -1.3158435390006074,
      -1.28718300671588,
      -1.2672148528491363,
      -1.3089891107982494,
      -1.3541869076166029,
      -1.3351197493621143,
      -1.3792936464359102,
      -1.297171609495158,
      -1.349906095968632,
      -1.3529899049755214,
      -1.3798976417442492,
      -1.31873576553117,
      -1.37142664537824,
      -1.2938199173773055,
      -1.3700481013436714,
      -1.3739384206466612,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2702715391474868,
      0.25039378046099214,
      0.2547918360191823,
      0.2490024396082764,
      0.19638459353240023,
      0.19848855043821567,
      0.253037125281963,
      0.25016965776666567,
      0.2486802568314559,
      0.23525792587177863,
      0.12965376975997556,
      0.25433529719017267,
      0.2595834750969179,
      0.24436279735873123,
      0.2563586440261596,
      0.24259708937198976,
      0.257537133641343,
      0.2439836010438814,
      0.22506993474440096,
      0.23819742343287942,
      0.17245387892045594,
      0.1694074060188986,
      0.2508472117486966,
      0.24439330797041547,
      0.25071303515408294,
      0.19844978428268822,
      0.23221555350229917,
      0.20809478397061065
    ],
    [
      0.335950442019193,
      0.0,
      0.5283864557409519,
      0.5270325445597845,
      0.4900096074203517,
      0.36492987574579017,
      0.4462251960520829,
      0.5432441331302853,
      0.5343735238769729,
      0.40933993651447786,
      0.4776316150953783,
      0.23190583815902777,
      0.5977847260342757,
      0.5261552660063578,
      0.45618601872376385,
      0.5030173072220225,
      0.4433580266961217,
      0.5243048404621107,
      0.4737157386108184,
      0.5187809864660546,
      0.4699258112373359,
      0.2975289627705522,
      0.30427059135157286,
      0.5367194476789472,
      0.5243797097505507,
      0.49132049282806123,
      0.44917179569790755,
      0.5045280406640493,
      0.2832160923854885
    ],
    [
      0.3333847265886152,
      0.5874827836927403,
      0.0,
      0.5378310212359327,
      0.4844676916711612,
      0.3711391259701764,
      0.43131564685716794,
      0.5000928349700482,
      0.5327735726901526,
      0.41804052684950554,
      0.48361563659354845,
      0.2218682806857457,
      0.4760072753597302,
      0.5334822346905752,
      0.42872495247236997,
      0.5186492794525279,
      0.4258677876386716,
      0.506526255538712,
      0.40472085325689533,
      0.5586692364187698,
      0.43367259016233284,
      0.3058806492808732,
      0.25877941309338337,
      0.546295346412718,
      0.44224236833629194,
      0.42192813649129834,
      0.5140744092923786,
      0.5258328211172172,
      0.2907786780982997
    ],
    [
      0.30264155727684083,
      0.5348551020228609,
      0.559754107145046,
      0.0,
      0.5563596253904426,
      0.3754599205260132,
      0.4589443885919804,
      0.48700739931419457,
      0.5430198882662265,
      0.3959325218456742,
      0.5235797154816042,
      0.31470851214005435,
      0.5389678419519683,
      0.5770481121700113,
      0.47489971443768786,
      0.5184428836384163,
      0.4698622403948942,
      0.5075228510050622,
      0.4668782592538363,
      0.543977484291952,
      0.48796655618502505,
      0.3176540931497416,
      0.2913268060411458,
      0.5110699789023974,
      0.47073699013475556,
      0.47899035896754083,
      0.46308206019466613,
      0.5254262188543763,
      0.3364721865522049
    ],
    [
      0.35175315903742277,
      0.5429268404556757,
      0.532067130119227,
      0.5508609989398432,
      0.0,
      0.37239733573905065,
      0.4249687130811741,
      0.5372197660157909,
      0.5176821580323336,
      0.4741463628903366,
      0.5381353311760917,
      0.31044266683039634,
      0.5203650562046549,
      0.5483231156078141,
      0.4690264691728403,
      0.5236777920604696,
      0.5221031846864153,
      0.4897617175373967,
      0.4590233701264961,
      0.5383032674655395,
      0.5474379905775744,
      0.3218359611029016,
      0.31306387389863244,
      0.5392307454704723,
      0.545467215667031,
      0.44094473858872396,
      0.5144037830596593,
      0.5055419316635672,
      0.32831418138791935
    ],
    [
      0.3191910305440968,
      0.43593544972126996,
      0.4259205955008749,
      0.4156815488548462,
      0.4187504930577548,
      0.0,
      0.3413870237821779,
      0.4239975498350661,
      0.449294882157633,
      0.4042153340355059,
      0.3919269256423681,
      0.25772373712266994,
      0.44344278822426775,
      0.42760110897787795,
      0.37585194864814686,
      0.4411308283850792,
      0.33000996287610884,
      0.4012865380335042,
      0.420901886635789,
      0.40397341081238536,
      0.3375499096946508,
      0.36257994558597084,
      0.27588100721034303,
      0.43077944359879905,
      0.3895568076117397,
      0.42145111624416876,
      0.4393167331033687,
      0.45829977947033784,
      0.2869955048780244
    ],
    [
      0.29167113722139915,
      0.5067420686769459,
      0.5037854365523726,
      0.4789911646882432,
      0.4738004000057878,
      0.36707129948478645,
      0.0,
      0.5110438744077928,
      0.5636389937568898,
      0.4172200531469361,
      0.47023404508750666,
      0.24674387348058646,
      0.4755981507936522,
      0.5367146839025776,
      0.4067972906306194,
      0.5385360570673372,
      0.45073006000378535,
      0.5229636953254089,
      0.4754898587787273,
      0.46186771394914183,
      0.43334897967006714,
      0.30958924935126597,
      0.2960234493116085,
      0.5474509012420499,
      0.49182658445671446,
      0.44593261161133046,
      0.46578375995446364,
      0.47120344209772,
      0.2708713005629797
    ],
    [
      0.2983916260951447,
      0.5458122160870744,
      0.505149564325909,
      0.4740085590209837,
      0.5171901324391159,
      0.35398437415979345,
      0.45668324391651227,
      0.0,
      0.5266466627584163,
      0.43412500438089685,
      0.46989216036421566,
      0.2630035493945442,
      0.5494390497363828,
      0.48881195042153847,
      0.4426227165787959,
      0.5175107812600048,
      0.47754097515338034,
      0.5375662766398623,
      0.4668363310856505,
      0.5162504830641206,
      0.49407341969934726,
      0.32074073341510867,
      0.3144638422769257,
      0.4974702816249046,
      0.49387561031646476,
      0.5172043690412371,
      0.477473735591357,
      0.49655448115019674,
      0.3027505273508255
    ],
    [
      0.2716171706099981,
      0.5510079461580022,
      0.5433336649639628,
      0.48011784189764883,
      0.5074397640175425,
      0.3510897647382889,
      0.48890257993361974,
      0.544863032321538,
      0.0,
      0.4234277049571986,
      0.48031585190767423,
      0.26604788897032994,
      0.570836058623194,
      0.5228324904607193,
      0.450350370023918,
      0.5660507849437726,
      0.41056079506603305,
      0.5416903467548828,
      0.4363646013555069,
      0.5444012985034947,
      0.41765242089913723,
      0.3128518793390862,
      0.3091010763427515,
      0.5587679267065266,
      0.443811419686853,
      0.48221592140882863,
      0.4509382051636581,
      0.5410019919329623,
      0.26852262374777447
    ],
    [
      0.4037467813930393,
      0.593348667021572,
      0.5396156897427435,
      0.47396310829519406,
      0.4949518303542255,
      0.41984142898825727,
      0.4612460287238713,
      0.5621888262437611,
      0.5088952742992818,
      0.0,
      0.47766942987404115,
      0.2658021559519912,
      0.5405063869276414,
      0.5187762328670062,
      0.48050314736635547,
      0.5626989512666087,
      0.5102375941622437,
      0.5385813465624876,
      0.5224761010950654,
      0.4599026530164114,
      0.5069089078289619,
      0.37076112925839944,
      0.3833519828888752,
      0.5270316312290404,
      0.5717290498028442,
      0.5732720537935903,
      0.5215652376629978,
      0.56584349191465,
      0.342988712664952
    ],
    [
      0.3364881588629731,
      0.5131034359082096,
      0.5105345269619335,
      0.4877225741915012,
      0.5445111162663074,
      0.33451724899350177,
      0.4169300319705629,
      0.5208897262150947,
      0.48756370905637403,
      0.4754770959996877,
      0.0,
      0.30123845564134855,
      0.49070441241042206,
      0.5526730671622306,
      0.4696726471869681,
      0.49439098959816197,
      0.5094179463460322,
      0.5344953262542012,
      0.4863010614183192,
      0.5401833766811452,
      0.5405914801310772,
      0.32419694477690175,
      0.30392688547447366,
      0.4791938266840947,
      0.4935990663187986,
      0.45326942619897004,
      0.5027253271719017,
      0.5105884321881247,
      0.33861470192084364
    ],
    [
      0.22040375730141015,
      0.32285796836534275,
      0.3025162117758535,
      0.2907832339674006,
      0.35039469976891646,
      0.31470998893936697,
      0.31164085260397645,
      0.28813624160134976,
      0.2834573345237803,
      0.295834148716126,
      0.34347178786890553,
      0.0,
      0.2613017761657279,
      0.3246569320275423,
      0.33009492039373245,
      0.31583298606841614,
      0.31332561546106574,
      0.28436939735610123,
      0.2845296937209112,
      0.3675147165977102,
      0.3182122952754298,
      0.30336410365545374,
      0.29365810485197086,
      0.3230715182824746,
      0.2826637045832523,
      0.28248638939486126,
      0.27811643333443126,
      0.34012211022239147,
      0.35493910972415477
    ],
    [
      0.30530697618288216,
      0.5928597481314679,
      0.4665293403117563,
      0.487203921470299,
      0.47959846623197566,
      0.3075232500644276,
      0.4428055226895877,
      0.5330685809770532,
      0.5319756797839428,
      0.3732325693132128,
      0.4413417705744571,
      0.23460545757372375,
      0.0,
      0.48262256063950093,
      0.436058517374178,
      0.46599427572188623,
      0.40708068071631365,
      0.5171598099642087,
      0.41676337695126575,
      0.4891998557931614,
      0.38948300269609026,
      0.3062590312265041,
      0.3113003110380179,
      0.480395602869951,
      0.4757508531184975,
      0.49721418502243364,
      0.41908065701736086,
      0.4364462675069427,
      0.2924853608874385
    ],
    [
      0.328667459312908,
      0.569246316232362,
      0.5770454759265906,
      0.5718769689320484,
      0.5489599239925684,
      0.3774907100661862,
      0.48690996967692146,
      0.5449666114460854,
      0.6118463973473525,
      0.44127783238595986,
      0.5805868468692299,
      0.2705002402518688,
      0.5425748087232831,
      0.0,
      0.529509368281811,
      0.5543172722616709,
      0.5028148312977854,
      0.5433484800422279,
      0.4776781676875066,
      0.6113091702863456,
      0.4885248968948075,
      0.33983985799327754,
      0.3238948961633661,
      0.5711473435285996,
      0.49274602352506847,
      0.5199757816856254,
      0.48149140475447805,
      0.5150601879325263,
      0.3361926758239058
    ],
    [
      0.3173253762280528,
      0.49145887340903305,
      0.44221706834820007,
      0.4706161475996662,
      0.47653446803054433,
      0.34957826892722665,
      0.43666424751159516,
      0.45019086680013887,
      0.4751531217338414,
      0.42941005932521503,
      0.4434791223082546,
      0.32533765875001674,
      0.4702697712550119,
      0.5294872481946333,
      0.0,
      0.49460095979186547,
      0.43688298767070965,
      0.4752767930906987,
      0.4100601307002154,
      0.5024053647240536,
      0.42212986606197034,
      0.3590794009196343,
      0.3392567189745519,
      0.4784282636933521,
      0.42029483638238085,
      0.5188917432323767,
      0.4141436181995153,
      0.45036442409544675,
      0.37848601749933386
    ],
    [
      0.29637385174081676,
      0.5068575050857571,
      0.47458012082772894,
      0.4881845851201936,
      0.44371207917931343,
      0.3116460882408425,
      0.4770084460045876,
      0.5016747022607828,
      0.526063668064255,
      0.4302068867168305,
      0.46187649242953976,
      0.25013242882953857,
      0.5555688414654045,
      0.5044507128050133,
      0.4452805371996138,
      0.0,
      0.4236539803389183,
      0.48502420322529627,
      0.47407351027001776,
      0.494017985443886,
      0.4302892966417413,
      0.30400378752910573,
      0.28299317380338085,
      0.5094757917045813,
      0.4622298510347702,
      0.4924675729873702,
      0.45227177454609024,
      0.4933408571050186,
      0.31158068573244746
    ],
    [
      0.35636548462412243,
      0.5599913404308496,
      0.5447846654462489,
      0.5556954129265625,
      0.5805483331639409,
      0.3471879741363697,
      0.4484577668065617,
      0.5302459046183883,
      0.5354788675248265,
      0.4951102492892294,
      0.5735237704171654,
      0.2899311847934627,
      0.5163243420078618,
      0.5849090133454717,
      0.5064243750286539,
      0.5195576574847092,
      0.0,
      0.5289257709016562,
      0.48611655397255804,
      0.5702167318819007,
      0.6084497320033417,
      0.33389285821132875,
      0.35902680526355457,
      0.537689671092697,
      0.6068434164831589,
      0.4875491314211047,
      0.49417716326727845,
      0.5358830346658225,
      0.2950126516777489
    ],
    [
      0.35430304496414555,
      0.6056886811847639,
      0.5808304941839646,
      0.5951705589548162,
      0.5793454149257011,
      0.4065824275700127,
      0.5763519074994712,
      0.6392590842780301,
      0.6386009130803245,
      0.508715989783991,
      0.5602672210901181,
      0.267852347397076,
      0.6395062909041902,
      0.5917713395695392,
      0.5019156444161696,
      0.6287835111924402,
      0.46444387258588904,
      0.0,
      0.5268950761399602,
      0.5841741771006619,
      0.5034144535551135,
      0.36500416816479997,
      0.34377148533553026,
      0.5982584191139533,
      0.5263323797828479,
      0.5885581758361134,
      0.5479869291695119,
      0.5918598368797576,
      0.3296486455594765
    ],
    [
      0.33753065286766226,
      0.5421765889856971,
      0.5158186370026794,
      0.46714539050199577,
      0.45732064157496644,
      0.3873093556071565,
      0.4414064588700475,
      0.5202161069932016,
      0.5169879997067963,
      0.474206016075583,
      0.47911500608796675,
      0.2776748410710186,
      0.538305976955161,
      0.49500861684425046,
      0.39742591462259735,
      0.49887518666440855,
      0.47168508930121655,
      0.4817691176700465,
      0.0,
      0.47020339431899827,
      0.46389509488589575,
      0.34752187625289754,
      0.32697071188876037,
      0.49509009792690084,
      0.4692200025863946,
      0.47918692765868576,
      0.4752910293809691,
      0.5511743621517593,
      0.28879086109289287
    ],
    [
      0.25945633579286587,
      0.5565993279905903,
      0.5432807501011532,
      0.5087867544026106,
      0.5009404845148675,
      0.29774221460233385,
      0.4320685971973932,
      0.4974447524385792,
      0.5220771479812061,
      0.3896753424466408,
      0.45998032243150977,
      0.2679081877825813,
      0.502894503911248,
      0.519372669441637,
      0.5076215593222035,
      0.5132759938631328,
      0.44227703721131006,
      0.4628958973047794,
      0.42924816711140545,
      0.0,
      0.42104360039442934,
      0.2982873129593624,
      0.2783997865725769,
      0.5178995062829959,
      0.4309368087630372,
      0.4491937701242563,
      0.4456414630385648,
      0.47289294186966524,
      0.33655877954272717
    ],
    [
      0.4195390263525953,
      0.6079847544049037,
      0.6128668896480036,
      0.5690342731252378,
      0.6619172476167452,
      0.3880050742397756,
      0.47626080425481065,
      0.6052578028154394,
      0.5407953063705571,
      0.5580141576525179,
      0.604034650898404,
      0.3219518635346166,
      0.5549339766437482,
      0.5908806711880477,
      0.518222126295621,
      0.5656521644094548,
      0.6159533202483574,
      0.5620884934272017,
      0.5611790629237716,
      0.6420410495853148,
      0.0,
      0.3759396206091139,
      0.36044272294486146,
      0.574131376957816,
      0.6041928544973874,
      0.5226608662084518,
      0.5457657609235689,
      0.5825480649864756,
      0.3926472942966184
    ],
    [
      0.4113191971372434,
      0.4281995833630361,
      0.44064485226232786,
      0.4200994424946032,
      0.45885815864460255,
      0.44987880592042906,
      0.3888239902657209,
      0.4571651987899408,
      0.4311490885900906,
      0.47136094880695945,
      0.4180970768796586,
      0.3725984446766428,
      0.42945226273980186,
      0.4178891601444277,
      0.4141333998625474,
      0.4475721192729183,
      0.3959460701867761,
      0.41834784039314266,
      0.413986382988734,
      0.44956876944970214,
      0.3829925435576842,
      0.0,
      0.4869377934289485,
      0.4402786911027574,
      0.44549229878106344,
      0.4648408669260886,
      0.4578653824408827,
      0.4850322050532212,
      0.41468189941403866
    ],
    [
      0.2324281976215521,
      0.29994003932926927,
      0.30066342825668424,
      0.31097368643236334,
      0.3032710991936787,
      0.3193230111566656,
      0.26416533917440166,
      0.3088401609321334,
      0.3382862202517596,
      0.32852950700759975,
      0.30838950808883103,
      0.28452818426465454,
      0.28254621485467846,
      0.3254745761785176,
      0.2787538426774865,
      0.320588775758464,
      0.3295156923315523,
      0.33250399191185354,
      0.31472385732399655,
      0.2843424281545823,
      0.29317824106192947,
      0.32192890118603446,
      0.0,
      0.3047742046802664,
      0.35973827412861925,
      0.32200238279782556,
      0.3179138540476347,
      0.34141950477258853,
      0.2565280406850565
    ],
    [
      0.337020918610774,
      0.6528000724942685,
      0.6079989147608591,
      0.5567929964672191,
      0.5412089714447512,
      0.42961017122531886,
      0.5217699265886819,
      0.5524788746129552,
      0.5844985315407973,
      0.48380779749695146,
      0.5534308066881202,
      0.2488324452653825,
      0.5577605775900745,
      0.5884105137678541,
      0.48677231291253675,
      0.5869977395543682,
      0.46344678659669847,
      0.5736097117987373,
      0.5077247029813794,
      0.6003573010334293,
      0.4879890422799025,
      0.3638353280222464,
      0.35560624855747824,
      0.0,
      0.5181187496028501,
      0.5156057914364838,
      0.5379245042788474,
      0.5639828184944842,
      0.3419192263591291
    ],
    [
      0.3938407244690292,
      0.5984287871504077,
      0.5450578781149773,
      0.5098372055116795,
      0.550104216552473,
      0.39448850140443104,
      0.46052666614616733,
      0.5419724188676887,
      0.561860194873562,
      0.5089136602334823,
      0.4931340676610163,
      0.22116920477755775,
      0.5421974928647697,
      0.520689565626913,
      0.4621575079922362,
      0.562609006801049,
      0.5479363603065805,
      0.5408325449766866,
      0.5380867475139812,
      0.5044807441153569,
      0.5176460545914492,
      0.33491101941607404,
      0.3642435631790919,
      0.5487688508619919,
      0.0,
      0.5274489414201169,
      0.551744410020671,
      0.5395999041244877,
      0.2750971145691212
    ],
    [
      0.2846261939774877,
      0.4762554081353685,
      0.4676891348174361,
      0.4603364273692734,
      0.41073582769707584,
      0.35876813855176004,
      0.4277250003308555,
      0.4820229850869291,
      0.5096001136189847,
      0.4434184879548744,
      0.3915840389905174,
      0.2616496745688952,
      0.485960030399714,
      0.49919403006469043,
      0.49277755959424696,
      0.47396586269268126,
      0.36613348367676646,
      0.45267007066514675,
      0.38250223682625784,
      0.4776251128740907,
      0.35185485856046506,
      0.31501501671438326,
      0.2990889089546518,
      0.4750199511995985,
      0.41612519447179697,
      0.0,
      0.3945271589013972,
      0.5040160624742536,
      0.29561693329304983
    ],
    [
      0.34354109652128373,
      0.5211753399902805,
      0.5502385645638777,
      0.4939751667793093,
      0.5143241822084674,
      0.412789293916511,
      0.47312438766602205,
      0.5099813039412071,
      0.5332742581886349,
      0.5233879179985919,
      0.5130415177880336,
      0.24504052008986554,
      0.49250136191206306,
      0.488563802513438,
      0.40191206066316987,
      0.5134609159988928,
      0.4930692306973803,
      0.5114007927245674,
      0.5035349441616159,
      0.5077413266066917,
      0.48281893103641793,
      0.33429188164306267,
      0.37351701296114936,
      0.5212996042433915,
      0.5514407035208282,
      0.46960683080565424,
      0.0,
      0.5750482920693722,
      0.2905252134577052
    ],
    [
      0.37003435216631786,
      0.5456532112519272,
      0.541203353197172,
      0.5196001154195815,
      0.5327526624307879,
      0.39582312247336815,
      0.4630929995954258,
      0.5458415393935712,
      0.5322136944787865,
      0.5113264567271403,
      0.4747026320725063,
      0.26616251381130174,
      0.5283292617165001,
      0.5016328102485017,
      0.459366283924844,
      0.5245696351044498,
      0.4445083040063178,
      0.5223944745014557,
      0.479258367737323,
      0.5086534945917405,
      0.4809521599051989,
      0.3633426125665329,
      0.3240511012961085,
      0.5336780988572263,
      0.5213476963151635,
      0.5324863047496307,
      0.49405991633683843,
      0.0,
      0.2843924330415408
    ],
    [
      0.33836381479736866,
      0.38539391514516996,
      0.3450506712099253,
      0.34753371107232045,
      0.38080612807047043,
      0.28865723424984124,
      0.3393409998394774,
      0.39167778739460823,
      0.380589718184561,
      0.33318601802945924,
      0.35652247375159507,
      0.30608514026705125,
      0.37498766514297155,
      0.40364819742769886,
      0.42361635129444264,
      0.3818420933453295,
      0.33664429652697603,
      0.3557114547814646,
      0.31153755770766867,
      0.39365959464842093,
      0.3409251081749469,
      0.3378412991680575,
      0.3109335623993297,
      0.3720954386124089,
      0.3194045587653389,
      0.3970112867662734,
      0.3207831027999075,
      0.31689278349691774,
      0.0
    ]
  ],
  "row_avgs": [
    0.2315975654354649,
    0.4569068936750102,
    0.4462194333899228,
    0.46544954907595076,
    0.47426517344983754,
    0.3903797603658866,
    0.4439882191149534,
    0.45557402347673964,
    0.454861193622675,
    0.48922870825700393,
    0.46262574992822003,
    0.3065166440195734,
    0.4292623439945907,
    0.4907071399757988,
    0.4360008365520548,
    0.4388942648690301,
    0.49243999510309194,
    0.5230461603649418,
    0.4524043555552359,
    0.4380142862641308,
    0.5333907598949793,
    0.43261473119907107,
    0.3066168272950243,
    0.5042968493736635,
    0.48777797693368036,
    0.4163037108022375,
    0.4694509448095531,
    0.47147962885418787,
    0.35324078439535717
  ],
  "col_avgs": [
    0.3268315089399015,
    0.512323339641869,
    0.49135562150962325,
    0.4751659712946551,
    0.48242200376688615,
    0.35871173568464576,
    0.42832983164532396,
    0.4922866139637005,
    0.4988559492323216,
    0.43215067297900145,
    0.46445849107107273,
    0.26861068092292584,
    0.49047864995387763,
    0.49502371990683225,
    0.4389657269448674,
    0.4931771591038107,
    0.43741440362701034,
    0.4789487561603658,
    0.4419496499778412,
    0.4917461093792665,
    0.43896873811054293,
    0.32572969654246886,
    0.3197746157686407,
    0.48951282758248604,
    0.46480344058553275,
    0.4658724717428994,
    0.45092033548689275,
    0.4868828515163083,
    0.31188293700629677
  ],
  "combined_avgs": [
    0.2792145371876832,
    0.4846151166584396,
    0.46878752744977303,
    0.47030776018530296,
    0.47834358860836185,
    0.3745457480252662,
    0.4361590253801387,
    0.47393031872022007,
    0.4768585714274983,
    0.4606896906180027,
    0.4635421204996464,
    0.28756366247124965,
    0.45987049697423416,
    0.49286542994131555,
    0.4374832817484611,
    0.4660357119864204,
    0.4649271993650511,
    0.5009974582626537,
    0.44717700276653854,
    0.46488019782169865,
    0.4861797490027611,
    0.37917221387076994,
    0.3131957215318325,
    0.49690483847807476,
    0.47629070875960655,
    0.4410880912725684,
    0.46018564014822294,
    0.4791812401852481,
    0.33256186070082694
  ],
  "gppm": [
    628.2149016632593,
    563.154831048202,
    575.7529987437264,
    582.6931914677,
    573.8837242735927,
    631.198951974493,
    604.5247760694546,
    571.7924128598486,
    569.9269452960908,
    598.7438664514777,
    588.9136365079088,
    682.8039901658819,
    575.9771615204115,
    572.9850257510042,
    597.7020321782828,
    573.7433976858739,
    599.0861336965467,
    581.0223058019213,
    596.1106397485116,
    575.9825653415593,
    596.5624504088673,
    654.2910456283695,
    657.700186651536,
    576.7409545002961,
    584.5088970145777,
    584.0653111517571,
    593.5218956169929,
    577.8187201635086,
    654.2750514314756
  ],
  "gppm_normalized": [
    1.4538085351506354,
    1.256734680318741,
    1.2821112557675043,
    1.2984353760969258,
    1.2775110391759463,
    1.4052309662135023,
    1.3482209696723804,
    1.2736029781999456,
    1.2665674834287348,
    1.3348362176353166,
    1.310292947745624,
    1.5246761309150418,
    1.2858862099759,
    1.2781040474132226,
    1.3299795018824803,
    1.2804683275123525,
    1.334993294171128,
    1.2930936081540645,
    1.3286830915915693,
    1.2751241748638227,
    1.324410546987064,
    1.455202997521203,
    1.4671717228061953,
    1.2765882816461236,
    1.2981406312254957,
    1.304300711957634,
    1.318572532777369,
    1.2913314768672974,
    1.4531684671543172
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487
  ],
  "response_lengths": [
    2805,
    2711,
    2265,
    2546,
    2717,
    2690,
    2626,
    2610,
    2513,
    2407,
    2365,
    2136,
    2509,
    2477,
    2636,
    2631,
    2293,
    2303,
    2511,
    2436,
    2142,
    2012,
    2383,
    2488,
    2576,
    2476,
    2301,
    2451,
    2750
  ]
}