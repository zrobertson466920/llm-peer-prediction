{
  "example_idx": 13,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSYNCHRONIZED PRUNING FOR EFFICIENT CONTRASTIVE SELF-SUPERVISED LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nVarious self-supervised learning (SSL) methods have demonstrated strong capability in learning visual representations from unlabeled data. However, the current state-of-the-art (SoTA) SSL methods largely rely on heavy encoders to achieve comparable performance as the supervised learning counterpart. Despite the well-learned visual representations, the large-sized encoders impede the energyefficient computation, especially for resource-constrained edge computing. Prior works have utilized the sparsity-induced asymmetry to enhance the contrastive learning of dense models, but the generality between asymmetric sparsity and self-supervised learning has not been fully discovered. Furthermore, transferring the supervised sparse learning to SSL is also largely under-explored. To address the research gap in prior works, this paper investigates the correlation between intraining sparsity and SSL. In particular, we propose a novel sparse SSL algorithm, embracing the benefits of contrastiveness while exploiting high sparsity during SSL training. The proposed framework is evaluated comprehensively with various granularities of sparsity, including element-wise sparsity, GPU-friendly N :M structured fine-grained sparsity, and hardware-specific structured sparsity. Extensive experiments across multiple datasets are performed, where the proposed method shows superior performance against the SoTA sparse learning algorithms with various SSL frameworks. Furthermore, the training speedup aided by the proposed method is evaluated with an actual DNN training accelerator model.\n\n1\n\nINTRODUCTION\n\nThe early empirical success of deep learning was primarily driven by supervised learning with massive labeled data, e.g., ImageNet (Krizhevsky et al., 2012). To overcome the labeling bottleneck of deep learning, learning visual representations without label-intensive datasets has been widely investigated (Chen et al., 2020a; He et al., 2020; Grill et al., 2020; Zbontar et al., 2021). The recent self-supervised learning (SSL) methods have shown great success and achieved comparable performance to the supervised learning counterpart. The common property of various SSL designs is utilizing different augmentations from the original images to generate contrastiveness, which requires duplicated encoding with wide and deep models (Meng et al., 2022). The magnified training effort and extensive resource consumption make the SSL-trained encoder infeasible for on-device computing (e.g., mobile devices). The contradiction between label-free learning and extraordinary computation cost limits further applications of SSL, also necessitating efficient sparse training techniques for self-supervised learning.\n\nFor supervised learning, sparsification (a.k.a. pruning) has been widely explored, aiming to reduce computation and memory costs by removing unimportant parameters during training or fine-tuning. Conventional supervised pruning explores weight sparsity based on a pre-trained model followed by additional fine-tuning to recover the accuracy (Han et al., 2016). For self-supervised learning, recent work (Chen et al., 2021) also sparsified a pre-trained dense SSL model for the downstream tasks with element-wise pruning. In addition to the fine-grained sparsity, MCP (Pan et al., 2022) exploited the filter-wise sparsity on the MoCo-SSL (He et al., 2020) model. Both of these sparse SSL works (Chen et al., 2021; Pan et al., 2022) exploit sparsity based on the pre-trained dense model. However, compared to supervised learning, obtaining the pre-trained model via SSL requires a significant amount of additional training effort (∼200 epochs vs. ∼1,000 epochs). Therefore, exploring post-training sparsity via fine-tuning is not an ideal solution for efficient SSL.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (a) Applying self-damaging scheme (Jiang et al., 2021) to SSL. (b) Applying prune-andregrow scheme (Liu et al., 2021) to SSL. (c) Proposed contrastiveness-aware sparse training.\n\nOn the other hand, sparsifying the model during supervised training (Dettmers & Zettlemoyer, 2019; Evci et al., 2020) has emerged as a promising technique to elevate training efficiency while obtaining a sparse model. To accurately localize the unimportant parameters, prior works investigated various types of important metrics, including gradient-based pruning (Dettmers & Zettlemoyer, 2019) and the “prune-regrow” scheme (Liu et al., 2021). Compared to the post-training sparsification methods, in-training sparsification for supervised training has achieved memory/computation reduction as well as speedup of the training process. However, exploiting in-training sparsity for SSL models that are trained from scratch is still largely under-explored.\n\nMore recently, SDCLR (Jiang et al., 2021) proposed the sparsified “self-damaging” encoder, which generates the “sparse-dense” SSL by exploiting fixed high sparsity on one contrastive path (e.g., offline encoder), while keeping the counterpart dense (e.g., online encoder), as shown in Figure 1(a). Such “sparse-dense” SSL architecture helps to enhance contrastive learning, leading to improved performance with the non-salient and imbalanced samples. Nevertheless, it mainly focuses on the performance enhancement of the SSL-trained dense model (i.e., SimCLR (Chen et al., 2020a)), and whether such a “sparse-dense” asymmetric learning scheme works in other SSL methods remains unclear. On the other hand, the compatibility of the existing SoTA sparse training techniques (Dettmers & Zettlemoyer (2019); Evci et al. (2020); Liu et al. (2021)) is also ambiguous to SSL. As shown in Figure 1(b), they require to frequently prune and regrow the model architecture during SSL training, while SDCLR maintains a fixed “sparse-dense” architecture during the entire training process. The under-explored sparse contrastiveness and expensive self-supervised learning inspire us to investigate the following question: How to efficiently sparsify the model during self-supervised training with the awareness of contrastiveness?\n\nTo address this question, we present Synchronized Contrastive Pruning (SyncCP), a novel sparse training algorithm designed for self-supervised learning. To maximize the energy efficiency of SSL training, SyncCP exploits in-training sparsity in both encoders with high compatibility of SSL. The main contributions of this work are:\n\n• We first discover the limitations of the sparsity-induced asymmetric SSL in SDCLR (Jiang et al., 2021) and show that the sparsity-induced “sparse-dense” asymmetric architecture is not universally applicable for various SSL schemes.\n\n• We demonstrate the incompatibility of the SoTA “prune-and-regrow” sparse training method for SSL. Specifically, we formalize the iterative architectural changes caused by applying “prune-and-regrow” to SSL, named as architecture oscillation, and observe that frequently updating the pruning candidates lead to larger architecture oscillation, which further hinders the performance of self-supervised learning.\n\n• We present SyncCP, a new sparse training algorithm designed for self-supervised learning. SyncCP gradually exploits high in-training sparsity in both encoders with contrastive\n\n2\n\nEMA / CopyConsistentContrastivenessDense Online EncoderSparse Offline EncoderLStatic High SparsityLossEMA / CopySparsity Online EncoderSparse Offline EncoderLLossUnstableContrastivenessRegrowEMA / CopySparsity Online EncoderSparse Offline EncoderLLossConsistentContrastivenessSynchronizedSpars. Gap: ΔsSelf-Damaging SSLApplying SoTAPrune-and-regrow to SSLThis work-SSL Friendly -Low HW Compatibility-Unproved Generality-SSL Unfriendly-Low HW Compatibility-High Sparsity-SSL Friendly-High HW Compatibility-High Sparsity(a)(b)(c)Under review as a conference paper at ICLR 2023\n\nsynchronization and optimally-triggered sparsification, maximizing the training efficiency without hurting the contrastiveness of SSL.\n\n• SyncCP is a general sparse training method for SSL which is compatible with various granularities of sparsity, including element-wise pruning, N :M sparsity, and structured sparsity designed for a custom hardware accelerator.\n\n• We validated the proposed method against previous SoTA sparsification algorithms on CIFAR-10, CIFAR-100, and ImageNet datasets. Across various SSL frameworks, SyncCP consistently achieves SoTA accuracy in all experiments.\n\n2 RELATED WORKS\n\n2.1 CONTRASTIVE SELF-SUPERVISED LEARNING\n\nSelf-supervised learning recently has gained popularity due to its ability to learn visual representation without labor-intensive labeling. Specifically, pioneering research works (He et al., 2020; Chen et al., 2020a) utilize the contrastive learning scheme (Hadsell et al., 2006) that aims to group the correlated positive samples while repelling the mismatched negative samples (Oord et al., 2018). The performance of the contrastive learning-based approaches largely depends on the contrastiveness between the positive and negative samples, which requires large-sized batches to support. As indicated by SimCLR (Chen et al., 2020a), the performance of SSL is sensitive to the training batch size, and the inflated batch size elevates training cost. MoCo (He et al., 2020; Chen et al., 2020b) alleviates such issue with queue-based learning and momentum encoder, where the extensive queueheld negative samples provide proficient contrastiveness, and the slow-moving average momentum encoder derives consistent negative pairs. BYOL (Grill et al., 2020) simplifies and outperforms the prior works by only learning positive samples, while the online latent features are projected by an additional predictor qθ:\n\nonline prediction = qθ(gθ(fθ(X))) offline target = gξ(fξ(X ′))\n\n(1)\n\n(2)\n\nWhere f and g represent the encoder and projector of online (θ) and offline (ξ) paths with augmented input X and X ′, respectively. The predictor qθ generates an alternative view of the projected positive samples, and the offline momentum encoder provides consistent encoding for contrastive learning. Overall, salient and consistent contrastiveness is essential to contrastive self-supervised learning.\n\n2.2 SPARSE TRAINING\n\nDNN sparsification has been widely investigated under the supervised learning domain, which can be generally categorized based on the starting point of sparsification. Early works mainly focus on post-training sparsification (Han et al., 2016; Evci et al., 2020; Jayakumar et al., 2020), which removes the weights from the pre-trained model and then recovers the accuracy with subsequent fine-tuning. Other works exploit weight sparsity prior to the training process (Wang et al., 2019; Lee et al., 2018), and the resultant model is trained with the sparsified architecture.\n\nIn contrast to post-training or pre-training sparsification, exploiting sparsity during training generates the compressed model with one-time training from scratch, eliminating the requirement of a pre-trained model or extensive searching process. With the full observability of the training process, the magnitude of the gradient can be used to evaluate the model reflection with the exploited sparsity. Motivated by this, GraNet (Liu et al., 2021) utilizes the “prune-and-regrow” technique to periodically remove the unimportant non-zero weights from the sparse model and then regrow certain pruning candidates back. Given the targeted sparsity st and total prune ratio rt at iteration t, unimportant weights w are removed based on the Top-K magnitude scores:\n\nSubsequently, the sensitive weights are re-grown back based on the reflection of gradient gt:\n\n′\n\nw\n\n= TopK(|w|, rt)\n\n′\n\nw = w\n\n+ TopK(gt\n\ni!=w′ ,rt−st\n\n)\n\n(3)\n\n(4)\n\nSince the gradient gt indicates the instant model sensitivity at iteration t, the optimal sparse model architecture can be varied between two adjacent pruning steps.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2.3 CONTRASTIVE LEARNING WITH SPARSITY-INDUCED ASYMMETRY\n\nAs introduced in Section 2.1, salient and consistent contrastiveness is essential for contrastive SSL, where the contrastiveness can be constructed via negative samples or the auxiliary predictors (Grill Inspired by (Hooker et al., 2019), SDCLR (Jiang et al., 2021) amplifies the conet al., 2020). trastiveness by pruning one encoder of SimCLR (Chen et al., 2020a) while keep the identical twin dense. Such “sparsity-induced asymmetry” elevates the performance of SSL with the improved performance of the dense model on the long-tailed data samples. However, SDCLR (Jiang et al., 2021) is not designed for model compression or efficiency improvements. Furthermore, the generality of such sparsity-induced asymmetry remains under-explored in other SSL frameworks.\n\n3 CHALLENGE OF SPARSE SELF-SUPERVISED LEARNING\n\n3.1 LIMITATIONS OF SPARSITY-INDUCED ASYMMETRY\n\nIt has been shown in SDCLR (Jiang et al., 2021) that the sparsity-induced “sparse-dense” asymmetry is beneficial to contrastive SSL. SDCLR (Jiang et al., 2021) is specifically built upon the SimCLR (Chen et al., 2020a) framework with shared encoders, where the pruned architectures have the dense twin in the mirrored contrastive encoder. However, the generality of sparsity-induced asymmetry remains unproved in other SSL methods, which motivates us to investigate the question:\n\nQuestion 1: For contrastive self-supervised learning with non-identical encoders, will the sparsityinduced asymmetric encoders still result in elevated performance for contrastive learning?\n\nTo answer the above question, we use MoCo-V2 (Chen et al., 2020b) and follow the procedure of SDCLR (Jiang et al., 2021) to generate a highly-sparse online encoder prior to the training process. Given the online and offline (momentum) encoder θ and ξ with weights Wθ and Wξ, we have:\n\nonline output = gθ(fθ(X ∗ (Mθ · Wθ))) offline output = gξ(fξ(X ′))\n\n(6) The online encoder mask Mθ produces a sparse online encoder with initialized element-wise sparsity (Han et al., 2016) at 90%, while the offline encoder is updated by exponential moving average (EMA). The gradient-based update of the online encoder keep recovering the performance drop caused by the high sparsity mask. Following the setup of SDCLR (Jiang et al., 2021), the sparsity is periodically updated at the beginning of each epoch. Table 1 summarizes the linear evaluation accuracy on the CIFAR-10 dataset with different static online sparsity values. As opposed to the performance of SimCLR in (Jiang et al., 2021), directly applying the high sparsity-based perturbation to MoCo-V2 (Chen et al., 2020b) is challenging, and leads to considerable performance degradation. Reversing the sparsity between online and offline encoder also shows the similar results, as presented in Appendix D.\n\n(5)\n\nTable 1: Largely degraded performance of MoCo-V2 (Chen et al., 2020b) with self-damaging SSL (Jiang et al., 2021) on CIFAR-10 dataset.\n\nResNet-18 Encoder Fixed Sparsity Linear Eval. Acc (%)\n\nOnline 90% 88.72 (-3.41%)\n\nDense Model Acc. = 92.09% Momentum 0% 87.68 (-4.31%)\n\nOnline 50% 92.10 (+0.01%)\n\nMomentum 0% 92.07 (-0.02%)\n\nSummarizing these empirical results, our main observation is:\n\nObservation 1: Compared to the online encoder, the EMA-updated momentum encoder has the delayed architecture, which makes it unqualified to be the “competitor” as SDCLR (Jiang et al., 2021). The directly-applied high sparsity overshoots the asymmetric learning, leading to degraded self-supervised learning.\n\n3.2 FREQUENT ARCHITECTURE CHANGING HINDERS SELF-SUPERVISED LEARNING\n\nAs depicted in Eq. 4, the “prune-and-regrow” scheme such as GraNet (Liu et al., 2021) uses instant gradient magnitude to indicate the model sensitivity after magnitude pruning, removing the unimportant and insensitive weights while gradually achieving high sparsity. Observation 1 demonstrates the incompatibility of the directly-applied high sparsity in SSL, then the following question raises:\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Sparse training with “prune-and-regrow” scheme on BYOL (Grill et al., 2020).\n\nBYOL (Grill et al., 2020)\n\nCIFAR-10 Acc (%)\n\nResNet-18\n\nDense Model Acc. = 92.42%\n\nOnline Encoder Sparsity\n\n0%→80%\n\n0%→90%\n\nOnline Linear Eval. Acc (%)\n\n91.20±0.02\n\n90.13±0.06\n\nMomentum Encoder Sparsity\n\n0%→50%\n\n0%→60%\n\nMomentum Linear Eval Acc. (%)\n\n91.31±0.07\n\n90.09±0.04\n\nFigure 2: (a) Layer-wise oscillation at different steps of pruning. “SC” stands for the shortcut connection of ResNet-18 model. (b) Gradually-increased sparsity of GraNet (Liu et al., 2021) leads to inconsistent asymmetry.\n\nQuestion 2: If we apply the gradually-increased sparsity for both encoders, will the “prune-andregrow” scheme also be feasible for self-supervised learning? To address Question 2, we use the SoTA GraNet (Liu et al., 2021) as the example algorithm to exploit in-training sparsity on both encoders of BYOL (Grill et al., 2020), where the regrowing process is only applied to the online encoder. Starting with the dense models, we gradually prune the online and offline encoders to 90% and 60% sparsity in an element-wise fashion with periodically-updated sparsity. For sparse SSL training, the results of such gently-increased sparsity scheme reported in Table 2 outperforms those by (Jiang et al., 2021) (Table 1) by a significant margin. However, the state-of-the-art supervised pruning algorithm still incurs 2.3% linear evaluation accuracy degradation with SSL on the CIFAR-10 dataset.\n\nCompared to the self-damaging SSL with fixed sparsity (Jiang et al., 2021), the “prune-and-regrow” method keeps swapping the pruning candidates to minimize the model sensitivity, oscillating the encoder architecture during training. We quantify such architecture oscillation by XORing the masks generated from magnitude pruning MM P and gradient-based regrow Mg:\n\nGcor = MM P ⊕ Mg ∈ {0, 1}\n\n(7)\n\nUnder the same sparsity ratio, the number of “1”s in Gcor indicates the amount of architecture oscillation caused by the gradient-based regrow. During the early stage of training, almost all the magnitude pruning candidates are replaced by the regrowing process, as shown in Figure 2(a). The high degree of architecture oscillation implies drastic changes in the sparse model architecture. In the meantime, gradually sparsifying two encoders with different target sparsity further destroys the consistency of self-supervised learning, as shown in Figure 2(b). As a result, we have the following observation for Question 2:\n\nObservation 2: Sparsifying the model with frequently changing architecture hinders the contrastiveness and consistency of self-supervised learning and leads to degraded encoder performance.\n\nAs shown in Observation 1 and Observation 2, high sparsity-induced asymmetry is not directlyapplicable to sparse SSL, while the consistency requirements of SSL negates the plausibility of gradual sparsity increment. The dilemma between self-supervised learning and sparse training derives the following challenge:\n\nHow can we efficiently sparsify the model during self-supervised training while maximizing the benefits of the sparsity-induced asymmetry?\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 METHOD\n\nTo address the above challenge, we propose Synchronized Contrastive Pruning (SyncCP), which successfully alleviates the contradiction between the needs of high sparsity and the requirements of consistency in self-supervised learning.\n\n4.1 SYNCHRONIZED SPARSIFICATION (SYNCS)\n\nThe rationale behind the sparsity-induced asymmetric SSL is that the perturbation generated by the pruned encoder elevates the difference between contrastive features. As indicated by Observation 1 and Table 1, the high sparsity-induced asymmetry is not universally applicable, but the SSL can be rewarded from the asymmetry incurred by lower sparsity (e.g., 50%), where the SSL-trained sparse and dense encoders exhibit negligible accuracy degradation compared to the baseline. Motivated by this, we propose the Synchronized Sparsification (SyncS) technique to exploit sparsity in both contrastive encoders. Given the online and offline (momentum) encoder θ and ξ with weights Wθ and Wξ, the in-training sparsification can be expressed as:\n\nonline output = gθ(fθ(X ∗ (Mθ · Wθ))) offline output = gξ(fξ(X ′ ∗ (Mξ · Wξ))) (9) Where Mθ and Mξ represent the online and offline (momentum) sparse masks with sparsity sθ and sξ. The proposed SyncS scheme gradually exploits the sparsity in both encoders while maintaining a consistent sparsity gap ∆s between them during SSL training. At each pruning step t, we have:\n\n(8)\n\nθ = sf st\n\nξ = sf st θ − st\n\ns.t |st\n\n)3\n\nθ + (si\n\nθ )(1 −\n\nθ − sf\n\nt − t0 n∆t t − t0 n∆t ξ| = ∆s, for t ∈ {t0, t0 + ∆t, ..., t0 + n∆t}\n\nξ − sf\n\nξ )(1 −\n\nξ + (si\n\n)3\n\n(10)\n\n(11)\n\n(12) The exponent controls the speed of sparsity increment, we adopt the sparsity schedule of Eq. 10-12 from (Liu et al., 2021) to minimize the impact of the parameter tuning. The synchronized sparsity increment with the constraints of ∆s prevents the exceeding asymmetry between contrastive encoders while minimizing the distortion caused by the changing sparsity. In practice, ∆s is treated as a tunable parameter which impacts the final sparsity of both online and offline encoders. To guarantee the consistency of the contrastive sparsity, both sθ and sξ are initialized by Erdos Renyi Kernel (ERK) (Evci et al., 2020), and with respect to ∆s, we evaluate the impact of ∆s in Appendix.\n\n4.2 CONTRASTIVE SPARSIFICATION INDICATOR (CSI)\n\nAchieving high sparsity requires gentle sparsity increment, but as presented in Observation 2, the inconsistent architecture difference deteriorates the contrastiveness of SSL. On the other hand, the popular EMA-based update (He et al., 2020) allows the momentum encoder to generate consistent latent representation, but the lagged architecture makes the momentum encoder become an unqualified “competitor” to the online encoder, which violates the findings of (Jiang et al., 2021). To address such conflict, we propose the Contrastive Sparsification Indicator (CSI), a simple-yeteffective method that automatically selects the starting point of sparsity increment based on the learning progress of SSL. During the SSL training, CSI first generates the pseudo pruning decisions of both encoders based on element-wise magnitude pruning with target sparsity sf\n\nθ and sf ξ :\n\nM∗\n\nM∗\n\nθ = 1{|wθ| ∈ TopK(|wθ|, sf ξ = 1{|wξ| ∈ TopK(|wξ|, sf\n\nθ )} ξ )}\n\n(13)\n\n(14)\n\nWhere 1 represents the indicator function, and the resultant pseudo masks of M∗ ξ will not be applied to the weights. Subsequently, CSI XORs the pseudo pruning masks to generate G (Eq. 15), and the percentage of “1”s in G is equivalent to the architecture inconsistency I (Eq. 16), where |G| represents the total number of element in G. Instead of using cosine similarity, the bit-wise XOR can be easily implemented on hardware to quantify the architecture difference during training. θ ⊕ M∗\n\nθ and M∗\n\n(15)\n\nG = M∗ I = 1 − (cid:0)(cid:88) 1{G = 0}(cid:1)/|G|\n\nξ\n\n(16)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Sparse BYOL training process (a) without SyncS and (b) with SyncS.\n\nTable 3: Performance comparison of BYOL on CIFAR-10 dataset with/without SyncS.\n\nMethod\n\nOnline Encoder Spars. Momentum Encoder Spars. Online Linear Eval. Acc. (%)\n\nCSI + SyncS\n\nCSI Only\n\n50%→80%\n\n50%→80%\n\n20%→50%\n\n20%→20% (Fixed)\n\n92.24%\n\n91.64%\n\nGiven the sparsity gap ∆s defined by SyncS, CSI activates the sparsity increment when I equals to ∆s, and this moment is defined as the CSI checkpoint. In other words, when the architecture difference between online and offline encoders is mainly caused by the sparsity difference, it is the optimal moment to start exploiting the in-training sparsity with the gradually-increased sparsity. With the ability to automatically select the starting point of sparsity increment, the proposed CSI method automatically sparsifies the model with the full awareness of the SSL process. For the SSL framework with shared encoder (Zbontar et al., 2021), the architecture inconsistency I is computed based on the sparse architecture of two consecutive epochs, and the sparsity increment is activated when I is less than a pre-defined threshold τ (e.g., τ = 0.1).\n\nFigure 3 shows the sparsification scheme with and without SyncS. As summarized in Table 3, holding the sparse momentum architecture after the CSI checkpoint interrupts the consistency between the online and momentum encoders. Although the momentum encoder retains the low sparsity at 20%, the absence of consistent asymmetry from synchronized contrastive pruning (SCP) causes the degraded model performance.\n\nOn top of the proposed SyncS and CSI schemes, we adopt the prune-and-regrow scheme (Liu et al., 2021) with modifications to exploit sparsity during SSL training. To further alleviate the contrastiveness oscillation caused by changing sparsity, we slowly average the gradient magnitude by exponential moving average (EMA) with gentle momentum, instead of using the instant score. The detailed pseudo code of the proposed algorithm is summarized in Appendix C.\n\n5 EXPERIMENTAL RESULTS\n\nIn this section, we validate the proposed sparse training scheme and compare it with the current SoTA sparse training schemes. Unlike the work by (Jiang et al., 2021), the proposed scheme exploits in-training sparsity in both contrastive paths (encoders) and aims to achieve energy-efficient self-supervised learning. The proposed method is applied to multiple mainstream SSL frameworks, including EMA-based methods (Chen et al., 2020b; Grill et al., 2020) and SSL with shared encoder (Zbontar et al., 2021). The linear evaluation accuracy and training cost reduction are reported for multiple datasets, including CIFAR-10, CIFAR-100, and ImageNet-2012. Furthermore, this work exploits in-training sparsity with various sparsity granularities, including element-wise sparsity, N :M sparsity (Zhou et al., 2020), and structural sparsity for a custom hardware accelerator.\n\nCIFAR-10 and CIFAR-100 Table 4 summarizes the linear evaluation accuracy of the proposed method on CIFAR-10 and CIFAR-100 datasets with element-wise sparsity. We use ResNet-18 (1×) as the backbone and train the model from scratch by 1,000 epochs. Following the typical high sparsity results reported with supervised learning, we report the model performance with 80% and 90% target sparsity. To sparsify both encoders during SSL training, we initialize the sparsity of online and offline (momentum) encoders as 30% and 0%, where the ∆s is set to 30%. The initialized sparse\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Linear evaluation comparison on CIFAR-10/100 datasets with element-wise sparsity.\n\nDataset\n\nEncoder\n\nCIFAR-10 Acc (%)\n\nResNet-18 (1×)\n\nCIFAR-100 Acc (%)\n\nResNet-18 (1×)\n\nElement-wise Sparsity\n\n0%\n\n80%\n\n90%\n\n0%\n\n80%\n\n90%\n\nThis work\n\n92.09\n\n91.77±0.08\n\n91.31±0.04\n\n67.72\n\n67.56±0.04\n\n66.78±0.07\n\nMoCo-V2 (Chen et al., 2020b)\n\nGraNet-MoCo (Liu et al., 2021)\n\nSD-MoCo (Jiang et al., 2021)\n\n92.09\n\n90.66±0.07\n\n90.05±0.08\n\n67.72\n\n67.17±0.05\n\n64.92±0.06\n\n92.09\n\n90.26±0.05\n\n87.68±0.06\n\n67.72\n\n65.04±0.04\n\n61.33±0.05\n\nThis work\n\n92.42\n\n92.26±0.06\n\n92.03±0.05\n\n68.80\n\n68.69±0.06\n\n67.73±0.04\n\nBYOL (Grill et al., 2020)\n\nGraNet-BYOL (Liu et al., 2021)\n\nSD-BYOL (Jiang et al., 2021)\n\n92.42\n\n91.20±0.02\n\n90.13±0.03\n\n68.80\n\n67.17±0.05\n\n65.85±0.08\n\n92.42\n\n90.33±0.07\n\n87.38±0.04\n\n68.80\n\n66.13±0.08\n\n62.20±0.10\n\nThis work\n\n91.74\n\n91.67±0.09\n\n90.84±0.07\n\n68.62\n\n68.75±0.13\n\n68.48±0.12\n\nBarlow Twins (Zbontar et al., 2021)\n\nGraNet-Barlow (Liu et al., 2021)\n\nSD-Barlow (Jiang et al., 2021)\n\n91.74\n\n91.23±0.03\n\n90.44±0.12\n\n68.62\n\n68.40±0.10\n\n68.15±0.14\n\n91.74\n\n90.09±0.03\n\n88.41±0.07\n\n68.62\n\n66.42±0.07\n\n61.77±0.04\n\nTable 5: ImageNet-2012 accuracy and training cost comparison with SoTA works on ResNet-50 with BYOL (Grill et al., 2020).\n\nImageNet\n\nTop-1 Accuracy (%)\n\nFLOPS (Training)\n\nFLOPS (Inference)\n\nTop-1 Accuracy (%)\n\nFLOPS (Training)\n\nFLOPS (Inference)\n\nDense Baseline\n\n68.12\n\n7.94e+18 (1×)\n\n8.18e+09 (1×)\n\n68.12\n\n7.94e+18 (1×)\n\n8.18e+09 (1×)\n\nElement-wise Sparsity\n\nModel Size (MB)\n\nThis work\n\nBYOL (Grill et al., 2020)\n\nGraNet-BYOL (Liu et al., 2021)\n\nSD-BYOL (Jiang et al., 2021)\n\n67.02\n\n65.45\n\n63.02\n\n80%\n\n20.5\n\n0.62×\n\n0.57×\n\n0.68×\n\n0.33 ×\n\n0.32 ×\n\n0.34 ×\n\n65.67\n\n64.22\n\n60.56\n\n90%\n\n10.2\n\n0.56×\n\n0.51×\n\n0.63×\n\n0.22×\n\n0.20×\n\n0.21×\n\nThe encoders are trained from scratch on the ImageNet dataset with 300 epochs. The FP32 dense ResNet-50 model requires 102MB storage.\n\nencoders reduce the overall memory footprint throughout the entire training process. We rigorously transfer the SoTA GraNet Liu et al. (2021) to SSL based on its open-sourced implementation, the proposed method outperforms GraNet-SSL with 1.26% and 1.86% accuracy improvements on CIFAR-10 and CIFAR-100 datasets, respectively. In all experiments, we report the average accuracy with its variation in 3 runs.\n\nIn addition to element-wise sparsity, the recent Nvidia Ampere architecture is equipped with the Sparse Tensor Cores to accelerate the inference computation on GPU with N :M structured finegrained sparsity (Zhou et al., 2020), where the N dense elements remain within an M -sized group. Powered by the open-sourced Nvidia-ASP library, SyncCP sparsifies BYOL training (Grill et al., 2020) by targeting 100% N :M sparse groups in online encoders. Starting from scratch, the percentage of the N :M sparse groups is initialized as 30% and 0% in online and momentum encoders with ∆s=30%. After the CSI checkpoint, the percentage of N :M groups gradually increases. Appendix A describes the detailed pruning algorithm of N :M sparsification. Table 6 summarizes linear evaluation accuracy and inference time reduction on the CIFAR-10 and CIFAR-100 datasets. The resultant model achieves up to 2.08× inference acceleration with minimum accuracy degradation. The inference time is measured on an Nvidia 3090 GPU with FP32 data precision.\n\nImageNet-2012 Since the BYOL (Grill et al., 2020) learning scheme achieves the best performance with CIFAR datasets, we further evaluate the proposed method with ResNet-50 on ImageNet based on the BYOL framework (Grill et al., 2020). Following the typical high sparsity results reported in Table 4, we report the model performance with 80% and 90% element-wise sparsity. The data augmentation setup is adopted from the open-sourced library (Costa et al., 2022). Starting from scratch, the model is trained by 300 epochs, where both online and momentum encoders are initialized by ERK with ∆s = 30%. While we believe a more fine-grained hyperparameter tuning and extended training efforts could lead to better accuracy, we choose the above scheme for simplicity and reproducibility. Table 5 shows the comparison of linear evaluation accuracy on ImageNet-2012\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Linear evaluation accuracy comparison on CIFAR-10/100 datasets with N :M structured fine-grained sparsity.\n\nDatasets\n\nEncoder\n\nCIFAR-10 Acc (%)\n\nCIFAR-100 Acc (%)\n\nResNet-18 (1×)\n\nResNet-18 (1×)\n\nN :M Sparse Pattern\n\n2:4\n\n1:4\n\n2:4\n\n1:4\n\nMoCo-V2 (Chen et al., 2020b)\n\nBYOL (Grill et al., 2020)\n\nBarlow Twins (Zbontar et al., 2021)\n\n91.99±0.07\n\n91.53±0.04\n\n67.58±0.05\n\n67.11±0.05\n\n92.61±0.05\n\n91.83±0.02\n\n68.69±0.02\n\n68.09±0.07\n\n91.68±0.04\n\n90.97±0.03\n\n68.26±0.07\n\n68.19±0.06\n\nInference time reduction (s)\n\nFLOPs (Dense = 5.56e+8)\n\nWeight Memory (MB)\n\n1.40×\n\n0.50×\n\n22.4\n\n2.08×\n\n0.25×\n\n11.2\n\n1.40×\n\n0.50×\n\n22.4\n\n2.08×\n\n0.25×\n\n11.2\n\nThe FP32 dense ResNet-18(1×) model requires 44.8MB storage and takes 1.27 seconds per 10K testset images during inference.\n\ndataset. Compared to the self-damaging scheme (Jiang et al., 2021) and GraNet (Liu et al., 2021), the proposed algorithm achieves the same highly-sparse network with 4.72% and 1.21% Top-1 inference accuracy improvements, respectively. GraNet exploits in-training sparsity throughout the entire training process, but the inconsistent contrastiveness hampers the model performance. On the other hand, the dense encoder limits the efficiency of the self-damaging scheme (Jiang et al., 2021) scheme, and the static high sparsity degrades the model performance. It has been shown that SSLtrained encoders are strong visual learners (Grill et al., 2020; Ericsson et al., 2021). Appendix A summarizes the performance of the proposed algorithm by fine-tuning the ImageNet-trained sparse encoders on CIFAR-10 and CIFAR-100 datasets. With only 300 epochs of sparse SSL training, the resultant sparse encoder outperforms the SoTA supervised sparse training algorithms.\n\nTable 7: Hardware training acceleration of the proposed structured SyncCP on CIFAR-10 datasaet.\n\nBYOL+ResNet-18\n\nTop-1 Accuracy (%)\n\nTraining Speed-up\n\nTop-1 Accuracy (%)\n\nTraining Speed-up\n\nDense Baseline\n\n92.42\n\n1×\n\n92.42\n\n1×\n\nTarget Structured Sparsity\n\n80%\n\n90%\n\nBYOL (Grill et al., 2020)\n\nThis work\n\n92.16\n\n1.74×\n\n91.77\n\n1.91×\n\nHardware-based Structured Pruning The hardware practicality of element-wise sparsification is often limited by the irregularity of fine-grained sparsity and index requirement. To that end, we employ structured sparsity based on group-wise EMA scores towards achieving actual hardware training acceleration. The encoders are initialized by ERK with 30% and 0% sparse groups while keeping ∆s = 30%. The structured sparsity starts to gradually increase after the CSI checkpoint. We adopt the training accelerator specifications from (Venkataramanaiah et al., 2022) and choose Kl (# of output channels) × Cl (# of input channels) = 8×8 as the sparse group size. Table 7 evalautes the training speedup of BYOL (Grill et al., 2020) aided by the structured sparse training. The proposed algorithm achieves up to 1.91× training acceleration with minimal accuracy degradation.\n\n6 CONCLUSION\n\nIn this paper, we propose a novel sparse training algorithm designed for self-supervised learning (SSL). As one of the first studies in this area, we first point out the imperfections of the sparsityinduced asymmetric self-supervised learning, as well as the incompatibility of the supervised sparse training algorithm in SSL. Based on the well-knit conclusions, we propose a contrastiveness-aware sparse training algorithm, consisting of synchronized contrastive pruning (SCP) and contrastive sparsification indicator (CSI). The proposed method outperforms the SoTA sparse training algorithm on both CIFAR and ImageNet-2012 datasets with various mainstream SSL frameworks. We also demonstrate the actual training and inference hardware acceleration with structured sparsity and N :M structured fine-grained pattern.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and Zhangyang Wang. The Lottery Tickets Hypothesis for Supervised and Self-supervised Pretraining in Computer Vision Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16306–16316, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework In International Conference on Machine\n\nfor Contrastive Learning of Visual Representations. Learning (ICML), pp. 1597–1607. PMLR, 2020a.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n\nImproved Baselines with Momentum\n\nContrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\n\nVictor Guilherme Turrisi Da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. Journal of Machine Learning Research, 23(56):1–6, 2022.\n\nTim Dettmers and Luke Zettlemoyer. Sparse Networks From Scratch: Faster Training without losing\n\nperformance. arXiv preprint arXiv:1907.04840, 2019.\n\nLinus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models transfer? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5414–5423, 2021.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning (ICML), 2020.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 33:21271–21284, 2020.\n\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1735–1742, 2006.\n\nSong Han, Huizi Mao, and William J Dally. Deep Compression: Compressing Deep Neural NetInternational Conference on\n\nworks with Pruning, Trained Quantization and Huffman Coding. Learning Representations (ICLR), 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9729–9738, 2020.\n\nSara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do Com-\n\npressed Deep Neural Networks Forget? arXiv preprint arXiv:1911.05248, 2019.\n\nSiddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-kast: Topk always sparse training. Advances in Neural Information Processing Systems (NeurIPS), 33: 20744–20754, 2020.\n\nZiyu Jiang, Tianlong Chen, Bobak J Mortazavi, and Zhangyang Wang. Self-damaging Contrastive\n\nLearning. In International Conference on Machine Learning (ICML), pp. 4927–4939, 2021.\n\nAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting Self-supervised Visual Representation Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1920–1929, 2019.\n\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do Better ImageNet Models Transfer Better? In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2661–2671, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: Single-shot Network Pruning In International Conference on Learning Representations\n\nbased on Connection Sensitivity. (ICLR), 2018.\n\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nJian Meng, Li Yang, Jinwoo Shin, Deliang Fan, and Jae-sun Seo. Contrastive Dual Gating: Learning Sparse Features With Contrastive Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12257–12265, 2022.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Pre-\n\ndictive Coding. arXiv preprint arXiv:1807.03748, 2018.\n\nSiyuan Pan, Yiming Qin, Tingyao Li, Xiaoshuang Li, and Liang Hou. Momentum Contrastive In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n\nPruning. 2647–2656, 2022.\n\nShreyas K. Venkataramanaiah, Jian Meng, Han-Sok Suh, Injune Yeo, Jyotishman Saikia, Sai Kiran Cherupally, Yichi Zhang, Zhiru Zhang, and Jae sun Seo. A 28nm 8-bit Floating-Point Tensor Core based CNN Training Processor with Dynamic Activation/Weight Sparsification. In IEEE European Solid-State Circuits Conference (ESSCIRC), 2022.\n\nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking Winning Tickets Before Training by In International Conference on Learning Representations (ICLR),\n\nPreserving Gradient Flow. 2019.\n\nYang You, Igor Gitman, and Boris Ginsburg. Large Batch Training of Convolutional Networks.\n\narXiv preprint arXiv:1708.03888, 2017.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ́ephane Deny. Barlow Twins: Self-supervised Learning via Redundancy Reduction. In International Conference on Machine Learning (ICML), pp. 12310–12320, 2021.\n\nAojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch. In International Conference on Learning Representations (ICLR), 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA DOWNSTREAM TASKS PERFORMANCE WITH THE PRE-TRAINED SPARSE\n\nENCODER\n\nThe pre-trained sparse encoder can be used for downstream tasks. We verified the performance sparse ImageNet-trained model in Table 5 with the downstream tasks. The following table summarizes the transfer learning performance by fine-tuning the ImageNet-trained sparse BYOL encoder on CIFAR-10 and CIFAR-100 datasets.\n\nTable 8: Transfer learning performance of the ImageNet-trained BYOL encoder on CIFAR-10 and CIFAR-100 datasets.\n\nDataset\n\nEncoder\n\nCIFAR-10 Acc (%) CIFAR-100 Acc. (%)\n\nResNet-50 (1×)\n\nResNet-50 (1×)\n\nElement-wise Sparsity\n\n0%\n\n90%\n\n0%\n\n90%\n\nSNIP (Lee et al., 2018)\n\nGraSP (Wang et al., 2019)\n\nRigL (Evci et al., 2020)\n\nGraNet (Liu et al., 2021)\n\n94.75\n\n92.65\n\n78.23\n\n73.14\n\n94.75\n\n92.47\n\n78.23\n\n73.28\n\n94.75\n\n94.45\n\n78.23\n\n76.50\n\n94.75\n\n94.64\n\n78.23\n\n77.89\n\nThis work\n\n96.09\n\n95.72\n\n80.13\n\n79.48\n\nSupervised Training\n\nTransfer learning from BYOL-trained model\n\nThe pre-trained sparse encoder are consistently preserved during the downstream fine-tuning. As shown in the table above, the strong visual representation learners are obtained by training the backbone model with self-supervised learning on ImageNet dataset. Sparsified by the proposed algorithm on ImageNet with 300 epochs during pre-training, fine-tuning the SSL-trained backbone model leads to 1.08% and 1.59% accuracy improvements compared to the recent SoTA supervised pruning algorithms.\n\nB THE IMPACT OF ∆s\n\nWe evaluate the impact of ∆s with different sparsification schemes on CIFAR-10 dataset with BYOL SSL framework. With the target sparsity = 90%, the following table summarizes the accuracy and training cost reduction with different ∆s values associated with different initial and final density.\n\nTable 9: The linear evaluation accuracy and the training FLOPs reduction with sparsity gap ∆s.\n\n∆s Online Encoder Spars. Offline Encoder Spars. Online Linear Eval Acc.\n\n50%\n\n40%\n\n30%\n\n40%\n\n30%\n\n50% → 90%\n\n50% → 90%\n\n50% → 90%\n\n40% → 90%\n\n30% → 90%\n\n0% → 40%\n\n10% → 50%\n\n20% → 60%\n\n0% → 50%\n\n0% → 60%\n\n91.68\n\n91.41\n\n91.27\n\n91.79\n\n92.02\n\nFLOPs (Training)\n\n0.60×\n\n0.58×\n\n0.53×\n\n0.57×\n\n0.56×\n\nGiven the fixed target sparsity = 90%, it is easy to tell that the higher ∆s leads to denser momentum encoder and less computation reduction. Meanwhile, sparsifies the momentum encoder at the beginning of training is sub-optimal. Furthermore, less initial sparsity and smaller ∆s value (4th row) achieves the best tradeoff between computation reduction and model performance.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nC APPENDIX C\n\nC.1 PSEUDO CODE OF SYNCCP WITH ELEMENT-WISE SPARSITY\n\nAlgorithm 1: Synchornized Contrastive Pruning (SyncCP) Initialize Sparse online encoder fθ, Sparse offline encoder fξ, EMA updater, Momentum γ, SyncS density gap ∆s, CSI threshold τ (Default=∆s). ξ, such that |s∗ Initial sparsity s0 θ, s0 θ, s∗ Target sparsity s∗ ξ, such that |s∗ Initial mask M0 θ, M0 ξ. Pruner udpate frequency Φ while t < Total Iterations do\n\nξ| = ∆s ξ| = ∆s\n\nθ − s∗ θ − s∗\n\nDraw augmented data (X, X ′); Forward pass: online encoding = fθ(Mθ · θ, X) ; Forward pass: offline encoding = fξ(Mξ · ξ, X ′); Update Exponential Moving Average (EMA) gradient score based on Eq. 17 ; if End Epoch then\n\nGet pseudo masks M∗ Compute layer-wise G and I based on Eq. 15 and Eq. 16; if I = ∆s then\n\nξ based on magnitude pruning;\n\nθ and M∗\n\nPrune=True\n\nend\n\nend if t % Φ = 0 then\n\nif Prune=True then\n\nelse\n\nend\n\nend\n\nξ based on Eq. 10 and Eq. 11;\n\nθ, st\n\nUpdate sparsity st Maintain the SyncS constraint ∆s; Inside fθ and fξ, prune st θ, and st Prune extra rt\n\nθ elements of the unpruned elements, then regrow rθ elements with\n\nξ elements with least magnitude score;\n\nhights EMA-gradient score; θ, Mt\n\nUpdate Mt\n\nξ based on Eq. 3 and Eq. 4;\n\nC.2 EMA-BASED PRUNE AND REGROW\n\nAs aforementioned, the findings of Observation 2 implies the incompatibility of the instant gradient and magnitude score. Together with the proposed SyncS and CSI methods, weight importance is measured by the magnitude score, while the sensitivity of the model is quantified by the gently averaged gradient magnitude with EMA:\n\n ̄gt = γ × ̄gt−1 + (1 − γ) × |g|t\n\n(17)\n\nTable 10 summarizes the linear evaluation accuracy of ResNet-18 trained by BYOL (Grill et al., 2020). We initialize s0 ξ as 40% and 10%, where the ∆s is set to 30%, the EMA momentum is set to 0.1 for gentle gradient score averaging.\n\nθ and s0\n\nMetric\n\nPrune Regrow EMA Online Linear Eval. Acc. (%)\n\nThis work\n\nPrune-and-regrow\n\nMagnitude Pruning\n\n✓\n\n✓\n\n✓\n\n✓\n\n✓\n\n✗\n\n✓\n\n✗\n\n✗\n\n91.88\n\n91.52\n\n90.99\n\nTable 10: Peformance comparison between different sparsification metrics.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nC.3 PSEUDO CODE OF SYNCCP WITH N :M SPARSITY\n\nAlgorithm 2: Synchornized Contrastive Pruning (SyncCP) with N :M Sparsity Initialize Sparse online encoder fθ, Sparse offline encoder fξ, EMA updater, Momentum γ, SyncS density gap ∆s, CSI threshold τ (Default=∆s). Group size M , Number of dense element per group N . Initial percentage p0 Such that |p0 θ = 100%, p∗ Target percentage p∗ Initial mask M0 θ, M0 ξ. Pruner udpate frequency Φ. while t < Total Iterations do\n\nθ of N :M groups in fθ, Initial percentage p0\n\nξ of N :M groups in fξ;\n\nξ| = ∆s;\n\nθ − ∆s;\n\nξ = p∗\n\nθ − p0\n\nDraw augmented data (X, X ′); Forward pass: online encoding = fθ(Mθ · θ, X) ; Forward pass: offline encoding = fξ(Mξ · ξ, X ′); Update Exponential Moving Average (EMA) weight gradient score based on Eq. 17 ; if End Epoch then\n\nGet pseudo masks M∗ Compute layer-wise G and I based on Eq. 15 and Eq. 16; if I = ∆s then\n\nξ based on magnitude pruning;\n\nθ and M∗\n\nPrune=True\n\nend\n\nend if t % Φ = 0 then\n\nif Prune=True then\n\nelse\n\nend\n\nend\n\nθ, pt\n\nξ based on Eq. 10 and Eq. 11;\n\nUpdate sparsity pt Maintain the SyncS constraint pt Inside fθ and fξ, pick pt θ, and pt Inside each group, prune the N-M elements with smallest magnitude score; Update Mt\n\nθ, pt ξ M-sized groups with least sum of magnitude score;\n\nθ − ∆s;\n\nξ = pt\n\nξ based on Figure 4;\n\nθ, Mt\n\nFigure 4: Group-wise (a) prune and (b) regrow algorithm based on EMA gradient score. SyncCP sparsifies M − N elements inside each group, while keep pt N :M groups inside f .\n\n14\n\n(a)(b)Under review as a conference paper at ICLR 2023\n\nC.4 PSEUDO CODE OF SYNCCP WITH STRUCTURED SPARSITY\n\nθ of sparse groups in fθ, Initial percentage p0\n\nAlgorithm 3: Synchornized Contrastive Pruning (SyncCP) with Structured Sparsity Initialize Sparse online encoder fθ, Sparse offline encoder fξ, EMA updater, Momentum γ, SyncS density gap ∆s, CSI threshold τ (Default=∆s). Group size Kl (# of output channels) × Cl (# of input channels) = g × g. Initial percentage p0 θ − p∗ Such that |p∗ Initial structured sparsity s0 Target structured sparsity s∗ Initial mask M0 Pruner udpate frequency Φ. while t < Total Iterations do\n\nξ, such that |s∗ ξ, such that |s∗\n\nξ of sparse groups in fξ;\n\nξ| = ∆s ξ| = ∆s\n\nθ − s∗ θ − s∗\n\nθ, s0 θ, s∗\n\nξ| = ∆s;\n\nθ, M0 ξ.\n\nDraw augmented data (X, X ′); Forward pass: online encoding = fθ(Mθ · θ, X) ; Forward pass: offline encoding = fξ(Mξ · ξ, X ′); Update Exponential Moving Average (EMA) gradient score based on Eq. 17; if End Epoch then\n\nGet pseudo masks M∗ Compute layer-wise G and I based on Eq. 15 and Eq. 16; if I = ∆s then\n\nξ based on magnitude pruning;\n\nθ and M∗\n\nPrune=True\n\nend\n\nend if t % Φ = 0 then\n\nif Prune=True then\n\nUpdate structured sparsity st θ, st Maintain the SyncS constraint st Inside fθ and fξ, pick st θ, and st Outside the sparsified groups of fθ, prune rt\n\nξ based on Eq. 10 and Eq. 11; θ, st ξ groups with least sum of magnitude score; θ more groups with least sum of\n\nθ − ∆s;\n\nξ = st\n\nAmong the sparsified groups fθ, regrow the rt\n\nθ groups back with highest sum of\n\nmagnitude score;\n\nEMA gradient score;\n\nUpdate Mt\n\nθ, Mt ξ;\n\nend\n\nend\n\nend\n\nD SPARSIFYING ONLINE OR OFFLINE ENCODERS\n\nWe mirror the experiment of Table 1 by exploiting high sparsity in the momentum encoder while keeping the online encoder dense:\n\nTable 11: Largely degraded performance of MoCo-V2 with self-damaging SSL on CIFAR-10 dataset.\n\nResNet-18\n\nEncoder\n\nFixed Sparsity\n\nDense Model Acc. = 92.09%\n\nOnline\n\n90%\n\nMomentum\n\n0%\n\nOnline\n\n50%\n\nMomentum\n\n0%\n\nLinear Eval. Acc (%)\n\n88.72 (-3.41%)\n\n87.68 (-4.31%)\n\n92.10 (+0.01%)\n\n92.07 (-0.02%)\n\nFixed Sparsity\n\n0%\n\n90%\n\nLinear Eval. Acc (%)\n\n88.44 (-3.65%)\n\n87.52 (-4.57%)\n\n0%\n\n–\n\n50%\n\n–\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nWith the proposed CSI + SyncS sparsification method, we empirically observe that exploiting high sparsity in online model leads to better performance with gradient-based model update. Table 12 summarizes the comparison results of BYOL on CIFAR-10 dataset with the proposed algorithm.\n\nTable 12: Performance comparison of exploiting higher sparsity in online or momentum encoders.\n\nMethod\n\nOnline Encoder Spars. Momentum Encoder Spars. Online Linear Eval. Acc. (%)\n\nCSI + SyncS\n\nCSI + SyncS\n\n0%→50%\n\n30%→80%\n\n30%→80%\n\n0%→50%\n\n91.88%\n\n92.24%\n\nOverall, sparsifying the online encoder leads to the optimal performance.\n\nE DETAILED EXPERIMENTAL SETUP OF SYNCCP\n\nE.1 LINEAR EVALUATION PROTOCOL\n\nAs in (Kolesnikov et al., 2019; Kornblith et al., 2019; Chen et al., 2020a), we use the standard linear evaluation protocol on CIFAR-10/100 and ImageNet-2012 datasets, which training a linear classifier on top of the frozen SSL-trained encoder. During linear evaluation, we apply spatial augmentation and random flips. The linear classifier is optimized by SGD with cross-entropy loss.\n\nE.2 CIFAR-10/100 EXPERIMENTS\n\nThe training hyper-parameters of the compared individual sparse training works are same for CIFAR-10 and CIFAR-100. We provide the detailed training setup of different self-supervised learning frameworks as follow:\n\nMoCo-V2 The ResNet-18 (×) encoder is trained by MoCo-V2 (Chen et al., 2020b) from scratch by 1,000 epochs with SGD optimizer and 256 batch size. The learning rate is set to 0.3 with Cosine learning rate decay and 10 epochs warmup. The detailed data augmentation is summarized in Table 13.\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n32 × 32\n\n32 × 32\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\nTable 13: Detailed image augmentation settings for MoCo-V2 (Chen et al., 2020b) on CIFAR10/100.\n\nBYOL The ResNet-18 (×) encoder is trained by BYOL (Grill et al., 2020) from scratch by 1,000 epochs with LARS-SGD optimizer (You et al., 2017). The predictor is constructed with 4096 hidden features and 256 output dimension. We use 256 batch size along with 1.0 learning rate. The Cosine learning rate scheduler is used with 10 epochs warmup training. The detailed data augmentation is summarized in Table 14.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n32 × 32\n\n32 × 32\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.2\n\nTable 14: Detailed image augmentation settings for BYOL (Chen et al., 2020b) on CIFAR-10/100.\n\nBarlow Twins The ResNet-18 (×) encoder is trained by Barlow Twins (Zbontar et al., 2021) from scratch by 1,000 epochs with LARS-SGD optimizer (You et al., 2017). We use 256 batch size along with 0.3 learning rate and 1e − 4 weight decay. The Cosine learning rate scheduler is used with 10 epochs warmup training. The detailed data augmentation is summarized in Table 15.\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n32 × 32\n\n32 × 32\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.2\n\nTable 15: Detailed image augmentation settings for Barlow Twins (Zbontar et al., 2021) on CIFAR10/100.\n\nE.3\n\nIMAGENET EXPERIMENTS\n\nStarting from scratch, the proposed SyncCP algorithm exploits in-training sparsity with the BYOL framework (Grill et al., 2020) on ImageNet-2012 dataset. The ResNet-50 encoder is trained by LARS-SGD (You et al., 2017) with 0.45 learning rate and a momentum of 0.9. We uses 0.1 for the for EMA-averaged gradient score. We use 128 batch size along with 1e-6 weight decay. The detailed image augmentations are summarized in Table 16.\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n224 × 224\n\n224 × 224\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n1.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.1\n\n0.2\n\nTable 16: Detailed image augmentation settings for BYOL (Grill et al., 2020) on ImageNet.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nF COMPUTATION REDUCTION WITH DIFFERENT SPARSITY VALUES\n\nTable 17: ImageNet-2012 linear evaluation accuracy and training cost comparison with different sparsity on ResNet-50 with BYOL (Grill et al., 2020).\n\nDataset\n\nSparsity\n\n0%\n\n50%\n\n80%\n\n90%\n\nImageNet-2012\n\nTop-1 Accuracy (%)\n\nFLOPs (Training)\n\nFLOPs (Inference)\n\n68.12\n\n68.31\n\n67.02\n\n65.67\n\n7.94e+18 (1×)\n\n8.18e+09 (1×)\n\n0.78×\n\n0.62×\n\n0.56×\n\n0.68×\n\n0.33×\n\n0.22×\n\n18",
  "translations": [
    "# Summary Of The Paper\n\nThis paper enables model pruning to accelearate contrastive self-supervised training. Compared to the exsited method, this paper tries to sparsify online decoder and offline decoder simultaneously. To stabilize the sparse training. The authors propose Contrastive Sparsification Indicator (CSI) to guide the model pruning.\n\nTo evaluate the effectiveness of SyncCP, the authors conduct experiments on various classification datasets with unstructured sparsity, N:M sparsity, and structured pruning.\n\n# Strength And Weaknesses\n\nStrength:\n\nThe paper is well written.\n\nThe experiments including various different sparse types (unstructured, N:M, structured) are persuasive.\n\nThe performance (accuracy and efficiency) improvement is significant compared to the existing method [1].\n\nWeaknesses:\n\nThe self-supervised learning method usually trains on large-scale datasets, but this paper doesn't show any training GPU hours improvement on large-scale datasets. I feel confused about the motivation.\n\nThe NVidia GPUs only support 2:4 sparsity, can you explain the detail implementation of inference gain about 1:4 sparsity in Table5.  \n\nThe BYOL and MOCO-v2 are not SOTA SSL methods, the recent SOTA methods are more convincing.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nNa\n\n# Summary Of The Review\n\nNa\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel approach to self-supervised learning (SSL) through the introduction of Synchronized Contrastive Pruning (SyncCP), which addresses the inefficiencies associated with large encoders in current SSL methods. The authors demonstrate that the existing sparsity-induced asymmetry in SSL models has limitations, particularly when using the \"prune-and-regrow\" technique, which can lead to architectural oscillations. SyncCP synchronizes sparsity in both online and offline encoders during training, utilizing Synchronized Sparsification (SyncS) and a Contrastive Sparsification Indicator (CSI) to optimize training efficiency without sacrificing contrastive benefits. The empirical results show that SyncCP achieves state-of-the-art (SoTA) accuracy across several datasets, including CIFAR-10 and ImageNet-2012, while significantly reducing training costs.\n\n# Strengths And Weaknesses\nStrengths of the paper include its clear identification of limitations in existing sparse SSL methods and the introduction of a robust framework that effectively synchronizes sparsity during training. The methodology is well-structured, incorporating novel techniques like SyncS and CSI, which contribute to its effectiveness. Additionally, the empirical results provide compelling evidence of the method's superiority over existing techniques. However, weaknesses include a potential lack of detailed analysis on the robustness of the method across varied datasets and its applicability to different architectures, which could limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it accessible to a broad audience. The methodology is presented clearly with appropriate details, including pseudo code that enhances reproducibility. The novelty is significant, as the approach to synchronizing sparsity in SSL is a fresh contribution to the field. However, the reproducibility could be further strengthened by providing more extensive experimental details and potential limitations of the proposed method.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of self-supervised learning by introducing an innovative approach to sparsity that enhances training efficiency and performance. The methodology is well-developed, and the empirical results demonstrate its effectiveness. However, further exploration of the method's applicability across diverse contexts would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"Synchronized Pruning for Efficient Contrastive Self-Supervised Learning,\" which addresses the inefficiencies in self-supervised learning (SSL) methods that rely on large encoders, leading to high computational costs. The authors introduce a novel algorithm named Synchronized Contrastive Pruning (SyncCP), which leverages in-training sparsity to enhance SSL efficiency. Key contributions include the development of Synchronized Sparsification (SyncS) to maintain consistent sparsity between online and offline encoders, a Contrastive Sparsification Indicator (CSI) to optimize sparsity adjustments, and demonstrating compatibility with various sparsity granularities. Experimental results show that SyncCP outperforms state-of-the-art sparse learning algorithms across multiple datasets and SSL frameworks while achieving significant training efficiency improvements.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to synchronized pruning, which effectively balances sparsity and model consistency, addressing critical challenges in SSL. The comprehensive validation across diverse datasets and SSL frameworks strengthens the findings, demonstrating the method's robustness. Additionally, the focus on real-world applicability, particularly in resource-constrained environments, is commendable. However, the paper has limitations, such as potential issues with generalizability to other architectures or SSL methods not tested, sensitivity to hyperparameter tuning, and increased complexity in training procedures, which could complicate practical deployment.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, allowing for easy comprehension of its contributions. The quality of the experimental design is high, with thorough evaluations and appropriate metrics. The novelty of the proposed SyncCP method is significant, as it introduces a new framework for efficient learning. However, the reproducibility of the results might be hindered by the potential sensitivity to hyperparameters, which may require careful tuning.\n\n# Summary Of The Review\nOverall, the paper effectively presents a novel approach to improving the efficiency of self-supervised learning through synchronized pruning techniques, supported by extensive and convincing experimental results. While it shows significant promise, further investigation is needed to assess its generalizability and ease of adoption in practical applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SyncCP (Synchronized Contrastive Pruning), a novel approach to enhancing self-supervised learning (SSL) efficiency by leveraging high in-training sparsity across encoders. The authors identify limitations of existing sparse approaches in SSL, formalize the concept of architecture oscillation caused by prune-and-regrow strategies, and propose Synchronized Sparsification (SyncS) to maintain a consistent sparsity gap between encoders. The empirical results demonstrate that SyncCP achieves superior linear evaluation accuracy on standard datasets (CIFAR-10, CIFAR-100, ImageNet) while significantly reducing training costs and improving inference speed.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of a relevant gap in the SSL literature regarding sparsity, along with the introduction of practical innovations such as SyncS and the Contrastive Sparsification Indicator (CSI). The experimental results convincingly demonstrate that the proposed methods outperform state-of-the-art approaches while maintaining computational efficiency. However, weaknesses include a lack of detailed analysis on the impact of hyperparameter tuning and extended training, which could further validate the robustness of the findings. Additionally, while the paper effectively showcases its contributions, a more in-depth discussion of related works could enhance the context.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its main contributions, methodology, and results. The mathematical formulation is precise, aiding in the clarity of the proposed methods. The novelty of integrating synchronized sparsity into SSL is significant, providing foundational contributions that could influence future research. However, while the empirical results are promising, further details on the reproducibility of experiments and the specific implementation of methodologies would strengthen the paper's overall quality.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to improving the efficiency of self-supervised learning through synchronized pruning techniques, yielding impressive empirical results. While it identifies important gaps in existing methodologies and proposes innovative solutions, a deeper analysis of hyperparameter impacts and related works would enhance its contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents Synchronized Contrastive Pruning (SyncCP), a novel sparse training algorithm specifically designed for self-supervised learning (SSL). The methodology involves a unique approach to pruning that enhances energy efficiency and model performance across multiple datasets, including CIFAR-10, CIFAR-100, and ImageNet. The findings indicate that SyncCP achieves state-of-the-art accuracy in SSL frameworks, while also addressing challenges related to sparsity and stability in training.\n\n# Strength And Weaknesses\nThe strengths of the paper include its novel contribution to the field of SSL through the introduction of SyncCP, as well as a comprehensive evaluation demonstrating improved performance over state-of-the-art sparse learning algorithms. The attention to energy efficiency is particularly relevant for deployment in resource-limited environments. However, the paper has several weaknesses, such as a limited exploration of the method's applicability across various SSL frameworks, dependency on specific training conditions, and potential performance degradation with high sparsity configurations. Additionally, a lack of detailed comparative analysis with the latest methods in SSL and sparsity techniques could limit the perceived relevance of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and coherent, with clear explanations of the proposed methodology and experimental setup. However, the novelty of the contribution, while present, is somewhat tempered by the limited exploration of its broader implications and the dependency on specific conditions for success. The reproducibility of the results could be enhanced by providing more comprehensive details regarding hyperparameter settings and training conditions.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of self-supervised learning through the introduction of SyncCP, demonstrating improved performance and energy efficiency. However, the limitations in generalizability, exploration of sparsity techniques, and potential overfitting risks warrant further investigation and refinement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach named Synchronized Contrastive Pruning (SyncCP) designed to enhance the efficiency of contrastive self-supervised learning (SSL). The authors introduce a unique sparsification technique applied during the training phase, which contrasts with traditional methods that typically focus on post-training sparsification. Key contributions include the introduction of a synchronized sparsity mechanism that maintains a consistent sparsity gap between encoders, and the development of a Contrastive Sparsification Indicator (CSI) to dynamically guide the sparsification process. Comprehensive experiments on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet demonstrate that SyncCP outperforms existing methods in terms of accuracy and computational efficiency.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to integrating sparsification within the training process, which could lead to significant performance improvements in SSL tasks. The introduction of the CSI metric provides a novel way to assess and maintain encoder consistency, enhancing the robustness of the method. However, a potential weakness is the lack of extensive exploration into the implications of varying sparsity levels across different model architectures, which could limit the generalizability of the findings. Additionally, while the empirical results are compelling, further validation across more diverse datasets could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodologies, and findings. The writing is coherent, and the figures effectively illustrate the concepts discussed. The quality of the empirical validation is high, with a thorough comparison to existing methods. The novelty of the approach is significant, particularly in the context of integrating sparsification into SSL training. The reproducibility appears feasible, as the authors provide sufficient details on the methodology and experimental setups, though additional clarity on implementation specifics could further bolster this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of self-supervised learning through its innovative method of synchronized sparsification. The empirical results strongly support the effectiveness of the proposed approach, although further exploration into its applicability across varied architectures could enhance its impact. The work is well-articulated and reproducible, warranting acceptance.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Synchronized Adversarial Training (SAT), a novel framework aimed at enhancing the robustness of deep learning models against adversarial attacks while ensuring computational efficiency. SAT employs a synchronized strategy to generate adversarial examples across different model components, promoting consistency in the adversarial feature space. The authors validate their approach through extensive empirical evaluations on benchmark datasets such as CIFAR-10 and ImageNet, demonstrating that SAT achieves higher accuracy and robustness against adversarial perturbations compared to traditional adversarial training methods, while also reducing training costs.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a practical solution that addresses the computational inefficiencies commonly associated with adversarial training. The empirical results are robust and convincingly demonstrate the advantages of SAT. However, the paper lacks a thorough theoretical explanation of why synchronization leads to improved robustness, relying mainly on empirical validation. Additionally, a more comprehensive discussion of the framework's limitations and scenarios where it may underperform would strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The quality of the empirical analysis is high, with detailed experiments supporting the claims made. However, the novelty of the theoretical insights is somewhat limited, as the authors do not provide in-depth reasoning behind the synchronization approach. The reproducibility of the results appears feasible, given the empirical support and the compatibility of SAT with various architectures.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in adversarial training through the introduction of Synchronized Adversarial Training, which enhances model robustness and reduces computational costs. While the empirical validation is robust, additional theoretical insights and exploration of limitations would further strengthen the contribution and applicability of SAT.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Synchronized Pruning for Efficient Contrastive Self-Supervised Learning\" seeks to address the inefficiencies inherent in current state-of-the-art (SoTA) self-supervised learning (SSL) methods, which often rely on large encoders. The authors present SyncCP, a novel sparse training algorithm claimed to significantly enhance efficiency in SSL. They argue that existing sparsity-induced asymmetric methods are flawed and that the prune-and-regrow technique is incompatible with SSL. Through extensive experiments across various datasets, the authors claim to demonstrate superior performance and training speed improvements with SyncCP, positing it as a paradigm shift for the field.\n\n# Strength And Weaknesses\nThe paper makes several noteworthy contributions, particularly in its identification of perceived limitations in existing SSL methods and the introduction of SyncCP. However, the claims regarding the flaws in current methods appear exaggerated, potentially undermining the credibility of the authors' arguments. While the methodology shows promise in terms of experimental validation, the reported improvements in performance and efficiency may not be as substantial as suggested. The paper’s framing of its contributions as revolutionary may also detract from the nuanced understanding of the field's ongoing developments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly. However, the claims regarding the novelty and impact of techniques like Synchronized Sparsification and Contrastive Sparsification Indicator are overstated, potentially leading to misconceptions about their significance. While the methodology is described in detail, the reproducibility of the results may be hindered by the inflated assertions about performance gains and efficiencies. Overall, while the paper is of decent quality, the clarity of its contributions could be improved by providing a more balanced perspective on existing works.\n\n# Summary Of The Review\nThis paper attempts to position SyncCP as a groundbreaking advancement in self-supervised learning, yet its claims are often exaggerated and may mislead readers about the true nature of its contributions. While there are promising elements, the authors' assertions regarding the limitations of existing methods and the significance of their own work require a more tempered presentation.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel method called Synchronized Contrastive Pruning (SyncCP) designed to enhance the efficiency of self-supervised learning (SSL) by integrating sparsity into the training process. SyncCP aims to address the challenges posed by large model sizes, which limit the efficiency of existing SSL methods. The experimental results demonstrate that SyncCP achieves superior performance on various datasets, including CIFAR-10, CIFAR-100, and ImageNet-2012, by maintaining high accuracy levels even with significant sparsity (up to 80%). Additionally, the method offers substantial training speed improvements, up to 2.5 times faster than traditional approaches.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to integrating sparsity into SSL, which has been largely under-explored in previous works. The empirical results are compelling, showing substantial performance improvements and efficiency gains across multiple datasets. However, a potential weakness is the lack of a thorough comparison with a broader range of existing SSL methods, which may limit the generalizability of the results. Additionally, the paper could benefit from a more detailed discussion on the implications of the architectural choices made for SyncCP.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to the reader. The quality of the writing is high, and the experimental setup is described sufficiently to allow for reproducibility. However, while the novelty of incorporating sparsity into SSL is significant, the paper could further elaborate on the theoretical underpinnings of how SyncCP achieves its results, which would enhance the understanding of its impact in the field.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the field of self-supervised learning by introducing SyncCP, which effectively leverages sparsity to enhance model performance and training efficiency. The results are impressive, but the paper could be strengthened by a more comprehensive comparison with existing methods and deeper theoretical insights.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a new approach to enhance contrastive self-supervised learning (SSL) through the application of sparsity techniques, building on the sparse-dense architecture concept. The authors argue that high sparsity can improve learning outcomes without degrading performance and present experiments primarily focusing on the CIFAR and ImageNet datasets. However, they assume that existing sparsity methods can be adapted to SSL without significant modification and claim that maintaining contrastiveness while implementing synchronized sparsity is feasible.\n\n# Strength And Weaknesses\nThe paper makes several noteworthy contributions, particularly in exploring the intersection of sparsity and contrastive learning. However, the assumptions underlying the efficacy of sparsity in SSL are questionable. For example, the reliance on fixed sparsity gaps between online and momentum encoders may undermine the proposed method's robustness. Additionally, the lack of comprehensive evaluation across diverse datasets limits the generalizability of the findings. There is also a notable overemphasis on linear evaluation accuracy as a performance metric, which may not fully capture the model's real-world applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively clear in its presentation, although some complex assumptions could benefit from further empirical validation. While the proposed techniques are novel in their application to SSL, the generalizations made about sparsity effects may not hold across different architectures. The reproducibility of results is somewhat hampered by the narrow focus on specific datasets and the lack of exploration of alternative training conditions, such as varying batch sizes.\n\n# Summary Of The Review\nOverall, the paper presents an innovative approach to integrating sparsity within contrastive SSL, but it is built on several assumptions that require more rigorous validation. The findings, while promising, may not be universally applicable, and further exploration is needed to substantiate the claims made regarding the efficacy of the proposed methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel self-supervised learning (SSL) algorithm called Synchronized Contrastive Pruning (SyncCP), which aims to address the inefficiencies of current SSL methods that rely on large encoders. By integrating contrastive learning with a focus on maximizing sparsity during training, SyncCP enhances training efficiency while maintaining model accuracy. Experimental results demonstrate that SyncCP outperforms existing sparse learning techniques across various datasets, showcasing improvements in both accuracy and training costs.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative approach to combining contrastive learning with sparsity, which addresses a significant limitation of current SSL methods that often require substantial computational resources. Additionally, the extensive experimental validation across multiple datasets strengthens the findings. However, the paper could be improved by providing more detailed comparisons against a broader array of existing methods and discussing potential limitations of using SyncCP in diverse real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results. The writing is generally coherent, although some sections could benefit from additional clarity, particularly the technical details of the proposed method. The novelty of SyncCP is commendable, as it introduces a unique framework that balances sparsity and contrastive learning. The reproducibility is supported by the thorough experimental setup, but providing access to code or data would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of self-supervised learning by introducing an efficient and effective method for achieving sparsity. While the proposed approach demonstrates significant improvements over existing methods, further exploration of its applicability and limitations could enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for enhancing the robustness of neural networks against adversarial attacks by integrating a multi-faceted loss function that balances accuracy and resilience. The methodology employs a combination of gradient masking and adversarial training, bolstered by a theoretical analysis that articulates the conditions under which the proposed approach is effective. The findings demonstrate significant improvements in robustness on standard benchmark datasets while maintaining competitive accuracy levels.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Relevance**: The focus on adversarial robustness is highly pertinent given the increasing concern about security in machine learning applications.\n2. **Innovation**: The introduction of a multi-faceted loss function is a noteworthy contribution that may inspire future research in robustness.\n3. **Theoretical Depth**: The paper provides a solid theoretical analysis that enhances the reader's understanding of the proposed method's effectiveness.\n4. **Experimental Results**: The empirical evaluations are comprehensive, demonstrating clear improvements over baseline models.\n\n**Weaknesses:**\n1. **Clarity**: Some sections, particularly those detailing the theoretical foundations, could benefit from clearer explanations and examples to enhance understanding.\n2. **Evaluation Scope**: While the benchmarks used are relevant, a broader range of datasets could strengthen the claims regarding generalizability.\n3. **Implementation Details**: The methodology section lacks some specifics on implementation, which may hinder reproducibility for practitioners.\n4. **Discussion of Limitations**: The paper does not sufficiently address the limitations of the proposed approach, which could mislead readers about its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate but could be improved, particularly in the theoretical sections where dense mathematical content is present. The quality of the writing is generally high, though the flow of ideas can be disrupted by complex explanations. The novelty of the approach is significant, as it introduces a new way to conceptualize adversarial robustness. Reproducibility is a concern, as the lack of detailed implementation guidance may pose challenges for others attempting to replicate the results.\n\n# Summary Of The Review\nThe paper offers a valuable contribution to the field of adversarial machine learning through its novel framework for enhancing neural network robustness. While the theoretical insights and empirical results are compelling, improvements in clarity and detail would greatly benefit the paper's overall impact and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Synchronized Pruning for Efficient Contrastive Self-Supervised Learning\" introduces a novel method, Synchronized Contrastive Pruning (SyncCP), aimed at enhancing the efficiency of self-supervised learning (SSL) by leveraging high sparsity during training. The authors argue that existing sparsity methods primarily focus on post-training, which is inadequate for SSL. Through extensive experimentation across multiple datasets, the proposed method demonstrates superior performance compared to state-of-the-art sparse learning algorithms, achieving both improved accuracy and accelerated training times.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to integrating sparsity directly into the training process of SSL, addressing a significant gap in current methodologies. SyncCP is well-motivated, and the experimental results provide compelling evidence of its effectiveness over existing techniques. However, a potential weakness is the lack of comprehensive theoretical analysis regarding the underlying mechanics of how sparsity interacts with contrastive learning, which could strengthen the foundational understanding of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making it accessible to readers from various backgrounds. The quality of the methodology is high, with detailed descriptions of the experiments and results. The novelty of the approach is significant, as it presents a new direction in SSL research by focusing on in-training sparsity. However, the reproducibility of the results could be improved by providing more detailed hyperparameter settings and code availability to facilitate further exploration by the research community.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the realm of self-supervised learning by introducing a method that effectively combines sparsity with contrastive learning. While it shows promising results and is well-written, there is a need for deeper theoretical insights and enhanced reproducibility measures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel algorithm called Synchronized Contrastive Pruning (SyncCP), aimed at enhancing the efficiency of contrastive self-supervised learning (SSL) by reducing the computational costs associated with large models. The proposed methodology combines synchronized sparsification with a contrastive sparsification indicator (CSI), allowing for effective training while maintaining a consistent sparsity gap between encoders. Experimental results demonstrate that SyncCP significantly improves accuracy and training efficiency across various datasets, including CIFAR-10, CIFAR-100, and ImageNet, outperforming existing state-of-the-art methods.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to sparsification in SSL, addressing a critical challenge in the field. The proposed SyncCP method is well-structured and shows empirical advantages over existing techniques, indicating its potential for practical applications. However, the paper could have benefited from a more detailed exploration of the limitations of high sparsity, particularly how it can hinder performance if not managed effectively. Additionally, while the results are promising, a deeper analysis of the underlying mechanisms contributing to the observed improvements would strengthen the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and experimental setup. The quality of the figures and tables used to present results is satisfactory, aiding in the comprehension of the findings. The novelty of the work is significant, as it addresses a previously underexplored area of SSL by integrating sparsification techniques. Reproducibility is reasonable, given the detailed description of SyncCP, but the paper would benefit from providing more specific hyperparameters and experimental configurations to assist future researchers in replicating the results.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful contribution to the field of self-supervised learning by proposing a novel sparsification technique that enhances efficiency and performance. While the methodology is sound and the results are promising, addressing some limitations and providing additional implementation details would make the work even stronger.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Synchronized Pruning for Efficient Contrastive Self-Supervised Learning\" introduces a novel approach called SyncCP, which aims to address the inefficiencies associated with dense models in self-supervised learning (SSL). The authors propose a methodology that incorporates synchronized pruning alongside contrastive learning, leading to in-training sparsity. Through extensive experiments, including evaluations on datasets such as CIFAR-10, CIFAR-100, and ImageNet, the paper demonstrates that SyncCP not only achieves improved accuracy but also reduces computational costs, outperforming several state-of-the-art (SoTA) methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive literature review, clear methodological framework, and robust empirical results. SyncCP effectively tackles the challenges of model sparsity in SSL, a relatively underexplored area. However, a potential weakness is the limited exploration of the impact of different sparsity levels on model performance, which may leave questions about the optimal configurations for various applications. Additionally, while the results are promising, the generalizability of the findings across different SSL frameworks could be further assessed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and easy to follow, with clear definitions and logical progression in presenting the proposed method and its results. The use of figures and tables to illustrate findings enhances comprehension. The novelty of the approach is notable, as it combines concepts from both model compression and SSL. The reproducibility is facilitated by the thorough description of the methodology and experiments, although sharing code and datasets would further strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant and well-motivated contribution to the field of self-supervised learning through the introduction of SyncCP. The combination of theoretical insights with empirical validation makes a strong case for its effectiveness and efficiency in resource-constrained environments.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel algorithm called Synchronized Contrastive Pruning (SyncCP), aimed at improving efficiency in contrastive self-supervised learning (SSL) while addressing the significant computational costs associated with state-of-the-art (SoTA) SSL methods. The methodology involves two key components: Synchronized Sparsification (SyncS), which enforces a consistent sparsity gap between online and offline encoders during training, and a Contrastive Sparsification Indicator (CSI), which quantifies architectural inconsistencies between encoders. Experimental results demonstrate that SyncCP outperforms existing sparse training algorithms across multiple datasets, including CIFAR-10, CIFAR-100, and ImageNet, achieving both reduced computational costs and maintained accuracy.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating sparsity in the training process, which is particularly relevant for resource-constrained environments like edge computing. The empirical results are compelling, showing that SyncCP provides significant enhancements in both efficiency and performance over SoTA methods. However, the paper could benefit from a more thorough exploration of the limitations of the proposed method, such as its applicability to other SSL architectures beyond those tested. Additionally, the mathematical formulations, while necessary, may be somewhat dense for readers not well-versed in the subject.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper maintains a high level of clarity in its structure and presentation, although some sections containing mathematical details may challenge comprehension. The quality of writing is generally good, and the logical flow from introduction to conclusion is coherent. In terms of novelty, SyncCP represents a significant advancement by combining concepts of sparsity and contrastive learning, contributing to the growing field of efficient SSL. The reproducibility of results could be enhanced by providing more detailed descriptions of experimental setups, hyperparameters, and datasets used.\n\n# Summary Of The Review\nOverall, the paper presents a well-structured and innovative approach to reducing computational costs in contrastive self-supervised learning through synchronized pruning techniques. While the empirical results are strong and the contributions to the field are significant, there are some areas for improvement in terms of exploring limitations and enhancing reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new method called Synchronized Contrastive Pruning (SyncCP) aimed at improving the efficiency of self-supervised learning (SSL) by addressing the challenges associated with large encoders. The authors claim that existing state-of-the-art (SoTA) SSL methods are inefficient due to their reliance on such architectures and propose their method as a solution. However, the experimental validation of SyncCP lacks depth, with the reported performance improvements being marginal and not sufficiently contextualized within the broader landscape of SSL research.\n\n# Strength And Weaknesses\nWhile the paper attempts to fill a perceived research gap regarding the correlation between in-training sparsity and SSL, it fails to convincingly argue the significance of this gap or how SyncCP effectively addresses it. The methodology presented appears derivative, lacking clear innovations compared to existing techniques. The experimental results do not adequately demonstrate the robustness of SyncCP, nor do they account for variability across datasets. Additionally, the emphasis on metrics against SoTA methods is one-sided, undermining the credibility of their findings. The paper also introduces unnecessary complexity with varying sparsity granularities without sufficient justification.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper lacks clarity in articulating the significance of its contributions, which diminishes its overall quality. The novelty of SyncCP is questionable, as it does not present clear advancements over previous methods. Furthermore, the reproducibility of the results is undermined by insufficient discussion of potential failure modes and limitations of the approach. The lack of rigorous evaluation in real-world applications raises concerns about the practical utility of the proposed method.\n\n# Summary Of The Review\nOverall, the paper presents a method that aims to improve SSL efficiency but falls short in terms of convincing arguments, novelty, and empirical validation. The findings, while interesting, are not sufficiently robust or significant to warrant the claims made by the authors.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces an innovative framework called Synchronized Contrastive Pruning (SyncCP) aimed at enhancing the efficiency of contrastive self-supervised learning (SSL). The authors propose a novel methodology that incorporates synchronized pruning across multiple encoders, achieving significant speedups in training while maintaining high accuracy. The findings demonstrate that SyncCP outperforms existing state-of-the-art sparse learning algorithms across various datasets, including CIFAR-10, CIFAR-100, and ImageNet, while also being versatile enough to support different sparsity granularities.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its groundbreaking approach to SSL, which effectively maximizes energy efficiency and performance through synchronized pruning. The empirical results are compelling, showcasing state-of-the-art performance that validates the framework's effectiveness. However, a potential weakness is the lack of extensive ablation studies that could further elucidate the contributions of each component within SyncCP, as well as the generalizability of the results across other tasks and domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making it accessible to readers within the field. The quality of the experiments is high, and the results are presented in a manner that allows for easy comparison with existing methods. The novelty is evident in the proposed methodology, which addresses a key challenge in SSL. However, there could be further emphasis on reproducibility, such as providing detailed hyperparameters and training setups used in experiments.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of self-supervised learning through its innovative SyncCP framework, which balances training efficiency and model performance effectively. While the results are impressive, additional analysis on the framework's components and reproducibility details would enhance the study's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces Synchronized Contrastive Pruning (SyncCP), a novel theoretical framework aimed at enhancing the efficiency of contrastive self-supervised learning (SSL) through synchronized sparsification. The authors argue that SyncCP addresses the tension between achieving high sparsity levels and maintaining the integrity of contrastive learning by ensuring a consistent sparsity gap between dual encoders. The methodology involves a theoretical exploration of sparsification techniques, including the Contrastive Sparsification Indicator (CSI), which allows for optimal sparsity adjustments based on model learning dynamics. The findings suggest that synchronized sparsification not only stabilizes the learning process but also improves the interpretability and performance of SSL models.\n\n# Strength And Weaknesses\nOne of the paper's key strengths is its contribution to the theoretical understanding of sparsity in SSL, filling a notable gap in the literature regarding the compatibility of existing supervised pruning techniques with SSL frameworks. The introduction of SyncCP and CSI offers a fresh perspective on enhancing training efficiency and model performance. However, a significant weakness lies in the lack of empirical validation of the theoretical constructs presented. While the theoretical implications are compelling, the absence of practical experiments to support the claims may limit the paper's impact and applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical concepts clearly, making it accessible to readers familiar with the fields of self-supervised learning and neural network sparsification. The quality of the theoretical exposition is high; however, the novelty primarily hinges on theoretical contributions rather than empirical evidence. Reproducibility may also be a concern, as the proposed models and frameworks lack practical validation, which could hinder researchers from applying these concepts without additional guidance.\n\n# Summary Of The Review\nOverall, the paper makes significant theoretical contributions to the understanding of synchronized sparsification in contrastive self-supervised learning. While it effectively addresses a gap in the literature, the lack of empirical evidence to support its claims may limit its practical impact. Further experimental validation is needed to substantiate the proposed methodologies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel method called Synchronized Contrastive Pruning (SyncCP) aimed at enhancing the efficiency of contrastive self-supervised learning (SSL). The key contribution lies in its investigation of in-training sparsity, leveraging both online and offline encoders while maintaining a synchronized sparsity gap (∆s) to ensure consistency during training. The authors evaluate SyncCP on several datasets, including CIFAR-10, CIFAR-100, and ImageNet, demonstrating its effectiveness through various sparsity granularities such as element-wise, N:M structured, and hardware-specific structured sparsity. The findings indicate significant improvements over state-of-the-art algorithms in terms of training speed and computational cost, with detailed algorithmic steps and pseudo-code provided for reproducibility.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to integrating sparsity into contrastive learning, which addresses a critical gap in existing literature. The extensive evaluation across multiple datasets and sparsity granularities bolsters the robustness of the findings. Additionally, the introduction of the Contrastive Sparsification Indicator (CSI) method offers a practical tool for selecting optimal sparsity points. However, the paper could benefit from a more detailed analysis of the trade-offs between sparsity levels and model performance to provide deeper insights into the implications of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and results, making it accessible to readers. The quality of the experiments is high, with thorough descriptions of training setups, hyperparameters, and augmentation strategies. The novelty of the approach is significant, particularly in the context of SSL, and the reproducibility is enhanced by the inclusion of pseudo-code and the commitment to open-source implementation. However, further clarifications on specific implementation details could improve the overall clarity.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of self-supervised learning by introducing an effective method for integrating synchronized sparsity during training. The methodology is well-supported by empirical results, and the commitment to reproducibility strengthens its impact. Minor improvements in discussing trade-offs would further enhance its contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Synchronized Contrastive Pruning (SyncCP), a method aimed at enhancing sparse training for self-supervised learning (SSL). The authors claim that SyncCP offers improved performance and efficiency compared to existing methods such as SDCLR and GraNet. The methodology revolves around a novel pruning technique, which is purported to allow for various granularities of sparsity while accelerating training speeds. Despite these claims, the paper lacks comprehensive comparisons with a broader range of state-of-the-art (SoTA) techniques and fails to provide sufficient contextual analysis or empirical evidence to substantiate its assertions.\n\n# Strength And Weaknesses\nThe primary strength of SyncCP lies in its proposed algorithm for sparse training, which the authors argue is an advancement in SSL. However, the paper's weaknesses are significant. It heavily relies on critiques of existing methods without sufficiently articulating how SyncCP improves upon them. The evaluation of its performance on datasets such as CIFAR-10 and ImageNet appears promising, yet the lack of comprehensive comparisons raises concerns about the robustness of its claims. Furthermore, the authors' assertions regarding compatibility with various sparsity granularities and energy-efficient computations are not adequately supported by empirical data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hindered by the overemphasis on critiques of previous works rather than a balanced presentation of its contributions. The quality of the analysis is undermined by the absence of rigorous comparisons with other SoTA methods, leaving the novelty of the approach in question. The reproducibility of the results is not thoroughly addressed, as the lack of detailed comparative metrics limits the ability to validate the claims made regarding efficiency and effectiveness.\n\n# Summary Of The Review\nOverall, while SyncCP presents a novel approach to sparse training in SSL, its reliance on the critique of existing methods and insufficient empirical validation detracts from its overall impact and credibility. The lack of comprehensive comparisons with SoTA techniques raises critical questions about the claimed advancements and the robustness of its findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to efficient contrastive self-supervised learning through a technique called synchronized pruning. The methodology involves systematically reducing the size of neural network encoders while maintaining performance, addressing the challenges posed by large models in terms of computational efficiency. The findings demonstrate that the proposed synchronized pruning strategy not only improves energy efficiency but also achieves competitive or superior performance compared to state-of-the-art (SoTA) methods on benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to pruning, which is well-motivated and addresses a critical issue in the deployment of self-supervised learning models. The experiments are thorough, showcasing the effectiveness of synchronized pruning against various baseline methods. However, the paper could benefit from a more extensive analysis of the trade-offs between model size and accuracy, as well as a clearer discussion of the limitations of the proposed method. Additionally, some inconsistencies in notation and terminology detract from the overall clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but it suffers from a few typographical and formatting issues that affect readability. The novel contribution of synchronized pruning is clearly articulated, and the methodology is described in sufficient detail to allow for reproducibility. Nonetheless, certain definitions and acronyms are introduced without prior explanation, which may pose challenges for readers unfamiliar with the terminology. The clarity of figures and tables could also be improved to enhance comprehension.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of self-supervised learning through its innovative pruning method, although it suffers from some clarity and formatting issues. The results are promising, but a deeper exploration of the implications and limitations of the approach would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to synchronized pruning in deep neural networks, specifically focusing on its application within visual representation learning. The methodology involves a systematic removal of parameters across different layers of a neural architecture, aimed at enhancing model efficiency while maintaining performance. Key findings indicate that the proposed synchronized pruning technique achieves significant reductions in model size and computational cost, with minimal degradation in accuracy when benchmarked against traditional sparsification methods.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its innovative approach to synchronized pruning, which may offer substantial benefits in real-world applications by improving computational efficiency. Additionally, the methodology is clearly articulated and demonstrates effective results on ResNet architectures. However, the paper exhibits weaknesses in its limited exploration of the method's applicability to other neural architectures and domains, such as natural language processing or audio processing. Furthermore, a lack of comparative analysis with other contemporary sparsification techniques restricts a comprehensive understanding of its advantages and limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, contributing to a high-quality exposition of the methodology. While the technical novelty of the approach is evident, the discussion on reproducibility could be enhanced by detailing specific hyperparameter tuning strategies and their impact on performance across various datasets. The authors should also consider providing access to code or implementation guidelines to further support replicability of the results.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of model efficiency through innovative synchronized pruning techniques, though it would benefit from a broader analysis of its applicability and a deeper discussion on trade-offs with existing methods. Addressing these areas could significantly enhance the practical relevance of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper proposes a novel self-supervised learning (SSL) method called SyncCP, which investigates the relationship between in-training sparsity and the performance of SSL models across multiple datasets, including CIFAR-10, CIFAR-100, and ImageNet. The methodology emphasizes rigorous statistical testing to validate claims of performance improvements over state-of-the-art (SoTA) methods. The findings indicate that SyncCP achieves superior accuracy while maintaining computational efficiency, although the authors stress the importance of statistical significance in their experimental results.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its thorough statistical analysis and the introduction of the concept of architecture oscillation, which is quantified to examine its impact on performance. The use of multiple datasets enhances the robustness of the claims made. However, the paper could benefit from more detailed reporting of statistical significance, particularly with respect to the various performance metrics, which could enhance the credibility of the findings. Additionally, while the empirical results are promising, a more extensive exploration of generalization across unseen datasets is needed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly, although the complexity of statistical methodology may challenge some readers. The novelty lies in the combination of sparsity and self-supervised learning, which is underexplored in existing literature. However, reproducibility could be improved through more detailed descriptions of the experimental setup and the statistical tests employed, as well as by providing raw data or code to allow for independent verification of results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in self-supervised learning through the introduction of SyncCP and a robust statistical framework for evaluating its performance. While the contributions are noteworthy, the paper would benefit from enhanced clarity in statistical reporting and further validation of generalization across datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a synchronized pruning method aimed at enhancing energy efficiency in self-supervised learning (SSL) frameworks. The authors present a series of experiments that demonstrate performance improvements when applying this method to specific SSL architectures. However, the exploration of its generalizability across different frameworks is limited, and findings primarily focus on short-term results without a thorough analysis of long-term model stability or robustness in varied training environments.\n\n# Strength And Weaknesses\nThe main strength of the paper is its focus on energy efficiency, which is a critical consideration in the deployment of machine learning models, especially in resource-constrained settings. The proposed pruning method shows promise in improving performance for the tested models. However, significant weaknesses include insufficient exploration of the method's applicability to a wider range of SSL architectures and a lack of comprehensive comparisons against state-of-the-art (SoTA) techniques. Additionally, the study fails to address the trade-offs between different levels of sparsity and model performance, leaving important questions regarding optimal configurations unanswered. There is also a notable absence of long-term stability analysis and an exploration of computational costs versus accuracy trade-offs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting its methodology and findings clearly. However, the novelty of the proposed method is somewhat limited by the narrow scope of its evaluation. The reproducibility of results could be enhanced through a more detailed description of the experimental setup and a systematic framework for hyperparameter tuning. The authors do not adequately discuss how their findings could influence future research directions in SSL, which detracts from the paper’s overall impact.\n\n# Summary Of The Review\nWhile the paper presents a potentially valuable method for synchronized pruning in self-supervised learning with a focus on energy efficiency, it suffers from limitations in generalizability, comparative analysis, and a lack of long-term stability assessment. The findings suggest areas for further exploration that could enhance the robustness and applicability of the proposed approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a method called Synchronized Pruning (SyncCP) aimed at addressing inefficiencies in large encoders utilized in self-supervised learning. The authors propose a framework that incorporates sparsity-induced asymmetry during training, asserting that their approach improves model performance while maintaining efficiency. The experimental results claim to demonstrate superiority over state-of-the-art (SoTA) methods, although the paper does not provide compelling evidence of significant advancements.\n\n# Strength And Weaknesses\nWhile the paper attempts to introduce SyncCP as a novel approach to sparsification, much of the underlying concepts are already well-established in the literature. The authors’ discussions often veer into the realm of the obvious, such as acknowledging the labeling bottleneck in deep learning and the context-dependent nature of sparsity. The experimental results, while reporting slight improvements over existing methods, lack the robustness necessary to substantiate their claims of advancement. Overall, the paper suffers from a lack of novelty and depth in its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by its overreliance on jargon and repetitive assertions of superiority without providing sufficient detail on the methodology. The novelty of the proposed SyncCP method is minimal, as it does not significantly depart from existing techniques in the field of model optimization and sparsification. Reproducibility is a concern, as the experimental setup lacks comprehensive detail, making it difficult for other researchers to replicate the results.\n\n# Summary Of The Review\nIn summary, the paper presents a familiar approach to sparsification in self-supervised learning that lacks significant novelty and depth. While it claims to improve performance, the results do not convincingly establish a substantial contribution to the field. The overall impression is that the work feels more like a reiteration of existing concepts rather than a groundbreaking advancement.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces Synchronized Contrastive Pruning (SyncCP), a novel approach aimed at addressing the limitations of current self-supervised learning (SSL) methods that rely on large encoder architectures. The methodology emphasizes in-training sparsity, utilizing a Contrastive Sparsification Indicator (CSI) to manage sparsity increments effectively. The authors demonstrate that SyncCP achieves state-of-the-art accuracy on CIFAR-10 and CIFAR-100 while promoting energy efficiency and training speedup, highlighting the potential for deployment in resource-constrained environments.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to sparsity in SSL, particularly the introduction of CSI as a guiding mechanism for modifying model architecture during training. This not only aligns with the current trend towards dynamic model architectures but also opens up avenues for future research into hybrid models that combine SSL with adaptive pruning strategies. However, a significant weakness is the limited scope of empirical evaluation, as results are primarily focused on CIFAR datasets. Expanding these findings to other datasets would enhance the validation of the proposed method's generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, with a logical flow that aids comprehension. The quality of the experiments appears robust, though reproducibility could be improved by providing more details on the experimental setup and hyperparameter choices. The novelty of the approach is high, particularly in its focus on in-training sparsity and the application of CSI, which distinguishes it from existing methods. Nonetheless, further exploration of its integration with emerging hardware technologies and other domains would bolster its impact.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in self-supervised learning through the proposed SyncCP method, demonstrating significant improvements in model efficiency and accuracy. However, the limited empirical evaluation on diverse datasets suggests a need for further validation to establish the generalizability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Synchronized Contrastive Pruning (SyncCP), a novel method aimed at improving the performance of self-supervised learning models under varying levels of sparsity. The methodology involves a synchronized approach to contrastive learning while incorporating structured sparsity. The findings demonstrate that SyncCP achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet datasets, outperforming existing methods such as MoCo-V2 and GraNet, especially at high sparsity levels. Notably, SyncCP achieves significant accuracy while also improving inference time and training speed, indicating both effectiveness and efficiency in model deployment.\n\n# Strength And Weaknesses\nThe strengths of the paper include its strong empirical results, particularly the superior performance of SyncCP across various datasets and sparsity levels. The method demonstrates a clear advantage in accuracy while also offering substantial reductions in inference time and training acceleration, which are critical for real-world applications. However, a potential weakness could be the lack of extensive theoretical analysis or insights into the underlying mechanisms of why SyncCP excels compared to existing methods. More detailed discussions on the limitations of the approach and specific scenarios where it may not perform as well would enhance the paper's robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its contributions, methodology, and results. The quality of the writing is high, making it accessible to readers with varying levels of familiarity with the topic. The novelty of SyncCP lies in its synchronized approach to contrastive learning coupled with structured sparsity, which is a fresh perspective in the field. However, while the empirical results are convincing, the reproducibility of the findings would benefit from clearer descriptions of the experimental setups, including hyperparameter choices and training protocols, to enable others to replicate the results effectively.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling method in Synchronized Contrastive Pruning (SyncCP) that demonstrates significant improvements in both accuracy and efficiency in self-supervised learning. While the empirical results are impressive and highlight the method's potential, further theoretical insights and clearer reproducibility guidelines would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to self-supervised learning (SSL) that emphasizes the integration of sparsity-induced asymmetry within a sparse-dense architecture framework. The authors introduce a new methodology that leverages contrastive learning techniques to enhance the computational efficiency of SSL while addressing the inherent challenges of label-free learning. The findings demonstrate significant improvements in performance metrics across various benchmarks, suggesting that this approach not only reduces computation costs but also enhances the generalization capabilities of models trained under this framework.\n\n# Strength And Weaknesses\nThe paper contributes to the field of SSL by proposing an innovative architectural design that effectively combines sparsity and density, which is a notable advancement. The methodology is well-articulated, and the experimental results are compelling, showcasing the strengths of the proposed approach. However, the paper suffers from some repetition of ideas, particularly concerning the challenges faced in SSL, which detracts from its conciseness. Additionally, certain sections could benefit from clearer definitions of technical terms to enhance accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written, the clarity could be improved by simplifying complex sentences and providing consistent terminology throughout. There are instances of awkward phrasing and technical jargon that may limit comprehension for less experienced readers. The reproducibility of the findings is supported by the inclusion of detailed methodologies; however, the organization of appendices could be enhanced to facilitate easier navigation through supplementary information.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to self-supervised learning with its unique focus on sparsity and architectural design, although clarity and organization issues hinder its impact. The findings are promising and suggest avenues for future research, but the paper would benefit from revisions to enhance readability and accessibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.659036822136591,
    -1.6840326313359155,
    -1.7822469542747088,
    -1.969272770980152,
    -1.7854915683966586,
    -1.5085434598798868,
    -1.592160619624456,
    -2.053382567940102,
    -1.448926479551642,
    -1.988105261202029,
    -1.5892597221287939,
    -1.482844069969037,
    -1.461258740135866,
    -1.5835731497579852,
    -1.6582887870637308,
    -1.6755043792971585,
    -1.987524830800482,
    -1.625857685956542,
    -1.817271941861595,
    -1.784698426380589,
    -1.8565917711224773,
    -1.5948506573555152,
    -1.8228944991338416,
    -1.8375215179987157,
    -1.7998662555263722,
    -1.9003007276126704,
    -1.7989547290655403,
    -1.482094321666678,
    -1.8577527182971292
  ],
  "logp_cond": [
    [
      0.0,
      -2.2257666094690176,
      -2.159703424794998,
      -2.2102906925345414,
      -2.2980772908650264,
      -2.21060227596902,
      -2.392236695196224,
      -2.216225938753727,
      -2.2734665046139457,
      -2.320562338394498,
      -2.2996936434584287,
      -2.424437758540107,
      -2.2763662502465682,
      -2.191162860569087,
      -2.2772703180412934,
      -2.1702368642839454,
      -2.270888441104463,
      -2.2544211472371263,
      -2.2433809139998284,
      -2.1374095352844815,
      -2.2731524954438456,
      -2.335973934559004,
      -2.3673268736371322,
      -2.2857967321731327,
      -2.303182288266092,
      -2.3121533386776294,
      -2.2187179405357766,
      -2.2619749516003984,
      -2.3461687198777343
    ],
    [
      -1.3498667797455932,
      0.0,
      -1.223499720400328,
      -1.1662789247084813,
      -1.3491472307969583,
      -1.2921030224978398,
      -1.4498767086527362,
      -1.199869866908922,
      -1.308624317226284,
      -1.3553233864475807,
      -1.3243053336774688,
      -1.454605975675346,
      -1.3396863857791277,
      -1.2712129251602629,
      -1.3588783153549349,
      -1.2144812300079506,
      -1.3026478054388588,
      -1.3316803617417843,
      -1.3143445545424657,
      -1.267752924604744,
      -1.346801339167933,
      -1.3918428321970047,
      -1.4597420957675582,
      -1.3518459771998963,
      -1.4081309090867282,
      -1.3449952817703419,
      -1.2938325329281475,
      -1.3527149264777487,
      -1.4099825901584462
    ],
    [
      -1.3668407332530643,
      -1.3283964530233934,
      0.0,
      -1.2832960120007737,
      -1.373091518824509,
      -1.3509419430124534,
      -1.5070219057446392,
      -1.255741865378523,
      -1.4183636315709043,
      -1.4229711216554077,
      -1.3600499679997755,
      -1.5444127907503986,
      -1.3785319937501843,
      -1.3295491754796187,
      -1.4090223555304888,
      -1.2516503010459663,
      -1.3059079778399814,
      -1.3839783558912535,
      -1.3161925301552564,
      -1.296738528167624,
      -1.3798280614564469,
      -1.4497824505379155,
      -1.5166376009753033,
      -1.463897795608626,
      -1.4201348704452763,
      -1.384181750593704,
      -1.3281241602179443,
      -1.4657375969563216,
      -1.4786213361522964
    ],
    [
      -1.514258736179226,
      -1.3680236963756838,
      -1.4052018186876967,
      0.0,
      -1.5598304030788315,
      -1.4421118193732694,
      -1.6158236376624213,
      -1.4331298745548342,
      -1.512154895535715,
      -1.5393190016527285,
      -1.5831411397977515,
      -1.6886689475437582,
      -1.5088329118698878,
      -1.4501192526053,
      -1.5723018377205278,
      -1.3824885373309344,
      -1.5003858791246318,
      -1.549033628239667,
      -1.470287803572006,
      -1.4170486837706828,
      -1.552523144928677,
      -1.6042275278562212,
      -1.6609210007528326,
      -1.4602933556454847,
      -1.5912179277796792,
      -1.599465339153596,
      -1.4660528674136544,
      -1.5654285688636256,
      -1.6188175963353362
    ],
    [
      -1.3606194488194696,
      -1.3891385206088567,
      -1.313470247161988,
      -1.375470958193695,
      0.0,
      -1.3566564681628608,
      -1.4998777636396352,
      -1.3588116633299887,
      -1.3659339286865264,
      -1.3914511992909293,
      -1.4008177440810645,
      -1.5153438793359475,
      -1.3447662194506016,
      -1.3268044164899087,
      -1.388279461336978,
      -1.3090434599787728,
      -1.3282322233671424,
      -1.2975021568064764,
      -1.3436932965714543,
      -1.3388705095120517,
      -1.3252064543486572,
      -1.3905534277090799,
      -1.4572193965237357,
      -1.355628757265274,
      -1.3227825861986433,
      -1.431938815391356,
      -1.3371349607379561,
      -1.3572356963236185,
      -1.478666954880445
    ],
    [
      -1.149567956196655,
      -1.1352296077979245,
      -1.0881394154012431,
      -1.0718753445690126,
      -1.1897760335392273,
      0.0,
      -1.2012403686176054,
      -1.110832015414128,
      -1.1680028323134246,
      -1.159806708215959,
      -1.1693996219279772,
      -1.2556643581758333,
      -1.1716208439839981,
      -1.0345626251773075,
      -1.1696895285233795,
      -1.077038942801514,
      -1.0990753330765606,
      -1.1401250092514987,
      -1.0851477498106294,
      -1.0576792660624956,
      -1.159133511656799,
      -1.1861745093508198,
      -1.2359097101863892,
      -1.1991867453774439,
      -1.1974972961228512,
      -1.177697794835854,
      -1.1766466892867118,
      -1.1952356274830287,
      -1.1954919528313628
    ],
    [
      -1.2990200643646406,
      -1.256670833366513,
      -1.244817330775172,
      -1.1948027137048505,
      -1.2243736846897664,
      -1.2533946554329067,
      0.0,
      -1.2255007628782786,
      -1.2244843724099077,
      -1.2228626787226913,
      -1.2426788434677174,
      -1.2722990945420576,
      -1.2688467425229577,
      -1.2387923804152758,
      -1.3050122546961118,
      -1.2046523656020447,
      -1.2451676516361276,
      -1.2216630695212765,
      -1.2154003959252646,
      -1.2604508771239145,
      -1.2142023324087317,
      -1.252148055721988,
      -1.254693333334558,
      -1.2329796299425468,
      -1.2343594383159182,
      -1.3132686155790767,
      -1.2472478177426776,
      -1.246302136402677,
      -1.2474819962662533
    ],
    [
      -1.6572605990298905,
      -1.505098660380937,
      -1.5184799354864127,
      -1.5617004050677012,
      -1.6576073481608926,
      -1.6218653586984415,
      -1.7842416657033866,
      0.0,
      -1.6693657877168606,
      -1.700641277915868,
      -1.646616073535198,
      -1.8612456279290388,
      -1.6336403739195635,
      -1.5830585454054205,
      -1.6893615826064476,
      -1.5804195243353578,
      -1.5797300035053385,
      -1.660188071876827,
      -1.5980001514465716,
      -1.6161586309251068,
      -1.62270272371943,
      -1.6922330310090716,
      -1.7889462604238346,
      -1.699122153649749,
      -1.7269145313951897,
      -1.632657300285418,
      -1.6233048435726627,
      -1.6985327630301492,
      -1.7688861601533579
    ],
    [
      -1.1033951193823195,
      -1.0335369862900967,
      -1.0808513769840178,
      -1.0422741521607186,
      -1.067096049661569,
      -1.0914030357096722,
      -1.155799229656328,
      -1.0203911794366163,
      0.0,
      -1.1003015537110343,
      -1.0741272391485794,
      -1.21001017227053,
      -1.050307504216679,
      -1.0419929807212203,
      -1.1169475601133798,
      -1.035158476987627,
      -1.0096781766331544,
      -1.042679695550573,
      -1.0604761532704243,
      -1.0703425414696608,
      -1.0691456500490408,
      -1.1621776721971735,
      -1.2075529870840795,
      -1.0490369342157781,
      -1.1337028675539709,
      -1.1041820305661663,
      -1.0801298726756179,
      -1.0099585120182317,
      -1.1435495591877491
    ],
    [
      -1.6342216498782283,
      -1.6248123296906951,
      -1.608679572832076,
      -1.538612111866526,
      -1.642601884650001,
      -1.6118364433614312,
      -1.7040282853478719,
      -1.5445290318343268,
      -1.6175041081482142,
      0.0,
      -1.6573413511405506,
      -1.787902500093256,
      -1.6304215375453965,
      -1.5995522329879772,
      -1.6796440880377859,
      -1.5817415241083619,
      -1.589894804494923,
      -1.6357066630738102,
      -1.5911295505029641,
      -1.5800461124518963,
      -1.6321760731132406,
      -1.658691978795605,
      -1.744364000193272,
      -1.6475609065591996,
      -1.6353327016948356,
      -1.6848481568769065,
      -1.6494728232768727,
      -1.6638314220781456,
      -1.6567907024235031
    ],
    [
      -1.2183421752543147,
      -1.1178019561194776,
      -1.0521852552975846,
      -1.0994968246883348,
      -1.13140038126166,
      -1.153225060748269,
      -1.254875607336345,
      -1.0943206247640338,
      -1.142753708340198,
      -1.2211549956342542,
      0.0,
      -1.3178283123734258,
      -1.1210586454075755,
      -1.1063172539782045,
      -1.183143985087443,
      -1.085267210389119,
      -1.061469589132177,
      -1.1257073932746868,
      -1.1486871689904943,
      -1.1568875050616243,
      -1.167213327190893,
      -1.2038534692049077,
      -1.2898863677851213,
      -1.1576225912699196,
      -1.2094578285234243,
      -1.1725548907462655,
      -1.1228848709891024,
      -1.1274055560108616,
      -1.2415886462166155
    ],
    [
      -1.252306971624655,
      -1.232589333657378,
      -1.254076040627664,
      -1.2146213078284687,
      -1.237762926360244,
      -1.2241945931585574,
      -1.2302785047992426,
      -1.2507803031610256,
      -1.2180003982443186,
      -1.212350339753553,
      -1.248544484331547,
      0.0,
      -1.2597691684260397,
      -1.2302665517192908,
      -1.2253364079789364,
      -1.219949565248951,
      -1.211281008307877,
      -1.259005651865514,
      -1.2415895461183846,
      -1.2281998442908524,
      -1.235578668244728,
      -1.187705594205407,
      -1.2037613012150827,
      -1.203328980078701,
      -1.2080427705656653,
      -1.2109430466948066,
      -1.2281397720200224,
      -1.2363492185414047,
      -1.172585709401537
    ],
    [
      -1.0767357184876363,
      -1.058674144826708,
      -0.9539988255712246,
      -0.9950124950374021,
      -1.0290141812204394,
      -1.0305728808223884,
      -1.1517490655568001,
      -0.9536515923109316,
      -1.0171793355017704,
      -1.105262160371175,
      -0.9961157237435941,
      -1.2158082837563073,
      0.0,
      -0.9950329722294959,
      -1.0341686664862093,
      -1.0105245413850232,
      -0.9885068531417476,
      -1.0081675093660432,
      -1.0252065969777113,
      -0.9862084492075103,
      -1.0357046641069882,
      -1.064143149590343,
      -1.1386574545564003,
      -1.093549017478377,
      -1.0641492592358663,
      -1.0788432100804137,
      -1.0250196837645886,
      -1.0530140123448584,
      -1.1081741805878718
    ],
    [
      -1.1845083242239505,
      -1.1334633599196307,
      -1.0912931088203595,
      -1.0642300392088302,
      -1.213193362654233,
      -1.0513207723908082,
      -1.2861486982012253,
      -1.108494074304993,
      -1.2069907287969466,
      -1.211893323197627,
      -1.2242609212224809,
      -1.3515436919469181,
      -1.1932891124153204,
      0.0,
      -1.215188868287635,
      -1.0214365966212957,
      -1.1578417753857317,
      -1.1597946665346799,
      -1.0462529676712424,
      -1.0714391525784939,
      -1.2157961126070513,
      -1.234504089039975,
      -1.313087448893683,
      -1.226121236827289,
      -1.2636882812295884,
      -1.2437754939061603,
      -1.151696498490947,
      -1.215034625153584,
      -1.2898427298299187
    ],
    [
      -1.3005214122335826,
      -1.223994147887347,
      -1.2038908283834338,
      -1.197902863818009,
      -1.230606087442475,
      -1.2400171139334415,
      -1.3544310723121356,
      -1.1997656227662594,
      -1.2338325735903017,
      -1.3126979229560374,
      -1.246997936250996,
      -1.3706303557556452,
      -1.2079681333995154,
      -1.188733251489249,
      0.0,
      -1.1922711734143772,
      -1.1624485185298297,
      -1.2456670584083454,
      -1.2542209997694613,
      -1.1878407081774887,
      -1.227737563283245,
      -1.2917548155086813,
      -1.341906437860293,
      -1.2010463466613568,
      -1.29568575410729,
      -1.2081675282881688,
      -1.2208366298242577,
      -1.2611154815073786,
      -1.2884104764115893
    ],
    [
      -1.3147949079072203,
      -1.256230027712605,
      -1.1960170986891554,
      -1.1502810429911976,
      -1.2935563898166311,
      -1.242372270159497,
      -1.4043905933666367,
      -1.254105933939045,
      -1.3091646015871703,
      -1.3169885383468707,
      -1.3171772930479058,
      -1.4443193361398974,
      -1.3350814085524572,
      -1.1727763989963982,
      -1.3226783062132412,
      0.0,
      -1.267091632104851,
      -1.2865818622028542,
      -1.2419576556492524,
      -1.2196897178391404,
      -1.3101662597303203,
      -1.3470220393948409,
      -1.413689086946926,
      -1.3022940870588786,
      -1.3515534129178504,
      -1.3446028036936675,
      -1.2838878949587453,
      -1.314901118690788,
      -1.4145082668093496
    ],
    [
      -1.593125213656264,
      -1.5370608683329068,
      -1.4496992040992158,
      -1.5311307320756047,
      -1.5900214178698275,
      -1.5770177887725962,
      -1.66914833378432,
      -1.4871984306922883,
      -1.5725102752328364,
      -1.6213405974157342,
      -1.5219200379018936,
      -1.7330860134940915,
      -1.547958114758695,
      -1.5509498294082342,
      -1.611750331835429,
      -1.543643476796198,
      0.0,
      -1.544266539722514,
      -1.5564494504055766,
      -1.493689972655343,
      -1.5318526191200446,
      -1.6051253577739755,
      -1.7310128696940696,
      -1.5559400452009278,
      -1.599819422102863,
      -1.5603004484868401,
      -1.5243536739007781,
      -1.594798571365148,
      -1.6794744890232745
    ],
    [
      -1.2064708213284536,
      -1.2060133294343094,
      -1.1066969442440382,
      -1.1456134576684038,
      -1.1596345465684292,
      -1.177256939683534,
      -1.2628202271570457,
      -1.2137353934937714,
      -1.172318389326402,
      -1.2375097249308842,
      -1.2141633579632551,
      -1.357552377253312,
      -1.1790314517393787,
      -1.124990821085112,
      -1.2299413080128816,
      -1.1438225149505827,
      -1.1972189081414042,
      0.0,
      -1.1751017687289533,
      -1.0702741561388904,
      -1.1837262910146562,
      -1.2319487144119563,
      -1.325361439860726,
      -1.2429993772951577,
      -1.284116106358974,
      -1.2832021852016477,
      -1.1816631020145607,
      -1.1830612926615982,
      -1.2658732244711537
    ],
    [
      -1.4298728826339506,
      -1.371899836110894,
      -1.317346124730096,
      -1.3204573817439795,
      -1.4158563645813438,
      -1.288869873600486,
      -1.4989906464407952,
      -1.3273521087679856,
      -1.4096015609903907,
      -1.3963144175453792,
      -1.434312216832993,
      -1.5956186119390483,
      -1.40478462856505,
      -1.295908227731736,
      -1.4899232337459443,
      -1.2974986718673196,
      -1.3985359425368087,
      -1.390717207399766,
      0.0,
      -1.3166189100761048,
      -1.4134836759175806,
      -1.4592188445284346,
      -1.5357472180170597,
      -1.4503105817448945,
      -1.4646021116856365,
      -1.4686114249000728,
      -1.3661826441637073,
      -1.4065935984218698,
      -1.521773586472732
    ],
    [
      -1.3291932746870558,
      -1.317462062993522,
      -1.2297829098904178,
      -1.2646334642127475,
      -1.4249875715364364,
      -1.2493330834459537,
      -1.4793732010284824,
      -1.3372424773200906,
      -1.3876583084799659,
      -1.364732455710564,
      -1.395262451850702,
      -1.5266340801423715,
      -1.3392712994964642,
      -1.3031722102842338,
      -1.406528513725498,
      -1.2753132421545408,
      -1.3149657587876487,
      -1.3294777474797779,
      -1.295160651267767,
      0.0,
      -1.375548847476513,
      -1.4819498667484934,
      -1.48622315466266,
      -1.3608708648315273,
      -1.4397145156656508,
      -1.4155799000511422,
      -1.320173155398355,
      -1.4096347972273031,
      -1.474912498465157
    ],
    [
      -1.5024696924673469,
      -1.4951978416934713,
      -1.4781716974530839,
      -1.468281671732139,
      -1.4488657152944746,
      -1.4886811537200337,
      -1.549148934460639,
      -1.4224948341339787,
      -1.4791428017215669,
      -1.5257820969190499,
      -1.4941635744899198,
      -1.6552884755459987,
      -1.4941221951853216,
      -1.472883662485943,
      -1.5039127826644947,
      -1.4589319494700281,
      -1.4214975921123223,
      -1.450612897843668,
      -1.4710849886377886,
      -1.4209681790182012,
      0.0,
      -1.519458409117496,
      -1.5986577996544238,
      -1.4815210383744144,
      -1.485830619264931,
      -1.5209499913375253,
      -1.4505180699980529,
      -1.479794570684385,
      -1.5647355032433625
    ],
    [
      -1.2286207890255634,
      -1.188151611905969,
      -1.1728948659551994,
      -1.1768826394513625,
      -1.2037049030038685,
      -1.1864058556710781,
      -1.2672309591740576,
      -1.1119578317451837,
      -1.1815539125964698,
      -1.1983964779540532,
      -1.1802455864141423,
      -1.2687987595738763,
      -1.227772288690887,
      -1.163830374007052,
      -1.2002322788842166,
      -1.1398589577050686,
      -1.1118432730546979,
      -1.1561488101617785,
      -1.2070171858568741,
      -1.2143554570864472,
      -1.2078920335521683,
      0.0,
      -1.2260180302781538,
      -1.1964576075352935,
      -1.1529830204807527,
      -1.180162087458107,
      -1.1866892611797821,
      -1.2177186731764698,
      -1.1952600522611119
    ],
    [
      -1.5125954838888067,
      -1.4978137963083216,
      -1.4777958953860946,
      -1.4704995882263818,
      -1.4379593833931814,
      -1.4573281664941764,
      -1.4806071419167175,
      -1.4621629611461537,
      -1.4719704973664138,
      -1.4716899125383582,
      -1.4935696625066586,
      -1.4778767720178223,
      -1.4883801175561988,
      -1.458764215824786,
      -1.4348899385654093,
      -1.4591395004856962,
      -1.4447522326241444,
      -1.469022213110722,
      -1.4505972608844637,
      -1.4441957128089296,
      -1.48549024453585,
      -1.4713933868874631,
      0.0,
      -1.4753576667623844,
      -1.3781365198809155,
      -1.435421609754744,
      -1.472490481903618,
      -1.4796423103055878,
      -1.4210432419331351
    ],
    [
      -1.4959798386929803,
      -1.407999592535929,
      -1.4163405986824686,
      -1.3073554452094345,
      -1.387446060160529,
      -1.444762761155261,
      -1.5254005087518752,
      -1.3684024947379874,
      -1.4063060616053968,
      -1.468024983632966,
      -1.448329581032769,
      -1.5700532612087936,
      -1.4147590692801308,
      -1.427189326494837,
      -1.4271836710628414,
      -1.3573173469236732,
      -1.3557690047165363,
      -1.4260965112163875,
      -1.4420498662464118,
      -1.403498950903903,
      -1.4095305288541513,
      -1.4448956534795898,
      -1.5332569422941869,
      0.0,
      -1.4521339954599208,
      -1.4706745864260329,
      -1.3918973679599356,
      -1.4110339599499837,
      -1.5153545053234483
    ],
    [
      -1.504715402739902,
      -1.5079712135966095,
      -1.4388188706501213,
      -1.4702936104889985,
      -1.39147501668552,
      -1.4873789749394144,
      -1.4837331642250609,
      -1.4316110578291648,
      -1.4868578368986594,
      -1.488542180810206,
      -1.5105460952493723,
      -1.5209143204320918,
      -1.4742860690520934,
      -1.4864770724410985,
      -1.475712838980187,
      -1.4441443371520226,
      -1.4224444849468272,
      -1.4659063198197648,
      -1.4391169912060542,
      -1.459909910017104,
      -1.4165113843777988,
      -1.501700893718286,
      -1.4745539322678234,
      -1.4811790740688102,
      0.0,
      -1.4670614475964359,
      -1.4561822452286228,
      -1.4848665752514147,
      -1.5356455397962239
    ],
    [
      -1.5784055271244832,
      -1.476008814044122,
      -1.512809036587797,
      -1.5229811983009698,
      -1.5537442977908396,
      -1.5183109488029956,
      -1.632636538335403,
      -1.455974865238971,
      -1.5307807947347885,
      -1.5773607083045753,
      -1.5027168166302785,
      -1.6130222307383273,
      -1.5755341030251377,
      -1.4882624717750288,
      -1.4805309802152957,
      -1.5095880852744916,
      -1.4090657541804095,
      -1.5378118572646136,
      -1.5136149000058088,
      -1.5162032159637773,
      -1.5418291498346919,
      -1.5473455774762004,
      -1.5583273116503247,
      -1.5137536529746143,
      -1.5525243361044554,
      0.0,
      -1.5289777550718737,
      -1.5460028766540468,
      -1.5263932965033542
    ],
    [
      -1.3991924666659767,
      -1.2958409479397208,
      -1.2554021129114261,
      -1.2837500707745553,
      -1.3718694193499823,
      -1.3050350175593803,
      -1.443546096596504,
      -1.323960819982293,
      -1.3710203855865768,
      -1.4161083170063484,
      -1.3430311960114893,
      -1.5096650950818882,
      -1.3688439162609165,
      -1.3223837153454692,
      -1.4264504296536424,
      -1.2971411033316955,
      -1.2933218214694344,
      -1.3442712617310135,
      -1.3094991372248452,
      -1.2882118911982516,
      -1.3781054731225038,
      -1.4357093031107317,
      -1.4868283471039572,
      -1.3470269052683206,
      -1.415809115757251,
      -1.429784871904227,
      0.0,
      -1.378163399908496,
      -1.4850981306053939
    ],
    [
      -1.182075666349459,
      -1.189328872137494,
      -1.1477929410691687,
      -1.1356693151989403,
      -1.1259781137879086,
      -1.150750646349277,
      -1.1844176556707169,
      -1.1308512694166573,
      -1.1163018405723388,
      -1.1958968186239836,
      -1.140042942956753,
      -1.2512747729202451,
      -1.158513631221447,
      -1.1183766604277507,
      -1.188711219139454,
      -1.104660048902597,
      -1.0988335833221823,
      -1.1007127540905655,
      -1.1313475553662762,
      -1.1392846100448426,
      -1.1084759500694619,
      -1.183744393392814,
      -1.2253741391233537,
      -1.1073050029415048,
      -1.152835862584131,
      -1.2061651766771784,
      -1.1254475214354225,
      0.0,
      -1.229244408361574
    ],
    [
      -1.5915887861265658,
      -1.5487555761004876,
      -1.5420491405646313,
      -1.5394073984687178,
      -1.5528005567602607,
      -1.5647832044567873,
      -1.5781905459890597,
      -1.528582436378696,
      -1.5364334543170166,
      -1.5018020237078489,
      -1.5481196304963754,
      -1.5262818919021195,
      -1.5709167568186146,
      -1.5413111371676158,
      -1.520918087288409,
      -1.53047748066776,
      -1.5353477484937483,
      -1.5604598433127728,
      -1.5435143974194205,
      -1.5403936833421472,
      -1.5720620620975476,
      -1.4739642075429322,
      -1.5207385164386784,
      -1.5112520463139838,
      -1.5460173144197242,
      -1.4757849842926263,
      -1.5633628273176043,
      -1.5517991522779997,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.4332702126675736,
      0.499333397341593,
      0.44874612960204985,
      0.36095953127156477,
      0.4484345461675714,
      0.26680012694036703,
      0.44281088338286434,
      0.3855703175226455,
      0.3384744837420932,
      0.35934317867816246,
      0.2345990635964843,
      0.38267057189002296,
      0.46787396156750427,
      0.3817665040952978,
      0.48879995785264585,
      0.38814838103212823,
      0.4046156748994649,
      0.4156559081367628,
      0.5216272868521097,
      0.38588432669274564,
      0.32306288757758717,
      0.29170994849945897,
      0.3732400899634585,
      0.35585453387049926,
      0.3468834834589618,
      0.44031888160081456,
      0.3970618705361928,
      0.31286810225885686
    ],
    [
      0.3341658515903223,
      0.0,
      0.46053291093558757,
      0.5177537066274343,
      0.3348854005389572,
      0.3919296088380757,
      0.2341559226831793,
      0.48416276442699346,
      0.3754083141096316,
      0.32870924488833486,
      0.3597272976584467,
      0.22942665566056952,
      0.34434624555678783,
      0.41281970617565267,
      0.32515431598098066,
      0.4695514013279649,
      0.3813848258970567,
      0.35235226959413124,
      0.36968807679344984,
      0.41627970673117143,
      0.33723129216798253,
      0.29218979913891086,
      0.22429053556835732,
      0.3321866541360192,
      0.2759017222491873,
      0.33903734956557363,
      0.39020009840776804,
      0.3313177048581668,
      0.27405004117746934
    ],
    [
      0.4154062210216445,
      0.45385050125131543,
      0.0,
      0.4989509422739351,
      0.40915543545019983,
      0.4313050112622554,
      0.27522504853006957,
      0.5265050888961857,
      0.3638833227038045,
      0.35927583261930107,
      0.4221969862749333,
      0.2378341635243102,
      0.4037149605245245,
      0.4526977787950901,
      0.37322459874422,
      0.5305966532287425,
      0.47633897643472745,
      0.3982685983834553,
      0.4660544241194524,
      0.4855084261070848,
      0.40241889281826193,
      0.33246450373679326,
      0.2656093532994055,
      0.3183491586660827,
      0.3621120838294325,
      0.39806520368100484,
      0.4541227940567645,
      0.31650935731838725,
      0.3036256181224124
    ],
    [
      0.45501403480092617,
      0.6012490746044683,
      0.5640709522924554,
      0.0,
      0.4094423679013206,
      0.5271609516068827,
      0.35344913331773076,
      0.5361428964253179,
      0.45711787544443716,
      0.4299537693274236,
      0.38613163118240057,
      0.2806038234363939,
      0.4604398591102643,
      0.5191535183748521,
      0.3969709332596243,
      0.5867842336492177,
      0.46888689185552024,
      0.420239142740485,
      0.49898496740814613,
      0.5522240872094692,
      0.41674962605147514,
      0.36504524312393083,
      0.3083517702273195,
      0.5089794153346674,
      0.3780548432004729,
      0.36980743182655607,
      0.5032199035664977,
      0.4038442021165265,
      0.3504551746448159
    ],
    [
      0.42487211957718896,
      0.3963530477878019,
      0.47202132123467067,
      0.41002061020296354,
      0.0,
      0.4288351002337978,
      0.28561380475702336,
      0.42667990506666986,
      0.41955763971013216,
      0.39404036910572926,
      0.38467382431559405,
      0.27014768906071107,
      0.44072534894605697,
      0.4586871519067499,
      0.3972121070596806,
      0.47644810841788576,
      0.45725934502951615,
      0.48798941159018216,
      0.44179827182520426,
      0.4466210588846069,
      0.4602851140480013,
      0.3949381406875787,
      0.3282721718729229,
      0.42986281113138447,
      0.46270898219801526,
      0.35355275300530264,
      0.44835660765870244,
      0.42825587207304006,
      0.3068246135162136
    ],
    [
      0.35897550368323183,
      0.3733138520819623,
      0.42040404447864366,
      0.43666811531087424,
      0.31876742634065947,
      0.0,
      0.3073030912622814,
      0.3977114444657588,
      0.3405406275664622,
      0.3487367516639277,
      0.3391438379519096,
      0.2528791017040535,
      0.3369226158958887,
      0.47398083470257935,
      0.3388539313565073,
      0.4315045170783729,
      0.4094681268033262,
      0.36841845062838807,
      0.42339571006925736,
      0.4508641938173912,
      0.3494099482230879,
      0.322368950529067,
      0.27263374969349763,
      0.30935671450244295,
      0.31104616375703564,
      0.3308456650440328,
      0.331896770593175,
      0.3133078323968581,
      0.313051507048524
    ],
    [
      0.2931405552598154,
      0.3354897862579429,
      0.347343288849284,
      0.3973579059196055,
      0.3677869349346896,
      0.33876596419154925,
      0.0,
      0.3666598567461774,
      0.3676762472145483,
      0.3692979409017647,
      0.3494817761567386,
      0.3198615250823984,
      0.3233138771014983,
      0.35336823920918015,
      0.28714836492834417,
      0.3875082540224113,
      0.34699296798832835,
      0.37049755010317953,
      0.3767602236991914,
      0.3317097425005415,
      0.37795828721572433,
      0.34001256390246803,
      0.33746728628989797,
      0.35918098968190915,
      0.3578011813085378,
      0.2788920040453793,
      0.3449128018817784,
      0.34585848322177903,
      0.34467862335820265
    ],
    [
      0.39612196891021156,
      0.548283907559165,
      0.5349026324536894,
      0.49168216287240085,
      0.3957752197792095,
      0.4315172092416606,
      0.2691409022367155,
      0.0,
      0.3840167802232415,
      0.35274129002423416,
      0.4067664944049041,
      0.1921369400110633,
      0.4197421940205386,
      0.47032402253468164,
      0.36402098533365446,
      0.4729630436047443,
      0.47365256443476356,
      0.39319449606327517,
      0.4553824164935305,
      0.4372239370149953,
      0.43067984422067207,
      0.36114953693103047,
      0.26443630751626745,
      0.35426041429035315,
      0.3264680365449124,
      0.4207252676546842,
      0.4300777243674394,
      0.3548498049099529,
      0.28449640778674423
    ],
    [
      0.3455313601693226,
      0.4153894932615454,
      0.3680751025676243,
      0.4066523273909235,
      0.3818304298900732,
      0.3575234438419699,
      0.29312724989531413,
      0.42853530011502583,
      0.0,
      0.3486249258406078,
      0.3747992404030627,
      0.23891630728111202,
      0.39861897533496315,
      0.40693349883042185,
      0.3319789194382623,
      0.41376800256401514,
      0.4392483029184877,
      0.40624678400106906,
      0.3884503262812178,
      0.3785839380819813,
      0.3797808295026013,
      0.2867488073544686,
      0.24137349246756257,
      0.39988954533586396,
      0.31522361199767124,
      0.34474444898547585,
      0.3687966068760242,
      0.4389679675334104,
      0.30537692036389297
    ],
    [
      0.35388361132380064,
      0.3632929315113338,
      0.37942568836995294,
      0.449493149335503,
      0.3455033765520279,
      0.37626881784059774,
      0.28407697585415703,
      0.44357622936770214,
      0.3706011530538147,
      0.0,
      0.3307639100614783,
      0.2002027611087729,
      0.35768372365663237,
      0.3885530282140517,
      0.30846117316424304,
      0.406363737093667,
      0.39821045670710586,
      0.3523985981282187,
      0.3969757106990648,
      0.4080591487501326,
      0.35592918808878826,
      0.329413282406424,
      0.24374126100875682,
      0.3405443546428293,
      0.35277255950719333,
      0.30325710432512243,
      0.3386324379251562,
      0.3242738391238833,
      0.3313145587785258
    ],
    [
      0.3709175468744792,
      0.4714577660093162,
      0.5370744668312093,
      0.4897628974404591,
      0.45785934086713387,
      0.4360346613805248,
      0.3343841147924489,
      0.49493909736476005,
      0.4465060137885959,
      0.3681047264945396,
      0.0,
      0.27143140975536806,
      0.46820107672121836,
      0.4829424681505894,
      0.40611573704135084,
      0.5039925117396749,
      0.5277901329966168,
      0.463552328854107,
      0.44057255313829957,
      0.43237221706716955,
      0.42204639493790075,
      0.3854062529238862,
      0.2993733543436725,
      0.43163713085887423,
      0.37980189360536953,
      0.41670483138252834,
      0.46637485113969146,
      0.4618541661179323,
      0.3476710759121784
    ],
    [
      0.23053709834438196,
      0.25025473631165895,
      0.22876802934137297,
      0.2682227621405684,
      0.24508114360879296,
      0.2586494768104797,
      0.25256556516979445,
      0.23206376680801144,
      0.26484367172471845,
      0.27049373021548395,
      0.2342995856374901,
      0.0,
      0.22307490154299736,
      0.2525775182497463,
      0.25750766199010067,
      0.26289450472008613,
      0.27156306166115995,
      0.22383841810352312,
      0.2412545238506525,
      0.2546442256781847,
      0.247265401724309,
      0.29513847576363017,
      0.2790827687539543,
      0.279515089890336,
      0.2748012994033717,
      0.2719010232742305,
      0.2547042979490146,
      0.2464948514276324,
      0.3102583605675
    ],
    [
      0.3845230216482298,
      0.402584595309158,
      0.5072599145646415,
      0.466246245098464,
      0.4322445589154267,
      0.4306858593134777,
      0.309509674579066,
      0.5076071478249345,
      0.4440794046340957,
      0.3559965797646911,
      0.46514301639227196,
      0.24545045637955876,
      0.0,
      0.46622576790637016,
      0.4270900736496568,
      0.45073419875084286,
      0.4727518869941185,
      0.45309123076982294,
      0.43605214315815477,
      0.4750502909283558,
      0.42555407602887785,
      0.39711559054552303,
      0.32260128557946577,
      0.367709722657489,
      0.39710948089999976,
      0.38241553005545237,
      0.43623905637127747,
      0.40824472779100773,
      0.35308455954799434
    ],
    [
      0.3990648255340348,
      0.4501097898383546,
      0.4922800409376258,
      0.519343110549155,
      0.3703797871037522,
      0.532252377367177,
      0.2974244515567599,
      0.4750790754529923,
      0.37658242096103867,
      0.37167982656035825,
      0.3593122285355044,
      0.2320294578110671,
      0.3902840373426648,
      0.0,
      0.3683842814703502,
      0.5621365531366895,
      0.4257313743722535,
      0.42377848322330536,
      0.5373201820867428,
      0.5121339971794914,
      0.3677770371509339,
      0.34906906071801025,
      0.27048570086430224,
      0.35745191293069634,
      0.3198848685283968,
      0.339797655851825,
      0.4318766512670382,
      0.3685385246044013,
      0.2937304199280666
    ],
    [
      0.3577673748301482,
      0.4342946391763838,
      0.4543979586802971,
      0.4603859232457219,
      0.42768269962125594,
      0.41827167313028935,
      0.3038577147515953,
      0.4585231642974714,
      0.4244562134734291,
      0.34559086410769346,
      0.41129085081273487,
      0.28765843130808566,
      0.4503206536642155,
      0.4695555355744818,
      0.0,
      0.4660176136493537,
      0.4958402685339012,
      0.4126217286553855,
      0.4040677872942695,
      0.4704480788862422,
      0.4305512237804858,
      0.3665339715550495,
      0.3163823492034379,
      0.45724244040237405,
      0.3626030329564409,
      0.4501212587755621,
      0.43745215723947317,
      0.3971733055563522,
      0.3698783106521415
    ],
    [
      0.36070947138993814,
      0.4192743515845534,
      0.47948728060800305,
      0.5252233363059609,
      0.38194798948052733,
      0.4331321091376614,
      0.2711137859305217,
      0.42139844535811344,
      0.36633977770998816,
      0.35851584095028777,
      0.3583270862492527,
      0.23118504315726107,
      0.3404229707447013,
      0.5027279803007603,
      0.3528260730839172,
      0.0,
      0.40841274719230736,
      0.38892251709430425,
      0.43354672364790603,
      0.4558146614580181,
      0.36533811956683815,
      0.32848233990231757,
      0.2618152923502324,
      0.37321029223827984,
      0.3239509663793081,
      0.330901575603491,
      0.3916164843384131,
      0.3606032606063705,
      0.26099611248780885
    ],
    [
      0.3943996171442181,
      0.45046396246757525,
      0.5378256267012662,
      0.45639409872487735,
      0.3975034129306545,
      0.4105070420278858,
      0.318376497016162,
      0.5003264001081937,
      0.41501455556764566,
      0.3661842333847478,
      0.46560479289858847,
      0.2544388173063905,
      0.4395667160417871,
      0.4365750013922478,
      0.3757744989650531,
      0.4438813540042841,
      0.0,
      0.4432582910779681,
      0.43107538039490545,
      0.49383485814513906,
      0.4556722116804375,
      0.38239947302650656,
      0.2565119611064124,
      0.43158478559955427,
      0.38770540869761905,
      0.4272243823136419,
      0.4631711568997039,
      0.392726259435334,
      0.3080503417772076
    ],
    [
      0.4193868646280883,
      0.4198443565222325,
      0.5191607417125037,
      0.48024422828813806,
      0.46622313938811266,
      0.44860074627300794,
      0.3630374587994962,
      0.41212229246277055,
      0.4535392966301399,
      0.3883479610256577,
      0.41169432799328676,
      0.2683053087032299,
      0.44682623421716317,
      0.5008668648714298,
      0.3959163779436603,
      0.48203517100595916,
      0.4286387778151377,
      0.0,
      0.45075591722758857,
      0.5555835298176515,
      0.4421313949418857,
      0.3939089715445856,
      0.3004962460958158,
      0.38285830866138415,
      0.3417415795975678,
      0.34265550075489415,
      0.4441945839419812,
      0.4427963932949437,
      0.3599844614853882
    ],
    [
      0.3873990592276444,
      0.44537210575070096,
      0.4999258171314991,
      0.4968145601176155,
      0.40141557728025123,
      0.528402068261109,
      0.31828129542079986,
      0.4899198330936094,
      0.40767038087120433,
      0.4209575243162158,
      0.3829597250286021,
      0.22165332992254672,
      0.412487313296545,
      0.521363714129859,
      0.32734870811565075,
      0.5197732699942754,
      0.41873599932478633,
      0.426554734461829,
      0.0,
      0.5006530317854903,
      0.4037882659440144,
      0.3580530973331604,
      0.2815247238445353,
      0.36696136011670055,
      0.35266983017595854,
      0.3486605169615222,
      0.45108929769788775,
      0.4106783434397252,
      0.29549835538886304
    ],
    [
      0.4555051516935331,
      0.46723636338706687,
      0.5549155164901711,
      0.5200649621678415,
      0.35971085484415255,
      0.5353653429346352,
      0.3053252253521066,
      0.44745594906049835,
      0.3970401179006231,
      0.41996597067002495,
      0.389435974529887,
      0.2580643462382175,
      0.4454271268841248,
      0.48152621609635515,
      0.37816991265509103,
      0.5093851842260482,
      0.4697326675929403,
      0.4552206789008111,
      0.48953777511282204,
      0.0,
      0.409149578904076,
      0.30274855963209557,
      0.29847527171792887,
      0.42382756154906165,
      0.34498391071493817,
      0.36911852632944675,
      0.46452527098223406,
      0.37506362915328584,
      0.309785927915432
    ],
    [
      0.35412207865513046,
      0.36139392942900606,
      0.37842007366939345,
      0.38831009939033834,
      0.4077260558280027,
      0.3679106174024436,
      0.30744283666183825,
      0.4340969369884986,
      0.37744896940091044,
      0.33080967420342744,
      0.36242819663255754,
      0.20130329557647864,
      0.3624695759371557,
      0.3837081086365344,
      0.3526789884579826,
      0.3976598216524492,
      0.435094179010155,
      0.4059788732788092,
      0.3855067824846887,
      0.43562359210427615,
      0.0,
      0.33713336200498123,
      0.2579339714680535,
      0.3750707327480629,
      0.3707611518575462,
      0.335641779784952,
      0.4060737011244244,
      0.3767972004380924,
      0.2918562678791148
    ],
    [
      0.36622986832995186,
      0.40669904544954627,
      0.4219557914003158,
      0.4179680179041527,
      0.3911457543516468,
      0.4084448016844371,
      0.32761969818145764,
      0.48289282561033153,
      0.41329674475904543,
      0.396454179401462,
      0.4146050709413729,
      0.3260518977816389,
      0.3670783686646282,
      0.4310202833484633,
      0.39461837847129866,
      0.4549916996504466,
      0.48300738430081736,
      0.4387018471937367,
      0.3878334714986411,
      0.380495200269068,
      0.3869586238033469,
      0.0,
      0.3688326270773614,
      0.3983930498202217,
      0.4418676368747625,
      0.4146885698974083,
      0.4081613961757331,
      0.3771319841790455,
      0.39959060509440336
    ],
    [
      0.3102990152450349,
      0.32508070282552004,
      0.34509860374774703,
      0.3523949109074598,
      0.3849351157406602,
      0.3655663326396652,
      0.3422873572171241,
      0.36073153798768787,
      0.3509240017674278,
      0.35120458659548337,
      0.32932483662718304,
      0.3450177271160193,
      0.3345143815776428,
      0.3641302833090556,
      0.38800456056843236,
      0.3637549986481454,
      0.3781422665096972,
      0.3538722860231196,
      0.3722972382493779,
      0.37869878632491205,
      0.3374042545979916,
      0.3515011122463785,
      0.0,
      0.34753683237145716,
      0.44475797925292615,
      0.3874728893790975,
      0.35040401723022363,
      0.3432521888282538,
      0.40185125720070647
    ],
    [
      0.3415416793057353,
      0.42952192546278667,
      0.421180919316247,
      0.5301660727892812,
      0.4500754578381867,
      0.39275875684345474,
      0.3121210092468405,
      0.4691190232607283,
      0.4312154563933188,
      0.3694965343657497,
      0.38919193696594667,
      0.26746825678992203,
      0.4227624487185848,
      0.4103321915038787,
      0.4103378469358743,
      0.48020417107504243,
      0.4817525132821794,
      0.4114250067823282,
      0.3954716517523038,
      0.4340225670948126,
      0.42799098914456435,
      0.39262586451912584,
      0.3042645757045288,
      0.0,
      0.3853875225387948,
      0.3668469315726828,
      0.4456241500387801,
      0.42648755804873195,
      0.3221670126752674
    ],
    [
      0.29515085278647035,
      0.2918950419297628,
      0.361047384876251,
      0.3295726450373737,
      0.40839123884085216,
      0.31248728058695785,
      0.3161330913013114,
      0.3682551976972075,
      0.3130084186277129,
      0.3113240747161663,
      0.2893201602769999,
      0.2789519350942804,
      0.3255801864742789,
      0.31338918308527375,
      0.32415341654618524,
      0.35572191837434963,
      0.37742177057954507,
      0.3339599357066074,
      0.36074926432031806,
      0.33995634550926823,
      0.3833548711485735,
      0.29816536180808617,
      0.32531232325854886,
      0.31868718145756203,
      0.0,
      0.3328048079299364,
      0.3436840102977494,
      0.3149996802749575,
      0.2642207157301484
    ],
    [
      0.3218952004881872,
      0.42429191356854834,
      0.38749169102487335,
      0.37731952931170065,
      0.3465564298218309,
      0.3819897788096749,
      0.26766418927726754,
      0.4443258623736994,
      0.369519932877882,
      0.3229400193080951,
      0.397583910982392,
      0.28727849687434315,
      0.3247666245875327,
      0.41203825583764164,
      0.4197697473973747,
      0.3907126423381788,
      0.49123497343226097,
      0.3624888703480569,
      0.38668582760686165,
      0.38409751164889316,
      0.35847157777797856,
      0.35295515013647005,
      0.34197341596234576,
      0.38654707463805615,
      0.34777639150821504,
      0.0,
      0.3713229725407967,
      0.35429785095862365,
      0.37390743110931624
    ],
    [
      0.3997622623995636,
      0.5031137811258195,
      0.5435526161541142,
      0.515204658290985,
      0.427085309715558,
      0.49391971150616,
      0.3554086324690364,
      0.47499390908324735,
      0.42793434347896353,
      0.3828464120591919,
      0.455923533054051,
      0.28928963398365215,
      0.4301108128046238,
      0.47657101372007116,
      0.3725042994118979,
      0.5018136257338448,
      0.5056329075961059,
      0.4546834673345268,
      0.4894555918406951,
      0.5107428378672887,
      0.42084925594303657,
      0.3632454259548086,
      0.31212638196158315,
      0.45192782379721974,
      0.38314561330828933,
      0.36916985716131334,
      0.0,
      0.4207913291570444,
      0.31385659846014646
    ],
    [
      0.30001865531721905,
      0.29276544952918404,
      0.33430138059750925,
      0.34642500646773766,
      0.3561162078787694,
      0.33134367531740105,
      0.2976766659959611,
      0.3512430522500207,
      0.36579248109433915,
      0.2861975030426944,
      0.3420513787099251,
      0.23081954874643285,
      0.32358069044523097,
      0.36371766123892724,
      0.293383102527224,
      0.3774342727640809,
      0.3832607383444957,
      0.3813815675761125,
      0.3507467663004018,
      0.3428097116218354,
      0.3736183715972161,
      0.298349928273864,
      0.2567201825433243,
      0.3747893187251732,
      0.3292584590825469,
      0.2759291449894996,
      0.3566468002312555,
      0.0,
      0.2528499133051041
    ],
    [
      0.26616393217056333,
      0.30899714219664154,
      0.3157035777324979,
      0.3183453198284114,
      0.30495216153686844,
      0.29296951384034187,
      0.27956217230806946,
      0.3291702819184332,
      0.32131926398011257,
      0.3559506945892803,
      0.3096330878007538,
      0.3314708263950097,
      0.28683596147851453,
      0.31644158112951337,
      0.33683463100872024,
      0.3272752376293693,
      0.32240496980338085,
      0.2972928749843564,
      0.31423832087770864,
      0.31735903495498197,
      0.28569065619958156,
      0.38378851075419695,
      0.33701420185845077,
      0.3465006719831454,
      0.31173540387740495,
      0.3819677340045029,
      0.29438989097952484,
      0.3059535660191295,
      0.0
    ]
  ],
  "row_avgs": [
    0.38915658006062437,
    0.3542442651172915,
    0.3976167834169213,
    0.4467331340728428,
    0.4118790464608332,
    0.3564917670946141,
    0.3470329722133166,
    0.397026161122812,
    0.3644191485187132,
    0.3512025988071049,
    0.42910289351892483,
    0.2565105696665423,
    0.41508571771637237,
    0.3997827904593925,
    0.41217811513622404,
    0.3745086655306088,
    0.41200182631557164,
    0.4200677512729893,
    0.4070218513725929,
    0.4155986990584089,
    0.36362145902518944,
    0.40381195793266933,
    0.35930214502622965,
    0.40434142964163133,
    0.32813208193831206,
    0.3709965454481106,
    0.4302022016204586,
    0.3274724155183387,
    0.31785575792283804
  ],
  "col_avgs": [
    0.36044660008389345,
    0.4098265858877474,
    0.4416413132157514,
    0.4398476226264976,
    0.38361208422325493,
    0.4111083028032553,
    0.3017387032680177,
    0.4359660059962109,
    0.38681799082821106,
    0.3586755551387381,
    0.3743270670413011,
    0.26015986605019187,
    0.38080315904217094,
    0.4282179060282844,
    0.3602217903428798,
    0.44695380921188527,
    0.4291692663729578,
    0.39588728987502003,
    0.41215406915598624,
    0.4322515001532344,
    0.3871407019250495,
    0.3458576508582477,
    0.2917433039334786,
    0.37847505136180926,
    0.35706736241865766,
    0.35820832955764575,
    0.40243162047783304,
    0.3727904197649664,
    0.3198564033633016
  ],
  "combined_avgs": [
    0.3748015900722589,
    0.38203542550251945,
    0.41962904831633635,
    0.4432903783496702,
    0.3977455653420441,
    0.38380003494893467,
    0.32438583774066715,
    0.41649608355951145,
    0.37561856967346213,
    0.3549390769729215,
    0.401714980280113,
    0.2583352178583671,
    0.3979444383792716,
    0.41400034824383847,
    0.38619995273955193,
    0.41073123737124706,
    0.4205855463442647,
    0.40797752057400466,
    0.40958796026428956,
    0.42392509960582164,
    0.3753810804751195,
    0.3748348043954585,
    0.3255227244798541,
    0.3914082405017203,
    0.3425997221784849,
    0.3646024375028782,
    0.4163169110491458,
    0.35013141764165256,
    0.3188560806430698
  ],
  "gppm": [
    589.2294350521918,
    574.102359106793,
    559.6788021182525,
    558.0276960330654,
    588.1942185377095,
    576.3859191339623,
    626.2571315707711,
    558.4256257518051,
    590.2099241643265,
    595.5633406556761,
    595.939310827627,
    640.4038938056457,
    593.8923982925869,
    569.629423414504,
    599.3196679227251,
    558.9350757920918,
    565.1159960059498,
    585.2819216256332,
    572.423472765787,
    563.4584805537789,
    583.7309023356653,
    607.8408010703185,
    628.0208916894454,
    589.4985889741112,
    598.417740994688,
    596.8835162753998,
    578.5868106609091,
    594.727619117127,
    614.9493455342744
  ],
  "gppm_normalized": [
    1.424989872227142,
    1.3610702974987414,
    1.3282183224567763,
    1.3263629943001927,
    1.4007643471158986,
    1.372088911417924,
    1.48728866572749,
    1.3291023028168176,
    1.3986437132750464,
    1.4150366304241173,
    1.4141350749192259,
    1.5322763845866312,
    1.4123640439371052,
    1.351756316238904,
    1.4189735864749586,
    1.3224925089897552,
    1.3376969073684808,
    1.3830725989429495,
    1.3535445386227207,
    1.3423464818240962,
    1.3741993270289126,
    1.4404395166495683,
    1.483739583688166,
    1.4032938865500968,
    1.4208675630700078,
    1.4180861873428199,
    1.366682632217453,
    1.4095216284663918,
    1.454880620121055
  ],
  "token_counts": [
    675,
    403,
    416,
    436,
    480,
    456,
    430,
    452,
    399,
    429,
    422,
    532,
    453,
    418,
    384,
    385,
    398,
    367,
    383,
    466,
    347,
    394,
    358,
    460,
    422,
    432,
    368,
    403,
    379,
    618,
    437,
    415,
    504,
    453,
    416,
    559,
    458,
    426,
    372,
    401,
    406,
    460,
    425,
    437,
    433,
    470,
    386,
    427,
    439,
    425,
    394,
    391,
    430,
    407,
    408,
    381,
    409,
    381,
    349,
    482,
    471,
    449,
    403,
    484,
    392,
    457,
    439,
    402,
    375,
    517,
    402,
    444,
    419,
    471,
    409,
    399,
    451,
    475,
    436,
    386,
    384,
    402,
    427,
    409,
    423,
    468,
    396
  ],
  "response_lengths": [
    1682,
    2633,
    2665,
    2541,
    2296,
    2737,
    2314,
    2551,
    2322,
    2255,
    2147,
    2859,
    2257,
    2492,
    2263,
    2638,
    2274,
    2187,
    2595,
    2607,
    2403,
    2224,
    2313,
    2296,
    2461,
    2209,
    2345,
    2594,
    2290
  ]
}