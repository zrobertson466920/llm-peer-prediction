{
  "example_idx": 64,
  "reference": "Published as a conference paper at ICLR 2023\n\nDISTRIBUTIONALLY ROBUST RECOURSE ACTION\n\nDuy Nguyen1, Ngoc Bui1, Viet Anh Nguyen2 1VinAI Research, Vietnam 2The Chinese University of Hong Kong\n\nABSTRACT\n\nA recourse action aims to explain a particular algorithmic decision by showing one specific way in which the instance could be modified to receive an alternate outcome. Existing recourse generation methods often assume that the machine learning model does not change over time. However, this assumption does not always hold in practice because of data distribution shifts, and in this case, the recourse action may become invalid. To redress this shortcoming, we propose the Distributionally Robust Recourse Action (DiRRAc) framework, which generates a recourse action that has a high probability of being valid under a mixture of model shifts. We formulate the robustified recourse setup as a min-max optimization problem, where the max problem is specified by Gelbrich distance over an ambiguity set around the distribution of model parameters. Then we suggest a projected gradient descent algorithm to find a robust recourse according to the min-max objective. We show that our DiRRAc framework can be extended to hedge against the misspecification of the mixture weights. Numerical experiments with both synthetic and three real-world datasets demonstrate the benefits of our proposed framework over state-of-the-art recourse methods.\n\n1\n\nINTRODUCTION\n\nPost-hoc explanations of machine learning models are useful for understanding and making reliable predictions in consequential domains such as loan approvals, college admission, and healthcare. Recently, recourse has been rising as an attractive tool to diagnose why machine learning models have made a particular decision for a given instance. A recourse action provides a possible modification of the given instance to receive an alternate decision (Ustun et al., 2019). Consider, for example, the case of loan approvals in which a credit application is rejected. Recourse will offer the reasons for rejection by showing what the application package should have been to get approved. A concrete example of a recourse in this case may be “the monthly salary should be higher by $500” or “20% of the current debt should be reduced”.\n\nA recourse action has a positive, forward-looking meaning: they list out a directive modification that a person should implement so that they can get a more favorable outcome in the future. If a machine learning system can provide the negative outcomes with the corresponding recourse action, it can improve user engagement and boost the interpretability at the same time (Ustun et al., 2019; Karimi et al., 2021b). Explanations thus play a central role in the future development of human-computer interaction as well as human-centric machine learning.\n\nDespite its attractiveness, providing recourse for the negative instances is not a trivial task. For realworld implementation, designing a recourse needs to strike an intricate balance between conflicting criteria. First and foremost, a recourse action should be feasible: if the prescribed action is taken, then the prediction of a machine learning model should be flipped. Further, to avoid making a drastic change to the characteristics of the input instance, a framework for generating recourse should minimize the cost of implementing the recourse action. An algorithm for finding recourse must make changes to only features that are actionable and should leave immutable features (relatively) unchanged. For example, we must consider the date of birth as an immutable feature; in contrast, we can consider salary or debt amount as actionable features.\n\nVarious solutions have been proposed to provide recourses for a model prediction (Karimi et al., 2021b; Stepin et al., 2021; Artelt & Hammer, 2019; Pawelczyk et al., 2021; 2020; Verma et al.,\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n2020). For instance, Ustun et al. (2019) used an integer programming approach to obtain actionable recourses and also provide a feasibility guarantee for linear models. Karimi et al. (2020) proposed a model-agnostic approach to generate the nearest counterfactual explanations and focus on structured data. Dandl et al. (2020) proposed a method that finds the counterfactual by solving a multi-objective optimization problem. Recently, Russell (2019) and Mothilal et al. (2020) focus on finding a set of multiple diverse recourse actions, where the diversity is imposed by a rule-based approach or by internalizing a determinant point process cost in the objective function.\n\nThese aforementioned approaches make a fundamental assumption that the machine learning model does not change over time. However, the dire reality suggests that this assumption rarely holds. In fact, data shifts are so common nowadays in machine learning that they have sparkled the emerging field of domain generalization and domain adaptation. Organizations usually retrain models as a response to data shifts, and this induces corresponding shifts in the machine learning models parameters, which in turn cause serious concerns for the feasibility of the recourse action in the future (Rawal et al., 2021). In fact, all of the aforementioned approaches design the action which is feasible only with the current model parameters, and they provide no feasibility guarantee for the future parameters. If a recourse action fails to generate a favorable outcome in the future, then the recourse action may become less beneficial (Venkatasubramanian & Alfano, 2020), the pledge of a brighter outcome is shattered, and the trust in the machine learning system is lost (Rudin, 2019).\n\nTo tackle this challenge, Upadhyay et al. (2021) proposed ROAR, a framework for generating instance-level recourses that are robust to shifts in the underlying predictive model. ROAR used a robust optimization approach that hedges against an uncertainty set containing plausible values of the future model parameters. However, it is well-known that robust optimization solutions can be overly conservative because they may hedge against a pathological parameter in the uncertainty set (Ben-Tal et al., 2017; Roos & den Hertog, 2020). A promising approach that can promote robustness while at the same time prevent from over-conservatism is the distributionally robust optimization framework (El Ghaoui et al., 2003; Delage & Ye, 2010; Rahimian & Mehrotra, 2019; Bertsimas et al., 2018). This framework models the future model parameters as random variables whose underlying distribution is unknown but is likely to be contained in an ambiguity set. The solution is designed to counter the worst-case distribution in the ambiguity set in a min-max sense. Distributionally robust optimization is also gaining popularity in many estimation and prediction tasks in machine learning (Namkoong & Duchi, 2017; Kuhn et al., 2019).\n\nContributions. This paper combines ideas and techniques from two principal branches of explainable artificial intelligence: counterfactual explanations and robustness to resolve the recourse problem under uncertainty. Concretely, our main contributions are the following:\n\n1. We propose the framework of Distributionally Robust Recourse Action (DiRRAc) for designing a recourse action that is robust to mixture shifts of the model parameters. Our DiRRAc maximizes the probability that the action is feasible with respect to a mixture shift of model parameters while at the same time confines the action in the neighborhood of the input instance. Moreover, the DiRRAc model also hedges against the misspecification of the nominal distribution using a min-max form with a mixture ambiguity set prescribed by moment information.\n\n2. We reformulate the DiRRAc problem into a finite-dimensional optimization problem with an explicit objective function. We also provide a projected gradient descent to solve the problem.\n\n3. We extend our DiRRAc framework along several axis to handle mixture weight uncertainty, to minimize the worst-case component probability of receiving the unfavorable outcome, and also to incorporate the Gaussian parametric information.\n\nWe first describe the recourse action problem with mixture shifts in Section 2. In Section 3, we present our proposed DiRRAc framework, its reformulation and the numerical routine for solving it. The extension to the parametric Gaussian setting will be discussed in Section 4. Section 5 reports the numerical experiments showing the benefits of the DiRRAc framework and its extensions. Notations. For each integer K, we have [K] = {1, . . . , K}. We use Sd ++) to denote the space of symmetric positive semidefinite (definite) matrices. For any A ∈ Rm×m, the trace operator is Tr (cid:2)A(cid:3) = (cid:80)d i=1 Aii. If a distribution Qk has mean μk and covariance matrix Σk, we write Qk ∼ (μk, Σk). If additionally Qk is Gaussian, we write Qk ∼ N (μk, Σk). Writing Q ∼ (Qk, pk)k∈[K] means Q is a mixture of K components, the k-th component has weight pk and distribution Qk.\n\n+ (Sd\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 RECOURSE ACTION UNDER MIXTURE SHIFTS\n\nWe consider a binary classification setting with label Y = {0, 1}, where 0 represents the unfavorable outcome while 1 denotes the favorable one. The covariate space is Rd, and any linear classifier Cθ : Rd → Y characterized by the d-dimensional parameter θ is of the form\n\nCθ(x) =\n\n(cid:26)1 0\n\nif θ(cid:62)x ≥ 0, otherwise.\n\nNote that the bias term can be internalized into θ by adding an extra dimension, and thus it is omitted.\n\nSuppose that at this moment (t = 0), the current classifier is parametrized by θ0, and we are given an input instance x0 ∈ Rd with unfavorable outcome, that is, Cθ0(x0) = 0. One period of time from now (t = 1), the parameters of the predictive model will change stochastically and are represented by a d-dimensional random vector ̃θ. This paper focuses on finding a recourse action x which is reasonably close to the instance x0, and at the same time, has a high probability of receiving a favorable outcome in the future. Figure 1 gives a bird’s eye view of the setup.\n\nFigure 1: A canonical setup of the recourse action under mixture shifts problem.\n\nTo measure the closeness between the action x and the input x0, we assume that the covariate space is endowed with a non-negative, continuous cost function c. In addition, suppose temporarily that ̃θ follows a distribution (cid:98)P. Because maximizing the probability of the favorable outcome is equivalent to minimizing the probability of the unfavorable outcome, the recourse can be found by solving\n\nmin\n\n(cid:110)\n\n(cid:98)P(C ̃θ(x) = 0) : x ∈ X, c(x, x0) ≤ δ\n\n(cid:111)\n\n.\n\n(1)\n\nThe parameter δ ≥ 0 in (1) governs how far a recourse action can be from the input instance x0. Note that we constrain x in a set X which captures operational constraints, for example, the highest education of a credit applicant should not be decreasing over time.\n\nIn this paper, we model the random vector ̃θ using a finite mixture of distributions with K components, the mixture weights are (cid:98)p satisfying (cid:80) k∈[K] (cid:98)pk = 1. Each component in the mixture represents one specific type of model shifts: the weights (cid:98)p reflect the proportion of the shift types while the component distribution (cid:98)Pk represents the (conditional) distribution of the future model parameters in the k-th shift. Further information on mixture distributions and their applications in machine learning can be found in (Murphy, 2012, §3.5). Note that the mixture model is not a strong assumption. It is well-known that the Gaussian mixture model is a universal approximator of densities, in the sense that any smooth density can be approximated with any specific nonzero amount of error by a Gaussian mixture model with enough components (Goodfellow et al., 2016; McLachlan & Peel, 2000). Thus, our mixture models are flexible enough to hedge against distributional perturbations of the parameters under large values of K. The design of the ambiguity set to handle ambiguous mixture weights and under the Gaussian assumption is extensively studied in the literature on distributionally robust optimization (Hanasusanto et al., 2015; Chen & Xie, 2021).\n\nIf each (cid:98)Pk is a Gaussian distribution N ((cid:98)θk, (cid:98)Σk), then (cid:98)P is a mixture of Gaussian distributions. The objective of problem (1) can be expressed as\n\n(cid:98)P(C ̃θ(x) = 0) =\n\n(cid:88)\n\nk∈[K]\n\n(cid:98)pk(cid:98)Pk(C ̃θ(x) = 0) =\n\n(cid:88)\n\nk∈[K]\n\n(cid:98)pkΦ\n\n(cid:16) −x(cid:62) (cid:98)θk\n\n(cid:113)\n\nx(cid:62) (cid:98)Σkx\n\n(cid:17)\n\n,\n\nwhere the first equality follows from the law of conditional probability, and Φ is the cumulative distribution function of a standard Gaussian distribution. Under the Gaussian assumption, we can solve (1) using a projected gradient descent type of algorithm (Boyd & Vandenberghe, 2004).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nRemark 2.1 (Nonlinear models). Our analysis focuses on linear classifiers, which is a common setup in the literature (Upadhyay et al., 2021; Ustun et al., 2019; Rawal et al., 2021; Karimi et al., 2020; Wachter et al., 2018; Ribeiro et al., 2016). To extend to nonlinear classifiers, we can follow a similar approach as in Rawal & Lakkaraju (2020b) and Upadhyay et al. (2021) by first using LIME Ribeiro et al. (2016) to approximate the nonlinear classifiers locally with an interpretable linear model, then subsequently applying our framework.\n\n3 DISTRIBUTIONALLY ROBUST RECOURSE ACTION FRAMEWORK\n\nOur Distributionally Robust Recourse Action (DiRRAc) framework robustifies formulation (1) by relaxing the parametric assumption and hedging against distribution misspecification. First, we assume that the mixture components (cid:98)Pk are specified only through moment information, and no particular parametric form of the distribution is imposed. In effect, (cid:98)Pk is assumed to have mean vector (cid:98)θk ∈ Rd and positive definite covariance matrix (cid:98)Σk (cid:31) 0. Second, we leverage ideas from distributionally robust optimization to propose a min-max formulation of (1), in which we consider an ambiguity set which contains a family of probability distributions that are sufficiently close to the nominal distribution (cid:98)P. We prescribe the ambiguity set using Gelbrich distance (Gelbrich, 1990). Definition 3.1 (Gelbrich distance). The Gelbrich distance G between two tuples (θ, Σ) ∈ Rd × Sd 2 (cid:3). and ((cid:98)θ, (cid:98)Σ) ∈ Rd × Sd\n\n+ amounts to G((θ, Σ), ((cid:98)θ, (cid:98)Σ)) (cid:44)\n\n2 + Tr (cid:2)Σ + (cid:98)Σ − 2((cid:98)Σ 1\n\n(cid:107)θ − (cid:98)θ(cid:107)2\n\n2 Σ(cid:98)Σ 1\n\n2 ) 1\n\n(cid:113)\n\n+\n\nIt is easy to verify that G is non-negative, symmetric and it vanishes to zero if and only if (θ, Σ) = ((cid:98)θ, (cid:98)Σ). Further, G is a distance on Rd × Sd + because it coincides with the type-2 Wasserstein distance between two Gaussian distributions N (μ, Σ) and N ((cid:98)μ, (cid:98)Σ) (Givens & Shortt, 1984). Distributionally robust formulations with moment information prescribed by the G distance are computationally tractable under mild conditions, deliver reasonable performance guarantees and also generate a conservative approximation of the Wasserstein distributionally robust optimization problem (Kuhn et al., 2019; Nguyen et al., 2021).\n\nIn this paper, we use the Gelbrich distance G to form a neighborhood around each (cid:98)Pk as\n\nBk((cid:98)Pk) (cid:44)\n\n(cid:110)\n\nQk : Qk ∼ (θk, Σk), G((θk, Σk), ((cid:98)θk, (cid:98)Σk)) ≤ ρk\n\n(cid:111)\n\n.\n\nIntuitively, one can view Bk((cid:98)Pk) as a ball centered at the nominal component (cid:98)Pk of radius ρk ≥ 0 prescribed using the distance G. This component set Bk((cid:98)Pk) is non-parametric, and the first two moments of Qk are sufficient to decide whether Qk belongs to Bk((cid:98)Pk). Moreover, if Qk ∈ Bk((cid:98)Pk), then any distribution Q(cid:48) k with the same mean vector and covariance matrix as Qk also belongs to Bk((cid:98)Pk). Notice that even when the radius ρk is zero, the component set Bk((cid:98)Pk) does not collapse into a singleton. Instead, if ρk = 0 then Bk((cid:98)Pk) still contains all distributions of the same moment ((cid:98)θk, (cid:98)Σk) with the nominal component distribution (cid:98)Pk, and consequentially it possesses the robustification effects against the parametric assumption on (cid:98)Pk. The component sets are utilized to construct the ambiguity set for the mixture distribution as\n\nB((cid:98)P) (cid:44)\n\n(cid:110)\n\nQ : ∃Qk ∈ Bk((cid:98)Pk) ∀k ∈ [K] such that Q ∼ (Qk, (cid:98)pk)k∈[K]\n\n(cid:111)\n\n.\n\nAny Q ∈ B((cid:98)P) is also a mixture distribution with K components, with the same mixture weights (cid:98)p. Thus, B((cid:98)P) contains all perturbations of (cid:98)P induced separately on each component by Bk((cid:98)Pk).\n\nWe are now ready to introduce our DiRRAc model, which is a min-max problem of the form\n\nQ(C ̃θ(x) = 0)\n\ninf x∈X\n\ns. t.\n\nsup Q∈B((cid:98)P) c(x, x0) ≤ δ\n\nsup Qk∈Bk((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) < 1\n\n∀k ∈ [K].\n\n(2)\n\nThe objective of (2) is to minimize the worst-case probability of unfavorable outcome of the recourse action. Moreover, the last constraint imposes that for each component, the worst-case conditional\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nprobability of unfavorable outcome should be strictly less than one. Put differently, this last constraint requires that the action should be able to lead to favorable outcome for any distribution in Bk((cid:98)Pk). By definition, each supremum subproblem in (2) is an infinite-dimensional maximization problem over the space of probability distributions, and thus it is inherently difficult. Fortunately, because we use the Gelbrich distance to prescribe the set Bk((cid:98)Pk), we can solve these maximization problems analytically. This consequentially leads to a closed-form reformulation of the DiRRAc model into a finite-dimensional problem. Next, we will reformulate the DiRRAc problem (2), provide a sketch of the proof and propose a numerical solution routine.\n\n3.1 REFORMULATION OF DIRRAC\n\nEach supremum in (2) is an infinite-dimensional optimization problem on the space of probability distributions. We now show that (2) can be reformulated as a finite-dimensional problem. Towards this end, let X be the following d-dimensional set.\n\nX (cid:44) (cid:8)x ∈ X : c(x, x0) ≤ δ, −(cid:98)θ(cid:62)\n\nk x + ρk(cid:107)x(cid:107)2 < 0 ∀k ∈ [K] (cid:9) .\n\n(3)\n\nThe next theorem asserts that the DiRRAc problem (2) can be reformulated as a d-dimensional optimization problem with an explicit, but complicated, objective function. Theorem 3.2 (Equivalent form of DiRRAc). Problem (2) is equivalent to the finite-dimensional optimization problem\n\ninf x∈X\n\n(cid:88)\n\nk∈[K]\n\n(cid:98)pkfk(x)2,\n\n(4)\n\nwhere the function fk admits the closed-form expression\n\nfk(x) =\n\nρk (cid:98)θ(cid:62)\n\nk x(cid:107)x(cid:107)2 +\n\n(cid:113)\n\nx(cid:62) (cid:98)Σkx\n\n(cid:113)\n\n((cid:98)θ(cid:62)\n\nk x)2 + x(cid:62) (cid:98)Σkx − ρ2\n\nk(cid:107)x(cid:107)2\n\n2\n\n((cid:98)θ(cid:62)\n\nk x)2 + x(cid:62) (cid:98)Σkx\n\n.\n\nNext, we sketch a proof of Theorem 3.2 and a solution procedure to solve problem (4).\n\n3.2 PROOF SKETCH\n\nFor any component k ∈ [K], define the following worst-case probability of unfavorable outcome\n\nfk(x) (cid:44) sup\n\nQk∈Bk((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) = sup\n\nQk∈Bk((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ 0)\n\n∀k ∈ [K].\n\n(5)\n\nTo proceed, we rely on the following elementary result from (Nguyen, 2019, Lemma 3.31). Lemma 3.3 (Worst-case Value-at-Risk). For any x ∈ Rd and β ∈ (0, 1), we have\n\n(cid:40)\n\ninf\n\nτ :\n\nsup Qk∈Bk((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ −τ ) ≤ β\n\n= −(cid:98)θ(cid:62)\n\nk x +\n\n(cid:41)\n\n(cid:115)\n\n1 − β β\n\n(cid:113)\n\nx(cid:62) (cid:98)Σkx +\n\nρk√ β\n\n(cid:107)x(cid:107)2.\n\n(6)\n\nNote that the left-hand side of (6) is the worst-case Value-at-Risk with respect to the ambiguity set Bk((cid:98)Pk). Leveraging this result, the next proposition provides the analytical form of fk(x). Proposition 3.4 (Worst-case probability). For any k ∈ [K] and ((cid:98)θk, (cid:98)Σk, ρk) ∈ Rd × Sd define the following constants Ak (cid:44) −(cid:98)θ(cid:62) k x, Bk (cid:44) x(cid:62) (cid:98)Σkx, and Ck (cid:44) ρk(cid:107)x(cid:107)2. We have (cid:40)1\n\n+ × R+,\n\nif Ak + Ck ≥ 0,\n\n(cid:113)\n\nfk(x) (cid:44) sup\n\nQk( ̃θ(cid:62)x ≤ 0) =\n\nQk∈Bk((cid:98)Pk)\n\n√\n\n(cid:16) −AkCk+Bk A2\n\nA2 k+B2\n\nk\n\nk+B2\n\nk−C2\n\nk\n\n(cid:17)2\n\n∈ (0, 1)\n\nif Ak + Ck < 0.\n\nThe proof of Theorem 3.2 follows by noticing that the DiRRAc problem (2) can be reformulated using the elementary functions fk as\n\n \n\n\n\n(cid:88)\n\nk∈[K]\n\nmin x∈X\n\n(cid:98)pkfk(x) : c(x, x0) ≤ δ, fk(x) < 1 ∀k ∈ [K]\n\n \n\n\n\n,\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwhere the objective function follows from the definition of the set B((cid:98)P). It suffices now to combine with Proposition 3.4 to obtain the necessary result. The detailed proof is relegated to the Appendix. Next we propose a projected gradient descent algorithm to solve the problem (4).\n\n3.3 PROJECTED GRADIENT DESCENT ALGORITHM\n\nWe consider in this section an iterative numerical routine to solve the DiRRAc problem in the equivalent form (4). First, notice that the second constraint that defines X in (3) is a strict inequality, thus the set X is open. We thus modify slightly this constraint by considering the following set\n\n(cid:110)\n\nXε =\n\nx ∈ X : c(x, x0) ≤ δ, −(cid:98)θ(cid:62)\n\nk x + ρk(cid:107)x(cid:107)2 ≤ −ε ∀k ∈ [K]\n\n(cid:111)\n\nfor some value ε > 0 sufficiently small. Moreover, if the parameter δ is too small, it may happen that the set Xε becomes empty. Define δmin ∈ R+ as the optimal value of the following problem\n\n(cid:110)\n\ninf\n\nc(x, x0) : x ∈ X, − (cid:98)θ(cid:62)\n\nk x + ρk(cid:107)x(cid:107)2 ≤ −ε ∀k ∈ [K]\n\n(cid:111)\n\n.\n\n(7)\n\nThen it is easy to see that Xε is non-empty whenever δ ≥ δmin. In addition, because c is continuous and X is closed, the set Xε is compact. In this case, we can consider problem (4) with the feasible set being Xε, for which the optimal solution is guaranteed to exist. Let us now define the projection (cid:9). If X is convex and c(·, x0) is a operator ProjXε as ProjXε(x(cid:48)) (cid:44) arg min (cid:8)(cid:107)x − x(cid:48)(cid:107)2 convex function, then Xε is also convex, and the projection operation can be efficiently computed using convex optimization. In particular, suppose that c(x, x0) = (cid:107)x − x0(cid:107)2 is the Euclidean norm and X is second-order cone representable, then the projection is equivalent to a second-order cone program, and can be solved using off-the-shelf solvers such as GUROBI Gurobi Optimization, LLC (2021) or Mosek (MOSEK ApS, 2019). The projection operator ProjXε now forms the building block of a projected gradient descent algorithm with a backtracking linesearch. The details regarding the algorithm, along with the convergence guarantee, are presented in Appendix E.\n\n2 : x ∈ Xε\n\nTo conclude this section, we visualize the geometrical intuition of our method in Figure 2.\n\nFigure 2: The feasible set X in (3) is shaded in blue. The circular arc represents the proximity boundary c(x, x0) = δ with c being an Euclidean distance. Dashed lines represent the hyperplane −(cid:98)θ(cid:62) k x = 0 for different k, while elliptic curves represent the robust margin −(cid:98)θ(cid:62) k x + ρk(cid:107)x(cid:107) = 0 with matching color. Increasing the ambiguity size ρk brings the elliptic curves towards the top-right corner and farther away from the dash lines. The set X taken as the intersection of elliptical and promixity constraints will move deeper into the interior of the favorable prediction region, resulting in more robust recourses.\n\n4 GAUSSIAN DIRRAC FRAMEWORK\n\nWe here revisit the Gaussian assumption on the component distributions, and propose the parametric Gaussian DiRRAc framework. We make the temporary assumption that (cid:98)Pk are Gaussian for all k ∈ [K], and we will robustify against only the misspecification of the nominal mean vector and covariance matrix ((cid:98)θk, (cid:98)Σk). To do this, we first construct the Gaussian component ambiguity sets\n\n∀k : BN\n\nk ((cid:98)Pk) (cid:44) (cid:8)Qk : Qk ∼ N (θk, Σk), G((θk, Σk), ((cid:98)θk, (cid:98)Σk)) ≤ ρk\n\n(cid:9) ,\n\nwhere the superscript emphasizes that the ambiguity sets are neighborhoods in the space of Gaussian distributions. The resulting ambiguity set for the mixture distribution is\n\nBN ((cid:98)P) =\n\n(cid:110)\n\nQ : ∃Qk ∈ BN\n\nk ((cid:98)Pk) ∀k ∈ [K] such that Q ∼ (Qk, (cid:98)pk)k∈[K]\n\n(cid:111)\n\n.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nThe Gaussian DiRRAc problem is formally defined as Q(C ̃θ(x) = 0)\n\nmin x∈X\n\ns. t.\n\nsup Q∈BN ((cid:98)P) c(x, x0) ≤ δ sup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) < 1\n\n2\n\n∀k ∈ [K].\n\n(8)\n\nSimilar to Section 3, we will provide the reformulation of the Gaussian DiRRAc formulation and a sketch of the proof in the sequence. Note that the last constraint in (8) has margin 1 2 instead of 1 as in the DiRRAc problem (2). The detailed reason will be revealed in the proof sketch in Section 4.2.\n\n4.1 REFORMULATION OF GAUSSIAN DIRRAC\n\nRemind that the feasible set X is defined as in (3). The next theorem asserts the equivalent form of the Gaussian DiRRAc problem (8).\n\nTheorem 4.1 (Gaussian DiRRAc reformulation). The Gaussian DiRRAc problem (8) is equivalent to the finite-dimensional optimization problem\n\nmin x∈X\n\n1 −\n\n(cid:88)\n\nk∈[K]\n\n(cid:98)pkΦ(gk(x)),\n\n(9)\n\nwhere the function gk admits the closed-form expression\n\ngk(x) =\n\n(cid:113)\n\n(cid:98)θ(cid:62) k x\n\n((cid:98)θ(cid:62)\n\nk x)2 − ρ2 (cid:113)\n\nk(cid:107)x(cid:107)2\n\n2\n\nx(cid:62) (cid:98)Σkx + ρk(cid:107)x(cid:107)2\n\n((cid:98)θ(cid:62)\n\nk x)2 + x(cid:62) (cid:98)Σkx − ρ2\n\nk(cid:107)x(cid:107)2\n\n2\n\n.\n\nProblem (9) can be solved using the projected gradient descent algorithm discussed in Section 3.3.\n\n4.2 PROOF SKETCH\n\nThe proof of Theorem 4.1 relies on the following analytical form of the worst-case Value-at-Risk (VaR) under parametric Gaussian ambiguity set (Nguyen, 2019, Lemma 3.31). Lemma 4.2 (Worst-case Gaussian VaR). For any x ∈ Rd and β ∈ (0, 1\n\n2 ], let t = Φ−1(1 − β). Then\n\n(cid:40)\n\ninf\n\nτ :\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ −τ ) ≤ β\n\n(cid:41)\n\n(cid:113)\n\n= −(cid:98)θ(cid:62)\n\nk x + t\n\nx(cid:62) (cid:98)Σkx + ρ\n\n(cid:112)\n\n1 + t2(cid:107)x(cid:107)2.\n\n(10)\n\nIt is important to note that Lemma 4.2 is only valid for β ∈ (0, 0.5]. Indeed, for β > 1 2 , evaluating the infimum problem in the left-hand side of (10) requires solving a non-convex optimization problem as t = Φ−1(1 − β) < 0. As a consequence, the last constraint of the Gaussian DiRRAc formulation (8) is capped at a probability value of 0.5 to ensure the convexity of the feasible set in the reformulation (9). The proof of Theorem 4.1 follows a similar line of argument as for the DiRRAc formulation, with gk being the worst-case Gaussian probability\n\ngk(x) (cid:44)\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) =\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ 0)\n\n∀k ∈ [K].\n\nTo conclude this section, we provide a quick sanity check: by setting K = 1 and ρ1 = 0, we have a special case in which ̃θ follows a Gaussian distribution N ((cid:98)μ1, (cid:98)Σ1). Thus, ̃θ(cid:62)x ∼ N ((cid:98)μ(cid:62) 1 x, x(cid:62) (cid:98)Σ1x) and it is easy to verify from the formula of g1 in the statement of Theorem 4.1 that g1(x) = ((cid:98)θ(cid:62)\n\n2 , which recovers the value of Pr( ̃θ(cid:62)x ≤ 0) under the Gaussian distribution.\n\n1 x)/(x(cid:62) (cid:98)Σ1x) 1\n\n5 NUMERICAL EXPERIMENTS\n\nWe compare extensively the performance of our DiRRAc model (2) and Gaussian DiRRAc model (8) against four strong baselines: ROAR (Upadhyay et al., 2021), CEPM (Pawelczyk et al.,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n2020), AR (Ustun et al., 2019) and Wachter (Wachter et al., 2018). We conduct the experiments on three real-world datasets (German, SBA, Student). Appendix A provides further comparisons with more baselines: Nguyen et al. (2022), Karimi et al. (2021a) and ensemble variants of ROAR, along with the sensitivity analysis of hyperparameters. Appendix A also contains the details about the datasets and the experimental setup.\n\nMetrics. For all experiments, we use the l1 distance c(x, x0) = (cid:107)x−x0(cid:107)1 as the cost function. Each dataset contains two sets of data (the present and shifted data). The present data is to train the current classifier for which recourses are generated while the remaining data is used to measure the validity of the generated recourses under model shifts. We choose 20% of the shifted data randomly 100 times and train 100 classifiers respectively. The validity of a recourse is computed as the fraction of the classifiers for which the recourse is valid. We then report the average of the validity of all generated recourses and refer this value as M2 validity. We also report M1 validity, which is the fraction of the instances for which the recourse is valid with respect to the original classifier.\n\nResults on real-world data. We use three real-world datasets which capture different data distribution shifts (Dua & Graff, 2017): (i) the German credit dataset, which captures a correction shift. (ii) the Small Business Administration (SBA) dataset, which captures a temporal shift. (iii) the Student performance dataset, which captures a geospatial shift. Each dataset contains original data and shifted data. We normalize all continuous features to [0, 1]. Similar to Mothilal et al. (2020), we use one-hot encodings for categorial features, then consider them as continuous features in [0, 1]. To ease the comparison, we choose K = 1. The choices of K are discussed further in Appendix A.\n\nFigure 3: Comparison of M2 validity as a function of the l1 distance between input instance and the recourse for our DiRRAc method and ROAR on real datasets.\n\nTable 1: Benchmark of M1 and M2 validity, l1 and l2 cost for linear models on real datasets.\n\nDataset Methods\n\nM1 validity M2 validity\n\nl1 cost\n\nl2 cost\n\nGerman AR\n\nSBA\n\nStudent\n\nWachter CEPM ROAR DiRRAc Gaussian DiRRAc\n\nAR Wachter CEPM ROAR DiRRAc Gaussian DiRRAc\n\nAR Wachter CEPM ROAR DiRRAc Gaussian DiRRAc\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n0.76 ± 0.26 0.82 ± 0.24 0.83 ± 0.38 0.94 ± 0.15 0.99 ± 0.06 0.99 ± 0.06\n\n0.41 ± 0.18 0.55 ± 0.22 0.94 ± 0.24 1.00 ± 0.00 1.00 ± 0.00 0.99 ± 0.02\n\n0.48 ± 0.19 0.53 ± 0.19 0.91 ± 0.15 0.94 ± 0.10 0.95 ± 0.09 0.74 ± 0.18\n\n0.61 ± 0.40 0.81 ± 0.51 1.30 ± 0.02 3.88 ± 0.54 1.62 ± 0.30 1.62 ± 0.30\n\n0.61 ± 0.42 2.30 ± 2.39 5.30 ± 0.01 3.10 ± 0.72 1.74 ± 0.44 1.60 ± 0.62\n\n0.29 ± 0.21 0.60 ± 0.43 4.52 ± 0.01 2.02 ± 0.38 1.55 ± 0.34 0.78 ± 0.30\n\n0.43 ± 0.25 0.41 ± 0.25 1.02 ± 0.04 1.61 ± 0.22 1.25 ± 0.21 1.05 ± 0.23\n\n0.56 ± 0.36 0.77 ± 0.66 2.18 ± 0.02 1.35 ± 0.30 1.34 ± 0.40 0.98 ± 0.42\n\n0.26 ± 0.18 0.30 ± 0.22 2.03 ± 0.01 0.96 ± 0.18 1.07 ± 0.23 0.54 ± 0.21\n\nWe split 80% of the original dataset and train a logistic classifier. This process is repeated 100 times independently to obtain 100 observations of the model parameters. Then we compute the empirical mean and covariance matrix for ((cid:98)θ1, (cid:98)Σ1). To evaluate the trade-off between l1 cost and M2 validity of DiRRAc and ROAR, we compute l1 cost and the M2 validity by running DiRRAc with varying\n\n8\n\n1234l1cost0.70.80.91.0M2validityGerman1.01.52.02.53.0l1cost0.70.80.91.0SBA1.01.52.0l1cost0.70.80.91.0StudentDiRRAcROARPublished as a conference paper at ICLR 2023\n\nvalues of δadd and ROAR with varying values of λ. We define δ = δmin + δadd, δmin is specified in (7). Figure 3 shows that the frontiers of DiRRAc dominate the frontiers of ROAR. This indicates that DiRRAc achieves a far smaller l1 cost for the robust recourses than ROAR. Next, we evaluate the l1 and l2 cost, M1 and M2 validity of DiRRAc, ROAR and other baselines. The results in Table 1 demonstrate that DiRRAc has high validity in all three datasets while preserving low costs (l1 and l2 cost) in comparison to ROAR. Our DiRRAc framework consistently outperforms the AR, Wachter, and CEPM in terms of M2 validity.\n\nNonlinear models. Following the previous work as in Rawal et al. (2021); Upadhyay et al. (2021) and Bui et al. (2022), we adapt our DiRRAc framework and other baselines (AR and ROAR) to non-linear models by first generating local linear approximations using LIME (Ribeiro et al., 2016). For each instance x0, we first generate a local linear model for the MLPs classifier 10 times using LIME, each time using 1000 perturbed samples. To estimate ((cid:98)θ1, (cid:98)Σ1), we compute the mean and covariance matrix of parameters θx0 of 10 local linear models. We randomly choose 10% of the shifted dataset and concatenate with training data of the original dataset 10 times, then train a shifted MLPs classifier. According to Table 2. On the German Credit and Student dataset, DiRRAc has a higher M2 validity than other baselines, and a slightly lower M2 validity on the SBA dataset than ROAR, while maintaining a low l1 cost relative to ROAR and CEPM.\n\nTable 2: Benchmark of M1 and M2 validity, l1 and l2 cost for non-linear models on real datasets.\n\nDataset Methods\n\nM1 validity M2 validity\n\nl1 cost\n\nl2 cost\n\nGerman\n\nSBA\n\nStudent\n\nLIME-AR Wachter CEPM LIME-ROAR LIME-DiRRAc LIME-Gaussian DiRRAc\n\nLIME-AR Wachter CEPM LIME-ROAR LIME-DiRRAc LIME-Gaussian DiRRAc\n\nLIME-AR Wachter CEPM LIME-ROAR LIME-DiRRAc LIME-Gaussian DiRRAc\n\n0.72 ± 0.45 1.00 ± 0.00 1.00 ± 0.00 0.60 ± 0.49 0.78 ± 0.42 0.70 ± 0.46\n\n0.65 ± 0.48 1.00 ± 0.00 1.00 ± 0.00 0.97 ± 0.16 0.93 ± 0.26 0.82 ± 0.38\n\n0.66 ± 0.48 1.00 ± 0.00 1.00 ± 0.00 0.97 ± 0.18 0.97 ± 0.18 0.69 ± 0.46\n\n0.71 ± 0.27 0.55 ± 0.42 0.74 ± 0.40 0.69 ± 0.27 0.75 ± 0.27 0.70 ± 0.31\n\n0.60 ± 0.49 0.61 ± 0.45 0.80 ± 0.40 0.97 ± 0.16 0.93 ± 0.26 0.80 ± 0.38\n\n0.53 ± 0.45 0.43 ± 0.39 0.70 ± 0.46 0.95 ± 0.20 0.97 ± 0.18 0.59 ± 0.46\n\n1.05 ± 0.20 0.20 ± 0.26 1.30 ± 0.01 2.52 ± 0.20 1.14 ± 0.27 1.11 ± 0.26\n\n0.53 ± 0.23 0.30 ± 0.24 2.24 ± 0.01 4.05 ± 0.36 1.10 ± 0.11 0.64 ± 0.29\n\n0.53 ± 0.63 0.40 ± 0.27 4.51 ± 0.00 6.30 ± 0.19 1.12 ± 0.23 0.58 ± 0.54\n\n1.00 ± 0.03 0.11 ± 0.16 1.02 ± 0.00 1.25 ± 0.07 1.02 ± 0.05 1.00 ± 0.06\n\n0.44 ± 0.23 0.11 ± 0.09 1.42 ± 0.00 1.45 ± 0.12 1.07 ± 0.05 0.43 ± 0.32\n\n0.37 ± 0.32 0.20 ± 0.14 2.03 ± 0.01 1.97 ± 0.16 1.12 ± 0.23 0.50 ± 0.51\n\nConcluding Remarks. In this work, we proposed the Distributionally Robust Recourse Action (DiRRAc) framework to address the problem of recourse robustness under shifts in the parameters of the classification model. We introduced a distributionally robust optimization approach for generating a robust recourse action using a projected gradient descent algorithm. The experimental results demonstrated that our framework has the ability to generate the recourse action that has high probability of being valid under different types of data distribution shifts with a low cost. We also showed that our framework can be adapted to different model types, linear and non-linear models, and allows for actionability constraints of the recourse action.\n\nRemark 5.1 (Extensions). The DiRRAc framework can be extended to hedge against the misspecification of the mixture weights (cid:98)p. Alternatively, the objective function of DiRRAc can be modified to minimize the worst-case component probability. These extensions are explored in Section C. Corresponding extensions for the Gaussian DiRRAc framework are presented in Section D.\n\nRemark 5.2 (Choice of ambiguity set). This paper’s results rely fundamentally on the design of ambiguity sets using a Gelbrich distance on the moment space. This Gelbrich ambiguity set leads to the (cid:107)·(cid:107)2-regularizations of the worst-case Value-at-Risk in Lemmas 3.3 and 4.2. If we consider other moment ambiguity sets, for example, the moment bounds in Delage & Ye (2010) or the KullbackLeibler-type sets in Taskesen et al. (2021), then these regularization equivalence are not available, and there is no trivial way to extend the results to reformulate the (Gaussian) DiRRAc framework.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nAcknowledgments. Viet Anh Nguyen acknowledges the generous support from the CUHK’s Improvement on Competitiveness in Hiring New Faculties Funding Scheme.\n\nREFERENCES\n\nAndr ́e Artelt and Barbara Hammer. On the computation of counterfactual explanations - A survey.\n\narXiv preprint arXiv:1911.07749, 2019.\n\nG. Bayraksan and D. K. Love. Data-driven stochastic programming using phi-divergences.\n\nIN-\n\nFORMS TutORials in Operations Research, pp. 1–19, 2015.\n\nAmir Beck. First-order Methods in Optimization. SIAM, 2017.\n\nAharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341–357, 2013.\n\nAharon Ben-Tal, Ruud Brekelmans, Dick den Hertog, and Jean-Philippe Vial. Globalized robust optimization for nonlinear uncertain inequalities. INFORMS Journal on Computing, 29(2):350– 366, 2017.\n\nD. Bertsimas, V. Gupta, and N. Kallus. Data-driven robust optimization. Mathematical Program-\n\nming, 167(2):235–292, 2018.\n\nEmily Black, Zifan Wang, and Matt Fredrikson. Consistent counterfactuals for deep models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=St6eyiTEHnG.\n\nS. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\n\nNgoc Bui, Duy Nguyen, and Viet Anh Nguyen. Counterfactual plans under distributional ambiguity.\n\nIn International Conference on Learning Representations, 2022.\n\nZhi Chen and Weijun Xie. Sharing the value-at-risk under distributional ambiguity. Mathematical\n\nFinance, 31(1):531–559, 2021.\n\nPaulo Cortez and Alice Silva. Using data mining to predict secondary school student performance.\n\nProceedings of 5th FUture BUsiness TEChnology Conference, 2008.\n\nSusanne Dandl, Christoph Molnar, Martin Binder, and Bernd Bischl. Multi-objective counterfactual explanations. In International Conference on Parallel Problem Solving from Nature, pp. 448–469. Springer, 2020.\n\nE. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with application\n\nto data-driven problems. Operations Research, 58(3):595–612, 2010.\n\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.\n\nics.uci.edu/ml.\n\nJohn C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A\n\ngeneralized empirical likelihood approach. Mathematics of Operations Research, 2021.\n\nL. El Ghaoui, M. Oks, and F. Oustry. Worst-case value-at-risk and robust portfolio optimization: A\n\nconic programming approach. Operations Research, 51(4):543–556, 2003.\n\nStanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape per-\n\nspective. arXiv preprint arXiv:1912.02757, 2019.\n\nM. Gelbrich. On a formula for the L2 Wasserstein metric between measures on Euclidean and\n\nHilbert spaces. Mathematische Nachrichten, 147(1):185–203, 1990.\n\nC.R. Givens and R.M. Shortt. A class of Wasserstein metrics for probability distributions. The\n\nMichigan Mathematical Journal, 31(2):231–240, 1984.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.\n\ngurobi.com.\n\nG.A. Hanasusanto, D. Kuhn, S. W. Wallace, and S. Zymler. Distributionally robust multi-item newsvendor problems with multimodal demand distributions. Mathematical Programming, 152 (1-2):1–32, 2015.\n\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, pp. 1929–1938, 2018.\n\nAmir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-agnostic counterfactual\n\nexplanations for consequential decisions. arXiv preprint arXiv:1905.11190, 2020.\n\nAmir-Hossein Karimi, Bernhard Sch ̈olkopf, and Isabel Valera. Algorithmic recourse: From counterfactual explanations to interventions. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp. 353–362, 2021a.\n\nAmirhossein Karimi, Bernhard Sch ̈olkopf, and Isabel Valera. A survey of algorithmic recourse: Contrastive explanations and consequential recommendations. arXiv preprint arXiv:2010.04050, 2021b.\n\nDavid J Ketchen and Christopher L Shook. The application of cluster analysis in strategic management research: an analysis and critique. Strategic Management Journal, 17(6):441–458, 1996.\n\nD. Kuhn, P. Mohajerin Esfahani, V.A. Nguyen, and S. Shafieezadeh-Abadeh. Wasserstein distributionally robust optimization: Theory and applications in machine learning. INFORMS TutORials in Operations Research, pp. 130–169, 2019.\n\nMin Li, Amy Mickel, and Stanley Taylor. “Should this loan be approved or denied?”: A large dataset\n\nwith class assignment guidelines. Journal of Statistics Education, 26(1):55–66, 2018.\n\nGeoffrey J. McLachlan and David Peel. Finite Mixture Models, volume 299 of Probability and\n\nStatistics – Applied Probability and Statistics Section. Wiley, New York, 2000.\n\nMOSEK ApS. MOSEK Optimizer API for Python 9.2.10, 2019. URL https://docs.mosek.\n\ncom/9.2/pythonapi/index.html.\n\nRamaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 607–617, 2020.\n\nK.P. Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.\n\nHongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In\n\nAdvances in Neural Information Processing Systems 30, pp. 2971–2980, 2017.\n\nTuan-Duy H Nguyen, Ngoc Bui, Duy Nguyen, Man-Chung Yue, and Viet Anh Nguyen. Robust\n\nBayesian recourse. In Uncertainty in Artificial Intelligence, pp. 1498–1508. PMLR, 2022.\n\nViet Anh Nguyen. Adversarial Analytics. PhD thesis, Ecole Polytechnique F ́ed ́erale de Lausanne,\n\n2019.\n\nViet Anh Nguyen, Soroosh Shafieezadeh Abadeh, Damir Filipovi ́c, and Daniel Kuhn. Mean-\n\ncovariance robust risk measurement. arXiv preprint arXiv:2112.09959, 2021.\n\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019.\n\nLeandro Pardo. Statistical Inference Based on Divergence Measures. CRC Press, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMartin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. On counterfactual explanations under\n\npredictive multiplicity. In UAI, 2020.\n\nMartin Pawelczyk, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji Kasneci. CARLA: A Python library to benchmark algorithmic recourse and counterfactual explanation algorithms. arXiv preprint arXiv:2108.00783, 2021.\n\nHamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv\n\npreprint arXiv:1908.05659, 2019.\n\nKaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. Advances in Neural Information Processing Systems, 33:12187–12198, 2020a.\n\nKaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and in-\n\nteractive summaries of actionable recourses. arXiv preprint arXiv:2009.07165, 2020b.\n\nKaivalya Rawal, Ece Kamar, and Himabindu Lakkaraju. Algorithmic recourse in the wild: Under-\n\nstanding the impact of data and model shifts. arXiv preprint arXiv:2012.11788, 2021.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135–1144, 2016.\n\nErnst Roos and Dick den Hertog. Reducing conservatism in robust optimization. INFORMS Journal\n\non Computing, 32(4):1109–1127, 2020.\n\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and\n\nuse interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.\n\nChris Russell. Efficient search for diverse coherent explanations. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 20–28. Association for Computing Machinery, 2019.\n\nAlexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Program-\n\nming: Modeling and Theory. SIAM, 2009.\n\nIlia Stepin, Jose M. Alonso, Alejandro Catala, and Mart ́ın Pereira-Fari ̃na. A survey of contrastive and counterfactual explanation generation methods for explainable artificial intelligence. IEEE Access, 9:11974–12001, 2021.\n\nBahar Taskesen, Man-Chung Yue, Jose Blanchet, Daniel Kuhn, and Viet Anh Nguyen. Sequential In Proceedings of the 38th\n\ndomain adaptation by synthesizing distributionally robust experts. International Conference on Machine Learning, 2021.\n\nRobert L Thorndike. Who belongs in the family. In Psychometrika. Citeseer, 1953.\n\nSohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards robust and reliable algorith-\n\nmic recourse. In Advances in Neural Information Processing Systems 35, 2021.\n\nBerk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 10–19, 2019.\n\nSuresh Venkatasubramanian and Mark Alfano. The philosophical basis of algorithmic recourse. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* ISBN ’20, pp. 284–293, New York, NY, USA, 2020. Association for Computing Machinery. 9781450369367. doi: 10.1145/3351095.3372876. URL https://doi.org/10.1145/ 3351095.3372876.\n\nSahil Verma, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P. Dickerson, and Chirag Shah. Counterfactual explanations and algorithmic recourses for machine learning: A review, 2020. URL https://arxiv.org/abs/2010.10596.\n\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harvard Journal of Law & Technology, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL EXPERIMENT RESULTS\n\nHere, we provide further details about the datasets, experimental settings, and additional results. Source code can be found at https://github.com/duykhuongnguyen/DiRRAc.\n\nA.1 DATASETS\n\nReal-world datasets. We use three real-world datasets which are popular in the settings of robust algorithmic recourse: German credit (Dua & Graff, 2017), SBA Li et al. (2018), and Student performance Cortez & Silva (2008). We select a subset of features from each dataset:\n\n• For the German credit dataset from the UCI repository, we choose five features: Status, Duration, Credit amount, Personal Status, and Age. We found in the descriptions of two datasets that feature Status in the data correction shift dataset corrects the coding errors in the original dataset (Dua & Graff, 2017).\n\n• For the SBA dataset, we follow Li et al. (2018) and Upadhyay et al. (2021) and we choose 13 features: Selected, Term, NoEmp, CreateJob, RetainedJob, UrbanRural, ChgOffPrinGr, GrAppv, SBA Appv, New, RealEstate, Portion, Recession. We use the instances during 1989-2006 as original data and the remaining instances as shifted data.\n\n• For the Student Performance dataset, motivated by Cortez & Silva (2008), we choose G3 - final grade for deciding the label pass or fail for each student. The student who has G3 < 12 is labeled 0 (failed) and 1 (passed) otherwise. For input features, we choose 9 features: Age, Study time, Famsup, Higher, Internet, Health, Absences, G1, G2. We separate the dataset into the original and the geospatial shift data by 2 different schools.\n\nWe report the accuracy of the current classifiers and shifted classifiers for two types of models: logistics classifiers (LR) and MLPs classifiers (MLPs) on each dataset in Table 3.\n\nTable 3: Accuracy of the underlying classifiers.\n\nDataset\n\nGerman\n\nShifted German\n\nSBA\n\nShifted SBA\n\nStudent\n\nShifted Student\n\nMethods\n\nAccuracy\n\nLR MLPs LR MLPs\n\nLR MLPs LR MLPs\n\nLR MLPs LR MLPs\n\n0.72 ± 0.00 0.76 ± 0.01 0.7 ± 0.00 0.72 ± 0.01\n\n0.79 ± 0.01 0.93 ± 0.02 0.77 ± 0.01 0.89 ± 0.01\n\n0.84 ± 0.01 0.91 ± 0.01 0.91 ± 0.00 0.99 ± 0.01\n\nSynthetic data. We synthesize two-dimensional data and simulate the shifted data by using K = 3 different shifts similar to Upadhyay et al. (2021): mean shift, covariance shift, mean and covariance shift. First, we fix the unshifted conditional distributions with X|Y = y ∼ N (μy, Σy) ∀y ∈ Y. 0 = μ0 + [α, 0](cid:62), where α is a mean shift magnitude. For For mean shift, we replace μ0 by μshift covariance shift, we replace Σ0 by Σshift 0 = (1 + β)Σ0, where β is a covariance shift magnitude. For mean and covariance shift, we replace (μ0, Σ0) by (μshift ). We generate 500 samples for each class from the unshifted distribution with μ0 = [−3; −3], μ1 = [3; 3], and Σ0 = Σ1 = I.\n\n, Σshift 0\n\n0\n\nTo visualize the decision boundaries of the linear classifiers for synthetic data, we synthesize the shifted data in total 100 times including 33 mean shifts, 33 covariance shifts and 34 both shifts, then we visualize the 100 model’s parameters in a two-dimensional space in Figure 4 and Figure 5.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n(a) Original data\n\n(b) Mean shift\n\n(c) Covariance shift\n\n(d) Both shift\n\nFigure 4: Synthetic data shifts and the corresponding model parameter shifts (decision boundaries).\n\nFigure 5: Parameter θ of the classifier with different types of data distribution shifts.\n\nA.2 EXPERIMENTAL SETTINGS\n\nImplementation details. For all the baselines, we use the implementation of CARLA (Pawelczyk et al., 2021). We use the hyperparameters of AR, Wachter and CEPM that are provided by CARLA. For ROAR, we use the same parameters as in ROAR (Upadhyay et al., 2021).\n\nExperimental settings. The experimental settings for the experiments in the main text are as follows:\n\n• In Figure 3, we fix ρ1 = 0.1 and vary δadd ∈ [0, 2.0] for DiRRAc. Then we fix δmax = 0.1 and\n\nvary λ ∈ [0.01, 0.2] for ROAR.\n\n• In Table 1 and Table 2, we first initialize ρ1 = 0.1 and we choose the δadd that maximizes the M1 validity. We follow the same procedure as in the original paper for ROAR (Upadhyay et al., 2021): choose δmax = 0.1 and find the value of λ that maximizes the M1 validity. The detailed settings are provided in Table 4.\n\nTable 4: Parameters for the experiments with real-world data in Table 1.\n\nParameters Values\n\nK δadd (cid:98)p ρ\nλ ζ\n\n1 1.0 [1] [0.1] 0.7 1\n\n14\n\n10010x11050510x210010x11050510x210010x11050510x210010x11050510x20.60.81.01.21.41.61.82.001.001.251.501.752.002.252.502.751OriginalMean shiftCov shiftBoth shiftPublished as a conference paper at ICLR 2023\n\nChoice of number of components K for real-world datasets. To choose K for real-world datasets, we use the same procedure in Section 5 to obtain 100 observations of the model parameters. Then we determine the number of components K on these observations by using K-means clustering and Elbow method (Thorndike, 1953; Ketchen & Shook, 1996). Then we train a Gaussian mixture model on these observations and obtain (cid:98)pk, (cid:98)θk, (cid:98)Σk for the optimal number of components K. The Elbow method visualization for each dataset is shown in Figure 6.\n\nFigure 6: Elbow method for determining the optimal number of components for parameter shifts. Dashed lines represent the optimal K for three real-world datasets. German Credit: Elbow at K = 5. SBA: Elbow at K = 4. Student Performace: Elbow at K = 6.\n\nTable 5: Performance of DiRRAc and Gaussian DiRRAc with K components on three real-world datasets.\n\nDataset Methods\n\nM1 validity M2 validity\n\nl1 cost\n\nl2 cost\n\nGerman DiRRAc (K = 5)\n\nGaussian DiRRAc (K = 5)\n\n1.00 ± 0.00 1.00 ± 0.00\n\n0.99 ± 0.07 0.99 ± 0.07\n\n1.73 ± 0.31 1.73 ± 0.31\n\n1.40 ± 0.20 1.23 ± 0.23\n\nSBA\n\nStudent\n\nDiRRAc (K = 4) Gaussian DiRRAc (K = 4)\n\n1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 0.99 ± 0.02\n\n1.83 ± 0.49 1.67 ± 0.68\n\n1.48 ± 0.29 0.98 ± 0.42\n\nDiRRAc (K = 6) Gaussian DiRRAc (K = 6)\n\n1.00 ± 0.00 1.00 ± 0.00\n\n0.96 ± 0.09 0.75 ± 0.19\n\n1.59 ± 0.33 0.82 ± 0.30\n\n1.04 ± 0.22 0.53 ± 0.21\n\nThe results in Table 5 indicate that as we deploy our framework with the optimal number of components K, then DiRRAc delivers a smaller cost in all three datasets. The M2 validity of Gaussian DiRRAc slightly increases in the Student Performance dataset.\n\nSensitivity analysis of hyperparameters δadd and ρk. Here we analyze the sensitivity of the hyperparameters δadd and ρk to the l1 cost of recourses and M2 validity of DiRRAc.\n\nFrom the results in Figure 3, we can observe that as δadd increases, both the cost and the robustness of the recourse increase.\n\nWe study the sensitivity of hyperparameters ρk to M2 validity by first fixing the δadd = 0.1 and vary ρk ∈ [0.0, 0.5]. According to Figure 7, we can observe that as ρk increases, the cost of recourses rises as well, yielding in more robust recourses.\n\n15\n\n23456789Numberofcomponents8101214DistortionGerman23456789Numberofcomponents17.520.022.525.0SBA23456789Numberofcomponents16182022StudentPublished as a conference paper at ICLR 2023\n\nFigure 7: Sensitivity analysis of hyperparameters ρk to l1 cost and M2 validity of DiRRAc.\n\nA.3 RESULTS ON REAL-WORLD DATA\n\nExperiments with prior on (cid:98)Σ. In some cases, we presume, we may not have access to the training data. We set (cid:98)θ1 = θ0, where θ0 is the parameters of the original classifier. Then we choose (cid:98)Σ1 = τ I with τ = 0.1. We generate recourse for each input instance and compute the M1 validity using the original classifier and the M2 validity using the shifted classifiers. The results in Table 6 show that our methods produce the same performance while at the same time keeping the l1 and l2 cost lower than ROAR in all three datasets.\n\nTable 6: Benchmark of M1 validity, M2 validity, l1 and l2 using (cid:98)θ1 = θ0 and (cid:98)Σ1 = 0.1I on different real-world datasets.\n\nDataset Methods\n\nM1 validity M2 validity\n\nl1 cost\n\nl2 cost\n\nGerman ROAR\n\nSBA\n\nStudent\n\nDiRRAc Gaussian DiRRAc\n\nROAR DiRRAc Gaussian DiRRAc\n\nROAR DiRRAc Gaussian DiRRAc\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n0.94 ± 0.15 0.96 ± 0.07 0.99 ± 0.06\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n0.94 ± 0.10 0.97 ± 0.06 0.88 ± 0.14\n\n3.88 ± 0.54 1.48 ± 0.39 1.58 ± 0.29\n\n3.10 ± 0.72 1.64 ± 0.37 1.64 ± 0.37\n\n2.02 ± 0.38 1.81 ± 0.19 1.18 ± 0.26\n\n1.61 ± 0.22 1.34 ± 0.41 1.35 ± 0.24\n\n1.35 ± 0.30 1.27 ± 0.30 1.25 ± 0.26\n\n0.96 ± 0.18 1.47 ± 0.13 0.82 ± 0.18\n\nExperiments with actionability constraints. Using our two methods (DiRRAc and Gaussian DiRRAc) and the AR method (Ustun et al., 2019), we analyze how the actionability constraints affect the cost and validity of the recourse. We select a subset of features from each dataset and define each feature as immutable or non-decreasing as follows:\n\n• In the German credit dataset, we select Personal status as an immutable attribute because it is challenging to impose changes in an individual‘s status and sex. We view age as a non-decreasing feature.\n\n• In the SBA dataset, we select UrbanRural and Recession as two immutable attributes since it will be difficult to change these features in the near future. RetainedJob is another feature that we view as non-decreasing.\n\n• In the Student Performance dataset, we assume that a student’s Higher education would not change, and select higher education as an immutable feature. Age and Absences are considered as non-decreasing.\n\n16\n\n0.00.20.40.60.81.01.2l1costGerman0.00.20.40.80.91.01.1SBA0.00.20.40.40.50.6Student0.00.20.4ρk0.850.900.95M2validity0.00.20.4ρk0.70.80.90.00.20.4ρk0.550.600.65Published as a conference paper at ICLR 2023\n\nThe above specifications are aligned with the existing numerical setup in algorithmic recourse (Ustun et al., 2019; Rawal & Lakkaraju, 2020a).\n\nFor each dataset, we run the process of generating the recourse action by adding constraints to the projected gradient descent algorithm. The experimental setup on three different real-world datasets is the same as in Section 5.\n\nThe results in Table 7 indicate that the M2 validity of our 2 methods drops in the German Credit dataset. The validity in shifted data of AR also decreases in this dataset. In other datasets, the performance of our 2 methods remains the same. The l1 and l2 cost of DiRRAc slightly increase in the Student Performance dataset. Furthermore, there exists recourse for every input instance.\n\nTable 7: Benchmark of M1 validity, M2 validity, l1 and l2 using actionability constraints on different real-world datasets.\n\nDataset Methods\n\nM1 validity M2 validity\n\nl1 cost\n\nl2 cost\n\nGerman AR\n\nSBA\n\nStudent\n\nDiRRAc Gaussian DiRRAc\n\nAR DiRRAc Gaussian DiRRAc\n\nAR DiRRAc Gaussian DiRRAc\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n0.76 ± 0.26 0.99 ± 0.06 0.99 ± 0.06\n\n0.41 ± 0.18 1.00 ± 0.00 0.99 ± 0.02\n\n0.48 ± 0.19 0.95 ± 0.09 0.74 ± 0.18\n\n0.61 ± 0.40 1.62 ± 0.30 1.62 ± 0.30\n\n0.61 ± 0.42 1.74 ± 0.44 1.60 ± 0.62\n\n0.29 ± 0.21 1.61 ± 0.31 0.81 ± 0.27\n\n0.43 ± 0.25 1.27 ± 0.20 1.09 ± 0.24\n\n0.56 ± 0.36 1.34 ± 0.40 0.98 ± 0.42\n\n0.26 ± 0.18 1.08 ± 0.24 0.55 ± 0.21\n\nComparison with RBR. Here we compare our approach on the nonlinear model settings to a more recent approach on robust recourse (Nguyen et al., 2022).\n\nTable 8: Comparison with RBR for non-linear models on real datasets.\n\nDataset Methods\n\nGerman RBR\n\nSBA\n\nStudent\n\nLIME-DiRRAc LIME-Gaussian DiRRAc\n\nRBR LIME-DiRRAc LIME-Gaussian DiRRAc\n\nRBR LIME-DiRRAc LIME-Gaussian DiRRAc\n\nM1 validity M2 validity\n\nl1 cost\n\nl2 cost\n\n0.98 ± 0.13 0.78 ± 0.42 0.70 ± 0.46\n\n1.00 ± 0.00 0.93 ± 0.26 0.82 ± 0.38\n\n1.00 ± 0.00 0.97 ± 0.18 0.69 ± 0.46\n\n0.71 ± 0.25 0.75 ± 0.27 0.70 ± 0.31\n\n0.97 ± 0.12 0.93 ± 0.26 0.80 ± 0.38\n\n0.90 ± 0.23 0.97 ± 0.18 0.59 ± 0.46\n\n1.11 ± 0.10 1.14 ± 0.27 1.11 ± 0.26\n\n1.42 ± 0.45 1.10 ± 0.11 0.64 ± 0.29\n\n1.02 ± 0.53 1.12 ± 0.23 0.58 ± 0.54\n\n0.50 ± 0.07 1.02 ± 0.05 1.00 ± 0.06\n\n0.59 ± 0.18 1.07 ± 0.05 0.43 ± 0.32\n\n0.42 ± 0.20 1.12 ± 0.23 0.50 ± 0.51\n\nWe provide the results in Table 8: we can observe that RBR has (nearly) perfect M1 validity. This result is natural because RBR is designed to handle the nonlinear predictive model directly. Our methods do not have the perfect M1 validity because we use the LIME approximation. However, it is important to note that in the problem of robust recourse facing future model shifts, we regard the M2 validity as the most crucial metric because it is the proportion of recourse instances that are valid with respect to the shifted (future) models.\n\nIn terms of l1 cost and M2 validity, the results demonstrate that our method has a competitive performance compared to the existing state-of-the-art methods. In particular, LIME-DiRRAc outperforms RBR in terms of M2 validity for two datasets (German and Student). In the SBA dataset, our approach has a lower M2 validity, but the cost of recourses generated by our method is also lower. This result is consistent with our discussion about the l1 cost and M2 validity trade-off in the Appendix.\n\nComparison with MINT on German Credit datasets. We add a more recent baseline MINT proposed by Karimi et al. (2021a) for comparison purpose. MINT requires a causal graph; thus, we restrict the experiment to the German Credit dataset (the specifications of the causal graphs are not available for SBA and Student Performance). We do not consider MACE as a baseline for nonlinear\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nmodel comparison because MACE is not applicable to neural network target models due to its high computational cost. We use the same set of features as in the MINT and ROAR paper (Karimi et al., 2021a; Upadhyay et al., 2021) with four features: Sex, Age, Credit Amount and Duration. The results in Table 9 demonstrate that the recourse generated by our framework is more robust to model shifts, but it has a higher l1 cost.\n\nTable 9: Comparison with MINT on German Credit dataset.\n\nMethods\n\nM1 validity M2 validity\n\nl1 cost\n\nMINT DiRRAc Gaussian DiRRAc\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n0.87 ± 0.09 0.99 ± 0.06 0.99 ± 0.06\n\n0.77 ± 0.23 1.62 ± 0.30 1.62 ± 0.30\n\nComparison with ensemble baselines. Prior work suggested that model ensembles can be effective for out-of-distribution prediction (Ovadia et al., 2019; Fort et al., 2019). Now we explore a model ensemble method to generate recourse based on ROAR as follows. First we follow the procedure in Section 5 to obtain 100 model parameters θi with i ∈ {1, . . . , 100}. Then we find recourse by solving the following problem:\n\nx(cid:48)(cid:48) = arg min x(cid:48)(cid:48)∈A\n\nmax δ∈∆\n\nmax i∈{1,...,100}\n\n(cid:96)\n\n(cid:16)\n\nCθi\n\nδ\n\n(cid:17)\n\n(x(cid:48)(cid:48)) , 1\n\n+ λc (x0, x(cid:48)(cid:48)) ,\n\nwhere (cid:96) is the cross-entropy loss function.\n\nSecond, we use the same 100 models and generate recourse for each model independently. Then we average the ROAR recourses across those 100 models as follows.\n\nx(cid:48)(cid:48) =\n\n1 100\n\n100 (cid:88)\n\ni=1\n\narg min x(cid:48)(cid:48)∈A\n\nmax δ∈∆\n\n(cid:96)\n\n(cid:16)\n\nCθi\n\nδ\n\n(cid:17)\n\n(x(cid:48)(cid:48)) , 1\n\n+ λc (x0, x(cid:48)(cid:48)) .\n\nTable 10: Benchmark of different variants of ROAR on three real-world datasets.\n\nDataset Methods\n\nM1 validity M2 validity\n\nl1 cost\n\nl2 cost\n\nGerman ROAR\n\nSBA\n\nStudent\n\nROAR-Ensemble ROAR-Avg DiRRAc Gaussian DiRRAc\n\nROAR ROAR-Ensemble ROAR-Avg DiRRAc Gaussian DiRRAc\n\nROAR ROAR-Ensemble ROAR-Avg DiRRAc Gaussian DiRRAc\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00\n\n0.94 ± 0.15 0.95 ± 0.15 0.95 ± 0.15 0.99 ± 0.06 0.99 ± 0.06\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 0.99 ± 0.02\n\n0.94 ± 0.10 0.98 ± 0.05 0.97 ± 0.10 0.95 ± 0.09 0.74 ± 0.18\n\n3.88 ± 0.54 5.11 ± 0.59 4.46 ± 0.36 1.62 ± 0.30 1.62 ± 0.30\n\n3.10 ± 0.72 4.54 ± 0.95 2.86 ± 0.70 1.74 ± 0.44 1.60 ± 0.62\n\n2.02 ± 0.38 3.73 ± 0.50 2.78 ± 0.31 1.55 ± 0.34 0.78 ± 0.30\n\n1.61 ± 0.22 2.12 ± 0.24 2.00 ± 0.14 1.25 ± 0.21 1.05 ± 0.23\n\n1.35 ± 0.30 1.91 ± 0.38 1.78 ± 0.35 1.34 ± 0.40 0.98 ± 0.42\n\n0.96 ± 0.18 1.43 ± 0.19 1.31 ± 0.17 1.07 ± 0.23 0.54 ± 0.21\n\nIn Table 10, we provide results for the ROAR ensemble method as ROAR-Ensemble and the average ROAR recourses as ROAR-Avg. From this table, the M1 and M2 validity of ROAR-Ensemble and ROAR-Avg remain the same for all datasets. In almost every benchmark, the recourses generated by those two approaches are more costly than ROAR. In comparison with our framework, our DiRRAc and Gaussian DiRRAc methods demonstrate advantages in terms of the cost of recourses.\n\nMore discussions about cost-validity trade-off. Previous work about robust recourses have suggested that recourses are more robust with the expense of higher costs (Rawal et al., 2021; Upadhyay et al., 2021; Pawelczyk et al., 2020; Black et al., 2022). Our results with DiRRAc and Gaussian\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nDiRRAc are consistent with this suggestion. However, our framework can achieve robust and actionable recourses with a far smaller cost than ROAR (Upadhyay et al., 2021) and CEPM (Pawelczyk et al., 2020).\n\nComparison of run time. Table 11 reports the average run time: we observe that Wachter has the smallest run time, and our (Gaussian) DiRRAc has a smaller run time than ROAR in all datasets.\n\nTable 11: Average runtime (seconds).\n\nMethods\n\nGerman\n\nSBA\n\nStudent\n\nAR Wachter ROAR DiRRAc Gaussian DiRRAc\n\n0.027 0.006 0.396 0.208 0.091\n\n0.046 0.011 0.355 0.363 0.117\n\n0.039 0.006 0.412 0.244 0.124\n\nA.4 RESULTS ON SYNTHETIC DATA\n\nWe define the adaptive mean and covariance shift magnitude as α = μadapt×iter, β = Σadapt×iter with μadapt, Σadapt are the factor of data shifts, iter is the index of iterative loop of synthesizing process.\n\nFigure 8: Comparison of M2 validity as a function of the l1 distance between input instance and the recourse for our DiRRAc method and ROAR on synthetic data.\n\nFor data distribution shifts, we generate mean shifts and covariance shifts 50 times each type with adaptive mean and covariance shift magnitude, with the parameters μadapt = Σadapt = 0.1. To estimate (cid:98)θk and (cid:98)Σk, we define valid mixture weights (cid:98)p and generate data for each component for 100 times with the same ratio as the mixture weight. We train 100 logistic classifiers to compute the empirical mean (cid:98)θk and the empirical covariance matrix (cid:98)Σk for the k-th component. We generate a recourse for each test instance that belongs to the negative class. In Figure 8, we present the results of the cost-robustness analysis of DiRRAc and ROAR on synthetic data.\n\nFigure 9: Impact of distribution shifts to the empirical validity. Left: mean shifts parametrized by α; Center: covariance shifts parametrized by β; Right: Mean and covariance shifts with α = β.\n\n19\n\n6.06.26.46.66.87.0l1 cost0.50.60.70.80.91.0M2 validityDiRRAcROAR0.000.250.500.751.001.251.501.752.000.40.50.60.70.80.91.0M2 validity0.000.250.500.751.001.251.501.752.000.40.50.60.70.80.91.00.000.250.500.751.001.251.501.752.000.40.50.60.70.80.91.0DiRRAcARWachterROARPublished as a conference paper at ICLR 2023\n\nB PROOFS\n\nB.1 PROOFS OF SECTION 3\n\nTo prove Proposition 3.4, we are using the notion of Value-at-Risk which is defined as follows. Definition B.1 (Value-at-Risk). For any fixed distribution Qk of ̃θ, the Value-at-Risk at the risk tolerance level β ∈ (0, 1) of the loss ̃θ(cid:62)x is defined as\n\nQk- VaRβ( ̃θ(cid:62)x) (cid:44) inf{τ ∈ R : Qk( ̃θ(cid:62)x ≤ τ ) ≥ 1 − β}.\n\nWe are now ready to provide the proof of Proposition 3.4.\n\nProof of Proposition 3.4. Using the definition of the Value-at-Risk in Definition B.1, we have\n\nsup Qk∈Bk((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ 0) = inf\n\nβ : β ∈ [0, 1],\n\n(cid:40)\n\n(cid:40)\n\n= inf\n\nβ : β ∈ [0, 1],\n\nQk( ̃θ(cid:62)x ≤ 0) ≤ β\n\n(cid:41)\n\nQk- VaRβ(− ̃θ(cid:62)x) ≤ 0\n\n(cid:41)\n\nsup Qk∈Bk((cid:98)Pk)\n\nsup Qk∈Bk((cid:98)Pk)\n\nBy Nguyen (2019, Lemma 3.31), we can reformulate the worst-case value-at-risk as\n\nsup Qk∈Bk((cid:98)Pk)\n\nQk- VaRβ(− ̃θ(cid:62)x) = −(cid:98)θ(cid:62)\n\nk x +\n\n(cid:115)\n\n(cid:113)\n\n1 − β β\n\nx(cid:62) (cid:98)Σkx +\n\nρk√ β\n\n(cid:107)x(cid:107)2.\n\nIt is now easy to observe that in the first case when −(cid:98)θ(cid:62) supQk∈Bk((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ 0) = 1.\n\nk x + ρk(cid:107)x(cid:107)2 ≥ 0, then we should have\n\nWe now consider the second case when −(cid:98)θ(cid:62) of the worst-case value-at-risk with respect to β, that the minimal value β(cid:63) should satisfies\n\nβ (cid:107)x(cid:107)2 < 0. It is easy to see, by the monotocity\n\nk x + ρk√\n\n−(cid:98)θ(cid:62)\n\nk x +\n\n(cid:115)\n\n(cid:113)\n\n1 − β(cid:63) β(cid:63)\n\nx(cid:62) (cid:98)Σkx +\n\nρk√ β(cid:63)\n\n(cid:107)x(cid:107)2 = 0.\n\nUsing the transformation t ←\n\n√\n\nβ(cid:63), we have\n\n−(cid:98)θ(cid:62)\n\nk xt +\n\n(cid:112)\n\n1 − t2\n\n(cid:113)\n\nx(cid:62) (cid:98)Σkx + ρk(cid:107)x(cid:107)2 = 0.\n\nBy rearranging terms and then squaring up both sides, we have the equivalent quadratic equation\n\nk)t2 + 2AkCkt + C 2\n\nk − B2\n\nk = 0\n\n(A2\n\nk + B2 (cid:113)\n\nwith Ak (cid:44) −(cid:98)θ(cid:62) the proposition. Note, moreover, that we also have A2\n\nk x ≤ 0, Bk (cid:44)\n\nx(cid:62) (cid:98)Σkx ≥ 0, and Ck (cid:44) ρk(cid:107)x(cid:107)2 ≥ 0 as defined in the statement of\n\nk ≥ C 2\n\nk. This leads to the solution\n\nt =\n\n−AkCk + Bk A2\n\n(cid:112)A2 k + B2\n\nk\n\nk + B2\n\nk − C 2\n\nk\n\n≥ 0.\n\nThus, we find\n\nfk(x) =\n\nThis completes the proof.\n\n(cid:16) −AkCk + Bk A2\n\n(cid:112)A2 k + B2\n\nk\n\nk + B2\n\nk − C 2\n\nk\n\n(cid:17)2\n\n.\n\nWe now provide the proof of Theorem 3.2.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nProof of Theorem 3.2. We first consider the objective function f of (2), which can be re-expressed as\n\nf (x) = sup\n\nP∈B((cid:98)P)\n\nP(C ̃θ(x) = 0)\n\n(cid:88)\n\nk∈[K]\n\n(cid:98)pkQk( ̃θ(cid:62)x ≤ 0)\n\nQk( ̃θ(cid:62)x ≤ 0)\n\n=\n\n=\n\n=\n\nsup Qk∈Bk((cid:98)Pk) ∀k (cid:88)\n\n(cid:98)pk × sup\n\nQk∈Bk((cid:98)Pk)\n\n(cid:98)pk × fk(x),\n\nk∈[K] (cid:88)\n\nk∈[K]\n\nwhere the equality in the second line follows from the non-negativity of (cid:98)pk, and the last equality follows from the definition of fk(x) in (5). Applying Proposition 3.4, we obtain the objective function of problem (4).\n\nConsider now the last constraint of (2). Using the result of Proposition 3.4, this constraint is equivalent to\n\n−(cid:98)θ(cid:62)\n\nk x + ρk(cid:107)x(cid:107)2 < 0\n\n∀k ∈ [K].\n\nThis leads to the feasible set X as is defined in (3). This completes the proof.\n\nB.2 PROOFS OF SECTION 4\n\nTo prove Theorem 4.1, we first define the following worst-case Gaussian component probability function\n\nk (x) (cid:44) f N\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) =\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ 0)\n\n∀k ∈ [K].\n\n(11)\n\nThe next proposition provides the reformulation of f N k . Proposition B.2 (Worst-case probability - Gaussian). For any x ∈ Rd, any k ∈ [K] and any + × R+, define the following constants Ak (cid:44) −(cid:98)θ(cid:62) ((cid:98)θk, (cid:98)Σk, ρk) ∈ Rd × Sd Ck (cid:44) ρk(cid:107)x(cid:107)2. The following holds:\n\nx(cid:62) (cid:98)Σkx, and\n\nk x, Bk (cid:44)\n\n(cid:113)\n\n(i) We have f N\n\nk (x) < 1\n\n2 if and only if Ak + Ck < 0.\n\n(ii) If x satisfies f N\n\nk (x) < 1\n\n2 , then\n\nf N\n\nk (x) = 1 − Φ\n\n(cid:16)\n\nA2\n\nk − C 2 (cid:112)A2\n\nk\n\n−AkBk + Ck\n\nk + B2\n\nk − C 2\n\nk\n\n(cid:17)\n\n.\n\nProof of Proposition B.2. We first prove Assertion (i). Pick any Qk ∈ BN sian distribution Qk ∼ N (θk, Σk), and thus\n\nk ((cid:98)Pk), then Qk is a Gaus-\n\nQk( ̃θ(cid:62)x ≤ 0) = Φ\n\n(cid:16) −θ(cid:62) √\n\nk x x(cid:62)Σx\n\n(cid:17)\n\n.\n\nGuaranteeing f N\n\nk (x) < 1\n\n2 is equivalent to guaranteeing\n\nsup G((θk,Σk),((cid:98)θk,(cid:98)Σk))≤ρk\n\n− θ(cid:62)\n\nk x ≤ 0.\n\nNote that we also have\n\nsup G((θk,Σk),((cid:98)θk,(cid:98)Σk))≤ρk\n\n− θ(cid:62)\n\nk x =\n\nsup θk:(cid:107)θk−(cid:98)θk(cid:107)2≤ρk\n\n− θ(cid:62)\n\nk x = −(cid:98)θ(cid:62)\n\nk x + ρk(cid:107)x(cid:107)2\n\nby the properties of the dual norm. This leads to the equivalent condition that Ak + Ck < 0.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nWe now prove Assertion (ii). Using the definition of the Value-at-Risk in Definition B.1, we have\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ 0) = inf\n\nβ : β ∈ [0,\n\n(cid:40)\n\n(cid:40)\n\n= inf\n\nβ : β ∈ [0,\n\n1 2\n\n1 2\n\n),\n\n),\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nsup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk( ̃θ(cid:62)x ≤ 0) ≤ β\n\n(cid:41)\n\nQk- VaRβ(− ̃θ(cid:62)x) ≤ 0\n\n(cid:41)\n\nUsing the result from Nguyen (2019, Lemma 3.31), we have\n\nQk- VaRβ(− ̃θ(cid:62)x) = −(cid:98)θ(cid:62)\n\nk x + t\n\n(cid:113)\n\nx(cid:62) (cid:98)Σkx + ρ\n\n(cid:112)\n\n1 + t2(cid:107)x(cid:107)2 = Ak + Bkt + Ck\n\n√\n\n1 + t,\n\nsup Qk∈Bk((cid:98)Pk)\n\nwith t = Φ−1(1 − β). Taking the infimum over β is then equivalent to finding the root of the equation\n\nAk + tBk + Ck\n\n1 + t2 = 0.\n\n(cid:112)\n\nUsing a transformation τ = 1/t, the above equation becomes\n\nAkτ + Bk + Ck\n\n(cid:112)\n\n1 + τ 2 = 0\n\nwith solution\n\n(cid:112)A2 −AkBk + Ck k − C 2 A2 k > C 2 k (x) = 1 − Φ(t) = 1 − Φ(1/τ ).\n\nNotice that Ak + Ck < 0, and we also have A2 by noticing that f N\n\nτ =\n\nk\n\nk + B2\n\nk − C 2\n\nk\n\n> 0.\n\nk, thus τ is well-defined. The result now follows\n\nWe are now ready to prove Theorem 4.1.\n\nProof of Theorem 4.1. Problem (8) is equivalent to\n\nmin (cid:80) s. t.\n\nk∈[K] (cid:98)pk × f N\n\nk (x)\n\nc(x, x0) ≤ δ k (x) < 1 f N\n\n2\n\n∀k ∈ [K].\n\nApplying Proposition B.2, we obtain the necessary result.\n\nC EXTENSIONS OF THE DIRRAC FRAMEWORK\n\nThroughout this section, we explore two extensions of our DiRRAc framework. In Section C.1, we study an additional layer of robustification with respect to the mixture weights (cid:98)p. Next, in Section C.2, we consider an alternative formulation of the objective function to minimize the worstcase component probability.\n\nC.1 ROBUSTIFICATION AGAINST MIXTURE WEIGHT UNCERTAINTY\n\nThe DiRRAc problem considered in Section 3 only robustifies the component distributions (cid:98)Pk. We now discuss a plausible approach to robustify against the misspecification of the mixture weights (cid:98)p. Because the mixture weights should form a probability vector, it is convenient to model the perturbation in the mixture weights using the φ-divergence. Definition C.1 (φ-divergence). Let φ : R → R be a convex function on the domain R+, φ(1) = 0, 0 × φ(a/0) = a × limt↑∞ φ(t)/t for a > 0, and 0 × φ(0/0) = 0. The φ-divergence Dφ between + amounts to Dφ(p (cid:107) (cid:98)p) (cid:44) (cid:80) two probability vectors p, (cid:98)p ∈ RK\n\nk∈[K] (cid:98)pk × φ(pk/(cid:98)pk).\n\nThe family of φ-divergences contains many well-known statistical divergences such as the KullbackLeibler divergence, the Hellinger distance, etc. Further discussion on this family can be found in Pardo (2018). Distributionally robust optimization models with φ-divergence ambiguity set were originally studied in decision-making problems (Ben-Tal et al., 2013; Bayraksan & Love, 2015) and\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nhave recently gained attention thanks to their successes in machine learning tasks (Namkoong & Duchi, 2017; Hashimoto et al., 2018; Duchi et al., 2021).\n\nLet ε ≥ 0 be a parameter indicating the uncertainty level of the mixture weights. The uncertainty set for the mixture weights is formally defined as\n\n∆ (cid:44) (cid:8)p ∈ [0, 1]K : 1(cid:62)p = 1, Dφ(p (cid:107) (cid:98)p) ≤ ε(cid:9) ,\n\nwhich contains all K-dimensional probability vectors which are of φ-divergence at most ε from the nominal weights (cid:98)p. The ambiguity set of the mixture distributions that hedge against the weight misspecification is\n\nU((cid:98)P) (cid:44) (cid:8)Q : ∃p ∈ ∆, ∃Qk ∈ Bk((cid:98)Pk) ∀k ∈ [K] such that Q ∼ (Qk, pk) (cid:9) , where the component sets Bk((cid:98)Pk) are defined as in Section 3. The DiRRAc problem with respect to the ambiguity set U((cid:98)P) becomes\n\nP(C ̃θ(x) = 0)\n\nmin\n\ns. t.\n\nsup P∈U ((cid:98)P) c(x, x0) ≤ δ\n\nsup Qk∈Bk((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) < 1\n\n∀k ∈ [K].\n\n(12)\n\nIt is important to note at this point that the feasible set of (12) coincides with the feasible set of (2). Thus, to resolve problem (12), it suffices to analyze the objective function of (12). Given the function φ, we define its conjugate function φ∗ : R → R ∪ {∞} by\n\nφ∗(s) = sup\n\n{ts − φ(t)} .\n\nt≥0\n\nThe next theorem asserts that the worst-case probability under U((cid:98)P) can be computed by solving a convex program. Theorem C.2 (Objective value). The feasible set of problem (12) coincides with X . Further, for every x ∈ X , the objective value of (12) equals to the optimal value of a convex optimization problem\n\nsup P∈U ((cid:98)P)\n\nP(C ̃θ(x) = 0) = min\n\nλ∈R+, η∈R\n\nη + ελ + λ\n\n(cid:88)\n\nk∈[K]\n\n(cid:98)pkφ∗(cid:16) fk(x) − η\n\nλ\n\n(cid:17)\n\n,\n\nwhere fk(x) are computed using Proposition 3.4.\n\nProof of Theorem C.2. From the definition of the set U((cid:98)P), we can rewrite F using a two-layer decomposition\n\nF (x) = sup\n\nP∈U ((cid:98)P)\n\nP(C ̃θ(x) = 0) = sup\n\np∈∆\n\nsup Qk∈Bk((cid:98)Pk) ∀k\n\npkQk( ̃θ(cid:62)x ≤ 0)\n\n(cid:88)\n\nk∈[K]\n\n= sup p∈∆\n\n= sup p∈∆\n\n(cid:88)\n\nk∈[K] (cid:88)\n\nk∈[K]\n\npk × sup\n\nQk( ̃θ(cid:62)x ≤ 0)\n\nQk∈Bk((cid:98)Pk)\n\npk × fk(x),\n\nwhere the equality in the second line follows from the non-negativity of pk, and the last equality follows from the definition of fk(x) in (5). By applying the result from Ben-Tal et al. (2013, Corollary 4.2), we have\n\nF (x) =\n\n \n\n\n\nmin η + ελ + λ\n\n(cid:88)\n\ns. t. λ ∈ R+, η ∈ R.\n\nk∈[K]\n\n(cid:98)pkφ∗(cid:16) fk(x) − η\n\nλ\n\n(cid:17)\n\nThe proof is complete.\n\nFrom the result of Theorem C.2, we can derive the gradient of the objective function of (12) using Danskin’s theorem Shapiro et al. (2009, Theorem 7.21), or simply using auto-differentiation. Furthermore, φ∗ is convex, and thus solving the minimization problem in Theorem C.2 can be done efficiently using convex optimization algorithms.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nC.2 MINIMIZING THE WORST-CASE COMPONENT PROBABILITY\n\nInstead of minimizing the (total) probability of unfavorable outcome, we can consider an alternative formulation where the recourse action minimizes the worst-case conditional probability of unfavorable outcome over all K components. Mathematically, if we opt for the component ambiguity sets Bk((cid:98)Pk) constructed in Section 3, then we can solve\n\nmin max k∈[K]\n\nsup Qk∈Bk((cid:98)Pk)\n\ns. t.\n\nc(x, x0) ≤ δ\n\nQk(C ̃θ(x) = 0)\n\nsup Qk∈Bk((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) < 1\n\n∀k ∈ [K].\n\n(13a)\n\nInterestingly, problem (13a) does not involve the mixture weighs (cid:98)p. As a consequence, a trivial advantage of this model is that it hedges automatically against the misspecification of (cid:98)p. To complete, we provide its equivalent finite-dimensional form. Corollary C.3 (Component Probability DiRRAc). Problem (13a) is equivalent to\n\nmin x∈X\n\nmax k∈[K]\n\nρk (cid:98)θ(cid:62)\n\nk x(cid:107)x(cid:107)2 +\n\n(cid:113)\n\nx(cid:62) (cid:98)Σkx\n\n(cid:113)\n\n((cid:98)θ(cid:62)\n\nk x)2 + x(cid:62) (cid:98)Σkx − ρ2\n\nk(cid:107)x(cid:107)2\n\n2\n\n((cid:98)θ(cid:62)\n\nk x)2 + x(cid:62) (cid:98)Σkx\n\n.\n\n(13b)\n\nD EXTENSIONS OF THE GAUSSIAN DIRRAC FRAMEWORK\n\nIn this section, we leverage the results in Section C to extend the Gaussian DiRRAc framework to (i) handle the uncertainty of the mixture weight and (ii) minimize the worst-case modal probability. Remind that each individual mixture ambiguity set BN (cid:110)\n\nk ((cid:98)Pk) is of the form\n\n(cid:111)\n\nBN\n\nk ((cid:98)Pk) =\n\nQk : Qk ∼ N (θk, Σk), G((θk, Σk), ((cid:98)θk, (cid:98)Σk)) ≤ ρk\n\n,\n\nwhich is a ball in the space of Gaussian distributions.\n\nD.1 HANDLING MIXTURE WEIGHT UNCERTAINTY - GAUSSIAN DIRRAC\n\nFollowing the notations in Section C.1, we define the set of possible mixture weights as\n\nand the ambiguity set with Gaussian information is defined as\n\n∆ = (cid:8)p ∈ [0, 1]K : 1(cid:62)p = 1, Dφ(p (cid:107) (cid:98)p) ≤ ε(cid:9)\n\nU N ((cid:98)P) =\n\n(cid:110)\n\nQ : ∃p ∈ ∆, ∃Qk ∈ BN\n\nk ((cid:98)Pk) ∀k ∈ [K] such that Q ∼ (Qk, pk)k∈[K]\n\n(cid:111)\n\n.\n\nThe distributionally robust problem with respect to the ambiguity set U((cid:98)P) is\n\nP(C ̃θ(x) = 0)\n\ninf\n\ns. t.\n\nsup P∈U N ((cid:98)P) c(x, x0) ≤ δ sup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) < 1\n\n2\n\n∀k ∈ [K].\n\n(14)\n\nFollowing the results in Section 4, the feasible set of (14) coincides with the set X . It suffices now to provide the reformulation for the objective function of (14). Corollary D.1. For any x ∈ X , we have \n\n\nη + ελ + λ\n\n(cid:88)\n\ninf\n\n(cid:17)\n\n(cid:98)pkφ∗(cid:16) f N\n\nk (x) − η λ\n\nsup P∈U N ((cid:98)P)\n\nP(C ̃θ(x) = 0) =\n\n\n\ns. t. λ ∈ R+, η ∈ R,\n\nk∈[K]\n\nwhere the values f N\n\nk (x) are obtained in Proposition B.2.\n\nCorollary D.2 follows from Theorem D.2 by replacing the quantities fk(x) by f N account the Gaussian parametric information. The proof of Corollary D.2 is omitted.\n\nk (x) to take into\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nD.2 MINIMIZING WORST-CASE COMPONENT PROBABILITY\n\nWe now consider the Gaussian DiRRAc that minimizes the worst-case modal probability of infeasibility. More concretely, we consider the recourse action obtained by solving\n\nQk(C ̃θ(x) = 0)\n\ns. t.\n\ninf max k∈[K]\n\nk ((cid:98)Pk)\n\nsup Qk∈BN c(x, x0) ≤ δ sup Qk∈BN\n\nk ((cid:98)Pk)\n\nQk(C ̃θ(x) = 0) < 1\n\n2\n\n∀k ∈ [K].\n\n(15a)\n\nThe next corollary provides the equivalent form of the above optimization problem.\n\nCorollary D.2. Problem (15a) is equivalent to\n\ninf x∈X\n\nmax k∈[K]\n\n(cid:32)\n\n1 − Φ\n\n \n\n\n\n(cid:98)θ(cid:62) k x\n\n(cid:113)\n\n((cid:98)θ(cid:62)\n\nk x)2 − ρ2 (cid:113)\n\nk(cid:107)x(cid:107)2\n\n2\n\nx(cid:62) (cid:98)Σkx + ρk(cid:107)x(cid:107)2\n\n((cid:98)θ(cid:62)\n\nk x)2 + x(cid:62) (cid:98)Σkx − ρ2\n\nk(cid:107)x(cid:107)2\n\n2\n\n(cid:33)\n\n \n\n\n\n.\n\n(15b)\n\nE PROJECTED GRADIENT DESCENT ALGORITHM\n\nThe pseudocode of the algorithm is presented in Algorithm 1. The convergence guarantee for Algorithm 1 follows from Beck (2017, Theorem 10.15), and is distilled in the next theorem.\n\nAlgorithm 1 Projected gradient descent algorithm with backtracking line-search\n\nInput: Input instance x0, feasible set Xε and objective function f Line search parameters: λ ∈ (0, 1), ζ > 0 (Default values: λ = 0.7, ζ = 1) Initialization: Set x0 ← ProjXε(x0) for t = 0, . . . , T − 1 do\n\nFind the smallest integer i ≥ 0 such that\n\nf (cid:0)ProjXε(xt − λiζ∇f (xt))(cid:1) ≤ f (xt) −\n\n1 2λiζ\n\n(cid:107)xt − ProjXε(xt − λiζ∇f (xt))(cid:107)2 2.\n\nSet xt+1 = ProjXε(xt − λiζ∇f (xt)).\n\nend for Output: xT\n\nTheorem E.1 (Convergence guarantee). Let {xt}t=0,1,...,T be the sequence generated by Algorithm 1. Then, all limit points of the sequence {xt}t=0,1,...,T are stationary points of problem (4) with the modified feasible set Xε. Furthermore, there exists some constant C > 0 such that for any T ≥ 1, we have (cid:13)xt − ProjXε (xt − ζ∇f (xt))(cid:13) (cid:13)2\n\n(cid:13)\n\nmin t=0,1,...,T\n\n≤\n\nC √\n\nT\n\n.\n\nζ\n\n25",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes a method to generate post-hoc recourses which remain valid under model shift, called DiRRAc. The model parameters are considered as a random vector modeled according to a mixture distribution (whose parameters are fit by taking models trained on trainset samples). Future model shifts are modeled by considering all distributions close to the initial distribution, called the ambiguity set. The paper proposes a minimax problem where inner max is over the ambiguity set and min is over possible counterfactuals. By choosing Gelbrich distance, the optimization problem is shown to be solvable, and the paper proposes a Projected GD algorithm. Experiments are conducted on 3 real-world datasets, and they compare against 4 baselines.\n\n# Strength And Weaknesses\n\n+ The paper deals with a practical problem; generating robust recourses is necessary for models which are to be deployed in the real world\n+ Experimental results are encouraging, as DiRRAc achieves best post-shift validity for all datasets shown, and compares favorably against ROAR [1] in cost of recourses\n+ Unlike ROAR [1], DiRRAc doesn’t have to specify the set of possible model parameter perturbations (\\Delta), but it has its own parameters to tune/specify. I am not sure if ROAR could have achieved similar M2 validity results with different \\Delta, but figure 3 seems to show DiRRAc consistently achieves lower cost recourses\n\n- DiRRAc is not the best performing method (in M2 validity) for non linear models, which are usually the ones deployed, since it relies on the local linear approximation \n- How can the initial distribution parameters be computed for classifiers which already exist? \n- How is \\rho_k determined? Ideally, we want to have a larger ball if we expect the model to change significantly. Is there a guideline for this?\n- How costly is the projection step? A study of time taken to generate recourses vs ROAR is necessary to determine practicality\n\nOther points\n* cite the source for Definition 3.1\n* possible typo on page 5, last paragraph, should it be f_k(x) <1 instead of f_k(x)\\leq 0 in the reformulation?\n* Citations are missing for the 3 datasets used\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is written clearly and technical contributions are described thoroughly. Code is provided for reproducibility. Some implementation details are not described.\n\n# Summary Of The Review\n\nThe paper applies distributionally robust optimization techniques to generate robust recourses in a technically sound way. Their way of quantifying model uncertainty, by using ambiguity sets, gets rid of having to specify an explicit set of possible future model parameters as in ROAR. Experimental results are convincing for linear models. DiRRAc may see limited success for highly non-linear models which can change in complicated ways.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents the **Distributionally Robust Recourse Action (DiRRAc)** framework, which addresses the challenge of generating recourse actions that remain valid under potential shifts in machine learning model parameters. The authors reformulate the problem of recourse generation as a finite-dimensional optimization problem and introduce a min-max optimization approach using Gelbrich distance to account for uncertainty in model parameters. Empirical evaluations on both synthetic and real-world datasets demonstrate that DiRRAc significantly outperforms existing methods in generating valid recourses while minimizing costs, highlighting its robustness against future model shifts.\n\n# Strength And Weaknesses\n**Strengths:**\n1. The introduction of a robust framework (DiRRAc) that incorporates parameter uncertainty is a significant advancement in the field of recourse generation.\n2. The authors provide a well-structured methodology, including a clear formulation of the optimization problem and the use of a Projected Gradient Descent Algorithm, which enhances the practical applicability of the proposed approach.\n3. Extensive empirical evaluations across diverse datasets strengthen the claims made regarding the effectiveness and robustness of the DiRRAc framework.\n\n**Weaknesses:**\n1. The paper primarily focuses on binary classification tasks, which may limit its immediate applicability to other types of machine learning problems.\n2. While the sensitivity analysis is a valuable addition, the paper could benefit from more extensive discussions on the implications of the ambiguity parameter and its selection in practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, making the complex concepts and methodologies accessible. The quality of the experimental results is high, and the authors provide sufficient detail to facilitate reproducibility, including an outline of the optimization algorithm and datasets used. On novelty, the integration of distributional robustness into recourse generation is a notable contribution, which adds to the originality of the work.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of machine learning by introducing a novel framework for generating robust recourse actions. The methodology is sound, and the empirical results validate the proposed approach effectively, although some aspects could be expanded for broader applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the Distributionally Robust Recourse Action (DiRRAc) framework, aimed at generating recourse actions that remain valid under shifts in model parameters due to data distribution changes. By formulating the recourse generation as a min-max optimization problem, where the robustness to parameter shifts is ensured through the use of Gelbrich distance over an ambiguity set, the authors provide a method to generate actionable modifications to instances that achieve favorable outcomes from machine learning models. Experimental results show that DiRRAc outperforms existing state-of-the-art methods in terms of validity under future distribution shifts while maintaining low costs, demonstrating its robustness across various datasets.\n\n# Strengths And Weaknesses\nThe DiRRAc framework's strengths include its robustness to parameter shifts, which is particularly relevant for real-world applications where such shifts are common. Additionally, the framework's flexibility in handling uncertainty and minimizing worst-case probabilities enhances its practical utility. The comprehensive evaluation using both synthetic and real-world datasets offers a well-rounded assessment of the method's capabilities. However, the framework relies on specific assumptions about model parameter distributions, which may limit its applicability in certain contexts. Furthermore, the computational complexity of the min-max optimization could pose challenges for scalability in high-dimensional spaces, and the actionability constraints may affect the practicality of the proposed recourse actions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation and methodology behind the DiRRAc framework. The experimental design is thorough and provides clear metrics for assessing the validity of the recourse actions. The novelty of the approach lies in its focus on robustness to distribution shifts, a critical factor in ensuring the reliability of machine learning models in dynamic environments. However, the reproducibility of results may be impacted due to the computational complexity of the proposed method, which could require careful tuning of hyperparameters.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of recourse generation by addressing the challenges posed by distribution shifts in model parameters. The DiRRAc framework demonstrates robustness and flexibility, making it a valuable contribution to the literature. However, attention should be paid to the assumptions and computational demands associated with its implementation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Distributionally Robust Recourse Action\" by Duy Nguyen et al. introduces the DiRRAc framework, which addresses the challenge of generating effective recourse actions in machine learning that remain valid despite data distribution shifts. The authors formulate the problem as a min-max optimization task utilizing Gelbrich distance to establish an ambiguity set around model parameters. They propose a projected gradient descent algorithm to solve this problem and demonstrate through extensive numerical experiments that DiRRAc outperforms existing state-of-the-art methods in terms of validity and cost efficiency across multiple real-world datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to handling distributional uncertainty in generating recourse actions, which is a critical aspect of machine learning applications in sensitive areas like loan approvals. The use of Gelbrich distance to create ambiguity sets provides a robust theoretical foundation. Additionally, the empirical results show significant improvements over baseline methods, validating the proposed framework's effectiveness. However, a potential weakness lies in the complexity of the proposed algorithms and their computational demands, which may limit practical applications in real-time systems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, theoretical foundations, and experimental results. The clarity of the explanations regarding the optimization formulations and the projected gradient descent algorithm is commendable. The novelty of the DiRRAc framework is significant, as it extends traditional recourse action methods by incorporating distributional robustness. The reproducibility of the results is enhanced by the detailed description of the experimental setup and the datasets used, although it would benefit from providing code or additional resources for implementation.\n\n# Summary Of The Review\nOverall, the DiRRAc framework represents a notable advancement in the generation of recourse actions within machine learning, effectively addressing the challenges posed by distributional shifts. The empirical validation demonstrates its superiority over existing methods, making it a valuable contribution to the field. However, the complexity of the algorithms may pose practical limitations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework named Distributionally Robust Recourse Action (DiRRAc), which aims to generate recourse actions that effectively address distribution shifts in machine learning models. The methodology employs advanced mathematical techniques, including min-max optimization, to ensure robustness against variations in model parameters and data distributions. Experimental results show that DiRRAc achieves high M2 validity across multiple datasets, outperforming existing state-of-the-art approaches while demonstrating flexibility across different model types.\n\n# Strength And Weaknesses\nThe DiRRAc framework is notable for its innovative approach to recourse generation, filling a critical gap in the literature regarding distribution shifts. However, the complexity of its implementation may hinder practical use, particularly for practitioners unfamiliar with optimization techniques. While the framework shows significant performance improvements on specific datasets, its generalizability to other contexts remains untested. The robustness against model misspecification is a strength, although it may result in overly conservative solutions that lack optimality. Furthermore, while the experimental evaluation is comprehensive, the inclusion of a broader range of baseline comparisons could further validate the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured with clear theoretical foundations, although the extensive mathematical details may pose challenges to readers without a strong background in the subject. The novelty of the approach is high, contributing significantly to the field of recourse actions in machine learning. However, reproducibility may be limited due to the complexity of the methods and the potential lack of accessibility to the underlying theory.\n\n# Summary Of The Review\nOverall, the paper introduces a promising and novel framework for generating distributionally robust recourse actions, demonstrating significant empirical performance. While the complexity and potential limitations in generalizability may hinder its practicality, the theoretical contributions and implications for high-stakes domains are noteworthy.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Distributionally Robust Recourse Action (DiRRAc) framework, which aims to enhance the feasibility of recourse actions in machine learning models facing distribution shifts. By employing a robust optimization approach, the framework generates recourse actions that maintain high validity probabilities even when model parameters fluctuate due to these shifts. Key contributions include the formulation of a min-max optimization problem, development of a projected gradient descent algorithm, and extensions to handle uncertainties in mixture weights. Empirical evaluations on both synthetic and real-world datasets demonstrate that DiRRAc significantly outperforms existing methods in terms of recourse validity and cost efficiency.\n\n# Strength And Weaknesses\n**Strengths:**\n- The DiRRAc framework presents an innovative approach to addressing a critical gap in the current methodologies for algorithmic recourse, particularly in the context of distribution shifts.\n- The comprehensive evaluation across multiple datasets showcases the practical advantages of the proposed framework, highlighting its applicability in real-world settings.\n- The introduction of an efficient algorithm that balances computational efficiency with robustness is noteworthy and provides a tangible benefit for practitioners.\n\n**Weaknesses:**\n- The complexity of the min-max formulation and the proposed algorithm may pose implementation challenges, potentially limiting its accessibility for broader adoption.\n- The reliance on specific forms of ambiguity sets for modeling uncertainty might restrict the framework's applicability in scenarios characterized by more complex or unstructured data distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear articulation of the problem, methodology, and findings. The novelty of the DiRRAc framework is significant, as it contributes to the evolving landscape of robust algorithmic recourse. However, the reproducibility of results may be hindered by the complexity of the proposed optimization approach and the assumptions regarding uncertainty sets, which could benefit from additional clarification or guidelines.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of algorithmic recourse through the innovative DiRRAc framework, which effectively addresses distributional shifts in machine learning models. While the contributions are substantial, the complexity of implementation and assumptions on uncertainty may pose challenges for practical adoption.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Distributionally Robust Adversarial Training\" by Duy Nguyen, Ngoc Bui, and Viet Anh Nguyen introduces a novel framework for adversarial training that addresses the limitations of traditional methods in handling distribution shifts in data. The authors propose the Distributionally Robust Adversarial Training (DiRAT) framework, which formulates adversarial training as a min-max optimization problem over an ambiguity set of potential distribution shifts, ensuring the generated adversarial examples remain robust even when model parameters change. Extensive experiments on both synthetic and real-world datasets validate the effectiveness of DiRAT, demonstrating significant improvements in robustness compared to existing adversarial training techniques.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to adversarial training by incorporating distributional robustness, which is crucial for practical applications where data distributions are not static. The theoretical foundation of the DiRAT framework is well-articulated, allowing readers to understand the robustness guarantees it offers. Comprehensive empirical results further support the claims made by the authors, showcasing the advantages of DiRAT across multiple scenarios. However, the paper would benefit from a deeper discussion on the computational complexity of the proposed methods in comparison to traditional adversarial training techniques. Additionally, exploring the impact of the ambiguity set on DiRAT's performance and its handling of extreme distribution shifts could enrich the paper's findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, making it accessible to readers. The quality of the theoretical formulation and the proposed algorithm is high, providing a solid foundation for reproducibility. The novelty of the DiRAT framework is significant, as it addresses a critical gap in the adversarial training literature by focusing on distributional robustness, which has been overlooked in prior work. Overall, the paper presents a compelling case for its methodology and findings.\n\n# Summary Of The Review\nThis paper makes substantial contributions to the field of adversarial machine learning by introducing the DiRAT framework, which effectively integrates distributional robustness into adversarial training. The clarity of the presentation, along with strong empirical validation, supports the relevance and significance of the work. Despite minor weaknesses regarding computational complexity and exploration of ambiguity sets, the paper provides valuable insights for enhancing model reliability in dynamic environments.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the DiRRAc framework, which aims to enhance recourse actions in machine learning by ensuring their validity in the face of data distribution shifts. The authors claim that traditional methods become obsolete when faced with such changes, and they propose a new optimization approach that utilizes min-max formulations. Their methodology includes finite-dimensional optimization techniques and extensions for handling uncertainty in mixture weights. However, the findings suggest that while DiRRAc shows some advantages over existing methods, the claims of superiority lack sufficient comparative context.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address the challenges posed by distributional changes in machine learning models. The DiRRAc framework and its optimization techniques are presented as significant contributions to the field. However, the weaknesses are notable: the claims of novelty are somewhat inflated, as many ideas presented are built on established optimization concepts. Furthermore, the experimental results lack thorough comparative analysis, making it difficult to assess the true impact of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and the methodology is detailed, but it does not sufficiently differentiate its contributions from existing literature. The clarity is somewhat undermined by the overstatement of the framework's novelty and significance. Reproducibility may be a concern, as the experiments lack robust statistical backing and comparative context, which could hinder the validation of the findings by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a framework that attempts to address a relevant issue in machine learning but lacks the novelty and empirical rigor needed to substantiate its claims. While the DiRRAc framework builds on important concepts, it does not offer the transformative advancements suggested by the authors.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces the Distributionally Robust Recourse Action (DiRRAc) framework, which addresses the limitations of existing recourse action methods in machine learning that assume static models. The authors propose a min-max optimization problem based on Gelbrich distance to ensure that recourse actions remain valid even in the presence of distribution shifts. Through experiments on three real-world datasets (German credit, SBA, and Student performance), the paper demonstrates that DiRRAc outperforms state-of-the-art methods in terms of validity and cost, making it a promising tool for ensuring robust recourse actions in practical applications.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel approach to recourse actions by accounting for distribution shifts, which is a significant step forward in the field of explainable AI. The empirical results show that DiRRAc consistently achieves higher validity metrics and lower costs compared to existing methods, indicating its practical applicability. However, a potential weakness lies in the limited exploration of the framework’s performance across a wider variety of datasets and settings, which could provide a more comprehensive understanding of its robustness and generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and findings. The introduction effectively sets the stage for the importance of robust recourse actions, while the methodology section provides sufficient detail on the optimization framework employed. The novelty of the approach is evident, particularly in its focus on distributional robustness. As for reproducibility, the authors provide detailed experimental setups and metrics, which should facilitate replication of their results, although additional details on hyperparameter tuning and implementation specifics could enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the domain of recourse actions in machine learning by introducing a robust approach that accounts for distribution shifts. DiRRAc shows significant improvements over existing methods in both validity and cost, although further validation across diverse datasets would bolster its claims.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper critiques the assumption of static model parameters in machine learning, highlighting the importance of addressing data distribution shifts that often occur in real-world applications. It proposes a robust optimization framework based on mixture models to account for these shifts, utilizing an ambiguity set defined by moment information and the Gelbrich distance. The findings suggest that while the proposed method enhances actionability and user trust through recourse actions, concerns about its generalizability, scalability, and the balance between robustness and actionable solutions remain.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its focus on the dynamic nature of machine learning models and the implications of distribution shifts, which is a timely and relevant topic. The use of mixture models is a notable contribution; however, the appropriateness of this choice for all types of shifts is questionable. Additionally, while the framework aims to provide interpretable recourse actions, the effectiveness of these actions in practice and their long-term validity under evolving models are not thoroughly addressed. The empirical evaluation, although based on real-world datasets, may not fully capture the diversity of potential applications or the biases present in those datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly; however, some sections could benefit from further elaboration, particularly around the implications of hyperparameter selection and the computational efficiency of the proposed methods. The novelty of the approach is significant, yet the exploration of its limitations and potential biases is insufficient. Reproducibility could be improved by providing more comprehensive guidelines for hyperparameter tuning and detailing the experimental setup to ensure that results can be reliably replicated.\n\n# Summary Of The Review\nOverall, the paper presents a compelling framework aimed at addressing the challenges posed by dynamic data distributions in machine learning. However, it falls short in fully addressing the generalizability of its methods, the implications of its assumptions, and the practical applicability of its proposed solutions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Distributionally Robust Recourse Action (DiRRAc) framework, aimed at generating recourse actions that remain effective despite shifts in machine learning model parameters. It formulates this challenge as a min-max optimization problem, integrating concepts from distributionally robust optimization. The authors validate the DiRRAc framework through extensive numerical experiments on real-world datasets, demonstrating its superior performance in generating valid and cost-effective recourse actions compared to existing methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the challenges posed by parameter shifts in machine learning models, with the DiRRAc framework showcasing robustness in recourse generation. The reformulation of the problem into a finite-dimensional optimization makes it more tractable for practical implementation. However, a potential weakness is the reliance on specific assumptions regarding the distribution of parameters, which may limit the generalizability of the findings. Additionally, while the numerical experiments are comprehensive, further exploration of edge cases and diverse datasets could enhance the robustness of the conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The quality of the writing is high, facilitating understanding of complex concepts and frameworks. The novelty of the DiRRAc framework and its implications for robust recourse actions are significant, addressing a notable gap in the literature. The reproducibility of the results is supported by detailed descriptions of the methodologies and datasets used, although providing access to the code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of machine learning by introducing a robust framework for generating recourse actions in the face of parameter uncertainty. The innovative approach, combined with solid empirical validation, makes it a relevant and significant addition to the literature, despite some limitations regarding assumptions and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at improving the interpretability and reliability of machine learning models, particularly in high-stakes applications. The authors introduce an innovative approach that combines elements from various machine learning paradigms to enhance decision-making processes. Through theoretical analysis and preliminary experiments, the findings suggest that the proposed method can outperform existing techniques in terms of robustness and clarity of model outputs.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Original Framework**: The integration of multiple machine learning concepts into a single framework is a significant contribution, providing new insights into model interpretability.\n2. **Theoretical Rigor**: The authors establish a solid theoretical foundation, demonstrating the method's effectiveness and relevance through rigorous proofs and discussions.\n3. **Comprehensive Literature Review**: The paper situates its contributions well within the existing body of research, highlighting gaps that the proposed method addresses.\n\n**Weaknesses:**\n1. **Insufficient Empirical Validation**: The experimental results presented are limited in scope and do not sufficiently validate the proposed approach across a diverse array of datasets.\n2. **Narrow Experimental Scope**: The range of scenarios tested is somewhat restricted, which raises concerns about the generalizability of the findings.\n3. **Potential Overfitting Risks**: There are indications that the method may overfit to the specific datasets used, potentially limiting its applicability in broader contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex ideas accessible to readers. The novelty of the approach is evident, as it contributes significantly to the field of machine learning. However, the reproducibility of the findings could be enhanced by providing additional details on the experimental setup and by including more comprehensive datasets for validation.\n\n# Summary Of The Review\nThis paper introduces a promising framework that advances the interpretability and reliability of machine learning models. While the theoretical contributions are strong, the empirical validation is lacking, which diminishes the overall impact. The work has potential but requires further experimentation to substantiate its claims fully.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the Distributionally Robust Recourse Action (DiRRAc) framework, which aims to generate recourse actions that remain valid despite shifts in data distribution and model parameters. The authors recognize that existing recourse methods often assume static models, leading to potentially invalid outcomes when conditions change. DiRRAc formulates the problem as a min-max optimization task and utilizes the Gelbrich distance to incorporate uncertainties in model parameters. The framework demonstrates superior performance in numerical experiments compared to existing state-of-the-art methods, offering a significant advancement in the field of algorithmic recourse.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to addressing the limitations of current recourse methods by incorporating distributional robustness. The formulation as a min-max optimization problem is a novel contribution that allows for adaptability in the face of data shifts. The empirical results show that DiRRAc outperforms existing methods, highlighting its practical applicability. However, a potential weakness is the complexity of the proposed method, which may limit its usability in real-world scenarios where interpretability and computational efficiency are essential. Additionally, the paper could benefit from a more detailed discussion on the limitations of the framework and potential challenges in implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodology, and findings. The quality of the writing is high, with appropriate use of technical language and clear explanations of concepts. The novelty of the approach is significant, as it addresses a critical gap in the existing literature on algorithmic recourse. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup, including datasets, hyperparameters, and implementation details.\n\n# Summary Of The Review\nOverall, this paper makes a meaningful contribution to the field of algorithmic recourse by introducing the DiRRAc framework, which enhances the robustness of recourse actions against data distribution shifts. While the methodology is innovative and the empirical results are promising, some aspects of the implementation and usability warrant further consideration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Distributionally Robust Recourse Action (DiRRAc) framework, which addresses the generation of robust recourse actions in machine learning, particularly in the face of distribution shifts. The authors reformulate the recourse generation as a min-max optimization problem, adapting to model shifts and ensuring feasibility. The methodology includes a projected gradient descent algorithm for solving the optimization problem, and the framework demonstrates strong performance in numerical experiments, consistently outperforming existing methods such as ROAR and CEPM in terms of validity and cost efficiency across various data distribution shifts.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating distributional robustness into recourse actions, which is particularly relevant for critical domains like finance and healthcare. The formulation of a min-max optimization problem is a strong theoretical contribution that provides a solid foundation for the proposed framework. Additionally, the empirical results clearly demonstrate the effectiveness of DiRRAc over state-of-the-art methods. However, a notable weakness is the limited exploration of uncertainty factors beyond mixture weight uncertainty, which could enhance the framework's applicability. Furthermore, while the algorithm is well-defined, further details on its convergence properties would strengthen the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex concepts accessible. The quality of the methodology is high, with a solid theoretical underpinning. The novelty of the DiRRAc framework stands out as it combines robust optimization principles with recourse generation, an area that has received limited attention in the literature. The experiments are reproducible, with appropriate metrics and datasets provided, although additional details on the experimental setup would be beneficial for full reproducibility.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the fields of explainable AI and robust optimization by introducing the DiRRAc framework for generating robust recourse actions in dynamic environments. While it excels in theoretical formulation and empirical validation, there is room for improvement in addressing additional uncertainty factors and providing more details on algorithm convergence.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Distributionally Robust Recourse Action\" introduces the DiRRAc framework, which aims to generate robust recourse actions that can withstand model shifts. The authors present a min-max optimization problem formulation utilizing the Gelbrich distance, and they empirically demonstrate the effectiveness of their approach through extensive testing on both synthetic and real-world datasets, showing significant performance improvements compared to existing methods.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative framework for addressing model shift uncertainties and the rigorous optimization reformulation that enhances its theoretical foundation. The comprehensive experimental setup, which includes various datasets and performance metrics, further strengthens the authors' claims. However, a notable weakness is the potential over-conservatism associated with robust optimization, which may limit the practicality of the recourse actions generated. Additionally, the paper could benefit from more extensive real-world applications to validate its findings beyond controlled scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its contributions and methodology. The theoretical proofs provided reinforce the framework's reliability, while the empirical results are presented with sufficient detail to allow for reproducibility. The novelty of the proposed DiRRAc framework, especially in the context of addressing uncertainties in parameter shifts, is a significant contribution to the field of algorithmic recourse.\n\n# Summary Of The Review\nOverall, the paper presents a novel and robust approach to generating recourse actions in the presence of model shifts, supported by solid theoretical foundations and comprehensive empirical validation. While the contributions are significant, further exploration of real-world applicability and potential limitations is warranted.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper presents the Distributionally Robust Recourse Action (DiRRAc) framework, which addresses the limitations of traditional recourse actions in machine learning models under data distribution shifts. The authors introduce a min-max optimization approach that leverages Gelbrich distance to ensure high probability feasibility of recourse actions despite parameter uncertainties. The methodology is backed by projected gradient descent algorithms and is empirically validated through experiments on both synthetic and real-world datasets, demonstrating superior performance compared to existing methods.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to handling uncertainty in model parameters, which is crucial for ensuring the reliability of recourse actions in dynamic environments. The use of Gelbrich distance and the min-max optimization framework provide a robust theoretical foundation that extends previous works in the field. However, the paper could further clarify the assumptions made regarding the Gaussian distributions of model parameters and the implications of these assumptions on the generalizability of the proposed framework. Additionally, while empirical results are promising, more extensive experiments across a variety of real-world scenarios would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, although some sections, particularly the mathematical formulations, may benefit from additional explanations for readers less familiar with the underlying concepts. The quality of the writing is high, and the figures and tables effectively support the findings. The novelty of the approach is significant, as it addresses a critical gap in the understanding of recourse actions under distribution shifts. However, reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and datasets used.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of robust machine learning by introducing the DiRRAc framework, which improves the feasibility of recourse actions in the presence of parameter uncertainties. While the methodology is innovative and well-founded, further clarification and more extensive empirical validation would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Distributionally Robust Recourse Action (DiRRAc) framework, aimed at providing recourse actions in uncertain environments by addressing the stochastic nature of model parameters. The methodology is built on a min-max optimization formulation and incorporates the Gelbrich distance for constructing ambiguity sets. The authors conduct numerical experiments on three real-world datasets, claiming that their framework outperforms existing methods in certain metrics.\n\n# Strength And Weaknesses\nWhile the proposed DiRRAc framework attempts to tackle the complexities of uncertain decision-making, it suffers from several weaknesses. The reliance on a complex min-max optimization problem could deter practical applications, as many users may prefer simpler solutions. Additionally, the assumption of stochastic changes in model parameters is questionable, as it does not account for retraining under consistent conditions. The limited scope of the numerical experiments raises concerns about the generalizability of the findings, and the choice of datasets may not adequately reflect the diversity of real-world challenges. The use of Gelbrich distance for ambiguity sets may not fully capture relevant uncertainties, and the computational efficiency of the method is not convincingly demonstrated, with run times that may be prohibitive for real-time use.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by its complexity and the superficial treatment of critical discussions, such as the trade-off between cost and validity. The contributions appear incremental rather than groundbreaking, lacking substantial advancements over existing literature on recourse actions. Furthermore, the empirical results do not provide strong evidence for the superiority of the DiRRAc framework, with improvements being marginal at best. The overall reproducibility of the results may be hindered by the limited datasets used for experimentation.\n\n# Summary Of The Review\nIn summary, while the DiRRAc framework presents an interesting approach to handling uncertainty in recourse actions, its complexity and questionable assumptions limit its practical applicability. The contributions seem to be minor compared to existing literature, and the empirical results do not convincingly demonstrate its advantages.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents the Distributionally Robust Recourse Action (DiRRAc) framework, which innovatively addresses the generation of recourse actions that are resilient to shifts in machine learning models. The authors propose a min-max optimization approach that maximizes the probability of actionable recourse under varying model parameters, ensuring that users receive effective modifications to improve their outcomes. Empirical evaluations demonstrate that DiRRAc outperforms existing state-of-the-art methods across multiple real-world datasets, enhancing interpretability and user engagement while addressing the practical constraints of immutable features.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative framework for generating robust recourse actions, which significantly enhances the reliability of algorithmic decisions. The application of min-max optimization showcases advanced mathematical techniques, providing a solid theoretical foundation. Additionally, the versatility of DiRRAc across different model types and its real-world applicability through actionable constraints are notable advantages. However, the paper could benefit from a more detailed discussion on the limitations of the framework and potential scenarios where it might fall short, as well as a deeper exploration of computational efficiency under varying conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to both practitioners and researchers. The quality of the experiments is high, with thorough evaluations against state-of-the-art methods that substantiate the claims made. The novelty of the DiRRAc framework is significant, as it advances the field of algorithmic recourse generation. However, while the proposed methods are described with clarity, additional details on experimental setups and hyperparameter tuning would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the Distributionally Robust Recourse Action framework is a significant contribution to the field of explainable AI, offering innovative solutions to enhance the reliability and interpretability of algorithmic decision-making. Its empirical superiority and potential for practical applications underscore its value, though further exploration of its limitations and reproducibility aspects would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the Distributionally Robust Recourse Action (DiRRAc) framework, which enhances the generation of recourse actions in machine learning by incorporating distributionally robust optimization (DRO) principles. The methodology includes defining an ambiguity set to account for potential distribution shifts, framing the problem as a min-max optimization challenge, and employing the Gelbrich distance as a metric for distribution proximity. Key findings suggest that the DiRRAc framework enables feasible and actionable recourse actions that remain robust under future uncertainties, with a focus on cost minimization and handling mixture weight uncertainties.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to modeling dynamic environments where data distributions may shift, thereby addressing a significant limitation of traditional recourse generation methods. The introduction of the Gelbrich distance as a distance metric is an important contribution that provides a robust theoretical basis for the framework. However, the paper could benefit from a more detailed empirical evaluation of the proposed methods. Specifically, while the theoretical framework is well-constructed, practical examples demonstrating the effectiveness of DiRRAc in real-world scenarios are lacking, which may limit the applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear and logical progression of ideas. The quality of the theoretical contributions is high, and the novel application of DRO principles to recourse actions in machine learning is significant. However, the reproducibility of the results may be hindered by the absence of empirical validation and implementation details. A more comprehensive exploration of the algorithm's performance in practical settings would enhance the overall clarity and impact of the findings.\n\n# Summary Of The Review\nOverall, the DiRRAc framework presents a compelling theoretical advancement in the field of explainable AI and robust decision-making under uncertainty. While the paper successfully addresses key challenges in generating recourse actions, the lack of empirical validation underscores a need for further research to demonstrate the practical utility of the proposed methods.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Distributionally Robust Recourse Action (DiRRAc)\" presents a framework aimed at optimizing recourse actions in the face of distributional shifts in data. The authors formulate the problem as a min-max optimization and leverage a projected gradient descent algorithm to derive robust recourse actions. Their empirical evaluations show that DiRRAc outperforms several baseline methods like ROAR, AR, and CEPM across multiple real-world and synthetic datasets, highlighting its effectiveness in providing robust solutions under model uncertainties.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its well-defined problem formulation and the thorough experimental evaluation on diverse datasets, which demonstrates the framework's applicability and robustness. The inclusion of a detailed pseudocode and convergence guarantee enhances the paper's methodological rigor. However, the paper could be improved by providing additional insights into the theoretical underpinnings of the proposed method and a more comprehensive discussion on the limitations of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, making it easy to follow the authors' reasoning and methodology. The quality of the experiments is high, with clear reporting of metrics and parameter settings. The novelty of the proposed approach is notable, particularly in its consideration of distributional robustness in recourse actions. The availability of source code on GitHub further supports reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of robust decision-making under uncertainty with a clear methodology and strong empirical results. While it excels in many areas, a deeper exploration of theoretical aspects and limitations could enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the DiRRAc framework, which aims to enhance existing recourse methods by introducing a distributionally robust optimization approach. The authors claim that DiRRAc outperforms established techniques, such as ROAR and Ustun et al. (2019), particularly in handling actionable constraints and generalizing to nonlinear models through LIME. The methodology includes a projected gradient descent algorithm designed to optimize decision-making under uncertainty, although specifics on computational efficiency and robustness remain underexplored.\n\n# Strength And Weaknesses\nWhile the DiRRAc framework proposes advancements in recourse methods, its contributions are undermined by several weaknesses. The comparison with existing methods lacks sufficient rigor, failing to convincingly demonstrate significant improvements. Assumptions about dynamic versus static model parameters are not adequately justified, raising concerns about the practical applicability of the proposed method. Additionally, the paper does not effectively address potential trade-offs between robustness and flexibility, and the sensitivity analysis appears less comprehensive compared to other frameworks. The lack of extensive field testing and limited acknowledgment of limitations further detracts from the overall strength of the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by the insufficient justification of its claims and the failure to thoroughly compare its methods against existing literature. The quality of the experiments is questionable due to biased performance metrics that favor DiRRAc without a balanced analysis. While the novelty of the framework is positioned as a significant contribution, the lack of a detailed exploration of its uniqueness compared to established methods diminishes its credibility. Reproducibility is also a concern as the paper does not provide comprehensive algorithmic details or convergence comparisons to support claims of improved performance.\n\n# Summary Of The Review\nOverall, the DiRRAc framework presents an interesting approach to recourse methods; however, its contributions are not sufficiently substantiated by rigorous comparisons and analyses. The paper lacks clarity in its claims and fails to convincingly demonstrate the practical advantages of its methodology over existing frameworks. \n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper proposes a distributionally robust recourse action (DiRRAc) that aims to provide actionable solutions to individuals affected by machine learning model decisions. The authors develop a min–max optimization framework that incorporates the Gelbrich Distance to measure the robustness of recourse actions under data uncertainty. Experimental results demonstrate that the proposed method outperforms existing approaches in terms of both actionability and robustness, highlighting its potential utility in real-world applications.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to addressing the challenges of providing recourse actions in a distributionally robust manner. The use of the Gelbrich Distance is a novel contribution that enhances the quality of recourse actions by ensuring they remain valid under data perturbations. However, the paper could improve in terms of clarity and consistency, particularly in mathematical notation and terminology. The presentation of results could also benefit from a more systematic approach to reporting numerical outcomes.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its contributions with a reasonable level of clarity; however, there are several areas where terminology and mathematical expressions could be standardized for better understanding. The novelty of the approach is commendable, particularly with the introduction of DiRRAc and its integration with Gelbrich Distance. Reproducibility may be hindered by inconsistencies in equation formatting and the need for clearer definitions of specific terms and algorithms used throughout the paper.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of robust recourse actions in machine learning, with a solid methodological foundation and promising empirical results. However, issues with clarity, consistency, and reproducibility need to be addressed to enhance the paper’s overall quality and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the DiRRAc framework aimed at improving recourse actions under mixture shifts in machine learning models. The authors propose a methodology that leverages projected gradient descent for optimizing recourse actions while ensuring robustness against certain uncertainties. However, the exploration of extreme shifts in model parameters and the implications of adversarial attacks on the input data is limited. The findings suggest that while the framework is promising, there are significant areas for improvement, especially in expanding its applicability to real-world scenarios and diverse datasets.\n\n# Strength And Weaknesses\nThe DiRRAc framework offers valuable insights into recourse actions under specific conditions, addressing mixture weight uncertainty and providing a structured approach to optimization. However, it has notable weaknesses, including a lack of depth in exploring extreme shifts, adversarial robustness, and the incorporation of user-specific constraints. The limited range of datasets used in experiments raises concerns about generalizability, and the paper does not sufficiently address the ethical implications of biased recourse actions across different demographic groups. Overall, the strengths of the framework are undermined by these weaknesses.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulated, making the core ideas accessible. However, the novelty of the proposed framework is somewhat limited by its narrow focus, and the lack of thorough exploration of alternative optimization methods may impact its perceived quality. Reproducibility could also be a concern, as the experiments are not validated across a broader set of datasets, and the implications of using non-Gaussian distributions are not adequately discussed.\n\n# Summary Of The Review\nThe DiRRAc framework presents an innovative approach to recourse actions under mixture shifts but falls short in several key areas, including robustness, generalizability, and ethical considerations. While the contributions are noteworthy, the limitations significantly affect the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the **Distributionally Robust Recourse Action (DiRRAc)** framework, which aims to improve the robustness of recourse actions in machine learning against changing data distributions. The main contributions include the development of a min-max optimization framework that maximizes the probability of valid recourse actions, a reformulation of the optimization problem to compute robust recourses, and extensions that address uncertainties in mixture weights and model parameter misspecifications. Empirical validation is performed using both synthetic and real-world datasets, demonstrating that DiRRAc outperforms existing methods like ROAR in maintaining recourse validity while incurring lower costs.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to incorporating statistical methodologies into the design of robust recourse actions, which is crucial for real-world applications that face data distribution shifts. The use of a well-defined optimization framework and robust performance metrics (M1 and M2 validity) enhances the reliability of the findings. However, the paper could benefit from a deeper exploration of the implications of different ambiguity sets and their impacts on performance, which may limit the generalizability of the results in diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly, with a logical flow from problem formulation to empirical results. The quality of the methodology is high, supported by rigorous statistical testing. The novelty lies in the integration of min-max optimization within the context of recourse actions, which is a relatively underexplored area. Reproducibility is facilitated by detailed descriptions of the experimental setup, including multiple runs for statistical significance, although additional details on implementation would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of robust machine learning by proposing a novel framework for recourse actions that effectively addresses uncertainties in data distribution. The empirical results are promising, but further exploration of ambiguity sets could enhance the framework's applicability.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThe paper presents the DiRRAc framework, which aims to generate recourse actions under the assumption of mixture shifts in model parameters. The methodology employs a min-max optimization problem to formulate the recourse generation process. The findings suggest that the proposed framework can effectively generate actionable recourse; however, the authors acknowledge limitations in handling more complex, nonlinear models, and the extensions for such models are not thoroughly explored or empirically validated.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to recourse generation through the use of a structured optimization problem, which lends itself to actionable insights for decision-making. However, the paper has notable weaknesses, including a lack of comprehensive analysis regarding the trade-offs between actionability and feasibility in various application domains. Additionally, the framework's reliance on the Gelbrich distance for defining the ambiguity set raises concerns about the robustness of the results, which are not fully explored. The discussion on computational complexity is limited, creating uncertainty about the scalability of the proposed methods. Furthermore, the experimental evaluation is largely confined to specific datasets, which may hinder the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting its ideas clearly, though the limitations and assumptions could benefit from more thorough discussion. The novelty of the approach is apparent, but the empirical validation raises concerns regarding reproducibility, as the authors do not provide extensive evaluations across diverse datasets or real-world scenarios. The proposed extensions to the DiRRAc framework are not sufficiently developed, and the lack of a clear roadmap for future research leaves questions about how to address the identified limitations.\n\n# Summary Of The Review\nOverall, the DiRRAc framework presents a promising approach to recourse generation, yet it falls short in thoroughly addressing complex model scenarios and empirical validation. The limitations in scalability, generalizability, and the lack of a comprehensive analysis of trade-offs significantly impact its practical applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"Distributionally Robust Recourse Action\" by Duy Nguyen, Ngoc Bui, and Viet Anh Nguyen proposes a framework called DiRRAc aimed at enhancing the robustness of recourse actions in machine learning models. The authors argue that existing methods fail to account for model shifts and propose a min-max optimization formulation that utilizes Gelbrich distance to address this oversight. Through numerical experiments, they claim that their approach outperforms existing methods on various real-world datasets, asserting that DiRRAc provides more reliable and actionable recourse actions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address the limitations of static model assumptions in recourse actions, which is a relevant issue in the field. However, the contributions feel somewhat superficial, as many of the ideas presented are not novel and have been explored in prior research. The proposed framework, while technically sound, does not significantly advance the state of the art in terms of originality. Additionally, the experimental results, though valid, do not provide compelling evidence of groundbreaking advancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from clarity issues, as much of the language used is unnecessarily complex, which may obscure the essential contributions. The quality of the writing could be improved to make the methodologies and findings more accessible. In terms of novelty, while the paper attempts to introduce a new framework, the concepts discussed are largely rehashed from established literature, leading to a perception of low originality. Reproducibility is not explicitly addressed, which raises concerns about the ability of others to replicate the results without clear guidelines.\n\n# Summary Of The Review\nOverall, the paper presents a familiar problem in machine learning recourse actions but fails to offer significant novel insights or methodologies. While the proposed DiRRAc framework is technically sound, it does not break new ground and is overshadowed by its reliance on established concepts. The writing could benefit from greater clarity and focus on substantive contributions.\n\n# Correctness\n4/5 - The methodologies and results presented in the paper are technically correct and adhere to established practices in the field.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions, while valid, do not offer significant new insights and largely reflect previous work in the area of recourse actions.\n\n# Empirical Novelty And Significance\n2/5 - The empirical findings, although statistically valid, do not present significant advancements compared to existing methodologies and largely reiterate known results within the literature.",
    "# Summary Of The Paper\nThe paper presents the DiRRAc framework, which addresses limitations in existing recourse methods by utilizing a robust optimization approach that accounts for data distribution shifts. It reformulates the optimization problem into a finite-dimensional format, allowing for more effective computation. The authors demonstrate that DiRRAc outperforms state-of-the-art methods in terms of recourse validity while minimizing associated costs through numerical experiments. Additionally, the framework includes innovative extensions such as handling the uncertainty in mixture weights and adapting to nonlinear models using LIME for better interpretability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing static assumptions in recourse generation methods, offering a significant improvement in handling dynamic model shifts. The introduction of Gelbrich distance for constructing ambiguity sets is particularly notable, providing a new avenue for robust machine learning applications. However, the paper could benefit from a more detailed exploration of how DiRRAc can be applied in online learning settings and the implications of its findings for other robust optimization scenarios. Additionally, while the projections of gradient descent are practical, further discussion on the computational efficiency and scalability of the proposed algorithm would strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly structured, making the methodology and findings accessible to readers. The quality of the experimental results is high, demonstrating the efficacy of the DiRRAc framework. In terms of novelty, the integration of a robust optimization approach with data distribution shifts represents a significant advancement over existing methods. The reproducibility of results, however, would be enhanced by providing more comprehensive details on the experimental setup and the specific datasets used for validation.\n\n# Summary Of The Review\nOverall, the DiRRAc framework introduces a compelling approach to generating algorithmic recourse that is robust against data distribution shifts. Its contributions to the field of machine learning are significant, particularly in enhancing the actionability and interpretability of recourse actions. Future work could further explore the integration of this framework with other techniques and its applicability in diverse settings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Distributionally Robust Recourse Action (DiRRAc) framework, designed to improve the generation of actionable recourse in the face of model shifts. The authors evaluate DiRRAc against four strong baselines—ROAR, CEPM, AR, and Wachter—using three real-world datasets: German, SBA, and Student. The findings indicate that DiRRAc consistently achieves high M2 validity while maintaining low costs, outperforming the baselines significantly in various metrics, demonstrating robustness across different model types and efficiency in runtime.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its comprehensive empirical evaluation, showcasing DiRRAc's superior performance in M2 validity and cost-effectiveness across multiple datasets. The adaptability of DiRRAc to non-linear models, as demonstrated through the LIME adaptation, adds to its robustness. However, a potential weakness is the lack of detailed exploration into the theoretical underpinnings of the framework, which could provide greater insights into its mechanisms and limitations. Additionally, while the empirical results are impressive, further exploration of edge cases or scenarios involving extreme distributions could enhance the robustness of the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow that allows readers to follow the methodology and findings easily. The quality of the experiments is high, with appropriate baselines and metrics used for evaluation. The novelty is evident in the introduction of DiRRAc as a robust framework for actionable recourse, though the paper could benefit from more detailed explanations of the methodologies employed. Reproducibility appears to be supported through the thorough presentation of results and the description of experimental setups, but sharing code or datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of actionable recourse with its DiRRAc framework, demonstrating both effectiveness and efficiency compared to existing methods. The empirical results are compelling, although further theoretical insights and reproducibility enhancements could strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"DISTRIBUTIONALLY ROBUST RECOURSE ACTION\" presents a framework for robust decision-making in uncertain environments, particularly in the context of recourse actions following decisions such as loan approvals. The authors propose a methodology that integrates distributional robustness into the decision-making process, allowing for more reliable outcomes despite variations in underlying data distributions. The findings indicate that their approach outperforms traditional methods in terms of both efficacy and reliability, providing a more resilient framework for decision-making.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of distributional robustness into decision-making frameworks, which is an important consideration in many applications, such as finance and healthcare. The methodology is well-grounded in theoretical principles, and the empirical results demonstrate the effectiveness of the proposed approach. However, the paper suffers from several weaknesses, including a lack of clarity in terminology and overly complex language, which may hinder accessibility for readers. Additionally, examples provided are not as direct as they could be, potentially diminishing their impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nClarity is a significant issue in this paper. The abstract is dense, and the introduction is lengthy, making it difficult for readers to grasp the core contributions quickly. Furthermore, inconsistent terminology and complex sentence structures detract from the overall quality. The novelty of the approach is notable, introducing a new perspective on decision-making under uncertainty, but the paper's reproducibility could be enhanced with clearer explanations of the methodology and better contextualization of figures and tables.\n\n# Summary Of The Review\nOverall, while the paper presents a novel approach to robust decision-making with significant implications, its clarity and accessibility are lacking. Improvements in structure and language could greatly enhance the reader's understanding and engagement with the material.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.525202599794973,
    -1.6105907215238209,
    -1.6935118573544048,
    -1.6491401821871634,
    -1.9362924449953618,
    -1.666479645080039,
    -1.512452121841627,
    -2.052221892982998,
    -1.5494057606992926,
    -1.903201201972675,
    -1.4227892676946374,
    -1.4688536666252816,
    -1.4072003999002693,
    -1.750506468411004,
    -1.8146271863881622,
    -1.524890071921211,
    -1.8477706763394703,
    -1.6886167666889331,
    -1.7408170315030083,
    -1.718900022412211,
    -2.1984799515952758,
    -1.6686007709226025,
    -2.0398305777592673,
    -1.8117256766602077,
    -2.024569258043946,
    -1.827528447866128,
    -1.9491265322059683,
    -1.7646767067259619,
    -1.6708782408541023
  ],
  "logp_cond": [
    [
      0.0,
      -2.2218696933554805,
      -2.266729480918347,
      -2.2368272699441385,
      -2.2771799204265304,
      -2.2555916877448747,
      -2.3563949377227624,
      -2.32631412205362,
      -2.2567806580354595,
      -2.33837011894038,
      -2.2870331299464133,
      -2.3878853730254264,
      -2.2730685503267343,
      -2.263860413719106,
      -2.2650251201789375,
      -2.28062018474144,
      -2.273092801046578,
      -2.2943461369328846,
      -2.27943719929528,
      -2.270392355167891,
      -2.266605306857688,
      -2.2791579173119896,
      -2.2963062965997,
      -2.2514026976274333,
      -2.272856228841305,
      -2.256231397573641,
      -2.249464192083086,
      -2.2474490531749787,
      -2.351698925362198
    ],
    [
      -1.205449728797491,
      0.0,
      -1.097322352665828,
      -1.1646361409491763,
      -1.2230273657338653,
      -1.1481668307288506,
      -1.3219630645537743,
      -1.2237908958559536,
      -1.1808615478977278,
      -1.2961424451974282,
      -1.1417247505044834,
      -1.3275473930884794,
      -1.1803537583962453,
      -1.1778385467393575,
      -1.1360415181473045,
      -1.1417854935177214,
      -1.1928317151811725,
      -1.1953743186483876,
      -1.1817890004440765,
      -1.2403492534030023,
      -1.252660264303252,
      -1.2279040202828093,
      -1.2595093184466872,
      -1.1707081826004677,
      -1.1910330383844756,
      -1.1986421230037725,
      -1.1547306891108422,
      -1.2516868246781165,
      -1.278139914524531
    ],
    [
      -1.3282681974753325,
      -1.223338689200933,
      0.0,
      -1.2993163684938407,
      -1.3169119997846508,
      -1.2807285955637844,
      -1.4220915725913568,
      -1.3382075287569555,
      -1.3151741033788347,
      -1.390188771342289,
      -1.310101145151167,
      -1.4465688391925424,
      -1.3022609390621471,
      -1.3370477912926733,
      -1.251375807064571,
      -1.3149893117962526,
      -1.2577330581982291,
      -1.3242405691405146,
      -1.3260350743429972,
      -1.3568045589562845,
      -1.3730224983846584,
      -1.373131180057205,
      -1.3592543270813868,
      -1.3188707106298032,
      -1.3029967150747952,
      -1.346903194676695,
      -1.3314361010763676,
      -1.3322807681771653,
      -1.4028332430516612
    ],
    [
      -1.2013850357056717,
      -1.1151702101845682,
      -1.1452007910638733,
      0.0,
      -1.1896834227398259,
      -1.1462740235512554,
      -1.2322139867471482,
      -1.1569430924445296,
      -1.1197581986093161,
      -1.2452689841080433,
      -1.141578314939763,
      -1.3592891352704035,
      -1.1183439831881512,
      -1.1185538244587323,
      -1.1388688539357745,
      -1.1191579875271307,
      -1.1198256057294884,
      -1.1992940973389308,
      -1.115169677958211,
      -1.1920561121563973,
      -1.2154260614859893,
      -1.2019112410792336,
      -1.2370345802960745,
      -1.1453578988864397,
      -1.1722333339487632,
      -1.1439778192855918,
      -1.12802763278936,
      -1.2012940023980243,
      -1.2759551015731603
    ],
    [
      -1.495580623440956,
      -1.493955455934985,
      -1.4730179195394248,
      -1.4658162537416504,
      0.0,
      -1.4235307180743293,
      -1.5646830577925772,
      -1.5083298068904456,
      -1.486570472941829,
      -1.5832790928044151,
      -1.4785390930732742,
      -1.615605166428559,
      -1.492997209768161,
      -1.4573318008454024,
      -1.5096689894221669,
      -1.4775721420539103,
      -1.4776657687031642,
      -1.4620720363622237,
      -1.4956150999282398,
      -1.5063525698983624,
      -1.5075369214289427,
      -1.4955089087201097,
      -1.4953626402510052,
      -1.4667811293963364,
      -1.4863072544640654,
      -1.5253295751470006,
      -1.5225375965654795,
      -1.4058187136185192,
      -1.5403825774183062
    ],
    [
      -1.309044865552629,
      -1.226506872118413,
      -1.2563667241810483,
      -1.2330466480156779,
      -1.2459119679073563,
      0.0,
      -1.3455718794728624,
      -1.2486917815227128,
      -1.2612407413305464,
      -1.356447191248945,
      -1.2701866011295213,
      -1.4001826925112912,
      -1.2251222448692256,
      -1.2107376582444684,
      -1.216991092608071,
      -1.2634309368717924,
      -1.3103065829737777,
      -1.2559135935120855,
      -1.2172338535517424,
      -1.2615894008199828,
      -1.2777947561568908,
      -1.28005960088494,
      -1.2496395713136834,
      -1.1671669151097486,
      -1.2868336415074582,
      -1.2814653034616252,
      -1.2278107282081268,
      -1.2488670605122456,
      -1.3474490537362036
    ],
    [
      -1.2264402977348834,
      -1.18289754154178,
      -1.1986489520600512,
      -1.1459594601375651,
      -1.2297533392145243,
      -1.1835551429678137,
      0.0,
      -1.235692895699731,
      -1.2008653450845053,
      -1.246633173244947,
      -1.1931531768793904,
      -1.2577052991098223,
      -1.2166289980898028,
      -1.1517589417428042,
      -1.1541946815590745,
      -1.2011866004539857,
      -1.2268582400149337,
      -1.1913670921667932,
      -1.1855519920766537,
      -1.1994560801735386,
      -1.2163509871293778,
      -1.235674302505887,
      -1.2459143827810204,
      -1.169082747951302,
      -1.2186416071012585,
      -1.164358918267023,
      -1.2100893985242045,
      -1.205756558546049,
      -1.248185100922658
    ],
    [
      -1.6430758505916,
      -1.5268172124672756,
      -1.5118125173491443,
      -1.5288462058943015,
      -1.5511902421634616,
      -1.4277785807940975,
      -1.6965527753495935,
      0.0,
      -1.534389876244083,
      -1.6613095351988885,
      -1.5363334928439827,
      -1.7025158133232672,
      -1.546348982794614,
      -1.5089730301294966,
      -1.5039041423363104,
      -1.59403616289861,
      -1.5348068617895543,
      -1.5867999155792933,
      -1.5150301446633487,
      -1.5637901020267875,
      -1.633915577829285,
      -1.5485084668386142,
      -1.5437611500474067,
      -1.458232571786645,
      -1.5447530944605012,
      -1.5423284015458867,
      -1.492794233728636,
      -1.5916849018601227,
      -1.614490728084875
    ],
    [
      -1.177307094063318,
      -1.0817708306075038,
      -1.0759817606668498,
      -1.0505741600319598,
      -1.088462897104796,
      -1.0871849522811363,
      -1.2147755519996386,
      -1.11500083185303,
      0.0,
      -1.1684410984074423,
      -1.1160194020764698,
      -1.2779237146124816,
      -1.0617309612850483,
      -1.0808230304490176,
      -1.0882266160823804,
      -1.0982859884682112,
      -1.0905011272367806,
      -1.1286247517974966,
      -1.117813674086613,
      -1.1403402014361592,
      -1.1735107777383222,
      -1.1049345638612815,
      -1.1523200934135416,
      -1.0515877063366272,
      -1.119460108480804,
      -1.0713776746470836,
      -1.10379740241783,
      -1.0342144045358947,
      -1.2117179545814147
    ],
    [
      -1.5419833238033651,
      -1.5351518129504316,
      -1.5331553101798665,
      -1.5286167867577487,
      -1.5334233192776348,
      -1.5260913791062558,
      -1.6023985199117996,
      -1.5501348174468565,
      -1.5242958329220975,
      0.0,
      -1.580478455432692,
      -1.6448760792666068,
      -1.5034898322669015,
      -1.543990592488704,
      -1.561674925550711,
      -1.5354404270004278,
      -1.5029334623784119,
      -1.5336318689249946,
      -1.505214141376745,
      -1.589309719270667,
      -1.5073212206907292,
      -1.5074470060239138,
      -1.5201999477950823,
      -1.5489691663168002,
      -1.4875857188198127,
      -1.5380361425497755,
      -1.4906225422574073,
      -1.5660846992277762,
      -1.613555110243586
    ],
    [
      -1.066879446941165,
      -0.8950185922861414,
      -0.9888034297863726,
      -0.9527467809695545,
      -0.9840052490463277,
      -0.9538401869995228,
      -1.07389028307972,
      -0.9978017833607467,
      -0.968274885447592,
      -1.072315842348033,
      0.0,
      -1.1504094939209244,
      -0.9576653963941103,
      -0.946552088929798,
      -0.977098828282303,
      -0.9714502698583577,
      -0.9574365854931843,
      -1.0037042325312338,
      -0.984540527835941,
      -1.0112369653384699,
      -1.0167673429752673,
      -1.0232288892564703,
      -1.0092844969521337,
      -0.9707221148126982,
      -0.9955571988685546,
      -1.0169660960113147,
      -0.9741969636377643,
      -0.9726153328171916,
      -1.0741558015304906
    ],
    [
      -1.1965584793279809,
      -1.128576401578107,
      -1.1997626817863718,
      -1.1988748372782552,
      -1.1775492402468246,
      -1.1707447001937943,
      -1.230618160821978,
      -1.1334303532823875,
      -1.2024413134544314,
      -1.1539094539963763,
      -1.1950705058531137,
      0.0,
      -1.2365065509224467,
      -1.1973061348141338,
      -1.167172881308376,
      -1.1964072335128488,
      -1.1238565776381824,
      -1.2031097737903702,
      -1.1750026256146853,
      -1.2031733358456913,
      -1.1653016612859404,
      -1.1896173734602125,
      -1.1526095769210816,
      -1.1956080053426685,
      -1.1085324188779084,
      -1.1715464624134504,
      -1.1901429098525467,
      -1.2000785973769492,
      -1.1525644471636505
    ],
    [
      -1.0365654945388902,
      -0.9433921594293213,
      -0.9718372056457959,
      -0.9549735631578219,
      -1.0142075280917306,
      -0.9597568062814322,
      -1.1240499789651177,
      -1.0169808029953673,
      -0.9176127283932721,
      -1.0363493105284487,
      -0.995474264792037,
      -1.1377933734301764,
      0.0,
      -0.9516134409193874,
      -0.9700227286609148,
      -0.9368892123432686,
      -0.9837745799242431,
      -0.9940115660238865,
      -0.983832638846729,
      -1.009719746518685,
      -1.0207019194607703,
      -0.9850275757941618,
      -1.0513427715654944,
      -1.0020228749693765,
      -1.0147632863267158,
      -0.9677078590513383,
      -0.9283745785998875,
      -0.9872593640386941,
      -1.0741815807287725
    ],
    [
      -1.3898277801350252,
      -1.3015554716293956,
      -1.3373359911124,
      -1.300756707641135,
      -1.3518626272103804,
      -1.2611115990678774,
      -1.404822536674422,
      -1.3197663672381796,
      -1.3204021837566475,
      -1.4196786433751707,
      -1.3178181807440128,
      -1.4831655317045611,
      -1.282704304456694,
      0.0,
      -1.3562735619864328,
      -1.3204932125741906,
      -1.3654711820678533,
      -1.3320863696446235,
      -1.2475859213749647,
      -1.3138208076862186,
      -1.3029097999941193,
      -1.367975224706133,
      -1.3231332763934647,
      -1.2706747598901063,
      -1.358008549837319,
      -1.3873888650475867,
      -1.3343577489766474,
      -1.289487508397516,
      -1.402313905576516
    ],
    [
      -1.3931441337447052,
      -1.20824420861256,
      -1.2813571434648328,
      -1.3057182819267035,
      -1.3081564503745111,
      -1.3064399242550795,
      -1.4533212387629355,
      -1.3664420748653112,
      -1.3154052652684498,
      -1.4345715810709314,
      -1.2942360156276838,
      -1.4859199182733085,
      -1.228783575934999,
      -1.333256679373035,
      0.0,
      -1.2939945841952496,
      -1.2500927336988776,
      -1.351799080510142,
      -1.3305128771587964,
      -1.3683973467929373,
      -1.4196132064177178,
      -1.3573893922843427,
      -1.3750115621012182,
      -1.3471389604441868,
      -1.2740694506356303,
      -1.305052057467289,
      -1.365915608324728,
      -1.3957251132598885,
      -1.4334053409693785
    ],
    [
      -1.141791359497946,
      -1.0532356082355159,
      -1.068768548995061,
      -1.019570854213557,
      -1.085475982563622,
      -1.014041943853601,
      -1.1595321954338997,
      -1.1307479483012246,
      -1.072008606312978,
      -1.1525252271196749,
      -1.11234194972557,
      -1.2523275493657804,
      -1.0064634188228747,
      -1.0446932702070852,
      -1.045353832698451,
      0.0,
      -1.0862276449573909,
      -1.1173693384927827,
      -1.054457043982935,
      -1.1191842603551552,
      -1.1103146220391156,
      -1.0733638252478241,
      -1.1022666871143927,
      -1.1026843294861226,
      -1.0876533122657581,
      -1.0698239853114198,
      -1.0840326721880336,
      -1.1529854231563754,
      -1.1553248889679768
    ],
    [
      -1.5225876097351159,
      -1.4471106585571951,
      -1.4443899524246377,
      -1.444223675170254,
      -1.4841218522471311,
      -1.4679484943555203,
      -1.5891530121641957,
      -1.4720238092791191,
      -1.4788641828290863,
      -1.5616347639475472,
      -1.4661839661523783,
      -1.577685519386269,
      -1.4534528828587134,
      -1.481369315568184,
      -1.4268459075407722,
      -1.5034571797972476,
      0.0,
      -1.5165990715246176,
      -1.474546741299936,
      -1.5200518256400162,
      -1.5130285672516057,
      -1.5040256308221336,
      -1.5025349516728033,
      -1.4957734350658791,
      -1.4124057029223804,
      -1.4207009645787985,
      -1.4839218564688184,
      -1.5059270386664996,
      -1.5201219106194228
    ],
    [
      -1.4177097139113373,
      -1.293097685010947,
      -1.3192467172153435,
      -1.3298537007506306,
      -1.2526337582777014,
      -1.2850400789565437,
      -1.38135095656076,
      -1.351405505628837,
      -1.3160630792952654,
      -1.3607935140809186,
      -1.303477292451438,
      -1.4494970578834179,
      -1.2886341009081612,
      -1.276976281008333,
      -1.3135375410399648,
      -1.307936544121492,
      -1.3271506925533376,
      0.0,
      -1.29886102337826,
      -1.323105544990633,
      -1.270049907607545,
      -1.3529334025599076,
      -1.3635264601671448,
      -1.371066094328032,
      -1.3232198557129056,
      -1.350462355301438,
      -1.3127394208915806,
      -1.2502620870404806,
      -1.383142675743131
    ],
    [
      -1.3682857997202889,
      -1.3614282597637137,
      -1.3202796103563097,
      -1.325104315475885,
      -1.3412005700967529,
      -1.3284531910457704,
      -1.408505392767064,
      -1.2986964526353175,
      -1.2993075892830712,
      -1.3895275359615862,
      -1.3733494438239293,
      -1.4868697167039282,
      -1.3366656522442983,
      -1.3072891748794713,
      -1.3476236208399899,
      -1.332027890277595,
      -1.323210400081909,
      -1.3518019565768606,
      0.0,
      -1.3902329791165815,
      -1.3672051352875554,
      -1.3273789162392977,
      -1.339305492288989,
      -1.3219586429708325,
      -1.2979240856551064,
      -1.3541556680876754,
      -1.2933118819801708,
      -1.3652831715843614,
      -1.430759910624791
    ],
    [
      -1.3134249550104713,
      -1.2216476006921613,
      -1.261619416755646,
      -1.2342015190411515,
      -1.2303224556864185,
      -1.2327255990430923,
      -1.3486474880753394,
      -1.2935577028020595,
      -1.2367140193068122,
      -1.3483724824231982,
      -1.246233260867057,
      -1.4205768855364038,
      -1.2322711343145054,
      -1.1477679629453783,
      -1.2153794556344935,
      -1.2254486451054674,
      -1.2768247830671517,
      -1.2209888240027618,
      -1.2724416807537,
      0.0,
      -1.2261142193044516,
      -1.2398336960669967,
      -1.2357518782225834,
      -1.2130770263448858,
      -1.303970281668868,
      -1.2697616845800805,
      -1.3104148327553615,
      -1.1559308307368326,
      -1.3162189842251115
    ],
    [
      -1.7402753373710345,
      -1.7853233378812596,
      -1.783631302813468,
      -1.7825380482999367,
      -1.7835857788375975,
      -1.7612489055928306,
      -1.8881524815105557,
      -1.7535383208076838,
      -1.788765140512459,
      -1.827201114831719,
      -1.7955570328514654,
      -1.9027636732359636,
      -1.766910772217895,
      -1.7587138314417952,
      -1.7790358099722094,
      -1.767870691246375,
      -1.7237131277208453,
      -1.729257868709581,
      -1.781119631036074,
      -1.7403823198525217,
      0.0,
      -1.7702641678240216,
      -1.7465590653540912,
      -1.7811386091279964,
      -1.7728033336627638,
      -1.7909431171104602,
      -1.7072687181527244,
      -1.7472798900338633,
      -1.861343774152203
    ],
    [
      -1.2884225217173138,
      -1.2191125289457212,
      -1.2317314050952315,
      -1.212194544625507,
      -1.2335260728353925,
      -1.223214463144484,
      -1.318961332539767,
      -1.2449964110995402,
      -1.190516517128306,
      -1.2382642029555446,
      -1.2480128819940206,
      -1.3740870739555568,
      -1.183832422210727,
      -1.2446378944542777,
      -1.2103735283239896,
      -1.1754042593980984,
      -1.2047313698135875,
      -1.2316788605925115,
      -1.1687390202696184,
      -1.225454908609846,
      -1.2293857338030132,
      0.0,
      -1.2569571102404173,
      -1.2299389768338975,
      -1.2105005235920412,
      -1.2131461350329091,
      -1.2303002136555583,
      -1.2303037131603574,
      -1.2855312389839004
    ],
    [
      -1.6469613911619956,
      -1.6185280607834784,
      -1.6070245426002636,
      -1.5920768448862643,
      -1.5792792146389336,
      -1.5471341289915281,
      -1.7253758811487712,
      -1.5598751764469017,
      -1.6190361348704594,
      -1.6543239979955988,
      -1.613577384068149,
      -1.7183323170828624,
      -1.6451741711627785,
      -1.5444157446695332,
      -1.598280051948081,
      -1.5944021222967275,
      -1.595550881568462,
      -1.6070221663759225,
      -1.5903134773781096,
      -1.601379659555845,
      -1.5861265303486107,
      -1.6542737247771537,
      0.0,
      -1.60142280894028,
      -1.525075399424957,
      -1.6746187167572681,
      -1.5895633345436315,
      -1.6170838037888713,
      -1.6891181930228663
    ],
    [
      -1.368230501431141,
      -1.3542668326220137,
      -1.397260625346078,
      -1.3427007152262136,
      -1.3667756921254357,
      -1.321482292565394,
      -1.4730254963525407,
      -1.3630577022047157,
      -1.3323586392839346,
      -1.4652716386277342,
      -1.3923023094765234,
      -1.55695068571635,
      -1.3852255937270141,
      -1.2984938021435393,
      -1.4056289104489998,
      -1.357941657610207,
      -1.3474729967151329,
      -1.4367564691524985,
      -1.3139892183925608,
      -1.4162243310365772,
      -1.4264864169162497,
      -1.4031955475667308,
      -1.4229737166773082,
      0.0,
      -1.427309879488101,
      -1.4483228378224993,
      -1.3794552377193634,
      -1.3476792580964194,
      -1.4531255141822295
    ],
    [
      -1.5494690112884224,
      -1.5549218103191005,
      -1.4882317226236246,
      -1.547933251173453,
      -1.5830566178904915,
      -1.5693841212025197,
      -1.698286212248318,
      -1.5303632707018489,
      -1.5914046480664594,
      -1.5826927791299763,
      -1.6008583973276502,
      -1.703177287425953,
      -1.563316727046141,
      -1.5994074086907375,
      -1.5240437064334649,
      -1.5825533917377501,
      -1.431996349646528,
      -1.6007996380764282,
      -1.538782761422292,
      -1.637857131603751,
      -1.5868589328656613,
      -1.626613381329402,
      -1.4757413762555227,
      -1.6012925824820095,
      0.0,
      -1.5632856664581558,
      -1.5112696120869449,
      -1.6170865057351784,
      -1.6624251113316497
    ],
    [
      -1.4994645350789642,
      -1.4082714544576342,
      -1.4275260784665362,
      -1.4049102328552687,
      -1.4674078206625745,
      -1.4607358002711412,
      -1.471989085458963,
      -1.4591946381555356,
      -1.3984583652687537,
      -1.5126384853734272,
      -1.4346543777262886,
      -1.5597888969318754,
      -1.4099478799692204,
      -1.4552927226754122,
      -1.3655742281806154,
      -1.4427454788541036,
      -1.3963872830207333,
      -1.444909030738532,
      -1.4549680892849008,
      -1.4517636700214847,
      -1.4808210196360054,
      -1.433120365978965,
      -1.4818442207814093,
      -1.4723832323453714,
      -1.4306018656230122,
      0.0,
      -1.435371820405623,
      -1.4612681549027804,
      -1.4957406410724812
    ],
    [
      -1.524744336645571,
      -1.4829115184356865,
      -1.512942341443254,
      -1.4918042847738942,
      -1.5565236383671035,
      -1.4638814203503827,
      -1.6519009035257328,
      -1.5508523468855702,
      -1.5167551420774799,
      -1.5006501957782334,
      -1.5483454504343372,
      -1.7097717441699711,
      -1.4870977795101281,
      -1.474326875048042,
      -1.5476979929943437,
      -1.4986433102596632,
      -1.5302178486739082,
      -1.5137036047127224,
      -1.5100080725518297,
      -1.5878359129959985,
      -1.466514652895468,
      -1.5439469724649846,
      -1.5378958416934645,
      -1.5335122029417068,
      -1.470315859289537,
      -1.5331187672341202,
      0.0,
      -1.521821328856267,
      -1.6647020068083822
    ],
    [
      -1.3500942938602847,
      -1.375229241936468,
      -1.3797973482419847,
      -1.3371366360541972,
      -1.3005264321962822,
      -1.366132898760336,
      -1.439340709610927,
      -1.3756641016661817,
      -1.3031602135452842,
      -1.4115763600873028,
      -1.3388526595402777,
      -1.4835208873076091,
      -1.3338630626717976,
      -1.3036684025865073,
      -1.3532871469090757,
      -1.357087172252228,
      -1.3651000996747042,
      -1.3173619352069936,
      -1.3615135472422037,
      -1.3389993705102776,
      -1.3405306488889706,
      -1.3940819274116278,
      -1.3779007918664805,
      -1.321637354769976,
      -1.3854074989309828,
      -1.4041765533340953,
      -1.3491752151452232,
      0.0,
      -1.4246149746249241
    ],
    [
      -1.3454067370370715,
      -1.2929192588582172,
      -1.3187834913410648,
      -1.2906985995418045,
      -1.3212132524557691,
      -1.3254641576316248,
      -1.3244534106982573,
      -1.305583722889988,
      -1.337433480618821,
      -1.3863166078756783,
      -1.342096998707034,
      -1.3378102431891397,
      -1.318859155594233,
      -1.2992426895877986,
      -1.2678505966897413,
      -1.3399433771059233,
      -1.2586698104447556,
      -1.3481309213794308,
      -1.3416807761666312,
      -1.3163472810369994,
      -1.350091125049282,
      -1.3065424766760612,
      -1.357153545674591,
      -1.3276266481029526,
      -1.3203824520011984,
      -1.288544035818574,
      -1.3659488667932844,
      -1.3197631996405366,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.3033329064394925,
      0.25847311887662583,
      0.28837532985083447,
      0.24802267936844258,
      0.26961091205009824,
      0.16880766207221054,
      0.19888847774135288,
      0.2684219417595135,
      0.18683248085459292,
      0.2381694698485597,
      0.1373172267695466,
      0.2521340494682387,
      0.2613421860758671,
      0.26017747961603543,
      0.24458241505353318,
      0.2521097987483949,
      0.23085646286208839,
      0.24576540049969298,
      0.2548102446270821,
      0.25859729293728506,
      0.2460446824829834,
      0.2288963031952731,
      0.2737999021675397,
      0.25234637095366796,
      0.26897120222133175,
      0.27573840771188696,
      0.2777535466199943,
      0.17350367443277515
    ],
    [
      0.40514099272632986,
      0.0,
      0.5132683688579929,
      0.44595458057464454,
      0.3875633557899556,
      0.46242389079497026,
      0.28862765697004655,
      0.3867998256678673,
      0.4297291736260931,
      0.3144482763263927,
      0.46886597101933747,
      0.2830433284353415,
      0.43023696312757553,
      0.4327521747844634,
      0.4745492033765164,
      0.4688052280060995,
      0.4177590063426484,
      0.41521640287543327,
      0.42880172107974435,
      0.3702414681208186,
      0.3579304572205688,
      0.38268670124101156,
      0.35108140307713365,
      0.43988253892335316,
      0.41955768313934527,
      0.4119485985200484,
      0.4558600324129787,
      0.35890389684570434,
      0.33245080699928997
    ],
    [
      0.36524365987907226,
      0.47017316815347177,
      0.0,
      0.3941954888605641,
      0.376599857569754,
      0.4127832617906204,
      0.271420284763048,
      0.3553043285974493,
      0.3783377539755701,
      0.3033230860121159,
      0.38341071220323775,
      0.24694301816186237,
      0.39125091829225767,
      0.35646406606173153,
      0.4421360502898337,
      0.37852254555815223,
      0.4357787991561757,
      0.3692712882138902,
      0.3674767830114076,
      0.3367072983981203,
      0.3204893589697464,
      0.32038067729719977,
      0.33425753027301797,
      0.3746411467246016,
      0.3905151422796096,
      0.34660866267770984,
      0.36207575627803723,
      0.36123108917723945,
      0.29067861430274355
    ],
    [
      0.44775514648149173,
      0.5339699720025952,
      0.5039393911232901,
      0.0,
      0.45945675944733755,
      0.502866158635908,
      0.4169261954400152,
      0.49219708974263376,
      0.5293819835778473,
      0.4038711980791201,
      0.5075618672474005,
      0.2898510469167599,
      0.5307961989990122,
      0.5305863577284311,
      0.5102713282513889,
      0.5299821946600327,
      0.529314576457675,
      0.44984608484823263,
      0.5339705042289524,
      0.4570840700307661,
      0.4337141207011741,
      0.44722894110792977,
      0.4121056018910889,
      0.5037822833007237,
      0.47690684823840024,
      0.5051623629015716,
      0.5211125493978035,
      0.4478461797891391,
      0.37318508061400313
    ],
    [
      0.4407118215544057,
      0.4423369890603768,
      0.46327452545593695,
      0.47047619125371143,
      0.0,
      0.5127617269210325,
      0.3716093872027846,
      0.4279626381049162,
      0.4497219720535328,
      0.35301335219094665,
      0.45775335192208755,
      0.3206872785668029,
      0.4432952352272008,
      0.4789606441499594,
      0.4266234555731949,
      0.4587203029414515,
      0.45862667629219755,
      0.4742204086331381,
      0.440677345067122,
      0.4299398750969994,
      0.4287555235664191,
      0.44078353627525213,
      0.4409298047443566,
      0.46951131559902537,
      0.4499851905312964,
      0.41096286984836117,
      0.4137548484298823,
      0.5304737313768426,
      0.3959098675770556
    ],
    [
      0.35743477952741,
      0.43997277296162607,
      0.41011292089899065,
      0.4334329970643611,
      0.42056767717268273,
      0.0,
      0.3209077656071766,
      0.4177878635573262,
      0.40523890374949256,
      0.3100324538310939,
      0.3962930439505177,
      0.26629695256874775,
      0.4413574002108134,
      0.45574198683557055,
      0.44948855247196806,
      0.40304870820824656,
      0.3561730621062613,
      0.41056605156795345,
      0.4492457915282966,
      0.40489024426005615,
      0.38868488892314823,
      0.3864200441950989,
      0.41684007376635557,
      0.4993127299702904,
      0.3796460035725808,
      0.38501434161841375,
      0.43866891687191223,
      0.4176125845677934,
      0.3190305913438354
    ],
    [
      0.28601182410674353,
      0.32955458029984697,
      0.31380316978157574,
      0.3664926617040618,
      0.2826987826271026,
      0.3288969788738132,
      0.0,
      0.27675922614189585,
      0.31158677675712165,
      0.26581894859668,
      0.31929894496223654,
      0.2547468227318046,
      0.29582312375182407,
      0.3606931800988227,
      0.35825744028255246,
      0.3112655213876412,
      0.2855938818266932,
      0.32108502967483377,
      0.3269001297649732,
      0.31299604166808837,
      0.2961011347122491,
      0.27677781933573997,
      0.2665377390606065,
      0.34336937389032496,
      0.29381051474036846,
      0.3480932035746038,
      0.3023627233174224,
      0.3066955632955779,
      0.2642670209189688
    ],
    [
      0.40914604239139796,
      0.5254046805157224,
      0.5404093756338537,
      0.5233756870886965,
      0.5010316508195365,
      0.6244433121889006,
      0.3556691176334046,
      0.0,
      0.517832016738915,
      0.3909123577841096,
      0.5158884001390154,
      0.3497060796597309,
      0.5058729101883841,
      0.5432488628535015,
      0.5483177506466876,
      0.4581857300843881,
      0.5174150311934438,
      0.4654219774037047,
      0.5371917483196493,
      0.4884317909562106,
      0.41830631515371297,
      0.5037134261443839,
      0.5084607429355914,
      0.5939893211963532,
      0.5074687985224968,
      0.5098934914371114,
      0.5594276592543621,
      0.4605369911228754,
      0.437731164898123
    ],
    [
      0.3720986666359747,
      0.4676349300917888,
      0.4734240000324428,
      0.4988316006673328,
      0.46094286359449654,
      0.46222080841815627,
      0.334630208699654,
      0.4344049288462626,
      0.0,
      0.38096466229185033,
      0.4333863586228228,
      0.27148204608681104,
      0.4876747994142443,
      0.46858273025027497,
      0.46117914461691223,
      0.45111977223108135,
      0.458904633462512,
      0.42078100890179604,
      0.43159208661267967,
      0.40906555926313337,
      0.37589498296097035,
      0.44447119683801106,
      0.397085667285751,
      0.49781805436266535,
      0.42994565221848857,
      0.47802808605220903,
      0.4456083582814625,
      0.5151913561633978,
      0.33768780611787785
    ],
    [
      0.36121787816930984,
      0.3680493890222434,
      0.37004589179280845,
      0.37458441521492625,
      0.3697778826950402,
      0.37710982286641914,
      0.30080268206087535,
      0.3530663845258184,
      0.3789053690505775,
      0.0,
      0.3227227465399829,
      0.2583251227060681,
      0.3997113697057735,
      0.3592106094839709,
      0.341526276421964,
      0.36776077497224713,
      0.4002677395942631,
      0.36956933304768036,
      0.3979870605959299,
      0.31389148270200784,
      0.3958799812819458,
      0.39575419594876116,
      0.3830012541775927,
      0.3542320356558748,
      0.41561548315286223,
      0.36516505942289945,
      0.41257865971526764,
      0.3371165027448988,
      0.28964609172908884
    ],
    [
      0.35590982075347233,
      0.527770675408496,
      0.43398583790826484,
      0.470042486725083,
      0.43878401864830974,
      0.46894908069511465,
      0.34889898461491753,
      0.4249874843338908,
      0.45451438224704543,
      0.3504734253466044,
      0.0,
      0.27237977377371303,
      0.4651238713005271,
      0.4762371787648394,
      0.44569043941233444,
      0.45133899783627973,
      0.4653526822014531,
      0.4190850351634037,
      0.4382487398586964,
      0.4115523023561676,
      0.40602192471937015,
      0.39956037843816716,
      0.4135047707425037,
      0.4520671528819392,
      0.4272320688260828,
      0.4058231716833227,
      0.4485923040568731,
      0.45017393487744584,
      0.3486334661641468
    ],
    [
      0.27229518729730073,
      0.3402772650471746,
      0.2690909848389098,
      0.26997882934702644,
      0.291304426378457,
      0.2981089664314873,
      0.23823550580330366,
      0.3354233133428941,
      0.2664123531708502,
      0.31494421262890526,
      0.2737831607721679,
      0.0,
      0.23234711570283495,
      0.27154753181114777,
      0.3016807853169057,
      0.2724464331124328,
      0.34499708898709924,
      0.2657438928349114,
      0.29385104101059634,
      0.2656803307795903,
      0.30355200533934124,
      0.27923629316506915,
      0.31624408970420004,
      0.2732456612826131,
      0.3603212477473732,
      0.2973072042118312,
      0.27871075677273494,
      0.2687750692483324,
      0.31628921946163113
    ],
    [
      0.3706349053613791,
      0.46380824047094804,
      0.43536319425447345,
      0.45222683674244746,
      0.3929928718085387,
      0.4474435936188371,
      0.28315042093515164,
      0.39021959690490204,
      0.4895876715069972,
      0.37085108937182065,
      0.4117261351082323,
      0.26940702647009296,
      0.0,
      0.455586958980882,
      0.4371776712393546,
      0.47031118755700074,
      0.42342581997602624,
      0.41318883387638283,
      0.42336776105354035,
      0.39748065338158445,
      0.3864984804394991,
      0.42217282410610757,
      0.35585762833477497,
      0.4051775249308929,
      0.3924371135735536,
      0.4394925408489311,
      0.47882582130038187,
      0.4199410358615753,
      0.3330188191714969
    ],
    [
      0.36067868827597893,
      0.44895099678160855,
      0.413170477298604,
      0.44974976076986906,
      0.3986438412006237,
      0.4893948693431267,
      0.3456839317365821,
      0.43074010117282446,
      0.4301042846543566,
      0.3308278250358334,
      0.4326882876669913,
      0.267340936706443,
      0.4678021639543102,
      0.0,
      0.3942329064245713,
      0.4300132558368135,
      0.38503528634315076,
      0.41842009876638064,
      0.5029205470360394,
      0.43668566072478554,
      0.44759666841688484,
      0.3825312437048711,
      0.4273731920175394,
      0.47983170852089785,
      0.3924979185736852,
      0.36311760336341736,
      0.4161487194343567,
      0.4610189600134882,
      0.3481925628344882
    ],
    [
      0.421483052643457,
      0.6063829777756022,
      0.5332700429233295,
      0.5089089044614588,
      0.5064707360136511,
      0.5081872621330827,
      0.3613059476252267,
      0.448185111522851,
      0.4992219211197124,
      0.3800556053172308,
      0.5203911707604785,
      0.32870726811485373,
      0.5858436104531632,
      0.4813705070151273,
      0.0,
      0.5206326021929126,
      0.5645344526892846,
      0.46282810587802015,
      0.4841143092293658,
      0.4462298395952249,
      0.3950139799704444,
      0.4572377941038195,
      0.439615624286944,
      0.46748822594397543,
      0.540557735752532,
      0.5095751289208732,
      0.4487115780634343,
      0.41890207312827377,
      0.3812218454187837
    ],
    [
      0.383098712423265,
      0.4716544636856952,
      0.45612152292615016,
      0.505319217707654,
      0.4394140893575891,
      0.51084812806761,
      0.36535787648731133,
      0.39414212361998646,
      0.45288146560823317,
      0.3723648448015362,
      0.4125481221956411,
      0.2725625225554307,
      0.5184266530983364,
      0.4801968017141258,
      0.47953623922276,
      0.0,
      0.4386624269638202,
      0.4075207334284283,
      0.470433027938276,
      0.4057058115660559,
      0.4145754498820955,
      0.4515262466733869,
      0.4226233848068184,
      0.42220574243508846,
      0.43723675965545294,
      0.4550660866097913,
      0.4408573997331775,
      0.37190464876483564,
      0.36956518295323426
    ],
    [
      0.3251830666043545,
      0.4006600177822752,
      0.4033807239148326,
      0.4035470011692164,
      0.36364882409233923,
      0.37982218198395,
      0.25861766417527465,
      0.3757468670603512,
      0.36890649351038407,
      0.28613591239192315,
      0.381586710187092,
      0.27008515695320123,
      0.3943177934807569,
      0.3664013607712864,
      0.4209247687986981,
      0.34431349654222276,
      0.0,
      0.33117160481485275,
      0.3732239350395343,
      0.32771885069945417,
      0.3347421090878646,
      0.34374504551733676,
      0.3452357246666671,
      0.3519972412735912,
      0.4353649734170899,
      0.4270697117606719,
      0.363848819870652,
      0.34184363767297077,
      0.32764876572004753
    ],
    [
      0.27090705277759586,
      0.3955190816779861,
      0.36937004947358965,
      0.35876306593830254,
      0.4359830084112317,
      0.4035766877323894,
      0.3072658101281731,
      0.33721126106009613,
      0.3725536873936677,
      0.3278232526080145,
      0.38513947423749517,
      0.23911970880551525,
      0.3999826657807719,
      0.4116404856806002,
      0.37507922564896834,
      0.38068022256744105,
      0.3614660741355955,
      0.0,
      0.3897557433106731,
      0.3655112216983001,
      0.418566859081388,
      0.33568336412902555,
      0.3250903065217883,
      0.3175506723609012,
      0.36539691097602756,
      0.338154411387495,
      0.37587734579735255,
      0.43835467964845254,
      0.305474090945802
    ],
    [
      0.3725312317827194,
      0.37938877173929453,
      0.42053742114669856,
      0.41571271602712323,
      0.3996164614062554,
      0.41236384045723784,
      0.33231163873594416,
      0.44212057886769074,
      0.44150944221993704,
      0.3512894955414221,
      0.36746758767907894,
      0.25394731479908006,
      0.4041513792587099,
      0.433527856623537,
      0.3931934106630184,
      0.4087891412254132,
      0.41760663142109933,
      0.3890150749261476,
      0.0,
      0.35058405238642676,
      0.3736118962154529,
      0.41343811526371055,
      0.40151153921401916,
      0.4188583885321757,
      0.44289294584790184,
      0.38666136341533286,
      0.4475051495228375,
      0.37553385991864685,
      0.31005712087821724
    ],
    [
      0.4054750674017398,
      0.4972524217200498,
      0.45728060565656503,
      0.4846985033710596,
      0.48857756672579256,
      0.48617442336911876,
      0.37025253433687166,
      0.42534231961015156,
      0.48218600310539883,
      0.3705275399890129,
      0.4726667615451541,
      0.29832313687580725,
      0.48662888809770566,
      0.5711320594668328,
      0.5035205667777176,
      0.4934513773067437,
      0.4420752393450593,
      0.4979111984094493,
      0.44645834165851106,
      0.0,
      0.49278580310775943,
      0.4790663263452144,
      0.4831481441896277,
      0.5058229960673253,
      0.414929740743343,
      0.44913833783213053,
      0.40848518965684955,
      0.5629691916753785,
      0.40268103818709955
    ],
    [
      0.4582046142242413,
      0.41315661371401613,
      0.4148486487818077,
      0.4159419032953391,
      0.41489417275767826,
      0.4372310460024451,
      0.31032747008472006,
      0.44494163078759197,
      0.4097148110828168,
      0.37127883676355666,
      0.40292291874381037,
      0.29571627835931213,
      0.4315691793773808,
      0.4397661201534806,
      0.4194441416230663,
      0.4306092603489007,
      0.4747668238744305,
      0.4692220828856948,
      0.4173603205592018,
      0.4580976317427541,
      0.0,
      0.42821578377125413,
      0.45192088624118454,
      0.4173413424672794,
      0.425676617932512,
      0.4075368344848156,
      0.49121123344255135,
      0.45120006156141246,
      0.3371361774430728
    ],
    [
      0.38017824920528875,
      0.4494882419768813,
      0.436869365827371,
      0.4564062262970956,
      0.43507469808721,
      0.4453863077781186,
      0.3496394383828354,
      0.4236043598230623,
      0.4780842537942964,
      0.4303365679670579,
      0.4205878889285819,
      0.29451369696704566,
      0.48476834871187546,
      0.42396287646832476,
      0.4582272425986129,
      0.49319651152450406,
      0.463869401109015,
      0.436921910330091,
      0.4998617506529841,
      0.44314586231275643,
      0.4392150371195893,
      0.0,
      0.41164366068218516,
      0.438661794088705,
      0.4581002473305613,
      0.45545463588969337,
      0.4383005572670442,
      0.43829705776224515,
      0.3830695319387021
    ],
    [
      0.39286918659727177,
      0.4213025169757889,
      0.43280603515900373,
      0.447753732873003,
      0.46055136312033373,
      0.4926964487677392,
      0.3144546966104962,
      0.4799554013123657,
      0.42079444288880796,
      0.38550657976366853,
      0.42625319369111825,
      0.32149826067640497,
      0.3946564065964888,
      0.4954148330897341,
      0.44155052581118626,
      0.44542845546253984,
      0.4442796961908053,
      0.4328084113833448,
      0.4495171003811578,
      0.4384509182034224,
      0.4537040474106566,
      0.3855568529821136,
      0.0,
      0.43840776881898735,
      0.5147551783343103,
      0.3652118610019992,
      0.4502672432156358,
      0.42274677397039606,
      0.3507123847364011
    ],
    [
      0.44349517522906656,
      0.4574588440381939,
      0.4144650513141297,
      0.4690249614339941,
      0.44494998453477197,
      0.4902433840948137,
      0.338700180307667,
      0.448667974455492,
      0.47936703737627306,
      0.34645403803247343,
      0.4194233671836842,
      0.2547749909438577,
      0.4265000829331935,
      0.5132318745166684,
      0.4060967662112078,
      0.45378401905000065,
      0.4642526799450748,
      0.3749692075077091,
      0.4977364582676469,
      0.3955013456236305,
      0.385239259743958,
      0.40853012909347686,
      0.38875195998289946,
      0.0,
      0.3844157971721067,
      0.3634028388377084,
      0.43227043894084427,
      0.46404641856378825,
      0.3586001624779782
    ],
    [
      0.4751002467555234,
      0.46964744772484535,
      0.5363375354203213,
      0.4766360068704929,
      0.44151264015345437,
      0.45518513684142614,
      0.32628304579562784,
      0.494205987342097,
      0.4331646099774864,
      0.44187647891396953,
      0.4237108607162956,
      0.32139197061799285,
      0.46125253099780483,
      0.42516184935320833,
      0.500525551610481,
      0.4420158663061957,
      0.5925729083974178,
      0.4237696199675176,
      0.48578649662165385,
      0.3867121264401949,
      0.43771032517828456,
      0.3979558767145439,
      0.5488278817884231,
      0.4232766755619364,
      0.0,
      0.46128359158579,
      0.513299645957001,
      0.4074827523087674,
      0.3621441467122961
    ],
    [
      0.32806391278716385,
      0.41925699340849376,
      0.4000023693995918,
      0.4226182150108593,
      0.36012062720355353,
      0.36679264759498675,
      0.355539362407165,
      0.3683338097105924,
      0.42907008259737434,
      0.31488996249270085,
      0.39287407013983944,
      0.2677395509342526,
      0.41758056789690756,
      0.3722357251907158,
      0.46195421968551265,
      0.38478296901202436,
      0.43114116484539466,
      0.38261941712759606,
      0.37256035858122716,
      0.37576477784464335,
      0.34670742823012257,
      0.3944080818871629,
      0.3456842270847187,
      0.3551452155207566,
      0.39692658224311583,
      0.0,
      0.39215662746050506,
      0.3662602929633476,
      0.33178780679364683
    ],
    [
      0.42438219556039725,
      0.46621501377028185,
      0.4361841907627144,
      0.45732224743207417,
      0.3926028938388648,
      0.4852451118555856,
      0.2972256286802355,
      0.39827418532039816,
      0.43237139012848846,
      0.4484763364277349,
      0.4007810817716311,
      0.2393547880359972,
      0.4620287526958402,
      0.47479965715792627,
      0.4014285392116246,
      0.4504832219463051,
      0.4189086835320601,
      0.43542292749324596,
      0.43911845965413865,
      0.36129061920996985,
      0.4826118793105003,
      0.40517955974098374,
      0.4112306905125038,
      0.41561432926426156,
      0.4788106729164314,
      0.4160077649718481,
      0.0,
      0.42730520334970135,
      0.2844245253975861
    ],
    [
      0.4145824128656772,
      0.3894474647894939,
      0.38487935848397714,
      0.4275400706717647,
      0.4641502745296797,
      0.39854380796562583,
      0.3253359971150349,
      0.3890126050597802,
      0.46151649318067767,
      0.3531003466386591,
      0.42582404718568423,
      0.28115581941835277,
      0.43081364405416434,
      0.4610083041394546,
      0.4113895598168862,
      0.4075895344737339,
      0.39957660705125764,
      0.44731477151896826,
      0.40316315948375814,
      0.42567733621568427,
      0.42414605783699133,
      0.37059477931433404,
      0.3867759148594814,
      0.44303935195598587,
      0.3792692077949791,
      0.3605001533918666,
      0.4155014915807387,
      0.0,
      0.34006173210103774
    ],
    [
      0.3254715038170308,
      0.37795898199588507,
      0.35209474951303754,
      0.38017964131229776,
      0.34966498839833315,
      0.34541408322247746,
      0.346424830155845,
      0.3652945179641143,
      0.33344476023528125,
      0.28456163297842396,
      0.3287812421470684,
      0.33306799766496265,
      0.35201908525986925,
      0.37163555126630365,
      0.403027644164361,
      0.33093486374817904,
      0.41220843040934674,
      0.3227473194746715,
      0.32919746468747113,
      0.3545309598171029,
      0.32078711580482033,
      0.36433576417804114,
      0.3137246951795114,
      0.3432515927511497,
      0.3504957888529039,
      0.3823342050355283,
      0.3049293740608179,
      0.35111504121356574,
      0.0
    ]
  ],
  "row_avgs": [
    0.24370291518946216,
    0.40480463238863235,
    0.36200786953315145,
    0.4743097175657402,
    0.4393728523291532,
    0.3992792893895722,
    0.30736779135300624,
    0.49347972974658083,
    0.4321661417507521,
    0.36191148196418205,
    0.4239619424906595,
    0.28970821326954005,
    0.408263294899493,
    0.41254973202173295,
    0.47237312189475406,
    0.4293698458886353,
    0.35881744496281753,
    0.3609820149969515,
    0.39127622948982604,
    0.4599629043776239,
    0.41929476580379743,
    0.4345309186007763,
    0.425711082715185,
    0.41872694385043957,
    0.44874392187968043,
    0.3768934666447848,
    0.41582501964104746,
    0.40076822512477606,
    0.3474869223324428
  ],
  "col_avgs": [
    0.37947518192268065,
    0.4384295149653635,
    0.4231003188377461,
    0.4310031885619748,
    0.408057821276822,
    0.4383829957319675,
    0.3215861401631286,
    0.39855642829273774,
    0.4215914813245267,
    0.347892528534909,
    0.40508953382554436,
    0.2771244686169925,
    0.4283559181441416,
    0.43473001180309956,
    0.42597167449229717,
    0.4172426646125899,
    0.4305955465213448,
    0.40205408192198455,
    0.42415298520474715,
    0.3865849419900368,
    0.39062287083293723,
    0.39154413142839256,
    0.39028430147223414,
    0.4185472173731863,
    0.4147541141085385,
    0.4040351901252609,
    0.41902455742160016,
    0.408972576221646,
    0.3358860463667654
  ],
  "combined_avgs": [
    0.3115890485560714,
    0.4216170736769979,
    0.3925540941854488,
    0.4526564530638575,
    0.42371533680298756,
    0.4188311425607698,
    0.3144769657580674,
    0.4460180790196593,
    0.4268788115376394,
    0.3549020052495455,
    0.4145257381581019,
    0.2834163409432663,
    0.41830960652181726,
    0.42363987191241625,
    0.4491723981935256,
    0.4233062552506126,
    0.39470649574208116,
    0.381518048459468,
    0.4077146073472866,
    0.42327392318383034,
    0.40495881831836733,
    0.41303752501458446,
    0.40799769209370956,
    0.41863708061181293,
    0.4317490179941095,
    0.3904643283850229,
    0.4174247885313238,
    0.404870400673211,
    0.34168648434960414
  ],
  "gppm": [
    592.8273261052012,
    598.5212363542913,
    603.3379156915697,
    602.8645221080491,
    611.02139796895,
    598.4175864158757,
    650.2323409296343,
    614.2323817316855,
    608.729301373153,
    636.572986533094,
    618.7601059018183,
    672.6501188471233,
    607.8508465075148,
    599.692438263958,
    605.7437953284382,
    611.5734094733725,
    600.1015358300095,
    615.171306334434,
    605.0405030873236,
    624.6495044565052,
    612.1646443616091,
    622.688432415681,
    617.2040551517509,
    605.965582604095,
    604.7542016401012,
    608.1120465285977,
    602.8549507693411,
    610.2031334109513,
    646.6302225098785
  ],
  "gppm_normalized": [
    1.3751216014287149,
    1.360026834714875,
    1.370059327954976,
    1.3648485679744415,
    1.3808905649713485,
    1.358084636953382,
    1.4805092531676778,
    1.3895281359082874,
    1.3770881618990938,
    1.4436893971108657,
    1.402759208100092,
    1.5288340789912052,
    1.3773546696434538,
    1.3556316634846843,
    1.3724314582823192,
    1.3873086316422747,
    1.3572832654660072,
    1.3866710617890348,
    1.3675714393614269,
    1.4186474973829817,
    1.374973657740426,
    1.4100437721222046,
    1.3904983627979084,
    1.3736920562320447,
    1.3663178546653652,
    1.3784996194229022,
    1.3647782955433365,
    1.377600673997533,
    1.4657685764435513
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364,
    668,
    476,
    491,
    435,
    399,
    469,
    505,
    387,
    435,
    421,
    416,
    456,
    451,
    447,
    366,
    429,
    444,
    439,
    432,
    365,
    436,
    383,
    403,
    437,
    433,
    512,
    444,
    453,
    398
  ],
  "response_lengths": [
    3203,
    2625,
    2781,
    2536,
    2339,
    2722,
    2873,
    2154,
    2433,
    2408,
    2389,
    2538,
    2557,
    2577,
    2113,
    2530,
    2482,
    2549,
    2454,
    2000,
    2509,
    2140,
    2295,
    2466,
    2458,
    2817,
    2611,
    2393,
    2257
  ]
}