{
  "example_idx": 62,
  "reference": "Published as a conference paper at ICLR 2023\n\nDISTRIBUTED DIFFERENTIAL PRIVACY IN MULTI-ARMED BANDITS\n\nSayak Ray Chowdhury ∗ Microsoft Research Bengaluru, Karnataka, India t-sayakr@microsoft.com\n\nXingyu Zhou ∗ Department of Electrical and Computer Engineering Wayne State University Detroit, USA xingyu.zhou@wayne.edu\n\nABSTRACT\n\nWe consider the standard K-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server. Under this trust model, previous work on private bandits largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves (cid:19)\n\n(cid:18)\n\n(ε, δ) or approximate-DP guarantee by sacrificing an additive O\n\nK log T √log(1/δ) ε\n\nfactor in T -step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger (ε, 0) or pure-DP guarantee under the widely used central trust model is\n\n, where, however, a trusted server is required. In this work, we aim\n\nonly Θ\n\n(cid:16)K log T\n\n(cid:17)\n\nε\n\nto obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under the central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures R ́enyi differential privacy – a stronger notion than approximate DP – under distributed trust model\n\nwith a privacy cost of O\n\nlog T ε\nnumerical evaluations on both synthetic and real-world data.\n\n. Our theoretical findings are corroborated by\n\n√\n\n(cid:16)K\n\n(cid:17)\n\n1\n\nINTRODUCTION\n\nThe multi-armed bandit (MAB) problem provides a simple but powerful framework for sequential decision-making under uncertainty with bandit feedback, which has attracted a wide range of practical applications such as online advertising (Abe et al., 2003), product recommendations (Li et al., 2010), clinical trials (Tewari & Murphy, 2017), to name a few. Along with its broad applicability, however, there is an increasing concern of privacy risk in MAB due to its intrinsic dependence on users’ feedback, which could leak users’ sensitive information (Pan et al., 2019).\n\nTo alleviate the above concern, the notion of differential privacy, introduced by Dwork et al. (2006) in the field of computer science theory, has recently been adopted to design privacy-preserving bandit algorithms (see, e.g., Mishra & Thakurta (2015); Tossou & Dimitrakakis (2016); Shariff & Sheffet (2018)). Differential privacy (DP) provides a principled way to mathematically prove privacy guarantees against adversaries with arbitrary auxiliary information about users. To achieve this, a differentially private bandit algorithm typically relies on a well-tuned random noise to obscure each user’s contribution to the output, depending on privacy levels ε, δ – smaller values lead to stronger protection but also suffer worse utility (i.e., regret). For example, the central server of a recommendation system can use random noise to perturb its statistics on each item after receiving feedback (i.e., clicks/ratings) from users. This is often termed as central model (Dwork et al., 2014), since the central server has the trust of its users and hence has a direct access to their raw\n\n∗Equal contributions\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Best-known performance of private MAB under different privacy models (K = number of arms, T = time horizon, ∆a= reward gap of arm a w.r.t. best arm, ε, δ, α = privacy parameters)\n\nTrust Model\n\nCentral\n\nLocal\n\nPrivacy Guarantee Best-Known Regret Bounds (cid:16)(cid:80) (cid:16) 1\n\na∈[K]:∆a>0 (cid:80)\n\n(ε, 0)-DP\n\nlog T ∆a\n\nΘ\n\n(cid:17)\n\n+ K log T\n\nε\n\n(ε, 0)-DP\n\na∈[K]:∆a>0\n\nlog T ∆a\n\nΘ\n\nε2\n\n(cid:32)\n\nDistributed\n\n(ε, δ)-DP\n\nDistributed\n\nDistributed\n\n(ε, 0)-DP O(α, αε2\n\n2 )-RDP\n\nO\n\nΘ\n\nO\n\n(cid:16)(cid:80) (cid:16)(cid:80)\n\n(cid:80)\n\nlog T ∆a\n\n+\n\nK log T √log 1 ε\n\nδ\n\na:∆a>0\n\na∈[K]:∆a>0\n\na∈[K]:∆a>0\n\n(cid:17)\n\nlog T ∆a log T ∆a\n\n+ K log T\n\nε √\n\n+ K\n\nlog T ε\n\n(Theorem 1) (cid:17)\n\n(Theorem 2)\n\n(cid:17)\n\n(Sajed & Sheffet, 2019)\n\n(Ren et al., 2020)\n\n(cid:33)\n\n(Tenenbaum et al., 2021)\n\n(cid:16) K log T\n\n(cid:17)\n\nε\n\ndata. Under this model, an optimal private MAB algorithm with a pure DP guarantee (i.e., when\n\nδ = 0) is proposed in Sajed & Sheffet (2019), which only incurs an additive O\n\nterm in the\n\ncumulative regret compared to the standard setting when privacy is not sought after (Auer, 2002). However, this high trust model is not always feasible in practice since users may not be willing to share their raw data directly to the server. This motivates to employ a local model (Kasiviswanathan et al., 2011) of trust, where DP is achieved without a trusted server as each user perturbs her data prior to sharing with the server. This ensures a stronger privacy protection, but leads to a high cost in utility due to large aggregated noise from all users. As shown in Ren et al. (2020), under the local model, private MAB algorithms have to incur a multiplicative 1/ε2 factor in the regret rather than the additive one in the central model.\n\nIn attempts to recover the same utility of central model while without a trustworthy server like the local model, an intermediate DP trust model called distributed model has gained an increasing interest, especially in the context of (federated) supervised learning (Kairouz et al., 2021b; Agarwal et al., 2021; Kairouz et al., 2021a; Girgis et al., 2021; Lowy & Razaviyayn, 2021). Under this model, each user first perturbs her data via a local randomizer, and then sends the randomized data to a secure computation function. This secure function can be leveraged to guarantee privacy through aggregated noise from distributed users. There are two popular secure computation functions: secure aggregation (Bonawitz et al., 2017) and secure shuffling (Bittau et al., 2017). The former often relies on cryptographic primitives to securely aggregate users’ data so that the central server only learns the aggregated result, while the latter securely shuffle users’ messages to hide their source. To the best of our knowledge, distributed DP model is far less studied in online learning as compared to supervised learning, with only known results for standard K-armed bandits in Tenenbaum et al. (2021), where secure shuffling is adopted. Despite being pioneer work, the results obtained in this paper have several limitations: (i) The privacy guarantee is obtained only for approximate DP (δ > 0) – a stronger pure DP (δ = 0) guarantee is not achieved; (ii) The cost of privacy is a multiplicative (cid:112)log(1/δ) factor away from that of central model, leading to a higher regret bound; (iii) The secure protocol works only for binary rewards (or communication intensive for real rewards).1\n\nOur contributions. In this work, we design the first communication-efficient MAB algorithm that satisfies pure DP in the distributed model while attaining the same regret bound as in the central model (see Table 1). We overcome several key challenges that arise in the design and analysis of distributed DP algorithms for bandits. We now list the challenges and our proposed solutions below.\n\n(a) Private and communication efficient algorithm design. Secure aggregation (SecAgg) works only in the integer domain due to an inherent modular operation (Bonawitz et al., 2017). Hence, leveraging this in bandits to achieve distributed DP with real rewards needs adopting data quantization, discrete privacy noise and modular summation arithmetic in the algorithm design. To this end, we take a batch version of the successive arm elimination technique as a building block of our algorithm, and on top of it, employ a privacy protocol tailored to discrete privacy noise and modular operation (see Algorithm 1). Instantiating the protocol at each user with P ́olya random noise, we\n\n1For more general linear bandits under distributed DP via shuffling, see Chowdhury & Zhou (2022b);\n\nGarcelon et al. (2022), which also have similar limitations.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nensure that our algorithm satisfies pure DP in the distributed model. Moreover, the communication bits per-user scale only logarithmicaly with the number of participating users in each batch.\n\n(b) Regret analysis under pure DP with SecAgg. While our pure DP guarantee exploits known results for discrete Laplace mechanism, the utility analysis gets challenging due to modular clipping of SecAgg. In fact, in supervised learning, no known convergence rate exists for SGD under pure DP with SecAgg (although the same is well-known under central model). This is because modular clipping makes gradient estimates biased, and hence, standard convergence guarantees using unbiased estimates do not hold. In bandits, however, we work with zeroth order observations to build estimates of arms’ rewards, and require high-confidence tight tail bounds for the estimates to analyse convergence. To this end, relying on tail properties of discrete Laplace and a careful analysis of modular operation, we prove a sublinear regret rate of our algorithm, which matches the optimal one in the central model, and thus, achieves the optimal rate under pure DP (see Theorem 1).\n\n(c) Improved regret bound under RDP. While our main focus were to design the first bandit algorithm with pure distributed DP that achieves the same regret rate under central model, our template protocol is general enough to obtain different privacy guarantees by tuning the noise at each user. We demonstrate this by achieving R ́enyi differential privacy (RDP) (Mironov, 2017) using a Skellam random noise. RDP is a weaker notion of privacy compared to pure DP, but it is still stronger than approximate DP. It also provides a tighter privacy accounting for composition compared to approximate DP. This is particularly useful for bandit algorithms, when users may participate in multiple rounds, necessitating the need for privacy composition. Hence, we focus on RDP with SecAgg and and show that a tighter regret bound compared to pure DP can be achieved (see Theorem 2) by proving novel tail-bound for Skellam distribution. We support our theoretical findings with extensive numerical evaluation over bandit instances generated from both synthetic and real-life data.\n\nFinally, our analysis technique is also general enough to recover best-known regrets under central and local DP models while only using discrete privacy noise (see Appendix H). This is important in practice since continuous Laplace noise might leak privacy on finite computers due to floating point arithmetic (Mironov, 2012), which is a drawback of existing central and local DP MAB algorithms.\n\n2 PRELIMINARIES\n\nIn this section, we formally introduce the distributed differential privacy model in bandits. Before that we recall the learning paradigm in multi-armed bandits and basic differential privacy definitions.\n\n∈\n\n[T ] :=\n\nLearning model and regret in MAB. At each time slot t , the agent (e.g., [K] (e.g., an advertisement), recommends it to a new user recommender system) selects an arm a t and obtains an i.i.d reward rt (e.g., a rating indicating how much she likes it), which is sampled from a distribution over [0, 1] with mean given by μa. Let a∗ := argmaxa∈[K] μa be the arm with the highest mean and denote μ∗ := μa∗ for simplicity. Let ∆a := μ∗ μa be the gap of the expected reward between the optimal arm a∗ and any other arm a. Further, let Na(t) be the total number of times that arm a has been recommended to first t users. The goal of the agent is to maximize its total reward, or equivalently to minimize the cumulative expected pseudo-regret, defined as (cid:20)(cid:88)T\n\n1, . . . , T\n\n(cid:20)(cid:88)\n\n−\n\n∈\n\n(cid:21)\n\n(cid:21)\n\n}\n\n{\n\nE [Reg(T )] := T\n\nμ∗\n\nE\n\n·\n\n−\n\nrt\n\n= E\n\nt=1\n\n∆aNa(T )\n\n.\n\na∈[K]\n\n∈\n\n∈\n\nD\n\n∈ D\n\n= [0, 1] be the data universe, and n\n\nN be the number of unique users. n are neighboring datasets if they only differ in one user’s reward preference for [n]. We have the following standard definition of differential privacy (Dwork et al., 2006). satisfies (ε, δ)-DP if\n\nDifferential privacy. Let we say D, D′ some i Definition 1 (Differential Privacy). For ε, δ > 0, a randomized mechanism for all neighboring datasets D, D′ and all events eε\n\nE P [ ·\nThe special case of (ε, 0)-DP is often referred to as pure differential privacy, whereas, for δ > 0, (ε, δ)-DP is referred to as approximate differential privacy. We also consider a related notion of privacy called R ́enyi differential privacy (RDP) Mironov (2017), which allows for a tighter composition compared to approximate differential privacy. Definition 2 (R ́enyi Differential Privacy). For α > 1, a randomized mechanism (α, ε(α))-RDP if for all neighboring datasets D, D′, we have Dα(\n\nin the range of (D′)\n\nsatisfies M\nε(α), where\n\nM , we have\n\nM ] + δ.\n\n(D′))\n\n(D),\n\n(D)\n\n∈ E\n\n∈ E\n\nP [\n\nM\n\nM\n\n≤\n\n]\n\nM\n\nM\n\n≤\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nDα(P, Q) is the R ́enyi divergence (of order α) of the distribution P from the distribution Q, and is (cid:17)α(cid:105)(cid:17) given by Dα(P, Q) := 1\n\nEx∼Q\n\n(cid:16)\n\n.\n\n(cid:104)(cid:16) P (x) Q(x)\n\nα−1 log\n\nS\n\n, and (iii) an analyzer\n\nDistributed differential privacy. A distributed bandit learning protocol of three parts: (i) a (local) randomizer\n\n) consists at each user’s side, (ii) an intermediate secure protocol on\n\nat the central server. Each user i first locally apply the randomizer\n\nS R\nits raw data (i.e., reward) Di, and sends the randomized data to a secure computation protocol (e.g., secure aggregation or shuffling). This intermediate secure protocol randomized data and generates inputs to the central server, which utilizes an analyzer the output (e.g., action) using received messages from\n\nS takes a batch of users’ to compute\n\n, R\n\n= (\n\nR\n\nA\n\nA\n\nA\n\nP\n\nS\n\nS\n\n,\n\n.\n\nS\n\nhas two main variations: secure shuffling and secure aggreThe secure computation protocol gation. Both of them essentially work with a batch of users’ randomized data and guarantee that the central server cannot infer any individual’s data while the total noise in the inputs to the analyzer provides a high privacy level. To adapt both into our MAB protocol, it is natural to divide participating users into batches. For each batch b is given (Dnb )). The goal is to guarantee that the the view of all B by R\nbatches’ outputs satisfy DP. To this end, we define a (composite) mechanism P = (\n\n[B] with nb users, the outputs of\n\nnb (D) :=\n\n(D1), . . . ,\n\nn1, . . . ,\n\nS ◦ R\n\n( R\n\nnB ),\n\n∈\n\nS\n\nS\n\nwhere each individual mechanism\n\nS ◦ R nb operates on nb users’ rewards, i.e., on a dataset from\n\nS ◦ R\n\nnb . With this notation, we have the following definition of distributed differential privacy.\n\nD Definition 3 (Distributed DP). A protocol distributed model if the mechanism\n\n= (\n\n, R\n\nA P satisfies Definition 1 (or Definition 2).\n\nP\n\nS\n\n,\n\n) is said to satisfy DP (or RDP) in the\n\nM\n\nS ◦ R\n\nM\n\nIn the central DP model, the privacy burden lies with a central server (in particular, analyzer ), which needs to inject necessary random noise to achieve privacy. On the other hand, in the local . In contrast, in the distributed DP DP model, each user’s data is privatized by local randomizer model, privacy without a trusted central server is achieved by ensuring that the inputs to the analyzer already satisfy differential privacy. Specifically, by properly designing the intermediate protocol A\n, one can ensure that the final added noise in the aggregated and the noise level in the randomizer S\ndata over a batch of users matches the noise that would have otherwise been added in the central model by the trusted server. Through this, distributed DP model provides the possibility to achieve the same level of utility as the central model while without a trustworthy central server.\n\nR\n\nR\n\nA\n\n3 A GENERIC ALGORITHM FOR PRIVATE BANDITS\n\nIn this section, we propose a generic algorithmic framework (Algorithm 1) for multi-armed bandits under the distributed privacy model.\n\n,\n\nS\n\nP\n\nA\n\n= (\n\n, R\n\nBatch-based successive arm elimination. Our algorithm builds upon the classic idea of successive arm elimination (Even-Dar et al., 2006) with the additional incorporation of batches and a black- ) to achieve distributed differential privacy. It divides the time horizon box protocol T into batches of exponentially increasing size and eliminates sub-optimal arms successively. To this end, for each active arm a at batch b, it first prescribes arm a to a batch of l(b) = 2b new users. After pulling the prescribed action a, each user applies the local randomizer to her reward and sends the randomized reward to the intermediary function , which runs a secure computation protocol (e.g., secure aggregation or secure shuffling) over the total l(b) number of randomized rewards. Then, upon receiving the outputs of to compute the the sum of rewards for batch b when pulling arm a (i.e., Ra(b)), which in turn gives the new mean estimate (cid:98)μa(b) of arm a after being divided by the total pulls l(b). Then, upper and lower confidence bounds, UCBa(b) and LCBa(b), respectively, are computed around the mean estimate (cid:98)μa(b) with a properly chosen confidence width β(b). Finally, after the iteration over all active arms in batch b (denoted by the set Φ(b)), it adopts the standard arm elimination criterion to remove all obviously sub-optimal arms, i.e., it removes an arm a from Φ(b) if UCBa(b) falls below LCBa′(b) of any other arm a′\n\nΦ(b). It now only remains to design a distributed DP protocol\n\n, the server applies the analyzer\n\nR\n\nA\n\nS\n\nS\n\n.\n\n∈\n\nP\n\nThere is one key difference between our algorithm and the VB-SDP-AE algorithm in Tenenbaum et al. (2021). At the start of a batch, VB-SDP-AE uses all the past data to compute reward estimates. In contrast, we adopt the idea of forgetting and use only the data of the last completed batch.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Private Batch-Based Successive Arm Elimination\n\nβ(b) , Estimate (cid:98)μa(1) = 0,\n\n{\n\nb≥1 a\n\n} ∀\n\n∈\n\n[K]\n\n// randomizer\n\n}\n\n{\n\nS\n\nR\n\na(b) =\n\nΦ(b) do\n\n1, . . . , K\n\na(b) (ri\n\na(b)) to\n\n∈ for each new user i from 1 to l(b) do Pull arm a and generate reward ri Send randomized data yi If total number of pulls reaches T , exit\n\n1: Parameters: # arms K, Time horizon T , privacy level ε > 0, Confidence radii 2: Initialize: Batch count b = 1, Active arm set Φ(b) = 3: for batch b = 1, 2, . . . do Set batch size l(b) = 2b 4: for each active arm a 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for 18: Subroutine: Local Randomizer 19: Require: precision g ∈\n20: Encode xi as (cid:98)xi = xig ⌋\n21: Generate discrete noise ηi (depending on n, ε, g) 22: Add noise and modulo clip yi = ((cid:98)xi + ηi) mod m 23: Subroutine: Secure Aggregation 24: Require: modulo m\n\nend for Send messages (cid:98)ya(b) = }1≤i≤l(b)) to A\n((cid:98)ya(b)) Compute the sum of rewards Ra(b) = A\nCompute mean estimate (cid:98)μa(b) = Ra(b)/l(b) Compute confidence bounds UCBa(b) = (cid:98)μa(b)+β(b) and LCBa(b) = (cid:98)μa(b)\n\nend for Update active set of arms: Φ(b+1) = (cid:8)a\n\n(Input: y1, . . . , yn, Output: (cid:98)y)\n\nN, batch size n xig\n\nN, modulo m + Ber(xig\n\n∈ (Input: xi\n\nN, privacy level ε\n\n[0, 1], Output: yi)\n\nΦ(b) : UCBa(b)\n\n∈ − ⌊\n\na(b)\n\nyi\n\nR\n\n≥\n\nN\n\n∈\n\n∈\n\nS\n\nS\n\n{\n\n⌊\n\n⌋\n\n(\n\n)\n\n25: Securely compute (cid:98)y = ((cid:80)n\n\n∈\n\ni=1 yi) mod m\n\n// secure computation // analyzer\n\nβ(b)\n\n−\n\nmaxa′∈Φ(b) LCBa′(b)(cid:9)\n\n// random noise generator\n\n(Input: (cid:98)y, Output: z) N, modulo m\n\nN, batch size n\n\n∈\n\n// black-box function\n\nN, accuracy level τ\n\nR\n\n∈\n\n∈\n\n// correction for underflow\n\n26: Subroutine: Analyzer 27: Require: precision g 28: if (cid:98)y > ng + τ then set z = ((cid:98)y 29: −\n30: else set z = (cid:98)y/g\n\nm)/g\n\nA ∈\n\nDistributed DP protocol via discrete privacy noise. Inspired by Balle et al. (2020); Cheu & Yan (2021), we provide a general template protocol for the distributed DP model, which relies only on discrete privacy noise.\n\nP\n\n∈\n\nR\n\nreceives each user i’s real-valued data xi and encodes it as an integer via Local randomizer fixed-point encoding with precision g > 0 and randomized rounding. Then, it generates a discrete noise, which depends on the specific privacy-regret trade-off requirement (to be discussed later under specific mechanisms). Next, it adds the random noise to the encoded reward, clips the sum with modulo m\n\nN and sends the final integer yi as input to secure computation function\n\n.\n\nhere.2 SecAgg is treated as a black-box We mainly focus on secure aggregation (SecAgg) for function as in previous work on supervised learning (Kairouz et al., 2021a), which implements Zm (i.e., integer in the following procedure: given n users and their randomized messages yi ∈\nsecurely computes the modular sum of the i=1 yi) mod m, while revealing no further information on individual messages to a potential attacker, ensuring that it is perfectly secure. Details of engineering implementations of SecAgg is beyond the scope of this paper, see Appendix G for a brief discussion on this.\n\nn messages, (cid:98)y = ((cid:80)n\n\n, the SecAgg function\n\n) obtained via\n\n0, 1, . . . , m\n\nR\n\n−\n\nS\n\nS\n\n}\n\n{\n\n1\n\nS\n\nA\n\nThe job of analyzer uses an accuracy parameter τ = ( and bias due to encoding. To sum it up, the end goal of our protocol that it provides the required privacy protection while guaranteeing an output z probability, which is the key to our privacy and regret analysis in the following sections.\n\nis to compute the sum of rewards within a batch as accurately as possible. It R and g to correct for possible underflow due to modular operation ,\n, ) is to ensure (cid:80)n A\nS i=1 xi with high\n\nR ≈\n\nP\n\n∈\n\n2Instead of SecAgg, we can also use secure shuffling as S (see Appendix D.2), since each has advantages over the other. The high-level idea is same for both techniques; i.e., to ensure that after receiving messages from S, the server cannot distinguish each individual’s message.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4 ACHIEVING PURE DP IN THE DISTRIBUTED MODEL\n\nIn this section, we show that Algorithm 1 achieves pure-DP in the distributed DP model via secure aggregation. To do so, we need to carefully determine the amount of (discrete) noise in so that the total noise in a batch provides (ε, 0)-DP. One natural choice is the discrete Laplace noise. Definition 4 (Discrete Laplace Distribution). Let b > 0. A random variable X has a discrete Laplace distribution with scale parameter b, denoted by LapZ(b), if it has a p.m.f. given by\n\nR\n\nZ, P [X = x] =\n\nx\n\n∀\n\n∈\n\n1\n\ne1/b −\ne1/b + 1 ·\n\ne−|x|/b.\n\nA key property of discrete Laplace that we will use is its infinite divisibility, which allows us to simulate it in a distributed way (Goryczka & Xiong, 2015, Theorem 5.1). Fact 1 (Infinite Divisibility of Discrete Laplace). A random variable X has a P ́olya distribution with parameters r > 0, β\n\n[0, 1], denoted by P ́olya(r, β), if it has a p.m.f. given by\n\nx\n\nN, P [X = x] =\n\nΓ(x + r) x!Γ(r)\n\nβx(1\n\nβ)r.\n\n−\n\n∈\n\n∀ ∈\ni , γ− γ+\n\nNow, for any n variable (cid:80)n\n\n∈ i=1(γ+\n\nN, let\n\ni −\n\ni }i∈[n] be 2n i.i.d samples3 from P ́olya(1/n, e−1/b), then the random\n\n{ γ− i ) is distributed as LapZ(b).\n\nArmed with the above fact and the properties of discrete Laplace noise (see Fact 3 in Appendix K), we are able to obtain the following main theorem, which shows that the same regret as in the central model is achieved under the distributed model via SecAgg. Theorem 1 (Pure-DP via SecAgg). Fix ε > 0 and T i.i.d. user in the batch be ηi = γ+ ∼\nτ =\n\nN. For each batch b, let noise for the i-th ,\n⌉ and m = ng + 2τ + 1. Then, Algorithm 1 achieves (ε, 0)-DP in the distributed\n\n∈ P ́olya(1/n, e−ε/g), set n = l(b), g =\n\ni , where γ+ γ−\n\ni , γ−\n\ni −\n\nε√n\n\n⌈\n\ng\n\ni\n\nε log(2T )\n\n⌈\n\n⌉ model. Moreover, setting β(b) = O\n\n(cid:18)(cid:113) log(|Φ(b)|b2T )\n\n+\n\n2 log(|Φ(b)|b2T ) εl(b)\n\n(cid:19)\n\n, it enjoys expected regret\n\n2l(b)\n\n(cid:18)(cid:88)\n\nE [Reg(T )] = O\n\nlog T ∆a\n\n+\n\nK log T ε\n\n(cid:19)\n\n.\n\na∈[K]:∆a>0\n\nTheorem 1 achieves optimal regret under pure DP. Theorem 1 achieves the same regret bound as the one achieved in Sajed & Sheffet (2019) under the central trust model with continuous Laplace noise. Moreover, it matches the lower bound obtained under pure DP in Shariff & Sheffet (2018), indicating the bound is indeed tight. Note that, we achieve this rate under distributed trust model – a stronger notion of privacy protection than the central model – while using only discrete noise.\n\nS\n\n, i.e., communicating bits scales logarithmically with the batch size.\n\nCommunication bits. Algorithm 1 needs to communicate O(log m) bits per user to the secure protocol In contrast, the number of communication bits required in existing distributed DP bandit algorithms that work with real-valued rewards (as we consider here) scale polynomially with the batch size (Chowdhury & Zhou, 2022b; Garcelon et al., 2022). Remark 1 (Pure DP via Secure Shuffling). It turns out that one can achieve same privacy and regret guarantees (orderwise) using a relaxed SecAgg protocol. Building on this result, we also establish pure DP under shuffling while again maintaining the same regret bound as the central model (see Theorem 3 in Appendix D.2). This improves the state-of-the-art result for MAB with shuffling (Tenenbaum et al., 2021) in terms of both privacy and regret.\n\n5 ACHIEVING RDP IN THE DISTRIBUTED MODEL\n\nA natural question to ask is whether one can get a better regret performance by sacrificing a small amount of privacy. We consider the notion of RDP (see Definition 2), which is a weaker notion of privacy than pure DP. However, it avoids the possible catastrophic privacy failure in approximate DP, and also provides a tighter privacy accounting for composition (Mironov, 2017).\n\n3One can sample from P ́olya as follows. First, sample λ ∼ Gamma(r, β/(1−β)) and then use it to sample\n\nX ∼ Poisson(λ), which is known to follow P ́olya(r, β) distribution (Goryczka & Xiong, 2015).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTo achieve RDP guarantee using discrete noise, we consider the Skellam distribution – which has recently been introduced in private federated learning (Agarwal et al., 2021). A key challenge in the regret analysis of our bandit algorithm is to characterize the tail property of Skellam distribution. This is different from federated learning, where characterizing the variance renders sufficient. In Proposition 1, we prove that Skellam has sub-exponential tails, which not only is the key to our regret analysis, but could also be of independent interest. Below is the formal definition of Skellam.\n\nDefinition 5 (Skellam Distribution). A random variable X has a Skellam distribution with mean μ and variance σ2, denoted by Sk(μ, σ2), if it has a probability mass function given by\n\nZ, P [X = x] = e−σ2 ∈\n) is the modified Bessel function of the first kind.\n\nx\n\n∀\n\nIx−μ(σ2),\n\nwhere Iν( ·\n\nTo sample from Skellam distribution, one can rely on existing procedures for Poisson samples. This Poisson(σ2/2), then X is Sk(0, σ2) distributed.\n\nis because if X = N1 Moreover, due to this fact, Skellam is closed under summation, i.e., if X1 Sk(μ2, σ2 Sk(μ1 + μ2, σ2\n\n− 2), then X1 + X2\n\nN2, where N1, N2\n\ni.i.d. ∼\n1 + σ2\n\nSk(μ1, σ2\n\n1) and X2\n\n2).\n\n∼\n\n∼\n\n∼ Proposition 1 (Sub-exponential Tail of Skellam). Let X exponential. Hence, for any p\n\n∼\n\nSk(0, σ2). Then, X is (2σ2,\n\n√\n\n2\n\n2 )-sub-\n\n(0, 1], with probability at least 1\n\np, −\n2σ(cid:112)log(2/p) + √2 log(2/p).\n\n∈\n\nX |\n\n| ≤\n\nWith the above result, we can establish the following privacy and regret guarantee of Algorithm 1.\n\nTheorem 2 (RDP via SecAgg). Fix ε > 0, T let noise for the i-th user be ηi √2 log(2T ) model for all α = 2, 3, . . ., with (cid:98)ε(α) = αε2\n\n1. For each batch b, (cid:112)log(2T ) + and m = ng + 2τ + 1. Then, Algorithm 1 achieves (α, (cid:98)ε(α))-RDP in the distributed . Moreover, setting\n\nN and a scaling factor s sε√n\n\nnε2 ), set n = l(b), g =\n\n4s2 + 3ε\n\nSk(0, g2\n\n(cid:110) (2α−1)ε2\n\n2s3 , 3ε2\n\n≥ , τ =\n\n2g ε\n\n∼\n\n∈\n\n(cid:111)\n\n2s\n\n⌉\n\n⌉\n\n⌈\n\n⌈\n\nβ(b) = O\n\n(cid:16)(cid:113) log(|Φ(b)|b2T )\n\n2l(b)\n\n+ (1+1/s) log(|Φ(b)|b2T )\n\nεl(b)\n\n, it enjoys the expected regret\n\n2 + min (cid:17)\n\nE [Reg(T )] = O\n\n(cid:18)(cid:88)\n\na∈[K]:∆a>0\n\nlog T ∆a\n\n+\n\nK√log T ε\n\n+\n\nK log T sε\n\n(cid:19)\n\n.\n\nS\n\nPrivacy-Regret-Communication Trade-off. Observe that the scaling factor s allows us to achieve different trade-offs. If s increases, both privacy and regret performances improve. In fact, for a sufficiently large value of s, the third term in the regret bound becomes sufficiently small, and we obtain an improved regret bound compared to Theorem 1. Moreover, the RDP privacy guarantee improves αε2 to (cid:98)ε(α) 2 , which is the standard RDP rate for Gaussian mechanism (Mironov, 2017). However, a larger s leads to an increase of communicating bits per user, but only grows logarithmically, since Algorithm 1 needs to communicate O(log m) bits to the secure protocol\n\n≈\n\n.\n\nRDP to Approximate DP. To shed more insight on Theorem 2, we convert our RDP guarantee to approximate DP for a sufficiently large s. It holds that under the setup of Theorem 2, for suf- (cid:1) ficiently large s, one can achieve (O(ε), δ)-DP with regret O(cid:0) (cid:80) (via Lemma 10 in Appendix K). Implication of this conversion is three-fold. First, this regret bound is O(√log T ) factor tighter than that achieved by Tenenbaum et al. (2021) using a shuffle protocol. Second, it yields a better regret performance compared to the bound achieved under (ε, 0)-DP in Theorem 1 when the privacy budget δ > 1/T . This observation is consistent with the fact that a weaker privacy guarantee typically warrants a better utility bound. Third, this conversion via RDP also yields a gain of O((cid:112)log(1/δ)) in the regret when dealing with privacy composition (e.g., when participating users across different batches are not unique) as compared to Tenenbaum et al. (2021) that only relies on approximate DP (see Appendix I for details). This results from the fact that RDP provides a tighter composition compared to approximate DP.\n\nK√log T log(1/δ) ε\n\nlog T ∆a\n\na:∆a>0\n\n+\n\nRemark 2 (Achieving RDP with discrete Gaussian). One can also achieve RDP using discrete Gaussian noise (Canonne et al., 2020). Here, we work with Skellam noise since it is closed under summation and enjoys efficient sampling procedure as opposed to discrete Gaussian (Agarwal et al., 2021). Nevertheless, as a proof of flexibility of our proposed framework, we show in Appendix F that Algorithm 1 with discrete Gaussian noise can guarantee RDP with a similar regret bound.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n6 KEY TECHNIQUES: OVERVIEW\n\nNow, we provide an overview of the key techniques behind our privacy and regret guarantees. We show that the results of Theorem 1 and 2 can be obtained via a clean generic analytical framework, which not only covers the analysis of distributed pure DP/RDP with SecAgg, but also offers a unified view of private MAB under central, local and distributed DP models.\n\nAs in many private learning algorithms, the key is to characterize the impact of added privacy noise on the utility. In our case, this reduces to capturing the tail behavior of total noise na(b) := Ra(b) (cid:80)l(b)\n\n− a(b) added at each batch b for each active arm a. The following lemma gives a generic regret\n\ni=1 ri\n\nbound of our algorithm under mild tail assumptions on na(b).\n\nLemma 1 (Generic regret). Let there exist constants σ, h > 0 such that, with probability\n\n(cid:16)\n\n:= O\n\nna(b) |\nradius β(b) = O\n\n| ≤ N\n\nσ(cid:112)log(KT /p)+h log(KT /p) (cid:17)\n\n(cid:16)(cid:112)log(KT /p)/l(b) +\n\n/l(b)\n\n(cid:17)\n\nfor all b\n\n1, a\n\n∈\n\n≥\n\n[K]. Then, setting confidence\n\nand p = 1/T , Algorithm 1 enjoys expected regret\n\n1\n\np,\n\n−\n\n≥\n\nN\n\nE [Reg(T )] = O\n\n(cid:16) (cid:88)\n\na∈[K]:∆a>0\n\nlog T ∆a\n\n+ Kσ(cid:112)log T + Khlog T\n\n(cid:17)\n\n.\n\non the noise is the tail bound of sub-exponential distribuAn acute reader may note that the bound N\ntion and it reduces to the bound for sub-Gaussian tail if h = 0. Our SecAgg protocol with discrete Laplace noise (as in Theorem 1) satisfy this bound with σ = √2/ε, h = 1/ε. Similarly, our protocol with Skellam noise (as in Theorem 2) satisfy this bound with σ = O(1/ε), h = 1/(sε). Therefore, we can build on the above general result to directly obtain our regret bounds. In the following, we present the high-level idea behind privacy and regret analysis in distributed DP model.\n\nP\n\nPrivacy. For distributed DP, by definition, the view of the server during the entire algorithm needs to be private. Since each user only contributes once,4 by parallel-composition of DP, it suffices to ensure that each view (cid:98)ya(b) (line 11 in Algorithm 1) is private. To this end, under SecAgg, the distribution of (cid:98)ya(b) can be simulated via ((cid:80) i (cid:98)xi + ηi) mod m that accepts an input by the distributive property of modular sum. Now, consider a mechanism i and outputs (cid:80) satisfies pure dataset DP or RDP. To this end, the variance σ2 i ηi needs to scale with the sensitivity of (cid:80) tot/n. Finally, by the particular distribution properties of the noise, one can show that is pure DP or RDP, and hence, obtain the privacy guarantees.\n\ntot of the total noise (cid:80) i (cid:98)xi. Thus, each user within a batch only needs to add a proper noise with variance of σ2\n\ni((cid:98)xi + ηi). By post-processing, it suffices to show that\n\ni yi) mod m, which further reduces to ((cid:80)\n\n{(cid:98)xi\n\nM\n\nM\n\nM\n\n}\n\nRegret. Thanks to Lemma 1, we only need to focus on the tail of na(b). To this end, fix any batch b and arm a. We have (cid:98)y = (cid:98)ya(b), xi = ri in Algorithm 1 and we need to establish that with probability at least 1\n\na(b), n = l(b) for\n\nP\n\n−\n\np, for some σ and h, (cid:88) i\n\n(cid:12) (cid:12) (cid:12) ≤\n\nxi\n\nO\n\n(cid:16)\n\n−\n\n(cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:17) σ(cid:112)log(1/p) + h log(1/p)\n\n.\n\n(1)\n\n|\n\n−\n\n(cid:80)\n\n|A\n\n((cid:98)y)\n\ni (cid:98)xi/g\n\nand Term (ii) =\n\nTo get the bound, inspired by Balle et al. (2020); Cheu & Yan (2021), we divide the LHS into (cid:80) Term (i) = , where Term (i) captures the er- |\n| ror due to privacy noise and modular operation, while Term (ii) captures the error due to random rounding. In particular, Term (ii) can be easily bounded via sub-Gaussian tail since the noise is bounded. Term (i) needs care for the possible underflow due to modular operation by considering two different cases (see line 28-30 in Algorithm 1). In both cases, one can show that Term (i) is upper bounded by τ /g with high probability, where τ is the tail bound on the total privacy noise (cid:80) i ηi. Thus, depending on particular privacy noise and parameter choices, one can find σ and h\n\ni (cid:98)xi/g\n\ni xi\n\nsuch that equation 1 holds, and hence, obtain the corresponding regret bound by Lemma 1.\n\n(cid:80)\n\n−\n\nRemark 3. As a by-product of our generic analysis technique, Algorithm 1 and privacy protocol P\nalong with Lemma 1 provide a new and structured way to design and analyze private MAB algorithms under central and local models with discrete private noise (see Appendix H for details). This enables us to reap the benefits of working with discrete noise (e.g., finite-computer representations, bit communications) in all three trust models (central, local and distributed).\n\n4This assumption that users only contributes once is adopted in nearly all previous works for privacy analysis\n\nin bandits. We also provide a privacy analysis for returning users via RDP, see Appendix I.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) ε = 0.1, K = 10\n\n(b) ε = 0.5, K = 10\n\n(c) ε = 0.1, K = 10\n\n(d) ε = 1, K = 50\n\n(e) ε = 5, K = 50\n\n(f) ε = 10, K = 50\n\nFigure 1: Comparison of time-average regret for Dist-DP-SE, Dist-RDP-SE, and DP-SE. Top: Synthetic Gaussian bandit instances with (a, b) large reward gap (easy instance) and (c) small reward gap (hard instance). Bottom: Bandit instances generated from MSLR-WEB10K learning to rank dataset.\n\n7 SIMULATION RESULTS\n\nWe empirically evaluate the regret performance of our successive elimination scheme with SecAgg protocol (Algorithm 1) under distributed trust model, which we abbreviate as Dist-DP-SE and Distis instantiated with P ́olya noise (for pure DP) and Skellam noise RDP-SE when the randomizer (for RDP), respectively. We compare them with the DP-SE algorithm of Sajed & Sheffet (2019) that achieves optimal regret under pure DP in the central model, but works only with continuous Laplace noise. We fix confidence level p = 0.1 and study comparative performances under varying privacy levels (ε < 1 for synthetic data, ε 1 for real data). We plot time-average regret Reg(T )/T in ≥\nFigure 1 by averaging results over 20 randomly generated bandit instances.\n\nR\n\nBandit instances. In the top panel, similar to Vaswani et al. (2020), we consider easy and hard MAB instances with K = 10 arms: in the former, arm means are sampled uniformly in [0.25, 0.75], while in the latter, those are sampled in [0.45, 0.55]. We consider real rewards – sampled from Gaussian distribution with aforementioned means and projected to [0, 1]. In the bottom panel, we generate bandit instances from Microsoft Learning to Rank dataset MSLR-WEB10K (Qin & Liu, 2013). The dataset consists of 1,200,192 rows and 138 columns, where each row corresponds to a query-url pair. The first column is relevance label 0, 1, ... , 4 of the pair, which we take as rewards. The second column denotes the query id, and the rest 136 columns denote contexts of a query-url pair. We cluster the data by running K-means algorithm with K = 50. We treat each cluster as a bandit arm with mean reward as the empirical mean of the individual ratings in the cluster. This way, we obtain a bandit setting with number of arms K = 50.\n\nObservations. We observe that as T becomes large, the regret performance of Dist-DP-SE matches the regret of DP-SE. The slight gap in small T regime is the cost that we pay to achieve distributed privacy using discrete noise without access to a trusted server (for higher ε value, this gap is even smaller). In addition, we find that a relatively small scaling factor (s = 10) provides a considerable gain in regret under RDP compared to pure DP, especially when ε is small (i.e., when the cost of privacy is not dominated by the non-private part of regret). The experimental findings are consistent with our theoretical results. Here, we note that our simulations are proof-of-concept only and we did not tune any hyperparameters. More details and additional plots are given in Appendix J.5\n\nConcluding remarks. We show that MAB under distributed trust model can achieve pure DP while maintaining the same regret under central model. In addition, RDP is also achieved in MAB under distributed trust model for the first time. Both results are obtained via a unified algorithm design and performance analysis. More importantly, our work also opens the door to a promising and interesting research direction – private online learning with distributed DP guarantees, including contextual bandits and reinforcement learning.\n\n5Code is available at https://github.com/sayakrc/Differentially-Private-Bandits.\n\n9\n\n0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1070.000.010.020.030.04Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.10.20.3Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.10.20.3Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.10.20.3Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)Published as a conference paper at ICLR 2023\n\n8 ACKNOWLEDGEMENTS\n\nXZ is supported in part by NSF CNS-2153220. XZ would like to thank Albert Cheu for insightful discussion on achieving pure DP via shuffling.\n\nREFERENCES\n\nNaoki Abe, Alan W Biermann, and Philip M Long. Reinforcement learning with immediate rewards\n\nand linear hypotheses. Algorithmica, 37(4):263–293, 2003.\n\nNaman Agarwal and Karan Singh. The price of differential privacy for online learning. In Interna-\n\ntional Conference on Machine Learning, pp. 32–40. PMLR, 2017.\n\nNaman Agarwal, Peter Kairouz, and Ziyu Liu. The skellam mechanism for differentially private\n\nfederated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nPeter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine\n\nLearning Research, 3(Nov):397–422, 2002.\n\nBorja Balle, James Bell, Adri`a Gasc ́on, and Kobbi Nissim. The privacy blanket of the shuffle model.\n\nIn Annual International Cryptology Conference, pp. 638–667. Springer, 2019.\n\nBorja Balle, James Bell, Adria Gasc ́on, and Kobbi Nissim. Private summation in the multi-message shuffle model. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pp. 657–676, 2020.\n\nAmos Beimel, Kobbi Nissim, and Eran Omri. Distributed private data analysis: Simultaneously solving how and what. In Annual International Cryptology Conference, pp. 451–468. Springer, 2008.\n\nJames Henry Bell, Kallista A Bonawitz, Adri`a Gasc ́on, Tancr`ede Lepoint, and Mariana Raykova. Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pp. 1253–1269, 2020.\n\nAndrea Bittau, ́Ulfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and Bernhard Seefeld. Prochlo: Strong privacy for analytics in the crowd. In Proceedings of the 26th Symposium on Operating Systems Principles, pp. 441–459, 2017.\n\nKeith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacypreserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 1175–1191, 2017.\n\nMark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and\n\nlower bounds. In Theory of Cryptography Conference, pp. 635–658. Springer, 2016.\n\nCl ́ement L Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential\n\nprivacy. Advances in Neural Information Processing Systems, 33:15676–15688, 2020.\n\nTH Hubert Chan, Elaine Shi, and Dawn Song. Optimal lower bound for differentially private multi-\n\nparty aggregation. In European Symposium on Algorithms, pp. 277–288. Springer, 2012.\n\nWei-Ning Chen, Christopher A Choquette Choo, Peter Kairouz, and Ananda Theertha Suresh. The fundamental price of secure aggregation in differentially private federated learning. In International Conference on Machine Learning, pp. 3056–3089. PMLR, 2022a.\n\nWei-Ning Chen, Ayfer Ozgur, and Peter Kairouz. The poisson binomial mechanism for unbiased federated learning with secure aggregation. In International Conference on Machine Learning, pp. 3490–3506. PMLR, 2022b.\n\nXiaoyu Chen, Kai Zheng, Zixin Zhou, Yunchang Yang, Wei Chen, and Liwei Wang. (locally) differentially private combinatorial semi-bandits. In International Conference on Machine Learning, pp. 1757–1767. PMLR, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAlbert Cheu and Chao Yan. Pure differential privacy from secure intermediaries. arXiv preprint\n\narXiv:2112.10032, 2021.\n\nAlbert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev. Distributed differential privacy via shuffling. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pp. 375–403. Springer, 2019.\n\nAlbert Cheu, Matthew Joseph, Jieming Mao, and Binghui Peng. Shuffle private stochastic convex\n\noptimization. arXiv preprint arXiv:2106.09805, 2021.\n\nSayak Ray Chowdhury and Xingyu Zhou. Differentially private regret minimization in episodic In Proceedings of the AAAI Conference on Artificial Intelligence,\n\nmarkov decision processes. volume 36, pp. 6375–6383, 2022a.\n\nSayak Ray Chowdhury and Xingyu Zhou. Shuffle private linear contextual bandits. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 3984–4009. PMLR, 17–23 Jul 2022b.\n\nSayak Ray Chowdhury, Xingyu Zhou, and Ness Shroff. Adaptive control of differentially private linear quadratic systems. In 2021 IEEE International Symposium on Information Theory (ISIT), pp. 485–490. IEEE, 2021.\n\nRoger Dingledine, Nick Mathewson, and Paul Syverson. Tor: The second-generation onion router.\n\nTechnical report, Naval Research Lab Washington DC, 2004.\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006.\n\nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found.\n\nTrends Theor. Comput. Sci., 9(3-4):211–407, 2014.\n\n ́Ulfar Erlingsson, Vitaly Feldman,\n\nIlya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep Thakurta. Amplification by shuffling: From local to central differential privacy via In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algoanonymity. rithms, pp. 2468–2479. SIAM, 2019.\n\nEyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006.\n\nVitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones: A simple and nearly optimal analysis of privacy amplification by shuffling. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 954–964. IEEE, 2022.\n\nEvrard Garcelon, Vianney Perchet, Ciara Pike-Burke, and Matteo Pirotta. Local differential privacy for regret minimization in reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nEvrard Garcelon, Kamalika Chaudhuri, Vianney Perchet, and Matteo Pirotta. Privacy amplification via shuffling for linear contextual bandits. In International Conference on Algorithmic Learning Theory, pp. 381–407. PMLR, 2022.\n\nBadih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh, and Ameya VelarXiv preprint\n\nPure differentially private summation from anonymous messages.\n\ningker. arXiv:2002.01919, 2020a.\n\nBadih Ghazi, Pasin Manurangsi, Rasmus Pagh, and Ameya Velingker. Private aggregation from fewer anonymous messages. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pp. 798–827. Springer, 2020b.\n\nAntonious Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuffled model of differential privacy in federated learning. In International Conference on Artificial Intelligence and Statistics, pp. 2521–2529. PMLR, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nSlawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions with differential privacy. IEEE transactions on dependable and secure computing, 14(5):463– 477, 2015.\n\nChristopher Hillar and Andre Wibisono. Maximum entropy distributions on graphs. arXiv preprint\n\narXiv:1301.3321, 2013.\n\nPeter Kairouz, Ziyu Liu, and Thomas Steinke. The distributed discrete gaussian mechanism for federated learning with secure aggregation. In International Conference on Machine Learning, pp. 5201–5212. PMLR, 2021a.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021b.\n\nShiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam\n\nSmith. What can we learn privately? SIAM Journal on Computing, 40(3):793–826, 2011.\n\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pp. 661–670, 2010.\n\nChonghua Liao, Jiafan He, and Quanquan Gu. Locally differentially private reinforcement learning\n\nfor linear mixture markov decision processes. arXiv preprint arXiv:2110.10133, 2021.\n\nAndrew Lowy and Meisam Razaviyayn. Private federated learning without a trusted server: Optimal\n\nalgorithms for convex losses. arXiv preprint arXiv:2106.09779, 2021.\n\nPaul Luyo, Evrard Garcelon, Alessandro Lazaric, and Matteo Pirotta. Differentially private exploration in reinforcement learning with linear representation. arXiv preprint arXiv:2112.01585, 2021.\n\nIlya Mironov. On significance of the least significant bits for differential privacy. In Proceedings of\n\nthe 2012 ACM conference on Computer and communications security, pp. 650–661, 2012.\n\nIlya Mironov. R ́enyi differential privacy. In IEEE 30th computer security foundations symposium\n\n(CSF), pp. 263–275. IEEE, 2017.\n\nNikita Mishra and Abhradeep Thakurta. (nearly) optimal differentially private stochastic multi-arm bandits. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pp. 592–601, 2015.\n\nXinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, and Dawn Song. How you act In Proceedings of the 18th\n\ntells a lot: Privacy-leaking attack on deep reinforcement learning. International Conference on Autonomous Agents and MultiAgent Systems, pp. 368–376, 2019.\n\nTao Qin and Tie-Yan Liu. Introducing LETOR 4.0 datasets. CoRR, abs/1306.2597, 2013. URL\n\nhttp://arxiv.org/abs/1306.2597.\n\nWenbo Ren, Xingyu Zhou, Jia Liu, and Ness B Shroff. Multi-armed bandits with local differential\n\nprivacy. arXiv preprint arXiv:2007.03121, 2020.\n\nTouqir Sajed and Or Sheffet. An optimal private stochastic-mab algorithm based on optimal private stopping rule. In International Conference on Machine Learning, pp. 5579–5588. PMLR, 2019.\n\nRoshan Shariff and Or Sheffet. Differentially private contextual linear bandits. Advances in Neural\n\nInformation Processing Systems, 31, 2018.\n\nYouming Tao, Yulian Wu, Peng Zhao, and Di Wang. Optimal rates of (locally) differentially private heavy-tailed multi-armed bandits. In International Conference on Artificial Intelligence and Statistics, pp. 1546–1574. PMLR, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJay Tenenbaum, Haim Kaplan, Yishay Mansour, and Uri Stemmer. Differentially private multiarmed bandits in the shuffle model. Advances in Neural Information Processing Systems, 34, 2021.\n\nAmbuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health.\n\nIn Mobile Health, pp. 495–517. Springer, 2017.\n\nAristide Charles Yedia Tossou and Christos Dimitrakakis. Achieving privacy in the adversarial\n\nmulti-armed bandit. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nAristide CY Tossou and Christos Dimitrakakis. Algorithms for differentially private multi-armed\n\nbandits. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nSharan Vaswani, Abbas Mehrabian, Audrey Durand, and Branislav Kveton. Old dog learns new tricks: Randomized ucb for bandit problems. In International Conference on Artificial Intelligence and Statistics, pp. 1988–1998. PMLR, 2020.\n\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\n\nvolume 47. Cambridge university press, 2018.\n\nGiuseppe Vietri, Borja Balle, Akshay Krishnamurthy, and Steven Wu. Private reinforcement learning with pac and regret guarantees. In International Conference on Machine Learning, pp. 9754– 9764. PMLR, 2020.\n\nKun Wang, Jing Dong, Baoxiang Wang, and Shuai Li. Cascading bandit under differential privacy. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4418–4422. IEEE, 2022.\n\nHuiming Zhang and Song Xi Chen. Concentration inequalities for statistical inference. arXiv\n\npreprint arXiv:2011.02258, 2020.\n\nKai Zheng, Tianle Cai, Weiran Huang, Zhenguo Li, and Liwei Wang. Locally differentially private (contextual) bandits learning. Advances in Neural Information Processing Systems, 33:12300– 12310, 2020.\n\nXingyu Zhou. Differentially private reinforcement learning with linear function approximation.\n\nProc. ACM Meas. Anal. Comput. Syst., 6(1), Feb 2022.\n\nXingyu Zhou. Differential privacy for bandits and rl, 2023. URL http://xingyuzhou.org/\n\nblog/notes/Differential-privacy-for-bandits-and-RL.\n\nXingyu Zhou and Jian Tan. Local differential privacy for bayesian optimization. Proceedings of the\n\nAAAI Conference on Artificial Intelligence, 35(12):11152–11159, May 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA OTHER RELATED WORK\n\nPrivate Multi-Armed Bandits. In addition to stochastic multi-armed bandits under the central model in Mishra & Thakurta (2015); Tossou & Dimitrakakis (2016); Sajed & Sheffet (2019), different variants of differentially private bandits have been studied, including adversarial bandits (Tossou & Dimitrakakis, 2017; Agarwal & Singh, 2017), heavy-tailed bandits (Tao et al., 2022), combinatorial semi-bandits (Chen et al., 2020), and cascading bandits (Wang et al., 2022). MAB under the local model is first studied in Ren et al. (2020) for pure DP and in Zheng et al. (2020) for appromixate DP. The local model have also been considered in Tao et al. (2022); Chen et al. (2020); Wang et al. (2022); Zhou & Tan (2021). Motivated by the regret gap between the central model and local model (see Table 1), Tenenbaum et al. (2021) consider MAB in the distributed model via secure shuffling where, however, only approximate DP is achieved and the resultant regret bound still has a gap with respect to the one under the central model.\n\nPrivate Contextual Bandits. In contextual bandits, in addition to the reward, the contexts are also sensitive information that need to be protected. However, a straightforward adaptation of the standard central DP in contextual bandits will lead to a linear regret, as proved in Shariff & Sheffet (2018). Thus, to provide a meaningful regret under the central model for contextual bandits, a relaxed version of DP called joint differential privacy is considered, which rougly means that the change of any user would not change the actions prescribed to all other users, but it allows the change of the action prescribed to herself. Under this central JDP, Shariff & Sheffet (2018) establishes a regret bound of (cid:101)O(√T ) for a private variant of LinUCB. On the other hand, contextual linear bandits under the local model incurs a regret bound of (cid:101)O(T 3/4) (Zheng et al., 2020). Very recently, motivated by the regret gap between the central and local model, two concurrent works consider contextual linear bandits in the distributed model (via secure shuffling only) (Chowdhury & Zhou, 2022b; Garcelon et al., 2022). In particular, Chowdhury & Zhou (2022b) show that a (cid:101)O(T 3/5) regret bound is achieved under the distributed model via secure shuffling.\n\nPrivate Reinforcement Learning (RL). RL is a generalization of contextual bandits in that contextual bandits can be viewed as a finite-horizon RL with horizon H = 1. This not only directly means that one also has to consider JDP in the central model for RL, but implies that RL becomes harder for privacy protection due to the additional state transition (i.e., H > 1). Tabular episodic RL under central JDP is first studied in Vietri et al. (2020) with an additive privacy cost while under the local model, a multiplicative privacy cost is shown to be necessary (Garcelon et al., 2021). In addition to valued-based algorithms considered in Vietri et al. (2020); Garcelon et al. (2021), similar performance is established for policy-based algorithms in tabular episodic RL under the central and local model (Chowdhury & Zhou, 2022a). Beyond the tabular setting, differentially private LQR control is studied in Chowdhury et al. (2021). More recently, private episodic RL with linear function approximation has been investigated in Luyo et al. (2021); Zhou (2022); Liao et al. (2021) under both central and local models, where similar regret gap as in contextual bandits exist (i.e., (cid:101)O(√T ) vs. (cid:101)O(T 3/4)).\n\nWe have also given more discussions on private (federated) supervised learning under the distributed model in Appendix G. For readers who are interested in the subtlety of DP definitions for bandits and RL, we refer to the blog post (Zhou, 2023).\n\nB A POSSIBLE APPROACH TO REDUCE THE COMMUNICATION COST\n\nWe first recall that the communication cost for RDP with SecAgg is roughly O(log(n+s/ε)), where n is the batch size and s is the scaling factor. A large s leads to better privacy and regret as shown in Theorem 2, but incurs a larger communication.\n\nOne can observe that the current communication cost is inverse with respect to ε. However, we tend to believe that one can break the privacy-communication trade-off above using a very recent technique proposed in Chen et al. (2022a). In particular, Chen et al. (2022a) shows that the fundamental communication cost for RDP with SecAgg for the mean estimation task scales with Ω(max(log(n2ε2), 1)), where n is the batch size. That is, for a stronger privacy guarantee (i.e., a smaller ε), each user should send less number of bits. The intuition is that if a user sends less bits, then she communicates less information about her local data (hence natural protection of privacy).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTo achieve this improvement, Chen et al. (2022a) proposes to use a linear compression scheme based on sparse random projections and distributed discrete Gaussian noise. Now, to apply the same technique to the private bandit problem, one needs to handle a different utility metric – that is, instead of the mean-square error in Chen et al. (2022a), one now needs to examine the tail concentration behavior. We leave it as an interesting future work.\n\nC A GENERAL REGRET BOUND OF ALGORITHM 1\n\nIn this section, we present the proof of our generic regret bound in Lemma 1.\n\nRecall that na(b) := Ra(b) during batch b. We consider the following tail property on na(b).\n\n−\n\na(b) is the total noise injected in the sum of rewards for arm a\n\n(cid:80)l(b)\n\ni=1 ri\n\nAssumption 1 (Concentration of Private Noise). Fix any p non-negative constants σ, h (possibly depending on b) such that, with probability at least 1\n\n(0, 1], a\n\n[K], b\n\n≥\n\n∈\n\n∈\n\n1, there exist\n\n2p,\n\n−\n\nna(b)\n\n| ≤\n\n|\n\nσ(cid:112)log(2/p) + h log(2/p).\n\nWe remark that this assumption naturally holds for a single (σ2, h)-sub-exponential noise and a single σ2-sub-Gaussian noise where h = 0 (cf. Lemma 7 and Lemma 6 in Appendix K) with constants adjustment.\n\nLemma 2 (Formal statement of Lemma 1). Let Assumption 1 hold and choose confidence radius σ(cid:112)log(2 |\nl(b)\n\nΦ(b) h log(2 |\n| l(b)\n\nΦ(b) log(4 |\n| 2l(b)\n\nΦ(b) |\n\nβ(b) =\n\nb2/p)\n\nb2/p)\n\nb2/p)\n\n(cid:115)\n\n+\n\n+\n\n,\n\n(2)\n\nis the number of active arms in batch b. Then, for any p\n\nwhere least 1\n\nΦ(b) |\n\n| −\n\n3p, the regret of Algorithm 1 satisfies\n\nReg(T ) = O\n\n\n\n\n\n(cid:88)\n\na∈[K]:\n\nlog(KT /p) ∆a\n\n+ Kσ(cid:112)log(KT /p) + Kh log(KT /p)\n\n .\n\n(0, 1], with probability at\n\n∈\n\n\n\nTaking p = 1/T and assuming T\n\nE [Reg(T )] = O\n\n≥ \n\n\n\nK, yields the expected regret\n\nlog T ∆a\n\n+ Kσ(cid:112)log T + Khlog T\n\n\n\n .\n\n(cid:88)\n\na∈[K]:∆>0\n\nb be the event that for all active arms\n\nProof. Let first show that with the choice of β(b) given by equation 2, we have P [ under Assumption 1. To see this, we note that\n\n|(cid:98)μa(b)\n\n| ≤\n\nμa\n\n−\n\nE\n\nβ(b) and ]\n\n≥\n\nE\n\nE 1\n−\n\n= b≥1 3p for any p\n\nb. Then, we (0, 1]\n\n∪\n\nE\n\n∈\n\n(cid:98)μa(b)\n\n−\n\nμa =\n\nna(b) + (cid:80)l(b)\n\ni=1 ri\n\na(b)\n\nl(b)\n\nBy Hoeffeding’s inequality (cf. Lemma 8), we have for any p 1\n\np,\n\n−\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:80)l(b)\n\ni=1 ri l(b)\n\na(b)\n\n(cid:115)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n=\n\nμa\n\n−\n\nlog(2/p) 2l(b)\n\n.\n\nμa.\n\n(0, 1), with probability at least\n\n−\n\n∈\n\nThen, by the concentration of noise na(b) in Assumption 1 and triangle inequality, we obtain for a given arm a and batch b, with probability at least 1\n\n3p\n\n|(cid:98)μa(b)\n\n−\n\nμa\n\n|\n\n=\n\n(cid:115)\n\nlog(2/p) 2l(b)\n\n+\n\n− σ(cid:112)log(1/p) l(b)\n\n+\n\nh log(1/p) l(b)\n\n.\n\nThus, by the choice of β(b) and a union bound, we have P [\n\n]\n\n1\n\n3p.\n\nE\n\n≥\n\n−\n\n. We first show that the optimal arm a∗ will In the following, we condition on the good event always be active. We show this by contradiction. Suppose at the end of some batch b, a∗ will be\n\nE\n\n15\n\nPublished as a conference paper at ICLR 2023\n\neliminated, i.e., UCBa∗ (b) < LCBa′(b) for some a′. This implies that under good event ≤ (cid:98)μa∗ (b) + β(b) < (cid:98)μa′(b)\n\nβ(b)\n\nμa′,\n\nμa∗\n\nwhich contradicts the fact that a∗ is the optimal arm.\n\n−\n\n≤\n\nE\n\nThen, we show that at the end of batch b, all arms such that ∆a > 4β(b) will be eliminated. To show this, we have that under good event (cid:98)μa(b) + β(b)\n\nμa(b) + 2β(b) < μa∗ (b)\n\n4β(b) + 2β(b)\n\nβ(b),\n\nE\n\nwhich implies that arm a will be eliminated by the rule. Thus, for each sub-optimal arm a, let ̃ba be the last batch that arm a is not eliminated. By the above result, we have\n\n≤ (cid:98)μa∗ (b)\n\n−\n\n−\n\n≤\n\n4β( ̃ba) = O\n\n∆a\n\n≤\n\n(cid:32)(cid:115)\n\nlog(KT /p) l( ̃ba)\n\n+\n\nσ(cid:112)log(KT /p) l( ̃ba)\n\n+\n\nh log(KT /p) l( ̃ba)\n\n(cid:33)\n\n.\n\nHence, we have for some absolute constants c1, c2, c3,\n\nl( ̃ba)\n\nmax\n\n≤\n\n(cid:40)\n\nc1 log(KT /p) ∆2 a\n\n,\n\n,\n\nc2σ(cid:112)log(KT /p) ∆a\n\nc3h log(KT /p) ∆a 4l( ̃ba) for each sub-optimal arm a. Therefore, K, we\n\n≤\n\n4l( ̃ba)∆a. Moreover, choose p = 1/T and assume T\n\n(cid:41)\n\n≥\n\nSince the batch size doubles, we have Na(T ) Reg(T ) = (cid:80) have that the expected regret satisfies\n\na∈[K] Na(T )∆a\n\n≤\n\nReg(T ) = E\n\n\n\n\n\n(cid:88)\n\na∈[K]\n\n\n\n∆aNa(T )\n\n\n\n(cid:3) P (cid:2) ̄ E\n\n·\n\n≤\n\nT + O\n\n\n\n\n\n= O\n\n(cid:88)\n\na∈[K]:∆>0\n\n\n\n\n\n(cid:88)\n\na∈[K]:∆>0\n\n\n\n + O\n\nlog T ∆a\n\n(cid:16)\n\nKσ(cid:112)log T\n\n(cid:17)\n\n+ O (Khlog T )\n\nlog T ∆a\n\n+ Kσ(cid:112)log T + Khlog T\n\n\n\n .\n\nRemark 4. In stead of a doubling batch schedule, one can also set l(b) = ηb for some absolute constant η > 1 while attaining the same order of regret bound.\n\nD APPENDIX FOR PURE DP IN SECTION 4\n\nIn this section, we provide proofs for Theorem 1 and Theorem 3, which show that pure DP can be achieved under the distributed model via SecAgg and secure shuffling, respectively. Both results build on the generic regret bound in Lemma 2.\n\nD.1 PROOF OF THEOREM 1\n\nProof. Privacy: We need to show that the server’s view at each batch has already satisfies (ε, 0)-DP, which combined with the fact of unique users and parallel composition, yields that Algorithm 1 satisfies (ε, 0)-DP in the distributed model. To this end, in the following, we fix a batch b and arm a, and hence xi = ri\n\na(b) and n = l(b). Note that the server’s view for each batch is given by\n\n\n\n\n\n\n\n\n\n(cid:88)\n\n(a) =\n\n(cid:98)y\n\n\n\ni∈[n]\n\nyi\n\n mod m\n\n(cid:88)\n\n(b) =\n\n\n\ni∈[n]\n\n(cid:98)xi + ηi\n\n mod m,\n\n(3)\n\nwhere (a) holds by SecAgg function; (b) holds by the distributive property: (a + b) mod c = Z. Thus, the view of the server can be simulated as a (a mod c + b mod c) mod c for any a, b, c i ηi. post-processing of a mechanism H\nis (ε, 0)-DP by post-processing of DP. To this end, we note that the Hence, it suffices to show that sensitivity of (cid:80) i ηi needs to be distributed as LapZ(g/ε)\n\ni (cid:98)xi is g, which, by Fact 3, implies that (cid:80)\n\ni and outputs (cid:80) }\n\nthat accepts an input dataset\n\ni (cid:98)xi + (cid:80)\n\n{(cid:98)xi\n\nH\n\n∈\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nin order to guarantee ε-DP. Finally, by Fact 1, it suffices to generate ηi = γ+ γ− i are i.i.d samples from P ́olya(1/n, e−ε/g). Regret: Thanks to the generic regret bound in Lemma 2, we only need to verify Assumption 1. To this end, fix any batch b and arm a, we have (cid:98)y = (cid:98)ya(b), xi = ri a(b) and n = l(b). Then, in the following we will show that with probability at least 1\n\ni , where γ+ γ−\n\ni and\n\ni −\n\n2p\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\nO\n\n(cid:18) 1 ε\n\n− (cid:112)log(1/p) +\n\n(cid:19)\n\nlog(1/p)\n\n,\n\n1 ε\n\n(4)\n\nwhich implies that Assumption 1 holds with σ = O(1/ε) and h = O(1/ε).\n\nInspired by Cheu & Yan (2021); Balle et al. (2020), we first divide the LHS of equation 4 as follows.\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A (cid:124)\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\ni\n\n(cid:123)(cid:122) Term (i)\n\n+\n\n(cid:98)xi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\n1 g\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:124)\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(cid:98)xi\n\n− (cid:123)(cid:122) Term (ii)\n\n,\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\nwhere Term (i) captures the error due to private noise and modular operation, and Term (ii) captures the error due to random rounding.\n\nTo start with, we will bound Term (ii). More specifically, we will show that for any p probability at least 1\n\np,\n\n−\n\nTerm (ii)\n\n(cid:18) 1 ε\n\nO\n\n≤\n\n(cid:112)log(1/p)\n\n(cid:19)\n\n.\n\n(0, 1], with\n\n∈\n\n(5)\n\n·\n\ng\n\nxi\n\nLet ̄xi := , then (cid:98)xi = ̄xi +Ber(xig ⌊\n⌋ ιi := ̄xi + Ber(xig (cid:80) (cid:80) (cid:80) −\nand as a result, 1 of sub-Gaussian (cf. Lemma 6), we have\n\n− i (cid:98)xi\n\n ̄xi)\n\n−\n\ng\n\ng\n\n ̄xi) = xig + ̄xi +Ber(xig\n\nxig. We have E [ιi] = 0 and ιi i xi = 1\n\nxig = xig +ιi, where −\n− 1, 1]. Hence, ιi is 1-sub-Gaussian i ιi is n/g2-sub-Gaussian. Therefore, by the concentration\n\n ̄xi)\n\n−\n\n−\n\n∈\n\n[\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\n1\n\ng · (cid:98)xi\n\n(cid:88)\n\n−\n\ni\n\n(cid:114)\n\n2\n\n>\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nxi\n\nn\n\n(cid:35)\n\ng2 log(2/p)\n\np.\n\n≤\n\ni ε√n\n\nHence, by the choice of g =\n\n, we establish equation 5.\n\n⌉ Now, we turn to bound Term (i). Recall the choice of parameters: g = and m = ng + 2τ + 1. We would like to show that\n\n⌈\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\n(cid:98)xi\n\ni\n\n(cid:35)\n\nτ g\n\n>\n\np,\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nwhich implies that for any p\n\n∈\n\n(0, 1], with probability at least 1 (cid:19)\n\nTerm (i)\n\nO\n\n≤\n\nlog(2/p)\n\n(cid:18) 1 ε\n\np\n\n− .\n\nε√n\n\n⌈\n\n⌉\n\n, τ =\n\ng\n\nε log(2/p) ⌉\n\n,\n\n⌈\n\n(6)\n\n(7)\n\nTo show equation 6, the key is to bound the error due to private noise and handle the possible underflow carefully. First, we know that the total private noise (cid:80) i ηi is distributed as LapZ(g/ε). Hence, by the concentration of discrete Laplace (cf. Fact 3), we have\n\n(cid:35)\n\nP\n\n(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) [(cid:80) i (cid:98)xi p. In the following, we condition on the event of\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i (cid:98)xi + ηi\n\n≤ τ, (cid:80)\n\n> τ\n\nηi\n\np.\n\n−\n\n∈\n\ni\n\n1\n\nE\n\nnoise]\n\nnoise denote the event that (cid:80) Let E\nwe have P [\n\n((cid:98)y). As already shown in equation 3, the input (cid:98)y = ((cid:80)\n\nE A\nWe let y = (cid:98)y. We will consider two cases of y as in the analyzer subroutine Case 1: y > ng + τ . We argue that this happens only when (cid:80) This is because for all i\n\nτ, 0), i.e., underflow. i (cid:98)xi + ηi [0, g], m = ng + 2τ + 1 and the total privacy noise is at most τ\n\ni (cid:98)xi + τ ], then by the above inequality, noise to analyze the output i (cid:98)xi + ηi) mod m is already an integer. .\nA [\n\n−\n\n−\n\n≥\n\n∈\n\n[n], (cid:98)xi\n\n∈\n\n∈\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nunder\n\nE\n\nnoise. Therefore,\n\nm =\n\ny\n\n−\n\n(cid:32)(cid:32)\n\n(cid:88)\n\ni\n\n(cid:33)\n\n(cid:33)\n\n(cid:98)xi + ηi\n\nmod m\n\nm\n\n−\n\n(cid:32)\n\n=\n\nm +\n\n(cid:88)\n\ni\n\n(cid:33)\n\n(cid:98)xi + ηi\n\nm\n\n−\n\n(cid:88)\n\n=\n\ni\n\n(cid:98)xi + ηi.\n\nThat is, y when y > ng + τ ,\n\nm\n\n−\n\n∈\n\n[(cid:80)\n\nA\n\nτ, (cid:80) i (cid:98)xi −\n((cid:98)y) = y−m\n\ng\n\ni (cid:98)xi + τ ] with high probability. In other words, we have shown that satisfies equation 6.\n\n≤\n\nCase 2: y since m = ng + 2τ + 1, which implies that\n\nng + τ . Here, we have noisy sum (cid:80)\n\n((cid:98)y) = y Hence, we have shown that the output of the analyzer under both cases satisfies equation 6, which implies equation 7. Combined with the bound in equation 5, yields the bound in equation 4. Finally, plugging in σ = O(1/ε), h = O(1/ε) into the generic regret bound in Lemma 2, yields the required regret bound and completes the proof.\n\ng satisfies equation 6.\n\n[0, ng + τ ]. Hence, y = (cid:80)\n\ni (cid:98)xi + ηi\n\ni (cid:98)xi + ηi\n\nA\n\n∈\n\nD.2 PURE DP VIA SHUFFLING\n\nAs stated in the main paper (see Remark 1), one can achieve same privacy and regret guarantees (orderwise) using a relaxed SecAgg protocol, which relaxes the SecAgg protocol mentioned above in the following sense: (i) relaxed correctness – the output of can be used to compute the correct modular sum except at most a small probability (denoted by (cid:98)q); (ii) relaxed security – the output of reveals only (cid:98)ε more information than the modular sum result. Putting the two aspects together, one S\nobtains a relaxed protocol denoted by ((cid:98)ε, (cid:98)q)-SecAgg. One important benefit of using this relaxation is that it allows us to achieve the same results of Theorem 1 via secure shuffling. More specifically, as shown in Cheu & Yan (2021), there exists a shuffle protocol that can simulate an ((cid:98)ε, (cid:98)q)-SecAgg. Hence, we can directly instantiate using this shuffle protocol to achieve pure DP in the distributed model while obtaining the same regret bound as in the central model. We provide more details on this shuffle protocol below.\n\nS\n\nS\n\nTo facilitate our discussion, we briefly give more formal definitions of a relaxed SecAgg based on Cheu & Yan (2021), which will also be used in our next proof. We denote a perfect SecAgg as Σ.\n\nTo start with, we first need the following two distance metrics between two probability distributions. As in Cheu & Yan (2021), for a given distribution D and event E, we write P [D E]. We let supp(D, D′) be the union of their supports. Definition 6 (Statistical Distance). For any pair of distributions D, D′, the statistical distance is given by\n\nE] for Pη∼D[η\n\n∈\n\n∈\n\nSD(D, D′) :=\n\nmax E∈supp(D,D′) |\n\nP [D\n\nE]\n\n−\n\n∈\n\nP [D′\n\nE]\n\n.\n\n|\n\n∈\n\nDefinition 7 (Log-Likelihood-Ratio (LLR) Distance). For any pair of distributions D, D′, the LLR distance is given by\n\nLLR(D, D′) :=\n\nmax E∈supp(D,D′)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nlog\n\n(cid:18) P [D P [D′\n\nE] E]\n\n(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\n∈ ∈\n\nPure DP can be defined using LLR distance.\n\nDefinition 8. A randomized mechanism (D′)) LLR(\n\n(D),\n\nε\n\nM\n\nM\n\n≤\n\nis (ε, 0)-DP if for any two neighboring datasets D, D′,\n\nM\n\nDefinition 9 (Relaxed SecAgg). We say if it satisfies the following two conditions for any input y.\n\nS\n\nis an ((cid:98)ε, (cid:98)q)-relaxation of Σ (i.e., ((cid:98)ε, (cid:98)q)-relaxed SecAgg)\n\n1. ((cid:98)q-relaxed correctness) There exists some post-processing function POST such that\n\nSD(POST(\n\n(y)), Σ(y))\n\nS\n\n≤ (cid:98)q.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n2. ((cid:98)ε-relaxed security) There exists a simulator SIM such that LLR(\n\n(y), SIM(Σ(y)))\n\n≤ (cid:98)ε.\n\nS\n\nR\n\n. More specifically, we let\n\nCheu & Yan (2021) show that one can simulate a relaxed SecAgg via a shuffle protocol. We now briefly talk about the high-level idea behind this idea and refer readers to Cheu & Yan (2021) for details. To simulate an ((cid:98)ε, (cid:98)q)-SecAgg via shuffling, the key is to introduce another local randomizer 1, on top of the original ≥\nS where R denotes the additional local randomizer at each user i (i.e., yi) into a random binary vector of a particular length d. Then, S denotes a standard shuffler that d bits). The nice uniformly at random permutes all the received bits from n users (i.e., a total of n ·\nRn can simulate an ((cid:98)ε, (cid:98)q)-SecAgg for construction of R in Cheu & Yan (2021) ensures that any (cid:98)ε, (cid:98)q The following theorem says that a relaxed SecAgg is sufficient for the same order of expected regret bound while guaranteeing pure DP.\n\nRn in Algorithm 1 with n = l(b), b\n\n[n] that maps the output of\n\n(0, 1).\n\n:= S\n\n= S\n\nR\n\n∈\n\n∈\n\nS\n\n◦\n\n◦\n\nTheorem 3 (Pure-DP via Shuffling). Fix ε > 0 and T simulated via shuffling) for Algorithm 16. Let noise for i-th user be ηi = γ+ P ́olya(1/n, e−ε/g). For each batch b, choose n = l(b), g = m = ng + 2τ + 1, (cid:98)ε = ε/4, ε′ = ε/2, (cid:98)q = p′ = 1 in the distributed model. Moreover, setting β(b) = O\n\nN and consider an ((cid:98)ε, (cid:98)q)-SecAgg (e.g., i , where γ+ γ−\n\ni.i.d. i\n∼ , and 2T . Then, Algorithm 1 achieves (ε, 0)-DP\n\n(cid:18)(cid:113) log(|Φ(b)|b2T )\n\nε′ log(2/p′)\n\ni − , τ =\n\n, it enjoys\n\ni , γ−\n\nε′√n\n\n+\n\n(cid:19)\n\n∈\n\n⌉\n\n⌈\n\n⌉\n\n⌈\n\ng\n\n2 log(|Φ(b)|b2T ) εl(b)\n\n2l(b)\n\nexpected regret\n\nE [Reg(T )] = O\n\n(cid:18)(cid:88)\n\na∈[K]:∆a>0\n\nlog T ∆a\n\n+\n\nK log T ε\n\n(cid:19)\n\n.\n\nMoreover, the communication per user before\n\nis O(log m) bits.\n\nS\n\nProof. This proof shares the same idea as in the proof of Theorem 1. We only need to highlight the difference.\n\nPrivacy: As before, we need to ensure that the view of the server already satisfies ε-DP for each batch b\n\n1. In the following, we fix any batch b and arm a, we have xi = ri\n\na(b) and n = l(b).\n\n≥\n\nThus, by Definition 8, it suffices to show that for any two neighboring datasets ε.\n\nLLR(\n\n(x1), . . . ,\n\n(xn)),\n\n(x′\n\n(x′\n\n(\n\n(\n\n1), . . . ,\n\nn)))\n\nS\n\nR\n\nR\n\nS\n\nR\n\nR\n\n≤\n\nTo this end, by the (cid:98)ε-relaxed security of (x1), . . . , (cid:88)\n\nLLR(\n\nR 2(cid:98)ε + LLR(SIM(\n\nS\n\n(\n\n≤\n\nS\n\nR\n\nR\n\ni\n\nused in Algorithm 1 and triangle inequality, we have n))) (xn)), (x′\n\nR (xi) mod m), SIM(\n\ni) mod m))\n\n1), . . . ,\n\nR (cid:88)\n\n(x′\n\n(x′\n\nS\n\n(\n\nR\n\ni\n\n2(cid:98)ε + LLR(\n\n≤\n\n(cid:88)\n\nR\n\ni\n\n(xi) mod m,\n\n(cid:88)\n\ni\n\n(x′\n\ni) mod m),\n\nR\n\nwhere the last inequality follows from data processing inequality. The remaining step to bound the second term above is the same as in the proof of Theorem 1. With (cid:98)ε = ε/4 and ε′ = ε/2, we have the total privacy loss is ε.\n\nRegret: As before, the key is to establish that with high probability\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\nO\n\n(cid:18) 1 ε\n\n(cid:112)log(2/p′) +\n\n(cid:19)\n\nlog(2/p′)\n\n,\n\n1 ε\n\n(8)\n\nwhere (cid:98)y := (cid:98)ya(b).\n\n6Since the output of a shuffling protocols is a multiset, we need to first compute ((cid:80)\n\nthe subroutine A in Algorithm 1.\n\n(cid:98)y) mod m as the (cid:98)y for\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nWe again can divide the LHS of equation 8 into\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A (cid:124)\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\ni\n\n(cid:123)(cid:122) Term (i)\n\n(cid:98)xi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\n+\n\n1 g\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:124)\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(cid:98)xi\n\n− (cid:123)(cid:122) Term (ii)\n\n.\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\nIn particular, Term (ii) can be bounded by using the same method, i.e., for any p′ probability at least 1\n\np′,\n\n(0, 1], with\n\n∈\n\n−\n\nTerm (ii)\n\n(cid:18) 1 ε′\n\nO\n\n≤\n\n(cid:112)log(2/p′)\n\n(cid:19)\n\n.\n\nFor Term (i), we would like to show that\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:98)xi\n\n(cid:35)\n\nτ g\n\n>\n\n≤ (cid:98)q + p′.\n\nThis can be established by the same steps in the proof of Theorem 1 while conditioning on the high is (cid:98)q-relaxed SecAgg. More specifically, compared to the proof of Theorem 1, probability event that − (cid:98)q by the\n\ni yi) mod m only holds with high probability 1\n\nS\n\nthe key difference here is that (cid:98)y = ((cid:80) definition of (cid:98)q-relaxed SecAgg. Thus, for any p′\n\n(cid:16) 1\n\nε\n\nO\n\n(cid:112)log(2/p′) + 1\n\n∈\n\n(0, 1), let (cid:98)q = p′, we have with probability at least 1 ε log(2/p′)\n\n| ≤ , which implies that Assumption 1 holds with σ = O(1/ε) and\n\n((cid:98)y)\n\n3p′,\n\ni xi\n\n|A\n\n−\n\n−\n\n(cid:17)\n\n(cid:80)\n\nh = O(1/ε).\n\nE APPENDIX FOR RDP IN SECTION 5\n\nE.1 PROOF OF PROPOSITION 1\n\nProof. We first establish the following result.\n\nClaim 1. For all λ\n\nR, we have\n\n∈\n\ncosh(λ)\n\neλ2/2.\n\n≤\n\nTo show this, by the infinite product representation of the hyperbolic cosine function, we have (cid:19) (a)\n\n(cid:33)\n\n(cid:18)\n\n(cid:32) ∞ (cid:88)\n\nexp\n\n4λ2\n\n(b)\n\n= exp(λ2/2),\n\ncosh(λ) =\n\n1 +\n\n∞ (cid:89)\n\nk=1\n\n4λ2\n\n−\n\nπ2(2k\n\n1)2\n\n≤\n\nπ2(2k\n\n1)2\n\nk=1\n\n− R and (b) follows from (cid:80)∞\n\nwhere (a) holds by the fact that 1 + x\n\n≤\n\nex for all x\n\n(2k−1)2 = π2 8 . Sk(0, σ2), then its moment generating function (MGF) is given by E (cid:2)eλX (cid:3) = 1). Further,\n\n∼ 1)). Hence, by the above claim, we have E (cid:2)eλX (cid:3) 2x for x\n\nexp(σ2(eλ2/2\n\nk=1\n\n≤\n\n−\n\n∈\n\n1\n\n∈\n\n[0, 1]. Thus, for E (cid:2)eλX (cid:3)\n\nλ | ≤ |\neλ2σ2\n\n√2, we have λ22σ2 2\n\n= e\n\n.\n\nThen, note that X exp(σ2(cosh(λ) note that ex 1\n\n− ≤\n\n−\n\n≤ Hence, by the definition of sub-exponential random variable (cf. Lemma 7), X is (2σ2, exponential, which again by Lemma 7 implies the required concentration result, i.e., for any p (0, 1], with probability at least 1\n\np,\n\n√\n\n2\n\n2 )-sub-\n\n∈\n\n− X\n|\n\n| ≤\n\n2σ(cid:112)log(2/p) + √2 log(2/p).\n\nE.2 PROOF OF THEOREM 2\n\nWe will leverage the following result in (Agarwal et al., 2021, Theorem 3.5 ) to prove privacy guarantee.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nZ, α > 1, let X Lemma 3. For α sensitivity-∆ query satisfies (α, ε(α))-RDP with ε(α) given by\n\n∼\n\n∈\n\nSk(0, σ2). Then, an algorithm M that adds X to a\n\nε(α)\n\n≤\n\nα∆2 2σ2 + min\n\n(cid:26) (2α\n\n−\n\n1)∆2 + 6∆ 4σ4\n\n,\n\n3∆ 2σ2\n\n(cid:27)\n\n.\n\nProof of Theorem 2. Privacy: As before, we fix any batch b and arm a, for simplicity, we let xi = ri a(b) and n = l(b). Then, it suffices to show that the mechanism that accepts an input dataset i and outputs (cid:80) {(cid:98)xi generates }\nSk(0, g2 noise ηi i ηi Sk(0, g2 g =\n\n∼ ε2 ). Thus, by Lemma 3 with ∆ = g, we have that for each batch b with n = l(b) and sε√n\n\nnε2 ) and Skellam is closed under summation, we have that the total noise (cid:80)\n\ni ηi is private. To this end, since each local randomizer\n\n, Algorithm 1 is (α, (cid:98)εn(α))-RDP with (cid:98)εn(α) given by\n\ni (cid:98)xi + (cid:80)\n\nR\n\nH\n\n∼\n\n⌈\n\n⌉\n\n(cid:98)εn(α) =\n\n+ min\n\nαε2 2\n\n(cid:26) (2α\n\n1)ε2\n\n− 4s2n\n\n+\n\n3ε 2s3n3/2\n\n,\n\n(cid:27)\n\n.\n\n3ε2 2s√n\n\nSince n = l(b) > 1, we have that for all batches b\n\n(cid:98)εn(α)\n\n≤ (cid:98)ε(α) :=\n\nαε2 2\n\n+ min\n\n1, ≥\n(cid:26) (2α\n\n1)ε2\n\n− 4s2\n\n+\n\n3ε\n\n2s3 ,\n\n3ε2 2s\n\n(cid:27)\n\n.\n\nRegret: We will establish the following high probability bound so that we can apply our generic regret bound in Lemma 2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A where (cid:98)y := (cid:98)ya(b) for each batch b and arm a. We again divide the LHS of equation 9 into\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\n((cid:98)y)\n\n(cid:88)\n\nxi\n\n−\n\nO\n\ni\n\n(cid:18) 1 ε\n\n(cid:112)log(1/p) +\n\n(cid:19)\n\nlog(2/p)\n\n,\n\n1 sε\n\n(9)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A (cid:124)\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\ni\n\n(cid:123)(cid:122) Term (i)\n\n(cid:98)xi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\n+\n\n1 g\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:124)\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(cid:98)xi\n\n− (cid:123)(cid:122) Term (ii)\n\n.\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\nIn particular, Term (ii) can be bounded by using the same method before, i.e.,\n\nHence, by the choice of g =\n\n⌈\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\n1\n\ng · (cid:98)xi\n\ni sε√n\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nxi\n\n(cid:114)\n\n2\n\n>\n\nn\n\n(cid:35)\n\ng2 log(2/p)\n\np.\n\n≤\n\n, we establish that with high probability\n\n⌉ Term (ii)\n\nO\n\n≤\n\n(cid:18) 1 ε\n\n·\n\ns\n\n(cid:112)log(1/p)\n\n(cid:19)\n\n.\n\nFor Term (i), as in the previous proof, the key is to show that\n\nP\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n(cid:88)\n\nηi\n\ni∈[n]\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n\n\n> τ\n\np.\n\n≤\n\nTo this end, we will utilize our established result in Proposition 1. Note that the total noise (cid:80) (cid:112)log(2/p) + √2 log(2/p) Sk(0, g2 have the above result. Following previous proof, this result implies that with high probability\n\nε2 ), and hence by Proposition 1 and the choice of τ =\n\n2g ε\n\n⌈\n\n⌉\n\ni ηi\n\n∼ , we\n\nTerm (i)\n\nτ g ≤\n\nO\n\n(cid:18) 1 ε\n\n≤\n\n(cid:112)log(1/p) +\n\n(cid:19)\n\nlog(1/p)\n\n.\n\n1 sε\n\nCombining the bounds on Term (i) and Term (ii), we have that the private noise satisfies Assumption 1 with constants σ = O(1/ε) and h = O( 1 sε ). Hence, by the generic regret bound in Lemma 2, we have established the required result.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nF ACHIEVING CDP IN THE DISTRIBUTED MODEL VIA DISCRETE GAUSSIAN\n\nWe first introduce the following definition of concentrated differential privacy Bun & Steinke (2016). 2 ε2-CDP Definition 10 (Concentrated Differential Privacy). A randomized mechanism 2 ε2α. if for all neighboring datasets D, D′ and all α Remark 5 (CDP vs. RDP). From the definition, we can see that 1 (α, 1\n\nM 2 ε2-CDP is equivalent to satisfying\n\n2 ε2α)-RDP simultaneously for all α > 1.\n\nsatisfies 1 (D′))\n\n), we have Dα(\n\n(D),\n\nM\n\nM\n\n(1,\n\n∞\n\n≤\n\n∈\n\n1\n\nThe discrete Gaussian mechanism is first proposed and investigated by Canonne et al. (2020) and has been recently applied to federated learning Kairouz et al. (2021a). We apply it to bandit learning to demonstrate the flexibility of our proposed algorithm and analysis. Definition 11 (Discrete Gaussian Distribution). Let μ, σ has a discrete Gaussian distribution with location μ and scale σ, denoted by probability mass function given by\n\nR with σ > 0. A random variable X Z(μ, σ2), if it has a\n\nN\n\n∈\n\nZ, P [X = x] =\n\nx\n\n∀\n\n∈\n\ne−(x−μ)2/2σ2 y∈Z e−(y−μ)2/2σ2 .\n\n(cid:80)\n\nThe following result from Canonne et al. (2020) will be useful in our privacy and regret analysis. Z satisfy Fact 2 (Discrete Gaussian Privacy and Utility). Let ∆, ε > 0. Let q : X\n∆ for all x, x′ differing on a single entry. Define a randomized algorithm q(x) |\nZ(0, ∆2/ε2). Then, M satisfies 1 2 ε2-concentrated M : differential privacy (CDP). Moreover, Y is ∆2/ε2-sub-Gaussian and hence for all t\n\nZ by M (x) = q(x)+Y , where Y\n\nq(x′)\n\n∼ N\n\n| ≤\n\n→\n\n→\n\n− n\n\n0,\n\nX\n\nn\n\n≥\n\nP [ Y\n|\n\nt]\n\n≤\n\n| ≥\n\n2e−t2ε2/(2∆2).\n\nHowever, a direct application of above results does not work. This is because the sum of discrete Gaussian is not a discrete Gaussian, and hence one cannot directly apply the privacy guarantee of discrete Gaussian when analyzing the view of the analyzer via summing all the noise from users. To overcome this, we will rely on a recent result in Kairouz et al. (2021a), which shows that under reasonable parameter regimes, the sum of discrete Gaussian is close to a discrete Gaussian. The regret analysis will again build on the generic regret bound for Algorithm 1. Theorem 4 (CDP via SecAgg). Fix ε ηi and m = ng + 2τ + 1. Then, Algorithm 1 achieves 1\n\n(0, 1) and a scaling factor s\n\n1. Let noise for i-th user be\n\n1, choose n = l(b), g =\n\nnε2 ). For each b\n\n(cid:112)2 log(2/p)\n\n, p = 1/T\n\nZ(0, g2\n\n≥ , τ =\n\nsε√n\n\n∼ N\n\n≥\n\n∈\n\ng ε\n\n⌉\n\n(cid:40)(cid:114)\n\nε2 +\n\nmin\n\n(cid:98)ε\n\n≤\n\n⌈\n\n⌈\n\n⌉ 2 (cid:98)ε2-CDP with (cid:98)ε given by 1\n2\n\nξ, ε + ξ\n\n(cid:41)\n\n,\n\nwhere ξ := 10\n\n·\n\n(cid:80) T\n\n2 −1\n\nk=1 e−2π2s2· k\n\nk+1 . Meanwhile, the regret is given by\n\n\n\n\n\n(cid:88)\n\na∈[K]:∆>0\n\nlog T ∆a\n\n+\n\nK√log T ε\n\n\n\n ,\n\nReg(T ) = O\n\nand the communication messages per user before\n\nare O(log m) bits.\n\nS Remark 6 (Privacy-Communication Trade-off). We can observe an interesting trade-off between privacy and communication cost. In particular, as s increases, ξ approaches zero and hence the privacy loss approaches the one under continuous Gaussian. However, a larger s leads to a larger m and hence a larger communication overhead.\n\nWe will leverage the following result in (Kairouz et al., 2021a, Proposition 13) to prove privacy. Lemma 4 (Privacy for the sum of discrete Gaussian). Let σ dently for each i. Let Zn = (cid:80)n satisfies 1\n\nZ(0, σ2) indepeni=1 Xi. Then, an algorithm M that adds Zn to a sensitivity-∆ query\n\n1/2. Let Xi\n\n2 ε2-CDP with ε given by\n\n∼ N\n\n≥\n\nε = min\n\n(cid:40)(cid:114)\n\n∆2 nσ2 +\n\n1 2\n\nξ,\n\n∆ √nσ\n\n(cid:41)\n\n+ ξ\n\n,\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nwhere ξ := 10\n\n·\n\n(cid:80)n−1\n\nk=1 e−2π2σ2 k\n\nk+1 .\n\nProof of Theorem 4. Privacy: As before, we fix any batch b and arm a, for simplicity, we let xi = ri that accepts an input generates dataset\n\na(b) and n = l(b). Then, it suffices to show that the mechanism {(cid:98)xi\n\ni and outputs (cid:80) }\n\ni ηi is private. To this end, each local randomizer 2 (cid:98)ε2\n\nnε2 ), and hence by Lemma 4 with ∆ = g, we have that\n\nn-concentrated\n\ni (cid:98)xi + (cid:80)\n\nis 1\n\nR\n\nH\n\nZ(0, g2 noise ηi differential privacy with (cid:98)εn given by\n\n∼ N\n\nH\n\n(cid:40)(cid:114)\n\n(cid:98)εn = min\n\nε2 +\n\n(cid:41)\n\nξn, ε + ξn\n\n,\n\n1 2\n\n(cid:80)n−1\n\nk=1 e−2π2 g2\n\n2 −1\n\nwhere ξn := 10 (cid:80) T k=1 e−2π2s2· k k+1 since n = l(b) differential privacy with (cid:98)ε given by\n\n·\n\n≤\n\nnε2 · k\n\n, we have τn k+1 . Note that g = ⌈\nT /2. Thus, we have that Algorithm 1 is is 1\n\nsε√n\n\n⌉\n\nτ := 10\n\n≤\n\n· 2 (cid:98)ε2-concentrated\n\n(cid:40)(cid:114)\n\n(cid:41)\n\nξ, ε + ξ\n\n,\n\n1 2\n\n(cid:98)ε = min\n\nε2 +\n\n(cid:80) T\n\n2 −1\n\nk=1 e−2π2s2· k\n\nk+1 .\n\nwhere ξ := 10\n\n·\n\nRegret: We will establish the following high probability bound so that we can apply our generic regret bound in Lemma 2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\nO\n\n(cid:18) 1 ε\n\n(cid:112)log(1/p)\n\n(cid:19)\n\n,\n\n(10)\n\nwhere (cid:98)y := (cid:98)ya(b). We again divide the LHS of equation 10 into\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A (cid:124)\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\ni\n\n(cid:123)(cid:122) Term (i)\n\n(cid:98)xi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\n+\n\n1 g\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:124)\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(cid:98)xi\n\n− (cid:123)(cid:122) Term (ii)\n\n.\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\nIn particular, Term (ii) can be bounded by using the same method before, i.e.,\n\nHence, by the choice of g =\n\n⌈\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\n1\n\ng · (cid:98)xi\n\ni sε√n\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nxi\n\n(cid:114)\n\n2\n\n>\n\nn\n\n(cid:35)\n\ng2 log(2/p)\n\np.\n\n≤\n\n, we establish that with high probability\n\n⌉ Term (ii)\n\nO\n\n≤\n\n(cid:18) 1 ε\n\n·\n\ns\n\n(cid:112)log(1/p)\n\n(cid:19)\n\n.\n\nFor Term (i), as in the previous proof, the key is to show that\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) This follows from the fact that discrete Gaussian is sub-Gaussian and sum of sub-Gaussian is still (cid:112)2 log(2/p), we have the above result, sub-Gaussian. Thus, by Lemma 6 and the choice of τ = g which implies that with high probability\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n> τ\n\n(cid:88)\n\ni∈[n]\n\nηi\n\np.\n\n≤\n\n\n\n\n\nP\n\nε\n\nTerm (i)\n\nτ g\n\n≤\n\n= O\n\n(cid:18) 1 ε\n\n(cid:112)log(1/p)\n\n(cid:19)\n\n.\n\nSince s 1, combining the bounds on Term (i) and Term (ii), we have that the private noise satisfies Assumption 1 with constants σ = O(1/ε) and h = 0. Hence, by the generic regret bound in Lemma 2, we have established the required result.\n\n≥\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nG MORE DETAILS ON SECURE AGGREGATION AND SECURE SHUFFLING\n\nIn this section, we provide more details about secure aggregation (SecAgg) and secure shuffling including practical implementations and recent theoretical results for DP guarantees. We will also discuss some limitations of both protocols, which in turn highlights that both have some advantages over the other and thus how to select them really depends on particular applications.\n\nG.1 SECURE AGGREGATION\n\nPractical implementations: SecAgg is a lightweight instance of secure multi-party computation (MPC) based on cryptographic primitives such that the server only learns the aggregated result (e.g., sum) of all participating users’ values. This is often achieved via additive masking over a finite group (Bonawitz et al., 2017; Bell et al., 2020). The high-level idea is that participating users add randomly sampled zero-sum mask values by working in the space of integers modulo m, which guarantees that each user’s masked value is indistinguishable from a random value. However, when all masked values are summed modulo m by the server, all the masks will be cancelled out and the server observes the true modulo sum. Bonawitz et al. (2017) proposed the first scalable SecAgg protocol where both communication and computation costs of each user scale linearly with the number of all participating users. Recently, Bell et al. (2020) presents a further improvement where both client computation and communication depend logarithmically on the number of participating clients.\n\nSecAgg for distributed DP: First note that the fact that the server only learns the sum of values under SecAgg does not necessarily imply differential privacy since this aggregated result still has the risk of leaking each user’s sensitive information. To provide a formal DP guarantee under SecAgg, each participating user can first perturb her own data with a moderate random noise such that in the aggregated sum, the total noise is large enough to provide a high privacy guarantee. Only until recently, SecAgg with DP has been systematically studied, mainly in the context of private (federated) supervised learning (Kairouz et al., 2021a; Agarwal et al., 2021) while SecAgg in the context of private online learning remains open until our work. More importantly, there exist no formal convergence guarantees of SGD in Kairouz et al. (2021a); Agarwal et al. (2021) due to the biased gradient estimates. To the best of our knowledge, the very recent work (Chen et al., 2022b) is the only one that derives upper bound on the convergence rate when working with SecAgg in private supervised learning. However, the privacy guarantee in it is only approximated DP rather than pure DP considered in our paper.\n\nLimitations of SecAgg: As pointed out in Kairouz et al. (2021b), several limitations of SecAgg still exist despite of recent advances. For example, it assumes a semi-honest server and allows it to see the per-round aggregates. Moreover, it is not efficient for sparse vector aggregation.\n\nG.2 SECURE SHUFFLING\n\nPractical implementations: Secure shuffling ensures that the server only learns an unordered collection (i.e., multiset) of the messages sent by all the participating users. This is often achieved by a third party shuffling function via cryptographic onion routing or mixnets (Dingledine et al., 2004) or oblivious shuffling with a trusted hardware (Bittau et al., 2017).\n\nSecure shuffling for distributed DP: The additional randomness introduced by the shuffling function can be utilized to achieve a similar utility as the central model while without a trustworthy server. This particular distributed model via secure shuffling is also often called shuffle model. In particular, Cheu et al. (2019) first show that for the problem of summing n real-valued numbers within [0, 1], the expected error under shuffle model with (ε, δ)-DP guarantee is O( 1 δ ). For comparison, under the central model, the standard Laplace mechanism achieves (ε, 0)-DP (i.e., pure DP) with an error O(1/ε) (Dwork et al., 2006) while an error Ω(√n/ε) is necessary under the local model (Chan et al., 2012; Beimel et al., 2008). Subsequent works on private scalar sum under the shuffle model have improved both the communication cost and accuracy in Cheu et al. (2019). More specifically, instead of sending O(ε√n) bits per user as in Cheu et al. (2019), the protocols proposed in Balle et al. (2020); Ghazi et al. (2020b) achieve (ε, δ)-DP with an error O(1/ε) while each user only sends O(log n + log(1/δ)) bits. A closely related direction in the shuffle model is privacy amplification bounds (Erlingsson et al., 2019; Balle et al., 2019; Feldman et al., 2022). That is,\n\nε log n\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2 Local Randomizer\n\n(Central Model)\n\nR [0, 1] N, modulo m\n\n1: Input: Each user data xi 2: Parameters: precision g ∈\nxig 3: Encode xi as (cid:98)xi = xig ⌊\n⌋ 4: Modulo clip yi = (cid:98)xi mod m // no local noise 5: Output: yi\n\n∈ ∈\n+ Ber(xig\n\n− ⌊\n\n⌋\n\nN, batch size n )\n\nN, privacy parameter φ\n\n∈\n\nthe shuffling of n locally private data yields a gain in privacy guarantees. In particular, Feldman et al. (2022)7 show that randomly shuffling of n ε0-DP locally randomized data yields an (ε, δ)-DP\n\nguarantee with ε = O\n\nε0\n\n(cid:18)\n\n(cid:19)\n\n√log(1/δ) √\nn\n\nwhen ε0\n\n≤\n\n1 and ε = O\n\n(cid:18)\n\n(cid:19)\n\n√eε0 log(1/δ) √\nn\n\nwhen ε0 > 1.\n\nShuffle model has also been studied in the context of empirical risk minimization (ERM) (Girgis et al., 2021) and stochastic convex optimization (Cheu et al., 2021; Lowy & Razaviyayn, 2021), in an attempt to recover some of the utility under the central model while without a trusted server. To the best of our knowledge, Tenenbaum et al. (2021); Chowdhury & Zhou (2022b); Garcelon et al. (2022) are the only works that study shuffle model in the context of bandit learning.\n\nIn all the above mentioned works on shuffle model, only approximate DP is achieved. To the best of our knowledge, there are only two existing shuffling protocols that can attain pure DP (Ghazi et al., 2020a; Cheu & Yan, 2021). In particular, by simulating a relaxed SecAgg via a shuffle protocol, Cheu & Yan (2021) are the first to show that there exists a shuffle protocol for bounded sums that satisfies (ε, 0)-DP with an expected error of O(1/ε). This is main inspiration for us to achieve pure DP in MAB with secure shuffling.\n\nLimitations of secure shuffling: As pointed out in Kairouz et al. (2021b), one of the limitations is the requirement of a trusted intermediary for the shuffling function. Another is that the privacy guarantee under the shuffle model degrades in proportion to the number of adversarial users.\n\nH ALGORITHM 1 UNDER CENTRAL AND LOCAL MODELS\n\nIn this section, we will show that Algorithm 1 and variants of our template protocol allow us to achieve central and local DP using only discrete noise while attaining the same optimal regret bound as in previous works using continuous noise (Sajed & Sheffet, 2019; Ren et al., 2020).\n\nP\n\nH.1 CENTRAL MODEL\n\nFor illustration, we will mainly consider as a SecAgg, while secure shuffling can be applied using the same way as in the main paper. In the central model, we just need to set ηi = 0 in the local (see Algorithm 2 and Algorithm 3). Then, we randomizer have the following privacy and regret guarantees.\n\nand add central noise in the analyzer\n\nR\n\nA\n\nS\n\nis any SecAgg protocol and\n\nTheorem 5 (Central Pure-DP via SecAgg). Let Algorithm 2, any ε m = ng + 2τ + 1. Then, Algorithm 1 instantiated with protocol the central model with expected regret given by\n\n1, choose n = l(b), g =\n\n(0, 1), for each b\n\n, R\n\nε√n\n\n= (\n\nA\n\n≥\n\n∈\n\nS\n\nS\n\n⌉\n\n⌈\n\n,\n\nP\n\n) be a protocol such that\n\ngiven by LapZ(g/ε). Fix ∼\n, p = 1/T and ε log(2/p) ⌉\nis able to achieve (ε, 0)-DP in\n\nR\n\n⌈\n\ng\n\n, τ =\n\nP is given by Algorithm 3 with η\n\nA\n\nE [Reg(T )] = O\n\n\n\n\n\n(cid:88)\n\na∈[K]:∆a>0\n\nlog T ∆a\n\n+\n\nK log T ε\n\n\n\n .\n\nMoreover, the communication messages per user before\n\nare O(log m) bits.\n\nS\n\nRemark 7. We can generate η polya(1, e−ε/g) distributed.\n\n∼\n\nLapZ(g/ε) as η = η1\n\n−\n\nη2, where η1, η2 is independent and\n\n7The results in Feldman et al. (2022) hold for adaptive randomizers, but then it needs first to shuffle and then apply the local randomizer. For a fixed randomizer, shuffle-then-randomize is equivalent to randomizethen shuffle.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nA\n\n(Central Model)\n\nAlgorithm 3 Analyzer 1: Input: (cid:98)y (output of SecAgg) 2: Parameters: precision g ∈\n3: Generate discrete Laplace noise η and set (cid:98)η = η mod m 4: Set y = ((cid:98)y + (cid:98)η) mod m 5: if y > ng + τ then set z = (y 6: −\n7: else set z = y/g 8: Output: z\n\nm)/g // correction for underflow\n\nN, modulo m\n\n∈\n\nN, batch size n\n\nN, accuracy parameter τ\n\n∈\n\nProof of Theorem 5. Privacy: By the definition of central DP and post-processing, it suffices to show that y in Algorithm 3 is private. To this end, we again apply the distributive property of modular sum to obtain that\n\ny = ((cid:98)y + (η mod m)) mod m =\n\n(cid:98)xi\n\n+ η\n\nmod m.\n\n(cid:32)(cid:32)\n\n(cid:88)\n\n(cid:33)\n\n(cid:33)\n\nSince the sensitivity of (cid:80) central model.\n\ni (cid:98)xi is g and η\n\n∼\n\ni\n\nLapZ(g/ε), by Fact 3, we have obtained (ε, 0)-DP in the\n\nRegret: As before, the key is to establish that with high probability\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ where (cid:98)y := (cid:98)ya(b). Then, we can apply our generic regret bound in Lemma 2. We again can divide the LHS of equation 11 into\n\n(cid:112)log(1/p) +\n\n(cid:18) 1 ε\n\nlog(1/p)\n\n((cid:98)y)\n\n(cid:88)\n\n1 ε\n\nxi\n\n−\n\nO\n\n(cid:19)\n\n,\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A (cid:124)\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\ni\n\n(cid:123)(cid:122) Term (i)\n\n(cid:98)xi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\n+\n\n1 g\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:124)\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(cid:98)xi\n\n− (cid:123)(cid:122) Term (ii)\n\n.\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\nIn particular, Term (ii) can be bounded by using the same method, i.e., for any p probability at least 1\n\np,\n\n−\n\nTerm (ii)\n\n(cid:18) 1 ε\n\nO\n\n≤\n\n(cid:112)log(1/p)\n\n(cid:19)\n\n.\n\n(11)\n\n(0, 1], with\n\n∈\n\nFor Term (i), we would like to show that\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\nP\n\n1 g\nnoise denote the event that ((cid:80) E\n\n((cid:98)y)\n\n−\n\n(cid:35)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:98)xi\n\n>\n\nτ g\n\n(cid:88)\n\ni\n\np.\n\n≤ [(cid:80)\n\nAs before, let concentration of discrete Laplace, we have P [ event of noise to analyze the output use the same steps as in the proof of Theorem 1 to conclude that (cid:19)\n\ni (cid:98)xi) + η noise] 1\n((cid:98)y). Note that y = (((cid:80) ≥\n\nA\n\n−\n\n∈\n\nE\n\nE\n\nTerm (i)\n\nO\n\n≤\n\n(cid:18) 1 ε\n\nτ, (cid:80)\n\ni (cid:98)xi\n\ni (cid:98)xi + τ ], then by the p. In the following, we condition on the i (cid:98)xi) + η) mod m and hence we can\n\n−\n\nlog(1/p)\n\n.\n\n(12)\n\nFinally, by our generic regret bound in Lemma 2, we have the regret bound.\n\nH.2 LOCAL MODEL\n\nA\n\nIn the local model, each local randomizer DP while the analyzer\n\nneeds to inject discrete Laplace noise to guarantee pure is simply the same as in the main paper (see Algorithm 4 and Algorithm 5).\n\nR\n\nTo analyze the regret, we need the concentration of the sum of discrete Laplace. To this end, we have the following result.\n\nLemma 5 (Concentration of discrete Laplace). Let according to LapZ(1/ε). Then, we have Xi is (c2 1\n\nn\n\nXi {\nε2 , c 1\n\ni=1 be i.i.d random variable distributed }\nε )-sub-exponential for some absolute con-\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 Local Randomizer\n\n(Local Model)\n\nR [0, 1] N, modulo m\n\n1: Input: Each user data xi 2: Parameters: precision g xig 3: Encode xi as (cid:98)xi = ⌋\n4: Generate discrete Laplace noise ηi 5: Add noise and modulo clip yi = ((cid:98)xi + ηi) mod m 6: Output: yi // for SecAgg\n\n∈ ∈\n+ Ber(xig\n\n∈ xig\n\n− ⌊\n\n⌊\n\n⌋\n\nS\n\nN, batch size n )\n\nN, privacy parameter φ\n\n∈\n\n(Local Model)\n\nA\n\nAlgorithm 5 Analyzer 1: Input: (cid:98)y (output of SecAgg) 2: Parameters: precision g 3: Set y = (cid:98)y 4: if y > ng + τ then set z = (y 5: −\n6: else set z = y/g 7: Output: z\n\n∈\n\nm)/g // correction for underflow\n\nN, modulo m\n\nN, batch size n\n\n∈\n\n∈\n\nN, accuracy parameter τ\n\nstant c > 0. As a result, we have for any p where v\n\n(cid:112)2n log(2/p), 2 c\n\nmax\n\n∈\n\n≥\n\nc ε\n\n{\n\n. ε log(2/p) }\n\n(0, 1], with probability at least 1\n\n(cid:80)n\n\ni=1 Xi\n\np,\n\n|\n\n−\n\nv,\n\n| ≤\n\n−\n\nLapZ(1/ε), then it can rewritten as X = N1\n\nProof. First, we note that if X N2 where N1, N2 ∼\nis geometrically distributed, i.e., P [N1 = k] = P [N2 = k] = βk(1 β) with β = e−ε. We can also write it as X = ̃N1 E [N1] and ̃N1 = N2 ̃N2, where ̃N1 = N1 E [N2], since E [N1] = E [N2]. −\nThen, by (Hillar & Wibisono, 2013, Lemma 4.3) and the equivalence of different definitions of sub-exponential (cf. Proposition 2.7.1 in Vershynin (2018)), we have ̃N1, ̃N2 are (c2 ε )-sub- ̃N2, by the weighted sum of exponential for some absolute constant c1 > 0. Now, since X = ̃N1 zero-mean sub-exponential (cf. Corollary 4.2 in Zhang & Chen (2020)), we have X is (c2 1 ε )- sub-exponential for some absolute constant c > 0. Thus, by Lemma 9, we have for any p (0, 1], let v v.\n\n, with probability at least 1\n\n(cid:112)2n log(2/p), 2 c\n\nε2 , c 1\n\nε2 , c1\n\n− −\n\nmax\n\n(cid:80)\n\n−\n\n−\n\np,\n\n∈\n\n1\n\n1\n\n1\n\n≥\n\nc ε\n\n{\n\nε log(2/p) }\n\n−\n\n|\n\ni Xi\n\n| ≤\n\nTheorem 6 (Local Pure-DP via SecAgg). Let Algorithm 4 with ηi ε\nin Lemma 5), p = 1/T and m = ng + 2τ + 1. Then, Algorithm 1 instantiated with protocol able to achieve (ε, 0)-DP in the local model with expected regret given by\n\ngiven by is given by Algorithm 5. Fix any (c is the constant is\n\nA (cid:112)2n log(2/p) cg ε\n⌉\n\nS 1, choose n = l(b), g =\n\nis any SecAgg protocol and\n\n∼ (0, 1), for each b\n\n) be a protocol such that\n\nLapZ(g/ε),\n\n, τ =\n\n, R\n\nε√n\n\n= (\n\nR\n\nA\n\n≥\n\nP\n\nP\n\n∈\n\nS\n\n⌉\n\n⌈\n\n⌈\n\n,\n\nE [Reg(T )] = O\n\n\n\n\n\n(cid:88)\n\na∈[K]:∆a>0\n\n\n\n .\n\nlog T ε2∆a\n\nMoreover, the communication messages per user before\n\nare O(log m) bits.\n\nS\n\nProof. Privacy: By the privacy guarantee of discrete Laplace and the sensitivity of (cid:98)xi, we directly have the local pure DP for our algorithm.\n\nRegret: The first step is to show that with high probabilit, for a large batch size n = l(b) 2 log(2/p),\n\n≥\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\nO\n\n(cid:18) 1 ε\n\n(cid:112)l(b) log(1/p)\n\n(cid:19)\n\n,\n\n(13)\n\nwhere (cid:98)y := (cid:98)ya(b).\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nWe again can divide the LHS of equation 13 into\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A\n\n((cid:98)y)\n\n(cid:88)\n\n−\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)A (cid:124)\n\n((cid:98)y)\n\n1 g\n\n−\n\n(cid:88)\n\ni\n\n(cid:123)(cid:122) Term (i)\n\n(cid:98)xi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\n+\n\n1 g\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:124)\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(cid:98)xi\n\n− (cid:123)(cid:122) Term (ii)\n\n.\n\nxi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:125)\n\nIn particular, Term (ii) can be bounded by using the same method, i.e., for any p probability at least 1\n\np,\n\n(0, 1], with\n\n∈\n\n−\n\nTerm (ii)\n\n(cid:18) 1 ε\n\nO\n\n≤\n\n(cid:112)log(1/p)\n\n(cid:19)\n\n.\n\nTo bound Term (i), the key is again to bound the total noise (cid:80)\n\ni ηi, i.e.,\n\nP\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n(cid:88)\n\ni∈[n]\n\nηi\n\n\n\n\n\n> τ\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 log(2/p) and τ = cg\n\np.\n\n≤\n\nTo this end, by Lemma 5, when n = l(b) concentration holds. Then, following the same steps as before, we can conclude that Term(i) O( 1 ε\n\n(cid:112)2l(b) log(1/p)) when l(b)\n\n2 log(2/p).\n\n≥\n\nε\n\n≤\n\n(cid:112)2n log(2/p), the above\n\nWe then make a minor modification of the proof of Lemma 2. The idea is simple: we divide the batches into two cases – one is that l(b) 2 log(2T ) (noting p = 1/T ) and the other is that 2 log(2T ). First, one can easily bound the total regret for the second case. In particular, l(b) during all the batches such that l(b) 1) log T ). For the first case where l(b) is large, by equation 13 and the same steps in the proof of Lemma 2, we can conclude that\n\n2 log(2T ), the total regret is bounded by O((K\n\n−\n\n≥\n\n≤\n\n≤\n\n≥\n\nl( ̃ba)\n\nmax\n\n≤\n\n(cid:26) c1 log(KT /p) ∆2 a\n\n,\n\nc2 log(KT /p) ε2∆2 a\n\n(cid:27)\n\n,\n\nwhere ̃ba is the last batch such that the suboptimal arm a is still active. Thus, putting everything (0, 1), we have together and noting that ∆a\n\n1 and ε\n\n≤\n\n∈\n\nE [Reg(T )] = O\n\n\n\n\n\n(cid:88)\n\na∈[K]:∆a>0\n\n\n\n .\n\nlog T ε2∆a\n\nI TIGHT PRIVACY ACCOUNTING FOR RETURNING USERS VIA RDP\n\nIn this section, we demonstrate the key advantages of obtaining RDP guarantees compared to approximate DP in the context of MAB. The key idea here is standard in privacy literature and we provide the details for completeness. The key message is that using the conversion from RDP to approximate DP allows us to save additional logarithmic factor in δ in the composition compared to using advanced composition theorem.\n\nIn the main paper, as in all the previous works on private bandit learning, we focus on the case where the users are unique, i.e., no returning users. However, if some user returns and participates in multiple batches, then the total privacy loss needs to resort to composition. In particular, let us consider the following returning situations.\n\nAssumption 2 (Returning Users). Any user can participate in at most B batches, but within each batch b, she only contributes once.\n\nNote that the total B batches can even span multiple learning process, each of which consists of a T -round MAB online learning.\n\nLet’s first consider approximate DP in the distributed model, e.g., binomial noise in Tenenbaum et al. (2021) via secure shuffling. To guarantee that each user is (ε, δ)-DP during the entire learning process, by advanced composition theorem (cf. Theorem 7), we need to guarantee that each batch b\n2 . Thus, for each batch the\n\n[B], it is (εi, δi)-DP, where εi =\n\n2B and δ′ = δ\n\n, δi = δ\n\nε 2√2B log(1/δ′)\n\n∈\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nvariance of the privacy noise is\n\nσ2\n\ni = O(log(1/δi)/ε2\n\ni ) = O\n\n(cid:18) B log(1/δ) log(B/δ) ε2\n\n(cid:19)\n\n.\n\n(14)\n\nNow, we turn to the case of RDP, (e.g., obtained via Skellam mechanism in Theorem 2 or via discrete Gaussian in Theorem 4). To gain the insight, we again consider the case that the scaling factor s is large enough such that for each batch b, it is (α, αε2 2 )-RDP for all α, i.e., it is approximately 2 -CDP. Thus, B composition of it yields that it is now Bε2 ε2 2 -CDP, and by the conversion lemma (cf. Lemma 10), it is (ε′, δ)-DP with ε′ = O(ε(cid:112)B log(1/δ)). Thus, in order to guarantee (ε, δ)-DP in the distributed model, the variance of the privacy noise at each batch is\n\nσ2\n\ni = O\n\n(cid:18) B log(1/δ) ε2\n\n(cid:19)\n\n.\n\n(15)\n\nComparing equation 14 and equation 15, one can immediately see the gain of log(B/δ) in the variance, which will translate to a gain of O((cid:112)log(B/δ)) in the regret bound.\n\nRemark 8. For a more accurate privacy accounting, a better way is to consider using numeric evaluation rather than the above loose bound.\n\nRemark 9. If one is interested in pure DP, then by simple composition, the privacy loss scales linearly with respect to B rather than √B.\n\nJ MORE DETAILS ON SIMULATIONS\n\nWe numerically compare the performance of Algorithm 1 under pure-DP and RDP guarantees in the distributed model (named Dist-DP-SE and Dist-RDP-SE, respectively) with the DP-SE algorithm of Sajed & Sheffet (2019), which achieves pure-DP under the central model. We vary the privacy level , where a lower value of ε indicates higher level of privacy. as ε\n\n0.1, 0.5, 1\n\n∈ {\n\n}\n\nIn Figure 2, we consider the easy instance, i.e., where arm means are sampled uniformly in [0.25, 0.75]. In Figure 3, we consider the hard instance, i.e., where arm means are sampled uniformly in [0.45, 0.55]. The sampled rewards are Gaussian distributed with the given means and truncated to [0, 1]. We plot results for K = 10 arms.\n\nWe see that, for higher value of time horizon T , the time-average regret of Dist-DP-SE is order-wise same to that of DP-SE, i.e., we are able to achieve similar regret performance in the distributed trust model as that is achieved in the central trust model. As mentioned before, we observe a gap for small value of T , which is the price we pay for discrete privacy noise (i.e., additional data quantization (cid:112)log(1/p))) and not requiring a trusted central server. Hence, if we lower error on the order of O( 1 the level of privacy (i.e., higher value of ε), this gap becomes smaller, which indicates an inherent trade-off between privacy and utility.\n\nε\n\nWe also observe that if we relax the requirement of privacy from pure-DP to RDP, then we can achieve a considerable gain in regret performance; more so when privacy level is high (i.e., ε is small). This gain depends on the scaling factor s – the higher the scale, the higher the gain in regret.\n\nIn Figure 4, we compare regret achieved by our generic batch-based successive arm elimination algorithm (Algorithm 1) instantiated with different protocols under different trust models and privacy guarantees: (i) central model with pure-DP (CDP-SE), (ii) local model with pure-DP (LDP-SE), (iii) Distributed model with pure-DP (Dist-DP-SE), Renyi-DP (Dist-RDP-SE) and ConcentratedDP (Dist-CDP-SE). First, consider the pure-DP algorithms. We observe that regret performance of CDP-SE and Dist-DP-SE is similar (with a much better regret than LDP-SE). Now, if we relax the pure-DP requirement, then we achieve better regret performance both for Dist-RDP-SE and DistCDP-SE. Furthermore, Dist-CDP-SE performs better in terms of regret than Dist-RDP-SE. This is due to the fact that under CDP, we use discrete Gaussian noise (which has sub-Gaussian tails) as opposed to the Skellam noise (which has sub-exponential tails) used under RDP.\n\nP\n\nIn Figure 5, we show clear plots for our experiment on bandit instances generated from Microsoft Learning to Rank dataset MSLR-WEB10K (Qin & Liu, 2013).\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Comparison of time-average regret for Dist-DP-SE, Dist-RDP-SE, and DP-SE in Gaussian bandit instances under large reward gap (easy instance) with privacy level ε = 0.1 (top), ε = 0.5 (mid) and ε = 1 (bottom)\n\n30\n\n0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)Published as a conference paper at ICLR 2023\n\nFigure 3: Comparison of time-average regret for Dist-DP-SE, Dist-RDP-SE, and DP-SE in Gaussian bandit instances under small reward gap (hard instance) with privacy level ε = 0.1 (top), ε = 0.5 (mid) and ε = 1 (bottom)\n\n31\n\n0.00.20.40.60.81.0Round×1070.000.010.020.030.04Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1070.000.010.020.030.04Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1070.000.010.020.030.04Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)Published as a conference paper at ICLR 2023\n\nFigure 4: Comparison of time-average regret for CDP-SE, LDP-SE, Dist-DP-SE, Dist-RDP-SE and Dist-CDP-SE in Gaussian bandit instances under large reward gap (easy instance) with privacy level ε = 0.1 (top), ε = 0.5 (mid) and ε = 1 (bottom)\n\n32\n\n0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretCDP-SELDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-CDP-SE(s=10)0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretCDP-SELDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-CDP-SE(s=10)0.00.20.40.60.81.0Round×1060.000.050.100.150.20Time-averageRegretCDP-SELDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-CDP-SE(s=10)Published as a conference paper at ICLR 2023\n\nFigure 5: Comparison of time-average regret for Dist-DP-SE, Dist-RDP-SE, and DP-SE in bandit instances generated from real data with privacy level ε = 1 (top), ε = 5 (mid) and ε = 10 (bottom)\n\n33\n\n0.00.20.40.60.81.0Round×1060.10.20.3Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.10.20.3Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)0.00.20.40.60.81.0Round×1060.10.20.3Time-averageRegretDP-SEDist-DP-SEDist-RDP-SE(s=10)Dist-RDP-SE(s=100)Published as a conference paper at ICLR 2023\n\nK AUXILIARY LEMMAS\n\nIn this section, we summarize some useful facts that have been used in the paper.\n\nFact 3 (Discrete Laplace Mechanism). Let ∆, ε > 0. Let q : for all D, D′ differing on a single user’s data. Define a mechansim M : q(D) + Y , where Y\n\nLapZ(∆/ε). Then, M satisfies (ε, 0)-DP. Moreover, for all m\n\nq(D) |\nn\n\n→\n\n→\n\nD\n\nD\n\nn\n\nZ satisfy\n\nq(D′)\n\n∆ −\nZ by M (D) =\n\n| ≤\n\nN,\n\n∈\n\n∼\n\nP [Y > m] = P [Y <\n\nm] =\n\n−\n\n∆\n\ne− εm e ε ∆ + 1\n\n.\n\nLemma 6 (Concentration of sub-Gaussian). A mean-zero random variable X is σ2-sub-Gaussian if for all λ\n\nR\n\n∈\n\nThen, it satisfies that for any p\n\n∈\n\nE (cid:2)eλX (cid:3)\n\n(cid:18) λ2σ2 2\n\n(cid:19)\n\n.\n\nexp\n\n≤\n\n(0, 1], with probability at least 1\n\n√2σ(cid:112)log(2/p).\n\nX |\n\n| ≤\n\np,\n\n−\n\nLemma 7 (Concentration of sub-exponential). A mean-zero random variable X is (σ2, h)-subexponential if for\n\n1/h\n\nλ\n\n|\n\n| ≤\n\nE (cid:2)eλX (cid:3)\n\n(cid:18) λ2σ2 2\n\n(cid:19)\n\n.\n\nexp\n\n≤\n\nThen, we have\n\nX |\nThus, it satisfies that for any p\n\nP [\n\n(cid:18)\n\n(cid:26) t2\n\n2σ2 ,\n\nt 2h\n\n(cid:27)(cid:19)\n\n.\n\n> t]\n\n2 exp\n\nmin\n\n−\n\n≤\n\n| (0, 1], with probability at least 1\n\np, −\n√2σ(cid:112)log(2/p) + 2h log(2/p).\n\n∈ X\n|\n\n| ≤\n\nLemma 8 (Hoeffding’s Inequality). Let X1, . . . , Xn be independent and identically distributed (i.i.d) random variables and Xi (0, 1], with probability at least 1\n\n[0, 1] with probability one. Then, for any p\n\np,\n\n∈\n\n∈\n\n−\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nXn\n\n−\n\n(cid:114)\n\nlog(2/p) 2n\n\n(cid:12) (cid:12) E [X1] (cid:12) (cid:12) (cid:12) ≤ i=1 be independent zero-mean (σ2 }\ni σ2\n\ni , hi)-subi , h∗)-sub-exponential, where h∗ := maxi hi.\n\n{\n\nn\n\n.\n\nLemma 9 (Sum of sub-exponential). Let exponential random variables. Then, (cid:80) Thus, we have\n\nXi i Xi is ((cid:80)\n\nP\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nXi\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:35)\n\n> t\n\n≤\n\n(cid:26) t2 2 (cid:80)\n\ni σ2\n\ni\n\n(cid:27)(cid:19)\n\n.\n\n,\n\nt 2h∗\n\n2 exp\n\nmin\n\n(cid:18)\n\n− (cid:112)2 (cid:80) {\n\ni σ2\n\ni log(2/p), 2h∗ log(2/p)\n\n, with probability\n\n}\n\nsatisfies (α, ε(α))-RDP, then for any δ\n\n(0, 1),\n\n∈\n\nM\n\nsatisfies\n\nM\n\nIn other words, for any p p, at least 1\n\ni Xi\n\n∈ v.\n\n(cid:80) |\n\n| ≤\n\n−\n\nLemma 10 (Conversion Lemma). If (ε, δ)-DP where\n\n(0, 1], if v\n\nmax\n\n≥\n\nε = inf α>1\n\nε(α) +\n\nsatisfies 1\n\nIf\n\nM\n\n2 ε2-CDP, then for for any δ log(1/(αδ))\n\nε′ = inf\n\nε2α +\n\n1 2\n\nlog(1/(αδ))\n\nα 1\n− (0, 1),\n\n∈\n\nM\n\n+ log(1\n\n1/α).\n\n−\n\nsatisfies (ε′, δ)-DP where\n\n+ log(1\n\n1/α)\n\nε\n\n(cid:16)(cid:112)2 log(1/δ) + ε/2\n\n(cid:17)\n\n.\n\nα>1\n\n1 satisfies (ε, 0)-DP, then it satisfies (α, 1\n\n−\n\n−\n\nα\n\n·\n\n≤ 2 ε2α)-RDP simultaneously for all α\n\n∞\n\nM\n\nMoreover, if ). (1, Theorem 7 (Advanced composition). Given target privacy parameters ε′ (0, 1) and δ′ > 0, to ensure (ε′, kδ + δ′)-DP for the composition of k (adaptive) mechanisms, it suffices that each mechanism is (ε, δ)-DP with ε =\n\n∈\n\n∈\n\n.\n\nε′ 2√2k log(1/δ′)\n\n34",
  "translations": [
    "# Summary Of The Paper\n\nThis paper considers the (stochastic) multiarmed bandits problems in the setting of distributed differential privacy, where each individual’s reward function needs to be protected, and the central server running the bandits algorithm is not fully trusted to receive private information in the clear.  The setting studied in the paper does, however, allow for a secure intermediary, such as secure aggregation, or shuffling.\n\nThe main result of the paper is an algorithm with excess regret of $O(\\varepsilon^{-1} k \\log T)$, where $k$ is the number of arms, $T$ is the number of time steps, and $\\varepsilon$ is the privacy parameter. This is optimal for pure differential privacy even in the central model, and effectively removes a $\\sqrt{log 1/\\delta}$ factor from prior work. The prior work also offered somewhat weaker privacy guarantees.\n\n# Strength And Weaknesses\n\nGetting a tight bound for a fundamental problem, in a distributed model of differential and yet matching the central model lower bound, is certainly nice. \n\nThe paper builds on ideas from prior works. The main algorithm is very similar but different from the VB-SDP-AE algorithm from the Tenebaum, Kaplan, Mansour, Stemmer [TMMS] paper. Part of the improvement over [TMMS] comes from using a better primitive for private binary summation, either by using a different secure intermediary primitive (secure aggregation) or by utilizing a more recent private summation protocol for the shuffle model, from a preprint of Cheu and Yan. Nevertheless, this is not enough to get the tight bound, and the additional tweak added to the TMMS algorithm seems to be essential.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe initial submission did not clearly explain the innovation in this paper over prior work. The latest revision, however, significantly improves in this aspect.\n\n# Summary Of The Review\n\nThe paper has a nice result, which builds on prior work, but also adds new ideas.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable",
    "# Summary Of The Paper\nThe paper titled \"Distributed Differential Privacy in Multi-Armed Bandits\" addresses the challenge of achieving pure differential privacy (DP) in the multi-armed bandit (MAB) problem without the need for a trusted server. The authors introduce a novel algorithm based on successive arm elimination, which employs discrete Laplace noise and secure computation protocols. Their approach achieves optimal regret rates and provides strong theoretical guarantees of privacy through Rényi differential privacy (RDP). Empirical evaluations on synthetic and real-world datasets demonstrate that the proposed method maintains competitive performance with respect to regret while ensuring user data privacy.\n\n# Strength And Weaknesses\nThe paper presents significant contributions to the field of differential privacy in online learning by introducing the first communication-efficient MAB algorithm that guarantees pure DP with optimal regret bounds. The methodology is well-structured, utilizing secure aggregation and local randomization effectively. However, the reliance on discrete Laplace noise may impose specific constraints on the types of applications that can directly benefit from this approach. Additionally, while the theoretical aspects are robust, the practical implications of implementing secure computation protocols in large-scale settings may require further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, methodology, and results. The organization of the content allows for easy navigation through the theoretical and empirical components. The novelty of the work lies in its approach to combining pure DP with optimal regret in a distributed setting, which is a significant advancement over prior models. The reproducibility of the experiments is supported by detailed numerical evaluations, although providing access to the code or datasets used would enhance transparency and facilitate further validation by the community.\n\n# Summary Of The Review\nOverall, the paper makes a compelling contribution to the intersection of differential privacy and multi-armed bandits, demonstrating how to achieve privacy without sacrificing utility. While the theoretical guarantees are strong, some practical challenges may arise in real-world implementations. Nonetheless, the findings are promising and point to new avenues for future research.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to the K-armed bandit problem within a distributed trust model of differential privacy (DP). The authors propose a generic bandit algorithm based on successive arm elimination that achieves pure differential privacy without requiring a trusted server. The methodology includes the addition of discrete Laplace noise via secure computation protocols to ensure privacy. The paper claims to achieve stronger privacy guarantees with Rényi differential privacy (RDP) while maintaining optimal regret bounds in both theoretical and empirical evaluations. The findings reveal that the proposed algorithms, Dist-DP-SE and Dist-RDP-SE, perform comparably to central model algorithms while ensuring robust privacy, although with some overhead in smaller time horizons.\n\n# Strength And Weaknesses\nStrengths of the paper include its significant contribution to the field of private bandit algorithms by addressing the limitations of existing approaches that focus on approximate DP. The theoretical guarantees provided for both privacy and regret performance are solid, establishing new, optimal bounds. Additionally, the empirical validation showcases the practicality of the proposed algorithms across various scenarios. However, the paper has notable weaknesses, such as the complexity associated with secure computation protocols that may introduce high computational and communication overhead. The effectiveness of the proposed methods is also sensitive to parameter choices, particularly privacy parameters and batch sizes, which could complicate real-world applications. Furthermore, the scope of evaluation is limited, primarily focusing on specific datasets, necessitating broader validation to strengthen claims of generality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem context, methodology, and findings. The quality of the writing is high, making it accessible to readers familiar with differential privacy and bandit algorithms. The novelty is significant, particularly in the context of achieving pure DP in distributed settings, which has not been extensively explored. However, reproducibility may be challenged due to the reliance on secure computation protocols and the need for careful tuning of parameters, which are not extensively detailed.\n\n# Summary Of The Review\nOverall, this paper makes a meaningful contribution to the field of differential privacy in the context of multi-armed bandits, offering a novel algorithm with strong theoretical and empirical support. While the findings are promising, practical applicability may be limited by computational overhead and dependency on parameter tuning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Distributed Differential Privacy in Multi-Armed Bandits\" by Sayak Ray Chowdhury and Xingyu Zhou addresses the K-armed bandit problem within a distributed trust model of differential privacy (DP). The authors propose a novel algorithm that achieves pure DP (ε, 0) without relying on a trustworthy server, while maintaining regret bounds comparable to traditional centralized models. The methodology employs a communication-efficient bandit algorithm based on successive arm elimination, integrating discrete Laplace noise and secure computation protocols to ensure privacy. Additionally, the paper demonstrates the achievement of Rényi differential privacy (RDP), providing tighter privacy guarantees and operational implications for scenarios with multiple user participations.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive approach to a significant challenge in the field of privacy-preserving algorithms, specifically in the context of multi-armed bandits. The proposed algorithm is innovative, offering a solid theoretical foundation and empirical results demonstrating competitive performance. The paper effectively addresses the limitations of previous DP models and successfully balances privacy and utility. However, one notable weakness is the limited exploration of the algorithm's scalability in larger datasets or highly dynamic environments, which could impact its practical applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presented, with all necessary components, including theoretical foundations and experimental validations, articulated effectively. The quality of the writing is high, and the mathematical formulations are precise and relevant. The novelty of the proposed approach is significant in the context of distributed differential privacy, presenting a fresh take on MAB algorithms. The reproducibility of the results could be enhanced by providing more detailed experimental setups and parameter choices used in the evaluations.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the literature on differential privacy in the context of multi-armed bandits. The proposed algorithm is theoretically sound and empirically validated, addressing key challenges in achieving privacy without sacrificing performance. Further exploration of scalability and practical implementations would strengthen its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel algorithm that achieves pure differential privacy (DP) within a distributed model while matching the regret bounds typically found in centralized models. The authors rigorously analyze the theoretical foundations of their approach, providing sublinear regret bounds and privacy guarantees through detailed proofs. Extensive numerical evaluations on synthetic and real-world data demonstrate the effectiveness of the proposed algorithm, particularly in terms of communication efficiency and regret performance.\n\n# Strength And Weaknesses\nThe paper has several strengths, including its achievement of pure DP in a distributed setting, which is a significant contribution to the field. The logarithmic scaling of communication bits with respect to the number of users is a notable improvement over existing methods. Furthermore, the robust theoretical analysis and extensive numerical evaluations lend credibility to the findings. However, limitations include the reliance on discrete noise, which complicates implementation, and potential challenges in real-world applications due to secure protocol overhead. While the introduction of Rényi differential privacy (RDP) enhances performance, its less understood nature compared to pure DP may necessitate further exploration. Additionally, the generalizability of the numerical evaluations could be questioned due to possible limitations in covering diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe methodology is well-structured and clearly articulated, making it accessible for implementation and future research. The theoretical guarantees presented are rigorous, but their practical applicability remains uncertain, as real-world factors may affect outcomes. The novelty of the approach, particularly in integrating RDP into multi-armed bandit (MAB) problems, is commendable, though further validation is needed to ensure its effectiveness across various contexts. Overall, the paper is of high quality but may require additional clarity on practical implementation details for reproducibility.\n\n# Summary Of The Review\nThis paper makes significant contributions to the intersection of differential privacy and multi-armed bandit problems through a novel algorithm that achieves pure DP and demonstrates improved communication efficiency. While the theoretical foundations are strong, practical implementation challenges and potential limitations in empirical evaluations warrant careful consideration. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel algorithm for the multi-armed bandit (MAB) problem that operates under a distributed differential privacy (DP) framework. It introduces a batch-based successive arm elimination algorithm that employs secure computation to achieve strong privacy guarantees while maintaining competitive regret performance akin to centralized models. Key contributions include the use of a discrete Laplace noise mechanism tailored for privacy preservation, a communication-efficient approach that scales logarithmically with user participation, and rigorous empirical validation demonstrating the algorithm's effectiveness in both synthetic and real-world scenarios.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative algorithm design that effectively merges secure aggregation with differential privacy, offering a significant contribution to the field of privacy-preserving machine learning. The rigorous theoretical analysis of regret bounds under pure DP conditions is another strong point, as it aligns with optimal performance guarantees found in non-private settings. However, the paper could improve by providing a more extensive comparison with other state-of-the-art methods in terms of both privacy guarantees and algorithmic performance. Additionally, while the empirical results are promising, more experiments across diverse datasets could further strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections that facilitate comprehension of the proposed methodology and its implications. The quality of writing is high, with clear definitions and a logical flow of ideas. The novelty of the approach lies in its combination of discrete noise mechanisms and secure computation in a distributed context, which has not been extensively explored in prior literature. The reproducibility is supported by detailed descriptions of the algorithm and empirical setup, although the inclusion of code or supplementary material would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the intersection of differential privacy and multi-armed bandits by providing a novel algorithm that effectively balances privacy and performance. The rigorous theoretical and empirical analyses substantiate the claims made, though there is room for deeper comparative assessments with existing approaches.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to adversarial training within the framework of multi-armed bandit (MAB) problems, addressing the critical challenge of maintaining robust performance against adversarial attacks while ensuring computational efficiency. The authors propose a distributed trust model that eliminates the dependency on a central server, thereby enhancing the robustness of bandit algorithms. They introduce a batch-based successive arm elimination algorithm designed to strategically select arms in the presence of adversarial noise, offering both theoretical guarantees and extensive numerical evaluations to validate the effectiveness of their approach.\n\n# Strength And Weaknesses\n**Strengths:**\n- The introduction of a distributed trust model for adversarial training is a notable innovation that significantly enhances the robustness of existing methods.\n- The proposed algorithm effectively balances robustness against adversarial attacks with the efficiency of decision-making, maintaining a low regret in various scenarios.\n- The paper is backed by strong theoretical foundations that provide guarantees for optimal regret bounds, enhancing the credibility of the proposed approach.\n\n**Weaknesses:**\n- The complexity of the algorithm may pose challenges for practical implementation, particularly in resource-constrained environments.\n- The focus on MAB problems raises questions about the generalizability of the proposed method to other domains in adversarial training.\n- The empirical validation, while extensive, could benefit from comparisons against a wider array of established adversarial training techniques to provide a more comprehensive understanding of the proposed method's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, with a logical progression from the introduction of the problem to the proposed solutions and their validation. The quality of the theoretical analysis is high, providing solid foundations for the claims. The novelty lies primarily in the combination of the distributed trust model with adversarial training in MAB problems. However, the complexity of the proposed algorithm may hinder reproducibility, especially for practitioners with limited computational resources.\n\n# Summary Of The Review\nThis paper presents a significant advancement in adversarial training for multi-armed bandit problems through a novel distributed trust model and a robust algorithm. While the theoretical insights and numerical evaluations are compelling, the complexity of implementation and limited comparative analysis could be addressed in future work to enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Distributed Differential Privacy in Multi-Armed Bandits\" by Sayak Ray Chowdhury and Xingyu Zhou presents a novel algorithm aimed at achieving distributed differential privacy (DP) in multi-armed bandit scenarios without relying on a trusted server. The authors claim that this is the first bandit algorithm to achieve pure DP in a distributed setting. They further assert that their approach matches the optimal regret performance under a centralized trust model. The methodology utilizes batch-based successive arm elimination combined with discrete Laplace noise to ensure privacy, while the performance evaluation includes extensive numerical tests on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its attempt to address the intersection of distributed systems and differential privacy, a topic of growing importance. However, the claim of being the first to achieve distributed DP in bandit settings can be viewed as an overstated contribution, considering prior works have tackled similar challenges in varied contexts. Furthermore, while the proposed algorithm achieves regret performance comparable to central models, it relies on established techniques, which diminishes the novelty of the results. The methodology is sophisticated, but the specific noise distributions and secure protocols required may limit the algorithm's general applicability. Additionally, the empirical results, while competitive, do not convincingly outperform existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, which facilitates understanding. However, claims regarding the novelty of the approach and its implications are somewhat overstated, suggesting a need for a more nuanced presentation of its contributions. The reproducibility of the results is supported by the detailed methodology and numerical evaluations, although the reliance on specific implementations may pose challenges for broader application.\n\n# Summary Of The Review\nIn summary, while the paper presents a structured and clear approach to achieving distributed differential privacy in multi-armed bandit problems, its contributions are largely incremental and reiterate previous findings in the field. The claims of novelty and impact should be tempered with a recognition of the existing body of work in this area.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper by Sayak Ray Chowdhury and Xingyu Zhou addresses the K-armed bandit problem while ensuring differential privacy (DP) in a distributed trust model. The authors propose a novel bandit algorithm based on successive arm elimination that utilizes discrete Laplace noise to achieve a pure-DP guarantee. Their theoretical contributions include improved regret bounds, specifically \\( O(\\sum_{a \\in [K]: \\Delta_a > 0} \\frac{\\log T}{\\Delta_a} + K \\log T \\epsilon) \\), and empirical results demonstrating that their proposed Dist-DP-SE algorithm significantly outperforms previous methods in both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe paper presents a strong theoretical foundation, establishing a clear improvement over existing methods in terms of regret bounds and privacy guarantees. The experimental results are robust, showcasing the effectiveness of the proposed algorithm across various scenarios, including both easy and hard instances. However, the paper could benefit from deeper discussions regarding the implications of the privacy cost and a more thorough exploration of the limitations of the proposed approach. Additionally, clarity on how the algorithm performs in highly dynamic environments would add to the overall strength of the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates its methodology and findings. The quality of the experiments is high, with detailed results that effectively demonstrate the proposed algorithm's performance. However, some technical details regarding the implementation and tuning of parameters could be elaborated to enhance reproducibility. The paper introduces significant novelty in transitioning from approximate to pure DP, which is a notable advancement in the field.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of privacy-preserving algorithms in multi-armed bandits, demonstrating both theoretical advancements and practical applicability. While the findings are compelling, further discussion on limitations and reproducibility could strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for ensuring privacy in distributed systems through the use of secure computation protocols, specifically focusing on discrete Laplace and Skellam noise for privacy guarantees. The authors propose a method that enables users to contribute data without the need for a trusted central server, aiming to balance privacy and utility in user interactions. The findings suggest that their approach can generalize to various trust models, although several assumptions underpinning the methodology may limit its applicability.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to privacy through secure computation and its potential to generalize across different trust models. However, several weaknesses undermine the robustness of the findings. For instance, the reliance on discrete noise raises concerns about the adequacy of privacy protection in practical applications. Additionally, assumptions regarding user participation and the efficacy of secure computation protocols, without thorough empirical validation, may lead to overoptimistic conclusions about the proposed methods' scalability and effectiveness. \n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. However, the novelty of the contributions is somewhat diminished by a lack of comparative analysis with existing work, making it difficult to assess the true advancements made. The reproducibility of the results is questionable due to the assumptions made regarding user behavior and the conditions under which the proposed methods are applicable. Further empirical validation with diverse real-world data would bolster the claims made in the paper.\n\n# Summary Of The Review\nWhile the paper presents a promising framework for privacy in distributed systems, it is hampered by several critical assumptions that may limit its applicability and generalizability. More rigorous analysis and empirical validation are needed to substantiate the claims about privacy guarantees and utility trade-offs.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper addresses the K-armed bandit problem within a distributed differential privacy framework, proposing a novel algorithm that achieves pure differential privacy (DP) without relying on a trusted server. The authors introduce a batch-based successive arm elimination algorithm that incorporates secure aggregation and discrete Laplace noise, ensuring both privacy and competitive regret performance. The theoretical guarantees provided demonstrate that the proposed method matches the regret bounds of central models, while empirical evaluations across synthetic and real-world datasets support the effectiveness of the approach.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by being the first to propose a bandit algorithm that achieves pure differential privacy in a distributed setting, addressing a critical gap in existing literature. The combination of secure aggregation and noise management is well-executed, and the extension to Rényi differential privacy presents a valuable addition to the discussion of privacy and regret trade-offs. However, the complexity of the algorithm may pose challenges for implementation in practical scenarios, and the reliance on specific noise distributions may limit generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow that effectively outlines the problem, methodology, and contributions. The quality of the theoretical analysis is high, and the empirical results are well-presented, illustrating the performance of the algorithm. The novelty of the approach is substantial as it introduces a new framework for considering distributed DP in bandit settings. However, the reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nThis paper presents a pioneering approach to the K-armed bandit problem in a distributed differential privacy context, achieving significant theoretical and empirical results. The proposed algorithm demonstrates effective privacy-preserving capabilities while maintaining competitive performance, marking a valuable contribution to the field. Overall, the work is well-structured and provides a strong foundation for future research in privacy-preserving online learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach to improving data efficiency in reinforcement learning (RL) through the use of a hybrid model that combines model-based and model-free techniques. The authors propose a framework that leverages a learned dynamics model to guide exploration and improve policy learning. Through rigorous experimentation on standard RL benchmarks, the authors demonstrate that their method achieves superior sample efficiency and performance compared to traditional model-free RL algorithms.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Relevance**: The focus on data efficiency in RL is crucial, especially in real-world applications where data collection can be costly or time-consuming.\n2. **Novelty**: The hybrid approach represents a significant advancement over existing methods, blending model-based and model-free strategies in a novel way that enhances learning efficiency.\n3. **Theoretical Foundation**: The paper provides a sound theoretical basis for the proposed method, including clear assumptions and derivations that support its effectiveness.\n4. **Empirical Results**: The extensive experimentation across various benchmarks showcases the robustness of the proposed method, with clear improvements over baselines.\n5. **Clarity**: The writing is concise and well-organized, with figures and tables that effectively illustrate key points and findings.\n\n**Weaknesses:**\n1. **Generalizability**: While the results are promising, the applicability of the method to more complex environments or tasks outside the tested benchmarks remains to be seen.\n2. **Comparative Depth**: The paper lacks a thorough comparative analysis with state-of-the-art hybrid methods, which would provide greater context for the contributions.\n3. **Limitations Discussion**: There is limited discussion on the limitations of the proposed approach, particularly in cases where the learned model may be inaccurate.\n4. **Hyperparameter Sensitivity**: The sensitivity of the method to hyperparameter tuning is not adequately addressed, which could impact reproducibility in practice.\n5. **Future Directions**: The paper could benefit from a more detailed discussion of potential future research directions and improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is high, with well-structured sections and clear explanations of concepts. The quality of the methodology is solid, supported by theoretical insights and empirical validation. The novelty lies in the effective integration of model-based and model-free techniques, although further exploration of alternative models and approaches could enhance its significance. Reproducibility could be improved by providing more detailed information on hyperparameter selection and experimental setup.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of reinforcement learning, particularly in addressing the challenge of data efficiency through a hybrid modeling approach. While the strengths of the proposed method are evident, enhancements in comparative analysis and limitations discussion would further solidify its impact and applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes a novel algorithm for the K-armed bandit problem within a distributed trust model of differential privacy (DP). The authors aim to achieve pure differential privacy with minimal regret through a bandit algorithm based on successive arm elimination, utilizing discrete Laplace noise and secure computation techniques. Notably, the algorithm also guarantees Rényi differential privacy (RDP), which is a stronger privacy notion. The findings indicate that the proposed method maintains regret levels comparable to those of central models while effectively addressing privacy concerns.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing the challenges of implementing differential privacy in a distributed setting, particularly in online learning scenarios. The proposed algorithm demonstrates a balance between privacy and performance, which is a critical area of research given the increasing privacy concerns in various applications. However, the paper could benefit from more extensive empirical evaluations to solidify the theoretical claims. Additionally, the discussion surrounding the limitations of the approach and potential trade-offs between privacy and utility could be more elaborated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers familiar with differential privacy and the multi-armed bandit problem. The quality of the writing is high, and the concepts are presented logically. The novelty of the approach, particularly the integration of pure DP in a distributed model, is commendable. However, reproducibility could be improved by providing more detailed descriptions of the experimental setup and the specific parameters used in the evaluations.\n\n# Summary Of The Review\nThis paper presents a noteworthy advancement in the field of differential privacy applied to the multi-armed bandit problem, offering a distributed approach that maintains competitive performance. While the theoretical contributions are solid, the empirical evaluations could be strengthened to provide a more comprehensive understanding of the algorithm's effectiveness across varying scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Distributed Differential Privacy in Multi-Armed Bandits\" by Sayak Ray Chowdhury and Xingyu Zhou addresses the K-armed bandit problem under the constraints of distributed differential privacy (DP). The authors present a novel algorithm that achieves pure DP without relying on a trusted server while maintaining optimal regret bounds. Their methodology involves a batch-based successive arm elimination approach and employs discrete Laplace noise for privacy. The theoretical results are corroborated by numerical evaluations on both synthetic and real-world datasets, showing that the proposed algorithm achieves performance comparable to central DP methods while ensuring stronger privacy guarantees through the use of Rényi differential privacy (RDP) and Skellam noise.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its significant contributions to the field of privacy-preserving algorithms for multi-armed bandits, particularly in achieving pure DP in a distributed setting without a trusted server. The algorithm's communication efficiency and effective regret analysis are particularly noteworthy. However, potential weaknesses include a lack of extensive empirical validation across diverse real-world scenarios, which may limit the generalizability of the findings. Additionally, while the theoretical contributions are robust, the practical implications could be further explored to enhance the applicability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making the complex concepts of differential privacy and multi-armed bandits accessible. The quality of the writing is high, with a logical flow that guides the reader through the problem, methodology, and results. The novelty is significant, as it introduces a new approach to achieving distributed DP in MAB problems. Reproducibility is facilitated by the detailed explanation of the algorithm and the theoretical underpinnings, although providing code or experimental details would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of differential privacy in multi-armed bandits, successfully balancing privacy and performance. While the theoretical results are compelling, further empirical validation across varied scenarios would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Distributed Differential Privacy in Multi-Armed Bandits\" by Sayak Ray Chowdhury and Xingyu Zhou addresses the K-armed bandit problem within the framework of distributed differential privacy (DP). The authors propose a novel bandit algorithm that employs successive arm elimination while ensuring privacy through the addition of discrete Laplace noise. The main contributions include achieving pure DP without relying on a trusted server and demonstrating comparable regret bounds to centralized models. The methodology involves a detailed analysis of algorithm design, secure aggregation techniques, and the implementation of noise mechanisms, supported by theoretical results and extensive empirical evaluations on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to achieving pure DP in a distributed setting, which is a significant advancement over existing local and central DP models. The authors also provide thorough experimental validation, showcasing the algorithm's effectiveness in various scenarios. However, the paper has some weaknesses, particularly regarding the assumptions made in the proposed method which may limit its practical applicability. Additionally, while the comparisons with existing methods are valuable, further analysis against a broader range of algorithms could enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, as it is well-structured with logical progression and clear section headings. The use of tables and figures effectively summarizes the results, aiding in the comprehension of complex information. The quality of the writing is high, with precise language that conveys the technical details without ambiguity. In terms of novelty, the paper presents a significant contribution to the field of distributed differential privacy, making it relevant to ongoing research. The reproducibility of the results is supported by detailed methodology, although additional clarity on the assumptions in the experimental setup would be beneficial for future researchers attempting to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to distributed differential privacy in multi-armed bandits, with strong theoretical and empirical support. Its contributions are significant, though the practical applicability could be improved with more explicit discussion of its assumptions. The findings have implications for future research and applications in private online learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper advances the study of distributed differential privacy (DP) within the K-armed bandit framework. The authors propose a novel algorithm for achieving pure differential privacy without relying on a central server, which traditionally has been a limitation in existing methods. They introduce a batch-based successive arm elimination approach that incorporates discrete Laplace noise and secure computation protocols, resulting in a regret overhead of Θ(K log T/ε) while ensuring privacy guarantees. The theoretical results demonstrate that the algorithm maintains sublinear regret rates and achieves Rényi differential privacy, which is empirically validated across synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to combining privacy with utility in a distributed setting, which is particularly relevant given the increasing privacy concerns in various applications. The algorithm's design is well-structured and addresses critical challenges in secure aggregation. However, a potential weakness is the reliance on the theoretical assumptions made during the analysis; real-world applicability may depend on the robustness of these assumptions. Additionally, the empirical results, while supportive, could benefit from further exploration across a broader range of scenarios to bolster generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its methodology clearly, allowing readers to grasp the complexities of the proposed algorithm. The quality of the theoretical analysis is high, with comprehensive proofs and a logical structure. The novelty of the approach lies in its integration of distributed differential privacy with a practical bandit algorithm, which is a significant contribution to the field. Reproducibility is supported by the detailed description of the algorithm and the empirical setup, though the authors could enhance this aspect by providing code or supplementary material.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the integration of differential privacy and multi-armed bandit algorithms, offering both theoretical and empirical contributions. The proposed algorithm demonstrates strong privacy guarantees while maintaining competitive performance, marking a meaningful step towards practical applications in privacy-sensitive environments.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a distributed differential privacy (DP) approach tailored for multi-armed bandit scenarios. It introduces a shuffle protocol aimed at achieving privacy while maintaining regret bounds comparable to a centralized model. However, the paper has weaknesses in addressing practical challenges, robust experimental validation, and the originality of the methods employed.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical analysis of regret, which aligns with established models in the literature. However, significant weaknesses include the lack of innovation in the shuffle protocol, which draws heavily from existing techniques without substantial modifications. Additionally, the claims regarding achieving pure DP while preserving regret bounds are overly ambitious and lack empirical evidence. The reliance on discrete Laplace noise introduces practical complications that are not thoroughly discussed, and the paper fails to provide meaningful improvements to existing secure aggregation methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, but the quality of the contributions is undermined by the superficial treatment of key issues, such as communication efficiency and practical implications. The novelty is limited, with many ideas being derivative rather than groundbreaking. Reproducibility may be challenging due to the insufficient experimental validation and the lack of comparative analysis with established methods.\n\n# Summary Of The Review\nOverall, the paper presents a theoretically interesting approach to distributed differential privacy in multi-armed bandits, but it overstates its contributions and lacks rigorous validation of its claims. The reliance on existing methods without significant improvements raises concerns about its practical applicability and impact.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThis paper introduces a novel algorithm that achieves pure differential privacy within the distributed model for multi-armed bandit problems. The authors demonstrate that their approach maintains the same regret bounds as traditional central models while ensuring robust privacy protections. The algorithm employs Rényi differential privacy (RDP) and is designed for communication efficiency, scaling logarithmically with the number of users. Extensive numerical evaluations on both synthetic and real-world datasets substantiate the algorithm's effectiveness, making it a significant advancement in privacy-preserving online learning.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to integrating differential privacy into multi-armed bandits, which is a crucial area with broad applicability in fields such as online advertising and clinical trials. The algorithm's ability to achieve optimal regret performance while providing strong privacy guarantees is commendable. However, a potential weakness could be the dependence on discrete Laplace noise, which may have limitations in certain contexts or applications. Additionally, while the theoretical contributions are robust, further exploration into edge cases or extreme scenarios could enhance the understanding of the algorithm's limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its methodology and findings clearly, making it accessible to a broad audience. The quality of the research is high, with a thorough theoretical foundation supported by empirical evaluations. The novelty of the approach is significant, particularly in the context of combining differential privacy with distributed learning. Reproducibility is bolstered by the comprehensive numerical evaluations provided, although the authors could consider including more details on their experimental setup to facilitate replication.\n\n# Summary Of The Review\nThis paper makes a substantial contribution to the field of privacy-preserving online learning by introducing a highly efficient algorithm that combines pure differential privacy with optimal performance in multi-armed bandits. The results are well-supported by empirical evidence, and the research opens up new avenues for future work in this area.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to the multi-armed bandit (MAB) problem by integrating distributed differential privacy (DP) into its framework. The authors propose a new bandit algorithm based on successive arm elimination that utilizes discrete Laplace noise to achieve strong privacy guarantees while maintaining a regret level comparable to the central trust model. The theoretical contributions include an innovative regret analysis that shows the algorithm can achieve sublinear regret similar to central models, and the introduction of Rényi differential privacy (RDP) as a stronger alternative to approximate DP, enhancing the privacy utility trade-off.\n\n# Strength And Weaknesses\nThe paper makes significant contributions to the theoretical understanding of privacy in MAB contexts, particularly through its algorithm design and rigorous regret analysis. The use of discrete Laplace noise and the exploration of RDP provide a robust framework for achieving privacy without sacrificing utility. However, the paper may lack empirical validation, as it primarily focuses on theoretical constructs rather than practical implementations or experiments that would demonstrate the algorithm's effectiveness in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and generally clear, with a logical flow of ideas that makes complex theoretical concepts accessible. The quality of the writing is high, and the mathematical rigor appears sound, which supports reproducibility efforts. However, the lack of empirical results may hinder the ability of other researchers to replicate the findings in practical settings, thus limiting the overall applicability of the proposed methods.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful theoretical advancement in the integration of distributed differential privacy within the multi-armed bandit framework. While the contributions are robust and offer valuable insights, the absence of empirical validation may limit its immediate practical impact in the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper \"Distributed Differential Privacy in Multi-Armed Bandits\" presents a novel approach to the K-armed bandit problem within a distributed trust framework, focusing on achieving pure differential privacy (DP) without incurring additional regret compared to centralized models. The authors introduce a generic bandit algorithm based on successive arm elimination combined with discrete Laplace noise, successfully demonstrating that this method can also achieve Rényi differential privacy. The implementation details highlight the algorithm's efficient communication costs and its performance on synthetic and real-world data, showcasing its advantages over existing techniques.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative algorithm design that achieves pure differential privacy while matching the regret performance of centralized models, addressing a significant gap in existing literature. The methodology is well-structured, with detailed implementation guidelines and an efficient communication protocol that improves upon previous approaches. However, the paper could benefit from a more comprehensive discussion of the limitations of the proposed methods and potential challenges in practical applications. Additionally, while the empirical evaluations are promising, further experimentation across diverse scenarios could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and logically organized, making it accessible to readers familiar with differential privacy and bandit algorithms. The quality of the writing is high, with thorough explanations of the proposed method and its significance. The novelty of the approach is notable, as it combines pure DP with competitive performance in a distributed setting, a topic of increasing relevance. Code availability on GitHub enhances reproducibility, but the paper could further improve by providing more detailed instructions for replication of the experiments.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of private bandit learning by introducing a novel algorithm that effectively balances privacy and performance in distributed settings. The clarity and quality of the work support its contributions, though further empirical validation and exploration of potential limitations could enhance the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a method for achieving distributed differential privacy (DP) in multi-armed bandit settings. The authors claim to provide a pure DP guarantee while emphasizing communication efficiency and optimal regret bounds. Their methodology includes the use of discrete noise, and they assert that their approach eliminates the need for a trusted server. The experimental findings suggest that their proposed method achieves comparable regret to central models under pure DP, although these claims are made without sufficiently contextualizing or comparing with existing works in the field.\n\n# Strength And Weaknesses\nThe paper's strengths include its focus on distributed differential privacy and the potential elimination of a trusted server, which could enhance privacy in practical applications. However, it exhibits several weaknesses, notably a biased comparison with previous works, failing to acknowledge significant contributions from related literature. It inadequately addresses the limitations of its approach, particularly regarding the use of discrete noise and the implications for accuracy. Additionally, the analysis of regret bounds lacks depth, and there is insufficient evaluation of the performance of existing algorithms under similar conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by an overly comparative tone that suggests superiority over prior works without adequate justification. The quality of the discussion is diminished by the lack of a balanced view of existing literature, which could lead to a misunderstanding of the contributions made. While the novelty of the proposed method is presented, the paper fails to place its findings within the broader context of differential privacy research. Reproducibility is not thoroughly addressed, as there is insufficient detail on experimental setups and comparisons with existing methodologies.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to distributed differential privacy in multi-armed bandits, but it suffers from a lack of rigorous comparison with existing literature and an imbalanced view of previous contributions. The claims made regarding novelty and performance are not sufficiently substantiated, which detracts from the overall impact of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Distributed Differential Privacy in Multi-Armed Bandits\" presents a novel approach to ensuring differential privacy in multi-armed bandit settings. The authors propose a distributed algorithm that allows for efficient decision-making while preserving privacy across multiple agents. The methodology involves leveraging differential privacy mechanisms to balance exploration and exploitation in a multi-armed bandit framework. The findings suggest that the proposed algorithm achieves significant privacy guarantees while maintaining competitive regret bounds compared to existing methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of differential privacy into multi-armed bandit problems, which is a timely and relevant contribution given the increasing importance of data privacy. The distributed nature of the proposed approach allows for scalability in real-world applications. However, the paper has weaknesses, including minor typographical errors and inconsistencies in notation that may detract from the overall clarity. Additionally, while the theoretical results are promising, empirical evaluations are somewhat limited, which raises questions about the practical implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured, but it suffers from several clarity issues, particularly concerning the consistency of notation and formatting. The novelty of integrating differential privacy into the multi-armed bandit framework is commendable, though the empirical validation requires more depth to support the theoretical claims. Reproducibility could be enhanced by providing additional details on the implementation of the algorithm and by ensuring that all mathematical expressions and variables are consistently defined and formatted.\n\n# Summary Of The Review\nThis paper addresses an important intersection of differential privacy and multi-armed bandit problems, offering a potentially significant contribution to the field. While the theoretical aspects are strong, the clarity and empirical validation could be improved. Overall, the paper presents a valuable approach but requires refinement to fully realize its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the application of distributed differential privacy (DP) within the framework of multi-armed bandits. The authors propose an algorithm that achieves pure DP while optimizing for regret, specifically utilizing a batch-based successive arm elimination technique. The findings demonstrate that their approach maintains privacy guarantees while performing competitively in terms of regret, although the evaluation is limited to synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its rigorous approach to achieving pure DP in a distributed setting and the clear demonstration of algorithmic performance through numerical evaluations. However, it lacks depth in discussing hybrid privacy models and fails to address other relevant privacy frameworks such as federated learning. Additionally, the regret analysis is somewhat narrow, focusing primarily on one technique without exploring alternative strategies, which could limit the understanding of the algorithm's applicability across different contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear exposition of methodologies and results. However, the novelty is somewhat constrained by the limited exploration of alternative models and scenarios. The reproducibility of findings would benefit from a more comprehensive discussion on the computational complexity and scalability of the proposed algorithms, especially concerning varying user participation rates and batch sizes.\n\n# Summary Of The Review\nOverall, the paper presents a solid contribution to the field of distributed differential privacy in multi-armed bandits, demonstrating a viable algorithm with privacy guarantees. Despite its strengths, the paper could have explored a wider range of models and scenarios to enhance its impact and relevance to real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel algorithm addressing the K-armed bandit problem within a distributed differential privacy (DP) framework, specifically achieving (ε, δ)-DP using a shuffle protocol. The main contributions include the design of an algorithm that achieves pure DP (ε, 0) with minimal regret increase compared to central models, alongside a rigorous analysis of regret bounds and privacy guarantees. The findings indicate that the proposed algorithm maintains optimal regret while ensuring strong privacy protections, substantiated through both theoretical analysis and empirical evaluations.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating differential privacy into distributed bandit problems, which is a timely and relevant topic given the increasing importance of privacy in machine learning. The thorough statistical analysis, including the use of the Discrete Laplace mechanism and Rényi Differential Privacy (RDP), adds robustness to the claims made concerning privacy and regret bounds. However, a potential weakness is the reliance on synthetic datasets for empirical validation, which may not fully capture the complexities of real-world applications. Additionally, more explicit discussions on the practical implications and limitations of the proposed algorithm could enhance its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with differential privacy and online learning. The quality of the statistical proofs and the rigor of the regret bound derivations are commendable, contributing to the overall strength of the work. The novelty lies in the integration of privacy and regret analysis in a distributed setting, a relatively underexplored area. Reproducibility is supported through detailed descriptions of the methodology and empirical evaluations, although providing code or additional datasets would further aid this aspect.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of distributed differential privacy in online learning, presenting a novel algorithm with strong theoretical backing and empirical support. While the work is innovative and well-presented, further exploration of practical implications and real-world testing would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to achieving pure differential privacy (DP) within a distributed trust model, focusing on multi-armed bandit problems. The authors propose a secure aggregation protocol that utilizes discrete Laplace noise and Skellam noise to ensure privacy while optimizing decision-making strategies. While the findings demonstrate the efficacy of the proposed methods in achieving privacy guarantees and reducing communication costs, the paper raises important questions about the scalability, robustness, and applicability of the approach in real-world scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its contribution to the intersection of differential privacy and distributed systems, specifically in the context of multi-armed bandits. The proposed method shows promise in reducing communication overhead compared to existing protocols. However, several weaknesses are noted, including the limited scope of privacy guarantees, reliance on specific noise distributions that may not be optimal for all applications, and the lack of comprehensive comparisons with established techniques. Additionally, the analysis fails to account for dynamic user participation and the implications of potential adversarial strategies, which may undermine the robustness of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. However, the novelty of the work is somewhat constrained by its limited empirical validation and the need for more thorough hyperparameter tuning. The reproducibility of the results is also questionable due to the insufficient exploration of the trade-offs between privacy and utility in practical applications. Overall, while the clarity is commendable, the depth of exploration into the proposed methods' implications is lacking.\n\n# Summary Of The Review\nThis paper offers a significant step forward in achieving differential privacy in distributed bandit settings, but it falls short in several key areas, including empirical validation, generalizability, and a comprehensive analysis of privacy-utility trade-offs. Future work should focus on addressing these limitations to enhance the methodology's applicability in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a new approach to the multi-armed bandit (MAB) problem under a distributed trust model, focusing on achieving differential privacy (DP) without incurring additional regret. The authors propose an algorithm based on successive arm elimination, claiming to match the regret bounds of centralized models while enhancing privacy guarantees. Their methodology includes the use of discrete Laplace noise for privacy preservation and demonstrates performance through numerical evaluations on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe main contributions of the paper include a novel algorithm that purportedly achieves strong privacy guarantees while maintaining comparable regret bounds to existing methods. However, the paper lacks significant originality, as the approach appears to rehash well-established concepts in the literature without providing substantial advancements. While the methodology is sound, it does not introduce any particularly innovative techniques or insights that would differentiate it from prior work in the domain of differential privacy and MABs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and detailed descriptions of the proposed algorithm and its theoretical foundations. However, the novelty of the work is questionable, as it closely resembles existing algorithms and strategies. The reproducibility of the results is supported by numerical evaluations; however, the reliance on established techniques may limit the paper's impact. The mathematical details, while precise, may be dense for some readers, potentially hindering accessibility.\n\n# Summary Of The Review\nOverall, the paper presents a method that aims to balance differential privacy and regret in multi-armed bandits, but its contributions are largely incremental and lack the innovation needed to significantly advance the field. While the methodology is sound and well-executed, the paper does not break new ground and instead feels like a rehash of existing literature.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to achieving differential privacy in distributed bandit settings by focusing on discrete noise mechanisms, such as discrete Laplace and Skellam distributions. The authors demonstrate the advantages of Rényi Differential Privacy (RDP) over approximate DP in terms of regret and provide empirical evaluations of their proposed algorithms. The findings suggest that the integration of discrete noise mechanisms can maintain privacy while achieving competitive performance, although the authors acknowledge the potential for further enhancements through hybrid approaches and extended applications, such as contextual bandits.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear demonstration of the effectiveness of discrete noise mechanisms in preserving privacy while minimizing regret. The empirical evaluations are a valuable addition, showcasing the practical applicability of the algorithms. However, the paper has notable weaknesses, including a limited exploration of continuous noise mechanisms, which could enhance utility, and the assumption of unique user participation in each batch, which oversimplifies real-world scenarios. Additionally, while the analysis of tail behavior and parameter sensitivity is introduced, it requires further depth for practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making the methodology and findings accessible to readers. The quality of the empirical evaluations is commendable, although the paper could benefit from a broader range of experiments across different settings. The novelty of the approach, particularly the emphasis on discrete noise mechanisms and the advantages of RDP, is significant. However, the reproducibility of the results may be challenged by the limited exploration of parameter sensitivities and the need for a framework for best practices.\n\n# Summary Of The Review\nOverall, the paper provides valuable insights into the trade-offs between privacy and regret in distributed bandit settings using discrete noise mechanisms. While it offers a solid foundation for future research and practical applications, the exploration of additional methodologies and real-world scenarios could significantly improve its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel algorithm, Dist-DP-SE, designed for the multi-armed bandit problem within the framework of distributed differential privacy (DP). The authors claim that their algorithm achieves pure DP while maintaining equivalent regret bounds to the optimal algorithms operating under a centralized trust model. The findings demonstrate that, although there is a slight initial gap in regret performance compared to the central model, Dist-DP-SE converges as the time horizon increases. The algorithm's performance is consistent across varying privacy levels and shows robustness in challenging scenarios, particularly in hard bandit instances with small reward gaps. Empirical validation on synthetic and real datasets further reinforces the algorithm's efficacy in balancing privacy and decision-making performance.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its significant theoretical contributions, specifically the achievement of pure DP in a distributed setting while maintaining optimal regret bounds. The algorithm demonstrates competitive performance across different privacy levels, and the empirical results are robust, showcasing the applicability of the proposed method in real-world scenarios. However, one weakness is the initial gap in regret performance when compared to the central model, which may raise concerns about the trade-offs in the early stages of deployment. Additionally, the paper could benefit from a more extensive discussion on the implications of relaxing privacy from pure DP to Rényi differential privacy (RDP).\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making the contributions and results clear and accessible. The methodology is presented comprehensively, with sufficient detail to allow for reproducibility. The novelty of the algorithm is commendable, particularly in the context of balancing privacy with decision-making efficiency. The empirical results are convincingly presented, although further exploration of the conditions under which the algorithm performs optimally could enhance the clarity of its applicability.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in the field of multi-armed bandits under distributed differential privacy, presenting a novel algorithm that performs comparably to centralized models while ensuring privacy. The empirical validation supports the theoretical claims, although there are minor concerns regarding initial regret performance that warrant consideration.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing the robustness of machine learning models in the context of differential privacy (DP). The authors propose a distributed trust model that balances privacy guarantees with the need for model accuracy. Through extensive numerical evaluations, the paper demonstrates that their method outperforms existing algorithms in terms of both privacy preservation and model performance. The findings indicate that the proposed solution is a significant step forward in the application of DP in real-world scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to integrating differential privacy with a distributed trust model, which addresses a critical gap in the current literature. The numerical evaluations provide compelling evidence of the method's effectiveness, showcasing clear advantages over prior work. However, the paper suffers from a lack of clarity in certain sections, particularly in the presentation of complex equations and numerical evaluations. Additionally, the engagement with the target audience could be improved by simplifying technical jargon and providing clearer definitions of key terms.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces novel concepts and methodologies, its clarity is somewhat hampered by dense language and the use of technical jargon that might alienate non-specialist readers. The overall quality of the writing could be improved through more concise phrasing and better explanations of complex ideas. The reproducibility of the results is not fully assured due to insufficient detail in the methodology and numerical evaluations. Clearer definitions and context around algorithms and results would enhance the paper's accessibility and reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of differential privacy by introducing a distributed trust model that improves model robustness. However, the presentation suffers from clarity issues that may hinder understanding and reproducibility. Addressing these concerns would significantly elevate the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.404397788877615,
    -1.6830194344369522,
    -1.8435485385141392,
    -1.7713117258346465,
    -1.892225185561487,
    -1.692319399193099,
    -1.559032287074112,
    -1.8286926132509091,
    -1.7764425265822223,
    -1.7593125069039646,
    -1.6721547682472597,
    -1.3110905821767564,
    -1.675347373400131,
    -1.5649190993670437,
    -1.7190965230620507,
    -1.7560506602901,
    -1.9331963650322208,
    -1.6211656504941743,
    -1.777371141392086,
    -1.8543508162283642,
    -1.839162014457267,
    -1.452192907825073,
    -1.9202507118156333,
    -1.6261317982297274,
    -1.7379883201086885,
    -1.7530729838024817,
    -1.8458658820319938,
    -1.7213094417236103,
    -1.5928656005021529
  ],
  "logp_cond": [
    [
      0.0,
      -2.1964468481489745,
      -2.206412883082933,
      -2.2093789745441774,
      -2.2181545927776574,
      -2.1966692461926485,
      -2.213095277115188,
      -2.200732115704181,
      -2.218865527272032,
      -2.2436878432716845,
      -2.1999430941623235,
      -2.235883844677728,
      -2.207969786346698,
      -2.1944218226821115,
      -2.187948906915467,
      -2.207845349878359,
      -2.2041454220462287,
      -2.196816275190508,
      -2.2098286650661096,
      -2.1939281344337638,
      -2.2243395162640973,
      -2.1875286165510888,
      -2.1982702044944347,
      -2.185555440003735,
      -2.2073769941531536,
      -2.214646237019063,
      -2.206025059098271,
      -2.1682744319622023,
      -2.2149110050567113
    ],
    [
      -1.3996539415130487,
      0.0,
      -1.2086639442291938,
      -1.1713352306317777,
      -1.3034282454645747,
      -1.26568004880924,
      -1.4128109200229895,
      -1.2844020715489775,
      -1.2879040548440672,
      -1.3638863973741822,
      -1.239515971140236,
      -1.4566585937234937,
      -1.2064795963097286,
      -1.1958739734972341,
      -1.2126779603503932,
      -1.2098508852338707,
      -1.342390509403441,
      -1.2848912536274206,
      -1.2773357856695473,
      -1.2388003184799579,
      -1.3079803467780344,
      -1.353242065424366,
      -1.2983388879154758,
      -1.2903016881281417,
      -1.3376068573145876,
      -1.3046587505037506,
      -1.3368562872798524,
      -1.3336861411964314,
      -1.3759541848160153
    ],
    [
      -1.5121805650899616,
      -1.436441240874133,
      0.0,
      -1.3211642358976288,
      -1.4528308656629814,
      -1.440398941186922,
      -1.5094551918781496,
      -1.4697662610778581,
      -1.3766839406691578,
      -1.5417029256430346,
      -1.4185542879336457,
      -1.613673271991277,
      -1.3774394735626752,
      -1.3798038768709853,
      -1.412739191391808,
      -1.3942264007308733,
      -1.4946303566083285,
      -1.479220886406194,
      -1.4258993151641786,
      -1.382446558414313,
      -1.4654324680174475,
      -1.523975570378098,
      -1.477418982286544,
      -1.4795924844150803,
      -1.466013496853103,
      -1.4789795968879622,
      -1.4633655100439986,
      -1.4396301899795196,
      -1.5321590517878143
    ],
    [
      -1.5105190708832499,
      -1.3148109496209266,
      -1.2542935345354569,
      0.0,
      -1.3860037244911636,
      -1.3918109489420951,
      -1.4957423947107331,
      -1.3116612568133887,
      -1.3401084218663428,
      -1.4445678233300498,
      -1.3731615366421253,
      -1.5309120350854928,
      -1.2775359263352,
      -1.2309189678413477,
      -1.2673223647248228,
      -1.3743613420122311,
      -1.4194438818358972,
      -1.426332362701997,
      -1.3376148658684766,
      -1.3009599224998272,
      -1.4570710436359358,
      -1.4699212692266452,
      -1.4280145897124763,
      -1.3952059710443627,
      -1.4546530589301094,
      -1.3902281271021188,
      -1.4145197102993363,
      -1.419952551055632,
      -1.478815330875732
    ],
    [
      -1.5963363601283855,
      -1.5363789234393892,
      -1.4892292503213698,
      -1.5134262445239826,
      0.0,
      -1.5471407976861913,
      -1.5948391342471895,
      -1.4901611430395687,
      -1.5857932072804313,
      -1.5786969125432007,
      -1.540344121667453,
      -1.6651838553687346,
      -1.5606741245663964,
      -1.5086411318283077,
      -1.5591848370998442,
      -1.5566907339343214,
      -1.5036575579513745,
      -1.4855436413456944,
      -1.530830464513958,
      -1.5172954483912389,
      -1.5703838220319108,
      -1.579421165162545,
      -1.5833379072624139,
      -1.5822616652855162,
      -1.5200171134407892,
      -1.5287605263612056,
      -1.5448694940629961,
      -1.5477280190929303,
      -1.6224869389888976
    ],
    [
      -1.334647105812935,
      -1.2086628578534064,
      -1.25880806449479,
      -1.2600400255211754,
      -1.2349839535758917,
      0.0,
      -1.3379569946824406,
      -1.2123551767808378,
      -1.3081468916997006,
      -1.3807311513043838,
      -1.1952304016174693,
      -1.4642160926151113,
      -1.2226320666179329,
      -1.197954062981119,
      -1.2569805864223609,
      -1.2072390097825043,
      -1.2719427222401933,
      -1.2364220232268488,
      -1.2669317139189589,
      -1.19848774509417,
      -1.2411027249724436,
      -1.3414522154201052,
      -1.2336276172352365,
      -1.2785966364749466,
      -1.281933907800586,
      -1.2812883945523963,
      -1.3169921774521505,
      -1.2580999598266334,
      -1.35647751904911
    ],
    [
      -1.3229377725875142,
      -1.2848104820259647,
      -1.255850272509993,
      -1.2883499097324518,
      -1.2978252065592524,
      -1.2169593572833206,
      0.0,
      -1.2358733952029488,
      -1.270916566683815,
      -1.2947842777383363,
      -1.2403868927220434,
      -1.3221676080903595,
      -1.2570760944937713,
      -1.24289044619189,
      -1.2869086279138617,
      -1.2693585455763543,
      -1.3128470594356785,
      -1.3153769965063928,
      -1.2635097079177637,
      -1.2403551358429623,
      -1.2805297132384907,
      -1.2805684803818473,
      -1.2323524929274634,
      -1.286945746655476,
      -1.2394688739136808,
      -1.2879909333456832,
      -1.2765666393974857,
      -1.2760764765122932,
      -1.2780607352926987
    ],
    [
      -1.549652646363861,
      -1.4045110669328265,
      -1.4581706201716151,
      -1.3739180100887562,
      -1.4566846447304618,
      -1.4677557156972918,
      -1.5284818438008723,
      0.0,
      -1.458173723188468,
      -1.51087832141153,
      -1.4119387452327135,
      -1.590943160719916,
      -1.4952610287903867,
      -1.356553070060776,
      -1.4014380691897934,
      -1.4400756947802718,
      -1.4953699163155318,
      -1.5076949020561012,
      -1.455677708413166,
      -1.4289336819704097,
      -1.5306697198373185,
      -1.5394577585888298,
      -1.4801966145198442,
      -1.534405913245268,
      -1.5028707633371348,
      -1.4005852323233625,
      -1.5230980393152,
      -1.4873466138975624,
      -1.5560862540449578
    ],
    [
      -1.4897442643296666,
      -1.3773093334022921,
      -1.2562381567248744,
      -1.3134262598039919,
      -1.4117017877491416,
      -1.434620446337916,
      -1.455292321006309,
      -1.3728576734121278,
      0.0,
      -1.472665926894293,
      -1.3977066393207702,
      -1.5361102180508859,
      -1.3222483143549395,
      -1.3336831334866084,
      -1.3250905561944972,
      -1.4162678120716705,
      -1.455420467149912,
      -1.4226989540556687,
      -1.4129829821809632,
      -1.3543870568123515,
      -1.4335533816334474,
      -1.4516501876950856,
      -1.422790756421117,
      -1.4232182214712703,
      -1.4007088237996954,
      -1.4108207831476642,
      -1.429647548440385,
      -1.359187100726221,
      -1.4697230686153393
    ],
    [
      -1.4738070443018658,
      -1.467943973368386,
      -1.3992533618866705,
      -1.388822848554725,
      -1.3839827518603571,
      -1.4174757612661908,
      -1.4626396772962285,
      -1.3785416613041037,
      -1.4488127015049166,
      0.0,
      -1.4462337780509544,
      -1.4872847100571946,
      -1.45239511026925,
      -1.417878364069093,
      -1.437983959367606,
      -1.462493454579215,
      -1.400949221973346,
      -1.4452555916952405,
      -1.432030083307398,
      -1.4347438529836853,
      -1.3966757927676212,
      -1.459290792600922,
      -1.4342800276279823,
      -1.4542934258561533,
      -1.344256456772796,
      -1.3994008857487719,
      -1.3996969607933087,
      -1.4373131844904559,
      -1.437943572810632
    ],
    [
      -1.2925754940288716,
      -1.1929377295469097,
      -1.1353443663157872,
      -1.15897219800688,
      -1.2638774235249002,
      -1.1529881765831487,
      -1.295025713252535,
      -1.171477907045311,
      -1.2111807053359245,
      -1.3716729265323246,
      0.0,
      -1.3907608747951086,
      -1.138347837828195,
      -1.135535422598744,
      -1.1102313106660673,
      -1.1359030158338514,
      -1.2941563307865838,
      -1.2501871800185296,
      -1.2565751637312144,
      -1.142541097811844,
      -1.2706194614585593,
      -1.332879937736082,
      -1.2427072153411596,
      -1.2245964340076707,
      -1.2528381429304816,
      -1.2812029056567533,
      -1.27535780290007,
      -1.246356542758309,
      -1.3310415499805877
    ],
    [
      -1.1427404861298254,
      -1.1242990132116635,
      -1.113224831773324,
      -1.1021066618050863,
      -1.0915579800318886,
      -1.1026005792909621,
      -1.0773656869417316,
      -1.0929384623755471,
      -1.0932016316833786,
      -1.0809277975594418,
      -1.1236984587724148,
      0.0,
      -1.1222972912404603,
      -1.1223097274825231,
      -1.109509200582581,
      -1.1196227895613353,
      -1.0854273274924944,
      -1.127700406404001,
      -1.0878132039441364,
      -1.101922829739081,
      -1.107354294875917,
      -1.1101761568246638,
      -1.0740327716486078,
      -1.1098659639527428,
      -1.0849994399348726,
      -1.1012402805888488,
      -1.0965403563274132,
      -1.0960505677109946,
      -1.1043685165304713
    ],
    [
      -1.3117432151077322,
      -1.1308932818055553,
      -1.0683633854280419,
      -1.0856233427115287,
      -1.2334265972073075,
      -1.224945088581959,
      -1.2976717637700097,
      -1.255172183637481,
      -1.2128058081038657,
      -1.3474475181552714,
      -1.1566448067832737,
      -1.4221331163268518,
      0.0,
      -1.1671489998162083,
      -1.1781122718453372,
      -1.1620160752817024,
      -1.3013816656885109,
      -1.25116473539727,
      -1.2389926800719664,
      -1.0919701202066685,
      -1.247230532932682,
      -1.2830470401687206,
      -1.2510626848941475,
      -1.2576240700925734,
      -1.2490709064977694,
      -1.2736863052803926,
      -1.2980101860557915,
      -1.229564874702467,
      -1.3276753251892486
    ],
    [
      -1.2585275435641619,
      -1.0906575888078285,
      -1.091029545530171,
      -0.9945269924835601,
      -1.1554212683955207,
      -1.1511844339843873,
      -1.2204306738868698,
      -1.120385151252771,
      -1.1158424867101062,
      -1.236171343415849,
      -1.1604122693071834,
      -1.3317498193293502,
      -1.1324979424660966,
      0.0,
      -1.058415887452588,
      -1.1191659961477562,
      -1.227700236107642,
      -1.1815268744822267,
      -1.1470106400843985,
      -1.1011101673844386,
      -1.1854789702485646,
      -1.2036209261272028,
      -1.1346694996565188,
      -1.1586567680605178,
      -1.2134126067592623,
      -1.2246516488884736,
      -1.1898058168769596,
      -1.2100811571003893,
      -1.2350087074899458
    ],
    [
      -1.4383435417186188,
      -1.3143591707051765,
      -1.2962605664898277,
      -1.2814009639369632,
      -1.3740094386933215,
      -1.3341597460509687,
      -1.42611449601149,
      -1.2678731990123142,
      -1.2835764725843966,
      -1.4357124652570135,
      -1.294987308960822,
      -1.514850021285715,
      -1.3378046183253927,
      -1.258451514399768,
      0.0,
      -1.3756800638276903,
      -1.3922775024380698,
      -1.39801213397588,
      -1.3883792818446725,
      -1.2783178019287054,
      -1.3673092645208633,
      -1.4359020157519335,
      -1.3679209643195205,
      -1.365008359157757,
      -1.4080545595828642,
      -1.3766748490777492,
      -1.4095795565334264,
      -1.3688459146573118,
      -1.4379622357976507
    ],
    [
      -1.435235664460406,
      -1.287677071853801,
      -1.2788164741672368,
      -1.2721531785453941,
      -1.3843961690240976,
      -1.2766817750512984,
      -1.4221403092232843,
      -1.3269996200250844,
      -1.3658314710752464,
      -1.4989285145304463,
      -1.2519010351918276,
      -1.5494721185626497,
      -1.3006892827403023,
      -1.262345356744415,
      -1.3399448623723496,
      0.0,
      -1.4097011601808682,
      -1.3686206106112095,
      -1.3541701435594315,
      -1.3224787422878514,
      -1.413666312174991,
      -1.4454281974285361,
      -1.3475252987590218,
      -1.3731655562294267,
      -1.445045169800442,
      -1.3980742845518073,
      -1.4328666075574326,
      -1.373182798059483,
      -1.4985359112123537
    ],
    [
      -1.623377702580315,
      -1.5597167671843886,
      -1.5684826471992583,
      -1.4937083306353076,
      -1.5057570585755822,
      -1.5291166942059826,
      -1.6217329978756334,
      -1.4527227790531434,
      -1.611469914248399,
      -1.5609615498084255,
      -1.5936113708493174,
      -1.671771464794313,
      -1.6040052322050824,
      -1.5440592125287222,
      -1.5575395703971882,
      -1.5643510217472232,
      0.0,
      -1.5420190176212736,
      -1.517190915547122,
      -1.5345892287325391,
      -1.50816396906237,
      -1.6332974168809669,
      -1.5880786074189475,
      -1.5432744399086211,
      -1.5680256337455272,
      -1.5056201325004355,
      -1.5793929405023943,
      -1.5414706984229778,
      -1.6514204910982375
    ],
    [
      -1.273989025377757,
      -1.187586979657487,
      -1.1770886582441902,
      -1.2101496786113304,
      -1.1081219398863535,
      -1.1895762999260033,
      -1.2683155607316363,
      -1.202824880566657,
      -1.2400479018212716,
      -1.2636764930767264,
      -1.208132327622495,
      -1.3381775093122565,
      -1.1542364824286995,
      -1.155625149172825,
      -1.2089296995935501,
      -1.2327121806629329,
      -1.230526364739146,
      0.0,
      -1.224964037470155,
      -1.196922535561002,
      -1.2299589412977507,
      -1.2760161712948248,
      -1.2561884912160297,
      -1.1939110577821959,
      -1.2072802171328028,
      -1.2287144192086787,
      -1.2075214857330363,
      -1.2243464600690288,
      -1.2980442290178413
    ],
    [
      -1.4909077818317715,
      -1.4023260944700022,
      -1.3735629860526717,
      -1.3020030773610096,
      -1.3773688327550946,
      -1.429168126709617,
      -1.449021643480309,
      -1.3627150235384757,
      -1.3801621113332396,
      -1.4433214745489407,
      -1.4330140040155275,
      -1.5296687043613462,
      -1.3437656902198984,
      -1.3821916570757646,
      -1.3937475006769005,
      -1.3814652159339418,
      -1.3937215587147005,
      -1.4378382697291474,
      0.0,
      -1.3537658533083468,
      -1.478985517626157,
      -1.4798099287277737,
      -1.4294685205742104,
      -1.4344143227902992,
      -1.454808252373486,
      -1.3735086037963247,
      -1.4138933773295481,
      -1.4129840928792867,
      -1.4934224536288636
    ],
    [
      -1.5099313231791656,
      -1.3888419253145636,
      -1.311923066510025,
      -1.3922148137939845,
      -1.3910013019278673,
      -1.3870126517350152,
      -1.515055683382996,
      -1.4043037563893204,
      -1.4153191432535208,
      -1.528683363485242,
      -1.372672831149074,
      -1.60640363608924,
      -1.3332368898047147,
      -1.3810623376537945,
      -1.3973378970284864,
      -1.4010552938747158,
      -1.4830249926256984,
      -1.4460832727417383,
      -1.3800879851221948,
      0.0,
      -1.4534860225692072,
      -1.4773091535754785,
      -1.4512491836192944,
      -1.4306434206241498,
      -1.4203033822835205,
      -1.4103119987045465,
      -1.437735823538044,
      -1.4319665996774238,
      -1.4816549168864463
    ],
    [
      -1.5232526276023677,
      -1.4630273997444136,
      -1.4641409558407485,
      -1.4954651504924399,
      -1.5126197972873288,
      -1.4791624108359502,
      -1.5597248577016305,
      -1.51403334717121,
      -1.5162687017341274,
      -1.5834786640074872,
      -1.470324323163858,
      -1.6264428020920898,
      -1.5161460891652405,
      -1.4225743218783198,
      -1.4678698197407014,
      -1.4992160551956446,
      -1.5461305198277258,
      -1.4643573599002848,
      -1.5173747057432718,
      -1.4783249132237706,
      0.0,
      -1.54045308560177,
      -1.4750241064799476,
      -1.4984715706431515,
      -1.5030785701389724,
      -1.5464316477859257,
      -1.4834767615074829,
      -1.4733705815941112,
      -1.5352539026301328
    ],
    [
      -1.1784066510648241,
      -1.1460052134921288,
      -1.137326158665626,
      -1.1747182491691859,
      -1.1141013340335926,
      -1.1814900340614976,
      -1.1351744633932819,
      -1.1110462284397356,
      -1.1344759414512755,
      -1.1596373278809977,
      -1.1804290967553972,
      -1.2039439181863527,
      -1.1564231399423806,
      -1.1384587692509656,
      -1.1305391864797967,
      -1.1669897736463308,
      -1.1582317641821391,
      -1.175984070654118,
      -1.144831991001366,
      -1.1324203071120003,
      -1.1340514728800248,
      0.0,
      -1.120208180591897,
      -1.1677550979270748,
      -1.1058380835330015,
      -1.1436390794797329,
      -1.1412721827122119,
      -1.121164649111618,
      -1.111096721957475
    ],
    [
      -1.5518959079334336,
      -1.4891834017947014,
      -1.4713646806162377,
      -1.503071978924566,
      -1.5051483910703354,
      -1.4716688857535083,
      -1.5258195942831922,
      -1.4437539142854734,
      -1.5108961380541635,
      -1.5526973951796978,
      -1.4815399899403612,
      -1.6667562819080208,
      -1.5129008278401317,
      -1.4619279555853968,
      -1.4548115522757659,
      -1.4566900137822094,
      -1.5120118230038377,
      -1.515236105192395,
      -1.5414677989868724,
      -1.4977457460630383,
      -1.5356806441146298,
      -1.5352716414453789,
      0.0,
      -1.4962547959089914,
      -1.4539306181484977,
      -1.5398165686934264,
      -1.3955254524837077,
      -1.5023729081021686,
      -1.5276335651500197
    ],
    [
      -1.3114827181046522,
      -1.241320781506626,
      -1.2177911338801524,
      -1.2278616445285335,
      -1.2259032406380987,
      -1.2512004616875083,
      -1.351800292367139,
      -1.251166987148869,
      -1.2617776221115904,
      -1.3370405060342987,
      -1.1873994918588255,
      -1.406112905245242,
      -1.2452352581144417,
      -1.1952084182631648,
      -1.2100890791677363,
      -1.195527133800585,
      -1.22594630450538,
      -1.2127608691712308,
      -1.2754820161615383,
      -1.2018820190177462,
      -1.245142930070231,
      -1.3419617036359832,
      -1.2462237130585676,
      0.0,
      -1.2638343747685763,
      -1.2791783922959368,
      -1.2491908631684505,
      -1.2109216158887077,
      -1.3324739827621102
    ],
    [
      -1.4826280948922823,
      -1.3697637663189268,
      -1.314715164590875,
      -1.3670601580211608,
      -1.340304855967582,
      -1.3644872989596963,
      -1.4236250379546416,
      -1.365652253002166,
      -1.3817831241269634,
      -1.292317535249604,
      -1.3682990447833898,
      -1.4962886418923511,
      -1.355753421071591,
      -1.3469492330213786,
      -1.3582309470697913,
      -1.3850310469328626,
      -1.340957515737822,
      -1.3590695052356991,
      -1.3991362567038108,
      -1.3354703136921826,
      -1.3570742887610225,
      -1.3609330567726583,
      -1.3438847167178885,
      -1.3850133001155505,
      0.0,
      -1.4018669270398112,
      -1.3157895677821771,
      -1.3472938216020416,
      -1.4079096709861765
    ],
    [
      -1.4432061852443758,
      -1.3416657189937633,
      -1.3665474526958838,
      -1.3103900723730162,
      -1.3212151356344672,
      -1.3614612633406138,
      -1.373919180229051,
      -1.2910529767078773,
      -1.3590020023381357,
      -1.425572323444272,
      -1.3833440097811864,
      -1.472145018044725,
      -1.414293179104553,
      -1.3526871010378925,
      -1.3455255253189404,
      -1.4018574382508664,
      -1.3687273430345208,
      -1.372235747912384,
      -1.3006546218276742,
      -1.2983792081716083,
      -1.40756154219088,
      -1.4435762626198967,
      -1.3788920592930114,
      -1.4128757406023873,
      -1.3888314950196399,
      0.0,
      -1.4229836103138118,
      -1.4130625269198152,
      -1.3819677290309675
    ],
    [
      -1.5450654070137004,
      -1.4947311344327332,
      -1.4265222167809926,
      -1.5175585089896468,
      -1.443841423195801,
      -1.501831025456513,
      -1.5768417471941836,
      -1.505379723498651,
      -1.5193334902829725,
      -1.5125275901790196,
      -1.4854601942901946,
      -1.641082550546855,
      -1.4819421492814855,
      -1.469822518302314,
      -1.512109424092081,
      -1.483466072718128,
      -1.5000446964822358,
      -1.4764995154885228,
      -1.4688044522231298,
      -1.4520248148813304,
      -1.4838025536633512,
      -1.517976282536362,
      -1.4399115361368142,
      -1.4634047602909357,
      -1.435767833723964,
      -1.528972438183076,
      0.0,
      -1.4802461399270248,
      -1.5508158369732559
    ],
    [
      -1.4105414521448376,
      -1.3678425056126648,
      -1.2689186514063187,
      -1.3185639550346733,
      -1.3478791475612244,
      -1.370953661632417,
      -1.43865303995328,
      -1.314340837192842,
      -1.300033590583802,
      -1.43666520516144,
      -1.3554330204412477,
      -1.4875805874009564,
      -1.3304972433930737,
      -1.3224975038345155,
      -1.3321649356110126,
      -1.3868195552056246,
      -1.3873584805767198,
      -1.3727182003329137,
      -1.3747427161394274,
      -1.328238465064378,
      -1.3833185001525654,
      -1.4170538792587837,
      -1.3535367360402433,
      -1.3639572737439718,
      -1.3871982402040415,
      -1.405462661487669,
      -1.412025021329733,
      0.0,
      -1.4375466258871195
    ],
    [
      -1.2872474599474824,
      -1.2686310972365833,
      -1.2410051490800822,
      -1.267805453606675,
      -1.2597922572065254,
      -1.2641486887847175,
      -1.2668842506407014,
      -1.2666674332454133,
      -1.2510282590190114,
      -1.3068425302054074,
      -1.2764653326175461,
      -1.3314582658961098,
      -1.253158230571738,
      -1.273103842872513,
      -1.2642004037098324,
      -1.2725927395277592,
      -1.302683398590739,
      -1.2790886327530255,
      -1.2886681388560404,
      -1.2605251798184476,
      -1.267387358296768,
      -1.2453719901269678,
      -1.2291940384578606,
      -1.2568809759895287,
      -1.235549746146012,
      -1.2502702690033702,
      -1.2742825786822005,
      -1.2311777220126556,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.20795094072864062,
      0.1979849057946823,
      0.1950188143334377,
      0.18624319609995776,
      0.20772854268496665,
      0.19130251176242696,
      0.20366567317343431,
      0.18553226160558323,
      0.16070994560593066,
      0.20445469471529165,
      0.16851394419988708,
      0.1964280025309173,
      0.20997596619550363,
      0.21644888196214795,
      0.19655243899925612,
      0.20025236683138647,
      0.2075815136871073,
      0.19456912381150548,
      0.21046965444385135,
      0.1800582726135178,
      0.21686917232652636,
      0.20612758438318046,
      0.21884234887388,
      0.19702079472446155,
      0.18975155185855197,
      0.19837272977934406,
      0.23612335691541286,
      0.18948678382090378
    ],
    [
      0.28336549292390356,
      0.0,
      0.47435549020775847,
      0.5116842038051745,
      0.37959118897237754,
      0.41733938562771233,
      0.27020851441396276,
      0.3986173628879748,
      0.395115379592885,
      0.3191330370627701,
      0.4435034632967163,
      0.22636084071345852,
      0.47653983812722367,
      0.4871454609397181,
      0.470341474086559,
      0.4731685492030815,
      0.3406289250335113,
      0.3981281808095316,
      0.4056836487674049,
      0.44421911595699437,
      0.3750390876589178,
      0.32977736901258625,
      0.38468054652147643,
      0.3927177463088105,
      0.3454125771223646,
      0.37836068393320166,
      0.34616314715709984,
      0.34933329324052087,
      0.30706524962093695
    ],
    [
      0.3313679734241777,
      0.4071072976400063,
      0.0,
      0.5223843026165105,
      0.3907176728511579,
      0.4031495973272172,
      0.33409334663598966,
      0.3737822774362811,
      0.46686459784498147,
      0.3018456128711047,
      0.42499425058049356,
      0.22987526652286228,
      0.4661090649514641,
      0.463744661643154,
      0.4308093471223313,
      0.4493221377832659,
      0.3489181819058107,
      0.36432765210794527,
      0.4176492233499607,
      0.46110198009982617,
      0.3781160704966917,
      0.3195729681360413,
      0.36612955622759524,
      0.36395605409905896,
      0.37753504166103635,
      0.36456894162617703,
      0.3801830284701406,
      0.4039183485346196,
      0.31138948672632494
    ],
    [
      0.2607926549513966,
      0.45650077621371987,
      0.5170181912991896,
      0.0,
      0.38530800134348286,
      0.37950077689255135,
      0.27556933112391335,
      0.4596504690212577,
      0.43120330396830364,
      0.3267439025045966,
      0.3981501891925212,
      0.24039969074915368,
      0.4937757994994465,
      0.5403927579932988,
      0.5039893611098236,
      0.3969503838224153,
      0.35186784399874926,
      0.34497936313264943,
      0.4336968599661699,
      0.4703518033348193,
      0.3142406821987107,
      0.30139045660800123,
      0.34329713612217017,
      0.37610575479028374,
      0.31665866690453703,
      0.3810835987325276,
      0.35679201553531015,
      0.35135917477901457,
      0.29249639495891455
    ],
    [
      0.2958888254331016,
      0.3558462621220979,
      0.40299593524011734,
      0.37879894103750456,
      0.0,
      0.34508438787529583,
      0.29738605131429763,
      0.4020640425219184,
      0.30643197828105584,
      0.31352827301828645,
      0.3518810638940342,
      0.22704133019275252,
      0.3315510609950907,
      0.3835840537331794,
      0.3330403484616429,
      0.33553445162716566,
      0.38856762761011265,
      0.4066815442157927,
      0.361394721047529,
      0.37492973717024825,
      0.32184136352957626,
      0.3128040203989422,
      0.30888727829907325,
      0.30996352027597096,
      0.37220807212069795,
      0.3634646592002815,
      0.347355691498491,
      0.34449716646855677,
      0.2697382465725895
    ],
    [
      0.35767229338016393,
      0.4836565413396925,
      0.4335113346983088,
      0.4322793736719235,
      0.4573354456172072,
      0.0,
      0.35436240451065837,
      0.4799642224122611,
      0.3841725074933984,
      0.3115882478887151,
      0.49708899757562963,
      0.2281033065779876,
      0.46968733257516604,
      0.49436533621198,
      0.43533881277073805,
      0.4850803894105946,
      0.4203766769529056,
      0.45589737596625013,
      0.42538768527414006,
      0.49383165409892893,
      0.45121667422065537,
      0.35086718377299375,
      0.4586917819578624,
      0.41372276271815234,
      0.4103854913925129,
      0.41103100464070264,
      0.37532722174094846,
      0.43421943936646556,
      0.33584188014398886
    ],
    [
      0.2360945144865978,
      0.2742218050481473,
      0.3031820145641191,
      0.27068237734166023,
      0.26120708051485964,
      0.3420729297907914,
      0.0,
      0.3231588918711632,
      0.2881157203902971,
      0.26424800933577575,
      0.3186453943520686,
      0.23686467898375252,
      0.3019561925803407,
      0.3161418408822221,
      0.2721236591602503,
      0.2896737414977577,
      0.24618522763843353,
      0.24365529056771917,
      0.2955225791563483,
      0.3186771512311497,
      0.27850257383562127,
      0.27846380669226467,
      0.3266797941466486,
      0.2720865404186361,
      0.31956341316043124,
      0.27104135372842886,
      0.2824656476766263,
      0.28295581056181884,
      0.2809715517814133
    ],
    [
      0.27903996688704824,
      0.4241815463180827,
      0.370521993079294,
      0.4547746031621529,
      0.3720079685204474,
      0.36093689755361735,
      0.3002107694500369,
      0.0,
      0.3705188900624412,
      0.31781429183937915,
      0.4167538680181957,
      0.2377494525309931,
      0.33343158446052246,
      0.4721395431901332,
      0.42725454406111574,
      0.3886169184706374,
      0.3333226969353773,
      0.3209977111948079,
      0.37301490483774313,
      0.39975893128049944,
      0.2980228934135907,
      0.2892348546620793,
      0.34849599873106496,
      0.29428670000564106,
      0.3258218499137744,
      0.42810738092754663,
      0.3055945739357091,
      0.34134599935334675,
      0.27260635920595133
    ],
    [
      0.2866982622525558,
      0.39913319317993023,
      0.520204369857348,
      0.4630162667782305,
      0.3647407388330808,
      0.3418220802443064,
      0.32115020557591345,
      0.4035848531700945,
      0.0,
      0.30377659968792936,
      0.37873588726145213,
      0.24033230853133647,
      0.45419421222728285,
      0.44275939309561396,
      0.4513519703877251,
      0.36017471451055183,
      0.3210220594323103,
      0.35374357252655364,
      0.36345954440125916,
      0.42205546976987085,
      0.342889144948775,
      0.32479233888713677,
      0.35365177016110527,
      0.35322430511095204,
      0.37573370278252693,
      0.3656217434345581,
      0.34679497814183735,
      0.4172554258560013,
      0.30671945796688305
    ],
    [
      0.28550546260209875,
      0.2913685335355787,
      0.3600591450172941,
      0.37048965834923964,
      0.37532975504360744,
      0.3418367456377738,
      0.2966728296077361,
      0.38077084559986085,
      0.310499805399048,
      0.0,
      0.31307872885301014,
      0.2720277968467699,
      0.3069173966347145,
      0.3414341428348715,
      0.32132854753635853,
      0.2968190523247496,
      0.35836328493061864,
      0.31405691520872403,
      0.32728242359656656,
      0.3245686539202792,
      0.36263671413634335,
      0.3000217143030426,
      0.3250324792759822,
      0.3050190810478113,
      0.41505605013116864,
      0.3599116211551927,
      0.3596155461106558,
      0.3219993224135087,
      0.32136893409333256
    ],
    [
      0.3795792742183881,
      0.47921703870035004,
      0.5368104019314726,
      0.5131825702403798,
      0.40827734472235955,
      0.519166591664111,
      0.3771290549947248,
      0.5006768612019488,
      0.46097406291133525,
      0.3004818417149351,
      0.0,
      0.28139389345215116,
      0.5338069304190647,
      0.5366193456485158,
      0.5619234575811924,
      0.5362517524134083,
      0.37799843746067596,
      0.42196758822873015,
      0.41557960451604536,
      0.5296136704354157,
      0.4015353067887004,
      0.3392748305111777,
      0.42944755290610015,
      0.447558334239589,
      0.41931662531677816,
      0.3909518625905064,
      0.39679696534718967,
      0.42579822548895074,
      0.34111321826667207
    ],
    [
      0.16835009604693107,
      0.18679156896509297,
      0.19786575040343246,
      0.20898392037167013,
      0.2195326021448678,
      0.20849000288579433,
      0.23372489523502482,
      0.2181521198012093,
      0.21788895049337786,
      0.23016278461731465,
      0.18739212340434164,
      0.0,
      0.18879329093629615,
      0.18878085469423334,
      0.20158138159417538,
      0.19146779261542113,
      0.22566325468426207,
      0.1833901757727554,
      0.22327737823262006,
      0.20916775243767538,
      0.20373628730083948,
      0.2009144253520927,
      0.23705781052814867,
      0.2012246182240136,
      0.22609114224188387,
      0.20985030158790763,
      0.21455022584934325,
      0.21504001446576182,
      0.20672206564628515
    ],
    [
      0.3636041582923988,
      0.5444540915945757,
      0.6069839879720891,
      0.5897240306886022,
      0.44192077619282344,
      0.4504022848181719,
      0.37767560963012126,
      0.42017518976265,
      0.4625415652962652,
      0.32789985524485954,
      0.5187025666168572,
      0.25321425707327916,
      0.0,
      0.5081983735839226,
      0.4972351015547938,
      0.5133312981184286,
      0.3739657077116201,
      0.42418263800286105,
      0.43635469332816457,
      0.5833772531934625,
      0.4281168404674489,
      0.39230033323141034,
      0.42428468850598344,
      0.4177233033075576,
      0.42627646690236154,
      0.40166106811973834,
      0.3773371873443394,
      0.44578249869766395,
      0.3476720482108824
    ],
    [
      0.3063915558028818,
      0.47426151055921517,
      0.4738895538368726,
      0.5703921068834835,
      0.4094978309715229,
      0.41373466538265635,
      0.3444884254801739,
      0.4445339481142727,
      0.44907661265693744,
      0.32874775595119465,
      0.4045068300598602,
      0.23316928003769344,
      0.43242115690094707,
      0.0,
      0.5065032119144557,
      0.4457531032192874,
      0.33721886325940176,
      0.38339222488481695,
      0.4179084592826452,
      0.4638089319826051,
      0.379440129118479,
      0.3612981732398408,
      0.4302495997105249,
      0.4062623313065259,
      0.35150649260778133,
      0.3402674504785701,
      0.375113282490084,
      0.3548379422666543,
      0.3299103918770978
    ],
    [
      0.28075298134343196,
      0.40473735235687425,
      0.42283595657222306,
      0.4376955591250875,
      0.34508708436872926,
      0.3849367770110821,
      0.29298202705056076,
      0.45122332404973653,
      0.4355200504776542,
      0.2833840578050373,
      0.4241092141012288,
      0.20424650177633574,
      0.3812919047366581,
      0.46064500866228264,
      0.0,
      0.3434164592343605,
      0.32681902062398094,
      0.32108438908617076,
      0.3307172412173782,
      0.4407787211333454,
      0.3517872585411874,
      0.2831945073101172,
      0.35117555874253026,
      0.35408816390429365,
      0.31104196347918656,
      0.34242167398430157,
      0.30951696652862437,
      0.350250608404739,
      0.28113428726440004
    ],
    [
      0.32081499582969397,
      0.46837358843629895,
      0.4772341861228633,
      0.4838974817447059,
      0.3716544912660025,
      0.4793688852388016,
      0.33391035106681577,
      0.4290510402650156,
      0.39021918921485366,
      0.25712214575965375,
      0.5041496250982724,
      0.20657854172745038,
      0.4553613775497978,
      0.4937053035456851,
      0.4161057979177505,
      0.0,
      0.3463495001092318,
      0.3874300496788905,
      0.4018805167306685,
      0.4335719180022486,
      0.342384348115109,
      0.3106224628615639,
      0.4085253615310782,
      0.38288510406067333,
      0.311005490489658,
      0.3579763757382928,
      0.3231840527326675,
      0.382867862230617,
      0.2575147490777463
    ],
    [
      0.3098186624519057,
      0.37347959784783225,
      0.36471371783296247,
      0.4394880343969132,
      0.42743930645663863,
      0.4040796708262382,
      0.3114633671565874,
      0.4804735859790774,
      0.3217264507838218,
      0.3722348152237953,
      0.33958499418290344,
      0.2614249002379079,
      0.3291911328271384,
      0.38913715250349856,
      0.37565679463503265,
      0.36884534328499763,
      0.0,
      0.39117734741094723,
      0.4160054494850989,
      0.3986071362996817,
      0.42503239596985076,
      0.29989894815125395,
      0.34511775761327335,
      0.3899219251235997,
      0.36517073128669364,
      0.42757623253178534,
      0.3538034245298265,
      0.391725666609243,
      0.2817758739339833
    ],
    [
      0.3471766251164172,
      0.43357867083668733,
      0.4440769922499841,
      0.4110159718828439,
      0.5130437106078207,
      0.43158935056817094,
      0.35285008976253796,
      0.41834076992751723,
      0.3811177486729027,
      0.3574891574174479,
      0.4130333228716794,
      0.28298814118191773,
      0.4669291680654748,
      0.46554050132134916,
      0.41223595090062415,
      0.3884534698312414,
      0.39063928575502826,
      0.0,
      0.39620161302401935,
      0.42424311493317224,
      0.39120670919642353,
      0.34514947919934946,
      0.36497715927814456,
      0.4272545927119784,
      0.4138854333613715,
      0.3924512312854955,
      0.41364416476113797,
      0.39681919042514546,
      0.32312142147633294
    ],
    [
      0.28646335956031455,
      0.37504504692208385,
      0.4038081553394144,
      0.47536806403107645,
      0.4000023086369915,
      0.34820301468246906,
      0.32834949791177714,
      0.41465611785361034,
      0.39720903005884645,
      0.3340496668431454,
      0.3443571373765586,
      0.2477024370307399,
      0.43360545117218763,
      0.39517948431632144,
      0.3836236407151856,
      0.3959059254581443,
      0.3836495826773856,
      0.33953287166293866,
      0.0,
      0.4236052880837393,
      0.29838562376592903,
      0.29756121266431235,
      0.3479026208178757,
      0.34295681860178684,
      0.3225628890186001,
      0.4038625375957614,
      0.36347776406253796,
      0.36438704851279935,
      0.28394868776322246
    ],
    [
      0.3444194930491986,
      0.46550889091380054,
      0.5424277497183392,
      0.4621360024343797,
      0.46334951430049687,
      0.467338164493349,
      0.33929513284536816,
      0.4500470598390438,
      0.43903167297484336,
      0.32566745274312225,
      0.48167798507929027,
      0.2479471801391242,
      0.5211139264236495,
      0.4732884785745697,
      0.45701291919987774,
      0.45329552235364834,
      0.37132582360266575,
      0.40826754348662586,
      0.4742628311061694,
      0.0,
      0.40086479365915695,
      0.37704166265288563,
      0.4031016326090697,
      0.42370739560421433,
      0.4340474339448437,
      0.4440388175238177,
      0.4166149926903202,
      0.4223842165509404,
      0.3726958993419178
    ],
    [
      0.3159093868548992,
      0.37613461471285325,
      0.37502105861651835,
      0.343696863964827,
      0.32654221716993814,
      0.35999960362131667,
      0.2794371567556364,
      0.32512866728605694,
      0.3228933127231395,
      0.25568335044977974,
      0.3688376912934088,
      0.2127192123651771,
      0.3230159252920264,
      0.4165876925789471,
      0.3712921947165655,
      0.3399459592616223,
      0.29303149462954114,
      0.3748046545569821,
      0.3217873087139951,
      0.3608371012334963,
      0.0,
      0.29870892885549694,
      0.36413790797731926,
      0.3406904438141154,
      0.3360834443182945,
      0.2927303666713412,
      0.35568525294978404,
      0.36579143286315574,
      0.30390811182713406
    ],
    [
      0.2737862567602489,
      0.30618769433294424,
      0.314866749159447,
      0.27747465865588716,
      0.33809157379148047,
      0.2707028737635755,
      0.31701844443179117,
      0.3411466793853375,
      0.31771696637379754,
      0.29255557994407533,
      0.27176381106967584,
      0.24824898963872033,
      0.2957697678826925,
      0.3137341385741075,
      0.32165372134527637,
      0.2852031341787422,
      0.2939611436429339,
      0.276208837170955,
      0.30736091682370703,
      0.3197726007130728,
      0.3181414349450482,
      0.0,
      0.33198472723317596,
      0.2844378098979983,
      0.34635482429207154,
      0.3085538283453402,
      0.3109207251128612,
      0.331028258713455,
      0.34109618586759805
    ],
    [
      0.36835480388219977,
      0.4310673100209319,
      0.44888603119939563,
      0.41717873289106744,
      0.41510232074529796,
      0.44858182606212504,
      0.3944311175324411,
      0.47649679753015994,
      0.4093545737614699,
      0.3675533166359355,
      0.43871072187527216,
      0.2534944299076125,
      0.40734988397550165,
      0.45832275623023655,
      0.4654391595398675,
      0.4635606980334239,
      0.4082388888117956,
      0.4050146066232383,
      0.378782912828761,
      0.422504965752595,
      0.38457006770100355,
      0.3849790703702545,
      0.0,
      0.423995915906642,
      0.46632009366713567,
      0.3804341431222069,
      0.5247252593319256,
      0.4178778037134647,
      0.39261714666561365
    ],
    [
      0.3146490801250752,
      0.38481101672310136,
      0.40834066434957506,
      0.3982701537011939,
      0.4002285575916287,
      0.3749313365422191,
      0.27433150586258837,
      0.37496481108085833,
      0.36435417611813703,
      0.28909129219542873,
      0.4387323063709019,
      0.22001889298448551,
      0.38089654011528573,
      0.4309233799665626,
      0.4160427190619911,
      0.43060466442914236,
      0.4001854937243474,
      0.4133709290584966,
      0.3506497820681891,
      0.42424977921198126,
      0.3809888681594964,
      0.2841700945937442,
      0.3799080851711598,
      0.0,
      0.3622974234611511,
      0.34695340593379065,
      0.3769409350612769,
      0.4152101823410197,
      0.29365781546761727
    ],
    [
      0.2553602252164062,
      0.3682245537897617,
      0.4232731555178135,
      0.37092816208752777,
      0.39768346414110645,
      0.3735010211489922,
      0.31436328215404696,
      0.37233606710652256,
      0.3562051959817252,
      0.4456707848590846,
      0.3696892753252987,
      0.24169967821633742,
      0.3822348990370976,
      0.39103908708730994,
      0.37975737303889723,
      0.3529572731758259,
      0.3970308043708666,
      0.3789188148729894,
      0.3388520634048777,
      0.4025180064165059,
      0.380914031347666,
      0.37705526333603023,
      0.3941036033908001,
      0.3529750199931381,
      0.0,
      0.33612139306887734,
      0.4221987523265114,
      0.39069449850664695,
      0.33007864912251206
    ],
    [
      0.30986679855810584,
      0.41140726480871836,
      0.3865255311065978,
      0.4426829114294655,
      0.4318578481680144,
      0.3916117204618679,
      0.37915380357343076,
      0.46202000709460433,
      0.39407098146434594,
      0.32750066035820957,
      0.36972897402129523,
      0.2809279657577566,
      0.3387798046979287,
      0.4003858827645892,
      0.40754745848354124,
      0.35121554555161527,
      0.38434564076796085,
      0.38083723589009777,
      0.45241836197480745,
      0.4546937756308733,
      0.34551144161160163,
      0.309496721182585,
      0.37418092450947027,
      0.34019724320009437,
      0.3642414887828418,
      0.0,
      0.33008937348866985,
      0.3400104568826665,
      0.3711052547715141
    ],
    [
      0.3008004750182933,
      0.35113474759926055,
      0.4193436652510012,
      0.3283073730423469,
      0.40202445883619276,
      0.3440348565754807,
      0.2690241348378102,
      0.34048615853334274,
      0.32653239174902127,
      0.33333829185297414,
      0.36040568774179915,
      0.20478333148513883,
      0.36392373275050827,
      0.37604336372967984,
      0.3337564579399128,
      0.36239980931386584,
      0.34582118554975794,
      0.369366366543471,
      0.3770614298088639,
      0.39384106715066336,
      0.3620633283686425,
      0.3278895994956317,
      0.40595434589517954,
      0.38246112174105806,
      0.41009804830802965,
      0.3168934438489177,
      0.0,
      0.36561974210496895,
      0.2950500450587379
    ],
    [
      0.3107679895787727,
      0.3534669361109455,
      0.4523907903172917,
      0.40274548668893706,
      0.37343029416238593,
      0.3503557800911934,
      0.28265640177033036,
      0.40696860453076833,
      0.42127585113980826,
      0.2846442365621704,
      0.36587642128236264,
      0.2337288543226539,
      0.3908121983305366,
      0.39881193788909486,
      0.38914450611259777,
      0.3344898865179857,
      0.3339509611468905,
      0.34859124139069664,
      0.3465667255841829,
      0.3930709766592324,
      0.3379909415710449,
      0.30425556246482666,
      0.36777270568336706,
      0.35735216797963854,
      0.33411120151956886,
      0.31584678023594126,
      0.3092844203938774,
      0.0,
      0.2837628158364909
    ],
    [
      0.3056181405546705,
      0.3242345032655696,
      0.3518604514220707,
      0.3250601468954779,
      0.33307334329562743,
      0.32871691171743533,
      0.32598134986145144,
      0.32619816725673956,
      0.3418373414831415,
      0.28602307029674545,
      0.31640026788460673,
      0.26140733460604304,
      0.33970736993041495,
      0.3197617576296399,
      0.3286651967923204,
      0.32027286097439367,
      0.290182201911414,
      0.31377696774912733,
      0.3041974616461125,
      0.3323404206837053,
      0.3254782422053848,
      0.3474936103751851,
      0.3636715620442923,
      0.3359846245126241,
      0.35731585435614077,
      0.34259533149878263,
      0.31858302181995235,
      0.36168787848949724,
      0.0
    ]
  ],
  "row_avgs": [
    0.1990727133736319,
    0.38655997332159403,
    0.38762621216757953,
    0.3821523335981046,
    0.3408210947912643,
    0.4193215492278904,
    0.28554141397840516,
    0.3520201318571868,
    0.37066566317904,
    0.33068111379092635,
    0.4379443801396739,
    0.20730869951902758,
    0.4412534954808333,
    0.39887792215273155,
    0.35381695067469776,
    0.3833480282907895,
    0.369806086270446,
    0.3999661798793648,
    0.3619771886834199,
    0.424353935280526,
    0.3328943341454775,
    0.3055622261445007,
    0.41621233408277053,
    0.3689204961239444,
    0.36772801421575635,
    0.3761575384640453,
    0.348873523576091,
    0.3494329527097712,
    0.32600447825566314
  ],
  "col_avgs": [
    0.30281820730718856,
    0.38614581766509987,
    0.4153924260241955,
    0.41061988472327526,
    0.3782257176916466,
    0.3746148814710458,
    0.31318648615386263,
    0.3956548076675971,
    0.3695714488919078,
    0.3078102870103357,
    0.38089091047839385,
    0.23867722992105275,
    0.3855569623437634,
    0.4131567020007222,
    0.39597157106081243,
    0.37675940270053676,
    0.3424957922038919,
    0.35469155719599543,
    0.3674116237137476,
    0.40466309397356925,
    0.3485968402101934,
    0.3166106703802633,
    0.36232955464191613,
    0.3539857767063835,
    0.357611525259568,
    0.3522903136928051,
    0.3496832623881105,
    0.365029316598452,
    0.30444889329882135
  ],
  "combined_avgs": [
    0.2509454603404102,
    0.38635289549334695,
    0.4015093190958875,
    0.39638610916068995,
    0.35952340624145546,
    0.39696821534946813,
    0.2993639500661339,
    0.37383746976239196,
    0.3701185560354739,
    0.31924570040063105,
    0.4094176453090339,
    0.22299296472004015,
    0.41340522891229836,
    0.40601731207672687,
    0.3748942608677551,
    0.38005371549566314,
    0.35615093923716895,
    0.3773288685376801,
    0.36469440619858373,
    0.41450851462704763,
    0.34074558717783543,
    0.31108644826238196,
    0.38927094436234333,
    0.36145313641516397,
    0.3626697697376622,
    0.36422392607842524,
    0.3492783929821007,
    0.3572311346541116,
    0.3152266857772422
  ],
  "gppm": [
    611.8410559117999,
    589.9720629856091,
    571.929143310841,
    577.1129756244792,
    590.0375643271353,
    594.9299574644286,
    621.2862157957204,
    582.8450523079383,
    595.732072279453,
    625.1259108869829,
    592.9803659630062,
    652.8716195728693,
    591.3773493086583,
    579.1707006616358,
    583.2541894845068,
    592.9512690036152,
    610.0834420753463,
    605.5511669554219,
    598.7526325294101,
    579.9294005180622,
    604.827661253672,
    624.9860710094979,
    601.2129989630182,
    604.709064586864,
    602.4570702255434,
    605.7571412944142,
    604.6786810227766,
    597.1869311015279,
    628.417129619161
  ],
  "gppm_normalized": [
    1.4273291938674848,
    1.3236961708797585,
    1.277956510515093,
    1.2983328154434974,
    1.3138757027111774,
    1.3300560715285294,
    1.4031201893811645,
    1.300957626116768,
    1.3307469639027187,
    1.3948262890351493,
    1.318182722392333,
    1.4762116414202626,
    1.3199197490743653,
    1.2933220395789486,
    1.3032728763477557,
    1.331542022658039,
    1.3601522846455196,
    1.3576262759010744,
    1.3365220607936952,
    1.291366132350293,
    1.3485082778691895,
    1.400017612899093,
    1.3419380634268172,
    1.348153785210272,
    1.339047266647091,
    1.3602912658757416,
    1.344072171048183,
    1.3334647737710497,
    1.3998864906899229
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401
  ],
  "response_lengths": [
    2260,
    2551,
    2840,
    2608,
    2630,
    2587,
    2830,
    2550,
    2285,
    2216,
    2453,
    3306,
    2377,
    2546,
    2763,
    2567,
    2007,
    2450,
    2200,
    2532,
    2453,
    2383,
    2036,
    2524,
    2428,
    2202,
    2450,
    2683,
    2299
  ]
}