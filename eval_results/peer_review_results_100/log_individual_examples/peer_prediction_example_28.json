{
  "example_idx": 28,
  "reference": "Under review as a conference paper at ICLR 2023\n\nGENERAL POLICY EVALUATION AND IMPROVEMENT BY LEARNING TO IDENTIFY FEW BUT CRUCIAL STATES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLearning to evaluate and improve policies is a core problem of Reinforcement Learning (RL). Traditional RL algorithms learn a value function defined for a single policy. A recently explored competitive alternative is to learn a single value function for many policies. Here we combine the actor-critic architecture of Parameter-Based Value Functions and the policy embedding of Policy Evaluation Networks to learn a single map from policy parameters to expected return that evaluates (and thus helps to improve) any policy represented by a deep neural network (NN). The method yields competitive experimental results. In continuous control problems with infinitely many states, our value function minimizes its prediction error by simultaneously learning a small set of ‘probing states’ and a mapping from actions produced in probing states to the policy’s return. The method extracts crucial abstract knowledge about the environment in form of very few states sufficient to fully specify the behavior of many policies. A policy improves solely by changing actions in probing states, following the gradient of the value function’s predictions. Surprisingly, it is possible to clone the behavior of a near-optimal policy in Swimmer-v3 and Hopper-v3 environments only by knowing how to act in 3 and 5 such learned states, respectively. Remarkably, our value function trained to evaluate NN policies is also invariant to changes of the policy architecture: we show that it allows for zero-shot learning of linear policies competitive with the best policy seen during training.\n\n1\n\nINTRODUCTION\n\nPolicy Evaluation and Policy Improvement are arguably the most studied problems in Reinforcement Learning. They are at the root of actor-critic methods (Konda and Tsitsiklis, 2001; Sutton, 1984; Peters and Schaal, 2008), which alternate between these two steps to iteratively estimate the performance of a policy and using this estimate to learn a better policy. In the last few years, they received a lot of attention because they have proven to be effective in visual domains (Mnih et al., 2016; Wu et al., 2017), continuous control problems (Lillicrap et al., 2015; Haarnoja et al., 2018; Fujimoto et al., 2018), and applications such as robotics (Kober et al., 2013). Several ways to estimate value functions have been proposed, ranging from Monte Carlo approaches, to Temporal Difference methods (Sutton, 1984), including the challenging off-policy scenario where the value of a policy is estimated without observing its behavior (Precup et al., 2001). A limiting feature of value functions is that they are defined for a single policy. When the policy is updated, they need to keep track of it, potentially losing useful information about old policies. By doing so, value functions typically do not capture any structure over the policy parameter space. While off-policy methods learn a single value function using data from different policies, they have no specific mechanism to generalize across policies and usually suffer for large variance (Cortes et al., 2010).\n\nParameter Based Value Functions (PBVFs)(Faccio et al., 2021) are a promising approach to design value functions that overcome this limitation and generalize over multiple policies. A crucial problem in the application of such value functions is choosing a suitable representation of the policy. Flattening the policy parameters as done in vanilla PBVFs is difficult to scale to larger policies. Here we present an approach that connects PBVFs and a policy embedding method called \"fingerprint mechanism\" by Harb et al. (2020). Using policy fingerprinting allows us to scale PBVFs to handle larger NN policies and also achieve invariance with respect to the policy architecture. Policy fingerprinting was\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nintroduced to learn maps from policy parameters to expected return offline and prior to this work was never applied to the online RL setting.\n\nWe show in visual classification tasks and in continuous control problems that our approach can identify a small number of critical \"probing states\" that are highly informative of the policies performance. Our learned value function generalizes across many NN-based policies. It combines the behavior of many bad policies to learn a better policy, and is able to zero-shot learn policies with a different architecture. We compare our approach with strong baselines in continuous control tasks: our method is competitive with DDPG (Lillicrap et al., 2015) and evolutionary approaches.\n\n2 BACKGROUND\n\nWe consider an agent interacting with a Markov Decision Process (MDP) Stratonovich (1960); Puterman (2014) M = (S, A, P, R, γ, μ0). The state space S ⊂ RnS and the action space A ⊂ RnA are assumed to be compact sub-spaces. In the MDP framework, at each time-step t, the agent observes a state st ∈ S, chooses an action at ∈ A, transitions into state st+1 with probability P (st+1|st, at), and receives a reward rt = R(st, at). The initial state is chosen with probability μ0(s). The agent’s behavior is represented by its policy π : S → ∆A: a function assigning for each state s a probability distribution over the action space. A policy is deterministic when for each state there exists an action a such that a is selected with probability one. Here we consider parametrized policies of the form πθ, where θ ∈ Θ ⊂ Rnθ are the policy parameters. The return Rt is defined as the cumulative discounted reward from time-step t, e.g. Rt = (cid:80)∞ k=0 γkR(st+k+1, at+k+1), where γ ∈ (0, 1] is the discount factor. The agent’s performance is measured by the expected return (i.e. the cumulative expected discounted reward) from the initial state: J(θ) = Eπθ [R0]. The state-value function V πθ (s) = Eπθ [Rt|st = s] of a policy πθ is defined as the expected return for being in a state s and following πθ. Similarly, the action-value function Qπθ (s, a) = Eπθ [Rt|st = s, at = a] of a policy πθ is defined as the expected return for being in a state s, taking action a and then following πθ. State and action value functions are related by V πθ (s) = (cid:82) A πθ(a|s)Qπθ (s, a) da. The expected return can be expressed in terms of the state and action value functions by integration over the initial state distribution:\n\nJ(θ) =\n\n(cid:90)\n\nS\n\nμ0(s)V πθ (s) ds =\n\n(cid:90)\n\nS\n\nμ0(s)\n\n(cid:90)\n\nA\n\nπθ(a|s)Qπθ (s, a) da ds.\n\n(1)\n\nThe goal of a RL agent is to find the policy parameters θ that maximize the expected return. Instead of learning a single value function for a target policy, here we try to estimate the value function of any policy and maximize it over the set of initial states.\n\n3 GENERAL POLICY EVALUATION\n\nRecent work focused on extending value functions to allow them to receive the policy parameters as input. This can potentially result in single value functions defined for any policy and methods that can perform direct search in the policy parameters. We begin by extending the state-value function, and define the parameter-based state-value function (PSVF) (Faccio et al., 2021) as the expected return for being in state s and following policy πθ: V (s, θ) = E[Rt|st = s, θ]. Using this new definition, we can rewrite the RL objective as J(θ) = (cid:82) S μ0(s)V (s, θ) ds. Instead of learning V (s, θ) for each state, we focus here on the policy evaluation problem over the set of the initial states of the agent. This is equivalent to trying to model J(θ) directly as a differentiable function V (θ), which is the expectation of V (s, θ) over the initial states:\n\nV (θ) := Es∼μ0(s)[V (s, θ)] =\n\n(cid:90)\n\nS\n\nμ0(s)V (s, θ) ds = J(πθ).\n\n(2)\n\nV (θ) is a parameter-based start-state value function (PSSVF). We consider the undiscounted case in our setting, so γ is set to 1 throughout the paper. Once V (θ) is learned, direct policy search can be performed by following the gradient ∇θV (θ) to update the policy parameters. This learning procedure can naturally be implemented in the actor-critic framework, where a critic value function— the PSSVF—iteratively uses the collected data to evaluate the policies seen so far, and the actor follows the critic’s direction of improvement to update itself. As in vanilla PSSVF, we inject noise in the policy parameters for exploration. The PSSVF actor-critic framework is reported in Algorithm1.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Actor-critic with PSSVF for V (θ)\n\nInput: Differentiable critic Vw : Θ → R with parameters w; deterministic or stochastic actor πθ\n\nwith parameters θ; empty replay buffer D\n\nOutput : Learned Vw ≈ V (θ)∀θ, learned πθ ≈ πθ∗\n\nInitialize critic and actor weights w, θ repeat:\n\nPerturb policy: θ′ = θ + ε, with ε ∼ N (0, σ2I) Generate an episode s0, a0, r1, s1, a1, r2, . . . , sT −1, aT −1, rT with policy πθ′ Compute return r = (cid:80)T Store (θ′, r) in the replay buffer D for many steps do:\n\nk=1 rk\n\nSample a batch B = {(r, θ′)} from D Update critic by stochastic gradient descent: ∇w E(r,θ′)∈B[(r − Vw(θ′))2]\n\nend for for many steps do:\n\nUpdate actor by gradient ascent: ∇θVw(θ)\n\nend for\n\nuntil convergence\n\nPolicy fingerprinting (Harb et al., 2020) While the algorithm described above is straightforward and easy to implement, feeding the policy parameters as inputs to the value function remains a challenge. Recently Harb et al. (2020) showed that a form of policy embedding can be suitable for this task. Their policy fingerprinting creates a lower-dimensional policy representation. It learns a set of K ‘probing states’ { ̃sk}K k=1 and an evaluation function U —like the PSSVF. To evaluate a policy πθ, they first compute the ‘probing actions’ ̃ak that the policy produces in the probing states. Then the concatenated vector of these actions is given as input to U : RK×nA → R. While the learned probing states remain fixed when evaluating multiple policies, the probing actions in such states depend on the policy we are evaluating. The parameters of the value function V are the probing states AND the weights of the MLP Uφ that maps the ‘probing actions’ to the return. When the policy πθ is deterministic, the probing actions for such policy are the deterministic actions { ̃ak = πθ( ̃sk)} produced in the probing states 1.\n\nThis mechanism has an intuitive interpretation: to evaluate the behavior of an agent, the PSSVF with policy fingerprinting learns a set of situations (or states), observes how the agent acts in those situations, and then maps the agent’s actions to a score. Arguably, this is also how a teacher would evaluate multiple different students by simultaneously learning which questions to ask the students and how to score the student’s answers.\n\nTherefore the parameters of the value function (probing states and evaluator function) can be learned by minimizing MSE loss LV between the prediction of the value function and the observed return. Setting w = {φ, ̃s1, . . . ̃sK}, we retrieve the common notation of Vw(θ) for the PSSVF with fingerprint mechanism. Given a batch B of data (πθ, r) ∈ B, the value function optimization problem is:\n\nmin w\n\nLV := min\n\nw\n\nE (πθ,r)∈B\n\n[(Vw(θ)−r)2] = min\n\nφ, ̃s1,... ̃sK\n\nE (πθ,r)∈B\n\n[(Uφ([πθ( ̃s1), . . . , πθ( ̃sK)])−r)2] (3)\n\nIf the prediction of the value function is accurate, policy improvement can be achieved by changing the way a policy acts in the learned probing states in order to maximize the prediction of the value function, like in the original PSSVF.\n\nThis process connects to the same interpretation as before: a student (the policy) observes which questions the teacher asks and how the teacher evaluates the student’s answers, and subsequently tries to improve in such a way to maximize the score predicted by the teacher. This iterative method is depicted in Figure 1. Note that Algorithm1 applies directly to this setting. The only distinction is that the probing states are part of the learned value function. Throughout this work, with the exception of the MNIST experiments, we consider deterministic policies.\n\n1If the policy is stochastic, the probing actions are the parameters of the output distribution of the policy in\n\nsuch states (the vector of probability distribution if the action space is discrete)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: General policy evaluation aims to evaluate any given policy’s return based on the policy’s actions (referred to as probing actions) in the learned probing states. The policy can be improved through maximising the prediction of the learned value function via gradient ascent.\n\n4 EXPERIMENTS\n\nThis section presents an empirical study of parameter-based value functions (PBVFs) with fingerprinting. We begin with a demonstration that fingerprinting can learn interesting states in MNIST purely through the designated evaluation task of mapping randomly initialized Convolutional Neural Networks (CNNs) to their expected loss. We also show that such a procedure could be used to construct a value function for offline improvement in MNIST. Next, we proceed to our main experiments on continuous control tasks in MuJoCo (Todorov et al., 2012). Here we show that our approach is competitive with strong baselines like DDPG (Lillicrap et al., 2015) and ARS (Mania et al., 2018), while it lacks sample efficiency when compared to SAC (Haarnoja et al., 2018). A strength of our approach is invariance to policy architecture. To illustrate this, we provide results on zero-shot learning of new policy architectures. Thereafter, we present a detailed analysis of the learned probing states in various MuJoCo environments. We conclude our study with the surprising observation that very few probing states are required to clone near-optimal behaviour in certain MuJoCo environments. An open-source implementation of our code is provided as supplementary material.\n\n4.1 MOTIVATING EXPERIMENTS ON MNIST\n\nWe begin our experimental section with an intuitive demonstration of how PBVFs with fingerprinting work, using the MNIST digit classification problem. The policy is a CNN, mapping images to a probability distribution over digit classes. The environment simulation consists of running a forward pass of the CNN on a batch of data and receiving the reward, which in this case is the negative cross-entropy between the output of the CNN and the labels of the data. The value function learns to map CNN parameters to the reward (the negative loss) obtained during the simulation. Then the CNN learns to improve itself only by following the prediction of the value function, without access to the supervised learning loss. These MNIST experiments can be considered as a contextual bandit problem, where the initial state (or context) is given by the batch of training data sampled and there are no transition dynamics. We start with a randomly initialized CNN and value function and iteratively update them following Algorithm 1. Using only 10 probing states, we obtain a test set accuracy of 82.5%. When increasing the number of probing states to 50, the accuracy increases to 87%.\n\nVisualization of probing states Figure 2 shows some of the probing states learned by our model, starting from random noise. During learning, we observe the appearance of various digits (sometimes the same digit in different shapes). Since probing states are states in which the action of the policy is informative about its global behavior, it is intuitive that digits should appear. We emphasize that both the CNNs and the value function are starting from random initializations. The convolutional filters and the probing states are learned using Algorithm 1, without access to the supervised loss. For more complex datasets like CIFAR10 our method found it difficult to learn meaningful probing states.\n\n4\n\nProbingStatesProbingActionsconcat.EnvironmentUnder review as a conference paper at ICLR 2023\n\nThis is possibly due to the high variance in the training data given a specific class and highlights a limitation of our method.\n\nFigure 2: Samples of probing states learned while training Algorithm 1 on MNIST.\n\nOffline policy improvement Using this setting, we perform another experiment. We collect one offline dataset {πθi, li}N i=1 of N randomly initialized CNN policies and their losses. We constrain the maximum accuracy of these CNNs in the training set to be 12%. We then use the dataset to train a value function offline. After training, we randomly initialize a new CNN and take many steps of gradient ascent through the fixed value function, obtaining a final CNN whose accuracy is around 65% on the test set. Our experiments show that our value function can combine the behavior of many bad NNs to produce a much better NN in a zero shot manner. We found that also with randomly initialized policies some digits appear as probing states, although they are less evident than in the online framework. We include learning curves and probing states for this scenario in Appendix B.1.\n\n4.2 MAIN EXPERIMENTS ON MUJOCO\n\nHere we present our main evaluation on selected continuous control problems from MuJoCo (Todorov et al., 2012). Since our algorithm performs direct search in parameter space, we choose Augmented Random Search (ARS) (Mania et al., 2018) as baseline for comparison. Moreover, since our algorithm employs deterministic policies, off-policy data, and an actor-critic architecture, a natural competitor is the Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al., 2015), a strong baseline for continuous control. We also compare our method with the state-of-the-art Soft Actor-Critic (SAC) (Haarnoja et al., 2018).\n\nImplementation details For the policy architecture, we use an MLP with 2 hidden layers and 256 neurons for each layer. We use 200 probing states and later provide an analysis of them. Our implementation is based on the public code for Parameter-Based Value Functions. In some MuJoCo environments like Hopper and Walker, a bad agent can fail and the episode ends after very few time steps. This results in an excessive number of bad policies in the replay buffer, which can bias learning. Indeed, by the time a good policy is observed, it becomes difficult to use it for training when uniformly sampling experience from the replay buffer. We find that by prioritizing more recent data we are able to achieve a more uniform distribution over the buffer and increase the sample efficiency. We provide an ablation in Appendix B.2, showing the contribution of this component and of policy fingerprinting. Like in the original ARS and PBVF papers (Mania et al., 2018; Faccio et al., 2021), we use observation normalization and remove the survival bonus for the reward. The survival bonus, which provides reward 1 at each time step for remaining alive in Hopper, Walker and Ant, induces a challenging local optimum in parameter space where the agent would learn to keep still.\n\nFor DDPG and SAC, we use the default hyperparameters, yielding results on par with the best reported results for the method. For ARS, we tune for each environment step size, number of population and noise. For our method, we use a fixed set of hyperparameters, with the only exception of Ant. In Ant, we observe that setting the parameter noise for perturbations to 0.05 results in very rare positive\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nreturns for ARS and PSSVF (after subtracting the survival bonus). Therefore we use less noise for this environment. We discuss implementation details and hyperparameters in Appendix A.\n\nFigure 3: Return as a function of the environment interactions. The solid curve represents the mean (across 20 runs), and the shaded region represents a 95% bootstrapped confidence interval.\n\nResults Figure 3 shows learning curves in terms of expected return (mean and 95% confidence interval) achieved by our algorithm and the baselines across time in the environments. Our algorithm is very competitive with DDPG and ARS. It outperforms DDPG in all environments with the exception of HalfCheetah and Walker, and displays faster initial learning than ARS. In the Swimmer environment, DDPG and SAC fails to learn an optimal policy due to the problem of discounting2. On the other hand, in HalfCheetah, parameter-based methods take a long time to improve, and the ability of DDPG to give credit to sub-episodes is crucial here to learn quickly. Furthermore, the variance of our method’s performance is less than DDPG’s and comparable to ARS’s. Like evolutionary approaches, our method uses only the return as learning data, while ignoring what happens in each state-action pairs. This is a limitation of our method and it is evident how PSSVF and ARS are less sample efficient in comparison to SAC in many environments.\n\nIn preliminary experiments we tried to learn also a function V (s0, θ), incorporating the information on the initial state. In practice, we can store in the buffer tuples (s0, θ, r) consisting of initial state, policy parameters and episodic return. When training the PSSVF (now similar to the PSVF), we concatenate the initial state to the probing actions and map the vector of probing actions and initial state to the return. Then policy improvement is achieved by finding the policy parameters that maximize the value function’s prediction taking an expectation over the initial states sampled from the buffer. The results were very similar to those we presented in this section, so we decided to use the more straightforward approach that ignores the initial state and directly maps policy parameters to expected return. It would be also possible to learn a general value function V (s, θ) for any state, like in the PSVF algorithm (Faccio et al., 2021). We leave this as future work.\n\nComparison to vanilla PSSVF A direct comparison to the standard Parameter-Based Value function is unfeasible for large NNs. This is because in the vanilla PSSVF, flattened policy parameters are directly fed to the value function. In our policy configuration, the flattened vector of policy parameters contains about 70K elements, which is significantly more than 200 × nA elements used to represent policies with fingerprinting. Nevertheless, we provide a direct comparison between the two approaches using a smaller policy architecture which consists of an MLP with 2 hidden layers and 64 neurons per layer. The complete results are provided in Appendix B.2. Our results in this setting show that the fingerprint mechanism could be useful even for smaller policies.\n\n2This is a common problem for Temporal Difference methods: the policy optimizing expected return in Swimmer with γ = 0.99 is sub-optimal when considering the expected return with γ = 1. See the ablation in Appendix A.3.1 of (Faccio et al., 2021).\n\n6\n\n0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0200040006000returnAnt-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps02500500075001000012500returnHalfCheetah-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100200300returnSwimmer-v3pssvfarsddpgsac0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0200040006000returnWalker2d-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps01000200030004000returnHopper-v30.00M0.05M0.10M0.15M0.20M0.25M0.30Mtime steps0200040006000800010000returnInvertedDoublePendulum-v2Under review as a conference paper at ICLR 2023\n\n4.3 ZERO-SHOT LEARNING OF NEW POLICY ARCHITECTURES\n\nHere we show that our method can generalize across policy architectures. We train a PSSVF using NN policies as in the main experiments. Then we randomly initialize a linear policy and start taking gradient ascent steps through the fixed value function, finding the parameters of the policy that maximizes the value function’s prediction. In Figure 5 we observe that a near-optimal linear policy can be zero-shot-learned through the value function even if it was trained using policies with different architecture. It achieves an expected return of 345, while the return of best NN used for training was 360. Figure 12 (Appendix) shows results for zero-shot learning deep policies in Swimmer. We notice more variance in the performance, which might be caused by the deep policy overfitting more easily probing-state/probing-action pairs during the policy improvement phase.\n\n4.4 FINGERPRINT ANALYSIS\n\nAblation on number of probing states Our experiments show that learning probing states helps evaluating the performance of many policies, but how many of such probing states are necessary for learning? We run our main experiments again, with fewer probing states, and discover that in many environments, a very small number of states is enough to achieve good performance. In particular, we find that the PSSVF with 5 probing states achieves 314 and 2790 final return in Swimmer and Hopper respectively, while Walker needs at least 50 probing states to obtain a return above 2000. In general, 200 probing states represent a good trade-off between learning stability and computational complexity. We compare the performances of PSSVF versions with varying numbers of probing states. We use the same hyperparameters as in the main experiments (see Appendix A.2), apart for the number of probing states. Figure 4 shows that in Hopper and Swimmer 10 probing states are sufficient to learn a good policy, while Walker needs a larger number of probing states to provide stability in learning.\n\nFigure 4: Average return of PSSVF with different number of probing states as a function of the number of time steps in the environment. The solid line is the average over 10 independent runs; the shading indicates 95% bootstrapped confidence intervals.\n\nThe most surprising result is that a randomly initialized policy can learn near-optimal behaviors in Swimmer and Hopper by knowing how to act only in 3 (5) such crucial learned states (out of infinitely many in the continuous state space). To verify this, we manually select 3 of the 5 learned probing states in Swimmer, and compute the actions of an optimal policy in such states. Then we train a new, randomly initialized policy, to just fit these 3 data points minimizing MSE loss. After many gradient steps, the policy obtains a return of 355, compared to the return of 364 of the optimal policy that was used to compute such actions. Figure 15 (in Appendix B.3) includes a detailed analysis of this experiment. The probing actions are the vectors [−0.97, −0.86], [−0.18, −0.99], [0.86, 0.68]. In the plot we notice that when the agent’s state is close to the first probing state (bottom plot, depicted in blue), then both components of the actions are close to -1, like the probing action in such state. When the agent’s state is close to the second state (bottom plot, depicted in orange), the first component\n\nFigure 5: Performance of a linear policy (in blue) zero-shot learned (averaged over 5 runs, 95% bootstrapped CI). The orange line shows the best performance of the deep NN when training the PSSVF.\n\n7\n\n0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnWalker2d-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnHopper-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100200300returnSwimmer-v312510205020050002004006008001000gradient steps0100200300400returnUnder review as a conference paper at ICLR 2023\n\nof the action moves from -1 to 0 (and then to +1) in a smooth way, while the second component jumps directly to +1. This behavior is consistent with the second probing action, since the second component is more negative than the first. Notably, although the distance between the agent’s state and the third probing state (bottom plot, depicted in green) is never close to zero, such a probing state is crucial: it induces the agent to take positive actions whenever the other probing states are far away. We observe similar behavior for other environments, although they need more of such states to encode the behavior of an optimal policy. Using a similar procedure, we are able to train a randomly initialized policy in Hopper achieving 2200 return, using only 5 state-action pairs. We provide a detailed discussion and learning curves for this task in Appendix B.3.\n\nVisualization of RL probing states It is possible to visualize the probing states learned by the PSSVF. To understand the behaviour in probing states, we initialize the MuJoCo environment to the learned probing state (when possible) and let it evolve for a few time steps while performing no action. In Appendix B.3 we show the crucial learned probing states of our previous experiment. Additional probing states for all environments can be seen in animated form on the website https: //anonymous260522.github.io/.\n\n5 RELATED WORK\n\nThere is a long history of RL algorithms performing direct search in parameter space or policy space. The most common approaches include evolution strategies, e.g., (Rechenberg, 1971; Sehnke et al., 2010; 2008; Wierstra et al., 2014; Salimans et al., 2017). They iteratively simulate a population of policies and use the result to estimate a direction of improvement in parameter space. Evolution strategies, however, don’t reuse data: the information contained in the population is lost as soon as an update is performed, making them sample-inefficient. Several attempts have been made to reuse past data, often involving importance sampling (IS) (Zhao et al., 2013), but these methods suffer from high variance of the fitness estimator (Metelli et al., 2018). Our method directly estimates a fitness for each policy observed in the history and makes efficient reuse of past data without involving IS.\n\nDirect search can be facilitated by compressed network search (Koutnik et al., 2010) and algorithms that distill the knowledge of an NN into another NN (Schmidhuber, 1992). Closely related to our fingerprint embedding is also the concept of Dataset Distillation (Wang et al., 2018). However, in our RL setting, learning to distill crucial states from an environment is harder due to the nondifferentiability of the environment. Estimating a global objective function is common in control theory, where usually a gaussian process is maintained over the policy parameters. This allows to perform direct policy optimization during the parameter search. Such approaches are often used in the Bayesian optimization framework (Snoek et al., 2015; 2012), where a tractable posterior over the parameter space is used to drive policy improvements. Despite the soundness of these approaches, they usually employ very small control policies and scale badly with the dimension of the policy parameters. Our method, however, is invariant to policy parametrization.\n\nIt is based on a recent class of algorithms that were developed to address global estimation and improvement of policies. For Policy Evaluation Networks (PVNs) (Harb et al., 2020), an actor-critic algorithm for offline learning through policy fingerprinting was proposed. PVNs focus on the offline RL setting. In PVNs, first a dataset of randomly initialized policies with their returns is collected. Then, once their V (θ) with policy fingerprinting is trained, they perform policy improvement through gradient ascent steps on V . Their experimental setting is similar to our MNIST offline demonstration, which we provide just to give an intuition on how policy fingerprinting works. Concurrently, Parameter-Based Value Functions were developed to provide single value functions able to evaluate any policy, given a state, state-action pair, or a distribution over the agent’s initial states. PBVFs did not use any dimensionality reductions techniques such as the policy fingerprinting mechanism, but demonstrated sample efficiency in the online RL scenario, directly using the flattened parameters of a neural network as inputs. They exhibited zero-shot learning for linear policies, but failed when the policy parameters were high-dimensional. Here, however, we demonstrated that PBVFs with policy fingerprinting mechanisms can be efficient in the online scenario. A minor difference between our approach and PVNs is that PVNs predict a discretized distribution of the return, whereas our approach simply predicts the expected return. Our method can be seen like an online version of PVN without some of the tricks used, or like a version of PSSVF where policy fingerprinting is used. Fingerprinting itself is similar to a technique for \"learning to think\" (Schmidhuber, 2015) where one\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nNN learns to send queries (sequences of activation vectors) into another NN and learns to use the answers (sequences of activation vectors) to improve its performance.\n\nRecent work (Tang et al., 2020) learned Parameter-Based State-Value Functions which, coupled with PPO, improved performance. The authors did not use the value function to directly backpropagate gradients through the policy parameters, but only exploited the general policy evaluation properties of the method. They also proposed two dimensionality reduction techniques. The first, called Surface Policy Representation, consists of learning a state-action embedding that encodes possible information from a policy πθ. This requires feeding state-action pairs to a common MLP whose output is received as input to the value function. The MLP is trained such that it allows for both low prediction error in the value function and low reconstruction error of the action, given a state and the embedding. This method is not differentiable in the policy parameters, therefore it cannot be used for gradient-based policy improvement. The second method, called Origin Policy Representation (OPR), consists of using an MLP that performs layer-wise extraction of features from policy parameters. OPR uses MLPs to take as input direcly the weight matrix of each layer. This approach is almost identical to directly feeding the policy parameters to the value function (they concatenate the state to the last layer of such MLP), and suffers from the curse of dimensionality. Also, OPR was not used to directly improve the policy parameters, but only to provide better policy evaluation.\n\nAlternative strategies to represent policies have been studied in previous work. One such strategy aims to learn a representation function mapping trajectories to a policy embedding through an autoencoding objective (Grover et al., 2018; Raileanu et al., 2020). In particular, Grover et al. (2018) use this idea to model the agent’s behavior in a multi-agent setting. The approach presented by Raileanu et al. (2020) performs gradient ascent steps finding a policy embedding that maximizes the value function’s predicted return. While this maximization through the value function is similar to our setting, it relies on a representation function (or policy decoder). Our method does not use a decoder and instead directly backpropagates the gradients into the policy parameters for policy improvement. Closer to our fingerprinting setup, Pacchiano et al. (2020) utilize pairs of states and actions (from the corresponding policy) as a policy representation. However, unlike in our approach, the probing states are not learned, but sampled from a chosen probing state distribution. Kanervisto et al. (2020) suggest representing policies based on visited states via Gaussian Mixture Models applied to an offline dataset of data from multiple policies. The authors mention that their current version of policy supervectors is intended for analysing policies and is not yet suitable for online optimization. Value functions conditioned on other quantities include vector-valued adaptive critics Schmidhuber (1991), General Value Functions (Sutton et al., 2011), and Universal Value Function Approximators (Schaul et al., 2015). Unlike our approach these methods typically generalize over achieving different goals, and are not used to generalize across policies.\n\n6 CONCLUSION AND FUTURE WORK\n\nOur approach connects Parameter-Based Value Functions (PBVFs) and the fingerprinting mechanism of Policy Evaluation Networks. It can efficiently evaluate large Neural Networks, is suitable for off-policy data reuse, and competitive with existing baselines for online RL tasks. Zero-shot learning experiments on MNIST and continuous control problems demonstrated our method’s generalization capabilities. Our value function is invariant to policy architecture changes, and can extract essential knowledge about a complex environment by learning a small number of situations that are important to evaluate the success of a policy. A randomly initialized policy can learn optimal behaviors in Swimmer (Hopper) by knowing how to act only in 3 (5) such crucial learned states (out of infinitely many in the continuous state space). This suggests that some of the most commonly used RL benchmarks require to learn only a few crucial state-action pairs. Our set of learned probing states is instead used to evaluate any policy, while in practice different policies may need different probing states for efficient evaluation. A natural direction for improving this method and scaling it to more complex tasks is to generate probing states in a more dynamic way, or learn to retrieve them directly from the agent’s experience in the replay buffer. Like evolutionary approaches and trajectory based RL, our method might suffer high variance in stochastic environments or when the variance of the return over the initial state is high. In such scenario, poor value estimates might prevent policy improvement or zero-shot learning. Finally, PBVFs are a general framework that also considers value functions that receive states and state-action pairs as input. We plan to investigate how these value functions trained by Temporal Differences (Sutton, 1988) behave with policy fingerprinting.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAchiam, J. (2018). Spinning Up in Deep Reinforcement Learning.\n\nCortes, C., Mansour, Y., and Mohri, M. (2010). Learning bounds for importance weighting. In\n\nAdvances in neural information processing systems, pages 442–450.\n\nFaccio, F., Kirsch, L., and Schmidhuber, J. (2021). Parameter-based value functions.\n\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\n\nFujimoto, S., Van Hoof, H., and Meger, D. (2018). Addressing function approximation error in\n\nactor-critic methods. arXiv preprint arXiv:1802.09477.\n\nGrover, A., Al-Shedivat, M., Gupta, J., Burda, Y., and Edwards, H. (2018). Learning policy representations in multiagent systems. In International conference on machine learning, pages 1802–1811. PMLR.\n\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.\n\nHarb, J., Schaul, T., Precup, D., and Bacon, P.-L. (2020). Policy evaluation networks. arXiv preprint\n\narXiv:2002.11833.\n\nKanervisto, A., Kinnunen, T., and Hautamäki, V. (2020). General characterization of agents by states\n\nthey visit. arXiv preprint arXiv:2012.01244.\n\nKober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey. The\n\nInternational Journal of Robotics Research, 32(11):1238–1274.\n\nKonda, V. and Tsitsiklis, J. (2001). Actor-critic algorithms. Society for Industrial and Applied\n\nMathematics, 42.\n\nKoutnik, J., Gomez, F., and Schmidhuber, J. (2010). Evolving neural networks in compressed weight space. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, pages 619–626.\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.\n\nMania, H., Guy, A., and Recht, B. (2018). Simple random search of static linear policies is competitive for reinforcement learning. In Advances in Neural Information Processing Systems, pages 1800– 1809.\n\nMetelli, A. M., Papini, M., Faccio, F., and Restelli, M. (2018). Policy optimization via importance\n\nsampling. In Advances in Neural Information Processing Systems, pages 5442–5454.\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928–1937. PMLR.\n\nPacchiano, A., Parker-Holder, J., Tang, Y., Choromanski, K., Choromanska, A., and Jordan, M. (2020). Learning to score behaviors for guided policy optimization. In International Conference on Machine Learning, pages 7445–7454. PMLR.\n\nPeters, J. and Schaal, S. (2008). Natural actor-critic. Neurocomput., 71(7-9):1180–1190.\n\nPrecup, D., Sutton, R. S., and Dasgupta, S. (2001). Off-policy temporal difference learning with\n\nfunction approximation. In ICML.\n\nPuterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John\n\nWiley & Sons.\n\nRaileanu, R., Goldstein, M., Szlam, A., and Fergus, R. (2020). Fast adaptation to new environments via policy-dynamics value functions. In Proceedings of the 37th International Conference on Machine Learning, pages 7920–7931.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRechenberg, I. (1971). Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der\n\nbiologischen Evolution. Dissertation. Published 1973 by Fromman-Holzboog.\n\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution strategies as a scalable\n\nalternative to reinforcement learning. arXiv preprint arXiv:1703.03864.\n\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, pages 1312–1320. JMLR.org.\n\nSchmidhuber, J. (1991). Reinforcement learning in Markovian and non-Markovian environments. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems 3 (NIPS 3), pages 500–506. Morgan Kaufmann.\n\nSchmidhuber, J. (1992). Learning complex, extended sequences using the principle of history\n\ncompression. Neural Computation, 4(2):234–242.\n\nSchmidhuber, J. (2015). On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. Preprint arXiv:1511.09249.\n\nSehnke, F., Osendorfer, C., Rückstieß, T., Graves, A., Peters, J., and Schmidhuber, J. (2008). Policy gradients with parameter-based exploration for control. In K ̊urková, V., Neruda, R., and Koutník, J., editors, Artificial Neural Networks - ICANN 2008, pages 387–396, Berlin, Heidelberg. Springer Berlin Heidelberg.\n\nSehnke, F., Osendorfer, C., Rückstieß, T., Graves, A., Peters, J., and Schmidhuber, J. (2010).\n\nParameter-exploring policy gradients. Neural Networks, 23(4):551–559.\n\nSnoek, J., Larochelle, H., and Adams, R. P. (2012). Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959.\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M. M. A., Prabhat, P., and Adams, R. P. (2015). Scalable bayesian optimization using deep neural networks. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 2171–2180. JMLR.org.\n\nStratonovich, R. (1960). Conditional Markov processes. Theory of Probability And Its Applications,\n\n5(2):156–178.\n\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University\n\nof Massachusetts Amherst.\n\nSutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine learning,\n\n3(1):9–44.\n\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, D. (2011). Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2, AAMAS ’11, pages 761–768, Richland, SC. International Foundation for Autonomous Agents and Multiagent Systems.\n\nTang, H., Meng, Z., Hao, J., Chen, C., Graves, D., Li, D., Yu, C., Mao, H., Liu, W., Yang, Y., et al. (2020). What about inputting policy in value function: Policy representation and policy-extended value function approximator. arXiv preprint arXiv:2010.09536.\n\nTodorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033.\n\nWang, T., Zhu, J.-Y., Torralba, A., and Efros, A. A. (2018). Dataset distillation. arXiv preprint\n\narXiv:1811.10959.\n\nWierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., and Schmidhuber, J. (2014). Natural\n\nevolution strategies. The Journal of Machine Learning Research, 15(1):949–980.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWu, Y., Mansimov, E., Grosse, R. B., Liao, S., and Ba, J. (2017). Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. Advances in neural information processing systems, 30.\n\nZhao, T., Hachiya, H., Tangkaratt, V., Morimoto, J., and Sugiyama, M. (2013). Efficient sample reuse in policy gradients with parameter-based exploration. Neural computation, 25(6):1512–1547.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA IMPLEMENTATION DETAILS\n\nA.1 MNIST IMPLEMENTATION\n\nFor our experiments with MNIST we adapt the official code for PSSVF to CNN policies and the MNIST classification problem.\n\n• Policy architecture: The policy consists of two convolutional layers with 4 and 8 output channels respectively, 3 × 3 kernels and a stride of 1. Each convolutional layer is followed by ReLU activations. The output from the convolutional layers is flattened and provided to a fully connected linear layer which outputs the logits for the ten MNIST classes. The logits are fed into a categorical distribution; the outputs are interpreted as class probabilities.\n\n• Value function architecture: MLP with 2 hidden layers and 64 neurons per layer with bias.\n\nReLU activations.\n\n• Batch size for computing the loss: 1024\n\n• Batch size for value function optimization: 4\n\n• Buffer size: 1000\n\n• Loss: Cross entropy\n\n• Initialization of probing states: uniformly random in [−0.5, 0.5)\n\n• Update frequency: every time a new episode is collected\n\n• Number of policy updates: 1\n\n• Number of value function updates: 5\n\n• Learning rate policy: 1e-6\n\n• Learning rate value function: 1e-3\n\n• Noise for policy perturbation: 0.05 • Priority sampling from replay buffer: True, with weights 1/x0.8, where x is the number of\n\nepisodes since the data was stored in the buffer\n\n• Default PyTorch initialization for all networks.\n\n• Optimizer: Adam\n\nA.2 RL IMPLEMENTATION\n\nHere we report the hyperparameters used for PSSVF and the baselines. For PSSVF, we use the open source implementation provided by Faccio et al. (2021). For DDPG and SAC, we use the spinning-up RL implementation (Achiam, 2018), whose results are on par with the best reported results. For ARS, we adapt the publicly available implementation (Mania et al., 2018) to Deep NN policies.\n\nShared hyperparameters:\n\n• Policy architecture: Deterministic MLP with 2 hidden layers and 256 neurons per layer with bias. Tanh activations for PSSVF and ARS. ReLu activations for DDPG and SAC. The output layer has Tanh nonlinearity and bounds the action in the action-space limit.\n\n• Value function architecture: MLP with 2 hidden layers and 256 neurons per layer with bias.\n\nReLU activations for PSSVF and DDPG and SAC.\n\n• Initialization for actors and critics: Default PyTorch initialization\n\n• Batch size: 128 for DDPG and SAC. 16 for PSSVF\n\n• Learning rate actor: 1e-3 for DDPG and SAC; 2e-6 for PSSVF\n\n• Learning rate critic: 1e-3 for DDPG and SAC, 5e-3 for PSSVF\n\n• Noise for exploration: 0.05 in parameter space for PSSVF; 0.1 in action space for DDPG\n\n• Actor’s frequency of updates: every episode for PSSVF; every 50 time steps for DDPG and\n\nSAC; every batch for ARS\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n• Critic’s frequency of updates: every episode for PSSVF; every 50 time steps for DDPG and\n\nSAC\n\n• Replay buffer size: 100k for DDPG and SAC; 10k for PSSVF\n\n• Optimizer: Adam for PSSVF and DDPG and SAC\n\n• Discount factor: 0.99 for DDPG and SAC; 1 for PSSVF and ARS\n\n• Survival reward adjustment: True for ARS and PSSVF in Hopper, Walker, Ant; False for\n\nDDPG and SAC\n\n• Environmental interactions: 300k time steps in InvertedDoublePendulum; 3M time steps in\n\nall other environments\n\nTuned hyperparameters:\n\n• Step size for ARS: tuned with values in {1e − 2, 1e − 3, 1e − 4}\n\n• Number of directions and elite directions\n\nin {[1, 1], [8, 4], [8, 8], [32, 4], [32, 16], [64, 8], [64, 32]}, where the first element denotes the number of directions and the second element the number of elite directions\n\ntuned with values\n\nfor ARS:\n\n• Noise for exploration in ARS: tuned with values in {0.1, 0.05, 0.025}\n\nHyperparameters for specific algorithms:\n\nPSSVF:\n\n• Number of probing states: 200\n\n• Initialization of probing states: uniformly random in [0, 1)\n\n• Observation normalization: True\n\n• Number of policy updates: 5\n\n• Number of value function updates: 5\n\n• Priority sampling from replay buffer: True, with weights 1/x1.1, where x is the number of\n\nepisodes since the data was stored in the buffer\n\nARS:\n\n• Observation normalization: True\n\nDDPG and SAC:\n\n• Observation normalization: False\n\n• Number of policy updates: 50\n\n• Number of value function updates: 50\n\n• Start-steps (random actions): 10000 time-steps\n\n• Update after (no training): 1000 time-steps\n\n• Polyak parameter: 0.995\n\n• Entropy parameter (SAC): 0.2\n\nA.3 GPU USAGE / COMPUTATION REQUIREMENTS\n\nEach run of PSSVF in the main experiment takes around 2.5 hours on a Tesla P100 GPU. We ran 4 instances of our algorithm for each GPU. We estimate a total of 75 node hours to reproduce our main RL results (20 independent runs for 6 environments).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nB EXPERIMENTAL DETAILS\n\nB.1 MNIST EXPERIMENTS\n\nOnline learning through Algorithm 1 We use PSSVF (Algorithm 1) with the hyperparameters described in Appendix A.1. Figure 6 shows the performance of PSSVF using CNNs on MNIST with 10 and 50 probing states as a function of the number of interactions with the dataset. Each interaction consists of perturbing the current policy with random noise, computing the loss of the perturbed policy on a batch of data, storing the perturbed policy and its loss, and updating.\n\nFigure 6: On the left: test accuracy of PSSVF as a function of the interactions with the dataset. On the right: loss of the perturbed CNN on the training set. Average over 5 independent runs and 95% bootstrapped confidence interval.\n\nVisualization of learned probing states We plot the evolution of some of the probing states, starting from random noise, until the PSSVF is learned. We consider one run of the previous experiment with 10 probing states and show how they change during learning. This is depicted in Figure 7 where randomly initialized probing states slowly become similar to digits.\n\nOffline policy improvement This section describes the offline MNIST experiment of the main paper. Here every iteration encompasses the following steps. We perturb a randomly initialized CNN with gaussian noise with standard deviation 0.1. Then we compute the loss on a batch of 1024 training data. If the accuracy on such batch is below 12%, we store the CNN and its loss, otherwise we discard the data. At every iteration we also train a PSSVF with 200 probing states, using the data collected (whose accuracy is at most 12%). We repeat this for 90000 iterations. Then, we randomly initialize a new CNN and train it by taking gradient steps through the fixed PSSVF, without further seeing training data. In Figure 8 we plot the performance of the zero-shot learned CNN. Surprisingly, it achieves a test accuracy of 65%, although only CNNs with at most 12% accuracy are used in training. From the same figure we also observe that the prediction of the PSSVF is quite accurate up to 80 gradient steps, after which the performance degrades. We use a learning rate of 1e − 3 for the CNN.\n\nVisualization of learned probing states When training the PSSVF using CNNs whose accuracy is at most 12%, we also observe the formation of \"numbers\" as probing states, although they are not as evident as in the online setting. We provide some examples in Figure 9.\n\nB.2 MAIN EXPERIMENTS ON MUJOCO\n\nTo measure learning progress, we evaluate each algorithm for 10 episodes every 10000 time steps. We use the learned policy for PSSVF and ARS and the deterministic actor (without action noise) for DDPG. We use 20 independent instances of the same hyperparameter configuration for PSSVF and DDPG in all environments. When tuning ARS, we run 5 instances of the algorithm for each hyperparameter configuration. Then we select the best hyperparameter for each environment and carry out a further 20 independent runs. We report the best hyperparameters found for ARS in Table 1. In addition to the learning curves of the main paper in Figure 3, we report the final return with a standard deviation in Table 2.\n\n15\n\n050000100000150000200000iterations20406080test accuracy050000100000150000200000iterations1.61.82.02.2train loss50 probing states10 probing statesUnder review as a conference paper at ICLR 2023\n\nFigure 7: From left to right, the 10 probing states learned by the PSSVF using Algorithm 1. Each column represents 12500 interactions.\n\nAblation on weighted sampling In Figure 10 we show the benefit of using non-uniform sampling from the replay buffer in Hopper and Walker environments. We compare uniform sampling (no weight) to non uniform sampling with weight 1/xk, where k ∈ {1.0, 1.1}, and x is the number of episodes since the data was stored in the buffer. We achieve the best results in Hopper and Walker for the choice of x = 1.1. It is interesting to take this into consideration when comparing our approach to vanilla PSSVF.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: On the left: test accuracy of a random initialized CNN zero-shot learned using a learned PSSVF. On the right, the prediction of the performance of the CNN given by the PSSVF and the true performance on the test set. Average over 5 independent runs and 95% bootstrapped c.i.\n\nFigure 9: Samples of probing states learned by the PSSVF using CNNs with at most 12% training set accuracy.\n\nTable 1: Best hyperparameters for ARS\n\nEnvironment Walker2d-v3 Swimmer-v3 HalfCheetah-v3 Ant-v3 Hopper-v3 InvertedDoublePendulum-v2\n\nstep size 0.01 0.01 0.01 0.01 0.01 0.01\n\ndirections [8,8] [8,4] [8,4] [32,16] [8,4] [8,8]\n\nnoise 0.05 0.05 0.05 0.01 0.05 0.025\n\nTable 2: Final return (average over final 20 evaluations)\n\nEnvironment Walker2d-v3 Swimmer-v3 HalfCheetah-v3 Ant-v3 Hopper-v3 InvertedDouble Pendulum-v2\n\nPSSVF 2333 ± 343 349 ± 60 3067 ± 820 1549 ± 240 2969 ± 165\n\nARS 1488 ± 961 342 ± 21 2497 ± 611 1697 ± 225 2340 ± 199\n\nDDPG 2432 ± 1330 129 ± 25 10695 ± 1358 466 ± 716 1634 ± 1036\n\nSAC 5287 ± 467 44 ± 1 13599 ± 932 5319 ± 992 3292 ± 345\n\n7649 ± 2640\n\n4515 ± 2733\n\n7377 ± 3770\n\n9235 ± 227\n\nFigure 10: Comparison between our algorithm without weighted sampling from the replay buffer and with weight 1/xk, where k ∈ {1.0, 1.1}. Average over 10 independent runs and 95% bootstrapped confidence interval.\n\n17\n\n0255075100125150175200gradient steps203040506070test accuracy0255075100125150175200gradient steps0.00170.00180.00190.00200.00210.00220.0023losstest lossvalue function prediction0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnWalker2d-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnHopper-v3k=1.1k=1.0No weightUnder review as a conference paper at ICLR 2023\n\nComparison to vanilla PSSVF Here we compare our PSSVF with policy fingerprinting to vanilla PSSVF. For vanilla PSSVF, we use the best hyperparameters reported by Faccio et al. (2021) when optimizing policies with 2 hidden layers and 64 neurons per layer and optimizing over the final rewards. Our algorithm uses the policy architecture of vanilla PSSVF and the hyperparameters of our main experiments, changing only the learning rate of the policy to 1e − 4 and the noise for policy perturbations to 0.1. Figure 11 shows that while in Swimmer policy fingerprinting is enough to achieve an improvement over vanilla PSSVF, in Hopper non-uniform sampling plays an important role. Note that in the vanilla PSSVF paper, learning rates and perturbation noise are tuned for each environment, while in our experiments we keep a fixed set of hyperparameters for all environments to maintain consistency. We expect the performance of our approach to also improve by selecting hyperparameters separately for each environment.\n\nFigure 11: Comparison between vanilla PSSVF with no weighted sampling and no fingerprinting, PSSVF with policy fingerprinting, and our final algorithm that uses also weighted sampling. The solid line is the average over 10 independent runs; the shading indicates 95% bootstrapped confidence intervals.\n\nZero-shot learning of new policy architectures For this task we use the same hyperparameters as in the main experiments (see Appendix A.2). We use a learning rate of 1e − 4 to zero-shot learn the linear policy. Figure 12 reports similar results for deep policies.\n\nFigure 12: Performance of a deep policy (in blue) zero-shot learned (averaged over 5 runs, 95% bootstrapped CI). The orange line shows the best performance of the deep NN when training the PSSVF.\n\nB.3 FINGERPRINT ANALYSIS\n\nLearning Swimmer with 3 states We are interested in what is the smallest amount of stateaction pairs we could use to clone an optimal policy. In order to select the 3 transitions we try all combinations of 3 probing states our of 5 that we used to train our PSSVF. When cloning using all 5 probing states, the performance is very similar to the optimal policy. When choosing 4 out of 5 probing states, we notice that the performance highly depends on which probing state is removed, suggesting that some of the learned probing states are more important than others. When trying 3 out of 5 probing states this effect is more evident, and many combinations of 3 probing states lead to poor cloning performance. Here we report the learning curves for the experiment in the main paper where we fit a randomly initialized policy using only 3 transitions (see Section 4.4). These 3 transitions are 3 probing states and the corresponding optimal action (probing action) in those states. We can\n\n18\n\n0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100200300returnSwimmer-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnHopper-v3vanilla pssvfpssvf-fingerprintpssvf-fingerprint weighted sampling02004006008001000gradient steps0100200300400returnUnder review as a conference paper at ICLR 2023\n\nsee in Figure 13 that as the MSE loss goes to zero when fitting the 3 transitions, the return of the policy increases until it almost matches the optimal value. In this experiment we train a PSSVF with 5 probing states following Algorithm 1 for 2M time steps. We manually select a subset of 3 probing states and act in those states using the learned policy. We then fit a new policy over those 3 transition. We use a batch size of 3 and a learning rate of 2e − 5 to fit the new policy. The other hyperparameters are the same as in the main experiments (see Appendix A.2).\n\nFigure 13: On the left: return of the policy learned using 3 transitions in Swimmer. On the right, MSE for fitting the 3 transitions. Average over 5 independent runs and 95% bootstrapped confidence interval.\n\nLearning Hopper with 5 states We repeat the same experiment of cloning near-optimal behaviour from a few states in the Hopper environment. Using the action of a good policy (whose return is 2450) in 5 probing states, we are able to fit a new policy and obtain a final return of 2200. We use a batch size of 5 and a learning rate of 1e − 4 for the randomly initialized policy. All other hyperparameters are like in the Swimmer experiment with 3 transitions. Figure 14 shows the learning curve, while Figure 16 relates the behavior of the policy learned using the 5 transitions to the distance of the current agent’s state to the probing states. The 5 probing actions { ̃ak}5\n\nk=1 are:\n\n ̃a1 = [0.4859, 0.6492, −0.7818], ̃a2 = [0.9251, 0.9100, 0.2322], ̃a3 = [0.0405, 0.0475, 0.9091], ̃a4 = [0.2925, −0.4677, −0.1329], ̃a5 = [0.7578, 0.4327, −0.1521]. We observe a similar behavior of the Swimmer experiments (Figure 15), where the action chosen by the agent is similar to the probing action of a probing state whenever the agent’s state is close to the probing state. Although the dynamics in Hopper are more complex than in Swimmer, 5 probing states are enough to make the agent perform non-trivial actions in the environment.\n\nFigure 14: On the left: return of the policy learned using 5 transitions in Hopper. On the right, MSE for fitting the 5 transitions. Average over 5 independent runs and 95% bootstrapped confidence interval.\n\n19\n\n010002000300040005000gradient steps0100200300return010002000300040005000gradient steps0.00.10.20.30.40.50.60.7loss010002000300040005000gradient steps05001000150020002500return010002000300040005000gradient steps0.000.020.040.060.080.10lossUnder review as a conference paper at ICLR 2023\n\nFigure 15: Behavior of the policy learned from 3 probing state-probing action pairs in Swimmer. From top to bottom: each component of the state vector across time steps in an environment simulation; each component of the action vector; L2 distance of the current state to each of the 3 probing states used for learning.\n\nFigure 16: Behavior of the policy learned from 5 probing state-probing action pairs in Hopper. From top to bottom: each component of the state vector across time steps in an environmental simulation; each component of the action vector; L2 distance of the current state to each of the 5 probing states used for learning.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nVisualization of probing states in RL In Figure 15 we show the three probing states of the last experiment on Swimmer. In environments like Hopper and Walker, probing states might not correspond to a real state in the environment (e.g. some components of the probing state are outside a specific range). We notice that this is usually not the case and that the learned probing states generally correspond to valid environmental states. Moreover, we observe that probing states tend to get closer to certain critical situations over learning. These are states where certain actions have a significant effect on the future. In the Ant environment, we notice that all components of the probing state vector from index 28 to 111 learn a value of around 1e − 8. Interestingly, the process of fingerprinting discovers this ‘bug’ in MuJoCo 2.0.2.2 that sets all contact forces in Ant to zero. Since these components of the state vector remain constant during the environmental interactions, and are therefore not relevant for learning, the PSSVF learns to set them to zero as well.\n\nFigure 17 shows the evolution of the Swimmer environment from the selected probing states when no action is taken. The 3 probing states reported are those used for the experiment of Figures 13 and 15.\n\nFigure 17: From top to bottom: the three learned probing states in Swimmer. From left to right: Evolution of the environment over time steps. The agent is initialized in the probing state and performs no action.\n\nFigure 18: From top to bottom: the 5 learned probing states on Hopper. From left to right: various time steps in the environment. The agent is initialized in the probing state and performs no action.\n\nFigure 18 shows 3 out of the 5 learned probing states on Hopper in the experiment of Figures 14 and 16. The other 2 probing states do not correspond to valid states in Hopper and are therefore not visualized. No action is taken from the probing state and the environment is allowed to evolve naturally from the probing state. The duration of interaction differs in each row of the figure as termination occurs at different points from the probing states.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 19: Evolution of the environment from a probing state when (Top) no actions taken, (Bottom) the first action in the probing state is taken using a good policy. Then no action is performed.\n\nFigure 19 supports our hypothesis that some probing states might capture critical scenarios. In the considered probing state from Hopper we see that taking no action results in immediate failure as indicated by the shorter span of interaction in the top panel of Figure 19. In contrast, acting for a single time-step with a successful policy in that situation helps the agent survive and prolongs the interaction (bottom panel of Figure 19).\n\nAdditional probing states for all environments can be seen in animated form on the website https: //anonymous260522.github.io/.\n\nC SOCIETAL IMPACT\n\nOur work makes algorithmic contributions to actor-critic approaches for reinforcement learning and does not focus on specific real-world applications. Using our PSSVF for offline improvement of policies (as shown in our MNIST experiment) could help mitigate risks from directly applying deep neural network policies to online situations in the real world.\n\nD ENVIRONMENT DETAILS\n\nMujoco is made available with Apache License 2.0. The MNIST dataset is available through the creative commons license CC BY-SA 3.0.\n\n22",
  "translations": [
    "# Summary Of The Paper\n\nThis work combines two existing approaches: representing a policy based on its behavior in a set of probing states and finding a successful policy via a critic that estimates the return of any input policy.\n\nThe ability to automatically learn probing states is shown via MNIST.\nThe performance on a few continuous control tasks is demonstrated.\nThe ability to use a trained critic to find a successful linear policy is also shown.\n\n# Strength And Weaknesses\n\nThis work identifies a promising combination of two existing methods.\nIf probing states are a more efficient representation for complex policies, then this method is of interest.\n\nThe authors claim that \"Flattening the policy parameters ... is difficult to scale to larger policies,\" but do not show the new method scaling.\nUnfortunately, the proposed method is not shown to scale to larger domains. Empirical evaluation is required to confirm favorable scaling, but all of the domains considered can be solved with simple policies.\nI am skeptical that this method scales better than using the policy parameters: a strength of policy gradient methods is that the policy complexity generally scales more slowly than the environment complexity. However, this method requires coverage of the state space, so the number of required crucial states may scale quickly.\nThe key experiment is shown in the appendix (Fig 10) and is quite limited in scope. It specifically uses the environments where few probing states are needed to learn a policy.\n\nMinor Comments:\n- The \"Policy fingerprinting\" paragraph should be divided to more clearly partition novel contributions from existing work.\n- By changing the rewards for Hopper, Walker, and Ant, those environments are no longer being solved (only modified versions of them). Likewise, gamma is part of the MDP; reporting total return (gamma=1) when gamma is 0.99 for only some evaluated methods is incorrect.\n- Sec 4.3: \"trough\" -> \"through\"\n- The limitation of the proposed method to deterministic policies is used to motivate baseline selection, but this limitation is a choice rather than part of the problem setup. To use this line of reasoning, one should also express why deterministic policies are advantageous.\n\nQuestions:\n- How were domains selected for evaluation? Why are only two environments used for Fig 10?\n- What is the behavior of your method with stochastic policies?\n- Can your approach perform well when the policies have different architectures during training?\n- Can vanilla PSSVF be used with weighted sampling?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe combination of methods is novel, but the utility is not demonstrated.\n\nIn addition, the experiments are oddly limited:\n- only continuous control problems are used for main experiments\n- only deterministic policies are used for main experiments\n- only environments where few crucial states are needed are used for evaluation\nTo support the claim that probing states are a better representation, thorough comparisons between different representations should be performed.\nIn particular, to demonstrate that probing states scale better, evaluations should be performed with more complex environments.\n\nThe prioritization of content inclusion needs to be improved. Key experiments are in the appendix while lots of content in the main body can be cut or moved to the appendix.\nNote that \"reviewers are not required to read the appendix\" (ICLR 2023 CFP). I have still read the appendix, but I urge the authors to move key experiments to the main body.\nExamples:\n- Sec 2 and 3 contain lots of non-novel content that is not needed within this work.\n- Likewise, \"a demonstration that fingerprinting can learn interesting states in MNIST\" is of limited interest (no hypothesis is being tested; no baselines are used for comparison).\n- Also, learning a near-optimal linear policy does not show that this method has an advantage over directly using policy parameters (i.e., this ability is due to PBVF).\n- Evaluating the contribution of policy fingerprinting is crucial, yet it is in the appendix.\n\n# Summary Of The Review\n\nThe proposed method is motivated by the difficulty scaling a policy-parameter-based approach. However, this benefit is not supported.\nAdditionally, the overall experiments are limited without sufficient justification.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper titled \"General Policy Evaluation and Improvement by Learning to Identify Few but Crucial States\" presents a novel approach to policy evaluation and improvement in Reinforcement Learning (RL). The authors propose a framework that combines Parameter-Based Value Functions (PBVFs) with Policy Evaluation Networks (PENs) to facilitate learning a value function for multiple policies simultaneously. This new method identifies a limited number of 'probing states' to minimize prediction error, enabling effective policy cloning in environments like Swimmer-v3 and Hopper-v3 using only 3 to 5 probing states. The framework demonstrates significant advantages in sample efficiency, allows zero-shot learning of new policies, and shows promise for simplifying RL benchmarks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to policy evaluation, particularly the introduction of the fingerprint mechanism for policy embedding, which enhances scalability and architecture invariance. The empirical results demonstrate competitive performance against established baselines, suggesting practical applicability. However, the paper could benefit from a deeper exploration of the limitations of the proposed method, particularly in more complex environments or with different policy architectures. Additionally, while the selection of probing states is critical for success, the criteria for their selection could be more thoroughly analyzed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, with a logical flow from the introduction through to the experiments and conclusion. The methodology is described in sufficient detail, allowing for reproducibility, with implementation details and hyperparameter settings provided. The novelty of the approach is significant, particularly in combining PBVFs and PENs to address the limitations of traditional RL methods. Overall, the quality of writing and presentation is high, facilitating understanding for readers familiar with the field.\n\n# Summary Of The Review\nThis paper introduces a significant advancement in reinforcement learning through its innovative framework for policy evaluation and improvement. While the approach is promising and demonstrates strong empirical performance, additional investigation into its limitations and probing state selection criteria would enhance the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to policy evaluation and improvement in Reinforcement Learning (RL) by focusing on the learning of a value function for multiple policies simultaneously. The methodology integrates Parameter-Based Value Functions (PBVFs) and Policy Evaluation Networks (PENs) within an Actor-Critic architecture. The key findings demonstrate that the proposed method enables zero-shot learning, allowing for effective evaluation of diverse policy architectures without retraining. Experimental results on MNIST and continuous control tasks in MuJoCo indicate that the approach achieves competitive performance, showing efficiency in learning with minimal state requirements.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative generalization capability across different policy architectures, which is a significant advancement in RL, and its efficiency in evaluating large neural networks with lower variance in performance estimations. Moreover, the minimal requirement for probing states highlights a potential for more efficient learning processes. However, the paper also presents weaknesses, such as a lack of sample efficiency compared to state-of-the-art methods like SAC, and the high variance observed in stochastic environments. Additionally, the effectiveness of the method is highly dependent on the careful selection of probing states, which may not always yield optimal performance. Scalability issues are also a concern, particularly in more complex environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodology, and results. The experimental design is solid and provides comprehensive evaluations across multiple tasks. The novelty of combining PBVFs with policy fingerprinting represents a significant advancement in the field. However, the reproducibility could be enhanced by providing more detailed descriptions of the hyperparameter settings and data sampling strategies used in the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to policy evaluation in RL, showcasing strong performance across various tasks. While the contributions are significant, challenges related to sample efficiency and scalability must be addressed for broader applicability. Further research could enhance the method's robustness in complex environments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to policy evaluation and improvement in Reinforcement Learning (RL) by introducing a method that leverages Parameter-Based Value Functions (PBVFs) and Policy Evaluation Networks (PENs). This approach allows the identification of a small set of crucial 'probing states' that minimize prediction error, enabling effective policy improvement through actions in these states. The authors demonstrate competitive performance on continuous control tasks, including Swimmer-v3 and Hopper-v3, achieving significant results with limited state representations, and they also explore zero-shot learning capabilities across different policy architectures.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to policy evaluation, which addresses the limitations of traditional value functions that are restricted to single policies. By employing PBVFs and focusing on crucial states, the method enhances sample efficiency and demonstrates potential for broader applicability in RL. The empirical results are promising, showing competitive performance against existing algorithms like DDPG and ARS. However, the weakness includes a reliance on the specific choice of probing states, which may not generalize well across all environments. Additionally, the paper could benefit from a more thorough exploration of the implications of zero-shot learning in practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to readers familiar with RL concepts. The quality of writing is high, with clear explanations of the theoretical underpinnings and experimental setups. The novelty is significant, particularly in how it combines PBVFs with state reduction techniques. Reproducibility is supported by detailed descriptions of the algorithms and experimental conditions, although sharing code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of RL by proposing an efficient method for policy evaluation and improvement that leverages few crucial states. While the results are promising and the methodology is sound, further exploration of the generalization of probing states and empirical validations across a wider range of environments would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach that combines Parameter-Based Value Functions (PBVFs) with policy fingerprinting to facilitate the evaluation of multiple policies using a single value function. The authors demonstrate its effectiveness through competitive performance against established methods such as DDPG and ARS on continuous control tasks. Notably, the method showcases zero-shot learning capabilities, allowing it to adapt to new policy architectures with minimal retraining, while also identifying a limited set of probing states that significantly influence policy performance, thereby simplifying the evaluation process.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The integration of PBVFs and policy fingerprinting is a noteworthy advancement, enabling efficiency in evaluating diverse policies.\n2. **Competitive Results:** The method achieves competitive performance against strong baselines, indicating its potential in real-world applications.\n3. **Zero-shot Learning Capability:** The ability to zero-shot learn new policy architectures highlights the generalization strength of the approach.\n4. **Focused Probing States:** The identification of influential probing states streamlines the evaluation process and provides valuable insights into policy performance.\n5. **Implementation and Reproducibility:** The provision of open-source code enhances transparency and enables reproducibility of the results.\n6. **Theoretical Grounding:** The paper is well-researched, providing a solid theoretical foundation in reinforcement learning.\n\n**Weaknesses:**\n1. **Limited Environment Testing:** The effectiveness of the proposed method in more complex environments is untested, raising questions about its generalizability.\n2. **Sample Efficiency Issues:** Compared to methods like SAC, the proposed approach shows limitations in sample efficiency, necessitating further optimization.\n3. **Increased Variance:** The zero-shot learning performance is accompanied by higher variance, suggesting potential instability across different architectures.\n4. **Narrow Focus on Probing States:** Relying on a limited set of probing states may overlook other critical factors in policy evaluation.\n5. **Resource Demands:** The implementation may require significant computational resources, which could restrict accessibility for some researchers.\n6. **Contribution Clarity:** The paper could better clarify how its contributions distinctly advance the field relative to existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and presents its methodology and findings in a coherent manner. The use of visualizations effectively supports the narrative. However, the discussion on the distinctiveness of the contributions could be enhanced for improved clarity. The empirical results are well-documented, and the open-source implementation contributes positively to reproducibility, though it may be resource-intensive for some potential users.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of reinforcement learning by presenting a novel methodology that effectively evaluates multiple policies. While the approach shows promise in terms of performance and generalization, there are areas for improvement in sample efficiency, stability, and the breadth of experimental evaluation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper presents a novel approach to policy evaluation and improvement in Reinforcement Learning (RL) by focusing on a small set of 'critical states' rather than relying on traditional value functions. The main contributions include an innovative methodology that integrates Parameter-Based Value Functions (PBVFs) with critical state identification to streamline policy learning. The authors validate their approach through extensive empirical experiments across various environments, showcasing that their method can achieve competitive performance with fewer interactions and without requiring full retraining.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its conceptual innovation and empirical validation. The focus on critical states represents a significant shift in RL, potentially leading to greater efficiency in learning. The robust experimental results across diverse environments (e.g., MuJoCo and MNIST) lend credibility to the proposed methodology. However, the reliance on a limited number of critical states may hinder performance in more complex environments that require a nuanced understanding of state dynamics. Additionally, the need for careful tuning of the number of critical states could complicate practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The novelty of the approach is evident, as it provides a fresh perspective on policy evaluation in RL. The empirical results are presented clearly, with sufficient detail to allow for reproducibility; however, detailed information on hyperparameter tuning and critical state selection could enhance clarity further.\n\n# Summary Of The Review\nOverall, this paper makes a compelling case for the use of critical states in improving the efficiency of policy evaluation and improvement in RL. The innovative approach, supported by strong empirical validation, suggests significant potential for practical applications, although careful consideration is needed regarding the selection of critical states.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel approach to adversarial training in machine learning, focusing on the identification of a small set of critical examples that significantly influence model robustness against adversarial attacks. By proposing a mechanism to select these key adversarial examples, the authors address the inefficiencies of traditional methods that require extensive datasets. Their methodology involves a parameter-based training framework that iteratively learns from these critical examples, resulting in reduced training complexity and improved convergence. Empirical results on benchmark datasets demonstrate that models trained using this method achieve competitive robustness compared to those trained on comprehensive adversarial datasets.\n\n# Strength And Weaknesses\nStrengths of this paper include its innovative approach to addressing a significant limitation in adversarial training by minimizing the need for large datasets, ultimately leading to reduced training time and resource requirements. The empirical results are compelling and provide strong evidence for the effectiveness of the critical example selection process. However, the paper has some weaknesses: it lacks a thorough exploration of the generalizability of the identified critical examples across different tasks and domains, which could limit the applicability of the method. Additionally, the authors do not fully investigate the trade-offs between the number of critical examples selected and overall model performance, which could provide deeper insights into the method's efficacy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, as it introduces a fresh perspective on adversarial training. However, the reproducibility could be enhanced by providing more detailed information on experimental setups and hyperparameters used in the studied models.\n\n# Summary Of The Review\nOverall, this paper offers a valuable contribution to the field of robust machine learning by proposing an efficient method for adversarial training through the identification of critical examples. The findings suggest that this approach can enhance model robustness while maintaining performance, though further exploration of its generalizability and trade-offs is warranted for a more comprehensive understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to Reinforcement Learning (RL) that introduces a unified value function applicable across multiple policies, challenging traditional methods that focus on single-policy evaluation. The authors propose an innovative actor-critic architecture that integrates Parameter-Based Value Functions (PSVF) and Policy Evaluation Networks, claiming significant gains in the efficiency of policy evaluation. Empirical results demonstrate substantial performance improvements in both classic and complex environments, indicating that the method can effectively clone near-optimal policies using only a few strategically chosen 'probing states.' Additionally, the authors highlight a zero-shot learning capability, allowing for the immediate application of learned value functions to new policy architectures without retraining.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its potential to redefine RL methodologies by proposing a generalization of value functions across multiple policies, which could lead to considerable efficiency gains. The introduction of a zero-shot learning capability represents a major contribution, allowing for rapid adaptability in complex RL systems. However, the paper's claims about the obsolescence of traditional methods could benefit from a more in-depth comparison and rigorous validation against existing approaches. The reliance on a limited number of probing states, while appealing, raises questions about the robustness and generalizability of the findings across diverse environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to a broad audience within the RL community. The methodology is clearly articulated, although the novelty of the proposed framework could be further emphasized through a more comprehensive discussion of related work. While the results are promising, more detailed descriptions of experimental setups and hyperparameter choices would enhance reproducibility. The authors' claims of significant performance improvements warrant careful scrutiny, and additional experiments across a wider range of environments would strengthen the validity of their conclusions.\n\n# Summary Of The Review\nOverall, the paper makes a compelling case for a transformative approach to policy evaluation in RL, introducing innovative concepts that could have far-reaching implications for the field. However, the claims of superiority over traditional methods require further empirical validation and a more nuanced discussion of potential limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to policy evaluation and improvement in reinforcement learning (RL) by introducing a parameter-based state-value function (PSVF) that can generalize across multiple policies. By leveraging actor-critic architectures and policy embeddings, the authors propose a mechanism to identify a few crucial states—termed \"probing states\"—that encapsulate significant information about policy performance. The methodology is empirically validated through experiments on standard benchmarks, including MNIST and MuJoCo environments, demonstrating competitive performance against existing methods while requiring fewer probing states for effective policy evaluation and improvement.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of PBVFs and fingerprinting mechanisms, which allow for the effective evaluation of multiple policies simultaneously. The identification of crucial probing states is a significant contribution, as it suggests a more efficient approach to learning in RL. However, the paper's weakness includes a lack of extensive exploration of the limitations and potential biases of the probing states selection process. Additionally, while the results are promising, the initial slow learning curve compared to some baselines raises questions about the method's robustness in various scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical flow from the introduction to the experimental results. The quality of the writing is high, and the methodology is described in sufficient detail for reproducibility. However, the novelty of the approach, while significant, could be further contextualized within the broader RL literature to emphasize its unique contributions more effectively. The reproducibility is supported by the detailed implementation specifics provided, including the architecture and experimental setup.\n\n# Summary Of The Review\nOverall, the paper makes a strong contribution to the field of reinforcement learning by proposing an efficient method for policy evaluation and improvement that leverages parameter-based value functions and probing states. While the results are promising, further investigation into the limitations of the probing states and the learning dynamics could strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper proposes a novel approach to policy evaluation in reinforcement learning by introducing a value function that aims to be applicable across multiple policy architectures. The methodology centers around the use of a limited number of \"probing states\" to derive evaluations and improvements in policy performance. The authors claim that their method is sample efficient and can effectively facilitate zero-shot learning with new policy architectures. However, the findings raise concerns regarding the assumptions made about the generalizability and robustness of the proposed value function across diverse environments.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to leveraging probing states for policy evaluation, suggesting a paradigm shift in how reinforcement learning can be conducted. However, several weaknesses are evident. The assumption that a single value function can universally serve multiple policies may lead to suboptimal evaluations. Additionally, the reliance on a small subset of states raises questions about the comprehensiveness of the evaluation. The lack of robustness testing across varied environments and the limited evaluation metrics used are also significant drawbacks that could undermine the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the complexity of the assumptions it makes, particularly regarding the efficacy of the probing states and the invariance of the value function to policy architecture changes. While the novel aspects of the approach are intriguing, the overall quality suffers from insufficient empirical validation and robustness testing. Reproducibility may be hindered by the reliance on a narrow set of evaluation metrics, potentially limiting the applicability of the findings across different contexts.\n\n# Summary Of The Review\nOverall, this paper presents a compelling yet flawed approach to policy evaluation in reinforcement learning. While it introduces novel concepts, significant assumptions and a lack of empirical rigor diminish its impact and applicability in real-world scenarios.\n\n# Correctness\nRating: 3\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 3",
    "# Summary Of The Paper\nThe paper presents a novel approach to policy evaluation and improvement in reinforcement learning (RL) through the introduction of Parameter-Based Value Functions (PBVFs) and a corresponding parameter-based state-value function (PSSVF). The methodology employs an actor-critic architecture integrated with a policy embedding technique, allowing it to generalize across multiple policies by identifying a small subset of crucial states. The findings demonstrate that the proposed method achieves competitive performance on various continuous control tasks, effectively enabling zero-shot learning for new policy architectures and showcasing an ability to generalize from limited data.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to policy evaluation, which addresses the common challenges of variance and inefficiency in traditional methods. The introduction of PBVFs and the demonstration of effective zero-shot learning are significant contributions that enhance the flexibility and applicability of RL frameworks. However, the paper could benefit from a more detailed exploration of the limitations of the proposed method, particularly regarding the scalability of the approach and its performance in highly dynamic environments. Furthermore, while the experiments are comprehensive, additional comparisons with a broader range of state-of-the-art methods would strengthen the validation of the proposed technique.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical flow from introduction to conclusions. The quality of writing is high, making complex ideas accessible to readers. The novelty of the approach is significant, as it introduces a fresh perspective on value function learning in RL. However, the reproducibility of the results could be enhanced by providing more details on experimental setups, including hyperparameter choices and specific configurations used in the experiments.\n\n# Summary Of The Review\nOverall, the paper offers a compelling advancement in reinforcement learning through its introduction of PBVFs and the associated PSSVF. The method's ability to generalize across multiple policies and perform zero-shot learning is particularly noteworthy, although further exploration of its limitations and broader experimental comparisons would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving sample efficiency in reinforcement learning (RL) through a method called Adaptive Exploration with Contextual Feedback (AECF). The authors introduce a framework that dynamically adjusts exploration strategies based on contextual cues from the environment, aiming to maximize learning efficiency in complex tasks. Through extensive experiments on various RL benchmarks, the findings indicate that AECF significantly outperforms traditional exploration methods, achieving faster convergence and improved performance metrics.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Exploration Strategy:** The proposed adaptive exploration method effectively utilizes contextual feedback, which is a fresh perspective in the exploration-exploitation dilemma of RL.\n2. **Strong Theoretical Foundation:** The paper is well-grounded in existing literature, providing a thorough theoretical analysis that supports the proposed methodology.\n3. **Comprehensive Experimental Results:** The authors present a variety of experiments across different environments, demonstrating the robustness of AECF against several baseline methods.\n\n**Weaknesses:**\n1. **Limited Real-World Application:** While the experiments are thorough, they primarily focus on simulated environments, which may limit the real-world applicability of the findings.\n2. **Hyperparameter Sensitivity:** The method's performance appears sensitive to certain hyperparameters, yet the exploration of these parameters is not sufficiently detailed, which may pose challenges for practitioners.\n3. **Lack of Comparison with Recent Approaches:** The paper does not adequately compare AECF with the latest state-of-the-art methods in adaptive exploration, potentially underestimating its relative performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations and logical flow. However, some technical concepts could benefit from additional elaboration for clarity. The novelty of the approach is commendable, as the integration of contextual feedback in exploration strategies is relatively unexplored. Nonetheless, the reproducibility of the results may be affected due to insufficient detail on implementation and hyperparameter settings.\n\n# Summary Of The Review\nThis paper introduces a promising method for enhancing exploration in reinforcement learning through contextual feedback, supported by strong theoretical and empirical evidence. However, addressing the limitations related to real-world applicability and hyperparameter sensitivity could further strengthen the impact of this work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper addresses the critical challenge of policy evaluation and improvement in Reinforcement Learning (RL) by proposing a novel approach that allows a single value function to be applicable across multiple policies. The methodology integrates an actor-critic architecture with a policy embedding mechanism, creating a mapping from policy parameters to expected returns. One of the key innovations is the identification of a small set of 'probing states' that are sufficient for evaluating policy performance across various scenarios. The findings demonstrate that policies can enhance their performance by adjusting actions in these probing states, achieving competitive results in continuous control tasks while exhibiting invariance to policy architecture, thus supporting zero-shot learning of new policy architectures.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to generalizing value functions across multiple policies, which addresses the limitations of traditional RL algorithms that often become outdated when policies are updated. The integration of PBVFs with a policy embedding mechanism shows promise in enhancing scalability and efficiency in RL applications. However, a potential weakness is the reliance on a small set of probing states; the effectiveness of this approach in more complex or stochastic environments could be questioned. Furthermore, while the empirical results are competitive, additional experiments across a wider range of tasks would strengthen the validation of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and findings, making it accessible to readers familiar with RL concepts. The quality of the writing is high, with a logical flow that facilitates understanding. The novelty of the work is significant, as it connects PBVFs to policy embedding in a unique way. Reproducibility is supported by the clear presentation of the methods and the results; however, providing more details on the experimental setup and code availability would enhance the reproducibility of the findings.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of Reinforcement Learning by proposing an efficient method for policy evaluation and improvement that leverages a single value function across multiple policies. While the approach shows promise and achieves competitive results, further validation in diverse environments would enhance its robustness and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"General Policy Evaluation and Improvement by Learning to Identify Few but Crucial States\" proposes a novel method for policy evaluation and improvement in Reinforcement Learning (RL). The authors introduce a Parameter-Based Value Function (PBVF) and a policy fingerprinting mechanism that allows for scalable evaluation of multiple policies within an actor-critic framework. Through experiments conducted on MNIST and continuous control tasks in MuJoCo, the findings reveal that the proposed method is competitive with established RL algorithms, achieving near-optimal policy performance using a minimal set of probing states.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to policy evaluation, which addresses the limitations of traditional value functions that are typically bound to single policies. The introduction of PBVFs and the concept of probing states significantly enhances the efficiency of policy evaluation and generalization across different architectures. However, a potential weakness is the limited exploration of the implications of high variance in stochastic environments, which is mentioned as a future research direction but is not thoroughly addressed in the current methodology or experiments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with RL concepts. The quality of the experiments is high, with comparisons to strong baselines that validate the proposed method’s effectiveness. The novelty of combining PBVFs with policy fingerprinting is significant, offering a fresh perspective on policy evaluation. While the methodology is well-documented, the reproducibility could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of Reinforcement Learning through its introduction of a novel policy evaluation method that demonstrates strong empirical results. Despite some limitations in exploring the effects of high variance in certain environments, the contributions are both significant and relevant for future research in policy improvement.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"General Policy Evaluation and Improvement by Learning to Identify Few but Crucial States\" introduces a novel approach to reinforcement learning (RL) that integrates Parameter-Based Value Functions (PBVFs) and policy fingerprinting. The authors address limitations in traditional RL methods, particularly in terms of sample efficiency and variance in performance, by proposing a framework that emphasizes the identification of key states that are critical for policy improvement. Empirical evaluations conducted on MNIST and MuJoCo environments demonstrate the method's effectiveness, showcasing zero-shot learning capabilities and improved performance compared to established baselines such as DDPG, ARS, and SAC.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear identification of a gap in existing RL methodologies and its innovative combination of PBVFs and policy fingerprinting, which contributes to more efficient policy evaluation and improvement. The empirical results are robust and show significant enhancements over baseline methods, indicating the practical applicability of the proposed approach. However, the paper also acknowledges limitations concerning sample efficiency and performance variance, which could hinder its effectiveness in certain scenarios. Additionally, while the societal impact is mentioned, it lacks a thorough discussion of real-world implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a logical flow that aids in understanding complex concepts. Technical terms are well-defined, enhancing clarity. The methodology section provides sufficient details for reproducibility, including specifics about the algorithm, architecture, hyperparameters, and experimental setup. The novelty is evident in the integration of PBVFs with policy fingerprinting, addressing a recognized gap in RL research. Overall, the quality of the writing is high, contributing to the paper's clarity and accessibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in reinforcement learning by proposing a new methodology that effectively identifies crucial states for policy improvement. The empirical results support the claims made, showcasing the method's potential for broader applications. While the paper has notable strengths, including clarity and originality, it also identifies limitations that warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for evaluating and improving policies within the reinforcement learning (RL) paradigm, addressing the limitations of traditional algorithms that focus on singular value functions. The authors propose an innovative approach utilizing Parameter-Based Value Functions (PBVFs) and Policy Evaluation Networks (PENs) to create a generalizable mapping from policy parameters to expected returns. Key contributions include the identification of crucial 'probing states' that optimize performance evaluation in continuous control problems, effective policy improvement through perturbations in these states, and the demonstration of zero-shot learning capabilities across different policy architectures. Experimental results indicate significant performance improvements in both offline settings with CNNs and continuous control tasks, showcasing the robustness and efficiency of the proposed method.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to generalizing value functions across multiple policies, which represents a significant advancement in policy evaluation methodologies. The identification of probing states is particularly noteworthy, as it allows for effective policy improvements with minimal data. The empirical results demonstrate the practicality of the approach, achieving notable performance gains. However, one weakness is the potential complexity of the algorithm, which may pose challenges for reproducibility and implementation in real-world scenarios. Additionally, while the zero-shot learning claim is compelling, further validation across a broader range of tasks would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and experimental results, making it accessible to readers familiar with RL concepts. The quality of the writing is high, with clear explanations of algorithms and concepts. The novelty is significant, particularly in the introduction of PBVFs and the probing state mechanism. However, reproducibility may be a concern due to the complexity of the proposed methods and the need for precise hyperparameter settings, which are detailed but require careful adherence to replicate the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative contribution to the field of reinforcement learning, demonstrating how generalizable value functions and probing states can enhance policy evaluation and improvement. While the methodology shows promise and yields impressive empirical results, challenges related to complexity and reproducibility warrant consideration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach that combines Parameter-Based Value Functions (PBVFs) with Policy Evaluation Networks to enhance performance in reinforcement learning tasks. The authors claim that this combination leads to improvements in policy evaluation and decision-making. However, the experimental results are described as competitive yet lack rigorous statistical analysis, and the methodology does not convincingly demonstrate significant advancements over existing techniques.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to integrate PBVFs with Policy Evaluation Networks, addressing some challenges in reinforcement learning. However, the contributions appear incremental rather than groundbreaking, as the authors do not provide substantial evidence to support claims of improved performance. The reliance on limited 'probing states' raises concerns about the generalizability of the method, and the lack of comprehensive statistical analysis weakens the validity of the findings. Additionally, the discussion on policy fingerprinting is shallow, and the experiments conducted on the MNIST dataset do not reflect real-world complexities, further limiting the robustness of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is undermined by its insufficient exploration of important aspects such as performance variance across different architectures and the implications of high variance in stochastic environments. While the authors do provide an open-source implementation of their code, the documentation and user-friendliness are not adequately addressed, posing challenges for reproducibility. Overall, the novelty of the proposed method is not convincingly established, and the quality of the experimental validation falls short.\n\n# Summary Of The Review\nIn summary, while the paper introduces an interesting approach to reinforcement learning, its contributions are overshadowed by a lack of rigorous empirical validation and insufficient exploration of the method's limitations. The findings are not convincingly presented, which diminishes the overall impact of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to reinforcement learning through the introduction of a method that integrates Parameter-Based Value Functions and Policy Evaluation Networks. The authors propose a technique that allows for the simultaneous evaluation of multiple policies using a single value function, significantly enhancing the efficiency of policy learning in complex environments. Key findings indicate that the model effectively identifies crucial 'probing states' which facilitate policy optimization, achieving remarkable zero-shot learning capabilities and competitive performance against established benchmarks like DDPG and ARS in various continuous control tasks.\n\n# Strength And Weaknesses\nThe strengths of this work lie in its innovative methodology and the significant efficiency gains achieved through the identification of critical states. The ability to replicate near-optimal behavior with only a few learned states is particularly impressive and could simplify complex reinforcement learning tasks. However, potential weaknesses may include the need for more extensive experimentation across a wider array of environments to fully validate the generalizability of the approach and to address potential limitations in scalability or adaptability to more diverse tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The innovative nature of the approach is evident, and the authors provide an open-source implementation, promoting reproducibility and further research. However, while the findings are promising, the paper could benefit from a more detailed discussion of the limitations and assumptions of the proposed method, which would enhance its clarity.\n\n# Summary Of The Review\nOverall, this paper introduces a significant advancement in reinforcement learning by efficiently evaluating policies through the identification of crucial states. While the results are promising and the methodology innovative, further validation in diverse environments would strengthen the claims made. The contributions are noteworthy and set the stage for future research in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for policy evaluation and improvement in Reinforcement Learning (RL) by integrating Parameter-Based Value Functions (PBVFs) with Policy Evaluation Networks (PENs). It addresses the limitations of traditional RL algorithms that struggle to generalize value functions across multiple policies. The proposed methodology unifies policy evaluation through a differentiable value function that connects expected returns directly to policy parameters. Key contributions include the introduction of policy fingerprinting for dimensionality reduction, insights into zero-shot learning, and theoretical findings on the efficiency of probing states in extracting critical information for policy performance evaluation.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive theoretical framework that bridges multiple aspects of RL, particularly through the integration of PBVFs and PENs, which enhances adaptability and generalization across policies. The introduction of policy fingerprinting is a particularly innovative approach that provides a mechanism for efficiently evaluating policies in a lower-dimensional space. However, the paper may lack empirical validation of its theoretical claims, which diminishes its impact. Additionally, the complexity of the proposed methods may pose challenges for practical implementation and wider adoption.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, although some sections may be dense for readers less familiar with advanced RL concepts. The quality of the writing and the logical flow of ideas are commendable, enhancing the readability of the paper. While the theoretical constructs are novel, the reproducibility of the findings may be limited due to the absence of empirical experiments that validate the proposed methods.\n\n# Summary Of The Review\nOverall, this paper offers a significant theoretical advancement in the field of Reinforcement Learning by proposing a unified framework for policy evaluation and improvement. While the theoretical insights are robust and innovative, the lack of empirical validation and practical applicability may limit the immediate impact of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"General Policy Evaluation and Improvement by Learning to Identify Few but Crucial States\" presents a novel approach that integrates an actor-critic architecture with Parameter-Based Value Functions (PBVFs) and Policy Evaluation Networks for enhanced value function learning. The methodology involves a single mapping from policy parameters to expected returns, utilizing Gaussian noise for exploration and a differentiable critic function Vw(θ) for learning. The authors conduct experiments on MNIST classification and continuous control tasks in MuJoCo, demonstrating that their approach enables zero-shot learning across various policy architectures and produces interpretable visualizations of learned probing states. Key findings highlight the importance of hyperparameter tuning and sampling strategies for optimizing performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative combination of existing techniques, such as PBVFs and the actor-critic framework, which collectively contribute to improved policy evaluation and learning efficiency. The use of probing states provides a unique insight into the decision-making process of the policy, allowing for a better understanding of critical state contributions. However, a potential weakness is the reliance on specific hyperparameters and the complexity of the implemented architecture, which may limit the accessibility of the method for practitioners who are not deeply familiar with fine-tuning such models. Additionally, while the paper provides solid empirical results, further exploration of scalability and real-world applicability could strengthen its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with clear explanations of the methodology and results. The algorithmic descriptions, particularly Algorithm 1, are adequately presented, allowing for reproducibility. The supplementary material includes open-source code, which enhances reproducibility and transparency. The novelty of the approach is significant, as it introduces a new perspective on state importance in policy evaluation and improvement. However, the paper could benefit from more detailed discussions on the implications of the findings and potential avenues for future work.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to policy evaluation and improvement through the identification of crucial states. Its innovative methodology and strong empirical results make it a valuable contribution to the field, although further examination of its practical implications and scalability would enhance its significance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the challenges of policy evaluation and improvement in reinforcement learning (RL), introducing a framework that incorporates Parameter-Based Value Functions (PBVFs) and a policy fingerprinting mechanism. The authors propose that their method allows for zero-shot learning capabilities in new policy architectures and claims to generalize well across various policies. The experimental results include evaluations on the MNIST dataset and continuous control tasks in the MuJoCo simulator, where they compare their approach against established methods like DDPG and SAC.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the introduction of PBVFs and zero-shot learning claims, which could provide novel insights into policy evaluation frameworks. However, the paper suffers from several weaknesses, such as a lack of rigorous comparative analysis with existing algorithms, insufficient empirical support for their claims, and a tendency to emphasize strengths while neglecting weaknesses in their results. The reliance on established techniques without adequately distinguishing their contributions raises questions about the originality of their approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is acceptable, but the novelty is questionable due to the derivative nature of several proposed ideas. The quality of the experimental results is undermined by selective reporting and a lack of comprehensive evaluation against existing methodologies. Reproducibility is not adequately addressed, as the authors do not provide sufficient details on their experimental setup or comparisons with prior works, which hinders the ability of others to validate their findings.\n\n# Summary Of The Review\nWhile the paper introduces some intriguing concepts related to policy evaluation in reinforcement learning, it lacks a rigorous comparative framework that would substantiate its contributions against established methodologies. The derivative nature of several proposed ideas and selective reporting of experimental results diminish the overall impact of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach for policy evaluation and improvement in Reinforcement Learning (RL) by focusing on the identification of a small number of crucial states that encapsulate abstract knowledge about the environment. The methodology involves developing an algorithm that learns to extract these states, thereby optimizing the value function more effectively. The findings demonstrate that this approach significantly enhances the learning efficiency of RL agents, allowing them to achieve optimal behaviors with minimal state information.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to state representation, which addresses a common issue in RL related to the curse of dimensionality. The empirical results show promising improvements in both learning speed and performance across various environments. However, weaknesses include a lack of extensive comparisons with existing methods, which limits the contextual understanding of its advantages. Additionally, the clarity of some sections could be improved to enhance reader comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but there are several areas where clarity can be enhanced, particularly in the description of the algorithm and its implementation. The novelty of the approach is significant, as it introduces a new perspective on state representation in RL. However, reproducibility may be hindered by insufficient details regarding hyperparameters and algorithmic choices, which could make it challenging for others to replicate the results.\n\n# Summary Of The Review\nOverall, the paper presents an innovative method for improving RL performance through the identification of crucial states. While the contributions are noteworthy, the paper would benefit from clearer explanations and more thorough comparisons with existing approaches. Addressing these issues would strengthen its impact and ensure reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to reinforcement learning by proposing a method that leverages learned probing states to improve policy performance in continuous control tasks. The authors focus on the invariance of the value function to changes in policy architecture, suggesting that only a limited number of probing states are necessary for effective learning. The methodology is demonstrated through experiments on continuous control tasks and the MNIST dataset, with findings indicating that the proposed method can lead to significant improvements in policy efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to utilizing probing states, which could simplify reinforcement learning tasks by reducing the number of states required for effective policy improvement. However, several weaknesses are noted. The experiments are limited in scope, primarily focusing on continuous control and MNIST, which may not fully capture the robustness of the method in diverse environments. Moreover, the paper lacks a thorough discussion of the limitations and failure cases of the proposed approach, particularly in stochastic environments. The authors also do not adequately address the computational complexity of their method compared to traditional approaches.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed methodology and findings. However, it would benefit from a more in-depth exploration of the implications of the approach and its applicability to various problem domains. The novelty of the approach is apparent, particularly in the context of using probing states, but the paper could enhance its contribution by providing a more comprehensive analysis of trade-offs and challenges associated with the method. Reproducibility is partially addressed, but more detailed descriptions of the experimental setup and comparisons with a broader set of state-of-the-art methods would strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper offers a promising approach to reinforcement learning through the use of probing states, demonstrating potential benefits in policy improvement. However, the limited experimental scope, lack of discussion on challenges, and insufficient comparison with existing methods diminish the overall impact and applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning (RL) method that integrates Parameter-Based Value Functions (PBVFs) and Policy Evaluation Networks (PENs) for enhanced policy evaluation and improvement. The authors conduct extensive experiments, comparing their approach against established baselines such as Deep Deterministic Policy Gradient (DDPG), Augmented Random Search (ARS), and Soft Actor-Critic (SAC). Results indicate that the proposed method not only improves performance but also demonstrates robustness through statistical analyses, including bootstrapped confidence intervals and ablation studies.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous experimental design and the thorough statistical analysis employed to validate findings. The comparative performance metrics, along with the demonstration of zero-shot learning capabilities, provide substantive evidence of the method's effectiveness. However, one potential weakness is the reliance on a limited set of environments for testing, which may not fully encompass the generalizability of the proposed approach. Additionally, while the statistical methods are well-founded, the paper could benefit from further discussion on the implications of hyperparameter tuning and its impact on the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of the writing is high, with a logical flow that aids in understanding complex concepts. The novelty of the approach is evident, combining PBVFs and PENs in a way that addresses existing gaps in policy evaluation. The reproducibility of the findings is supported by detailed descriptions of experimental setups, statistical methodologies, and hyperparameter configurations, although access to code or datasets would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the reinforcement learning field by introducing a novel method that is both statistically rigorous and empirically validated. While the strengths of the approach are notable, the authors should consider expanding the scope of their experiments and discussing hyperparameter impacts in greater detail.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to policy evaluation and improvement in reinforcement learning using a method termed Probing State Value Function (PSVF). The methodology involves learning value functions through a limited set of probing states, enabling the agent to clone behavior in specific environments. Findings indicate that while the method demonstrates effectiveness in certain scenarios, its performance is inconsistent across varied environments and architectures.\n\n# Strength And Weaknesses\nThe main strength of this work lies in its innovative attempt to utilize probing states for policy evaluation, which allows for some degree of sample efficiency. However, the approach has significant weaknesses, including a lack of flexibility in adapting probing states based on the agent's experience in dynamic environments. Additionally, the reliance on perturbing policy parameters for exploration may not suffice in high-dimensional spaces, potentially hindering the sample efficiency and robustness of the method. The limitations regarding zero-shot learning and the absence of a thorough investigation into different sampling strategies further undermine the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is acceptable, but it lacks depth in certain areas, such as the analysis of learned probing states and their significance. The quality of the research is undermined by the absence of rigorous comparisons with other methods, particularly with larger neural networks. While the concept is novel, the reproducibility of results may be compromised due to the specifics of the architectures used and the high variance observed in performance.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting method for policy evaluation using probing states but suffers from significant limitations in flexibility, robustness, and thoroughness in empirical exploration. The proposed approach requires further refinement and validation to enhance its applicability across diverse environments and architectures.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to policy evaluation and improvement in reinforcement learning (RL) by introducing Parameter-Based Value Functions (PBVFs) and a method termed \"probing states.\" The authors claim to advance the state of the art by enabling the evaluation of multiple policies without losing track of previously evaluated ones, ostensibly facilitating zero-shot learning of policies from minimal data. The findings indicate competitive results against established methods such as DDPG and ARS in various MuJoCo environments. However, the contributions seem to lack substantial novelty and depth.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address policy evaluation, a central problem in RL, and the introduction of probing states as a way to represent policies. However, the weaknesses are pronounced: the methods proposed do not introduce significant advancements to the existing body of work, and the concepts presented, such as zero-shot learning and the visualization of probing states, are not sufficiently novel. The authors rely heavily on established frameworks without providing fresh insights or methodologies that would substantially move the field forward.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in a clear and organized manner, making it accessible for readers familiar with RL concepts. However, the novelty of the contributions is minimal, as many ideas presented are already well-explored in the literature. The reproducibility of results is not adequately addressed; while they report competitive results, the details necessary for replication are lacking, diminishing the overall impact of the findings.\n\n# Summary Of The Review\nOverall, this paper attempts to tackle important problems in reinforcement learning but ultimately falls short in delivering novel and significant contributions. The methods discussed are largely rehashes of established ideas, and while the presentation is clear, it lacks depth and originality.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach that integrates Parameter-Based Value Functions (PBVFs) with policy fingerprinting mechanisms in reinforcement learning (RL). The methodology emphasizes policy architecture independence, allowing the proposed method to adapt dynamically to various policy structures. Key findings include the identification of crucial probing states that enhance policy evaluation and improvement, alongside the ability to zero-shot learn new policy architectures. The experiments conducted, particularly with offline learning techniques on the MNIST dataset, highlight the method's potential for safer applications of RL.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of multiple RL techniques, which opens new avenues for research in adaptive architectures and exploration strategies. The claim of policy architecture independence is particularly noteworthy, suggesting broader applicability and flexibility. However, the paper acknowledges limitations in sample efficiency relative to existing methods like Soft Actor-Critic (SAC). This aspect, along with the reported high variance in performance, presents challenges that need addressing in future work. The proposed directions for further research, such as dynamic generation of probing states and exploration of alternative learning paradigms, are promising but require thorough investigation to validate their effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of writing is high, making complex concepts accessible. The novelty of combining PBVFs with policy fingerprinting is significant, although the empirical results suggest that further refinements are necessary to enhance robustness and efficiency. The reproducibility of the results could be improved by providing more detailed experimental setups and code availability to facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling integration of techniques that enhances the adaptability of reinforcement learning policies. While it shows significant promise, particularly in zero-shot learning and probing state identification, there are notable weaknesses in sample efficiency and robustness that warrant further exploration. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning method designed for continuous control tasks, demonstrating competitive performance against established baselines such as DDPG, ARS, and SAC. The methodology includes innovative features such as zero-shot learning capabilities for new policy architectures and the ability to achieve near-optimal performance with a minimal number of probing states. Experimental results indicate that the proposed method consistently matches or exceeds the performance of existing approaches in various environments, with notable final returns in tasks like Swimmer and Hopper.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its competitive performance metrics and innovative capabilities, particularly the ability to generalize across policy architectures and efficiently utilize a small number of probing states. The method shows remarkable stability in performance, as evidenced by its lower variability compared to DDPG. However, a notable weakness is its lack of sample efficiency when compared to SAC, indicating an area for potential refinement. This limitation may affect its applicability in scenarios where data efficiency is critical.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed method's contributions, methodology, and findings. The empirical results are presented in a systematic manner, facilitating an easy comparison with other methods. The implementation details are sufficient for reproducibility, though the computational demands are acknowledged. The novelty of the approach is commendable, particularly in its zero-shot learning aspect, which adds a unique dimension to the existing body of work.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of reinforcement learning for continuous control tasks, showcasing a method that excels in performance and adaptability. While the results are promising, the lack of sample efficiency presents a challenge that warrants further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to reinforcement learning, focusing on the integration of probing states and parameter-based value functions to enhance policy improvement methods. The authors employ a combination of theoretical analysis and empirical validation, demonstrating that their approach improves learning efficiency in Markov Decision Processes (MDPs). Key findings indicate that their method outperforms traditional techniques across several benchmark tasks, offering both faster convergence and superior performance metrics.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative contribution to reinforcement learning methodologies, particularly in the context of actor-critic methods. The theoretical framework is well-founded, and the empirical results are compelling, showcasing significant improvements over existing methods. However, the paper suffers from clarity issues due to dense technical jargon and complex sentence structures, which may hinder accessibility for a broader audience. Additionally, the repetitive nature of some explanations could be streamlined to enhance the narrative flow.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper provides valuable insights into the proposed methodology, the clarity is compromised by the heavy use of technical terms without adequate definitions, particularly for readers unfamiliar with the field. The quality of the writing could be improved by simplifying sentence structures and enhancing the organization of ideas. In terms of novelty, the approach is indeed original and signifies a meaningful advance in reinforcement learning. However, reproducibility may be challenging due to the lack of detailed explanations of the experimental setup and parameter choices.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of reinforcement learning, introducing a novel methodology that shows promise in improving learning efficiency. However, the clarity and organization of the paper could be improved to make it more accessible and reproducible for readers. Addressing these issues would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.591010939888883,
    -1.7336098482552402,
    -1.9178563267473272,
    -1.7760036725965442,
    -1.6388707200445112,
    -1.7333902288307612,
    -1.6018677258945513,
    -1.8732466840225805,
    -1.680671247890497,
    -1.7512550908883753,
    -1.6333149238707032,
    -1.4191532180366804,
    -1.6726985088193504,
    -1.6851397162649584,
    -1.7661440143741136,
    -1.7379639106742102,
    -1.885119982657779,
    -1.9344096988427921,
    -1.7717348966851534,
    -1.8088457163632055,
    -1.9530691598347403,
    -1.6210654158775462,
    -1.6266151587279605,
    -1.5460873481890067,
    -2.0417546624494087,
    -1.9771841037393925,
    -2.0492048765817037,
    -1.807674721854032,
    -1.5982355717269494
  ],
  "logp_cond": [
    [
      0.0,
      -2.4613847976211964,
      -2.445042633426618,
      -2.4564592964664795,
      -2.4163348257093684,
      -2.492029176034551,
      -2.5154237646360675,
      -2.4628719347399852,
      -2.451493291467764,
      -2.4611007440249626,
      -2.4412744940979616,
      -2.5155515575338745,
      -2.4522902362146386,
      -2.4392151304895293,
      -2.4517210001010836,
      -2.471127616504894,
      -2.4498796833771883,
      -2.4565774654438126,
      -2.471678627999459,
      -2.4474552736415904,
      -2.4616477432914734,
      -2.5001392009044627,
      -2.464346908696754,
      -2.4798529271823346,
      -2.473648912346113,
      -2.4537732446651885,
      -2.4356995675349506,
      -2.469701319926139,
      -2.4808019602080025
    ],
    [
      -1.49039951741958,
      0.0,
      -1.309604238944751,
      -1.2162775528604297,
      -1.3256990829086521,
      -1.4199942679061104,
      -1.4728215153498532,
      -1.292204843093983,
      -1.3191699171552953,
      -1.3976606943094012,
      -1.3409935287547026,
      -1.458744000832055,
      -1.3104789498738614,
      -1.3205474072397008,
      -1.2786955166127005,
      -1.2838426229093354,
      -1.3797964465329577,
      -1.3187151779459096,
      -1.3166189048397132,
      -1.294013969040515,
      -1.3737194286282446,
      -1.4866898146909726,
      -1.4389871673128725,
      -1.3279050195434903,
      -1.427859016446286,
      -1.373333057541107,
      -1.347479358125348,
      -1.3680469184981003,
      -1.4539474312560177
    ],
    [
      -1.5203783512244426,
      -1.451597862450163,
      0.0,
      -1.4400581299414048,
      -1.377768224563048,
      -1.4762751531552487,
      -1.6553105717391259,
      -1.4527565819323964,
      -1.444382402925128,
      -1.506392650819907,
      -1.4572420419269008,
      -1.6816176656567419,
      -1.4602017504402176,
      -1.3866432602955139,
      -1.4583342023191488,
      -1.4036772362730017,
      -1.4598744783972457,
      -1.4749501405182484,
      -1.4727568272650327,
      -1.384405570242245,
      -1.4267483245870314,
      -1.5915691208722058,
      -1.5455735921416027,
      -1.5676765209997514,
      -1.5408000116113474,
      -1.4658783333011207,
      -1.3665941896122424,
      -1.4778461691816136,
      -1.614341716075293
    ],
    [
      -1.4308174214653804,
      -1.1616187451882865,
      -1.2487522546755285,
      0.0,
      -1.2579611980990903,
      -1.3750528658731465,
      -1.4414024642621281,
      -1.2641801096692256,
      -1.2712429398220833,
      -1.3550769993923546,
      -1.2419145525325497,
      -1.5278559421961466,
      -1.1972442919508737,
      -1.277054432168477,
      -1.266276428609923,
      -1.187525140086469,
      -1.348570626387412,
      -1.2639030091841146,
      -1.2810929605745789,
      -1.2848103865567824,
      -1.3533423689025086,
      -1.4106436137397729,
      -1.3944660109218847,
      -1.3257494710619337,
      -1.3924664866906236,
      -1.28619905582183,
      -1.264001362701662,
      -1.3171034117491167,
      -1.4489260059899323
    ],
    [
      -1.3834065382196425,
      -1.3174082314971622,
      -1.2841279420104232,
      -1.318834910708306,
      0.0,
      -1.418914666396179,
      -1.4492092383895108,
      -1.339308627669713,
      -1.3349650051734823,
      -1.3699370775343864,
      -1.351882664495396,
      -1.392521446339471,
      -1.326762487839946,
      -1.3356474612850746,
      -1.3243713382425908,
      -1.3219893170550654,
      -1.3239175679480737,
      -1.2971470311935644,
      -1.3194449954874576,
      -1.3648940234454046,
      -1.3111993116082197,
      -1.4335270212192253,
      -1.3951231504324115,
      -1.3456855349329042,
      -1.3917157406273535,
      -1.3109874995617243,
      -1.2733654959257121,
      -1.3102812803330517,
      -1.4125078407924907
    ],
    [
      -1.414087699702768,
      -1.3227460980927415,
      -1.2805928035846506,
      -1.271366545900486,
      -1.3193534886021778,
      0.0,
      -1.3584563921362018,
      -1.2741294124523712,
      -1.2725334595780884,
      -1.3516071732112798,
      -1.3343106493296946,
      -1.4375087314435644,
      -1.3086459926633518,
      -1.285003560046812,
      -1.2447487665946568,
      -1.2909182339938934,
      -1.3157261233083917,
      -1.324634416740283,
      -1.3106074246558699,
      -1.2835722566872076,
      -1.3082232647700465,
      -1.3668585456172215,
      -1.3863656342294364,
      -1.3372925950447174,
      -1.3946874931401547,
      -1.3213576101455902,
      -1.3174150384654126,
      -1.3933422058070435,
      -1.3869968120786398
    ],
    [
      -1.3276314843737311,
      -1.2874212471205853,
      -1.2632438589619097,
      -1.2475890865727064,
      -1.2773167921206159,
      -1.2442603777956565,
      0.0,
      -1.2694419805397905,
      -1.2558524814450946,
      -1.3037548679001665,
      -1.2532331383528317,
      -1.3013640843581769,
      -1.2658028781870103,
      -1.2797912830384277,
      -1.2562088815936623,
      -1.2466438024460755,
      -1.2799594253866917,
      -1.283765098041341,
      -1.2985460398406932,
      -1.2786029881545136,
      -1.2997573280161416,
      -1.2789207487124565,
      -1.2795384113754544,
      -1.3009526959856463,
      -1.2940544403220056,
      -1.29373496246671,
      -1.2694600696039797,
      -1.293169867731058,
      -1.3161602011517153
    ],
    [
      -1.5300467046770732,
      -1.43515078110773,
      -1.3960280406273118,
      -1.4065799302190956,
      -1.4598161011113278,
      -1.5174399723580756,
      -1.5523604824016186,
      0.0,
      -1.4081765722211204,
      -1.4630493937485503,
      -1.4238263777720026,
      -1.6090311950158187,
      -1.4026887451346381,
      -1.4417928702630596,
      -1.5215708856299826,
      -1.3976069889329692,
      -1.4533640334144293,
      -1.4355990034308157,
      -1.435100808549658,
      -1.4293081411122714,
      -1.4656367125250067,
      -1.5590383074020433,
      -1.5195946626826113,
      -1.507079347085428,
      -1.4880874687402905,
      -1.4372376752308382,
      -1.4540857534738492,
      -1.4577098937359516,
      -1.5308990840833594
    ],
    [
      -1.3220237807382182,
      -1.1756488094270154,
      -1.1538507634190605,
      -1.1877853742468527,
      -1.218882452716722,
      -1.2602472743619961,
      -1.3162084829558236,
      -1.2210679201737256,
      0.0,
      -1.2756044508671387,
      -1.1602362305315501,
      -1.3918485525193305,
      -1.228696924143549,
      -1.1292137655836942,
      -1.1758782336079021,
      -1.2053210332912707,
      -1.2286844605625955,
      -1.2353330586532039,
      -1.2332547641581604,
      -1.2093754445707776,
      -1.2148729706543115,
      -1.3217448085060588,
      -1.2890945578105912,
      -1.3111477315575315,
      -1.2919526909824819,
      -1.2024514450995436,
      -1.1692425983816077,
      -1.2880736142798477,
      -1.3061283127846834
    ],
    [
      -1.4471635576351867,
      -1.365425875686037,
      -1.3936385313245403,
      -1.3651031959960558,
      -1.3445792429303633,
      -1.4727697326852252,
      -1.5082921729657006,
      -1.3423487942685997,
      -1.4200731187366304,
      0.0,
      -1.409278784065692,
      -1.5084418325369817,
      -1.3395346053082726,
      -1.4383393122702495,
      -1.4390289117337487,
      -1.3811314265042167,
      -1.416202938093487,
      -1.4051607220949855,
      -1.4253461315242486,
      -1.4116699670753843,
      -1.414915062081664,
      -1.4888754519802208,
      -1.3885625224753184,
      -1.4772488050062536,
      -1.3754958555704846,
      -1.3959960113755745,
      -1.373605841407137,
      -1.367979084274562,
      -1.509723619769613
    ],
    [
      -1.2931299386743456,
      -1.2178957006578117,
      -1.175345002593397,
      -1.1725249003572071,
      -1.221615871642125,
      -1.2430790022898197,
      -1.2960288820732693,
      -1.180754711178928,
      -1.1569547475866155,
      -1.2734775593324799,
      0.0,
      -1.3902048479001508,
      -1.1208978912871435,
      -1.1725654264006384,
      -1.213259920143782,
      -1.1678743418147746,
      -1.2822670747447105,
      -1.2139711963040203,
      -1.2212120835731757,
      -1.1925405017195678,
      -1.2329017670957665,
      -1.284895494451559,
      -1.3057592617416536,
      -1.304255886747612,
      -1.2816953354664786,
      -1.228370589215048,
      -1.1678330049230528,
      -1.2854074740611565,
      -1.3515329670160923
    ],
    [
      -1.1832264388867855,
      -1.1016730991665329,
      -1.1527044165425993,
      -1.1462658936721382,
      -1.0800793575302312,
      -1.1398946802610588,
      -1.1212947880137585,
      -1.150693120676553,
      -1.1490958004061307,
      -1.1207510210281402,
      -1.1228713552685132,
      0.0,
      -1.1603249013849646,
      -1.157385837665003,
      -1.1426588931020953,
      -1.1279510508602557,
      -1.1318142109430998,
      -1.1434921540084868,
      -1.126030665297855,
      -1.1482293277856455,
      -1.1681627647560566,
      -1.110421105971591,
      -1.1365310249386087,
      -1.0859895413504745,
      -1.1383846939031705,
      -1.1169956504455691,
      -1.1239202089632765,
      -1.1614432544489939,
      -1.121781243891065
    ],
    [
      -1.3496122948604257,
      -1.2549064931393514,
      -1.2467616058835604,
      -1.1643524808001577,
      -1.2662581051971138,
      -1.2998002605714816,
      -1.398590196754958,
      -1.240116778606541,
      -1.2401399340611787,
      -1.2797295714820618,
      -1.2112725454172857,
      -1.4607845464934663,
      0.0,
      -1.246720798546009,
      -1.3179948133875465,
      -1.1797981159571826,
      -1.3045770363987483,
      -1.2662244150919575,
      -1.2466206920385108,
      -1.2522179870574683,
      -1.2891852391407055,
      -1.3661926113488854,
      -1.3219458186609838,
      -1.340248576585233,
      -1.342110679409599,
      -1.254437966706586,
      -1.2168719080767585,
      -1.3321572686242718,
      -1.3762288224516357
    ],
    [
      -1.3540484933389776,
      -1.1732973495281118,
      -1.1382782691248383,
      -1.2150724120016356,
      -1.2267204320771778,
      -1.3619182389963187,
      -1.413820791706579,
      -1.251435642152787,
      -1.1899430375737197,
      -1.3251678195166856,
      -1.2613283034227036,
      -1.4218978398042699,
      -1.215940130287414,
      0.0,
      -1.1855692806166618,
      -1.2418053927699908,
      -1.2121912139515176,
      -1.2796832104338727,
      -1.2465056088849418,
      -1.1723533158964603,
      -1.1976897485699127,
      -1.3934981520676686,
      -1.3418748327897452,
      -1.3063236336088275,
      -1.3491612688104833,
      -1.2533344891183809,
      -1.2274313994854207,
      -1.318519854191975,
      -1.3633984441620866
    ],
    [
      -1.4702925561137852,
      -1.2937120775990472,
      -1.2860547228838706,
      -1.3364958253259707,
      -1.3104773980630218,
      -1.4179186267721442,
      -1.447877683189203,
      -1.396119192424022,
      -1.339910849853264,
      -1.4561097026551177,
      -1.375627202824615,
      -1.4938964778740376,
      -1.400685781993455,
      -1.3051386040020625,
      0.0,
      -1.3540062317319361,
      -1.35407314079906,
      -1.366562796931627,
      -1.41050466299541,
      -1.3096014764741903,
      -1.3041493444189787,
      -1.482815470387192,
      -1.459501046354231,
      -1.2921675147344858,
      -1.4529476409063922,
      -1.3500769426627004,
      -1.2420815881312208,
      -1.369664659133841,
      -1.4186542854947635
    ],
    [
      -1.4312584241142565,
      -1.2972413945829058,
      -1.276044661973249,
      -1.2164830426262865,
      -1.3482611446865675,
      -1.3871496544076516,
      -1.4186014483635963,
      -1.272566881766236,
      -1.307689898259599,
      -1.3647454109581134,
      -1.3215857594316844,
      -1.4847096356073235,
      -1.2321441272600175,
      -1.306712433768293,
      -1.3380177710993382,
      0.0,
      -1.3623243512327665,
      -1.3423570242398013,
      -1.3041145111371237,
      -1.2665346117945666,
      -1.332194229822128,
      -1.405745837222519,
      -1.3952551290615622,
      -1.394165634852124,
      -1.407983226009114,
      -1.3295017086884275,
      -1.3058954210065663,
      -1.371624777519837,
      -1.4205734497759581
    ],
    [
      -1.4847977370823737,
      -1.4563428272553918,
      -1.343003195796981,
      -1.4060026211564443,
      -1.381148593205327,
      -1.485224153574925,
      -1.5591660183131002,
      -1.40820728920974,
      -1.4180435323583986,
      -1.4842991655791489,
      -1.4767401594666734,
      -1.6091757719686737,
      -1.4376172508259935,
      -1.3811000466097882,
      -1.4324382137882994,
      -1.4109426858004956,
      0.0,
      -1.3975699418365404,
      -1.4091621954711098,
      -1.36459913787593,
      -1.4373711652723598,
      -1.591433732710141,
      -1.4568063064771526,
      -1.4739515039992959,
      -1.4825025715937596,
      -1.4539826733840704,
      -1.356350535323831,
      -1.4761871414937318,
      -1.5266318747494143
    ],
    [
      -1.5257231215360545,
      -1.3438574464296942,
      -1.4085260568069466,
      -1.3001945964722659,
      -1.323064359538663,
      -1.4803878819702976,
      -1.5637226457721272,
      -1.3850620683848847,
      -1.4621993063532932,
      -1.5103595145187232,
      -1.457404390557702,
      -1.6559153071769266,
      -1.4504411560392643,
      -1.4589858383920116,
      -1.412085311797152,
      -1.3665658295870355,
      -1.400618690636676,
      0.0,
      -1.4625305575112588,
      -1.36038558016189,
      -1.4870138751372122,
      -1.5609832253531506,
      -1.5266474705992132,
      -1.4361145811063163,
      -1.5313379863156777,
      -1.4053782262304044,
      -1.4159389851940207,
      -1.4759990853163467,
      -1.5674859814543003
    ],
    [
      -1.4208873114776266,
      -1.3251457512261469,
      -1.239169004959632,
      -1.2961467032613534,
      -1.3178878807511685,
      -1.3615967521515462,
      -1.4177595789415272,
      -1.286436009285762,
      -1.3043606434929453,
      -1.3622632406789708,
      -1.3515091389705014,
      -1.476750829807341,
      -1.2538958424761486,
      -1.3068216465216957,
      -1.332757781518313,
      -1.2680356278443115,
      -1.297763836387281,
      -1.312712106664617,
      0.0,
      -1.283796909171708,
      -1.3565751517140465,
      -1.4279639241606639,
      -1.4013131084552264,
      -1.3856205963915509,
      -1.3959778735377606,
      -1.2902051998195574,
      -1.2928952073623576,
      -1.3721963188257316,
      -1.412121389685575
    ],
    [
      -1.505368799552574,
      -1.4046507500082321,
      -1.374436732661016,
      -1.4260529844038925,
      -1.4297755606489904,
      -1.4735289987873799,
      -1.552367462705991,
      -1.4385349603744546,
      -1.4635422037733254,
      -1.4830240897370006,
      -1.4364006343769107,
      -1.5672321577606263,
      -1.4341998278483818,
      -1.4365572232360135,
      -1.3831375045801297,
      -1.3953522522375885,
      -1.414778026779926,
      -1.407619412842034,
      -1.435817195332732,
      0.0,
      -1.4557414111795903,
      -1.5185644838118708,
      -1.4911138411989453,
      -1.4717374692179528,
      -1.4668685415647398,
      -1.4766205833224357,
      -1.4234582344397846,
      -1.4625878200367841,
      -1.5034594419560894
    ],
    [
      -1.578285272677405,
      -1.578290409838632,
      -1.4265889422639666,
      -1.4936108307640614,
      -1.491233111709348,
      -1.5716308835529471,
      -1.6552971811821646,
      -1.51991477202208,
      -1.5189584948166475,
      -1.568128882323631,
      -1.4771394456934694,
      -1.6993580334170857,
      -1.5213873680959242,
      -1.4526228478091268,
      -1.4718963438389616,
      -1.4781865942714185,
      -1.4755199489765154,
      -1.5489769740192783,
      -1.5027193864707606,
      -1.498087264267257,
      0.0,
      -1.6310575432104173,
      -1.5934385486682285,
      -1.556779565947412,
      -1.5906000149880055,
      -1.4785338885223664,
      -1.441841366837337,
      -1.5277868926950116,
      -1.6108139443100444
    ],
    [
      -1.2616294492664282,
      -1.2432162019222743,
      -1.1733053251511765,
      -1.2141589360495997,
      -1.2700264216628623,
      -1.2299718163609021,
      -1.2360503958173128,
      -1.2295755654745182,
      -1.2257554078567277,
      -1.2395750510707904,
      -1.2317515553523168,
      -1.2876624586695957,
      -1.2336838741494958,
      -1.2119804015529494,
      -1.2278543566978342,
      -1.2085000219811493,
      -1.2367305291166693,
      -1.2217666765248676,
      -1.226833806946966,
      -1.2222587016904571,
      -1.241857050471242,
      0.0,
      -1.2449122619690751,
      -1.2872387778807541,
      -1.2256096617902887,
      -1.2186437987081358,
      -1.2289882891912978,
      -1.234313166422158,
      -1.2322300624654765
    ],
    [
      -1.2085841348640753,
      -1.294686074299457,
      -1.2081436232374305,
      -1.2263029670382877,
      -1.2563738326328167,
      -1.2953953810716101,
      -1.2970164485631284,
      -1.2431732345612363,
      -1.2341848955642443,
      -1.1562114330531454,
      -1.259094256907694,
      -1.3511288709989826,
      -1.1953249807566388,
      -1.1964202401631197,
      -1.3002974709064443,
      -1.2173977765800856,
      -1.209546108171309,
      -1.2466129801107342,
      -1.2863109365320857,
      -1.238921301627103,
      -1.2239457915743883,
      -1.343375900316374,
      0.0,
      -1.3188639839806306,
      -1.2117803324641454,
      -1.2520206146682133,
      -1.2083889063475153,
      -1.2366595541826202,
      -1.287899338025677
    ],
    [
      -1.2747192691671534,
      -1.115739164122578,
      -1.1448573656917436,
      -1.0770330052538786,
      -1.102779469443078,
      -1.171613469977605,
      -1.2402043764702215,
      -1.1625032066148981,
      -1.1677473190791758,
      -1.1863577939361911,
      -1.165915406490685,
      -1.2454710703723846,
      -1.1709858620860356,
      -1.200896154349303,
      -1.080077506841336,
      -1.109354620514519,
      -1.1563366408300049,
      -1.088697800919955,
      -1.1178672543491928,
      -1.1462941601897187,
      -1.1472297437104917,
      -1.236413631445745,
      -1.2224995761217985,
      0.0,
      -1.1802999256347722,
      -1.1215645308721922,
      -1.09946389982911,
      -1.1577785531419622,
      -1.2363195988754878
    ],
    [
      -1.6055591261364415,
      -1.6024953414159042,
      -1.5494306218122762,
      -1.5080179159542433,
      -1.6166411945999988,
      -1.6332210495705548,
      -1.7033207198912808,
      -1.5739936027278276,
      -1.5898824393017243,
      -1.541984631324512,
      -1.62761121114805,
      -1.7576746234002085,
      -1.5643655875707103,
      -1.5841374667795518,
      -1.5864701321569157,
      -1.5439142630645406,
      -1.5524704141654055,
      -1.5845485128312278,
      -1.581651312620541,
      -1.5311768468235591,
      -1.6180845006479725,
      -1.6200102796715208,
      -1.5807182507312842,
      -1.687458065038432,
      0.0,
      -1.5440908359764536,
      -1.5252168283757193,
      -1.5953462379354844,
      -1.6433671254794648
    ],
    [
      -1.6289568885219785,
      -1.5541750887290318,
      -1.4869385614956538,
      -1.443745494598993,
      -1.5056576196166858,
      -1.5662774634847276,
      -1.6441650654400417,
      -1.5110721945798484,
      -1.5127418404455808,
      -1.6097869373629874,
      -1.4975646044918796,
      -1.747660854844802,
      -1.511610604898332,
      -1.5003806682077214,
      -1.4861417264460222,
      -1.4884579051218572,
      -1.5603288312450603,
      -1.46428662841666,
      -1.502804996971813,
      -1.4891556746557417,
      -1.4912837121292046,
      -1.6480090592320942,
      -1.6156843948256723,
      -1.5859244911378418,
      -1.5688892873747489,
      0.0,
      -1.4967727818624634,
      -1.5292884190294793,
      -1.5930488006754797
    ],
    [
      -1.6182801329388998,
      -1.6106670716488563,
      -1.504563416101954,
      -1.5714912540916237,
      -1.4992804092003826,
      -1.6618938719933112,
      -1.689892009911353,
      -1.6411287766722924,
      -1.5882385451994268,
      -1.6382188164486298,
      -1.623660278848809,
      -1.775450100870366,
      -1.5758085140570088,
      -1.559946758624649,
      -1.5833201583779786,
      -1.5503605224602723,
      -1.5691146721820373,
      -1.6099471856528043,
      -1.5993589643919703,
      -1.5912575317616824,
      -1.5627999926161964,
      -1.7288785782018354,
      -1.636561904743974,
      -1.620528114581034,
      -1.6337835015624793,
      -1.6193678409617038,
      0.0,
      -1.5953581376528723,
      -1.7153165503427832
    ],
    [
      -1.476087459025136,
      -1.4895288297047389,
      -1.363309556187059,
      -1.424144297200609,
      -1.2946489328664865,
      -1.477488736996244,
      -1.477252435633676,
      -1.4218032017226687,
      -1.4378580777295846,
      -1.415143352739909,
      -1.4979285581478265,
      -1.5264041561619843,
      -1.4400834847600776,
      -1.478376807554404,
      -1.4462917402561983,
      -1.4109790625251337,
      -1.4225029583651476,
      -1.3868798567249334,
      -1.4410161257726897,
      -1.41679937301734,
      -1.4196596017913705,
      -1.4882102648666045,
      -1.437261942194026,
      -1.4606211158983173,
      -1.410234135385912,
      -1.388166248551346,
      -1.3576694991269909,
      0.0,
      -1.492980965953566
    ],
    [
      -1.2355714012549355,
      -1.2752262478028498,
      -1.1901908306718585,
      -1.218191247978328,
      -1.2348052335631203,
      -1.217982959044761,
      -1.2598374116655353,
      -1.2138000776915097,
      -1.2101622571487343,
      -1.2383023694890654,
      -1.2288238030246537,
      -1.2240636479519915,
      -1.2242197736471252,
      -1.2362727709338892,
      -1.2178059776601546,
      -1.1745644808852587,
      -1.177009615010399,
      -1.1939573341325693,
      -1.219950301967808,
      -1.205507425452773,
      -1.243763679001302,
      -1.1988314516449259,
      -1.2170322663563695,
      -1.2479032451686038,
      -1.230472679904214,
      -1.1918604558346217,
      -1.1991963109536548,
      -1.2197877337039233,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.12962614226768654,
      0.14596830646226477,
      0.13455164342240344,
      0.17467611417951456,
      0.098981763854332,
      0.07558717525281544,
      0.12813900514889776,
      0.13951764842111913,
      0.12991019586392039,
      0.14973644579092138,
      0.07545938235500849,
      0.13872070367424438,
      0.15179580939935367,
      0.13928993978779936,
      0.1198833233839891,
      0.14113125651169467,
      0.13443347444507037,
      0.11933231188942406,
      0.14355566624729255,
      0.12936319659740958,
      0.09087173898442025,
      0.12666403119212877,
      0.11115801270654835,
      0.11736202754276981,
      0.13723769522369444,
      0.15531137235393233,
      0.12130961996274392,
      0.1102089796808805
    ],
    [
      0.24321033083566013,
      0.0,
      0.42400560931048914,
      0.5173322953948105,
      0.4079107653465881,
      0.31361558034912984,
      0.26078833290538705,
      0.44140500516125725,
      0.4144399310999449,
      0.335949153945839,
      0.39261631950053766,
      0.2748658474231853,
      0.42313089838137885,
      0.41306244101553946,
      0.4549143316425397,
      0.44976722534590485,
      0.3538134017222825,
      0.4148946703093306,
      0.416990943415527,
      0.4395958792147252,
      0.35989041962699564,
      0.24692003356426762,
      0.29462268094236777,
      0.40570482871174995,
      0.3057508318089541,
      0.3602767907141333,
      0.3861304901298923,
      0.3655629297571399,
      0.27966241699922256
    ],
    [
      0.39747797552288455,
      0.4662584642971641,
      0.0,
      0.4777981968059224,
      0.5400881021842792,
      0.4415811735920785,
      0.2625457550082013,
      0.46509974481493077,
      0.4734739238221992,
      0.41146367592742017,
      0.46061428482042643,
      0.23623866109058533,
      0.4576545763071096,
      0.5312130664518133,
      0.4595221244281784,
      0.5141790904743255,
      0.4579818483500815,
      0.44290618622907885,
      0.44509949948229455,
      0.5334507565050821,
      0.4911080021602958,
      0.3262872058751214,
      0.3722827346057245,
      0.3501798057475758,
      0.3770563151359798,
      0.4519779934462065,
      0.5512621371350848,
      0.4400101575657136,
      0.3035146106720341
    ],
    [
      0.34518625113116386,
      0.6143849274082578,
      0.5272514179210157,
      0.0,
      0.5180424744974539,
      0.4009508067233978,
      0.3346012083344161,
      0.5118235629273187,
      0.504760732774461,
      0.42092667320418964,
      0.5340891200639946,
      0.24814773040039761,
      0.5787593806456706,
      0.4989492404280673,
      0.5097272439866212,
      0.5884785325100752,
      0.42743304620913225,
      0.5121006634124297,
      0.49491071202196535,
      0.49119328603976187,
      0.42266130369403565,
      0.36536005885677136,
      0.3815376616746595,
      0.4502542015346105,
      0.3835371859059207,
      0.4898046167747143,
      0.5120023098948823,
      0.4589002608474275,
      0.32707766660661197
    ],
    [
      0.25546418182486863,
      0.32146248854734893,
      0.35474277803408794,
      0.32003580933620523,
      0.0,
      0.21995605364833226,
      0.1896614816550004,
      0.29956209237479814,
      0.3039057148710289,
      0.26893364251012475,
      0.2869880555491151,
      0.24634927370504012,
      0.3121082322045652,
      0.30322325875943656,
      0.3144993818019204,
      0.3168814029894458,
      0.3149531520964375,
      0.3417236888509467,
      0.31942572455705354,
      0.2739766965991066,
      0.3276714084362915,
      0.20534369882528591,
      0.24374756961209965,
      0.29318518511160696,
      0.2471549794171577,
      0.32788322048278684,
      0.36550522411879904,
      0.3285894397114595,
      0.22636287925202048
    ],
    [
      0.3193025291279932,
      0.41064413073801975,
      0.45279742524611066,
      0.4620236829302753,
      0.4140367402285834,
      0.0,
      0.3749338366945594,
      0.45926081637839,
      0.4608567692526728,
      0.38178305561948145,
      0.39907957950106665,
      0.2958814973871968,
      0.4247442361674094,
      0.4483866687839493,
      0.4886414622361044,
      0.44247199483686783,
      0.41766410552236954,
      0.4087558120904782,
      0.42278280417489134,
      0.44981797214355357,
      0.42516696406071475,
      0.3665316832135397,
      0.34702459460132484,
      0.3960976337860438,
      0.3387027356906065,
      0.41203261868517105,
      0.4159751903653486,
      0.3400480230237177,
      0.34639341675212143
    ],
    [
      0.27423624152082016,
      0.31444647877396603,
      0.33862386693264157,
      0.3542786393218449,
      0.3245509337739354,
      0.35760734809889483,
      0.0,
      0.33242574535476077,
      0.34601524444945664,
      0.2981128579943848,
      0.3486345875417196,
      0.30050364153637443,
      0.33606484770754097,
      0.3220764428561236,
      0.345658844300889,
      0.35522392344847575,
      0.32190830050785957,
      0.3181026278532102,
      0.3033216860538581,
      0.3232647377400377,
      0.30211039787840965,
      0.32294697718209475,
      0.32232931451909685,
      0.30091502990890495,
      0.3078132855725457,
      0.30813276342784124,
      0.33240765629057156,
      0.3086978581634934,
      0.285707524742836
    ],
    [
      0.3431999793455074,
      0.43809590291485057,
      0.47721864339526876,
      0.466666753803485,
      0.4134305829112528,
      0.35580671166450495,
      0.32088620162096193,
      0.0,
      0.4650701118014602,
      0.4101972902740303,
      0.44942030625057794,
      0.26421548900676184,
      0.4705579388879424,
      0.43145381375952097,
      0.351675798392598,
      0.47563969508961135,
      0.4198826506081512,
      0.43764768059176484,
      0.4381458754729226,
      0.4439385429103091,
      0.40760997149757383,
      0.31420837662053724,
      0.3536520213399692,
      0.36616733693715253,
      0.38515921528229,
      0.4360090087917423,
      0.4191609305487314,
      0.41553679028662893,
      0.34234759993922115
    ],
    [
      0.3586474671522788,
      0.5050224384634816,
      0.5268204844714366,
      0.49288587364364433,
      0.46178879517377514,
      0.4204239735285009,
      0.36446276493467344,
      0.4596033277167715,
      0.0,
      0.40506679702335835,
      0.5204350173589469,
      0.28882269537116656,
      0.451974323746948,
      0.5514574823068028,
      0.5047930142825949,
      0.47535021459922633,
      0.45198678732790154,
      0.4453381892372932,
      0.44741648373233667,
      0.47129580331971943,
      0.4657982772361855,
      0.35892643938443825,
      0.3915766900799058,
      0.36952351633296554,
      0.38871855690801516,
      0.47821980279095344,
      0.5114286495088893,
      0.39259763361064937,
      0.3745429351058136
    ],
    [
      0.3040915332531886,
      0.38582921520233837,
      0.357616559563835,
      0.3861518948923195,
      0.4066758479580119,
      0.2784853582031501,
      0.24296291792267466,
      0.4089062966197756,
      0.33118197215174483,
      0.0,
      0.3419763068226833,
      0.24281325835139356,
      0.41172048558010266,
      0.3129157786181258,
      0.3122261791546266,
      0.37012366438415856,
      0.3350521527948882,
      0.34609436879338973,
      0.32590895936412667,
      0.339585123812991,
      0.3363400288067113,
      0.2623796389081545,
      0.36269256841305686,
      0.2740062858821217,
      0.3757592353178907,
      0.3552590795128008,
      0.37764924948123824,
      0.3832760066138132,
      0.2415314711187624
    ],
    [
      0.34018498519635765,
      0.4154192232128915,
      0.4579699212773063,
      0.46079002351349607,
      0.41169905222857817,
      0.3902359215808835,
      0.33728604179743393,
      0.45256021269177515,
      0.47636017628408767,
      0.35983736453822335,
      0.0,
      0.24311007597055245,
      0.5124170325835598,
      0.4607494974700648,
      0.4200550037269213,
      0.46544058205592864,
      0.35104784912599274,
      0.4193437275666829,
      0.41210284029752753,
      0.4407744221511354,
      0.4004131567749367,
      0.3484194294191443,
      0.32755566212904963,
      0.32905903712309126,
      0.3516195884042246,
      0.40494433465565516,
      0.46548191894765045,
      0.3479074498095467,
      0.28178195685461094
    ],
    [
      0.2359267791498949,
      0.3174801188701475,
      0.2664488014940811,
      0.2728873243645422,
      0.33907386050644917,
      0.2792585377756216,
      0.2978584300229219,
      0.26846009736012744,
      0.2700574176305497,
      0.2984021970085402,
      0.29628186276816715,
      0.0,
      0.2588283166517158,
      0.26176738037167735,
      0.27649432493458503,
      0.29120216717642466,
      0.2873390070935806,
      0.27566106402819357,
      0.2931225527388255,
      0.2709238902510349,
      0.25099045328062375,
      0.30873211206508944,
      0.2826221930980717,
      0.33316367668620583,
      0.28076852413350983,
      0.30215756759111123,
      0.2952330090734039,
      0.2577099635876865,
      0.29737197414561534
    ],
    [
      0.3230862139589248,
      0.417792015679999,
      0.42593690293579,
      0.5083460280191927,
      0.40644040362223666,
      0.3728982482478689,
      0.27410831206439235,
      0.43258173021280943,
      0.4325585747581717,
      0.39296893733728866,
      0.46142596340206476,
      0.2119139623258841,
      0.0,
      0.4259777102733415,
      0.3547036954318039,
      0.4929003928621678,
      0.3681214724206021,
      0.40647409372739296,
      0.42607781678083967,
      0.4204805217618821,
      0.38351326967864496,
      0.3065058974704651,
      0.35075269015836663,
      0.33244993223411745,
      0.3305878294097515,
      0.41826054211276453,
      0.45582660074259196,
      0.3405412401950787,
      0.29646968636771476
    ],
    [
      0.3310912229259808,
      0.5118423667368466,
      0.5468614471401201,
      0.47006730426332277,
      0.45841928418778055,
      0.3232214772686397,
      0.27131892455837936,
      0.4337040741121714,
      0.49519667869123873,
      0.3599718967482728,
      0.42381141284225476,
      0.26324187646068853,
      0.4691995859775444,
      0.0,
      0.4995704356482966,
      0.4433343234949676,
      0.47294850231344077,
      0.4054565058310857,
      0.4386341073800166,
      0.5127864003684981,
      0.48744996769504567,
      0.2916415641972898,
      0.3432648834752132,
      0.37881608265613087,
      0.33597844745447514,
      0.43180522714657754,
      0.4577083167795377,
      0.36661986207298347,
      0.32174127210287184
    ],
    [
      0.29585145826032844,
      0.47243193677506645,
      0.48008929149024304,
      0.42964818904814295,
      0.45566661631109184,
      0.3482253876019694,
      0.31826633118491054,
      0.3700248219500917,
      0.4262331645208497,
      0.31003431171899587,
      0.3905168115494986,
      0.27224753650007605,
      0.36545823238065855,
      0.4610054103720511,
      0.0,
      0.4121377826421775,
      0.41207087357505356,
      0.39958121744248665,
      0.3556393513787035,
      0.4565425378999233,
      0.46199466995513494,
      0.28332854398692153,
      0.30664296801988256,
      0.47397649963962785,
      0.31319637346772145,
      0.4160670717114132,
      0.5240624262428928,
      0.39647935524027256,
      0.34748972887935015
    ],
    [
      0.30670548655995367,
      0.4407225160913044,
      0.4619192487009611,
      0.5214808680479237,
      0.38970276598764264,
      0.3508142562665586,
      0.31936246231061394,
      0.4653970289079743,
      0.43027401241461116,
      0.3732184997160968,
      0.41637815124252575,
      0.2532542750668867,
      0.5058197834141926,
      0.43125147690591725,
      0.399946139574872,
      0.0,
      0.37563955944144367,
      0.3956068864344089,
      0.43384939953708646,
      0.4714292988796436,
      0.4057696808520821,
      0.3322180734516913,
      0.34270878161264795,
      0.3437982758220861,
      0.3299806846650961,
      0.40846220198578265,
      0.4320684896676439,
      0.3663391331543733,
      0.31739046089825207
    ],
    [
      0.4003222455754052,
      0.42877715540238714,
      0.5421167868607979,
      0.47911736150133466,
      0.503971389452452,
      0.3998958290828538,
      0.3259539643446787,
      0.476912693448039,
      0.4670764502993803,
      0.40082081707863004,
      0.40837982319110555,
      0.27594421068910524,
      0.44750273183178546,
      0.5040199360479907,
      0.45268176886947953,
      0.47417729685728327,
      0.0,
      0.48755004082123854,
      0.47595778718666915,
      0.520520844781849,
      0.44774881738541916,
      0.2936862499476378,
      0.4283136761806263,
      0.41116847865848305,
      0.40261741106401927,
      0.4311373092737085,
      0.5287694473339479,
      0.40893284116404716,
      0.3584881079083646
    ],
    [
      0.40868657730673763,
      0.5905522524130979,
      0.5258836420358455,
      0.6342151023705263,
      0.611345339304129,
      0.45402181687249454,
      0.37068705307066496,
      0.5493476304579075,
      0.4722103924894989,
      0.42405018432406894,
      0.47700530828509,
      0.27849439166586554,
      0.48396854280352786,
      0.47542386045078056,
      0.5223243870456402,
      0.5678438692557566,
      0.5337910082061161,
      0.0,
      0.4718791413315333,
      0.5740241186809021,
      0.44739582370557995,
      0.37342647348964153,
      0.4077622282435789,
      0.49829511773647583,
      0.4030717125271144,
      0.5290314726123877,
      0.5184707136487714,
      0.4584106135264454,
      0.36692371738849183
    ],
    [
      0.3508475852075268,
      0.4465891454590065,
      0.5325658917255214,
      0.4755881934238,
      0.4538470159339849,
      0.4101381445336072,
      0.3539753177436262,
      0.4852988873993913,
      0.46737425319220804,
      0.4094716560061826,
      0.420225757714652,
      0.2949840668778123,
      0.5178390542090048,
      0.46491325016345764,
      0.43897711516684046,
      0.5036992688408419,
      0.4739710602978724,
      0.45902279002053636,
      0.0,
      0.4879379875134453,
      0.41515974497110686,
      0.3437709725244895,
      0.370421788229927,
      0.3861143002936025,
      0.3757570231473928,
      0.481529696865596,
      0.47883968932279575,
      0.3995385778594218,
      0.3596135069995783
    ],
    [
      0.3034769168106315,
      0.4041949663549733,
      0.4344089837021894,
      0.38279273195931296,
      0.3790701557142151,
      0.3353167175758256,
      0.25647825365721455,
      0.37031075598875085,
      0.34530351258988,
      0.32582162662620484,
      0.3724450819862948,
      0.24161355860257916,
      0.3746458885148236,
      0.3722884931271919,
      0.4257082117830757,
      0.41349346412561694,
      0.39406768958327953,
      0.4012263035211714,
      0.37302852103047335,
      0.0,
      0.35310430518361513,
      0.29028123255133464,
      0.31773187516426016,
      0.33710824714525267,
      0.3419771747984657,
      0.3322251330407697,
      0.38538748192342087,
      0.34625789632642134,
      0.3053862744071161
    ],
    [
      0.37478388715733524,
      0.3747787499961084,
      0.5264802175707737,
      0.45945832907067885,
      0.46183604812539225,
      0.38143827628179316,
      0.29777197865257565,
      0.43315438781266025,
      0.4341106650180928,
      0.38494027751110926,
      0.47592971414127083,
      0.2537111264176546,
      0.43168179173881605,
      0.5004463120256135,
      0.48117281599577866,
      0.47488256556332176,
      0.4775492108582249,
      0.404092185815462,
      0.45034977336397963,
      0.45498189556748336,
      0.0,
      0.322011616624323,
      0.35963061116651174,
      0.3962895938873283,
      0.36246914484673476,
      0.47453527131237383,
      0.5112277929974032,
      0.4252822671397287,
      0.34225521552469584
    ],
    [
      0.359435966611118,
      0.3778492139552718,
      0.4477600907263697,
      0.4069064798279465,
      0.3510389942146839,
      0.391093599516644,
      0.38501502006023336,
      0.391489850403028,
      0.3953100080208185,
      0.3814903648067558,
      0.3893138605252293,
      0.33340295720795043,
      0.3873815417280504,
      0.40908501432459676,
      0.393211059179712,
      0.4125653938963969,
      0.3843348867608769,
      0.39929873935267857,
      0.3942316089305802,
      0.398806714187089,
      0.3792083654063041,
      0.0,
      0.37615315390847104,
      0.333826637996792,
      0.3954557540872574,
      0.4024216171694104,
      0.3920771266862484,
      0.3867522494553881,
      0.38883535341206965
    ],
    [
      0.41803102386388513,
      0.33192908442850344,
      0.41847153549052996,
      0.4003121916896728,
      0.3702413260951438,
      0.33121977765635036,
      0.3295987101648321,
      0.3834419241667242,
      0.3924302631637162,
      0.47040372567481503,
      0.36752090182026653,
      0.2754862877289779,
      0.43129017797132163,
      0.4301949185648408,
      0.32631768782151616,
      0.40921738214787484,
      0.41706905055665144,
      0.3800021786172263,
      0.34030422219587475,
      0.38769385710085746,
      0.40266936715357216,
      0.28323925841158637,
      0.0,
      0.3077511747473298,
      0.41483482626381507,
      0.3745945440597471,
      0.4182262523804452,
      0.3899556045453403,
      0.33871582070228357
    ],
    [
      0.27136807902185334,
      0.4303481840664287,
      0.40122998249726316,
      0.46905434293512815,
      0.4433078787459288,
      0.37447387821140166,
      0.3058829717187852,
      0.3835841415741086,
      0.3783400291098309,
      0.3597295542528156,
      0.3801719416983218,
      0.30061627781662215,
      0.3751014861029711,
      0.34519119383970365,
      0.46600984134767076,
      0.4367327276744877,
      0.38975070735900186,
      0.4573895472690517,
      0.42822009383981396,
      0.399793187999288,
      0.39885760447851504,
      0.3096737167432617,
      0.32358777206720823,
      0.0,
      0.3657874225542346,
      0.4245228173168145,
      0.44662344835989676,
      0.3883087950470445,
      0.3097677493135189
    ],
    [
      0.43619553631296726,
      0.4392593210335045,
      0.49232404063713253,
      0.5337367464951654,
      0.42511346784940995,
      0.4085336128788539,
      0.338433942558128,
      0.4677610597215811,
      0.4518722231476844,
      0.4997700311248967,
      0.41414345130135866,
      0.2840800390492002,
      0.4773890748786984,
      0.4576171956698569,
      0.45528453029249305,
      0.4978403993848681,
      0.4892842482840032,
      0.45720614961818096,
      0.4601033498288678,
      0.5105778156258496,
      0.42367016180143624,
      0.42174438277788795,
      0.4610364117181245,
      0.35429659741097663,
      0.0,
      0.49766382647295515,
      0.5165378340736895,
      0.4464084245139244,
      0.3983875369699439
    ],
    [
      0.34822721521741395,
      0.4230090150103607,
      0.4902455422437386,
      0.5334386091403995,
      0.4715264841227067,
      0.41090664025466483,
      0.33301903829935076,
      0.46611190915954404,
      0.46444226329381166,
      0.36739716637640507,
      0.4796194992475129,
      0.22952324889459041,
      0.4655734988410605,
      0.47680343553167104,
      0.4910423772933703,
      0.4887261986175353,
      0.4168552724943322,
      0.5128974753227324,
      0.47437910676757955,
      0.48802842908365074,
      0.48590039161018783,
      0.3291750445072983,
      0.3614997089137202,
      0.3912596126015506,
      0.4082948163646436,
      0.0,
      0.48041132187692903,
      0.44789568470991314,
      0.38413530306391275
    ],
    [
      0.43092474364280386,
      0.43853780493284744,
      0.5446414604797496,
      0.47771362249008,
      0.5499244673813211,
      0.3873110045883925,
      0.3593128666703507,
      0.4080760999094113,
      0.46096633138227694,
      0.4109860601330739,
      0.42554459773289466,
      0.27375477571133766,
      0.47339636252469486,
      0.48925811795705476,
      0.4658847182037251,
      0.49884435412143135,
      0.48009020439966643,
      0.43925769092889944,
      0.44984591218973335,
      0.4579473448200213,
      0.4864048839655073,
      0.32032629837986826,
      0.4126429718377298,
      0.42867676200066973,
      0.4154213750192244,
      0.42983703561999986,
      0.0,
      0.45384673892883143,
      0.3338883262389205
    ],
    [
      0.33158726282889606,
      0.31814589214929323,
      0.44436516566697315,
      0.383530424653423,
      0.5130257889875456,
      0.33018598485778816,
      0.3304222862203561,
      0.3858715201313634,
      0.3698166441244475,
      0.392531369114123,
      0.30974616370620556,
      0.2812705656920478,
      0.36759123709395447,
      0.3292979142996282,
      0.3613829815978338,
      0.3966956593288984,
      0.3851717634888845,
      0.4207948651290987,
      0.36665859608134244,
      0.39087534883669206,
      0.3880151200626616,
      0.31946445698742765,
      0.3704127796600061,
      0.3470536059557148,
      0.3974405864681201,
      0.41950847330268615,
      0.45000522272704124,
      0.0,
      0.3146937559004661
    ],
    [
      0.36266417047201394,
      0.32300932392409964,
      0.4080447410550909,
      0.3800443237486215,
      0.3634303381638291,
      0.38025261268218835,
      0.3383981600614141,
      0.38443549403543975,
      0.38807331457821514,
      0.35993320223788405,
      0.36941176870229575,
      0.3741719237749579,
      0.3740157980798242,
      0.3619628007930602,
      0.3804295940667948,
      0.42367109084169075,
      0.4212259567165504,
      0.4042782375943801,
      0.37828526975914145,
      0.3927281462741765,
      0.3544718927256474,
      0.39940412008202353,
      0.3812033053705799,
      0.35033232655834556,
      0.36776289182273536,
      0.40637511589232767,
      0.39903926077329466,
      0.37844783802302606,
      0.0
    ]
  ],
  "row_avgs": [
    0.12749224937865286,
    0.37131537087767075,
    0.433511645302064,
    0.459030438443908,
    0.29033202553151316,
    0.40435135640137715,
    0.32164706440902097,
    0.40403575785519025,
    0.44053301551245255,
    0.3360432656249312,
    0.3958773745495467,
    0.28450798585208564,
    0.3846321672925767,
    0.41234655180463115,
    0.3912467464194834,
    0.3936252820575098,
    0.4350914811513828,
    0.4795908032588811,
    0.4306432764872582,
    0.3541125519212271,
    0.41883041864940085,
    0.3872768436556418,
    0.37647010982798934,
    0.38440804903432035,
    0.4470096932654157,
    0.43286943960216373,
    0.43583081900680426,
    0.37198433696617567,
    0.3787679649574875
  ],
  "col_avgs": [
    0.33822192306408544,
    0.41030137411090184,
    0.4447431708952724,
    0.43895903521834356,
    0.42928146547099705,
    0.3552446604784543,
    0.306056419267484,
    0.4112410684264214,
    0.4091867294054806,
    0.369432949453469,
    0.3986236462520356,
    0.264789951181425,
    0.41873342002246844,
    0.4150638543059726,
    0.41043375028551,
    0.4361215709267768,
    0.39900482230808476,
    0.40454060931621066,
    0.3984287303851067,
    0.42666168665414295,
    0.39430205881002295,
    0.3120294748225719,
    0.34714547599765394,
    0.3589509927076093,
    0.3507155698243095,
    0.40506831599971876,
    0.43510105583517766,
    0.37714868767293785,
    0.3178816159981179
  ],
  "combined_avgs": [
    0.23285708622136914,
    0.3908083724942863,
    0.4391274080986682,
    0.4489947368311258,
    0.35980674550125513,
    0.37979800843991574,
    0.3138517418382525,
    0.40763841314080584,
    0.4248598724589666,
    0.3527381075392001,
    0.3972505104007912,
    0.27464896851675535,
    0.4016827936575226,
    0.4137052030553019,
    0.4008402483524967,
    0.4148734264921433,
    0.41704815172973375,
    0.4420657062875459,
    0.41453600343618247,
    0.390387119287685,
    0.40656623872971187,
    0.34965315923910684,
    0.3618077929128216,
    0.3716795208709648,
    0.39886263154486257,
    0.4189688778009413,
    0.43546593742099093,
    0.37456651231955673,
    0.3483247904778027
  ],
  "gppm": [
    611.8575116216543,
    633.3936630520109,
    616.869487740638,
    621.0943712242039,
    618.3220166934185,
    661.9282021765808,
    682.5431167929111,
    631.0770458892017,
    635.9054506799431,
    653.4706771135322,
    640.6825897596444,
    700.2266785378085,
    630.0847563095316,
    633.8229611496612,
    633.5861522773976,
    621.977866918355,
    639.437247407836,
    636.4381726744389,
    640.4041463651309,
    624.7306201574775,
    640.968110419686,
    683.0186449083707,
    663.9457139296841,
    659.7385886506852,
    659.4652971740496,
    635.560126679656,
    619.6651690956653,
    650.4740915298273,
    679.1911634163545
  ],
  "gppm_normalized": [
    1.4121457754766558,
    1.4175869694449414,
    1.3785910921250053,
    1.3840934539923269,
    1.37623683689453,
    1.4824265180206724,
    1.5329963655885421,
    1.40819728454055,
    1.417930076502916,
    1.461405847695428,
    1.4327316239657562,
    1.571964433212852,
    1.4076040159024317,
    1.4127817593392653,
    1.4143525569943107,
    1.3901850956354065,
    1.4261556359949985,
    1.415019745792463,
    1.4276730731889606,
    1.3995733594487976,
    1.4225797252444714,
    1.527710782782924,
    1.4785305004159095,
    1.475039469166535,
    1.4711945313524775,
    1.4182590455565727,
    1.3819151906138634,
    1.4489623179874713,
    1.519546223224289
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389
  ],
  "response_lengths": [
    4674,
    2561,
    2553,
    2544,
    3518,
    2220,
    2622,
    2752,
    2519,
    2327,
    2544,
    2783,
    2711,
    2421,
    2578,
    2818,
    2282,
    2317,
    2406,
    2795,
    2246,
    2103,
    2532,
    2400,
    2200,
    2152,
    2489,
    2192,
    2297
  ]
}