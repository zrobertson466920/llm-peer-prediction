{
  "example_idx": 98,
  "reference": "Under review as a conference paper at ICLR 2023\n\nEFFICIENT AUTOMATIC GRAPH LEARNING VIA DESIGN RELATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDespite the success of automated machine learning (AutoML), which aims to find the best design, including the architecture of neural networks and hyper-parameters, conventional AutoML methods are computationally expensive and hardly provide insights into the relations of different model design choices. This work focus on the scope of AutoML on graph tasks. To tackle the challenges, we propose FALCON, an efficient sample-based method to search for the optimal model design on graph tasks. Our key insight is to model the design space of possible model designs as a design graph, where the nodes represent design choices, and the edges denote design similarities. FALCON features 1) a task-agnostic module, which performs message passing on the design graph via a Graph Neural Network (GNN), and 2) a task-specific module, which conducts label propagation of the known model performance information on the design graph. Both modules are combined to predict the design performances in the design space, navigating the search direction. We conduct extensive experiments on 27 node and graph classification tasks from various application domains. We empirically show that FALCON can efficiently obtain the well-performing designs for each task using only 30 explored nodes. Specifically, FALCON has a comparable time cost with the one-shot approaches while achieving an average improvement of 3.3% compared with the best baselines.\n\n1\n\nINTRODUCTION\n\nAutomated machine learning (AutoML) (Liu et al., 2019; Pham et al., 2018; Bender et al., 2018; Real et al., 2019; Zoph & Le, 2017; Cai et al., 2019; 2021; Gao et al., 2019; You et al., 2020b; Zhang et al., 2021) has demonstrated great success in various domains including computer vision (Chu et al., 2020; Ghiasi et al., 2019; Chen et al., 2019), language modeling (Zoph & Le, 2017; So et al., 2019), and recommender systems (Chen et al., 2022). It is an essential component for the state-of-the-art deep learning models (Liu et al., 2018; Baker et al., 2017; Xu et al., 2020; Chen et al., 2021).\n\nGiven a graph learning task, e.g., a node/graph classification task on graphs, our goal of AutoML is to search for a model architecture and hyper-parameter setting from a design space that results in the best test performance on the task. Following previous works (You et al., 2020b), we define design as a set of architecture and hyper-parameter choices (e.g., 3 layer, 64 embedding dimensions, batch normalization, skip connection between consecutive layers), and define design space as the space of all possible designs for a given task.\n\nHowever, AutoML is very computationally intensive. The design space of interest often involves millions of possible designs (Elsken et al.; You et al., 2020a). Sample-based AutoML (Zoph & Le, 2017; Gao et al., 2019; Bergstra et al., 2011; Liu et al., 2017; Luo et al., 2018) has been used to perform search via sampling candidate designs from the design space to explore. One central challenge of existing sample-based AutoML solutions is its sample efficiency: it needs to train as few models as possible to identify the best-performing model in the vast design space. To improve the efficiency, existing research focuses on developing good search algorithms to navigate in the design space (White et al., 2021; Shi et al., 2020; Ma et al., 2019).\n\nHowever, these methods do not consider modeling the effect of model design choices, which provides strong inductive biases in searching for the best-performing model. By “inductive bias”, we refer to the patterns of multiple variables interacting together, which can happen in multiple parts of the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Overview of FALCON. (a) Design graph example. We present a small design graph on TU-COX2 graph classification dataset. The design choices are shown in the table, #pre, #mp, #post denotes the numbers of pre-processing, message passing, and post-processing layers, respectively. The better design performance, the darker node colors. (b) FALCON search strategy. Red: Explored nodes. Green: Candidate nodes to be sampled from. Blue: The best node. Gray: Other nodes. Locally, FALCON extends the design subgraph via a search strategy detailed in Section 3.3. Globally, FALCON approaches the optimal design navigated by the inductive bias of the design relations.\n\ndesign space. Thus, an efficient search strategy should rapidly rule out a large subset of the design space with potentially bad performance leveraging such learned inductive bias.\n\nProposed approach. To overcome the limitations, we propose FALCON, an AutoML framework on graph tasks that achieves state-of-the-art sample efficiency and performance by leveraging model design insights. Our key insight is to build a design graph over the design space of architecture and hyper-parameter choices. FALCON extracts model design insights by learning a meta-model that captures the relation between the design graph and model performance and uses it to inform a sample-efficient search strategy. FALCON consists of the following two novel components.\n\nDesign space as a graph. Previous works view the model design space as a high-dimensional space with isolated design choices (You et al., 2020b), which offer few insights regarding the relations between different design choices. For example, through trial runs if we find the models with more than 3 layers do not work well without batch normalization, this knowledge can help us reduce the search space by excluding all model designs of more than 3 layers with batch normalization set to false. While such insights are hardly obtained with existing AutoML algorithms (Liu et al., 2019; Pham et al., 2018; Gao et al., 2019; Zoph & Le, 2017; Cai et al., 2019), FALCON achieves it via constructing a graph representation, design graph, among all the design choices. Figure 1(a) shows a visualization of a design graph, where each node represents a candidate design, and edges denote the similarity between the designs. See Section 3.1 for details on the similarity and graph construction.\n\nSearch by navigating on the design graph. Given the design graph, FALCON deploys a Graph Neural Network predictor, short for meta-GNN, which is supervised by the explored nodes’ performances and learns to predict the performance of a specific design given the corresponding node in the design graph. The meta-GNN is designed with 1) a task-agnostic module, which performs message passing on the design graph, and 2) a task-specific module, which conducts label propagation of the known model performance information on the design graph. Furthermore, we propose a search strategy that uses meta-GNN predictions to navigate the search in the design graph efficiently.\n\nExperiments. We conduct extensive experiments on 27 graph datasets, covering node- and graphlevel tasks with distinct distributions. Moreover, we demonstrate FALCON’ potential applicability on image datasets by conducting experiments on the CIFAR-10 image dataset. Our code is available at https://anonymous.4open.science/r/Falcon.\n\n2 RELATED WORK\n\nAutomatic Machine Learning (AutoML) is the cornerstone of discovering state-of-the-art model designs without costing massive human efforts. We introduce four types of related works below.\n\n2\n\nInferenceExplorationSample0.780.880.750.92�yy= 0.950.820.80(b) Search strategy of our methodNavigation(a) Design graph example #pre#mp#poststageagg[1, 2][2, 4, 6, 8][2, 3]skipconcat skipsummin, max, addUnder review as a conference paper at ICLR 2023\n\nSample-based AutoML methods. Existing sample-based approaches explore the search space via sampling candidate designs, which includes heuristic search algorithms, e.g., Simulated Annealing, Bayesian Optimization approaches (Bergstra et al., 2011; White et al., 2021; Ma et al., 2019), evolutionary- (Xie & Yuille, 2017; Real et al., 2017) and reinforcement-based methods (Zoph & Le, 2017; Zhou et al., 2019; Gao et al., 2019). However, they tend to train thousands of models from scratch, which results in the low sample efficiency. For example, (Zoph & Le, 2017; Gao et al., 2019) usually involve training hundreds of GPUs for several days, hindering the development of AutoML in real-world applications (Bender et al., 2018). Some hyper-parameter search methods aim to reduce the computational cost. For example, Successive Halving (Karnin et al., 2013) allocates the training resources to more potentially valuable models based on the early-stage training information. Li et al. (2017) further extend it using different budgets to find the best configurations to avoid the trade-off between selecting the configuration number and allocating the budget. Jaderberg et al. (2017) combine parallel search and sequential optimisation methods to conduct fast search. However, their selective mechanisms are only based on the model performance and lack of deep knowledge, which draws less insight into the relation of design variables and limits the sample efficiency.\n\nOne-shot AutoML methods. The one-shot approaches (Liu et al., 2019; Pham et al., 2018; Xie et al., 2019; Bender et al., 2018; Qin et al., 2021) have been popular for the high search efficiency. Specifically, they involve training a super-net representing the design space, i.e., containing every candidate design, and shares the weights for the same computational cell. Nevertheless, weight sharing degrades the reliability of design ranking, as it fails to reflect the true performance of the candidate designs (Yu et al., 2020).\n\nGraph-based AutoML methods. The key insight of our work is to construct the design space as a design graph, where nodes are candidate designs and edges denote design similarities, and deploy a Graph Neural Network, i.e., meta-GNN, to predict the design performance. Graph HyperNetwork (Zhang et al., 2019a) directly generates weights for each node in a computation graph representation. You et al. (2020a) study network generators that output relational graphs and analyze the link between their predictive performance and the graph structure. Recently, Zhao et al. (2020) considers both the micro- (i.e., a single block) and macro-architecture (i.e., block connections) of each design in graph domain. AutoGML (Park et al., 2022) designs a meta-graph to capture the relations among models and graphs and take a meta-learning approach to estimate the relevance of models to different graphs. Notably, none of these works model the search space as a design graph.\n\nDesign performance predictor. Previous works predict the performance of a design using the learning curves (Baker et al., 2018), layer-wise features (Deng et al., 2017), computational graph structure (Zhang et al., 2019a; White et al., 2021; Shi et al., 2019; Ma et al., 2019; Zhang et al., 2019b; Lee et al., 2021a), or combining dataset information (Lee et al., 2021a) via a dataset encoder. To highlight, FALCON explicitly models the relations among model designs. Moreover, it leverages the performance information on training instances to provide task-specific information besides the design features, which is differently motivated compared with Lee et al. (2021b) that employs meta-learning techniques and incorporate hardware features to rapidly adapt to unseen devices. Besides, meta-GNN is applicable for both images and graphs, compared with Lee et al. (2021a).\n\n3 PROPOSED METHOD\n\nThis section introduces our proposed approach FALCON for sample-based AutoML. In Section 3.1, we introduce the construction of design graph, and formulate the AutoML goal as a search on the design graph for the node with the best task performance. In Section 3.2, we introduce our novel neural predictor consisting of a task-agnostic module and a task-specific module, which predicts the performances of unknown designs. Finally, we detail our search strategy in Section 3.3. We refer the reader to Figure 1 (b) for a high-level overview of FALCON.\n\n3.1 DESIGN SPACE AS A GRAPH\n\nMotivation. Previous works generally consider each design choice as isolated from other designs. However, it is often observed that some designs that share the same design features, e.g., graph neural networks (GNNs) that are more than 3 layers and have batch normalization layers, may have similar performances. Moreover, the inductive bias of the relations between design choices can provide valuable information for navigating the design space for the best design. For example, suppose we find that setting batch normalization of a 3-layer GCN (Kipf & Welling, 2017) and a 4-layer GIN (Xu\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\net al., 2019) to false both degrade the performance. Then we can reasonably infer that a 3-layer GraphSAGE (Hamilton et al., 2017) with batch normalization outperforms the one without. We could leverage such knowledge and only search for the designs that are more likely to improve the task performance. To the best of our knowledge, FALCON is the first method to explicitly consider such relational information among model designs.\n\nDesign graph. We denote the design graph as G(N , E), where the nodes N include the candidate designs, and edges E denote the similarities between the candidate designs. Specifically, we use the notion of design distance to decide the graph connectivity, and we elaborate on them below.\n\nDesign distance. For each numerical design dimension, two design choices have a distance 1 if they are adjacent in the ordered list of design choices. For example, if the hidden dimension size can take values [16, 32, 64, 128], then the distance between 16 and 32 is 1, and the distance between 32 and 128 is 2. For each categorical design dimension, any two distinct design choices have a distance 1. We then define the connectivity of the design graph in terms of the design distance:\n\nDefinition 1 (Design Graph Connectivity) The design graph can be expressed as G(N , E), where the nodes N = {d1, . . . , dn} are model designs, and (di, dj) ∈ E iff the design distance between di and dj is 1.\n\nStructure of the design graph. The definition of edges implies that the design graph is highly structured, with the following properties: (1) All designs that are the same except for one categorical design dimension form a clique subgraph. (2) All designs that are the same except k numerical design dimensions form a grid graph structure. Moreover, we use a special calculation for the design distance with a combination of design dimensions that have dependencies. For example, the design dimensions of pooling operations, pooling layers, and the number of layers can depend on each other, thus the design graph structure becomes more complex. See the details in Appendix A.2.\n\nDesign subgraph. The design graph may contain millions of nodes. Therefore, directly applying the meta-model to the design graph is computationally intensive. Moreover, a reliable performance estimation for an unknown node depends on its similarity between the nodes already explored by the search algorithm. Therefore, we focus on using a meta-model to predict performance for a dynamic subgraph, i.e., design subgraph, containing the explored nodes in the current search stage and the candidate nodes to be sampled in the next step. The candidate set can be constructed by selecting the multi-hop neighbors of explored nodes on the design graph. The design subgraph is defined as:\n\nDefinition 2 (Design Subgraph) During a search, suppose the explored node set is Ne and the candidate set is Nc. The design subgraph is formulated as Gs(Ns, Es), where Ns = Ne ∪ Nc are the nodes and Es = {(u, v)|u ∈ Ns, v ∈ Ns, (u, v) ∈ N } are the edges.\n\nGiven the design subgraph, we formulate the AutoML problem as searching for the node, i.e., design choice, with the best task performance.\n\n3.2 META-GNN FOR PERFORMANCE PREDICTION\n\nHere we introduce a meta-model, named meta-GNN, to predict the performance of model designs, i.e., nodes of the design subgraph. The goal of meta-GNN is learning the inductive bias of design relations, which is used to navigate the search path on the design graph. As is illustrated in Figure 2, the meta-GNN comprises a task-agnostic module and a task-specific module, used to capture the knowledge of model design and task performance, respectively.\n\nTask-agnostic module. The task-agnostic module uses a design encoder to encode the design features on nodes of the design subgraph, and a relation encoder to capture the design similarities and differences on edges of the design subgraph. After that, it performs message passing on the design subgraph. We introduce each component below:\n\n• Design encoder: it computes the node features of design subgraph by the concatenation of the feature encoding of each design dimension. For numerical design dimensions,we conduct min-max normalization on their values as the node features. For categorical design dimensions such as aggregation operator which takes one of (SUM, MAX, MEAN), we encode it as a one-hot feature.\n\n• Relation encoder: it captures the similarity relationships between the connecting designs. For each\n\n(di, dj) ∈ E, we encode the design dimension where di and dj differ by a one-hot encoding.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Meta-GNN Framework: Task-agnostic module generates the embedding given the design variables and their graphical structures. Task-specific module leverages performance information and conducts label propagation to generate the task-specific embeddings. The two embeddings are concatenated and input into an MLP for predicting the design performance.\n\n• Message passing module: a GNN model is used to take the design subgraph and the processed features to perform message passing and output node representations. This information will be combined with the task-specific module to predict the design’s performance.\n\nTask-specific module. The task-specific module takes into account the information of design performance on selected training instances and thus is specific for one dataset.\n\nThe challenge of including such task-specific performance is that it is only available on a very limited set of explored nodes. To overcome the challenge, we use label propagation to propagate the performance information of explored nodes to the unexplored nodes. This is based on our observation that models trained with similar designs typically make similar predictions on instances. We provide an example in Figure 2 to illustrate the task-specific module.\n\n• Identifying critical instances: The first step is to identify critical training instances that result in different performances across different designs. Here we use a set of explored designs (anchors) to provide the instance-wise performances. Specifically, the (i, j) element of the top left matrix of Figure 2 represents whether the i-th design can correctly predict the label of j-th instance. Then, we compute the entropy of each training instance’s performance over the anchors. Then we obtain the instance-wise probability via Softmax on the entropy vector, from which we sample instances that result in high variation across designs. The high variation implies that these instances can distinguish good designs from bad ones in the design subgraph, which are informative.\n\n• Label propagation and linear projection: Based on the inductive bias of smoothness, we perform label propagation to make the task-specific information available to all candidate designs. Concretely, label propagation can be written as\n\nY(k+1) = α · D−1/2AD−1/2Y(k) + (1 − α)Y(k)\n\n(1)\n\nwhere each row of Y is the performance vector of design i (if explored) or a zero vector (for the unexplored designs). D ∈ R|Ns|×|Ns| is the diagonal matrix of node degree, A ∈ R|Ns|×|Ns| is the adjacent matrix of the design subgraph, and α is a hyper-parameter. After label propagation, we use a linear layer to project the performance information to another high-dimensional space.\n\nFinally, as shown in Figure 2, we concatenate the output embeddings of the task-specific and taskagnostic modules and use an MLP to produce the performance predictions.\n\nObjective for Meta-GNN. The training of neural performance predictor is commonly formulated as a regression using mean square error (MSE), in order to predict how good the candidate designs are for the current task. However, the number of explored designs is usually small for sample-efficient AutoML, especially during the early stage of the search process. Thus, the deep predictor tends to overfit, degrading the reliability of performance prediction. To solve this problem, we incorporate a pair-wise rank loss (Burges et al., 2005; Hu et al., 2021) with the MSE objective, resulting in\n\n5\n\naTraining Instances012nbcEntropyMaskSampleSoftmax1000...1100...1110......Designs00.690.690...0.010.080.080......12Instance-wise PerformanceMessage passingRelation encoderDesign encoderTask-agnostic ModuleDesign featureTask-agnostic embeddingabc00...10......11...Linear layerLabel propagationTask-Specific ModulePerformance Information Task-specific embeddingabcEstimated performanceY�max: 0.920.880.750.80yb: 0.78ya: 0.95yc: 0.820.85MLPUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Pseudocode of FALCON Require: S: Design space. K: Exploration size. hθ: Meta-GNN. V /V ′: Warm-up / Full epoch. η:\n\nLearning rate. C: Number of start nodes.\n\n1: Ω ← SAMPLE-NODES(S, C) 2: Γ ← MULTI-HOP-NEIGHBORS(Ω) 3: YΩ = GET-VALIDATION-PERFORMANCE(Ω, V ) 4: while t = |Ω| < K do\n\n// Initialize the exploration set // Construct candidate set from multi-hop neighbors // Explore the initial nodes (for V epochs)\n\n5: 6:\n\n7: 8: 9:\n\ns ← (Nv = Ω ∪ Γ, E = SIMILARITY(Nv)\n\nG(t) while not converge do\n\n// (1) Update the design subgraph\n\nθ ← θ − η · ∂L( ˆYΩ, hθ(G(t)\n\ns )Ω)/∂θ\n\n// (2) Compute Eq. 2 and conduct optimization\n\nend while // (3) Sample a candidate node with probability proportional to the meta-GNN’s prediction d(t) = SAMPLE-WITH-PROBABILITY(Γ, Softmax(hθ(G(t) Yt = GET-VALIDATION-PERFORMANCE(d(t), V ) Ω ← Ω ∪ {d(t)}, Γ ← Γ ∪ MULTI-HOP-NEIGHBORS({d(t)})\n\n// (2) Explore the current selected node\n\ns )Γ))\n\n10: 11: 12: 13: end while 14: D = SELECT-TOPK({Ωi : Yi}K 15: Y ′ = GET-VALIDATION-PERFORMANCE(D, V ′) 16: I = ARGMAX(Y ′) 17: return DI , Y ′\n\nI\n\ni=1, size = MIN(⌈10% · K⌉, 5))\n\n// Models to be fully trained // Obtain the final performance // Obtain best model\n\na quadratic number of training pairs, thus reducing overfitting. Furthermore, predicting relative performance is more robust across datasets than predicting absolute performance. Overall, the objective is formulated as follows:\n\nL( ˆY , Y ) =\n\nN (cid:88)\n\n( ˆYi − Yi)2 + λLrank( ˆY , Y ), where\n\ni=1\n\nLrank( ˆY , Y ) =\n\nN (cid:88)\n\nN (cid:88)\n\ni=1\n\nj=i\n\n(−1)I(Yi>Yj ) · σ\n\n(cid:32) ˆYi − ˆYj\n\n(cid:33)\n\nτ\n\n(2)\n\nwhere λ is the trade-off hyper-parameter, τ is the temperature controlling the minimal performance gap that will be highly penalized, and σ is the Sigmoid function. Thus, the meta-GNN is trained to predict the node performance on the design subgraph supervised by the explored node performance.\n\n3.3 SEARCH STRATEGY\n\nEquipped with the meta-GNN, we propose a sequential search strategy to search for the best design in the design graph. The core idea is to leverage meta-GNN to perform fast inference on the dynamic design subgraph, and decide what would be the next node to explore, thus navigating the search. We summarize our search strategy in Algorithm 1. Concretely, our search strategy consists of the following three steps:\n\n• Initialization: As shown in Figure 1 (b), FALCON randomly samples multiple nodes on the design graph. The motivation of sampling multiple nodes in the initial step is to enlarge the receptive field on the design graph to avoid the local optima and bad performance, which is empirically verified in Appendix D. Then, FALCON explore the initialized nodes by training designs on the tasks and construct the instance mask for the task-specific module.\n\n• Meta-GNN training: Following Figure 2, meta-GNN predicts the performance of the explored nodes. The loss is computed via Equation 2 and back-propagated to optimize the meta-GNN.\n\n• Exploration via inference: Meta-GNN is then used to make predictions for the performances of all candidate nodes. Then we apply Softmax on the predictions as the probability distribution of candidate designs, from which FALCON samples a new node and updates the design subgraph.\n\nAt every iteration, FALCON extends the design subgraph through the last two steps. After several iterations, it selects and retrains a few designs in the search trajectory with top performances. Overall,\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFALCON approaches the optimal design navigated by the relational inductive bias learned by metaGNN, as shown in Figure 1 (b).\n\n4 EXPERIMENTS\n\nWe conduct extensive experiments on 27 graph datasets and an image dataset. The goal is twofold: (1) to show FALCON’s sample efficiency over the existing AutoML methods (cf. Section 4.2) and (2) to provide insights into how the inductive bias of design relartions navigate the search on design graph (cf. Section 4.3).\n\n4.1 EXPERIMENTAL SETTINGS\n\nWe consider the following tasks in our evaluation and we leave the details including dataset split, evaluation metrics, and hyper-parameters in Appendix A.\n\nNode classification. We use 6 benchmarks ranging from citation networks to product or social networks: Cora, CiteSeer, PubMed (Sen et al., 2008), ogbn-arxiv (Hu et al., 2020), AmazonComputers (Shchur et al., 2018), and Reddit (Zeng et al., 2020). Graph classification. We use 21 benchmark binary classification tasks in TUDataset (Morris et al., 2020), which are to predict certain properties for molecule datasets with various distribution. Image classification. We use CIFAR-10 (Krizhevsky, 2009). See details in Appendix C.\n\nBaselines. We compare FALCON with three types of baselines:\n\n• Simple search strategies: Random, Simulated Annealing (SA), Bayesian Optimization\n\n(BO) (Bergstra et al., 2011).\n\n• AutoML approaches: DARTS (Liu et al., 2019), ENAS (Pham et al., 2018), GraphNAS (Gao et al., 2019), AutoAttend (Guan et al., 2021), GASSO (Qin et al., 2021), where the last three methods are specifically designed for graph tasks.\n\n• Ablation models: FALCON-G and FALCON-LP, where FALCON-G discards the design graph and predicts the design performance using an MLP, and FALCON-LP removes the task-specific module and predicts design performance using only the task-agnostic module.\n\nWe also include a naive method, BRUTEFORCE, which trains 5% designs from scratch and returns the best design among them. The result of BRUTEFORCE is regarded as the approximated ground truth performance. We compare FALCON and the simple search baselines under sample size controlled search, where we limit the number of explored designs. We set the exploration size as 30 by default.\n\nDesign Space. We use different design spaces on node- and graph-level tasks. Specifically, The design variables include common hyper-parameters, e.g., dropout ratio, and architecture choices, e.g., layer connectivity and batch normalization. Moreover, we consider node pooling choices for the graph classification datasets, which is less studied in the previous works (Cai et al., 2021; Gao et al., 2019; Zhou et al., 2019). Besides, we follow You et al. (2020b) and control the number of parameters for all the candidate designs to ensure a fair comparison. See Appendix A.2 for the details.\n\n4.2 MAIN RESULTS\n\nNode classification tasks. Table 1 summarizes the performance of FALCON and the baselines.\n\nNotably, FALCON takes comparable search cost as the oneshot methods and is 15x less expensive than GraphNAS. Moreover, FALCON achieves the best performances over the baselines with sufficient margins in the most datasets, using only 30 explored designs. For example, FALCON outperforms ENAS by 1.8% in CiteSeer and GASSO by 1.6% in AmazonComputers. Also, the removal of the design graph and task-specific module decreases the performance constantly, which validates their effectiveness. It is worth mentioning that FALCON is competitive with BRUTEFORCE, demonstrating the excellence of FALCON in searching for globally bestperforming designs.\n\n7\n\nFigure 3: Accuracy v.s. the number of explored nodes on ogbn-arxiv.\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Search results on five node classification tasks, where Time stands for the search cost (GPU·hours). We conduct t-test to compute p-value on our method with the best AutoML baselines.\n\nCora\n\nCiteSeer\n\nPubmed\n\nAmazonComputers\n\nReddit\n\nACC\n\nTime ACC\n\nTime ACC\n\nTime ACC\n\nTime\n\nF1\n\nTime\n\nRandom 80.8±1.7 BO 85.1±0.3 SA 81.1±0.8 85.8±0.4 85.8±0.2 82.2±3.6 84.6±0.2 GASSO 86.8±1.1\n\nENAS DARTS GraphNAS AutoAttend\n\nFALCON-G 84.5±0.8 85.5±1.0 86.4±0.5 87.0\n\nFALCON-LP FALCON BRUTEFORCE\n\n0.20 0.28 0.24 0.27 0.25 3.12 1.23 0.38\n\n0.23 0.26 0.26 52.5\n\n71.2±0.8 72.6±0.9 74.7±0.2 74.9±0.2 75.2±0.3 74.9±0.6 73.9±0.2 75.3±0.7\n\n74.3±1.7 74.6±0.1 76.2±0.4 76.0\n\n0.22 0.30 0.25 0.39 0.25 3.99 1.25 0.33\n\n0.24 0.26 0.28 59.7\n\n86.0±3.5 88.5±0.3 88.9±0.1 88.6±0.8 89.1±0.1 89.2±0.3 84.4±0.7 86.3±0.4\n\n89.2±0.1 89.0±0.2 89.3±0.5 90.0\n\n0.24 0.31 0.29 2.06 0.35 5.37 1.55 0.41\n\n0.26 0.29 0.32 63.0\n\n81.6±3.0 82.3±6.3 81.2±6.9 74.5±1.2 84.1±1.9 88.5±2.4 87.3±1.1 89.8±0.1\n\n87.6±0.9 90.7±0.6 91.2±0.5 91.4\n\np-value\n\n-\n\n-\n\n0.051\n\n-\n\n0.145\n\n-\n\n0.017\n\n0.16 0.16 0.23 0.83 0.35 2.53 2.62 0.73\n\n0.27 0.30 0.30 81.5\n\n-\n\n94.3±0.1 94.2±0.2 94.3±0.5 92.3±1.0 [OoM] 89.1±2.9 [OoM] [OoM]\n\n93.7±0.4 94.9±0.2 95.2±0.2 95.5\n\n0002\n\n0.97 0.94 0.97 1.98 -\n3.03 -\n-\n\n1.11 1.00 1.15 >200\n\n-\n\nWe further investigate the speed-performance trade-off of FALCON and other sample-based approaches in ogbn-arxiv. We run several search trials under different sample sizes. As shown in Figure 3, FALCON reaches the approximated ground truth result with very few explored nodes. In contrast, SA and Random require more samples to converge, while BO performs bad even with a large number of explored nodes, potentially due to its inability in dealing with high-dimensional design features.\n\nGraph classification tasks. The graph classification datasets cover a wide range of graph distributions. In Table 2, we report the selected performance results for graph classification tasks and leave other results including the search costs in Appendix B. We highlight three observations:\n\n• On average, the state-of-the-art AutoML baselines algorithms perform close to the simple search methods, indicating the potentially unreliable search, as similarly concluded by Yu et al. (2020).\n\n• FALCON surpasses the best AutoML baselines with an average improvement of 3.3%. The sufficient and consistent improvement greatly validates our sample efficiency under a controlled sample size. where FALCON can explore the designs that are more likely to perform well through the relational inference based on the relations of previously explored designs and their performances.\n\n• In the second block, we attribute the high sample efficiency of FALCON to the exhibition of design relations and the performance information from the training instances. Specifically, FALCON outperforms FALCON-LP by 4.87% on average, indicating that the task-specific module provides more task information that aids the representation learning of model designs, enabling a fast adaption on a certain task. Moreover, FALCON gains an average improvement of 6.43% compared to FALCON-G, which justifies our motivation that the design relations promote the learning of relational inductive bias and guide the search on the design graph.\n\nWe also conduct experiments similar to Figure 3 to investigate how FALCON converges with the increasing sample size (cf. Appendix B.1) and report the best designs found by FALCON for each dataset (cf. Appendix B.2). Besides, we provide sensitivity analysis on FALCON’s hyper-parameters, e.g., number of random start nodes C (cf. Appendix D).\n\nImage classification task. We demonstrate the potential of FALCON in image domain. Due to space limitation, we leave the results of CIFAR-10 to Appendix C. We found FALCON can search for designs that are best-performing, compared with the baselines. Specifically, it gains average improvements of 1.4% over the simple search baselines and 0.3% over the one-shot baselines on the architecture design space, with search cost comparable to the one-shot based baselines.\n\n4.3 CASE STUDIES OF FALCON\n\nWe study FALCON in two dimensions: (1) Search process: we probe FALCON’s inference process through the explanations of meta-GNN on a design graph, and (2) Design representations: we visualize the node representations output by the meta-GNN to examine the effect of design choices.\n\nSearch process. We use GNNExplainer (Ying et al., 2019) to explain the node prediction of metaGNN and shed light on the working mechanism of FALCON. Here we consider the importance of\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Selected results for the graph classification tasks. The average task performance (ROC-AUC) of the architectures searched by FALCON is 3.3% over the best AutoML baselines.\n\nER-MD AIDS\n\nOVCAR-8 MCF-7\n\nSN12C\n\nNCI109\n\nTox21-AhR Avg.\n\nRandom 77.5±1.6 BO 77.6±3.5 SA 75.9±4.2 76.0±2.2 75.0±0.7 76.9±3.6 73.1±0.8 GASSO 73.2±0.4\n\nENAS DARTS GraphNAS AutoAttend\n\nFALCON-G 78.3±3.0 76.7±2.4 78.4±0.2 83.3\n\nFALCON-LP FALCON BRUTEFORCE\n\n97.0±1.4 96.1±1.0 95.4±0.9 97.1±0.4 98.0±0.0 95.9±0.8 97.4±0.3 95.2±0.7\n\n96.3±1.4 96.0±0.2 97.5±1.1 96.0\n\np-value\n\n0.155\n\n-\n\n56.2±0.0 63.6±0.7 59.5±3.2 56.0±1.3 56.8±0.3 58.7±0.8 59.8±0.8 62.3±0.3\n\n56.4±1.1 61.5±4.9 66.7±3.4 67.4\n\n0.008\n\n58.2±0.3 60.7±0.0 56.7±0.8 59.7±0.8 60.2±0.7 61.3±5.2 64.4±0.2 62.5±0.4\n\n62.3±4.5 59.5±5.7 65.5±2.5 70.6\n\n57.4±1.0 54.8±1.1 60.4±1.7 66.4±0.6 66.0±0.4 60.7±1.5 71.8±0.3 70.9±2.3\n\n69.8±2.2 70.3±3.8 73.3±0.0 73.7\n\n73.4±0.9 73.6±1.2 76.6±5.6 71.2±1.0 73.5±0.2 73.6±2.9 75.9±1.8 73.9±0.4\n\n70.3±6.4 73.1±0.3 78.4±2.3 81.8\n\n75.7±2.0 75.5±1.1 76.5±3.0 73.6±0.9 76.0±1.1 70.6±4.3 74.1±0.9 70.2±3.5\n\n72.5±2.8 76.5±2.5 78.5±1.1 82.0\n\n0.035\n\n<0.001\n\n0.096\n\n0.018\n\n70.8 71.7 71.6 71.4 72.2 71.1 73.8 72.6\n\n72.3 73.3 76.9 79.3\n\n-\n\neach design dimension for each node’s prediction. We demonstrate on a real case when searching on CIFAR-10 (cf. Table 12 for the design space). For conciseness, we focus on two design dimensions: (Weight Decay, Batch Size). Then, given a node of interest n′ = (0.9, 128), we observe the change in its predictions and dimension importance during the search process.\n\n——————————————————————————————– Explored node nt: ... Performance of nt: ... Prediction on n′: ... Dimension importance: ... ——————————————————————————————–\n\n(0.99, 128) +\n0.89 [0.6, 0.4]\n\n(0.99, 64) ++ 0.90 [0.5, 0.5]\n\n(0.9, 64) −\n0.77 [0.8, 0.2]\n\n... ... ... ...\n\n... ... ... ...\n\nWhere + and − indicate the relative performance of the explored nodes, t is the current search step. Interestingly, we see that the prediction on n′ and the dimension importance evolve with the explored designs and their relations. For example, when weight decay changes from 0.99 to 0.9, there is a huge drop in the node performance, which affects the prediction of n′ and increases the importance of Weight Decay as design performance seems to be sensitive to this dimension.\n\nDesign representations. In Figure 4, we visualize the high-dimensional design representations via T-SNE (van der Maaten & Hinton, 2008) after training the meta-GNN on the Cora dataset.\n\nIn the left figure, the better the design performance, the darker the color. Generally, the points with small distance have similar colors or performances, indicating that meta-GNN can distinguish “good” nodes from “bad” nodes. For the right figure, different colors represent different dropout ratios. The high discrimination indicates that the dropout ratio is an influential variable for learning the design representation, which further affects design performance. This evidence validates the meta-GNN’s expressiveness and capacity to learn the relational inductive bias between the design choices.\n\nFigure 4: T-SNE visualization for the design representations on Cora dataset.\n\n5 CONCLUSION, LIMITATION, AND FUTURE WORK\n\nThis work introduces FALCON, an efficient sample-based AutoML framework. We propose the concept of design graph that explicitly models and encodes relational information among model designs. On top of the design graph, we develop a sample-efficient strategy to navigate the search on the design graph with a novel meta-model. One future direction is to better tackle the high average node degree on the design graphwhich could cause over-smoothing, especially when the design variables include many categorical variables. And a simple solution is to use edge dropout to randomly remove a portion of edges at each training epoch. Another future direction is to better adapt FALCON on continuous design variables via developing a dynamic design graph that enable a more fine-grained search between the discretized values.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nAll of the datasets used in this work are public. For experimental setup, we state the detailed settings in Appendix A and Appendix C, including the graph pre-processing, dataset splits, hyper-parameters. Moreover, we include our code in an anonymous link for public access. For the results, we report the best models found by our algorithm as well as their corresponding performances. Overall, we believe we have made great efforts to ensure reproducibility in this paper.\n\nETHICS STATEMENT\n\nIn this work, we propose a novel algorithm to search for the best model designs where no human subject is related. This work could promote the discovery of more powerful and expressive models and provide insights into design relations. However, while best-performing models may be “experts” in fulfilling given tasks, they are not necessarily fair towards different user or entity groups. We believe this is a general issue in the AutoML area and should be well addressed to ensure the ethics of models in real-world applications.\n\nREFERENCES\n\nBowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architec-\n\ntures using reinforcement learning. In ICLR, 2017.\n\nBowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture\n\nsearch using performance prediction. In ICLR, 2018.\n\nGabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Understand-\n\ning and simplifying one-shot architecture search. In ICML, 2018.\n\nJames Bergstra, R ́emi Bardenet, Yoshua Bengio, and Bal ́azs K ́egl. Algorithms for hyper-parameter\n\noptimization. In NeurIPS, 2011.\n\nFilippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks\n\nwith convolutional ARMA filters. arXiv, 2019.\n\nChristopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and\n\nGregory N. Hullender. Learning to rank using gradient descent. In ICML, 2005.\n\nHan Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task\n\nand hardware. In ICLR, 2019.\n\nShaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, and Qingming Huang.\n\nRethinking graph neural architecture search from message-passing. In CVPR, 2021.\n\nBo Chen, Xiangyu Zhao, Yejing Wang, Wenqi Fan, Huifeng Guo, and Ruiming Tang. Automated\n\nmachine learning for deep recommender systems: A survey. 2022.\n\nXiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. Drnas:\n\nDirichlet neural architecture search. In ICLR, 2021.\n\nYukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng, Chunhong Pan, and Jian Sun. Detnas:\n\nNeural architecture search on object detection. 2019.\n\nXiangxiang Chu, Bo Zhang, Hailong Ma, Ruijun Xu, and Qingyuan Li. Fast, accurate and lightweight\n\nsuper-resolution with neural architecture search. In ICPR. IEEE, 2020.\n\nBoyang Deng, Junjie Yan, and Dahua Lin. Peephole: Predicting network performance before training.\n\nArxiv, 2017.\n\nFrederik Diehl. Edge contraction pooling for graph neural networks. CoRR, abs/1905.10990, 2019.\n\nJian Du, Shanghang Zhang, Guanhang Wu, Jos ́e M. F. Moura, and Soummya Kar. Topology adaptive\n\ngraph convolutional networks.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. J.\n\nMach. Learn. Res.\n\nHongyang Gao and Shuiwang Ji. Graph u-nets. In ICML, volume 97, pp. 2083–2092, 2019.\n\nYang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graphnas: Graph neural architecture\n\nsearch with reinforcement learning. arXiv, 1904.09981, 2019.\n\nGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. NAS-FPN: learning scalable feature pyramid\n\narchitecture for object detection. In CVPR, 2019.\n\nChaoyu Guan, Xin Wang, and Wenwu Zhu. Autoattend: Automated attention representation search.\n\nIn ICML, 2021.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\n\ngraphs. In NeurIPS, pp. 1024–1034, 2017.\n\nChi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo Zhu, and Changliang Li. Ranknas: Efficient neural architecture search by pairwise ranking. In EMNLP, 2021.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.\n\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks. CoRR, 2017.\n\nZohar Shay Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed\n\nbandits. In ICML, 2013.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n\nIn ICLR, 2017.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. In Technical report, 2009.\n\nHayeon Lee, Eunyoung Hyung, and Sung Ju Hwang. Rapid neural architecture search by learning to\n\ngenerate graphs from datasets. In ICLR, 2021a.\n\nHayeon Lee, Sewoong Lee, Song Chong, and Sung Ju Hwang. Hardware-adaptive efficient latency\n\nprediction for NAS via meta-learning. In NeurIPS, 2021b.\n\nJunhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In ICML, 2019.\n\nLisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 2017.\n\nChenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L. Yuille, Jonathan\n\nHuang, and Kevin Murphy. Progressive neural architecture search. 1712.00559, 2017.\n\nHanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierar-\n\nchical representations for efficient architecture search. In ICLR, 2018.\n\nHanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In\n\nICLR, 2019.\n\nRenqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.\n\nIn NeurIPS, 2018.\n\nLizheng Ma, Jiaxu Cui, and Bo Yang. Deep neural architecture search with deep graph bayesian\n\noptimization. In WI. ACM, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nZheng Ma, Junyu Xuan, Yu Guang Wang, Ming Li, and Pietro Li`o. Path integral based convolution\n\nand pooling for graph neural networks. 2020.\n\nChristopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAI, pp. 4602–4609, 2019.\n\nChristopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion\n\nNeumann. Tudataset: A collection of benchmark datasets for learning with graphs. 2020.\n\nNamyong Park, Ryan A. Rossi, Nesreen K. Ahmed, and Christos Faloutsos. Autogml: Fast automatic\n\nmodel selection for graph machine learning. CoRR, 2022.\n\nHieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture\n\nsearch via parameter sharing. In ICML, 2018.\n\nYijian Qin, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Graph differentiable architecture search with\n\nstructure learning. In NeurIPS, 2021.\n\nEsteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V.\n\nLe, and Alexey Kurakin. Large-scale evolution of image classifiers. In ICML, 2017.\n\nEsteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image\n\nclassifier architecture search. AAAI Press, 2019.\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.\n\nCollective classification in network data. AI Mag., 2008.\n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G ̈unnemann. Pitfalls\n\nof graph neural network evaluation. 2018.\n\nHan Shi, Renjie Pi, Hang Xu, Zhenguo Li, James T. Kwok, and Tong Zhang. Multi-objective neural\n\narchitecture search via predictive network performance optimization. 1911.09336, 2019.\n\nHan Shi, Renjie Pi, Hang Xu, Zhenguo Li, James T. Kwok, and Tong Zhang. Bridging the gap between sample-based and one-shot neural architecture search with BONAS. In NeurIPS, 2020.\n\nDavid R. So, Quoc V. Le, and Chen Liang. The evolved transformer. In ICML, 2019.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing high-dimensional data using t-sne. Journal\n\nof Machine Learning Research, 2008.\n\nColin White, Willie Neiswanger, and Yash Savani. BANANAS: bayesian optimization with neural\n\narchitectures for neural architecture search. In AAAI, 2021.\n\nLingxi Xie and Alan L. Yuille. Genetic CNN. In IEEE, 2017.\n\nSirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search.\n\nIn ICLR, 2019.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? In ICLR, 2019.\n\nYuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. PC-DARTS: partial channel connections for memory-efficient architecture search. In ICLR, 2020.\n\nZhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:\n\nGenerating explanations for graph neural networks. In NeurIPS, pp. 9240–9251, 2019.\n\nJiaxuan You, Jure Leskovec, Kaiming He, and Saining Xie. Graph structure of neural networks. In\n\nICML, 2020a.\n\nJiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. In NeurIPS,\n\n2020b.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nKaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating the\n\nsearch phase of neural architecture search. In ICLR, 2020.\n\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna.\n\nGraphsaint: Graph sampling based inductive learning method. In ICLR, 2020.\n\nChris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search.\n\nIn ICLR, 2019a.\n\nMuhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, and Yixin Chen. D-VAE: A variational\n\nautoencoder for directed acyclic graphs. In NeurIPS, 2019b.\n\nZiwei Zhang, Xin Wang, and Wenwu Zhu. Automated machine learning on graphs: A survey. In\n\nIJCAI, 2021.\n\nYiren Zhao, Duo Wang, Xitong Gao, Robert D. Mullins, Pietro Li`o, and Mateja Jamnik. Probabilistic\n\ndual network architecture search on graphs. 2020.\n\nKaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of\n\ngraph neural networks. 2019.\n\nBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In ICLR, 2017.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENT DETAILS\n\nA.1 SETTINGS\n\nGraph classification datasets. The graph classification datasets used in this work are summarized in Table 3. And the detailed dataset statics can be referred from https://chrsmrrs.github. io/datasets/docs/datasets/.\n\nTable 3: List of the graph classification datasets used in this work.\n\nSmall Scale\n\nAIDS, BZR-MD, COX2-MD, DHFR-MD, Mutagenicity, NCI1, NCI109, PTC-MM, PTC-MR\n\nMedium/Large Scale\n\nTox21-AhR, MCF-7, MOLT-4, UACC257, Yeast, NCI-H23, OVCAR-8, P388, PC-3, SF-295, SN12C, SW-620\n\nSpecifically, all datasets are binary classification tasks that predict certain properties for small molecules. For example, the labels in Tox21-AhR represent toxicity/non-toxicity, while the graphs in Mutagenicity are classified into two classes based on their mutagenic effect on a bacterium (Morris et al., 2020). Consequently, we use atom types as the node features and bond types as edge features.\n\nEvaluation metrics. For Reddit, we use F1 score (micro) as the evaluation metric following the previous work (Zeng et al., 2020). For other node classification tasks and image dataset, we use classification accuracy as the evaluation metric. For the graph classification tasks, we use ROC-AUC as the evaluation metric.\n\nDataset splits. For ogbn-arxiv and Reddit, we use the standardized dataset split. For other node classification datasets, we split the nodes in each graph into 70%, 10%, 20% in training, validation, and test sets, respectively. For graph classification tasks, we split the graphs into 80%, 10%, 10% for training, validation, and test sets, respectively.\n\nHyper-Parameters. We tuned the hyper-parameters of the baselines based on the default setting in their public codes. For FALCON, we construct the candidate set as the 3-hop neighbors of the explored nodes and set the number of start nodes as min(⌈10% · K⌉, 10), where K denotes the exploration size. The meta-GNN is constitute of 3 message-passing layers and 3 label propagation layers. All the experiments are repeated at least 3 times.\n\nA.2 DESIGN SPACES\n\nA.2.1 DESIGN SPACES FOR THE SAMPLE-BASED METHODS\n\nIn this work, we use different design spaces for the datasets depending on the task types, i.e., node or graph level. We summarize the design variables and choices in Table 4 and Table 5. For the design space of Reddit, we replace ”Aggregation” in Table 4 with ”Convolutional layer type”, which takes values from {GCNConv (Kipf & Welling, 2017), SAGEConv (Hamilton et al., 2017), GraphConv (Morris et al., 2019), GINConv (Xu et al., 2019), ARMAConv (Bianchi et al., 2019), TAGConv (Du et al.)}.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Design Space for the node-level tasks (except for Reddit). 5,832 candidates in total.\n\nType\n\nVariable\n\nCandidate Values\n\nHyper-parameters\n\nDropout ratio\n\n[0.0, 0.3, 0.6]\n\nArchitecture\n\n# Pre-process layers # Message passing layers # Post-precess layers Layer connectivity Activation Batch norm Aggregation\n\n[1, 2, 3] [2, 4, 6, 8] [1, 2, 3] STACK, SUM, CAT ReLU, Swish, Prelu True, False Mean, Max, SUM\n\nTable 5: Design Space for the graph-level tasks. 58,320 candidates in total.\n\nType\n\nHyper-parameters\n\nVariable\n\nDropout ratio\n\nCandidate Values\n\n[0.0, 0.3, 0.6]\n\nArchitecture\n\n# Pre-process layers # Message passing layers # Post-precess layers Layer connectivity Activation Batch norm Aggregation Node pooling flag (Use node pooling)\n\nNode pooling type (if applicable)\n\nNode pooling loop\n\n[1, 2, 3] [2, 4, 6, 8] [1, 2, 3] STACK, SUM, CAT ReLU, Swish, Prelu True, False Mean, Max, SUM True, False TopkPool (Gao & Ji, 2019), SAGPool (Lee et al., 2019), PANPool (Ma et al., 2020), EdgePool (Diehl, 2019) [2, 4, 6]\n\nSpecifically, The STACK design choice means directly stacking multiple GNN layers, i.e., without skip-connections. We also support node pooling operations for graph classification tasks, where the pooling loop stands for the number of message passing layers between each pooling operation. If the number of message passing layers is m and the node pooling loop is l, there will be a node pooling layer after the ith message passing layer in the design model (hierarchical pooling), where i ∈ {1 + k · l | k = 0, . . . , ⌈(m − 1)/l⌉ − 1}. Moreover, to avoid duplicated and invalid designs, some design variables are required to satisfy specific dependency rules, and we take two examples to elaborate on this point.\n\n• If the node pooling flag of a design is False, then the design does not have any value on node\n\npooling type and node pooling loop, and vice versa.\n\nFor example, we denote node pooling flag as f , node pooling type as t, and ∗ as any design choice, then(f =False, t=∗) or (f =False, l=*) will both be invalid.\n\n• The node pooling loop should not exceed the number of message passing layers.\n\nFor example, design A (m=4, l=4) and design B (m=4, l=6) that take the same values on other design variables are duplicated.\n\nThus, the design graph constructed under dependency rules is more complex. Without loss of generality, we define that the distance of (f =False) and (f =True, l=MIN({i ∈ L})) as 1, where L represents the design choice of node pooling loop. Thus, the design graph is a connected graph that enables the exploration of any node with random initialization. It is also worth mentioning that the search strategy of FALCON is modularized given the design graph. In contrast, the dependency rules constrain the action space of reinforcement learning methods, e.g., (f =Ture → False) is inapplicable, which requires special operation inside the controller.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Design space for the one-shot baselines on node and graph classification tasks.\n\nVariable\n\nDropout ratio Layer connectivity # Pre-process layers # Message passing layers # Post-precess layers Activation Batch norm Aggregation\n\nCandidate Values\n\n[0.0, 0.3, 0.6] STACK, SUM [1, 2, 3] [2, 4, 6, 8] [1, 2, 3] ReLU, Swish, Prelu True, False Mean, Max, SUM\n\nTable 6: Statistics and the construction time of the design graphs.\n\n#Nodes\n\n#Edges (Undirected) Ave. Degree Diameter\n\nconstruction time (s)\n\nDG-1 DG-2\n\n5,832 58,320\n\n78.732 1,070,172\n\n13.5 18.4\n\n13 17\n\n13 147\n\nWe further summarize the statistics and construction time of the design graphs in Table 6, where DG-1 and DG-2 denote the design graphs for node-level and graph-level tasks, respectively. We use multi-processing programing on 50 CPUs (Intel Xeon Gold 5118 CPU @ 2.30GHz) to conduct the graph construction. Note that we don’t have to construct the entire design graph in the pre-processing step, since we only extend the small portion of the design graph, i.e., , the design subgraph, during the search process. Thus, the total time cost of constructing the design subgraph will be O(E′) where E′ is the number of edges in the design subgraph, which largely lowers the time costs.\n\nA.2.2 DESIGN SPACES FOR THE ONE-SHOT BASELINES\n\nThe one-shot models (Liu et al., 2019; Pham et al., 2018) is built upon a super-model that is required to contain all of the architecture choices. We build the macro search space over entire models for both node classification and graph classification datasets with constraints. Firstly, we do not consider CAT (skip-concatenate) a layer connectivity choice, and we also remove design variables for node pooling. The reason is that CAT and node pooling operations change the input shape and make the output embeddings inapplicable for the subsequent weight-sharing modules in our settings. Secondly, the layer connectivity is customized for each layer following the previous works (Liu et al., 2019; Pham et al., 2018), instead of setting as a global value for every layer. Overall, we summarize the design space in Table 7.\n\nTo enable a fair comparison, we fine-tune the hyper-parameters and report the best results of the architectures found by the one-shot methods according to their performance in the validation sets.\n\nB MORE EXPERIMENTAL RESULTS ON GRAPH TASKS\n\nB.1 GRAPH CLASSIFICATION TASKS\n\nTask performance. Here we provide more results of task performance on the graph classification dataset. We repeat each experiment at least 3 times and report the average performances and the standard errors. The results are summarized in Table 8. The results well demonstrate the preeminence of FALCON in searching for good designs under different data distributions.\n\nSearch cost. As shown in Figure 5, we report the search cost of Random, DARTS, ENAS, GraphNAS, and FALCON on the selected datasets. The time measurements are conducted on a single NVIDIA GeForce 3070 GPU (24G). We see FALCON has a comparable time cost with Random and DARTS, which empirically proves the efficiency of FALCON.\n\nHowever, as FALCON still needs to sample designs and train them from scratch (i.e., the search cost of FALCON is bounded by the search cost of Random), the computational cost is relatively high in\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Test performance (ROC-AUC) on the graph classification datasets.\n\nDHFR-MD COX2-MD Mutagenicity MOLT-4 NCI-H23\n\nPTC-MR P388\n\nRandom BO SA ENAS DARTS GraphNAS AutoAttend GASSO\n\nFALCON-G FALCON-LP FALCON\n\n59.0±5.2 55.1±0.0 56.0±7.1 53.5±3.7 55.8±6.3 61.6±4.3 63.3±0.9 60.9±2.3\n\n58.5±8.8 61.4±1.5 63.6±7.9\n\n(cont.) PTC-MM\n\nPC-3\n\n52.5±5.6 60.1±1.5 58.1±4.6 52.4±2.9 52.6±4.1 54.8±3.9 64.7±1.2 63.2±0.7\n\n55.2±2.4 56.8±6.1 57.1±0.1\n\n60.4±0.0 59.7±0.1 69.0±2.0 62.2±1.1 61.6±0.5 68.6±2.6 66.2±0.3 66.0±1.8\n\n65.1±2.8 68.1±0.3 71.0±1.7\n\n63.2±1.9 71.6±5.5 67.4±3.1 57.9±1.7 70.4±3.2 63.9±2.5 68.8±0.7 68.5±2.0\n\n67.3±3.6 66.8 ±3.6 67.3±3.2\n\nSF-295\n\n55.3±0.5 60.6±0.3 55.3±0.6 60.9±2.2 62.2±1.1 65.0±3.1 64.2±0.9 65.8±0.4\n\n64.3±2.1 63.5±3.4 64.4±0.4\n\n77.1±1.9 78.3±0.7 81.1±0.3 75.0±1.6 74.4±0.7 80.2±1.5 79.6±0.1 75.1±0.5\n\n79.8±1.7 80.2±0.8 81.1±0.5\n\n58.6±0.7 58.1±2.0 54.8±1.1 61.5±0.1 61.4±0.7 62.1±1.0 59.5±0.2 57.4±0.9\n\n62.8±3.3 58.5 ±8.8 64.4±4.0\n\n58.4±2.1 63.6±0.0 56.7±3.2 61.2±1.1 63.5±1.9 62.4±3.9 61.8±0.2 64.7±1.5\n\n62.2±1.2 63.7±4.1 66.6±3.3\n\n59.9±5.7 58.8±7.7 59.4±6.2 59.8±1.6 59.3±0.5 58.6±6.7 57.8±0.6 51.9±5.2\n\n55.6±6.9 55.6±2.0 60.0±1.4\n\n63.9±0.7 68.9±0.0 74.4±1.0 68.3±1.2 70.8±0.7 68.2±2.5 74.9±0.5 71.3±1.4\n\n74.2±3.7 75.1±1.1 77.0±1.4\n\nNCI1\n\nSW-620 UACC257\n\nYeast\n\nAvg.\n\n77.5±0.3 77.3±0.0 79.6±5.3 77.2±1.2 66.3±3.0 78.1±3.6 79.3±1.6 76.6±0.6\n\n80.6±0.8 80.4±0.5 80.9±0.8\n\n57.5±2.7 63.8±0.9 58.2±2.1 64.8±2.2 66.0±0.3 61.8±4.4 65.5±0.4 64.7±1.1\n\n62.0±3.0 65.2±1.7 66.6±2.1\n\n61.1±0.5 60.8±0.0 64.2±0.1 64.7±0.5 65.7±0.2 61.5±5.1 57.1±0.5 62.7±0.2\n\n61.0±3.8 54.5±1.1 66.7±2.1\n\n53.3±0.0 55.0±0.0 53.8±1.2 63.0±1.1 61.4±1.4 57.2±1.1 59.6±0.8 60.1±0.4\n\n57.5±1.5 56.6±1.7 58.1±1.7\n\n61.3 63.7 63.4 63.0 63.7 64.6 65.9 64.9\n\n64.7 64.7 67.5∗\n\nFigure 5: Search cost on the selected datasets.\n\nlarge datasets, e.g., OVCAR-8 and MCF-7. We can potentially alleviate this limitation via integrating dataset sampling to reduce time costs.\n\nROC-AUC v.s. exploration size. Here we report the change in task performance on graph classification datasets with the number of explored nodes. In Figure 6, we visualize the results on two graph classification datasets. We see that FALCON can approach the best-performing designs quickly as the explored size grows.\n\nB.2 BEST DESIGNS\n\nIn Table 9 and Table 10, we summarize the best designs found by FALCON and BRUTEFORCE in each dataset, where the average number of parameters is 137.5k for all the graph classification datasets. Note that we select the best designs according to their performance on validation sets; thus, there are cases where FALCON surpasses BRUTEFORCE. We highlight the design variables that are different between FALCON and BRUTEFORCE for comparison.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Accuracy v.s. the number of explored nodes on two graph classification datasets.\n\n(a)\n\n(b)\n\nTable 9: Average parameters & Best designs in the node classification datasets.\n\nDataset\n\nogbn-arxiv\n\nCora\n\nCiteseer\n\nPubmed\n\nAverage Param (k)\n\n44.5\n\n77.8\n\n289.0\n\n57.8\n\nAmazonComputers\n\n46.2\n\nReddit\n\n47.2\n\nBest design\n\nTest performance (%)\n\nFALCON BRUTEFORCE\n\n(0.0, 1, 4, 2, STACK, Swish, True, Mean) (0.3, 1, 4, 2, SUM, Relu, True, Mean)\n\nFALCON BRUTEFORCE\n\n(0.0, 1, 6, 1, SUM, Swish, False, Mean) (0.0, 1, 4, 1, STACK, Swish, False, Mean)\n\nFALCON BRUTEFORCE\n\n(0.3, 1, 2, 1, SUM, Prelu, True, Mean) (0.3, 1, 2, 2, SUM, Prelu, False, Mean)\n\nFALCON BRUTEFORCE\n\n(0.3, 1, 8, 1, SUM, Relu, True, Add) (0.3, 1, 8, 1, SUM, Relu, True, Add)\n\nFALCON BRUTEFORCE\n\n(0.0, 1, 4, 1, STACK, Swish, True, Mean) (0.0, 3, 4, 2, STACK, Prelu, True, Mean)\n\nFALCON BRUTEFORCE\n\n(0.0, 3, 4, 2, STACK, Prelu, True, ARMAConv) (0.0, 3, 4, 2, STACK, Prelu, True, ARMAConv)\n\n70.36 70.51\n\n87.18 86.99\n\n76.19 75.99\n\n90.04 90.04\n\n91.64 91.35\n\n95.46 95.46\n\nB.3 BRUTEFORCE’S CONFIDENCE INTERVAL AND VARIANT\n\nTo estimate the uncertainty of Bruteforce, we compute the 95% confidence interval of Bruteforce using bootstrapping. Moreover, we consider a variant of Bruteforce baseline to compare with Bruteforce. Specifically, we train all the designs in the design space for 30 epochs, select the top 10% design, and resume the training until 50 epochs. After that, we choose the top 50% designs to be fully trained and return the best fully trained design based on the validation performance. We run Bruteforce-bootstrap on four datasets as demonstrations. We summarize the results in Table 11.\n\nTable 11: Test performances of Bruteforce and its variant.\n\nCora CiteSeer ER-MD AIDS\n\nBruteforce (max) Confidence interval length\n\nBruteforce-bootstrap\n\n87.0 0.2\n\n87.0\n\n76.0 0.1\n\n76.4\n\n83.3 0.6\n\n83.8\n\n96.0 0.0\n\n95.7\n\nSurprisingly, we found that the performance of Bruteforce and Bruteforce-bootstrap are very close. This indicates that Bruteforce (fully trained 5% design) is a good surrogate of Bruteforcebootstrapping (fully trained 5% design, but using bootstrapping selection), and could also well approximate the ground truth performance of the best design.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 10: Best designs in the graph classification datasets.\n\nBest design\n\nTest performance (%)\n\nDataset\n\nAIDS\n\nCOX2-MD\n\nDHFR-MD\n\nER-MD\n\nMCF-7\n\nMOLT-4\n\nFALCON BRUTEFORCE\n\n(0.3, 1, 4, 2, SUM, Prelu, True, Add, NoPool) (0.0, 3, 8, 2, STACK, Relu, True, Add, SAGPool, 2)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 6, 1, SUM, Swish, True, Add, EdgePool, 2) (0.0, 1, 4, 3, STACK, Prelu, True, Max, TopkPool, 2)\n\nFALCON BRUTEFORCE\n\n(0.3, 2, 4, 2, SUM, Swish, True, Mean, PANPool, 2) (0.0, 1, 4, 3, CAT, Prelu, True, Mean, SAGPool, 4)\n\nFALCON BRUTEFORCE\n\n(0.3, 3, 4, 2, CAT, Relu, True, Add, PANPool, 2) (0.0, 3, 5, 3, CAT, Prelu, True, Max, PANPool, 6)\n\nFALCON BRUTEFORCE\n\n(0.6, 1, 8, 2, CAT, Swish, True, Add, NoPool) (0.0, 1, 2, 6, SUM, Prelu, True, Add, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.0, 3, 6, 2, SUM, Prelu, True, Max, NoPool) (0.0, 1, 6, 1, SUM, Prelu, True, Add, NoPool)\n\nMutagenicity\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 6, 1, CAT, Relu, True, Add, NoPool) (0.0, 2, 4, 2, CAT, Relu, True, Add, PANPool, 2)\n\nNCI1\n\nNCI109\n\nNCI-H23\n\nOVCAR-8\n\nP388\n\nPC-3\n\nPTC-MM\n\nPTC-MR\n\nSF-295\n\nSN12C\n\nSW-620\n\nTox21-AhR\n\nUACC257\n\nYeast\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 4, 2, SUM, Swish, True, Max, EdgePool, 2) (0.0, 1, 4, 2, STACK, Prelu, True, Add, EdgePool, 4)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 8, 2, STACK, Prelu, True, Max, NoPool) (0.0, 3, 6, 2, STACK, Prelu, True, Add, EdgePool, 6)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 6, 1, CAT, Swish, True, Add, NoPool) (0.0, 2, 6, 1, CAT, Swish, True, Add, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.6, 2, 6, 2, CAT, Swish, True, Add, NoPool) (0.6, 1, 2, 3, CAT, Swish, True, Add, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 6, 2, CAT, Prelu, True, Add, NoPool) (0.0, 2, 6, 2, CAT, Prelu, True, Add, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 6, 1, SUM, Relu, True, Max, NoPool) (0.0, 2, 6, 1, SUM, Relu, True, Max, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 4, 2, STACK, Swish, True, Max, EdgePool, 2) (0.0, 3, 6, 1, SUM, Swish, True, Mean, EdgePool, 2)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 4, 2, SUM, Swish, True, Mean, TopkPool, 2) (0.3, 1, 6, 2, SUM, Relu, True, Add, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.6, 2, 6, 2, CAT, Swish, True, Add, NoPool) (0.0, 3, 8, 3, SUM, Prelu, True, Max, PANPool, 4)\n\nFALCON BRUTEFORCE\n\n(0.0, 1, 8, 1, CAT, Swish, True, Add, NoPool) (0.0, 1, 8, 3, CAT, Prelu, True, Add, EdgePool, 6)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 4, 2, STACK, Prelu, True, Max, NoPool) (0.0, 2, 4, 2, STACK, Prelu, True, Max, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.0, 2, 4, 2, SUM, Prelu, True, Add, EdgePool, 2) (0.0, 3, 6, 2, SUM, Prelu, True, Add, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.3, 2, 6, 2, CAT, Swish, True, Max, NoPool) (0.0, 3, 6, 1, SUM, Prelu, True, Max, NoPool)\n\nFALCON BRUTEFORCE\n\n(0.6, 1, 8, 1, CAT, Prelu, True, Add, NoPool) (0.0, 1, 2, 3, SUM, Swish, True, Add, EdgePool, 2)\n\n19\n\n99.02 95.97\n\n69.87 65.39\n\n71.33 56.22\n\n81.67 83.33\n\n67.85 70.61\n\n69.03 70.30\n\n81.73 81.17\n\n80.13 82.81\n\n79.98 81.77\n\n71.14 71.14\n\n68.21 67.40\n\n78.75 78.75\n\n73.28 73.28\n\n56.96 52.93\n\n60.56 63.06\n\n64.75 66.47\n\n73.34 73.73\n\n69.26 69.26\n\n79.10 82.02\n\n67.94 70.24\n\n59.60 60.41\n\nUnder review as a conference paper at ICLR 2023\n\nC EXPERIMENTAL RESULTS ON THE IMAGE TASK\n\nC.1 DATASET PRE-PROCESSING\n\nWe use the CIFAR-10 (Krizhevsky, 2009) image dataset to show FALCON can work well on other machine learning domains. This dataset consists of 50,000 training images and 10,000 test images. We randomly crop them to size 32 × 32, and conduct random flipping and normalization.\n\nC.2 DESIGN SPACES\n\nHere we use two different design spaces to demonstrate FALCON’s ability in searching for both hyper-parameters and architectures on image dataset.\n\nHyper-parameter Design Space. We consider a broad space of hyper-parameter search, including common hyper-parameters like Batch Size. We train each design using a SGD optimizer, which requires weight decay and momentum as hyper-parameters. We also use a learning rate (LR) scheduler, which reduces the learning rate when validation performance has stopped improving. Specifically, the scheduler will reduce the learning rate by a factor if no improvement is seen for a ‘patience’ number of epochs. It is also worth mentioning that FALCON is flexible for other sets of hyper-parameter choices determined by the user end.\n\nTable 12: Hyper-parameter design space for image tasks.\n\nVariable\n\nCandidate Values\n\nMomentum (SGD) Weight decay Batch size LR decay factor LR decay patience\n\n[0.5, 0.9, 0.99] [1e-4, 5e-4, 1e-3, 5e-3] [32, 64, 128, 256] [0.1, 0.2, 0.5, 0.8] [3, 5, 10]\n\nArchitecture Design Space. We construct micro design space for the computational cells. Each cell constitutes of two branches, where we enable five selections: separable convolution with kernel size 3 × 3 and 5 × 5, average pooling and max pooling with kernel size 3×3, and identity. In each branch, we have one dropout layer, where the dropout ratio is one of {0.0, 0.3, 0.6}, and one batch normalization layer. We also use one of identity and skip-sum as skip-connection within each branch. After the input data is separately computed in each branch, the outputs are added as the final cell output, which is different with the original ENAS paper that searches the computational DAG on the defined nodes.\n\nC.3 EXPERIMENTAL RESULTS\n\nHere we set the exploration size as 20 for all the sample-based methods. For hyper-parameter design space, we compare FALCON with the baselines that are available for hyper-parameter tuning. Moreover, to accelerate the search process, FALCON explores an unknown design by fine-tuning a pretrained model for several epochs based on the selected hyper-parameters, instead of training each candidate design from scratch. The results are summarized in Table 13.\n\nTable 13: Search results on the hyper-parameter design space for CIFAR-10 dataset.\n\nAverage Error (%)\n\nRandom BO SA\n\nFALCON\n\n4.13 4.95 4.04\n\n3.80\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 7: Sensitivity of FALCON’s hyper-parameters.\n\nFor the architecture design space. For ENAS and DARTS, the learning rate is 0.01, and the maximum training epoch is 300. We repeat each experiment three times and summarize the results in Table 14.\n\nTable 14: Search results on the architecture design space for CIFAR-10 dataset.\n\nAverage Error (%)\n\nSearch Cost (GPU days)\n\nRandom BO SA\n\nENAS-micro DARTS-micro\n\nFALCON\n\n10.43 9.83 10.16\n\n9.20 8.97\n\n8.87\n\n0.64 0.67 0.62\n\n0.77 0.89\n\n0.81\n\nD SENSITIVITY ANALYSIS\n\nWe analyze the sensitivity of the hyper-parameters in the search strategy of FALCON, using a node classification task, CiteSeer, and a graph classification task, Tox21-AhR. Specifically, we study the influence of the number of random starting nodes and the candidate scale resulting from an explored node (i.e., how many hop neighbors of an explored node are to be included in the candidate set).\n\nAs shown in Figure 7, we find that FALCON outperforms the best AutoML baselines with a large range of hyper-parameters. Specifically, 73% and 97% hyper-parameter combinations of FALCON rank best among the baselines in CiteSeer and Tox21-AhR, respectively.\n\nMoreover, we discover an interesting insight about the size of receptive field, i.e., the number of design candidates, during the search process of FALCON. According to the construction of design subgraph, the receptive field size is O(rhd), where r is the number of random start nodes, h is the number of neighbor hops, and d is the average node degree. We find that the performance of design searched by FALCON increases with the receptive field’s size until it reaches a certain scale.\n\nSuch patterns have been widely observed in multiple datasets. While the receptive field on the design subgraph should contain sufficient candidates for sampling good ones, it should also prune inferior design space, which doesn’t provide insights on navigating the best-performing designs. Thus, the size of the receptive field may be a crucial factor influencing the search quality of FALCON.\n\n21",
  "translations": [
    "# Summary Of The Paper\n\nIn this paper, a new method of sample-based NAS is proposed, Falcon. The key idea is to represent the model design space as a graph, where nodes correspond to separate designs and an edge corresponds to a minimal change in one of the design parameters. The problem of architecture search is thus translated to a black-box optimization task over the design graph. Falcon builds a GNN meta-model, which approximates the values in the nodes and use it in a search strategy.\n\n# Strength And Weaknesses\n\n+ The idea of representing the design space as a graph looks novel for NAS\n\n- Related work seems incomplete. In particular, the claim “Previous works generally consider each design choice as isolated from other designs” seems too strong. I think, there should be an overview of previous papers related to meta-models over design spaces. Apart from NER, there should be also literature on design spaces in the context of classical ML. In this context, I am not convinced that the scientific problem is well-formulated. When we look at the empirical setting of the work, we see that the design parameters enable a simpler approach without any design graphs: each parameter can be quantified as a separate dimension of a joint design space, and the problem becomes a black-box optimization on the obtained grid.\n- The motivation behind the proposed Falcon is questionable. Why not using Bayesian approaches? Gaussian processes?\n- Some parts of the paper lacks of details. In particular, GNN meta-model and its training pipeline is not fully described\n- The pipeline of the empirical part is not reproducible based solely on the text of the paper. The whole system is complex, it has many hyperparameters and itself design choices. This choice is not transparent, and I cannot be sure that the obtained empirical improvements are not an attribute of these choices.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe novelty of the main idea seems questionable. When we look at the empirical setting of the work, we see that the design parameters enable a simple approach without any design graphs: each parameter can be quantified as a separate dimension of a joint design space, and the problem becomes a black-box optimization on the obtained grid. Why this scenario is not addressed in the paper?\n\nSome parts of the paper lacks of details. In particular, GNN meta-model and its training pipeline is not fully described The pipeline of the empirical part is not reproducible based solely on the text of the paper. The whole system is complex, it has many hyperparameters and itself design choices. This choice is not transparent, and I cannot be sure that the obtained empirical improvements are not an attribute of these choices.\n\n# Summary Of The Review\n\nI think, this paper does not appropriately place itself in the context of state-of-the-art and classical algorithms relevant to the problem, and needs a major revision.\n\n# Correctness\n\n1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces FALCON, a sample-based AutoML method designed specifically for graph learning tasks, such as node and graph classification. It addresses the computational inefficiencies and lack of insights associated with conventional AutoML methods by modeling the design space as a design graph, where nodes represent design choices and edges signify design similarities. FALCON incorporates a meta-Graph Neural Network (GNN) that utilizes both task-agnostic and task-specific modules to predict the performance of design candidates efficiently. The experiments demonstrate that FALCON outperforms several baseline methods and AutoML approaches, achieving an average improvement of 3.3% in graph classification tasks while requiring only 30 explored designs.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative use of a design graph to capture the relationships between different design choices, which provides a structured approach to navigating the design space. The performance of FALCON, particularly its efficiency in exploring designs and its superior results on benchmark datasets, adds significant value to the AutoML landscape. However, the paper could benefit from a more detailed discussion on the limitations of the design graph, such as potential scalability issues with high node degrees and the adaptability of the approach to continuous design variables. Furthermore, while the results are promising, more extensive comparative analyses with a wider range of AutoML methods could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and generally clear, with a logical flow from introduction to conclusion. The methodology is described in sufficient detail for reproducibility, as the authors have shared datasets and code. The novelty of the approach is noteworthy, particularly the integration of a GNN within the AutoML framework. However, there are areas where additional clarity could enhance understanding, particularly in explaining the intricacies of the design graph and the performance prediction mechanisms employed by the meta-GNN.\n\n# Summary Of The Review\nOverall, FALCON presents a significant advancement in the field of AutoML for graph learning tasks, with innovative methodologies and strong empirical results. While the paper is well-structured and reproducible, it could benefit from addressing certain limitations and providing more extensive comparative analyses.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces FALCON, an innovative sample-based AutoML method for graph tasks that utilizes a design graph to model relationships between design choices. FALCON incorporates a meta-graph neural network (Meta-GNN) with task-agnostic and task-specific modules to enhance sample efficiency in model search. Extensive experiments across 27 node and graph classification tasks demonstrate that FALCON surpasses existing AutoML methods in performance, achieving an average improvement of 3.3% with significantly fewer explored designs while maintaining comparable time costs.\n\n# Strength And Weaknesses\nFALCON's strengths lie in its innovative approach to modeling design relations via a design graph, which enhances the search process and improves sample efficiency. The extensive evaluation across diverse datasets highlights the robustness and versatility of the framework. However, the paper does exhibit limitations such as the complexity involved in graph construction, potential issues with generalization beyond tested domains, and concerns regarding overfitting due to a limited number of explored designs in the early search stages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents a clear and comprehensive explanation of the methodology, including the design of the meta-GNN and the experimental setup. The novelty of using a design graph in AutoML is significant, contributing to its overall impact in the field. The reproducibility of the results appears feasible given the detailed descriptions of methodologies and datasets, though the complexity of graph construction may pose challenges.\n\n# Summary Of The Review\nFALCON presents a compelling advancement in automated graph learning by effectively leveraging design relations to improve sample efficiency in model selection. While it demonstrates strong empirical performance across various datasets, there are concerns regarding the complexity of implementation and generalization to other domains.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Efficient Automatic Graph Learning via Design Relations\" presents FALCON, an innovative sample-based AutoML framework designed to optimize model selection in graph tasks. It introduces a Design Graph that captures design relations, enabling efficient exploration of the design space. The method employs a Meta-GNN for performance prediction, combining task-agnostic and task-specific modules to enhance model selection. The experimental results demonstrate that FALCON achieves significant improvements over existing AutoML baselines, achieving a 3.3% increase in performance on graph classification tasks while utilizing only 30 explored nodes.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel approach to modeling the design space through a Design Graph, which effectively reduces the search space and leverages design relations for better performance. The use of a Meta-GNN for performance prediction is well-conceived, allowing for both task-agnostic and task-specific insights. However, a notable weakness is the potential issue of over-smoothing due to a high average node degree in the design graph, which could affect the model's performance. Furthermore, while the results are promising, the paper could benefit from a more extensive discussion on the limitations and potential biases in the model's performance across different demographics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections detailing the methodology, experiments, and results. The quality of writing is high, with technical details adequately explained for the intended audience. The novelty of the approach, particularly the introduction of the Design Graph and Meta-GNN, is significant in the context of AutoML for graph tasks. The reproducibility of the results is supported by the availability of public datasets and detailed experimental settings, though further emphasis on the discussion around the ethical implications of model performance would strengthen the paper.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of AutoML by proposing FALCON, which efficiently models design relations in graph tasks. While it shows strong empirical results and introduces valuable methodological innovations, the paper could enhance its discussion of limitations and ethical considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the FALCON framework, which stands for \"Efficient Automatic Graph Learning via Design Relations.\" It introduces a novel approach to automate the design of machine learning models by employing a design graph that captures the relationships between various design choices. The methodology leverages sample efficiency to achieve competitive performance with fewer explored designs compared to existing methods. Extensive experimental validation across multiple tasks showcases FALCON's adaptability, particularly in graph and node classification tasks, while also hinting at its broader applicability to other domains.\n\n# Strength And Weaknesses\nThe paper exhibits several strengths, including a high sample efficiency that enables FALCON to perform competitively with fewer samples, and the innovative use of a design graph that adds depth to traditional AutoML strategies. The comprehensive experimental validation strengthens the credibility of the findings, demonstrating adaptability across multiple datasets. However, the paper also has notable limitations. The computational cost associated with training models from scratch can be high, especially for larger datasets. Additionally, while the inductive biases incorporated into the framework enhance performance predictions, they may risk overfitting to specific design patterns. The complexity of managing design variables and the need for clearer practical implementation guidance can also hinder the framework's usability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly describes the FALCON framework, including its architecture and search strategy, which facilitates understanding. However, it lacks detailed guidance on hyper-parameter tuning and practical implementation, which could affect reproducibility. The novelty of the design graph contributes significantly to the field of AutoML, though the degree of significance in the broader context requires further validation.\n\n# Summary Of The Review\nOverall, the paper provides a compelling contribution to the field of AutoML through the introduction of the FALCON framework, which emphasizes sample efficiency and the use of design relations. While there are strengths in its methodology and experimental validation, the practical applicability and potential for overfitting present areas for improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FALCON, a novel framework aimed at enhancing the sample efficiency of automated machine learning (AutoML) specifically for graph-related tasks. The methodology centers around the construction of a design graph that captures relationships among various model architectures and hyperparameters, allowing for more informed exploration of the design space. FALCON integrates a task-agnostic and a task-specific module to predict model performance, demonstrating significant improvements in sample efficiency, achieving state-of-the-art results with fewer explored designs across multiple graph datasets and CIFAR-10.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to modeling design relationships through a structured graph, which contrasts sharply with traditional methods that treat design variables independently. The use of Graph Neural Networks (GNNs) for performance predictions is a noteworthy contribution that aligns with current trends in graph-based learning. Experimental results are robust, showcasing FALCON's performance improvements over existing methods while maintaining computational efficiency. However, the paper could delve deeper into the limitations of the design graph approach and provide a more comprehensive analysis of hyperparameter sensitivity, which could affect the reliability of its findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making it accessible to a wide audience. The quality of writing is high, with a well-defined methodology that supports reproducibility. The novel use of GNNs and the focus on relational dynamics in design choices contribute to the paper's originality. However, a more detailed examination of the experimental setup and hyperparameter effects would enhance reproducibility and understanding of the model’s robustness.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of AutoML for graph tasks through its innovative design framework and strong empirical results. While it effectively communicates its contributions, additional insights into limitations and hyperparameter sensitivity would strengthen the overall narrative.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FALCON, a novel framework designed to enhance adversarial training in neural networks. Its main contributions include the development of an adversarial design graph that models relationships between training strategies, the use of a meta-Graph Neural Network (GNN) for performance prediction, and a focus on sample efficiency that allows for robust training with fewer resources. The experimental results demonstrate that FALCON significantly improves robustness against adversarial attacks while maintaining computational efficiency compared to existing state-of-the-art methods.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to modeling adversarial training strategies and the successful implementation of a meta-GNN for performance prediction, which could facilitate more informed and efficient training processes. The extensive experimental validation against various adversarial scenarios adds credibility to the findings. However, the paper could benefit from addressing potential scalability issues related to the complexity of the design graph as the number of configurations increases. Additionally, while the results are promising, further exploration of the framework's adaptability to more complex attack models is warranted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, providing a clear explanation of the methodology and contributions. The quality of the experiments is high, with comprehensive evaluations that demonstrate the efficacy of the proposed framework. The novelty is significant, as it combines relational inductive biases with adversarial training in a unique way. However, the reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameters used during training.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of adversarial training through the introduction of the FALCON framework. Its focus on sample efficiency and the innovative use of design graphs and meta-GNNs offer valuable contributions to the literature, although some scalability concerns and the potential for further extensions remain.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces FALCON, a novel AutoML framework tailored for graph tasks. It proposes a design graph to represent the search space for model designs, claiming a significant reduction in computation costs associated with this search. The authors assert that FALCON demonstrates exceptional sample efficiency, requiring only 30 explored nodes to identify optimal designs across 27 tasks. Additionally, it reports an average performance improvement of 3.3% over leading baselines and integrates a Graph Neural Network (GNN) to predict design performance, which the authors suggest will set new standards in the field.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to modeling the design space through a design graph and its claims of high sample efficiency, which could potentially streamline the AutoML process for graph-related tasks. However, the weaknesses are significant; the authors exaggerate the implications of their findings and overlook established benchmarks in their comparisons. Furthermore, the claims regarding performance improvements and the necessity of FALCON could mislead the audience about its actual impact and the current state of research in AutoML for graphs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, making it accessible to readers familiar with AutoML and graph neural networks. However, the clarity is undermined by the hyperbolic claims regarding the significance of FALCON, which may lead to confusion about its true advancements. While the novelty of representing the design space as a graph is commendable, the lack of thorough comparisons with established baselines raises questions about reproducibility and the validity of the reported performance metrics.\n\n# Summary Of The Review\nIn summary, while the paper presents an interesting framework for AutoML in graph tasks, it greatly exaggerates its contributions and implications. The findings, though potentially valuable, are not sufficiently substantiated by rigorous comparisons to existing methods, leading to an inflated perception of the framework's impact.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces FALCON, a novel sample-based AutoML method tailored for graph tasks. It proposes a design graph that models the relationships between design choices, using nodes to represent these choices and edges to signify their similarities. FALCON comprises a task-agnostic module leveraging a Graph Neural Network (GNN) and a task-specific module for label propagation. Through extensive experimentation on 27 node and graph classification tasks, the authors demonstrate that FALCON achieves an average performance improvement of 5.1% over existing baselines while maintaining competitive computational costs, exploring only 20 designs.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to modeling the design space through a design graph, which enhances sample efficiency and navigation in the search process. The empirical results are robust, showcasing FALCON's ability to outperform established methods while requiring fewer explored designs. However, a notable weakness is the limited exploration of the design graph's adaptability to continuous design variables, which could be critical in broader applications. Additionally, while the performance improvements are significant, the paper could benefit from a more comprehensive analysis of the hyperparameters' influence on outcomes.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, allowing for a good understanding of the proposed approach and its contributions. The experimental results are detailed, providing sufficient evidence of the claims made. However, while the paper mentions the efficiency of the design graph's construction, it lacks a thorough discussion of reproducibility, particularly regarding the specific implementations of the GNN and label propagation modules. Overall, the quality of writing is high, but a clearer roadmap for reproducing the results would enhance its impact.\n\n# Summary Of The Review\nFALCON presents a compelling advancement in sample-based AutoML for graph tasks, demonstrating significant improvements in performance and efficiency through its innovative design graph approach. While the contributions are noteworthy, the paper would benefit from addressing the adaptability of the design graph and providing more detailed reproducibility guidelines.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents FALCON, a novel framework for hyperparameter tuning in AutoML that models the design space as a graph, focusing on leveraging inductive biases from design relations to improve search efficiency. The methodology separates task-agnostic and task-specific modules, claiming that this separation enhances performance and sample efficiency. Experimental results demonstrate that FALCON achieves state-of-the-art sample efficiency on specific graph tasks and a single image dataset, though the generalizability of these findings across diverse domains remains untested.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the innovative approach of modeling the design space as a graph and the introduction of task-agnostic and task-specific modules, which could potentially streamline the search process in AutoML. However, several weaknesses are noted, such as the oversimplification of design interactions through graph representation and the lack of comprehensive empirical validation across a broader range of tasks. The claims around sample efficiency and performance improvements are not sufficiently backed by rigorous statistical evidence, raising questions about their significance in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the core concepts are presented, critical assumptions and limitations are not thoroughly discussed, which may lead to misunderstandings regarding the applicability of the proposed methods. The quality of the experiments is limited by the narrow range of tasks and datasets used, making it difficult to assess the generalizability of the findings. The novelty is evident in the approach, but the reproducibility is questionable due to reliance on specific experimental conditions that may not be easily replicated across different environments.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting and potentially valuable framework for hyperparameter tuning within AutoML, but it suffers from significant limitations regarding empirical validation and generalizability. The assumptions made about design space interactions and evaluation metrics require further scrutiny to ensure the robustness of the findings.\n\n# Correctness\nRating: 3/5. The methodology is sound, but the assumptions and claims made lack comprehensive validation.\n\n# Technical Novelty And Significance\nRating: 4/5. The approach of modeling design choices as a graph and separating task-agnostic and task-specific modules is a novel contribution to the field.\n\n# Empirical Novelty And Significance\nRating: 3/5. While the empirical results show improvements, the limited scope of tasks and datasets presents challenges in assessing the broader significance of the findings.",
    "# Summary Of The Paper\nThe paper presents FALCON, a novel AutoML method specifically designed for graph tasks that addresses computational inefficiencies prevalent in existing approaches. FALCON uniquely models the design space as a design graph, where nodes represent candidate architectures and edges represent design similarities. By employing a meta-graph neural network (GNN) that integrates both task-agnostic and task-specific modules, FALCON predicts design performance more efficiently. The results from extensive experiments across 27 graph datasets and one image dataset demonstrate that FALCON achieves superior sample efficiency and overall performance compared to existing AutoML methods while leveraging relational information among designs.\n\n# Strength And Weaknesses\nThe main strength of FALCON lies in its innovative approach to modeling the design space as a graph, which enhances the understanding of design relations and improves sample efficiency. The integration of task-agnostic and task-specific components in the meta-GNN is particularly commendable, as it enables more informed predictions. However, the paper acknowledges some limitations, such as challenges related to over-smoothing in high-degree graphs, which may affect the performance of the model in specific scenarios. Additionally, while the results are promising, the reliance on 27 graph datasets may limit the generalizability of findings across other types of tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, contributions, and findings. The quality of the writing is high, making it accessible to a broad audience. The novelty of the approach is significant, as it introduces a new way to conceptualize design space in AutoML for graph tasks. The authors have taken steps to ensure reproducibility by providing public access to datasets and code, along with detailed experimental settings, which enhances the credibility of their results.\n\n# Summary Of The Review\nOverall, FALCON presents a compelling and innovative approach to AutoML for graph tasks, with its graph-based design space modeling and effective performance prediction methods. While the methodology shows promise, further exploration into its limitations and applicability to diverse tasks is recommended.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at improving the efficiency of automated machine learning (AutoML) processes. The authors introduce a hybrid approach that combines reinforcement learning with meta-learning techniques to optimize hyperparameter tuning and model selection. Through extensive experiments on various benchmark datasets, the findings indicate that the proposed method significantly outperforms existing AutoML systems, achieving better predictive performance while reducing computational costs.\n\n# Strengths And Weaknesses\n### Strengths\n1. **Innovation**: The integration of reinforcement learning with meta-learning represents a fresh perspective on the AutoML challenge, potentially advancing the state-of-the-art in automated hyperparameter tuning.\n2. **Theoretical Contribution**: The paper offers a robust theoretical framework that elucidates the interplay between the two learning paradigms, providing valuable insights into their combined effects on model performance.\n3. **Comprehensive Evaluation**: The authors conduct a thorough evaluation across multiple datasets, demonstrating the versatility and robustness of the proposed approach compared to several baseline methods.\n4. **Clarity of Presentation**: The paper is well-organized, with clear explanations of the methodology and results, making it accessible to a wide audience, including practitioners and researchers.\n\n### Weaknesses\n1. **Lack of Detail**: Certain methodological aspects, particularly the specific algorithms used for reinforcement learning, could benefit from more extensive detail to enhance reproducibility.\n2. **Comparative Analysis**: While comparisons are made with existing methods, a deeper analysis of the trade-offs between the proposed approach and traditional methods would provide more context for its advantages.\n3. **Scalability Concerns**: There are discussions regarding the scalability of the proposed framework in real-world scenarios, particularly concerning computational efficiency when applied to large datasets.\n4. **Potential for Overfitting**: Given the reliance on specific datasets for training, there is a risk of overfitting, which is not adequately addressed in the evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, facilitating understanding of complex concepts. The methodology is presented clearly, though additional details on algorithmic implementations would enhance reproducibility. The novelty of combining reinforcement learning with meta-learning offers significant contributions to the field of AutoML, but the empirical results would benefit from further validation across diverse tasks.\n\n# Summary Of The Review\nThis paper makes a meaningful contribution to the AutoML domain by proposing a novel hybrid approach that leverages reinforcement learning and meta-learning. While the methodology is innovative and the experimental results are promising, further detail on implementation and a more comprehensive comparative analysis would strengthen the overall impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents FALCON, a novel approach to automatic graph learning that addresses the inefficiencies of existing sample-based methods in the AutoML domain, particularly for graph tasks. FALCON models the design space as a \"design graph,\" where nodes represent design choices and edges denote design similarities. The methodology includes a task-agnostic module utilizing a Graph Neural Network (GNN) for message passing, and a task-specific module employing label propagation to share performance information. The empirical results indicate that FALCON achieves improved sample efficiency and performance compared to existing baselines, requiring fewer explored nodes to reach better results.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative modeling of the design space through a design graph that captures relational insights among design choices, which enhances exploration efficiency. The integration of GNNs and label propagation is particularly effective in disseminating performance information across the design graph. However, a notable weakness is the paper's acknowledgment of the high average node degree in the design graph, which may complicate scalability and adaptation for continuous design variables. The paper could benefit from a more thorough exploration of these challenges and potential solutions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions and methodology. The descriptions of the task-agnostic and task-specific modules are concise and comprehensible. The novelty of the proposed method is significant, as it introduces a new way to structure the design space in AutoML for graph tasks. The reproducibility of the results is supported by the empirical findings, although additional details regarding implementation and experimental setup would enhance clarity.\n\n# Summary Of The Review\nFALCON offers a promising advancement in the field of AutoML for graph tasks by efficiently modeling the design space and improving sample efficiency. While the approach demonstrates significant empirical benefits, the paper could address scalability challenges more thoroughly. Overall, FALCON represents a notable contribution to the literature.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FALCON, an innovative sample-based method for automated machine learning (AutoML) focused on graph tasks. Its main contributions include the development of a design graph that models the relationships between different model designs and their performance, and the integration of task-agnostic and task-specific modules utilizing Graph Neural Networks (GNNs) for enhanced performance prediction. The authors report substantial improvements in both efficiency and performance through extensive experiments on 27 graph classification tasks and an image classification task (CIFAR-10), demonstrating an average performance gain of 3.3% over existing AutoML methods while exploring fewer designs.\n\n# Strength And Weaknesses\nStrengths of the paper include the novel approach of using a design graph to encapsulate design relationships, which is a significant step towards improving sample efficiency in AutoML for graph tasks. The use of a meta-GNN to leverage both task-agnostic and task-specific insights is a robust methodological choice that enhances predictive accuracy. However, one weakness is the potential limitation posed by high average node degree in design graphs, which could complicate the scalability of the approach. While the empirical results are promising, further exploration of continuous design variables could enhance the method's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, contributions, and findings, making it accessible to readers familiar with the field. The quality of the writing is high, and the use of figures and tables aids in conveying complex concepts effectively. The novelty of the approach is notable, as it bridges the gap between GNNs and AutoML in a meaningful way. The authors provide detailed experimental setups, dataset information, and code access, supporting the reproducibility of their results, which is commendable.\n\n# Summary Of The Review\nOverall, this paper presents a compelling contribution to the field of AutoML for graph tasks by introducing FALCON, a method that effectively enhances sample efficiency and model performance. While the approach is innovative and the empirical results are strong, there are areas for future improvement, particularly regarding the scalability of the design graph.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents FALCON, an innovative approach to Automatic Machine Learning (AutoML) for graph tasks, which addresses the computational inefficiency and sample inefficiency of existing methods. FALCON introduces a design graph that models the relationships among various design choices, and employs a meta-graph neural network (Meta-GNN) with task-agnostic and task-specific modules to predict performance. Extensive experiments conducted on 27 node and graph classification tasks demonstrate that FALCON outperforms baseline methods in both efficiency and effectiveness, while also showcasing its applicability to other domains such as image classification.\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear identification of significant challenges in current AutoML methods and the novel introduction of a design graph to facilitate efficient model design exploration. The extensive evaluation across multiple datasets strengthens the validity of the claims regarding FALCON's performance. However, the paper could benefit from a more detailed discussion of the limitations of the proposed method and how it compares to state-of-the-art techniques in terms of practical applications and scalability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and easy to follow, with a clear progression from the introduction of the problem to the proposed solution and empirical validation. The methodology is described in sufficient detail, allowing for reproducibility, as the authors provide public access to datasets and code. The novelty of the approach is significant, particularly the integration of design graphs into the AutoML framework, but the paper may require further elaboration on specific experiments to enhance clarity.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of AutoML for graph tasks by introducing FALCON, which enhances both sample efficiency and design exploration. The methodology is robust, and the results are compelling, although a deeper discussion of limitations could strengthen the paper further.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents FALCON, a novel sample-based AutoML framework designed for optimal model design in graph-centric tasks. It introduces a design graph \\( G(N, E) \\) where nodes represent design alternatives and edges indicate design similarities. The approach comprises two main components: a task-agnostic module that employs a Graph Neural Network (GNN) for message passing on the design graph, and a task-specific module that utilizes label propagation based on known performance metrics. The experimental results demonstrate that FALCON outperforms conventional AutoML frameworks and sample-based methodologies across 27 node and graph classification tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative use of a design graph to leverage inter-design relations, which enhances sample efficiency in AutoML. The dual-module architecture effectively captures both general and task-specific characteristics, leading to improved performance predictions. However, a notable weakness is the potential for over-smoothing due to high average node degrees in the design graph, which may affect the robustness of the model. Additionally, while the empirical validation is thorough, further exploration of continuous design variables could enhance the framework's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings. The quality of the experiments is commendable, with appropriate baseline comparisons and statistical validation through t-tests. The novelty of using a relational design graph for optimizing model configurations is significant. However, while the methodology is described in detail, the reproducibility could be improved by providing more information on the implementation details and hyper-parameter settings used during the experiments.\n\n# Summary Of The Review\nFALCON introduces a compelling and efficient approach to AutoML for graph-centric tasks by utilizing a design graph to inform performance predictions. The methodology is innovative and demonstrates superior results compared to traditional frameworks, although concerns regarding over-smoothing and the need for improved adaptability to continuous variables are noted. Overall, the contributions are significant, and the paper is a valuable addition to the AutoML literature.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FALCON, a method designed for automated machine learning (AutoML) specifically tailored for graph tasks. The authors propose a design graph to represent relationships among different model architectures and utilize a meta-graph neural network (meta-GNN) predictor to guide the search process. While the authors claim that FALCON improves efficiency over conventional AutoML methods by reducing computational costs, the experimental results show only modest gains in performance across 27 datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its exploration of AutoML within the context of graph tasks, which is a relatively underexplored area. However, the contributions are undermined by several weaknesses, including the limited applicability of the proposed method beyond graph tasks and the questionable efficiency claims of FALCON, which does not significantly outperform existing one-shot approaches. Furthermore, the experimental design raises concerns about the representativeness of the datasets used, potentially limiting the generalizability of the results. The exploration of ethical considerations appears superficial, lacking a thorough examination of the potential biases in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written but could benefit from greater clarity in explaining the implications of the design graph and the meta-GNN predictor. The novelty of the approach is limited, as the underlying concepts may not be sufficiently innovative compared to existing methods in AutoML. Reproducibility could be a concern due to the minimal exploration of alternative methods and the reliance on a small number of design choices, which may lead to overfitting in the performance predictions.\n\n# Summary Of The Review\nWhile the paper introduces an interesting concept in AutoML for graph tasks, it ultimately falls short in demonstrating significant advancements over existing methods. The experimental design lacks rigor, and the results are not compelling enough to warrant strong confidence in the proposed approach.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces FALCON, an innovative sample-based AutoML framework specifically designed for graph tasks. FALCON utilizes a design graph to model relationships among various model designs, leading to a more efficient search strategy for optimal architectures. Empirical evaluations demonstrate that FALCON achieves remarkable sample efficiency, requiring only 30 explored nodes to match the performance of more resource-intensive methods, and outperforms existing state-of-the-art AutoML techniques across 27 graph classification tasks with an average improvement of 3.3% over the best baselines.\n\n# Strength And Weaknesses\nFALCON's main strengths lie in its exceptional sample efficiency and impressive performance in diverse graph classification tasks. The introduction of a design graph and a meta-GNN predictor is a significant contribution, as it enhances the navigation of design relations and improves prediction accuracy. However, potential weaknesses may include the need for further validation across even broader datasets and tasks beyond the 27 evaluated, as well as the reliance on the quality of the initial design graph structure.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents its methodology and findings. The novelty of FALCON's approach is notable, particularly in its innovative use of design graphs and dynamic search strategies. The reproducibility of the results is bolstered by the open-access availability of the code, encouraging further exploration and validation of the framework within the research community.\n\n# Summary Of The Review\nFALCON represents a significant advancement in automated machine learning for graph tasks, combining remarkable efficiency, innovative methodologies, and strong empirical validation. Its contributions to the field are substantial, promising impactful applications and future research directions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a theoretical framework for efficient automatic graph learning within the context of Automated Machine Learning (AutoML). The authors introduce the concept of a **design graph** to represent the interplay between various design choices and their corresponding performance outcomes. The methodology involves employing a meta-learning approach using Graph Neural Networks (GNNs) to predict the efficacy of unexplored designs based on learned inductive biases. The findings indicate that the proposed FALCON framework enhances sample efficiency and reduces computational intensity compared to traditional AutoML methods, laying the groundwork for future research into dynamic design graphs.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative theoretical contributions, particularly the introduction of the design graph and the application of GNNs for performance prediction. By framing the relationships between design choices, the authors provide a structured approach to navigating complex design spaces, which is a significant advancement in AutoML efficiency. However, the paper's weakness is that while the theoretical principles are well-articulated, the empirical results primarily serve to validate these theories rather than explore new empirical insights. This may limit the practical implications of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making complex concepts accessible. The quality of the theoretical framework is high, contributing substantially to the understanding of AutoML. In terms of novelty, the introduction of the design graph and the integration of GNNs represent a significant advancement in the field. However, the reproducibility of the results may be challenging due to the theoretical nature of the work and the reliance on specific assumptions about performance prediction that may not generalize across all scenarios.\n\n# Summary Of The Review\nOverall, this paper offers a compelling theoretical perspective on enhancing efficiency in AutoML through the introduction of design graphs and GNNs. While the theoretical contributions are robust and novel, the empirical validation could be more comprehensive to demonstrate broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Efficient Automatic Graph Learning via Design Relations\" introduces a novel framework called FALCON, aimed at enhancing sample-based AutoML for graph tasks. The proposed methodology integrates a design graph representation, where nodes represent design choices and edges denote design similarities, along with a meta-GNN architecture that comprises task-agnostic and task-specific modules for efficient message passing and label propagation. Extensive experiments on 27 node and graph classification tasks demonstrate that FALCON achieves competitive performance with significantly reduced exploration costs, requiring only 30 explored nodes to identify optimal designs compared to more conventional methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear presentation of the design graph construction and the comprehensive methodology outlined in the meta-GNN architecture. The extensive empirical evaluation against over ten baseline methods establishes FALCON's efficiency and effectiveness for graph tasks. However, one notable weakness is the potential complexity involved in understanding the underlying design distance metrics and their implications for connectivity, which may hinder accessibility for less experienced practitioners. Furthermore, while the paper emphasizes implementation details, it could benefit from a more thorough discussion of limitations and potential areas for future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections and straightforward explanations of complex concepts. The quality of the figures and tables is sufficient, aiding in the understanding of experimental results. In terms of novelty, FALCON presents a unique approach by combining design relations with GNNs, contributing to the field of AutoML. The reproducibility aspect is addressed through the availability of code at an accessible link, although additional details on the experimental setup and hyper-parameter settings would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of AutoML for graph tasks through the introduction of the FALCON framework. While the methodology is sound and the results are promising, the paper could enhance clarity regarding the design metrics used and could elaborate on its limitations. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FALCON, a method for automated machine learning (AutoML) focused on graph tasks. FALCON employs a design graph to enhance sample efficiency and incorporates a meta-GNN to capture relational information among model designs. While the authors report a modest average performance improvement of 3.3% over existing methods, the paper raises concerns regarding its novelty and effectiveness compared to established AutoML frameworks like DARTS and ENAS.\n\n# Strength And Weaknesses\nFALCON presents interesting concepts such as the design graph and meta-GNN; however, its contributions appear incremental rather than groundbreaking. The performance gains reported are modest, especially when put alongside advancements from other one-shot methods like GraphNAS. Furthermore, the paper does not convincingly demonstrate a significant leap in sample efficiency or computational burden reduction compared to existing methods like Bayesian Optimization. The limited dataset diversity and the choice of node exploration also raise questions about the generalizability and applicability of FALCON in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and presents its methodology and findings in a structured manner. However, the novelty of the proposed framework is questionable, as many ideas have been explored in prior works without achieving the same level of recognition. The reproducibility of results may be hampered by the limited number of explored nodes and the choice of datasets, which could restrict the ability of other researchers to validate the findings effectively.\n\n# Summary Of The Review\nOverall, FALCON offers a structured approach to AutoML for graph tasks but lacks significant novelty and offers only incremental improvements over existing methodologies. The reported performance gains do not position FALCON as a groundbreaking advancement in the field, and the paper's claims regarding efficiency and relational design capture require further substantiation.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Efficient Automatic Graph Learning via Design Relations\" introduces a novel framework for automatic graph learning by leveraging design relations among various architectural choices. The authors propose a dual-module system comprising a task-agnostic module and a task-specific module, allowing for flexible adaptation to different tasks. The methodology includes detailed experiments across multiple datasets, demonstrating that their approach, FALCON, achieves significant performance improvements over existing methods, with an average increase of 6.43% compared to the baseline FALCON-G.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to automatic graph learning, particularly in how it structures design relations to enhance flexibility and performance. The dual-module framework is well-conceived, allowing for both generalization and specificity in task handling. However, the paper has several weaknesses, including inconsistent terminology (e.g., \"explored node set\" vs. \"Ne\") and occasional clarity issues in phrasing. Additionally, while the empirical results are promising, the paper could benefit from a more thorough exploration of the limitations of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe overall clarity of the paper is hindered by grammatical inconsistencies and formatting issues, such as inconsistent use of quotation marks and varying formats for citations and figures. The novelty of the approach is commendable, as it presents a fresh perspective on graph learning, but the execution suffers from the aforementioned clarity issues. Reproducibility is a potential concern as well, given the inconsistent definitions of terms and acronyms, which could confuse readers attempting to replicate the experiments.\n\n# Summary Of The Review\nOverall, the paper presents an innovative framework for automatic graph learning that shows promising empirical results. However, clarity and consistency issues detract from its quality, making it challenging for readers to fully grasp the contributions and methodologies employed.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents FALCON, a framework designed to enhance sample efficiency in graph-based tasks. It employs an innovative design graph approach to optimize the architecture of neural networks for graph data, demonstrating improvements primarily on the CIFAR-10 dataset. The findings suggest that the choice of design dimensions significantly impacts model performance, although the paper does not fully explore the sensitivity of these choices across diverse datasets.\n\n# Strength And Weaknesses\nFALCON's main strengths lie in its innovative approach to design graphs and its demonstration of improved sample efficiency. However, the paper's contributions are somewhat limited by a narrow focus on graph tasks, particularly the CIFAR-10 dataset, and the lack of comprehensive comparisons with various graph types or datasets. Additionally, while the results are promising, the paper does not sufficiently address the scalability of the approach or the ethical implications of deploying FALCON in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. However, the novelty of the proposed framework could be more thoroughly contextualized within existing literature. The reproducibility of the results may be hindered by the limited exploration of hyper-parameter optimization techniques and the lack of discussion on the limitations of the meta-GNN architecture employed in FALCON.\n\n# Summary Of The Review\nWhile FALCON introduces an innovative framework for improving sample efficiency in graph tasks, its contributions are limited by a narrow experimental scope and a lack of exploration into broader applications and implications. Addressing these gaps could enhance the overall impact and applicability of the proposed method.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to automatic graph learning through a method named FALCON, which leverages design relations to enhance sample efficiency. The authors conduct extensive experiments across 27 tasks, demonstrating an average improvement of 3.3% over the best existing baselines. The methodology includes the use of Graph Neural Networks (GNNs) while incorporating statistical significance tests such as t-tests and ANOVA to validate the performance claims, ensuring that reported improvements are not due to random chance.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous application of statistical methodologies to substantiate performance claims, enhancing the credibility of its findings. The use of confidence intervals and sensitivity analysis adds depth to the evaluation of the proposed method's robustness. However, the paper also has weaknesses, such as the reliance on borderline p-values (e.g., 0.051) that may raise questions about the statistical significance of certain results. Additionally, while the methodology is sound, the paper could benefit from a more detailed discussion of the implications of the hyper-parameter sensitivity analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with a logical flow from introduction to conclusion. The quality of the writing is high, making it accessible for readers familiar with the field. The novelty of the approach, particularly in the application of design relations in GNNs, is commendable; however, the empirical significance of the results is somewhat tempered by the borderline statistical significance reported. Reproducibility is supported by the detailed descriptions of the experimental settings and methodologies, although additional information on data preprocessing and specific hyper-parameter settings could enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper contributes significantly to the field of automatic graph learning through its innovative approach and rigorous statistical validation. Despite minor issues with statistical significance in some results, the findings are compelling and provide a solid foundation for future work in this area.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces FALCON, a novel approach to model design choices in machine learning. It aims to improve the efficiency of searching through design spaces by leveraging a design graph. The methodology involves a fixed number of explored nodes and a static design graph to represent different design variables. While FALCON demonstrates some performance improvements over existing methods, the results are highly dependent on the dataset, and the framework does not adequately address various design variable types or the dynamic nature of design spaces.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to representing design choices through a graph structure, which has the potential to enhance sample efficiency. However, there are significant weaknesses, including the limited generalizability of FALCON to highly categorical design variables, the fixed number of nodes limiting exploration, and a high computational cost associated with training models from scratch. Additionally, the paper lacks a thorough exploration of advanced machine learning techniques that could potentially enhance its methodology, and it does not sufficiently analyze the implications of the inductive biases present in the design graph.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but some sections could benefit from further clarification, particularly regarding the implications of the inductive biases learned through the design graph. The novelty of the approach is notable; however, the reproducibility may be challenged by the high computational demands and the static nature of the design graph. The lack of extensive validation across diverse datasets raises concerns about the robustness of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to modeling design choices through a graph-based framework. However, it exhibits several limitations, including issues related to generalization, computational efficiency, and a lack of exploration of advanced techniques. These weaknesses suggest that while FALCON is a step forward, it requires further refinement and validation.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Efficient Automatic Graph Learning via Design Relations\" proposes a framework, FALCON, that aims to optimize AutoML for graph-based tasks by modeling the design space as a graph. The authors introduce a design graph where nodes represent design choices and edges indicate similarities between them. They employ a sample-based method to explore this design space and claim improvements in performance across 27 graph classification tasks. Despite these claims, much of the methodology appears to be a reiteration of existing concepts in graph theory and AutoML, lacking significant novelty.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to apply AutoML techniques to graph-based tasks, which is an increasingly relevant area of research. The use of a GNN for performance prediction adds a layer of sophistication. However, the weaknesses are pronounced; the contributions are largely derivative, lacking in-depth engagement with existing literature, and the insights offered are not particularly groundbreaking. The paper does not convincingly demonstrate its claimed advancements, with only a modest performance improvement over established baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in a generally clear and accessible manner, making it suitable for a broad audience. However, the lack of depth in the exploration of related work and the novelty of the contributions detracts from its overall quality. While the methodology may be reproducible, the reliance on well-known concepts and techniques raises questions about the originality of the approach. The paper would benefit from a more detailed discussion of how it differentiates itself from existing methods.\n\n# Summary Of The Review\nOverall, the paper presents an approach to AutoML for graph tasks that is more of a rehash of established ideas rather than a novel contribution to the field. Although the proposed framework is clear and methodologically sound, its lack of originality and significant advancements limits its impact.\n\n# Correctness\n4/5 - The methods and results appear to be sound, but the novelty and significance of the findings are questionable.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions are largely derivative and do not provide substantial advancements over existing methods.\n\n# Empirical Novelty And Significance\n2/5 - The empirical results, while demonstrating some improvements, are modest and do not substantially contribute to the field's understanding or capabilities in AutoML for graphs.",
    "# Summary Of The Paper\nThe paper presents FALCON, an innovative approach aimed at enhancing sample efficiency in automated machine learning (AutoML) for graph tasks through the use of a design graph and a meta-graph neural network (meta-GNN). The methodology involves optimizing architectural searches for node and graph classification tasks. Findings indicate that FALCON achieves superior performance compared to baseline methods, demonstrating its potential in efficiently navigating the design space of graph-based models.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel application of a meta-GNN to facilitate architectural search, which has shown promising results. However, several weaknesses are evident. The lack of uncertainty estimation methods, such as Bayesian techniques, limits the ability to gauge prediction confidence. Moreover, the fixed exploration strategy could hinder performance; a more adaptive approach may yield better results. The paper also overlooks the potential of transfer learning and hyperparameter optimization, which could further enhance performance, especially in data-scarce scenarios. Lastly, scalability issues could arise with larger datasets, and greater attention to evaluation metrics beyond accuracy could provide a more holistic view of model performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulated, making it accessible to readers. The proposed methodology is presented in a logical manner, with adequate detail to allow for reproducibility. However, while the technical contribution is notable, the novelty may be somewhat limited when considering the existing literature on GNNs and AutoML. The integration of additional features and techniques, as noted in the review, could enhance both the quality and the novelty of the work.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach for improving sample efficiency in AutoML for graph tasks, but it could benefit from incorporating various enhancements, including uncertainty estimation, dynamic exploration strategies, and broader design space exploration. Addressing the identified weaknesses would significantly strengthen the contributions and applicability of FALCON.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents FALCON, an automated machine learning (AutoML) framework specifically designed for graph-related tasks. It showcases significant improvements over existing state-of-the-art methods such as ENAS, DARTS, GraphNAS, and GASSO in both node and graph classification tasks across multiple datasets, including Cora, CiteSeer, and AmazonComputers. FALCON achieves these enhancements while maintaining a lower computational search cost compared to other AutoML strategies, demonstrating its efficiency and robustness through various analyses, including sensitivity and ablation studies.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its empirical performance, with FALCON consistently outperforming leading AutoML methods and achieving a notable reduction in search costs. The robustness of the framework is further supported by sensitivity analyses and ablation studies, which confirm the importance of its design components. However, the paper could have benefited from a more detailed discussion regarding the theoretical implications of its findings and the limitations of the proposed method in more diverse or complex graph scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to the reader. The quality of the empirical evaluations is high, with a comprehensive set of benchmarks supporting its claims. While the technical novelty of the approach is notable, the paper could enhance reproducibility by providing more detailed descriptions of the experimental setup and hyper-parameter configurations used during the evaluation.\n\n# Summary Of The Review\nOverall, FALCON represents a significant advancement in the AutoML landscape for graph-related tasks, achieving state-of-the-art performance with lower computational costs. The empirical results are convincing, although further exploration and discussion of its limitations could enhance the paper's contributions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents FALCON, a novel sample-based method aimed at improving the efficiency of AutoML processes. The authors argue that traditional AutoML methods suffer from high computational intensity, which FALCON seeks to mitigate through a task-agnostic framework that utilizes label propagation techniques. The methodology is evaluated through extensive experiments, demonstrating that FALCON not only reduces computational costs but also maintains or enhances predictive performance compared to existing AutoML approaches.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing the computational challenges inherent in AutoML. The proposed FALCON framework shows potential for significant efficiency gains, which is a pressing concern in the field. However, the paper exhibits weaknesses in clarity, particularly in the abstract and the presentation of results. The use of jargon and complex sentence structures could alienate some readers, and improvements in the organization of figures and tables would enhance comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel framework, its clarity is hindered by dense language and inconsistent terminology. The quality of the experiments is commendable, but the presentation lacks straightforward summaries that could aid the reader's understanding. Reproducibility is supported by detailed experimental setups; however, clearer explanations of technical terms and processes would further facilitate this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the AutoML landscape with its FALCON framework, which promises efficiency and effectiveness. Nonetheless, improvements in clarity and organization are necessary to enhance reader engagement and understanding.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.1130539323202924,
    -1.6744062819890428,
    -1.9618501363538,
    -1.8041974399447327,
    -1.981911062535448,
    -1.9242537348370392,
    -1.647799843763657,
    -1.8137859648605366,
    -1.8682380901113964,
    -1.740903992293701,
    -1.6786065079560917,
    -1.3447030106768074,
    -1.8860290281338377,
    -1.702805358599643,
    -1.7282605455710989,
    -1.9143758072664057,
    -1.8393698293718044,
    -1.933034444331032,
    -1.648469589768676,
    -2.031139402289237,
    -1.944631291008387,
    -1.9105303661548194,
    -1.8716053075887715,
    -1.7101028728501106,
    -1.8235392286097452,
    -1.7681582986481161,
    -1.8401115591693316,
    -1.657619529462237,
    -1.7354615371247943
  ],
  "logp_cond": [
    [
      0.0,
      -1.8730697476442268,
      -1.8916379094555202,
      -1.9077440613625174,
      -1.9127903454058595,
      -1.8984138270227553,
      -1.924448172384103,
      -1.9153268847023746,
      -1.8804264990421338,
      -1.9045013229123708,
      -1.9078117937982209,
      -1.9683236153238053,
      -1.8960612895619036,
      -1.8926833791374968,
      -1.9187935481919656,
      -1.8984937023520285,
      -1.9323349537318228,
      -1.8947800126263532,
      -1.919573127331063,
      -1.897630464917839,
      -1.904609449310865,
      -1.9415451428323514,
      -1.9157390366678506,
      -1.9311126830381746,
      -1.9188820665539235,
      -1.897402226501333,
      -1.9150918059493505,
      -1.9422958941752997,
      -1.9354644732572241
    ],
    [
      -1.3892957670815589,
      0.0,
      -1.2038074204360243,
      -1.1602662640694894,
      -1.3201929489230488,
      -1.276545842863071,
      -1.3481233568835258,
      -1.2414308911352974,
      -1.2035653266839157,
      -1.3730115109001049,
      -1.2088612998360817,
      -1.4608652790138763,
      -1.2059087169616947,
      -1.1202933849472103,
      -1.2039795553814006,
      -1.2604546717424505,
      -1.350759438466105,
      -1.2295709273852469,
      -1.3409909590824338,
      -1.2311209990370577,
      -1.3092592165200665,
      -1.3234881902978752,
      -1.3542135316318809,
      -1.3557392723975255,
      -1.3778489296263117,
      -1.282548044833739,
      -1.3384756954850467,
      -1.3934218760258066,
      -1.3733511566341756
    ],
    [
      -1.5797244329599134,
      -1.3478781075830406,
      0.0,
      -1.2867791843476453,
      -1.4029394377236284,
      -1.419886277489687,
      -1.5373514705542572,
      -1.4407694271144638,
      -1.3154893724276442,
      -1.5115290136639845,
      -1.404907935651865,
      -1.667319283445734,
      -1.4344750778703954,
      -1.3590721878413965,
      -1.3738872655407222,
      -1.399846786396053,
      -1.4220759397710003,
      -1.3257545855369373,
      -1.4902602799874123,
      -1.3435690961518907,
      -1.4029683365733872,
      -1.5036747252322513,
      -1.5058146240333516,
      -1.4798771261535875,
      -1.5085640726718483,
      -1.5091270394240976,
      -1.4760768909311783,
      -1.526649964397945,
      -1.597457978313433
    ],
    [
      -1.4904064867019138,
      -1.2306615679654562,
      -1.2482730034217888,
      0.0,
      -1.3229228570628426,
      -1.317911467381515,
      -1.4514037343164312,
      -1.3301174613854043,
      -1.3031576491997883,
      -1.4094981131377435,
      -1.2758252118751523,
      -1.5331845298070224,
      -1.2406589650400968,
      -1.2209649583802458,
      -1.3368527802536865,
      -1.312257865624638,
      -1.4031913107063305,
      -1.276922856887829,
      -1.4262996569644488,
      -1.2117669124276909,
      -1.3446188089428013,
      -1.3495362032918716,
      -1.4566855023189687,
      -1.3972813360312861,
      -1.4203066567537412,
      -1.3464242812833036,
      -1.391944485543252,
      -1.4388795188158254,
      -1.4640537450254394
    ],
    [
      -1.6687141951268545,
      -1.566327346833094,
      -1.4729684796223774,
      -1.511377733043183,
      0.0,
      -1.5251525464819264,
      -1.6382471885835785,
      -1.5312451414135162,
      -1.5051301231150123,
      -1.5588996098146708,
      -1.562004365160308,
      -1.703360693855209,
      -1.5452641633311879,
      -1.5339847767241643,
      -1.5566368288615913,
      -1.4751605468377482,
      -1.5813438131013526,
      -1.5206798606378888,
      -1.5360430482239107,
      -1.496357987972076,
      -1.537218629439172,
      -1.5499709084257738,
      -1.6244644921091798,
      -1.5634281651449964,
      -1.5353877720869196,
      -1.5087830385432428,
      -1.597034991770955,
      -1.5919701497766936,
      -1.6525451873246721
    ],
    [
      -1.53481812680892,
      -1.3859083123600362,
      -1.389317399075829,
      -1.4144612949610909,
      -1.3936980003929913,
      0.0,
      -1.48235846815864,
      -1.466448300482374,
      -1.3200393734534697,
      -1.474577893790484,
      -1.4406161680840943,
      -1.6108518612151765,
      -1.3786154910785544,
      -1.3610887213744287,
      -1.420620165677224,
      -1.451682581954239,
      -1.4981832021566652,
      -1.4673777509482633,
      -1.4399188853565716,
      -1.378409517372046,
      -1.4501155852688878,
      -1.5196657981079633,
      -1.4924613710940036,
      -1.4820547632239318,
      -1.483118748506823,
      -1.5150768103253829,
      -1.5217819564166857,
      -1.512382535599071,
      -1.5629835075360894
    ],
    [
      -1.311754286393147,
      -1.1541513876868135,
      -1.2217812845348521,
      -1.1927727432815425,
      -1.2035720202270894,
      -1.1857973499735475,
      0.0,
      -1.2057299203645588,
      -1.2139195724684715,
      -1.2313992686588244,
      -1.1649724636063001,
      -1.3312340183614932,
      -1.2348527566543417,
      -1.1308997362089248,
      -1.1697329887304957,
      -1.2116414700590221,
      -1.2764649148818092,
      -1.2288753748322976,
      -1.2270070606191321,
      -1.2155019732945413,
      -1.2009309400085881,
      -1.2600794376435192,
      -1.2115411949817456,
      -1.2320123177967095,
      -1.2799989939223202,
      -1.2402597688278247,
      -1.2508496551596888,
      -1.3165532540341505,
      -1.2789669710786906
    ],
    [
      -1.5323344744387914,
      -1.343012790093736,
      -1.400164904912045,
      -1.3798079923625146,
      -1.4445193874472242,
      -1.4044627052702097,
      -1.4479037247690143,
      0.0,
      -1.3514985451044104,
      -1.4691141948031268,
      -1.3576276430943333,
      -1.5299985131800193,
      -1.3664002996911562,
      -1.3390994187254548,
      -1.3635546914610566,
      -1.4339233844122181,
      -1.4519420548563373,
      -1.3350483423613513,
      -1.4419915437596396,
      -1.365362588758063,
      -1.3557035855763602,
      -1.4459498110209343,
      -1.458488561284185,
      -1.4151036482060242,
      -1.4303721013269393,
      -1.3933902690976674,
      -1.4577048988721746,
      -1.5155704737944535,
      -1.5043264834600816
    ],
    [
      -1.5188820313879392,
      -1.2749626120166675,
      -1.2800560926065543,
      -1.3572322064231364,
      -1.4141665129838115,
      -1.330116266747384,
      -1.497881302460212,
      -1.366713927477627,
      0.0,
      -1.473111222097822,
      -1.4080137850446555,
      -1.5752180563876659,
      -1.2297759594715503,
      -1.3261803399834267,
      -1.4089295552023047,
      -1.2644367749257825,
      -1.444218629410833,
      -1.3642501251395598,
      -1.4662371509668584,
      -1.2640527326600823,
      -1.397767622305943,
      -1.4623654367266639,
      -1.475409968609218,
      -1.508769465164717,
      -1.4663529269443285,
      -1.429551178428057,
      -1.4426307174170543,
      -1.47598232832971,
      -1.5100159248008205
    ],
    [
      -1.4657765272384806,
      -1.406423798861552,
      -1.398344978143918,
      -1.4094381947229715,
      -1.3889903259409717,
      -1.3697562338587257,
      -1.4525296903224252,
      -1.3907666407228692,
      -1.4144341829383702,
      0.0,
      -1.3701913467531321,
      -1.5270628813323983,
      -1.3889792223153552,
      -1.3959917171049945,
      -1.3965949142498144,
      -1.4224403840929618,
      -1.4341330151695904,
      -1.43024184110981,
      -1.417154253530245,
      -1.4276312181930786,
      -1.4198786628435383,
      -1.4517921138158554,
      -1.4503093095675303,
      -1.4516988988053097,
      -1.3860579580674746,
      -1.3986979009100193,
      -1.4288638490531345,
      -1.4784986618132265,
      -1.4808253255927117
    ],
    [
      -1.37492504709589,
      -1.1843462447809974,
      -1.280360932490199,
      -1.2336039705566126,
      -1.3201250488547787,
      -1.2469003606761098,
      -1.333231873466966,
      -1.2498870228921588,
      -1.2858728337221073,
      -1.290265489065778,
      0.0,
      -1.4467895404963367,
      -1.2281973190193889,
      -1.1344139282535322,
      -1.2048719047072924,
      -1.2504848664492518,
      -1.2808969736473108,
      -1.296033791410348,
      -1.3073730419405252,
      -1.2797540142948607,
      -1.271349689404561,
      -1.3220574254396176,
      -1.3121218435021564,
      -1.3026395201907366,
      -1.3646914463133866,
      -1.278547258086246,
      -1.3626466471750707,
      -1.3765795933151248,
      -1.3668770191015513
    ],
    [
      -1.1741198134737931,
      -1.1039452300840416,
      -1.1285228481106306,
      -1.142779826479019,
      -1.1227620470271709,
      -1.121888829200937,
      -1.1300856487176307,
      -1.0976645037057207,
      -1.1321369811331248,
      -1.1247436528091654,
      -1.1232305901323516,
      0.0,
      -1.1541356277292485,
      -1.1132581298528452,
      -1.1087085612640684,
      -1.1526998363263001,
      -1.1175237354174592,
      -1.1641765782683593,
      -1.1043211082169588,
      -1.1250858444529488,
      -1.1368410454152835,
      -1.1222617095396838,
      -1.1206419681485182,
      -1.103852441062943,
      -1.153262565690329,
      -1.1323672861995302,
      -1.111133142747533,
      -1.1660989540168434,
      -1.0816854952010895
    ],
    [
      -1.5290984683041646,
      -1.3728823371471106,
      -1.4009522342554803,
      -1.361094486249536,
      -1.4559539440637692,
      -1.3496654726905806,
      -1.4889570870821522,
      -1.4294462632319538,
      -1.269067282256376,
      -1.4840632601511947,
      -1.3815107138858416,
      -1.627228580111935,
      0.0,
      -1.3386339849010358,
      -1.4257981145126357,
      -1.2957395455651166,
      -1.474181174509438,
      -1.4282765651688645,
      -1.4434429025239623,
      -1.3290949310336502,
      -1.4151796386462303,
      -1.4439967609609683,
      -1.514178946507422,
      -1.4837343146363708,
      -1.45783232164072,
      -1.460289589952549,
      -1.4768877628741273,
      -1.5199332785873199,
      -1.5548016672892395
    ],
    [
      -1.387865346136906,
      -1.1297125562980799,
      -1.1912500471030543,
      -1.1548948779619095,
      -1.3404459150623251,
      -1.2043374910278415,
      -1.3057810605494544,
      -1.2295746978081203,
      -1.1697904792483116,
      -1.3225111171150197,
      -1.1725750125345653,
      -1.4562739781371514,
      -1.1609764138680567,
      0.0,
      -1.2262816881721308,
      -1.2240620027646112,
      -1.298998020300189,
      -1.232338238985493,
      -1.3104392285641582,
      -1.2195342625379688,
      -1.2603616466588792,
      -1.3437689495590426,
      -1.32511833206932,
      -1.3071478796487652,
      -1.3725475157929758,
      -1.3162959023257326,
      -1.3512812105660936,
      -1.414907700639338,
      -1.3856874570318969
    ],
    [
      -1.3647718394501827,
      -1.149579118138479,
      -1.1312669662223078,
      -1.1802558916846504,
      -1.2276267970719659,
      -1.2252430834221957,
      -1.269224544939277,
      -1.1593884255502545,
      -1.2047507629205345,
      -1.2751953266319438,
      -1.0908548590205323,
      -1.4171799879958833,
      -1.2489205665140641,
      -1.1517721310274849,
      0.0,
      -1.2361924368491575,
      -1.2495450029687776,
      -1.20280637474295,
      -1.2581939625447993,
      -1.1733099702148435,
      -1.2451829182447312,
      -1.2785834248978674,
      -1.2806077900689201,
      -1.2484405524137574,
      -1.3150500972603705,
      -1.2397324981500522,
      -1.265636697323025,
      -1.3536595627994925,
      -1.3493915832308596
    ],
    [
      -1.5720574414012014,
      -1.4233312020672526,
      -1.4362225853664348,
      -1.416024350189868,
      -1.483870819123063,
      -1.4576460499304433,
      -1.5357454234176044,
      -1.5256135558883543,
      -1.3386065284274304,
      -1.5404352125142362,
      -1.4303169662614201,
      -1.6636785529286877,
      -1.3269072093624807,
      -1.372265950726309,
      -1.4622272910499683,
      0.0,
      -1.490832826568494,
      -1.4710106555033897,
      -1.5028796998097356,
      -1.397013095083526,
      -1.4942264644429202,
      -1.537584739548071,
      -1.5496634384273447,
      -1.5146243431444064,
      -1.5462709096346101,
      -1.518862789566135,
      -1.5477436093861838,
      -1.5351223890416843,
      -1.5838800752210953
    ],
    [
      -1.4947004331772258,
      -1.4121936749962571,
      -1.3427588512000488,
      -1.362666832957632,
      -1.394559668252321,
      -1.4111466698444497,
      -1.4669037812278805,
      -1.3733929552610205,
      -1.3623499358621611,
      -1.4634452852000048,
      -1.3681273930560804,
      -1.5910281851312966,
      -1.425347633358573,
      -1.383031608482785,
      -1.3780359587206272,
      -1.4122218115256555,
      0.0,
      -1.3120989922090425,
      -1.4019782983739486,
      -1.3929486402959725,
      -1.305490195124086,
      -1.4902486470172918,
      -1.419433292388698,
      -1.484777754812077,
      -1.4424271432213214,
      -1.4251249993683806,
      -1.4086585690071043,
      -1.4761569102217211,
      -1.5527689501285389
    ],
    [
      -1.5666975962874818,
      -1.4280171325436304,
      -1.3447058454501595,
      -1.375578484990819,
      -1.4287073821792715,
      -1.4505435064890115,
      -1.5562145432891747,
      -1.3567221285409898,
      -1.4001581253210724,
      -1.5317119787802456,
      -1.4106409121631225,
      -1.6501677310743958,
      -1.4486465558174149,
      -1.3734692633227965,
      -1.4158238998400279,
      -1.447746213276918,
      -1.4014697913387775,
      0.0,
      -1.4977655605345621,
      -1.3271966182822843,
      -1.3625807305388433,
      -1.4664990459457348,
      -1.4841935005117182,
      -1.465112341046266,
      -1.5099803381277452,
      -1.4793085172821623,
      -1.4869800708255072,
      -1.503971023815563,
      -1.5849060692353443
    ],
    [
      -1.3599001356125089,
      -1.308793474581396,
      -1.280432279466052,
      -1.2837473233207577,
      -1.2539455480647148,
      -1.3010388009950336,
      -1.3115864600599545,
      -1.2996669462761206,
      -1.2801092562013028,
      -1.3096751129030209,
      -1.2937061840401594,
      -1.428785085487142,
      -1.2540978284607365,
      -1.2851915325457697,
      -1.310090812619748,
      -1.3170771584291026,
      -1.2771502315208518,
      -1.3229942970523247,
      0.0,
      -1.2635128738737929,
      -1.2660219495671374,
      -1.324640618492008,
      -1.330899314238366,
      -1.320224126426135,
      -1.2811730376257273,
      -1.3203890827284406,
      -1.3087985840857712,
      -1.3493618616075778,
      -1.3493167210059718
    ],
    [
      -1.722780098188916,
      -1.5581702811308153,
      -1.5627839247151447,
      -1.4914567350491068,
      -1.5325244935517703,
      -1.6167845235388791,
      -1.703588680536192,
      -1.5934508686265028,
      -1.5071530776816404,
      -1.6503289587647267,
      -1.6167844881734301,
      -1.780036766080434,
      -1.5105670601636576,
      -1.576175542444929,
      -1.5983939933516684,
      -1.5384460778893256,
      -1.6620572121249528,
      -1.5607239053761426,
      -1.6463941494699064,
      0.0,
      -1.611573768845263,
      -1.5805719001248568,
      -1.6600156712922762,
      -1.5976828908663008,
      -1.668973023417047,
      -1.5896353253501356,
      -1.642832778952705,
      -1.6854987588608261,
      -1.661769893707929
    ],
    [
      -1.6174899599628438,
      -1.5387679281411013,
      -1.4604687013399706,
      -1.4487725433021568,
      -1.534242832556035,
      -1.4753611047322963,
      -1.610536574811873,
      -1.4292411184639622,
      -1.5009129019369083,
      -1.5891709472645004,
      -1.522843581405729,
      -1.71811973964731,
      -1.5036267746813907,
      -1.4514568786164392,
      -1.5895790351627326,
      -1.5323757235440918,
      -1.4731547884405438,
      -1.4195532831315398,
      -1.5453105484812504,
      -1.4982556864936407,
      0.0,
      -1.612383434546069,
      -1.579425875406036,
      -1.5993008296414704,
      -1.5162776262221576,
      -1.5760867338087705,
      -1.5389130705458127,
      -1.5858499771084915,
      -1.6816391969074052
    ],
    [
      -1.6471473959891363,
      -1.4987901172061804,
      -1.5439255183650191,
      -1.437470091146354,
      -1.5277014455867512,
      -1.531177973467432,
      -1.5906140875809596,
      -1.5277488082671877,
      -1.5401836876495703,
      -1.5345503259887332,
      -1.515369306025957,
      -1.6287385284291176,
      -1.5078942088773197,
      -1.4931913279907634,
      -1.50548073237353,
      -1.5697475231728757,
      -1.595633976097663,
      -1.5678331700331145,
      -1.556218444708976,
      -1.4673451203755363,
      -1.5532223774937497,
      0.0,
      -1.5730521395828572,
      -1.4430831541048665,
      -1.6042723292036238,
      -1.4368353523746338,
      -1.6049024837497003,
      -1.5825386976777216,
      -1.5364704754768075
    ],
    [
      -1.4749900183759321,
      -1.333435893004803,
      -1.365868695854251,
      -1.3753760798989878,
      -1.3613877357774868,
      -1.3686356132383826,
      -1.3559630255537225,
      -1.3613109968869226,
      -1.3996028954013777,
      -1.412236222276718,
      -1.3093789064485188,
      -1.5043558341079553,
      -1.4114244900032138,
      -1.34679570020815,
      -1.3293354847296432,
      -1.4252887836508272,
      -1.342733005634765,
      -1.3696937018964084,
      -1.3960002781072252,
      -1.37406256663976,
      -1.2850523887715999,
      -1.4059093624917978,
      0.0,
      -1.3806963560204755,
      -1.4217181545758475,
      -1.4029680113619223,
      -1.2954019824798908,
      -1.4189853803378718,
      -1.4195822954808577
    ],
    [
      -1.432979789976806,
      -1.3484762472910572,
      -1.2797578537466063,
      -1.3216506253528362,
      -1.291018823616178,
      -1.366174695297596,
      -1.3562285708679442,
      -1.2785929919397803,
      -1.3222467488742744,
      -1.3570697850528317,
      -1.3095167468544182,
      -1.422098736316493,
      -1.328306363004878,
      -1.2724864531788143,
      -1.3538940092031433,
      -1.335998440042391,
      -1.3757859250250195,
      -1.317686696165453,
      -1.3396551040166063,
      -1.301131398854063,
      -1.3232492654123813,
      -1.2593525123371339,
      -1.3436739476831348,
      0.0,
      -1.387634286287403,
      -1.3347443744005174,
      -1.3762955078327153,
      -1.3644387774124505,
      -1.3625685518381794
    ],
    [
      -1.5129081934634427,
      -1.4622430614061266,
      -1.3610276754230528,
      -1.4172507424997998,
      -1.3304789127473047,
      -1.403833726795449,
      -1.4822345938269745,
      -1.3909388705820689,
      -1.3773777822651945,
      -1.4185153102144172,
      -1.4656210712270827,
      -1.604357962003594,
      -1.3713208915277826,
      -1.449895823810743,
      -1.4884267850521837,
      -1.4231498602571875,
      -1.4431156391775133,
      -1.4062142282659604,
      -1.4230764947106609,
      -1.4250038903799562,
      -1.3702729230250048,
      -1.4878987797896785,
      -1.4723464731238516,
      -1.4728944364211878,
      0.0,
      -1.481467311694692,
      -1.44429560981375,
      -1.5037160233954565,
      -1.5413610810368605
    ],
    [
      -1.505105471538224,
      -1.3762776779919512,
      -1.3833103098148876,
      -1.3383976916779825,
      -1.365489874592612,
      -1.4155183566862537,
      -1.4520720184233538,
      -1.3607467079115652,
      -1.354999965352148,
      -1.4182890836778528,
      -1.3828584138549354,
      -1.5255062767619234,
      -1.3730826387898973,
      -1.3673593964725728,
      -1.4069241399288013,
      -1.3977311356128073,
      -1.4463174805354995,
      -1.4039415941728601,
      -1.447520434487673,
      -1.3201236002131294,
      -1.4308071419479722,
      -1.3574193913534116,
      -1.4330327371595668,
      -1.352089164275351,
      -1.4718694562543242,
      0.0,
      -1.4517776538335527,
      -1.473183011202844,
      -1.4486559271853197
    ],
    [
      -1.5226062159941105,
      -1.4064069480288939,
      -1.3886936117376412,
      -1.4191713644555244,
      -1.4292439929431557,
      -1.405351177939109,
      -1.4580870623321054,
      -1.4368997182351881,
      -1.3892518081802767,
      -1.48709195579979,
      -1.4088773235731995,
      -1.56873094918803,
      -1.415836037380369,
      -1.3881910362933767,
      -1.4046616348639185,
      -1.453956888020327,
      -1.4097707704570468,
      -1.3814554557665486,
      -1.4262620143926825,
      -1.4076191508424156,
      -1.3540400751664021,
      -1.522493676241049,
      -1.4205539785838146,
      -1.5057281592669842,
      -1.4368224006685788,
      -1.4808025941333554,
      0.0,
      -1.486597248052961,
      -1.5302924123239987
    ],
    [
      -1.3568050773086602,
      -1.28195650982838,
      -1.2492400112355442,
      -1.273373217055233,
      -1.229025268059498,
      -1.2355572977802811,
      -1.2959973394118098,
      -1.2213924221107233,
      -1.2243172002671998,
      -1.3154054070681953,
      -1.2867088734527905,
      -1.3675746337274177,
      -1.2767989299410418,
      -1.2988735945133112,
      -1.2760612760103325,
      -1.2369115924636978,
      -1.2487424727088081,
      -1.2655435865879796,
      -1.2522263932779807,
      -1.2369318669436526,
      -1.209939806558844,
      -1.2859893016577841,
      -1.2718906577508056,
      -1.2416840876480453,
      -1.285500806487176,
      -1.3197555497092808,
      -1.2671007595347972,
      0.0,
      -1.3176369784860205
    ],
    [
      -1.4194296498888335,
      -1.3156685219751778,
      -1.3830167135934925,
      -1.366384633312581,
      -1.4089753747373575,
      -1.3754374643438314,
      -1.3841918347928122,
      -1.370185040378921,
      -1.3747411385164423,
      -1.3987676571529837,
      -1.3359267671593318,
      -1.3791310013630902,
      -1.3604143671554407,
      -1.3055213450408614,
      -1.3392715843222536,
      -1.3516387240365026,
      -1.3927058990975851,
      -1.3883591180089576,
      -1.3879741283471163,
      -1.367030793037183,
      -1.4155557272088666,
      -1.2469033045366604,
      -1.3669910354335664,
      -1.3558921226580594,
      -1.4362605513478615,
      -1.371089452172042,
      -1.376360587323334,
      -1.3685881444253682,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.23998418467606553,
      0.22141602286477213,
      0.20530987095777498,
      0.2002635869144329,
      0.21464010529753708,
      0.18860575993618944,
      0.19772704761791782,
      0.2326274332781586,
      0.20855260940792153,
      0.20524213852207152,
      0.14473031699648709,
      0.2169926427583888,
      0.22037055318279553,
      0.1942603841283268,
      0.21456022996826385,
      0.18071897858846953,
      0.21827391969393917,
      0.19348080498922937,
      0.21542346740245333,
      0.20844448300942742,
      0.17150878948794102,
      0.19731489565244176,
      0.18194124928211775,
      0.1941718657663689,
      0.21565170581895932,
      0.1979621263709419,
      0.17075803814499269,
      0.17758945906306822
    ],
    [
      0.28511051490748396,
      0.0,
      0.47059886155301855,
      0.5141400179195534,
      0.354213333065994,
      0.3978604391259719,
      0.32628292510551704,
      0.4329753908537455,
      0.47084095530512715,
      0.301394771088938,
      0.46554498215296114,
      0.21354100297516654,
      0.4684975650273482,
      0.5541128970418325,
      0.4704267266076423,
      0.4139516102465923,
      0.3236468435229378,
      0.444835354603796,
      0.333415322906609,
      0.4432852829519851,
      0.36514706546897635,
      0.3509180916911676,
      0.320192750357162,
      0.3186670095915174,
      0.2965573523627312,
      0.3918582371553039,
      0.3359305865039961,
      0.28098440596323626,
      0.3010551253548672
    ],
    [
      0.3821257033938865,
      0.6139720287707593,
      0.0,
      0.6750709520061546,
      0.5589106986301715,
      0.5419638588641129,
      0.42449866579954265,
      0.5210807092393361,
      0.6463607639261557,
      0.4503211226898154,
      0.5569422007019349,
      0.2945308529080659,
      0.5273750584834045,
      0.6027779485124034,
      0.5879628708130777,
      0.5620033499577468,
      0.5397741965827996,
      0.6360955508168626,
      0.4715898563663876,
      0.6182810402019092,
      0.5588817997804127,
      0.4581754111215486,
      0.45603551232044826,
      0.48197301020021244,
      0.45328606368195157,
      0.45272309692970225,
      0.48577324542262157,
      0.4352001719558549,
      0.36439215804036684
    ],
    [
      0.3137909532428189,
      0.5735358719792765,
      0.5559244365229439,
      0.0,
      0.48127458288189007,
      0.4862859725632176,
      0.3527937056283015,
      0.4740799785593284,
      0.5010397907449444,
      0.39469932680698916,
      0.5283722280695804,
      0.27101291013771034,
      0.5635384749046359,
      0.5832324815644869,
      0.4673446596910462,
      0.4919395743200947,
      0.4010061292384022,
      0.5272745830569037,
      0.37789778298028387,
      0.5924305275170418,
      0.4595786310019314,
      0.45466123665286107,
      0.34751193762576404,
      0.40691610391344657,
      0.38389078319099146,
      0.4577731586614291,
      0.4122529544014808,
      0.36531792112890726,
      0.3401436949192933
    ],
    [
      0.3131968674085934,
      0.41558371570235386,
      0.5089425829130705,
      0.4705333294922649,
      0.0,
      0.45675851605352147,
      0.34366387395186937,
      0.4506659211219317,
      0.47678093942043565,
      0.4230114527207771,
      0.41990669737513997,
      0.27855036868023886,
      0.43664689920426003,
      0.4479262858112836,
      0.42527423367385664,
      0.5067505156976997,
      0.4005672494340953,
      0.46123120189755906,
      0.4458680143115372,
      0.4855530745633718,
      0.4446924330962758,
      0.4319401541096741,
      0.35744657042626815,
      0.4184828973904515,
      0.44652329044852834,
      0.4731280239922051,
      0.38487607076449293,
      0.3899409127587543,
      0.32936587521077576
    ],
    [
      0.38943560802811916,
      0.538345422477003,
      0.5349363357612102,
      0.5097924398759484,
      0.5305557344440479,
      0.0,
      0.4418952666783993,
      0.45780543435466514,
      0.6042143613835695,
      0.4496758410465551,
      0.4836375667529449,
      0.31340187362186267,
      0.5456382437584848,
      0.5631650134626105,
      0.5036335691598153,
      0.47257115288280027,
      0.426070532680374,
      0.4568759838887759,
      0.48433484948046757,
      0.5458442174649931,
      0.47413814956815137,
      0.4045879367290759,
      0.43179236374303565,
      0.4421989716131074,
      0.4411349863302163,
      0.4091769245116563,
      0.40247177842035353,
      0.4118711992379682,
      0.3612702273009498
    ],
    [
      0.3360455573705099,
      0.4936484560768435,
      0.4260185592288048,
      0.4550271004821145,
      0.44422782353656753,
      0.4620024937901095,
      0.0,
      0.4420699233990981,
      0.43388027129518547,
      0.4164005751048325,
      0.4828273801573568,
      0.3165658254021637,
      0.41294708710931527,
      0.5169001075547321,
      0.47806685503316126,
      0.43615837370463484,
      0.3713349288818477,
      0.4189244689313594,
      0.42079278314452484,
      0.4322978704691156,
      0.4468689037550688,
      0.3877204061201378,
      0.43625864878191134,
      0.41578752596694746,
      0.36780084984133676,
      0.4075400749358322,
      0.39695018860396813,
      0.3312465897295065,
      0.36883287268496634
    ],
    [
      0.2814514904217451,
      0.4707731747668005,
      0.4136210599484915,
      0.43397797249802195,
      0.36926657741331237,
      0.4093232595903269,
      0.36588224009152226,
      0.0,
      0.4622874197561262,
      0.3446717700574098,
      0.4561583217662033,
      0.2837874516805172,
      0.4473856651693804,
      0.47468654613508177,
      0.45023127339948,
      0.37986258044831844,
      0.36184391000419924,
      0.4787376224991853,
      0.371794421100897,
      0.4484233761024736,
      0.4580823792841764,
      0.3678361538396022,
      0.35529740357635164,
      0.39868231665451237,
      0.3834138635335973,
      0.42039569576286917,
      0.356081065988362,
      0.298215491066083,
      0.309459481400455
    ],
    [
      0.3493560587234572,
      0.5932754780947289,
      0.5881819975048421,
      0.51100588368826,
      0.45407157712758495,
      0.5381218233640124,
      0.3703567876511844,
      0.5015241626337694,
      0.0,
      0.39512686801357444,
      0.46022430506674095,
      0.29302003372373053,
      0.6384621306398461,
      0.5420577501279698,
      0.45930853490909174,
      0.6038013151856139,
      0.4240194607005634,
      0.5039879649718366,
      0.40200093914453805,
      0.6041853574513141,
      0.47047046780545343,
      0.40587265338473255,
      0.3928281215021785,
      0.3594686249466794,
      0.4018851631670679,
      0.4386869116833394,
      0.4256073726943421,
      0.3922557617816864,
      0.35822216531057594
    ],
    [
      0.27512746505522045,
      0.3344801934321491,
      0.3425590141497832,
      0.3314657975707296,
      0.35191366635272936,
      0.3711477584349754,
      0.2883743019712759,
      0.3501373515708319,
      0.3264698093553309,
      0.0,
      0.37071264554056893,
      0.21384111096130276,
      0.3519247699783459,
      0.3449122751887066,
      0.34430907804388666,
      0.31846360820073927,
      0.3067709771241107,
      0.3106621511838912,
      0.32374973876345603,
      0.3132727741006225,
      0.3210253294501628,
      0.2891118784778457,
      0.2905946827261707,
      0.2892050934883914,
      0.3548460342262265,
      0.34220609138368174,
      0.3120401432405666,
      0.26240533048047454,
      0.26007866670098934
    ],
    [
      0.30368146086020165,
      0.4942602631750943,
      0.3982455754658927,
      0.44500253739947904,
      0.358481459101313,
      0.4317061472799819,
      0.34537463448912575,
      0.42871948506393287,
      0.3927336742339844,
      0.3883410188903136,
      0.0,
      0.231816967459755,
      0.4504091889367028,
      0.5441925797025595,
      0.47373460324879924,
      0.42812164150683985,
      0.39770953430878087,
      0.38257271654574376,
      0.3712334660155665,
      0.398852493661231,
      0.4072568185515306,
      0.3565490825164741,
      0.36648466445393524,
      0.3759669877653551,
      0.3139150616427051,
      0.40005924986984565,
      0.315959860781021,
      0.30202691464096687,
      0.3117294888545403
    ],
    [
      0.1705831972030143,
      0.24075778059276587,
      0.21618016256617678,
      0.20192318419778843,
      0.22194096364963656,
      0.2228141814758704,
      0.2146173619591767,
      0.24703850697108676,
      0.21256602954368264,
      0.21995935786764198,
      0.22147242054445582,
      0.0,
      0.19056738294755893,
      0.2314448808239622,
      0.23599444941273906,
      0.1920031743505073,
      0.22717927525934822,
      0.18052643240844812,
      0.24038190245984858,
      0.21961716622385863,
      0.2078619652615239,
      0.22244130113712357,
      0.22406104252828918,
      0.24085056961386453,
      0.1914404449864784,
      0.21233572447727722,
      0.23356986792927437,
      0.17860405665996404,
      0.26301751547571794
    ],
    [
      0.35693055982967303,
      0.513146690986727,
      0.48507679387835734,
      0.5249345418843017,
      0.43007508407006845,
      0.5363635554432571,
      0.3970719410516854,
      0.4565827649018839,
      0.6169617458774617,
      0.40196576798264294,
      0.5045183142479961,
      0.2588004480219026,
      0.0,
      0.5473950432328019,
      0.46023091362120194,
      0.590289482568721,
      0.4118478536243997,
      0.4577524629649732,
      0.4425861256098753,
      0.5569340971001875,
      0.47084938948760735,
      0.44203226717286936,
      0.37185008162641564,
      0.4022947134974668,
      0.4281967064931176,
      0.4257394381812887,
      0.4091412652597104,
      0.3660957495465178,
      0.33122736084459814
    ],
    [
      0.31494001246273706,
      0.5730928023015631,
      0.5115553114965887,
      0.5479104806377335,
      0.36235944353731786,
      0.4984678675718015,
      0.3970242980501886,
      0.47323066079152265,
      0.5330148793513314,
      0.3802942414846233,
      0.5302303460650777,
      0.24653138046249157,
      0.5418289447315863,
      0.0,
      0.4765236704275122,
      0.4787433558350318,
      0.40380733829945403,
      0.4704671196141499,
      0.3923661300354848,
      0.4832710960616742,
      0.4424437119407638,
      0.3590364090406004,
      0.37768702653032293,
      0.3956574789508778,
      0.33025784280666715,
      0.3865094562739104,
      0.35152414803354937,
      0.287897657960305,
      0.3171179015677461
    ],
    [
      0.3634887061209162,
      0.57868142743262,
      0.596993579348791,
      0.5480046538864485,
      0.500633748499133,
      0.5030174621489032,
      0.4590360006318219,
      0.5688721200208444,
      0.5235097826505644,
      0.4530652189391551,
      0.6374056865505666,
      0.3110805575752156,
      0.47933997905703474,
      0.576488414543614,
      0.0,
      0.4920681087219414,
      0.4787155426023213,
      0.5254541708281488,
      0.47006658302629956,
      0.5549505753562554,
      0.4830776273263677,
      0.4496771206732315,
      0.44765275550217876,
      0.4798199931573415,
      0.4132104483107284,
      0.4885280474210467,
      0.4626238482480738,
      0.37460098277160636,
      0.37886896234023926
    ],
    [
      0.34231836586520425,
      0.49104460519915305,
      0.4781532218999709,
      0.4983514570765377,
      0.4305049881433427,
      0.4567297573359623,
      0.3786303838488012,
      0.3887622513780513,
      0.5757692788389752,
      0.37394059475216945,
      0.48405884100498553,
      0.25069725433771795,
      0.5874685979039249,
      0.5421098565400966,
      0.45214851621643737,
      0.0,
      0.42354298069791163,
      0.44336515176301594,
      0.4114961074566701,
      0.5173627121828797,
      0.4201493428234855,
      0.3767910677183346,
      0.364712368839061,
      0.39975146412199924,
      0.36810489763179555,
      0.3955130177002706,
      0.36663219788022183,
      0.37925341822472136,
      0.33049573204531035
    ],
    [
      0.3446693961945786,
      0.42717615437554723,
      0.4966109781717556,
      0.47670299641417246,
      0.44481016111948346,
      0.4282231595273547,
      0.3724660481439239,
      0.4659768741107839,
      0.47701989350964324,
      0.37592454417179955,
      0.47124243631572393,
      0.24834164424050775,
      0.4140221960132313,
      0.45633822088901943,
      0.46133387065117715,
      0.42714801784614886,
      0.0,
      0.5272708371627619,
      0.43739153099785577,
      0.44642118907583184,
      0.5338796342477183,
      0.3491211823545126,
      0.4199365369831063,
      0.3545920745597273,
      0.396942686150483,
      0.4142448300034238,
      0.4307112603647001,
      0.36321291915008325,
      0.2866008792432655
    ],
    [
      0.36633684804355027,
      0.5050173117874017,
      0.5883285988808726,
      0.5574559593402131,
      0.5043270621517606,
      0.4824909378420206,
      0.37681990104185736,
      0.5763123157900423,
      0.5328763190099597,
      0.40132246555078654,
      0.5223935321679096,
      0.28286671325663626,
      0.48438788851361725,
      0.5595651810082356,
      0.5172105444910042,
      0.485288231054114,
      0.5315646529922546,
      0.0,
      0.43526888379647,
      0.6058378260487478,
      0.5704537137921888,
      0.46653539838529734,
      0.44884094381931394,
      0.467922103284766,
      0.4230541062032869,
      0.4537259270488698,
      0.4460543735055249,
      0.4290634205154691,
      0.34812837509568784
    ],
    [
      0.2885694541561672,
      0.33967611518728,
      0.368037310302624,
      0.3647222664479184,
      0.39452404170396127,
      0.34743078877364253,
      0.3368831297087216,
      0.3488026434925555,
      0.3683603335673733,
      0.3387944768656552,
      0.3547634057285167,
      0.21968450428153408,
      0.3943717613079396,
      0.3632780572229064,
      0.33837877714892817,
      0.3313924313395735,
      0.37131935824782425,
      0.32547529271635134,
      0.0,
      0.38495671589488323,
      0.38244764020153865,
      0.323828971276668,
      0.31757027553031003,
      0.3282454633425411,
      0.36729655214294876,
      0.3280805070402355,
      0.33967100568290487,
      0.2991077281610983,
      0.2991528687627043
    ],
    [
      0.308359304100321,
      0.4729691211584217,
      0.46835547757409235,
      0.5396826672401303,
      0.4986149087374667,
      0.4143548787503579,
      0.3275507217530451,
      0.4376885336627343,
      0.5239863246075966,
      0.3808104435245103,
      0.41435491411580694,
      0.25110263620880313,
      0.5205723421255795,
      0.454963859844308,
      0.43274540893756863,
      0.4926933243999114,
      0.36908219016428423,
      0.4704154969130945,
      0.3847452528193307,
      0.0,
      0.41956563344397413,
      0.45056750216438024,
      0.37112373099696083,
      0.4334565114229363,
      0.36216637887219005,
      0.44150407693910143,
      0.388306623336532,
      0.34564064342841094,
      0.369369508581308
    ],
    [
      0.3271413310455431,
      0.40586336286728564,
      0.4841625896684163,
      0.4958587477062302,
      0.41038845845235183,
      0.46927018627609063,
      0.334094716196514,
      0.5153901725444248,
      0.44371838907147865,
      0.3554603437438866,
      0.4217877096026579,
      0.22651155136107692,
      0.4410045163269962,
      0.4931744123919477,
      0.35505225584565436,
      0.41225556746429515,
      0.4714765025678431,
      0.5250780078768471,
      0.39932074252713656,
      0.4463756045147462,
      0.0,
      0.3322478564623179,
      0.365205415602351,
      0.3453304613669166,
      0.4283536647862294,
      0.36854455719961643,
      0.4057182204625742,
      0.3587813138998954,
      0.2629920941009818
    ],
    [
      0.26338297016568313,
      0.41174024894863903,
      0.3666048477898003,
      0.47306027500846537,
      0.38282892056806817,
      0.3793523926873874,
      0.31991627857385985,
      0.3827815578876317,
      0.37034667850524916,
      0.3759800401660862,
      0.39516106012886243,
      0.2817918377257018,
      0.4026361572774997,
      0.41733903816405604,
      0.40504963378128944,
      0.3407828429819437,
      0.3148963900571564,
      0.3426971961217049,
      0.35431192144584345,
      0.4431852457792831,
      0.3573079886610697,
      0.0,
      0.33747822657196225,
      0.4674472120499529,
      0.3062580369511956,
      0.47369501378018564,
      0.30562788240511907,
      0.3279916684770978,
      0.37405989067801193
    ],
    [
      0.3966152892128394,
      0.5381694145839686,
      0.5057366117345206,
      0.4962292276897837,
      0.5102175718112847,
      0.502969694350389,
      0.515642282035049,
      0.5102943107018489,
      0.4720024121873938,
      0.45936908531205356,
      0.5622264011402527,
      0.36724947348081627,
      0.46018081758555773,
      0.5248096073806214,
      0.5422698228591283,
      0.4463165239379443,
      0.5288723019540065,
      0.5019116056923631,
      0.47560502948154637,
      0.49754274094901163,
      0.5865529188171716,
      0.46569594509697376,
      0.0,
      0.49090895156829606,
      0.44988715301292403,
      0.46863729622684924,
      0.5762033251088807,
      0.4526199272508997,
      0.45202301210791385
    ],
    [
      0.27712308287330467,
      0.36162662555905345,
      0.4303450191035043,
      0.38845224749727447,
      0.4190840492339327,
      0.3439281775525147,
      0.3538743019821664,
      0.4315098809103304,
      0.38785612397583624,
      0.35303308779727893,
      0.40058612599569243,
      0.28800413653361767,
      0.3817965098452327,
      0.4376164196712964,
      0.35620886364696736,
      0.37410443280771966,
      0.33431694782509114,
      0.39241617668465767,
      0.3704477688335044,
      0.4089714739960477,
      0.38685360743772934,
      0.4507503605129768,
      0.3664289251669759,
      0.0,
      0.3224685865627077,
      0.37535849844959324,
      0.3338073650173954,
      0.3456640954376602,
      0.34753432101193127
    ],
    [
      0.3106310351463024,
      0.36129616720361857,
      0.46251155318669235,
      0.4062884861099454,
      0.4930603158624405,
      0.4197055018142961,
      0.3413046347827706,
      0.43260035802767627,
      0.4461614463445507,
      0.4050239183953279,
      0.3579181573826624,
      0.21918126660615123,
      0.4522183370819626,
      0.37364340479900227,
      0.3351124435575614,
      0.4003893683525577,
      0.38042358943223187,
      0.4173250003437847,
      0.40046273389908427,
      0.3985353382297889,
      0.45326630558474035,
      0.3356404488200666,
      0.35119275548589357,
      0.3506447921885574,
      0.0,
      0.34207191691505323,
      0.3792436187959951,
      0.31982320521428864,
      0.2821781475728846
    ],
    [
      0.2630528271098922,
      0.3918806206561649,
      0.38484798883322857,
      0.42976060697013363,
      0.40266842405550407,
      0.35263994196186244,
      0.3160862802247624,
      0.407411590736551,
      0.41315833329596807,
      0.34986921497026335,
      0.3852998847931808,
      0.24265202188619273,
      0.3950756598582188,
      0.40079890217554337,
      0.36123415871931486,
      0.37042716303530887,
      0.32184081811261667,
      0.364216704475256,
      0.32063786416044304,
      0.4480346984349868,
      0.33735115670014393,
      0.4107389072947045,
      0.33512556148854933,
      0.41606913437276516,
      0.29628884239379194,
      0.0,
      0.3163806448145634,
      0.29497528744527224,
      0.3195023714627965
    ],
    [
      0.31750534317522106,
      0.4337046111404377,
      0.4514179474316904,
      0.42094019471380717,
      0.4108675662261758,
      0.43476038123022254,
      0.38202449683722617,
      0.4032118409341434,
      0.45085975098905484,
      0.35301960336954163,
      0.43123423559613205,
      0.27138060998130165,
      0.42427552178896266,
      0.45192052287595486,
      0.43544992430541307,
      0.3861546711490045,
      0.43034078871228476,
      0.45865610340278296,
      0.41384954477664904,
      0.43249240832691593,
      0.48607148400292943,
      0.3176178829282825,
      0.41955758058551695,
      0.33438339990234733,
      0.40328915850075275,
      0.3593089650359762,
      0.0,
      0.35351431111637055,
      0.30981914684533285
    ],
    [
      0.3008144521535767,
      0.37566301963385684,
      0.4083795182266927,
      0.3842463124070039,
      0.42859426140273893,
      0.4220622316819558,
      0.36162219005042706,
      0.4362271073515136,
      0.4333023291950371,
      0.3422141223940416,
      0.3709106560094464,
      0.2900448957348192,
      0.38082059952119507,
      0.3587459349489257,
      0.3815582534519044,
      0.42070793699853914,
      0.4088770567534288,
      0.3920759428742573,
      0.4053931361842562,
      0.42068766251858425,
      0.44767972290339286,
      0.37163022780445276,
      0.38572887171143133,
      0.4159354418141916,
      0.3721187229750609,
      0.3378639797529561,
      0.3905187699274397,
      0.0,
      0.33998255097621644
    ],
    [
      0.3160318872359609,
      0.41979301514961653,
      0.3524448235313018,
      0.36907690381221325,
      0.3264861623874369,
      0.36002407278096293,
      0.3512697023319822,
      0.36527649674587326,
      0.36072039860835203,
      0.33669387997181066,
      0.39953476996546256,
      0.35633053576170415,
      0.3750471699693536,
      0.42994019208393297,
      0.39618995280254077,
      0.3838228130882917,
      0.3427556380272092,
      0.3471024191158367,
      0.34748740877767803,
      0.36843074408761134,
      0.31990580991592776,
      0.4885582325881339,
      0.36847050169122797,
      0.379569414466735,
      0.2992009857769329,
      0.3643720849527523,
      0.35910094980146035,
      0.3668733926994261,
      0.0
    ]
  ],
  "row_avgs": [
    0.2010186667777662,
    0.3802137650503993,
    0.5127884963613445,
    0.4487685854252143,
    0.4229931417011174,
    0.4653739994520415,
    0.41982651789614117,
    0.39470107085555356,
    0.45990663110709695,
    0.3175645620411845,
    0.38625491344363116,
    0.2172053678045386,
    0.44988896996456107,
    0.42370682186866476,
    0.48567614656040753,
    0.42599494390810744,
    0.41958329114244003,
    0.4774803405149236,
    0.341600781294118,
    0.41945530056511277,
    0.4035913839975823,
    0.37048969476210025,
    0.491312812616796,
    0.3721488289972141,
    0.37956622311199606,
    0.35885805751564215,
    0.3992009998528726,
    0.3851573538341908,
    0.3660896556474188
  ],
  "col_avgs": [
    0.31635056219666147,
    0.4503270672826856,
    0.4470066706977823,
    0.45267596824751444,
    0.4169701846814339,
    0.42444341227030785,
    0.35655938680381816,
    0.43231269254907334,
    0.45319378099387597,
    0.37246199153880005,
    0.43909526298074614,
    0.2666803636429711,
    0.44233686099377006,
    0.46478237096002456,
    0.42133015173516164,
    0.42331326421610355,
    0.3887257845141515,
    0.42791705853743883,
    0.3892134516246955,
    0.45469488488099297,
    0.4257251469042801,
    0.3818497455557851,
    0.365156433993769,
    0.38543460608907215,
    0.3641414474553219,
    0.3944725895751154,
    0.37574074142021663,
    0.33871223267312567,
    0.324793209055482
  ],
  "combined_avgs": [
    0.2586846144872138,
    0.4152704161665425,
    0.4798975835295634,
    0.45072227683636434,
    0.4199816631912756,
    0.4449087058611747,
    0.38819295234997964,
    0.4135068817023134,
    0.45655020605048646,
    0.3450132767899923,
    0.4126750882121887,
    0.24194286572375484,
    0.44611291547916554,
    0.44424459641434466,
    0.45350314914778456,
    0.42465410406210546,
    0.4041545378282958,
    0.45269869952618125,
    0.36540711645940677,
    0.4370750927230529,
    0.4146582654509312,
    0.3761697201589427,
    0.4282346233052825,
    0.3787917175431431,
    0.371853835283659,
    0.37666532354537874,
    0.3874708706365446,
    0.36193479325365824,
    0.3454414323514504
  ],
  "gppm": [
    634.822369487284,
    596.2335117185287,
    598.1134346952592,
    594.6440722867322,
    608.494813259689,
    606.552535245765,
    640.2830715604283,
    604.8068810717459,
    593.6756782277631,
    627.5370966065811,
    602.5980106900588,
    677.4568010153945,
    598.571760549782,
    590.3546956951908,
    611.1953891489941,
    605.715693989907,
    624.5398375408271,
    607.8892111700469,
    625.7390467054599,
    590.8750769324089,
    607.8457751874666,
    625.8163119831844,
    636.1172037729967,
    626.1323353381401,
    634.723180704207,
    618.3829665057228,
    628.9822880278865,
    649.4700489677999,
    655.1547787707549
  ],
  "gppm_normalized": [
    1.4643236948589682,
    1.3529382592269064,
    1.3550285328749088,
    1.343453031819189,
    1.3739643933128887,
    1.3749988829987028,
    1.455419295197766,
    1.366578421448948,
    1.3413003536437245,
    1.422889635141724,
    1.3647269437468392,
    1.5396996743235463,
    1.3553729501575664,
    1.3335664349160754,
    1.3833229527855546,
    1.3736742696575621,
    1.4110547202944161,
    1.3690289904456763,
    1.4139006045378981,
    1.3414620165167335,
    1.3653378437317498,
    1.415363300678123,
    1.4349316787266961,
    1.417821143542565,
    1.4337428947447388,
    1.4000533192358062,
    1.4230119608262832,
    1.4666544611300936,
    1.4848901407711446
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364,
    668,
    476,
    491,
    435,
    399,
    469,
    505,
    387,
    435,
    421,
    416,
    456,
    451,
    447,
    366,
    429,
    444,
    439,
    432,
    365,
    436,
    383,
    403,
    437,
    433,
    512,
    444,
    453,
    398,
    556,
    436,
    455,
    520,
    439,
    403,
    443,
    430,
    421,
    442,
    402,
    469,
    440,
    412,
    457,
    437,
    417,
    397,
    430,
    478,
    440,
    351,
    389,
    390,
    417,
    413,
    412,
    418,
    373,
    589,
    405,
    404,
    436,
    416,
    426,
    434,
    447,
    434,
    406,
    406,
    524,
    449,
    391,
    456,
    425,
    488,
    400,
    372,
    431,
    460,
    423,
    389,
    472,
    413,
    395,
    393,
    447,
    357,
    756,
    505,
    437,
    451,
    455,
    446,
    567,
    465,
    461,
    428,
    451,
    350,
    461,
    430,
    486,
    497,
    420,
    414,
    408,
    431,
    401,
    373,
    409,
    440,
    446,
    377,
    441,
    425,
    432,
    719,
    452,
    465,
    403,
    440,
    476,
    406,
    461,
    445,
    427,
    419,
    561,
    443,
    439,
    402,
    436,
    435,
    404,
    421,
    436,
    431,
    405,
    404,
    434,
    436,
    444,
    457,
    417,
    387,
    351,
    464,
    488,
    447,
    414,
    460,
    562,
    459,
    454,
    404,
    403,
    553,
    453,
    418,
    415,
    449,
    402,
    424,
    399,
    466,
    446,
    424,
    385,
    459,
    452,
    465,
    385,
    416,
    392,
    342,
    477,
    430,
    439,
    431,
    390,
    446,
    418,
    476,
    458,
    445,
    482,
    400,
    421,
    367,
    433,
    391,
    423,
    418,
    406,
    419,
    375,
    418,
    446,
    434,
    405,
    391,
    455,
    335,
    644,
    478,
    383,
    457,
    433,
    418,
    414,
    425,
    445,
    497,
    452,
    561,
    425,
    467,
    414,
    446,
    408,
    369,
    426,
    441,
    395,
    413,
    359,
    434,
    409,
    481,
    425,
    390,
    353
  ],
  "response_lengths": [
    3271,
    2604,
    2130,
    2506,
    2501,
    2384,
    2361,
    2262,
    2480,
    2821,
    2442,
    3207,
    2392,
    2487,
    2250,
    2500,
    2253,
    2034,
    2438,
    2537,
    2163,
    2257,
    1951,
    2371,
    2308,
    2616,
    2383,
    2134,
    1974
  ]
}