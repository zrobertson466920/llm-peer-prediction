{
  "example_idx": 92,
  "reference": "Published as a conference paper at ICLR 2023\n\nON THE USEFULNESS OF EMBEDDINGS, CLUSTERS AND STRINGS FOR TEXT GENERATOR EVALUATION\n\nTiago Pimentel∗,1, Clara Meister∗,2, Ryan Cotterell2 1 University of Cambridge, tp472@cam.ac.uk, {clara.meister, ryan.cotterell}@inf.ethz.ch\n\n2 ETH Zürich\n\nABSTRACT\n\nA good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality. Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is the recently proposed MAUVE. In theory, MAUVE measures an informationtheoretic divergence between two probability distributions over strings: one representing the language generator under evaluation and the other representing the true natural language distribution. MAUVE’s authors argue that its success comes from the qualitative properties of their proposed divergence. Yet in practice, as this divergence is uncomputable, MAUVE approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pre-trained language model’s embeddings. As we show, however, this is not a tight approximation—in either theory or practice. This begs the question: why does MAUVE work so well? In this work, we show that MAUVE was right for the wrong reasons, and that its newly proposed divergence is not necessary for its high performance. In fact, classical divergences paired with its proposed cluster-based approximation may actually serve as better evaluation metrics. We finish the paper with a probing analysis; this analysis leads us to conclude that—by encoding syntactic- and coherence-level features of text, while ignoring surface-level features—such cluster-based substitutes to string distributions may simply be better for evaluating state-of-the-art language generators.1\n\n1\n\nINTRODUCTION\n\nProbabilistic text generators have improved greatly over the last years, with models producing increasingly human-like text (Yang et al., 2019; Brown et al., 2020; Raffel et al., 2020; Rae et al., 2021; Hoffmann et al., 2022). As the gap between human and model-generated text closes, the quality of our evaluation metrics becomes ever more important for determining generator quality, especially given the increasing number of user-facing systems employing these generators. While human evaluations serve as the gold standard, they are costly (in both time and money), leading researchers to rely on automatic metrics—i.e., metrics that can be measured by a computer—for the bulk of their development process.\n\nMany automatic language generator evaluation metrics share the same underlying mechanism: the quantitative comparison of two probability distributions. Specifically, most metrics measure a difference between the distributions over strings defined by: (1) a language generation model2 and (2) the natural language itself. This includes some of the most widely used language evaluation metrics:3 cross-entropy (Shannon, 1948), perplexity (Jelinek et al., 1977), and (more recently) MAUVE (Pillutla et al., 2021). As typically applied to evaluate language generators, however, these metrics have a number of computational and qualitative issues (discussed in §3). Such issues manifest empirically: the most commonly used automatic metrics are known to correlate poorly with human judgements (Wiseman et al., 2017; Reiter, 2018; Sellam et al., 2020; Gehrmann et al., 2021).\n\n∗Equal contribution. 1Code available at https://github.com/rycolab/clusters-in-language-evaluation. 2We define a language generator as a probability distribution qw over strings w. Specifically, we consider this distribution as used during generation. E.g., if decoding is performed with nucleus sampling, we consider the final distribution where every sentence with tokens not in the nucleus is assigned a probability of 0.\n\n3 Most measures we consider are not metrics in a strict sense; we use the term “metric” out of convention.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nA newly proposed metric stands apart: MAUVE (Pillutla et al., 2021). In theory, MAUVE measures the area under the curve formed by the divergence between two probability distributions, qualitatively mimicking a precision–recall quantification (Djolonga et al., 2020; Kynkäänniemi et al., 2019). The authors attribute the success of their metric to the qualitative properties of this new class of divergences. Yet, due to this divergence being in practice uncomputable, Pillutla et al. propose an approximation to it. Specifically, rather than directly comparing the original two distributions over strings, MAUVE first clusters samples taken from these distributions based on the embeddings of a pre-trained language model; it then estimates the proposed divergence using the samples’ empiricallyobserved multinomial distributions over cluster assignments. As we will show, this approximation is bad in both theory §4 and practice §5.1—to the point that the term “approximation” is arguably a misnomer. Thus, the reasons why MAUVE works well—knowledge which is important for the continued progress of language generator evaluation metrics—are still unknown.\n\nIn this work, we aim to uncover these reasons. To this end, we consider the axes on which MAUVE differs from other evaluation metrics: is MAUVE’s success due to the new divergence metric, to its “approximation”, or both? Empirically, we identify MAUVE’s substitution of probability distributions over strings with probability distributions over embedding-based clusters as the main factor for its success. We show that mathematically, this substitution leads to a quite biased estimator of the original string-based divergences. Yet it also leads to metrics with lower variance and stronger correlations with human judgements. In fact, all divergence measures analysed here correlate more strongly with human judgements when cluster-based distributions are used in place of string-based ones.\n\nFinally, in order to understand the root of the effectiveness of these cluster-based metrics, we probe the clusters themselves. We find that sentence-level permutations within texts noticeably affect cluster assignments, suggesting that cluster-based metrics are susceptible to attributes such as coherence. On the other hand, basic manipulations that render text unhuman-like, such as removing all articles from the input text, do not seem to affect these metrics significantly. Together, these results lead us to conjecture that embedding-based metrics may be favourable when estimating the quality of state-of-the-art (SOTA) language generators, as SOTA models are known to (at least typically) produce grammatical text. That is, by ignoring surface-level features of text—while emphasising discourse- and coherence-level ones—clustered embeddings may simply be better suited for the evaluation of the top language generation systems. Yet these findings also suggest routes through which such metrics can be gamed, bringing into question their robustness. We believe these findings, along with the theoretical framework we provide for evaluation metrics’ comparison, are important for the further development of language generator evaluation metrics.\n\n2 DIVERGENCE METRICS FOR LANGUAGE GENERATOR EVALUATION\n\nWhen evaluating language generation systems, we will first assume the existence of an unknown ground-truth distribution pw. This distribution is defined over strings w and its domain spans W ≡ Σ∗, where Σ is an alphabet of words and Σ∗ is its Kleene closure. Second, we are given a probabilistic text generator qw, which is also a distribution over W. An evaluation metric for a language generator qw can now be defined as a measure of its “distance” from pw: ∆(pw, qw). In short, ∆(·, ·) should return high values if qw is a bad approximation to pw, and it should return low values if it is a good one.\n\nNotably, it is not clear whether qw being a good approximation to pw in terms of an arbitrary ∆(·, ·) guarantees that it will be a good language generator. Indeed, models that perform well in terms of standard metrics, such as perplexity, often still produce poor-quality text (Holtzman et al., 2020). Thus, we are interested specifically in ∆(·, ·) that correlate highly with human quality judgements.\n\nMore formally, we define human quality judgements as a (potentially noisy) mapping α(qw) from a language generator to a real-valued score. For fixed pw, a useful metric ∆(pw, ·) for evaluating the quality of a language generator qw is one whose scores correlate highly with humanscores(·). This notion can be operationalised as follows. Assume we have N language generator models. Let us define:\n\nδhuman(q(1)\n\nw , . . . , q(N )\n\nw ) =\n\nδmetric(q(1)\n\nw , . . . , q(N )\n\nw ) =\n\n(cid:104)\n\n(cid:104)\n\nα\n\n∆\n\n(cid:17)\n\n(cid:16)\n\nq(1)\n\nw\n\n, . . . , α\n\n(cid:17)(cid:105)\n\n(cid:16)\n\nq(N )\n\nw\n\n(cid:16)\n\npw, q(1)\n\nw\n\n(cid:17)\n\n, . . . , ∆\n\n(cid:16)\n\npw, q(N )\n\nw\n\n(cid:17)(cid:105)\n\n(1)\n\n(2)\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nWe then quantify a metric’s usefulness on a specific natural language task (and its distribution pw) as:\n\nquality(∆, pw) = |corr (δhuman, δmetric) |\n\n(3)\n\nWe now review common choices for ∆(·, ·). Given the probabilistic nature of most language generators, a number of divergence measures are among these choices, which quantify the difference between two probability distributions.4 The rest of this work focuses primarily on this class of metrics.\n\nForward Divergence. Cross-entropy, ∆H(pw, qw) def= H(pw, qw), which is equivalent (up to an additive constant) to the forward Kullback–Leibler (KL) divergence, is one such choice:\n\n∆→(pw, qw) def= KL(pw || qw) = H(pw, qw) − H(pw)\n\n(1) (cid:47) H(pw, qw) = ∆H(pw, qw)\n\n(4)\n\nwhere we use (cid:47) to signify additive or multiplicative equivalence. (1) is true since H(pw) is constant with respect to qw. Since Pearson and Spearman correlations—the metrics we use to evaluate ∆’s quality—are invariant to translational shifts, the cross-entropy and forward KL are equivalent as language generator metrics. We will refer to them interchangeably during subsequent comparisons.\n\nBackward Divergence. Albeit much less common, another potential evaluation metric would be the backward (exclusive) KL divergence:\n\n∆←(pw, qw) def= KL(qw || pw) As opposed to the forward KL, when use as an evaluation metric, Eq. (5) is not effectively equivalent to the cross-entropy between qw and pw, as H(qw) is not constant across language generators qw.\n\n(5)\n\nExponentiated Divergence. By far, the most common choice of ∆ to evaluate language models is the perplexity: ∆perp(pw, qw) def= eH(pw,qw). Notably, perplexity is equivalent (up to a multiplicative constant) to an exponentiated Kullback–Leibler divergence between pw and qw, which follows from the same relationship as in Eq. (4). Given the property that both Pearson and Spearman correlations are invariant to a change in scale, the perplexity and exponentiated KL will thus be equivalent as language generator metrics. For consistency, we will use solely the exponentiated KL in our analyses:\n\n∆exp(pw, qw) def= eKL(pw||qw)\n\n(6)\n\nJensen–Shannon Divergence. Note that the KL divergence is non-symmetric and unbounded. On the other hand, the Jensen–Shannon (JS) divergence—defined as the average of two KLs—is symmetric with respect to its inputs and is guaranteed to produce bounded values:\n\n∆JS(pw, qw) def=\n\n1 2\n\n(cid:0)KL(pw || r.5\n\nw ) + KL(qw || r.5\n\nw )(cid:1) ,\n\nrλ w = λ pw + (1 − λ) qw\n\n(7)\n\nArea Under the Curve (AUC) Divergence. Finally, information divergence frontiers are a recently proposed class of metrics for generative models (Sajjadi et al., 2018; Kynkäänniemi et al., 2019). The variant proposed by Pillutla et al. (2021) computes the area under the curve formed by a series of Kullback-Leibler divergences as we change a mixing parameter λ:\n\n∆AUC(pw, qw) = 1 − AUC\n\n(cid:16)\n\ne−s KL(pw||rλ\n\nw), e−s KL(qw||rλ\n\nw)(cid:17)\n\n,\n\nrλ w = λ pw + (1 − λ) qw (8)\n\nwhere λ is varied across the interval [0, 1], and s ∈ R>0 is a strictly positive real-valued scaling constant. Note that we define the AUC divergence as 1 − AUC(·, ·) so that a larger value indicates a greater discrepancy with the reference corpus pw.\n\n3\n\nINFELICITIES AND APPROXIMATIONS\n\nThere are several issues, both computational and qualitative, with using the divergences presented in §2 to evaluate language generators. We now review these issues, along with both commonly-used and newly-proposed methods to address them via approximations.\n\n4Here we make use of shifted divergences: a divergence measure that potentially has an additive constant, i.e., there exists a constant c ∈ R such that ∆(·, ·) + c is a divergence. For our purposes, an additive constant should not affect the quality of our metrics, as the correlation in Eq. (3) is translation-invariant.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3.1 NECESSITY OF FULL SUPPORT\n\nA well-known property of the (forward) KL divergence between two distributions pw and qw is that it is infinite for any qw that assigns 0 probability to an event in the support of pw (i.e., for which pw(w) > 0). The above is often not an issue for ∆exp and ∆→: most neural language generators cannot assign 0 probability to any string due to the final softmax operation typically used to project their outputs onto the probability simplex. However, these same models are often used with decoding strategies that prune the space W: e.g., both top-k and nucleus sampling modify qw such that strings which do not meet a certain criterion are reassigned 0 probability. While top-k and nucleus sampling typically lead to systems with qualitatively better text, they will likely be given an infinitely bad score by both ∆exp and ∆→, which is perhaps too harsh a penalty for an otherwise good language generator.\n\n3.2\n\npw IS UNKNOWN\n\nn }N\n\nIn practice, we do not have access to the true distribution pw. Rather, we are typically given a corpus {wpw n=1, whose instances we assume to be sampled i.i.d. from pw. The common approach to address this issue is thus to derive a statistical estimator (cid:98)∆ that uses this corpus to approximate ∆. There are two common strategies for building such estimators: Monte Carlo and plug-in estimation.\n\nMonte Carlo Estimation. Our i.i.d. assumption w.r.t. samples in {wpw Monte Carlo estimator for certain divergences. We start with the forward KL divergence:\n\nn=1 allows us to derive a\n\nn }N\n\n(cid:99)KL(pw || qw) def=\n\n1 N\n\nN (cid:88)\n\nn=1\n\nlog\n\npw(wpw n ) qw(wpw n )\n\n= −\n\n1 N\n\nN (cid:88)\n\nn=1\n\nlog qw(wpw\n\nn ) + const\n\n(9)\n\nwhere const ∈ R is constant with respect to qw. Eq. (9) is an unbiased estimator of KL divergence, which in turn allows us to build estimators (cid:98)∆→ and (cid:98)∆exp. Unfortunately, unbiased estimates of ∆←, ∆JS and ∆AUC are not as straightforward to compute, as they require explicit knowledge of pw rather than just samples (see App. A). This issue motivates the use of our next set of estimation techniques.\n\nPlug-in Estimation. Here we consider estimation via building an approximation of pw itself to use in the formulas given in §2. Specifically, we construct a density estimator for pw (which we denote as (cid:98)pw) and “plug it into” a given ∆.5 However, this is a bit circular: the task of building a language generator qw itself is often framed as density estimation of pw. Thus, if we think qw is the “best” estimator for pw, we should logically use it in our plug-in estimator. Yet, using qw would be nonsensical; by the definition of a (shifted) divergence, it would always lead to the lowest possible value of ∆, e.g., ∆→(qw, qw) = 0. To use plug-in estimation in this setting, we should therefore choose a different estimator for pw, e.g., from a family of density estimators that differs from those used to create qw. More formally, we consider a function π which takes a corpus as input and produces a (queryable) distribution (cid:98)pw n=1). This function typically induces a secondary model, e.g., an n-gram model or neural network, trained on the corpus {wpw\n\ndef= π({wpw\n\nn }N\n\nn }N\n\nn=1.\n\nOur chosen π may introduce biases (e.g., from the inductive biases of the architecture parameterising π) into our metrics’ estimation. To balance out such biases, we may consider using the same method to create an approximation (cid:98)qw for use in our plug-in estimators, rather than directly querying qw:\n\n(cid:98)∆←({wpw\n\nn }N\n\nn=1 , qw) def= (cid:99)KL((cid:98)qw || (cid:98)pw)\n\n(10)\n\nPlug-in estimators for ∆JS and ∆AUC are defined similarly. Further, if (cid:98)qw is a smoothed approximation to the original qw, using it may also mitigate the issues discussed in §3.1. We thus also compute estimators for the forward/exponentiated divergences using plug-in estimators, e.g.,:\n\n(cid:98)∆→({wpw\n\nn }N\n\nn=1 , qw) def= −\n\n1 N\n\nN (cid:88)\n\nn=1\n\nlog (cid:98)qw(wpw n )\n\n(11)\n\n5Often, plug-in and Monte Carlo estimators must be used together. Even if we are measuring the divergence\n\nbetween two queryable distributions, the sum over W is infinite and non-decomposable, thus uncomputable.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nUnfortunately, most functions π cannot produce a good estimate of pw using only a small corpus, which is the case we consider since we rely on evaluation sets for {wpw n=1. While the best available language models are a class of π typically trained on millions (if not billions) of sentences, a standard evaluation set is quite small—on the order of one to ten thousand sentences—and we cannot expect π to provide a good (cid:98)pw when fit using only such a small dataset. Accordingly, depending on our choice of π, this class of metrics may be either high variance or high bias, both of which are problematic.\n\nn }N\n\n3.3 CLUSTERING-BASED APPROXIMATIONS\n\nFor the (cid:98)∆ above that require density estimators for pw and/or qw, our choice of π will have a large effect on its value. We may thus wish to rethink our approximation technique altogether, and instead work with different distributions for which we can create lower variance density estimators. This is the approach used by Pillutla et al. (2021) when approximating ∆AUC. Specifically, instead of computing the above metrics on the original distributions pw and qw, they use the cluster-based distributions pc and qc. Given a pre-trained language model, these cluster-based distributions are defined as:\n\npc(c) =\n\n(cid:88)\n\nw∈W\n\npw(w) 1\n\n(cid:110)\n\nc = φ(PLM(w))\n\n(cid:111)\n\n(12)\n\nwhere PLM(·) takes as input an utterance w and outputs an embedding r, and φ(·) is a pretrained clustering function. Note that the function φ(·) is trained jointly on samples from the two distributions under consideration as we will detail later; we defer the reader to Pillutla et al. (2021) for a more detailed explanation of the procedure. Given these distributions, we can evaluate cluster-based versions of all the divergences above, simply by substituting the original pw and qw with the new pc and qc.\n\n4 ANALYSING CLUSTER-BASED APPROXIMATIONS\n\nWe now take a closer look at the biases introduced by the substitution of cluster-based distributions suggested in §3.3. For simplicity, we focus on the bias introduced to KL(pw || qw)—a computation involved in MAUVE’s ∆AUC. This divergence can be decomposed as:\n\nKL(pw || qw) (cid:124) (cid:125) (cid:123)(cid:122) string−based KL\n\n(1) = KL(p(c) || q(c)) + KL(p(w | c) || q(w | c)) (cid:125) (cid:124)\n\n(cid:123)(cid:122) ≥0\n\n≥ KL(pc || qc) (cid:123)(cid:122) (cid:125) (cid:124) cluster−based KL\n\n(13)\n\nwhere (1) follows from the fact that p(c, w) = p(w), which is true because the cluster assignment is deterministic, i.e.: p(c | w) = 1{c = φ(PLM(w))}. See the full decomposition of this equation in App. B. Notably, as KL divergences are always non-negative, the cluster-based version is negatively biased, lower-bounding the string-based one. Further, the actual measurement is done on the distribution over cluster assignments p(c); the distribution p(w | c) is completely ignored.\n\nAssuming a reasonable number of clusters is used when defining pc, however, it should be easier to create good approximations of distributions over clusters than distributions over strings due to the sheer difference in the size of the supports alone. Consequently, the variance of cluster-based metrics should be lower, at the cost of the bias introduced by this substitution. Further, it is not clear whether this bias is inherently bad when evaluating the quality of language generators: the answer to this question must be determined empirically (by measuring the correlation in Eq. (3)). To this end, we now provide an empirical comparison between string- and cluster-based language generation evaluation.\n\n5 EXPERIMENTS\n\nSetup. We follow the setup of Pillutla et al. throughout our experiments. We compare systems for open-ended text generation qw with human-generated text pw. As human-generated samples, we use 5k strings taken from WebText’s test set. As model-generated text, we sample 5k strings from each of our evaluated systems, conditioning our models on the first 10 words of human-generated strings before sampling (i.e., a text-completion task). For our language generators, we compare 4 model architectures (all variants of GPT-2), each under two decoding strategies, giving us a total of 8 systems. Explicitly, we compare the small, medium, large, and XL versions of GPT-2, decoding strings using either ancestral or nucleus sampling. Following Pillutla et al. (2021), we use a nucleus\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nprobability of 0.9 for small and medium, while 0.95 for large and XL GPT-2’s. Importantly, we run our experiments only on English text, which is a notable limitation of our work; future work should verify that findings hold across languages.\n\nString-based Approximations (cid:98)pw. To compute our string-based divergences, we require a secondary language model (cid:98)pw to estimate pw. Further, following the issues highlighted in §3, we will also rely on a secondary language model (cid:98)qw to estimate qw. We will use n-gram models for these approximations. Specifically, we use Kneser-Essen-Ney smoothed 5-gram models, as implemented in KenLM (Ney et al., 1994; Heafield, 2011). We choose n-gram models explicitly because—while they are by no means SOTA language models—they should have inductive biases which are different from the models we are trying to evaluate. We present results using LSTM-based estimators in App. E. When computing ∆AUC, we use a scaling constant s of 0.2.\n\nCluster-based Approximations (cid:98)pc. Cluster-based distributions, as presented in Eq. (12), are defined by a choice of PLM(·) and pre-trained clustering function φ(·). We rely on GPT-2 XL as our PLM, and use K-means as our clustering function; results using other pre-trained language models are in App. E. Specifically, we first extract embeddings from the last word in each sentence using GPT-2 XL and then use PCA to reduce their dimensionality (keeping 90% of the original variance explained); results using mean of word embeddings can be found in App. E. We then train K-means (with K = 500) on a joint set of GPT-2 embeddings extracted from: the 5k human-generated strings, and 5k model-generated sentences. Finally, we approximate (cid:98)pc and (cid:98)qc by computing the frequency with which strings (among these 5k used ones) are assigned to each cluster. To avoid infinite divergence measures, we estimate distributions using Laplace smoothing with α = 1 (which is equivalent to imposing a Dirichlet distributed prior with α = 1 over the cluster allocation). When computing ∆AUC, we use a scaling constant s of 5.67\n\n5.1 DOES pc APPROXIMATE pw?\n\nOur first experiment tries to identify whether pc and qc provide faithful approximations of pw and qw. To this end, we compare both (cid:98)qw and (cid:98)qc to the true qw, i.e., the language generator under evaluation. Explicitly, we compute the Spearman correlations between the probabilities assigned by each model to the strings in {wqw\n\nn }N\n\nn=1.\n\nFig. 1 presents these correlations. We see that—despite being estimated on very little data—probability estimates from our n-gram models correlate strongly with the ground-truth probabilities of qw; this result holds for all four GPT-2 architectures. On the other hand, our cluster-based probabilities consistently present negative correlations with qw. This result has an important implication: if cluster distributions do not correlate with qw, then KL((cid:98)pc || (cid:98)qc) is likely a poor estimate of KL(pw || qw). This further implies that the approximation used by Pillutla et al. is not an accurate estimate of ∆AUC(pw, qw), which brings into question whether this new divergence is really responsible for MAUVE’s success.\n\nFigure 1: Correlations between the true qw and the estimated (cid:98)qc and (cid:98)qw.\n\n5.2 ∆ AS TEXT EVALUATION METRICS\n\nWe now compare how various string- and cluster-based divergence measures correlate with human judgement scores.8 In short, Fig. 2 shows that all divergences do better when estimated with cluster distributions. These results evince that MAUVE’s (Pillutla et al., 2021) high correlations with human judgements (represented here as (cid:98)∆AUC(pc, qc)) are mainly due to their use of clusterbased approximations (pc, qc), rather than to their proposed divergence ∆AUC. In fact, we see\n\n6We ran our entire pipeline (from sampling strings {wqw\n\nn }N\n\nn=1 from qw to evaluating the KLs) with 5\n\ndifferent seeds. The variance across seeds is depicted as error bars in Figs. 1 and 2.\n\n7We follow Pillutla et al.’s (2021) experimental setup here. Note that Pillutla et al. provide an in-depth\n\nanalysis of experimental design choices and show MAUVE is robust to various hyperparameter choices.\n\n8We use the human judgement scores collected by Pillutla et al. (2021).\n\n6\n\nsmallmediumlargexlModel50250255075Correlation (%)qcqwPublished as a conference paper at ICLR 2023\n\n(a) Interestingness\n\n(b) Sensibility\n\n(c) Human likeness\n\nFigure 2: Correlations between string- and cluster-based divergences with human judgement scores. Legend: ∆exp in dark green; ∆→ in orange; ∆← in blue; ∆JS in pink; ∆AUC in lime green.\n\nslight improvements over (cid:98)∆AUC when using the divergences (cid:98)∆← and (cid:98)∆JS instead. Furthermore, cluster-based divergences appear to be more stable, exhibiting smaller variances across random seeds. Collectively, our results suggest that cluster-based divergences may produce better metrics of text quality than string-based divergences. This motivates the question: Which components of natural language are captured by cluster-based distributions pc, and which are overlooked by ignoring p(w | c) when computing the cluster-based divergences? Our next set of experiments aim to answer this.\n\n6 PROBING CLUSTERS\n\nTo better understand the aspects of natural language that our cluster distributions encode, we must first understand how φ(PLM(·)) partitions the string space W. In other words, we must understand what components of natural language—e.g., semantics, syntactic attributes, or surface features—lead to strings being assigned to similar or different clusters. Such an analysis should provide a deeper insight into the actual similarity being measured by cluster-based divergences (while also revealing how such a metric might be gamed). To this end, we probe (Alain & Bengio, 2016) the clusters learned by the MAUVE algorithm for a number of linguistic attributes—including subject matter, sentiment, prose style, word order, basic grammaticality and document length—looking at how they affect both cluster assignment and the divergence scores between texts that differ in these characteristics. Notably, we probe cluster assignments directly—without relying on any diagnostic classifiers (Adi et al., 2017). Our probing analyses are thus exempt from most recent criticism against probing methodologies (Hewitt & Liang, 2019; Pimentel et al., 2020a;b; Ravichander et al., 2021; Elazar et al., 2021).\n\n6.1 FINDING FEATURES pc ENCODES\n\nSetup. We look at texts annotated with different attribute categories in order to explore correlations between the presence of these attributes and cluster membership. Specifically, we analyse texts’: sentiment, authorship, and topic (using the Yelp Polarity, News Category, and 20 NewsGroup datasets, respectively). Further details on datasets are provided in App. D. For each of these classification datasets, we compute the cluster–category distributions that result from the MAUVE algorithm using the standard training split for the respective datasets; all evaluations are then performed on test splits. Explicitly, we first learn a partitioning φ(·) of the embedding space (w.r.t a language model PLM(·)). Each cluster is then labelled with the majority category represented in that cluster by training examples; text categories in the test set are then predicted using this labelling, depending on which of the clusters the example falls into. For comparison’s sake, we use four language models as PLM(·): GPT-2 with small, medium, large, and XL architectures. Results using embeddings from BERT (Devlin et al., 2019) can be found in App. E. Further, we use two methods for learning clusters:\n\n• φ(·) Learned on WebText. Using the same procedure as in §5.2, we train PCA and K-means functions to partition the embedding space (again relying on WebText’s test set for our data). This mimics the setting under which our partitions would be learned in practice.9\n\n• φ(·) Learned on Training Set. We instead train the PCA and K-means clustering functions on the analysed dataset’s training set. This setting studies the partitioning that our clustering functions have the capacity to learn in an ideal setting, i.e., where the attribute in question is one of the main differentiating factors between texts.\n\n9If no strings in the training set are assigned to a cluster, we label it with the overall majority category.\n\n7\n\n(pc,qc)(pw,qw)0255075100Correlation (%)(pc,qc)(pw,qw)0255075100Correlation (%)(pc,qc)(pw,qw)0255075100Correlation (%)Published as a conference paper at ICLR 2023\n\n(a) Sentiment\n\n(b) Authorship\n\n(c) Topic\n\nFigure 3: Accuracy when predicting different attributes of text from their cluster assignments. Assignments (i.e. φ(·)) are learned using text from either WebText, or the training set of the respective classification datasets. Dashed lines represent baseline accuracies, i.e., always guessing the majority class.\n\nResults. In Fig. 3a, we see that, at least for large numbers of clusters, cluster assignment is indeed indicative of a text’s sentiment. Interestingly, this is the case even when clusters are trained on data that is not particularly polar in sentiment (i.e., on WebText). On the other hand, we are only able to predict author and topic (with reasonable accuracy) when clusters are learned on text data with authorship and topic as distinguishing factors. These results indicate that, while writing style and subject matter are captured by the text embeddings, they likely were not being used as distinguishing features between corpora in our cluster-based divergences. We further see that, in all classification settings, the capacity to encode these analysed attributes appears to increase with model size, perhaps suggesting that the embedding spaces of larger models decompose along higher-level features of text.\n\n6.2 HOW TEXT FEATURES IMPACT ∆\n\nWe next assess how changing different features of our evaluated text impacts divergence scores. Specifically, we look at the impact of: text truncation; article removal; stopword removal; sentencelevel permutations; and word-level permutations.\n\nSetup. We follow a similar setup to §5. In order to create a more controlled setting, though, we primarily consider human-generated text in these experiments (i.e., the 5k human-written articles in WebText’s test set). We take the first 2500 articles of this dataset as our reference corpus {wpw n=1. We then use the remaining 2500 reference strings as the comparison corpus, i.e., in place of the model-generated text that we would typically evaluate {wqw n=1. In order to explore how changing specific features of text affects ∆ w.r.t. the reference corpus, we then compute scores when making the following modifications to the comparison corpus:\n\nn }N\n\nn }N\n\n• No modification (p). This is a baseline experiment where we keep the original strings unchanged. • Text Truncation (pshort). We truncate texts to 1/3 of their original length. This allows us to understand whether the divergence metrics pick up on differences in dataset length statistics. • Article Removal (pno art). We remove all articles (‘a’, ‘an’ and ‘the’) in the text. This allows us to understand whether the divergence metrics can distinguish between texts with or without basic levels of fluency and grammaticality.\n\n• Stopwords Removal (pno stop). We remove all stopwords (e.g., ‘that’ or ‘so’) in the text. This allows us to understand whether the divergence metrics can detect differing levels of syntactic coherence, rather than just focusing on content words.10\n\n• Sentence-level Permutation (pswap). We permute the first halves of texts (as delineated by sentences) across the entire corpus (i.e. randomly reassigning the strings’ first halves). This allows us to understand whether the divergence metrics detect coherence.\n\n• Word-level Permutation (prand). We randomly permute all words in a text. This allows us to understand whether the divergence metrics can only distinguish between bag-of-word level features. • GPT-2 Baseline (q). As an extra baseline, we also use the first 2500 generations from GPT-2\n\nXL on the WebText text-completion task, as in §5.\n\n10Stopwords are defined as common words, such as “that” or “so”, that primarily serve a syntactic function.\n\nWe use the set of English stopwords defined by NLTK (Bird et al., 2009).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Divergence measures between two corpora: the reference text is unmodified while the comparison text undergoes perturbation. Higher values indicate a greater discrepancy according to ∆.\n\nResults. Fig. 4 shows that certain alterations to the evaluated text—such as completely removing articles—have almost no impact on its divergences from the reference corpora for various ∆. In fact, text without any articles is judged as better than GPT-2 XL’s by all of the cluster-based divergences (see Fig. 10 for a zoomed-in version). Further, while this perturbation undoubtedly affects the text’s fluency, it has less of an effect on ∆ than, e.g., truncating texts. This is arguably undesirable: A metric of text quality should place more emphasis on fluency than surface statistics, such as length.\n\nOn the other hand, our metrics deem text with stopwords removed as utterly different from the reference. Permuting words within texts has a similar effect, demonstrating that, at least to some extent, the embedding space captures notions of syntax and grammaticality, rather than pure unigram statistics. The increase in ∆ shown when performing sentence-level permutations likewise suggests that the clusters delineate different levels of coherence to some extent. In Fig. 13 (in App. E), we perform an additional experiment where we again probe the clusters (as in §6.1), but for surface features of text this time, such as the percentage of stopwords and punctuation symbols in a text. There we see evidence that such features of text are not strongly encoded in the clustering scheme.\n\nPerhaps surprisingly, when applied to cluster-based distributions, all of the studied ∆ metrics rank the distance of the perturbed texts from the reference texts in the exact same order (this is clear in Fig. 10, a zoomed-in version of Fig. 4). For these perturbations, the ∆ differ only in the magnitude of their outputs, further suggesting that the ∆AUC metric itself is likely not critical for the effectiveness of MAUVE. One potential avenue for future research would be investigating whether different algorithms for discretisation of the embedding space create clusters that align with specific linguistic attributes; this could be a useful diagnostic for language generators’ improvement.\n\nThis section’s results—along with those of §6.1—suggest that divergences based on PLM embeddings are more sensitive to syntax- and coherence-related properties of the target text than to its superficial features. The opposite, however, might be said of our string-based distributions. These findings offer a potential explanation for the effectiveness of metrics that make use of PLM embeddings, such as MAUVE or BERTSCORE (Zhang et al., 2020). As current SOTA language generators already typically produce grammatical text, being invariant to surface statistics may perhaps be a feature—as opposed to a bug—when trying to assess the quality of the text they produce. Yet, this may also reveal potential ways in which such metrics can be gamed, bringing this paradigm’s robustness into question.\n\n7 CONCLUSION\n\nIn this paper, we analyse MAUVE, a recently-proposed automatic metric for language generator evaluation. While MAUVE correlates quite well with human quality judgements, it is unclear which of the metric’s design choices are in fact responsible for its success—a shortcoming that impedes the further development of language generator evaluation metrics. We attempt to rectify this shortcoming. We first provide a general theoretical framework for the comparison of language generator evaluation metrics. Then through a series of empirical studies, we identify MAUVE’s substitution of probability distributions over embedding-based clusters—in place of the traditional distributions over strings—as the attribute largely responsible for the metric’s success. In order to better understand the nature of this improvement, we probe the clusters used by the density estimators, analysing what they ignore and what they emphasise about the input text. We find that, while distributions over clusters are sensitive to syntax- or coherence-level perturbations to the text, this is not the case for several surfacelevel perturbations. We thus conjecture that, by focusing on higher-level text features, cluster-based evaluation metrics may simply be better suited to rank high-performing models, and that this is a general paradigm worth further exploration.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n8 ACKNOWLEDGEMENTS\n\nWe would like to thank the authors of MAUVE for sharing their human evaluation data with us. We would also like to thank Gábor Melis for his insightful feedback on an initial version of this paper, our anonymous reviewers for their help in improving our final draft, and Luca Malagutti for detailed feedback on clarity and presentation.\n\nREFERENCES\n\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=BJh6Ztuxl.\n\nGuillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016. URL https://arxiv.org/abs/1610.01644.\n\nSatanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65–72, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https: //aclanthology.org/W05-0909.\n\nSteven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python. O’Reilly Media, Inc., 1st edition, 2009. ISBN 0596516495. URL https://www.nltk.org/book/.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //www.aclweb.org/anthology/N19-1423.\n\nJosip Djolonga, Mario Lucic, Marco Cuturi, Olivier Bachem, Olivier Bousquet, and Sylvain Gelly. Precision-recall curves using information divergence frontiers. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 2550–2559. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/ djolonga20a.html.\n\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160–175, 2021. doi: 10.1162/tacl_a_00359. URL https://aclanthology. org/2021.tacl-1.10.\n\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical Neural Story Generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/P18-1082.\n\nSebastian Gehrmann, Tosin P. Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nDas, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, Ondrej Dusek, Chris C. Emezue, Varun Gangal, Cristina Garbacea, Tatsunori B. Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur P. Parikh, Laura Perez-Beltrachini, Niranjan Rao, Vikas Raunak, Juan Diego Rodríguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. CoRR, abs/2102.01672, 2021. URL http://arxiv.org/abs/2102.01672.\n\nKenneth Heafield. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pp. 187–197, Edinburgh, Scotland, July 2011. Association for Computational Linguistics. URL https://aclanthology.org/ W11-2123.\n\nJohn Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2733– 2743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1275. URL https://aclanthology.org/D19-1275.\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL https://arxiv.org/abs/2203.15556.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH.\n\nF. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker. Perplexity—a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63–S63, 1977. doi: 10.1121/1.2016299. URL https://doi.org/10.1121/1.2016299.\n\nTuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf.\n\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.\n\nClara Meister and Ryan Cotterell. Language model evaluation beyond perplexity. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5328–5339, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long. 414. URL https://aclanthology.org/2021.acl-long.414.\n\nRishabh Misra. News category dataset. CoRR, abs/2209.11429, 2022. doi: 10.13140/RG.2.2.20331.\n\n18729. URL https://arxiv.org/abs/2209.11429.\n\nRishabh Misra and Jigyasa Grover. Sculpting Data for ML: The first act of Machine Learning.\n\nJanuary 2021. ISBN 9798585463570.\n\nHermann Ney, Ute Essen, and Reinhard Kneser. On structuring probabilistic dependences in stochastic language modelling. Computer Speech & Language, 8(1):1–38, 1994. ISSN 0885-2308.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\ndoi: https://doi.org/10.1006/csla.1994.1001. URL https://www.sciencedirect.com/ science/article/pii/S0885230884710011.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for autoIn Proceedings of the 40th Annual Meeting of the matic evaluation of machine translation. Association for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\n\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin text and huChoi, and Zaid Harchaoui. Mauve: Measuring the gap between neural Advances in Neural Information Processing Sysman text using divergence frontiers. tems, 34, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 260c2432a0eecc28ce03c10dadc078a4-Abstract.html.\n\nTiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. Pareto probing: Trading off accuracy for complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3138–3153, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.254. URL https://aclanthology.org/2020.emnlp-main.254.\n\nInformation-theoretic probing for linguistic structure.\n\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan In Proceedings of the 58th Cotterell. Annual Meeting of the Association for Computational Linguistics, pp. 4609–4622, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.420. URL https://aclanthology.org/2020.acl-main.420.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv. cloudfront.net/better-language-models/language-models.pdf.\n\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. URL https://arxiv.org/abs/2112.11446.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\nAbhilasha Ravichander, Yonatan Belinkov, and Eduard Hovy. Probing the probing paradigm: Does probing accuracy entail task relevance? In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 3363–3377, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.295. URL https://aclanthology.org/2021.eacl-main.295.\n\nEhud Reiter. A structured review of the validity of BLEU. Computational Linguistics, 44(3):393– 401, September 2018. doi: 10.1162/coli_a_00322. URL https://aclanthology.org/ J18-3002.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 5234–5243, Red Hook, NY, USA, 2018. Curran Associates Inc. URL https://proceedings.neurips.cc/paper/2018/ file/f7696a9b362ac5a51c3dc8f098b73923-Paper.pdf.\n\nThibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881–7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https://aclanthology.org/2020.acl-main. 704.\n\nClaude E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):379–423, 1948. doi: 10.1002/j.1538-7305.1948.tb01338.x. URL https://doi.org/ 10.1002/j.1538-7305.1948.tb01338.x.\n\nMiloš Stanojevi ́c and Khalil Sima’an. BEER: BEtter evaluation as ranking. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 414–419, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-3354. URL https://aclanthology.org/W14-3354.\n\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. In International Conference on Learning\n\nNeural text generation with unlikelihood training. Representations, 2020. URL https://openreview.net/forum?id=SJeYe0NtvH.\n\nSam Wiseman, Stuart Shieber, and Alexander Rush. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2253–2263, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1239. URL https://aclanthology.org/D17-1239.\n\nJiannan Xiang, Yahui Liu, Deng Cai, Huayang Li, Defu Lian, and Lemao Liu. Assessing dialogue systems with distribution distances. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 2192–2198, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.193. URL https://aclanthology.org/2021. findings-acl.193.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understandIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garing. nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf.\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.\n\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ 250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.\n\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 563–578, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1053. URL https://aclanthology.org/D19-1053.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA A MONTE CARLO ESTIMATOR FOR BACKWARD, JS, AND AUC\n\nDIVERGENCES\n\nConsider a Monte Carlo estimator for ∆←:\n\n∆←(pw, qw) ≈ (cid:99)KL(qw || pw) =\n\n1 N\n\nN (cid:88)\n\nn=1\n\nlog\n\nqw(wqw n ) pw(wqw n )\n\n(14)\n\nwhere wqw n ∼ qw. We can easily sample from qw. However, computing Eq. (14) requires knowledge of log pw, which we do not have. Thus we resort to other techniques for estimating these divergences, as discussed in §3.\n\nB STRING VS. CLUSTER-BASED KULLBACK–LEIBLER DECOMPOSITION\n\nThe decomposition in Eq. (13) can be shown as follows:\n\nKL(pw || qw) =\n\n(cid:88)\n\nw∈W\n\np(w) log\n\np(w) q(w)\n\nK (cid:88)\n\n(cid:88)\n\n(1) =\n\nc=1\n\nw∈W\n\np(c, w) log\n\np(c) p(w | c) q(c) q(w | c)\n\n(15)\n\n=\n\n=\n\nK (cid:88)\n\n(cid:88)\n\nc=1\n\nw∈W\n\n(cid:18)\n\np(c, w)\n\nlog\n\np(c) q(c)\n\n+ log\n\n(cid:19)\n\np(w | c) q(w | c)\n\nK (cid:88)\n\np(c) log\n\nc=1 (cid:124) (cid:125) (cid:123)(cid:122) Marginalise over all w\n\np(c) q(c)\n\nK (cid:88)\n\n(cid:88)\n\n+\n\nc=1\n\nw∈W\n\np(w | c)p(c) (cid:125) (cid:123)(cid:122) (cid:124) =p(c,w)\n\nlog\n\np(w | c) q(w | c)\n\n=\n\nK (cid:88)\n\nc=1\n\np(c) log\n\np(c) q(c)\n\n+ Epc\n\n(cid:34)\n\n(cid:88)\n\nw∈W\n\np(w | c) log\n\n(cid:35)\n\np(w | c) q(w | c)\n\n= KL(p(c) || q(c)) + KL(p(w | c) || q(w | c)) (cid:125) (cid:124)\n\n(cid:123)(cid:122) ≥0\n\n≥ KL(pc || qc)\n\nwhere again (1) follows from the fact that p(c, w) = p(w), which is true because the cluster assignment is deterministic, i.e.:\n\np(c | w) = 1\n\n(cid:110)\n\nc = φ(PLM(w))\n\n(cid:111)\n\n(16)\n\nThus, KL(p(c) || q(c)) provides a biased estimate of KL(pw || qw).\n\nC RELATED WORK\n\nOver the years, a number of evaluation metrics have been proposed for language generation tasks (such as translation and summarization); the most well-established and commonly-used include BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee & Lavie, 2005). However, these metrics—which rely on n-gram statistics—have been shown to correlate poorly with human judgement (Reiter, 2018). Recent metrics have improved upon these correlations with more advanced techniques, e.g., BEER (Stanojevi ́c & Sima’an, 2014), MoverScore (Zhao et al., 2019) and BLEURT (Sellam et al., 2020). These metrics, though, are intended for language generation tasks with a strict set of reference texts. While reasonably effective for directed generation tasks, they do not transfer well to the open-ended domain.\n\nFor tasks in which there is not a clear reference, e.g., story generation, basic statistics are typically employed to provide a preliminary evaluation of generated text. Such statistics include n-gram\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nrepetitions (Welleck et al., 2020), Zipfian coefficient (Holtzman et al., 2020), or the perplexity of generated text (Fan et al., 2018). Final assessments of language generation systems are still often performed using human evaluations, as automatic metrics on their own have not proven sufficient for differentiation between top-end language generation systems.\n\nAutomatic evaluation metrics for language models based on statistical divergences have been proposed by a number of different authors. For instance, Meister & Cotterell (2021) assessed the quality of language models while using a number of divergences between distributions over surface statistics in text corpora. Xiang et al. (2021) propose the approximation of distributions over strings with distributions in the embedding space in standard divergence metrics. Pillutla et al. (2021) present MAUVE—the object of study of this work—which is based on a new divergence metric inspired by the information frontier divergences Djolonga et al. (2020).11 Besides proposing this AUC divergence metric, Pillutla et al. (2021) also propose a new way to approximate it using clusters over word embeddings. We provide an analysis of this paradigm, showing that in practice, we should expect this method to provide a poor estimation of the intended quantity. We go on to perform empirical experiments to identify the proposed use of distributions over clusters itself is likely responsible for the metric’s success, rather than characteristics of the new divergence. We see this work as complementary to Pillutla et al. (2021), providing deeper and more comprehensive insights into the metric’s inner workings.\n\nD EXPERIMENTAL SETUP\n\nEmbedding Models. To obtain text embeddings for our input strings, we use several different PLM. Namely, we use the four sizes of GPT-2 (Radford et al., 2019), as well as BERT-base and BERT-large (both cased; Devlin et al., 2019). For the former, we use the embedding of the final token. For the latter, we use the embedding associated with the special CLS token. In both cases, we additionally present results using the average across token embeddings in App. E. All texts are truncated to 512 tokens (for experiments in §6.2, this truncation is performed before any manipulations of the text), in order to ensure that all models can process the input text in their context.\n\nFor our probing analysis in §6, we employ the following datasets:\n\n• Sentiment. To analyse sentiment, we use Yelp Polarity (Zhang et al., 2015), a dataset extracted from the Yelp Dataset Challenge 2015, which contains binary sentiment classifications for highly polar Yelp reviews. We use 10k examples randomly sampled from the training set and 5k examples randomly sampled from the test set.\n\n• Authorship. To analyse authorship, we use News Category (Misra & Grover, 2021; Misra, 2022), a dataset consisting of 200k news headlines from the years 2012–2018 obtained from HuffPost. We scrape entire articles from the URLs provided by the dataset. We only use the subset of articles for which the article’s author has ≥ 400 articles within the dataset, giving us a training set of 32k and a test set of 6k with 46 unique authors.\n\n• Topic. To analyse topic, we use the 20 NewsGroup dataset, which contains 18k newsgroups posts (partitioned into train and test sets using the original splits) on 20 topics, such as subcategories of science, politics and religion. The distribution over text topics is relatively uniform.\n\n11Specifically, Djolonga et al. proposed a framework that measures the trade-off between precision and recall\n\nusing Rényi divergences.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nE ADDITIONAL EXPERIMENTS\n\n(a) Interestingness\n\n(b) Sensibility\n\n(c) Human likeness\n\nFigure 5: Correlations between string- and cluster-based divergences and human judgement scores using a number of estimators (with different PLM(·) when defining pc, or language models for pw). Legend: ∆exp in dark green; ∆→ in orange; ∆← in blue; ∆JS in pink; ∆AUC in lime green.\n\n(a) Interestingness\n\n(b) Sensibility\n\n(c) Human likeness\n\nFigure 6: Correlations between cluster-based divergences and human judgement scores using a number of PLM(·) to define pc. In this figure, we use the average embedding per sentence produced by a PLM(·) to compute pc, as opposed to the final embedding. Legend: ∆exp in dark green; ∆→ in orange; ∆← in blue; ∆JS in pink; ∆AUC in lime green.\n\n16\n\n(pc,qc)GPT-2 small(pc,qc)GPT-2 medium(pc,qc)GPT-2 large(pc,qc)GPT-2 xl(pc,qc)BERT base(pc,qc)BERT large(pw,qw)n-gram(pw,qw)LSTM0255075100Correlation (%)(pc,qc)GPT-2 small(pc,qc)GPT-2 medium(pc,qc)GPT-2 large(pc,qc)GPT-2 xl(pc,qc)BERT base(pc,qc)BERT large(pw,qw)n-gram(pw,qw)LSTM0255075100Correlation (%)(pc,qc)GPT-2 small(pc,qc)GPT-2 medium(pc,qc)GPT-2 large(pc,qc)GPT-2 xl(pc,qc)BERT base(pc,qc)BERT large(pw,qw)n-gram(pw,qw)LSTM0255075100Correlation (%)(pc,qc)GPT-2 small(pc,qc)GPT-2 medium(pc,qc)GPT-2 large(pc,qc)GPT-2 xl(pc,qc)BERT base(pc,qc)BERT large0255075100Correlation (%)(pc,qc)GPT-2 small(pc,qc)GPT-2 medium(pc,qc)GPT-2 large(pc,qc)GPT-2 xl(pc,qc)BERT base(pc,qc)BERT large0255075100Correlation (%)(pc,qc)GPT-2 small(pc,qc)GPT-2 medium(pc,qc)GPT-2 large(pc,qc)GPT-2 xl(pc,qc)BERT base(pc,qc)BERT large0255075100Correlation (%)Published as a conference paper at ICLR 2023\n\n(a) Sentiment\n\n(b) Authorship\n\n(c) Topic\n\nFigure 7: Accuracy when predicting different attributes of text from their cluster assignments. Same plot as Fig. 3 albeit using embeddings from BERT.\n\nFigure 8: ∆AUC scores between reference text and alternate text distributions as a function of number of clusters used to estimate (cid:98)pc and (cid:98)qc. Importantly, in this figure, we compare the first 2500 sentences (p(1)) in the human-generated WebText test set to these same strings, but under the proposed interventions. I.e., for all points corresponding to an (altered) distribution p(1), we estimate our cluster distributions on the same set of human-generated sentences, but where the strings in one group have been intervened on. The baseline distribution p(2) still represents the final 2500 sentences in WebText (as in the original plot); and baseline distribution q is text sampled from GPT-2 XL.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: ∆ scores between reference text and alternate text distributions as a function of number of clusters used to estimate (cid:98)pc and (cid:98)qc. Results shown for embeddings produced using multiple LMs\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Zoomed in version of Fig. 4 to give a closer look at different scores assigned to texts manipulated in different ways.\n\nFigure 11: Version of Fig. 9 that uses the mean of the contextual embeddings from a text to form clusters.\n\nFigure 12: Version of Fig. 10 that uses the mean of the contextual embeddings from a text to form clusters.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: R2 when using cluster assignments to predict % of tokens in a text that are either punctuation or stopwords. Setup follows that of §6.1, albeit using solely the WebText dataset to train our clustering functions in this setting. We compute the average percentage of stopwords or punctuation per cluster in half of our strings and use these pre-computed averages when predicting the percentages in the other half, computing this prediction’s R2 (i.e. the percentage of explained variance). We see that larger PLMs—which are often claimed to provide better representations of language—do encode more information about such surface features than smaller models. This could simply be due to the fact that the embeddings from larger PLMs are typically of a larger dimension and, thus, have the capacity to encode additional (perhaps “less critical”) attributes of text. While these attributes do not appear to be differentiating factors when partitioning the embedding space into a small number of clusters, they become relevant when partitioning into a larger number of clusters. Even with several clusters and large PLMs, though, the R2 values we find are still quite small, at around 0.20.\n\n20",
  "translations": [
    "# Summary Of The Paper\n\nThe paper is about evaluation of open-ended generation of LMs, in particular the Mauve (pillutla et al.) metric.\n\nEvaluation of open ended text generation takes a distributional format. It is framed in this paper Divergence(p_w, q_w). This task has two sub-tasks, (1) Density estimation of p_w and q_w (when necessary). (2) calculating a distribution divergence metric of choice and -if necessary- approximate p_w or q_w to calculate this divergence metric tractably.  \n\nThe paper starts with laying a common formalism to compare these divergence metrics and notably revisits the AUC divergence metric used in the Mauve metric. This metric is intractable to calculate in practice and therefore P_true and P_model are replaced by multinomial probability over clusters of sentence embeddings using an external encoder. \n\nThis paper is a critique of the Mauve metric both theoretically and empirically, mostly the replacement of p_w and q_w with its clustering based approximation:\n- Section 4: it shows that the approximation kl(p_w,q_w) to kl(p_c,q_c) is biased\n- Section 5.1: it shows empirically that string based density estimations (using basic n-gram lm) of p_w and q_w correlations better with original data distribution than their cluster based counter parts\n- Section 5.2: Despite the results shown in section 5.1, authors show that divergence metrics correlate *correlate better with human judgements* when cluster based density estimation is used as an approximation. \n- Section 6: by probing the clusters authors show that clustering based on sentence embeddings (the core component of the clustering based density estimation approx of mauve) are more oriented to capture global features such as sentiment, authorship and topic and less sensitive to surface modification which is probably why they correlates with human judgements overall since most recent language models don’t suffer from disfluency,  although this could be a way to game the mauve score\n\n# Strength And Weaknesses\n\nStrengths:\n- The paper is well written the related work is quite educational, instead of listing papers authors make the effort of laying a theoretical ground for comparing difference divergence metrics and ways to approximate their intractability.\n- The paper shows theoretical and empirical drawbacks of the mauve score, and show why it does work in practice \n- I find the findings from section 5.1 and 5.2 to be interesting, mainly that methods that correlate with the real probability distributions $\\hat{p}_w$ and $\\hat{q}_w$ are not necessarily what is best to correlate with Human judgements $\\hat{p}_c, \\$hat{q}_c. This opens more research questions and could have made this paper have a wider scope.  \n\nWeaknesses:\n\nThe scope of the paper is quite narrow, and solely serves as a critique to the Mauve score paper which was published last year and yet not so widely adopted in LM evaluation. It is not clear how those conclusions could inspire future works for language model evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written, the proofs and formulations are quite correct to my best judgment.\n\nDocumentations of experimental work in section 5 and 6 could have used more clarity and being specific about experiments details, not only this can help in reproducibility but also help judging the correctness of the results in the experiments.\n\n# Summary Of The Review\n\nThe paper overall serves as a good critique of the mauve score, empirical and theoretical findings in the paper seems to be solid, however the overall scope of the paper is quite narrow and focuses only on critiquing one paper.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation\" critiques the MAUVE metric used for evaluating language generators, highlighting that its effectiveness may not be due to its proposed divergence measure but rather the use of cluster-based approximations. The authors explore several classical divergence metrics, providing a theoretical framework for evaluating language generation through cluster-based distributions. Empirical experiments demonstrate that classical divergences, when combined with cluster-based approximations, yield better correlations with human judgments than string-based methods. The findings suggest a need for further investigation into cluster-based metrics in evaluating language generators.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous theoretical analysis of divergence metrics and the empirical validation of its claims through well-structured experiments. The comparison between cluster-based and string-based distributions is insightful, providing clear evidence for the proposed advantages of cluster-based metrics. However, a notable weakness is the reliance on specific datasets and systems (e.g., GPT-2 and WebText), which may limit the generalizability of the results. Additionally, while the paper identifies the importance of cluster-based approximations, it does not sufficiently address potential biases introduced by these approximations in diverse contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodology, and findings. The clarity of explanations, especially regarding the theoretical framework and experimental setup, is commendable. The quality of the writing is high, with appropriate use of technical language and terminology. The novelty lies in the critical perspective on the MAUVE metric and the emphasis on cluster-based approximations. Reproducibility is supported by the availability of code in a GitHub repository, which facilitates further exploration and validation of the findings.\n\n# Summary Of The Review\nOverall, the paper provides valuable insights into the evaluation of language generators, challenging existing metrics and highlighting the benefits of cluster-based approximations. While the contributions are significant, the generalizability of the results could be a concern, and addressing potential biases in cluster-based metrics would strengthen the work.\n\n# Correctness\nRating: 4/5  \nThe theoretical and empirical claims made in the paper are well-supported and mostly accurate, although the potential biases introduced by cluster-based approaches require further discussion.\n\n# Technical Novelty And Significance\nRating: 4/5  \nThe paper presents a novel perspective on language generator evaluation by emphasizing the role of cluster-based metrics, which is a significant contribution to the field.\n\n# Empirical Novelty And Significance\nRating: 4/5  \nThe empirical findings reinforce the theoretical arguments and provide new insights into the effectiveness of different divergence metrics, although the focus on specific datasets limits broader applicability.",
    "# Summary Of The Paper\nThe paper titled \"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation\" critically re-evaluates the MAUVE metric, which is designed to measure the divergence between outputs generated by language models and natural language distributions. The authors argue that the performance of MAUVE is not intrinsically linked to its proposed divergence metric, but rather to the effectiveness of clustering embeddings that capture higher-level features of language. Through a comprehensive experimental setup involving eight variants of GPT-2, the authors demonstrate that cluster-based divergences outperform traditional string-based metrics in correlating with human judgments, indicating a potential shift in how text generation systems should be evaluated.\n\n# Strengths and Weaknesses\nThe paper presents several strengths, including an innovative approach to re-evaluating a widely used metric in language generation and providing empirical evidence supporting the superiority of cluster-based metrics over string-based ones. Additionally, the probing analysis offers valuable insights into the linguistic features captured by cluster distributions, which could inform the development of future evaluation methodologies. However, the study's limitations include its focus on English text, which may restrict the generalizability of the findings across different languages, and the potential biases introduced by the clustering methodology. Moreover, the findings raise concerns regarding the robustness of the metrics against manipulation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its objectives and findings. The methodology is detailed, allowing for reproducibility, although the reliance on specific clustering techniques may introduce variability in results depending on the implementation. The novelty lies in the approach of prioritizing cluster-based approximations over complex divergence metrics, suggesting a paradigm shift in evaluation practices. However, the implications regarding the potential for gaming the metrics could be more thoroughly addressed.\n\n# Summary Of The Review\nOverall, this paper provides a compelling re-evaluation of the MAUVE metric for text generation evaluation, advocating for cluster-based methodologies that better capture linguistic qualities. The empirical results are robust, though the generalizability of the findings and the potential biases in clustering warrant careful consideration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper \"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation\" examines the effectiveness of the MAUVE metric for evaluating text generation systems. It critiques existing evaluation methods, particularly those based on KL divergence, and argues that MAUVE's success is less about its proposed divergence measure and more about its use of cluster-based approximations derived from pre-trained language models (PLMs). The authors conduct a thorough analysis, comparing cluster-based and string-based metrics, and present empirical results indicating that while cluster-based metrics can result in biased divergence estimations, they exhibit stronger correlation with human judgments due to reduced variance.\n\n# Strength And Weaknesses\nOne of the paper's strengths lies in its comprehensive critique of current text evaluation metrics, highlighting the limitations of traditional methods like cross-entropy and perplexity. The experimental methodology is robust, utilizing a significant dataset and employing various decoding strategies. However, a notable weakness is the paper's reliance on cluster-based approximations, which may introduce bias, potentially undermining the validity of the results. Additionally, the conclusions drawn about the efficacy of MAUVE could benefit from a clearer distinction between correlation and causation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly structured, with well-defined sections that guide the reader through the theoretical foundations and empirical analyses. The quality of writing is high, making complex concepts accessible. The novelty primarily lies in the focus on cluster-based approximations for evaluating text generation, which is a fresh perspective in the field. However, reproducibility may be hindered by the specific nature of the clustering approach and the reliance on pre-trained models, which could vary across implementations.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of text generation evaluation by challenging conventional metrics and providing empirical support for the efficacy of cluster-based approximations. While it offers significant insights, the reliance on biased estimations warrants caution in the interpretation of results.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel evaluation metric for language generators, named MAUVE, which utilizes cluster-based approximations to enhance the assessment of generated language quality. The authors empirically validate MAUVE through a series of experiments that show its effectiveness in correlating with human judgments more strongly than traditional string-based metrics. Additionally, the paper establishes a theoretical framework for comparing different language evaluation metrics, providing insights into the impact of linguistic features on clustering outcomes and the biases inherent in existing approaches.\n\n# Strength And Weaknesses\nThe paper's strengths include its unique contribution to the evaluation of language generation through MAUVE, which offers a fresh perspective on metric formulation by leveraging cluster-based techniques. The empirical validation demonstrates the metric's effectiveness, though it is limited to English text, raising concerns about its applicability across other languages. The theoretical framework enhances understanding of evaluation metrics, but may not fully address the complexities of real-world applications. While the probing analysis provides valuable insights, it could benefit from a broader consideration of linguistic attributes. The identification of biases is crucial, but the lack of concrete solutions to mitigate these biases is a notable gap. Finally, the paper is well-organized and clearly written, yet additional visual aids could enhance comprehension for a wider audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, effectively communicating complex concepts related to language evaluation metrics. The novelty of MAUVE and its empirical backing contribute to its quality. However, the reproducibility of the findings may be limited due to the focus on English text and the absence of detailed experimental designs for future research. The paper references a wide range of related works, although some recent advancements in the field might be missing.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of language evaluation by introducing the MAUVE metric, supported by empirical validation and a theoretical framework. However, its limitations in generalizability and practical implications suggest areas for further exploration and refinement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"On the Usefulness of Embeddings, Clusters, and Strings for Text Generator Evaluation\" by Tiago Pimentel, Clara Meister, and Ryan Cotterell addresses the limitations of existing evaluation metrics for language generation systems, particularly critiquing the MAUVE metric. The authors introduce a novel divergence-based evaluation framework that leverages embeddings and clustering derived from a pre-trained language model. Their empirical findings demonstrate that this cluster-based approach outperforms traditional string-based measures in correlating with human evaluations, highlighting the significance of coherence and syntactic features in text evaluation.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including its innovative approach to tackling the shortcomings of existing evaluation metrics and its rigorous empirical analysis demonstrating the effectiveness of cluster-based divergences. The methodology is well-conceived, utilizing modern techniques in natural language processing to enhance evaluation reliability. However, the paper could benefit from a more detailed exploration of the impact of different clustering algorithms and parameters on the evaluation outcomes, which may affect the generalizability of their findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, making it accessible to readers familiar with the field. The quality of the writing is high, with a logical flow of ideas and a solid presentation of results. In terms of novelty, the introduction of a cluster-based evaluation framework offers a fresh perspective in the area of text generation evaluation. While the methodology appears reproducible, further details on implementation specifics and parameter tuning for the clustering algorithm would enhance its reproducibility.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of text generation evaluation by proposing a novel, cluster-based framework that aligns more closely with human judgments than traditional metrics. The empirical results support the authors' claims, although additional detail on the clustering methodology could improve the overall clarity and reproducibility of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper investigates the effectiveness of various adversarial training strategies aimed at enhancing the robustness of language models. The authors develop a comprehensive theoretical framework to analyze the dynamics of adversarial training, presenting empirical evaluations of different techniques such as projected gradient descent and adversarial perturbation methods. Key findings reveal that models trained with adversarial examples demonstrate improved resilience to input perturbations, while new divergence metrics introduced correlate strongly with model robustness. Additionally, a probing analysis highlights that adversarial training shifts model focus towards syntactic and coherence-level features, potentially at the expense of surface fluency.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its integration of theoretical and empirical insights, providing a well-rounded perspective on adversarial training's role in language model robustness. The introduction of new divergence metrics and the probing analysis enrich the understanding of how adversarial training influences feature encoding. However, the paper could benefit from a deeper exploration of the computational costs associated with implementing these techniques on large-scale models. Additionally, certain sections could be more succinct to enhance clarity for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly, although some technical sections may be dense for readers unfamiliar with adversarial methods. The quality of the empirical evaluation is high, and the novel contributions, particularly regarding divergence metrics and feature encoding insights, are significant. Reproducibility is likely facilitated by the detailed descriptions of the methodologies employed, although explicit sharing of code and data would enhance this aspect.\n\n# Summary Of The Review\nThis paper contributes significantly to the understanding of adversarial training in language models by combining theoretical insights with empirical validation. While the findings highlight important implications for model robustness, some areas for improvement include a more thorough discussion of computational costs and enhanced clarity in certain sections.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"ON THE USEFULNESS OF EMBEDDINGS, CLUSTERS AND STRINGS FOR TEXT GENERATOR EVALUATION\" presents a critical reevaluation of metrics used for assessing language generation systems. The authors argue that existing metrics, particularly MAUVE, are flawed and propose a return to classical divergence measures, which they claim yield more reliable results. Their methodology includes a theoretical framework that challenges the underlying principles of current metrics and introduces cluster-based evaluation methods using embeddings as a crucial enhancement. Empirical findings suggest that their approach provides better alignment with human judgments, which the authors claim represents a significant advancement in the field.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its bold critique of established methods, which could stimulate necessary discussions in the community about the adequacy of current evaluation metrics. The introduction of cluster-based metrics is intriguing and may offer new perspectives on evaluation methodologies. However, the paper's weaknesses include an overstatement of the novelty and significance of its contributions, as well as a lack of empirical validation for the claimed superiority of their methods over existing ones. The simplification of the evaluation landscape may also undermine the sophistication required for nuanced assessments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its arguments in a clear and structured manner, making it accessible to readers. However, the claims regarding the inadequacy of existing metrics and the proposed solutions lack empirical backing, which raises concerns about the reproducibility of the results. While the theoretical insights are presented with clarity, the novelty appears overstated, as the authors do not sufficiently acknowledge the complexities of current evaluation practices.\n\n# Summary Of The Review\nThe paper offers a provocative perspective on the evaluation of language generators, challenging established metrics and proposing a return to classical methods. While it presents interesting ideas, the claims of revolutionary change lack sufficient empirical validation, and the significance of the contributions may be overstated.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents an evaluation of MAUVE, a proposed automatic metric for assessing language generators that measures information-theoretic divergence between two probability distributions. The authors argue that while the theoretical divergence is uncomputable, MAUVE approximates it through clustering based on embeddings. Their findings indicate that MAUVE's effectiveness is primarily due to classical divergences combined with cluster-based approximations rather than its innovative divergence metric alone. The study demonstrates that cluster-based distributions can yield stronger correlations with human judgments compared to traditional metrics, suggesting potential improvements in evaluation methodologies for language generation.\n\n# Strength And Weaknesses\nThe paper's strength lies in its empirical validation of MAUVE, revealing that its cluster-based approach shows a significant correlation with human judgments, outperforming traditional metrics like cross-entropy and perplexity. Moreover, the probing analysis highlights the ability of cluster-based distributions to capture semantic and syntactic features effectively. However, a notable weakness is the paper's shift from claiming MAUVE's superiority to suggesting that classical divergences with clustering might be more effective. This dilutes the novelty of the original claim and raises questions about the true innovation presented by the authors.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that effectively guides the reader through the methodology and results. The quality of the experimental setup is commendable, employing a substantial dataset and a rigorous approach to clustering. However, the novelty is somewhat diminished by the authors' conclusions that prioritize classical metrics over MAUVE's divergence. Reproducibility is facilitated by a thorough description of the methods, although the reliance on specific clustering parameters may limit generalizability.\n\n# Summary Of The Review\nOverall, the paper provides valuable insights into the evaluation of language generation metrics, emphasizing the role of clustering in enhancing correlation with human judgments. While the findings contribute to the ongoing discussion regarding automatic evaluation metrics, the reduced emphasis on MAUVE's innovative aspects poses questions about its significance as a standalone metric.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to automatic evaluation metrics for text generation, focusing on the MAUVE (Metric for Authorship and Understanding Via Embeddings) divergence measure. It claims that this measure can effectively capture quality judgments by correlating with human evaluations through cluster-based approximations of text embeddings. The study evaluates the performance of these metrics across various models, particularly highlighting the balance between empirical correlation with human judgments and the theoretical foundations of the metrics used.\n\n# Strength And Weaknesses\nThe paper effectively highlights the importance of correlation with human judgments as a metric for evaluating automatic text generation. However, it overlooks the inherent variability in human evaluations, which may undermine the reliability of this approach. The dismissal of the theoretically grounded MAUVE divergence measure in favor of clustering raises concerns about the simplification of complex linguistic features. Additionally, the reliance on clustering methods introduces an inductive bias that may lead to misleading evaluations, particularly if the clusters fail to capture the nuances of language. The generalizability of the findings is also limited, as the experiments are confined to English text and specific models, which raises questions about applicability to wider contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings clearly. However, it could benefit from a deeper exploration of the theoretical implications of the proposed metrics, especially regarding their limitations. The novelty lies in the use of cluster-based approximations; however, this approach raises concerns about the loss of information and the potential for oversimplification. Reproducibility may be hindered due to the reliance on specific models and the clustering methods employed, which may not transfer well to other contexts or languages.\n\n# Summary Of The Review\nWhile the paper presents an interesting approach to evaluating text quality through automatic metrics, its reliance on clustering and the assumptions made regarding human judgment correlation raise significant concerns. The findings, though potentially valuable, are limited in their generalizability and robustness due to the theoretical and empirical weaknesses identified.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper critiques the MAUVE metric used for evaluating automatic text generation, asserting that its divergence approximation is insufficient. The authors propose the use of traditional divergence measures with cluster-based approximations, suggesting that these may correlate more effectively with human judgments. The methodology includes formalizing evaluation metrics as distances between a generator's distribution and the true string distribution, followed by an empirical comparison between cluster-based and string-based metrics. The findings indicate that while cluster-based metrics can yield better correlations with human evaluations, they may introduce biases that need careful consideration.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough critique of existing evaluation metrics and the introduction of cluster-based approximations, which offer a novel perspective on measuring text generation quality. The empirical results are compelling, demonstrating a correlation between cluster-based metrics and human judgments that surpasses that of traditional string-based approaches. However, weaknesses include the potential overreliance on cluster-based metrics that might obscure certain linguistic features, as the probing analysis reveals that some surface-level characteristics are neglected. The paper could also benefit from a more extensive discussion on the implications of these biases in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that guides the reader through the critique of MAUVE, the proposed methodology, and the experimental results. The quality of the writing is high, although some technical sections may require more background for readers unfamiliar with advanced metrics. The novelty of the approach is significant, as it challenges prevailing methodologies in text generation evaluation. Reproducibility is supported by a detailed description of the experimental setup, though sharing code and data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful critique of the MAUVE metric while introducing a promising alternative in cluster-based divergence measures. The findings are well-supported by experiments, though the potential biases introduced by these measures warrant further investigation. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses a critical challenge in machine learning regarding the evaluation of models, proposing a novel framework that aims to enhance the understanding and effectiveness of existing performance metrics. The methodology introduced combines theoretical insights with empirical validation, demonstrating improvements over traditional evaluation approaches. The findings suggest that the proposed framework not only clarifies existing metrics but also provides a pathway for more reliable model assessments across various scenarios.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The paper tackles a significant issue in the machine learning community, focusing on model evaluation, which is crucial for the development of robust algorithms.\n2. **Novelty:** The authors present an innovative methodology that appears to provide fresh insights into the evaluation process, potentially leading to advancements in the field.\n3. **Theoretical Foundation:** A strong theoretical basis underpins the proposed work, facilitating a deeper understanding of its implications and potential applications.\n4. **Empirical Validation:** The inclusion of extensive experiments adds credibility to the claims made, supporting the effectiveness of the proposed framework.\n5. **Clarity:** The paper is generally well-structured and clearly written, enhancing reader comprehension.\n\n**Weaknesses:**\n1. **Limited Scope:** The applicability of the proposed methods may be restricted to specific datasets or contexts, raising concerns about generalizability.\n2. **Comparative Analysis:** A lack of comprehensive comparisons with existing benchmarks makes it challenging to fully evaluate the performance of the proposed approach relative to established methods.\n3. **Assumptions:** The reliance on certain assumptions may limit the robustness of the conclusions, particularly in diverse real-world scenarios.\n4. **Complexity:** Some elements of the methodology could be perceived as overly complex, which may impede practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and written in a manner that is accessible to its intended audience. The quality of the theoretical and empirical contributions is high; however, the complexity of some methodological aspects could hinder reproducibility. While the novel concepts presented are promising, further clarity in certain areas would enhance overall understanding and facilitate implementation by practitioners.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of machine learning by proposing a novel framework for model evaluation that combines theoretical rigor with empirical validation. While the proposed methodology shows promise, there are areas for improvement, particularly regarding generalizability and comparative analysis with existing benchmarks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper investigates the efficacy of various evaluation metrics for text generation, highlighting the importance of automatic metrics that correlate with human judgments. It focuses on the MAUVE metric, which measures the divergence between the distribution of generated text and natural language, revealing its limitations due to the impracticality of the divergence calculation. The authors find that MAUVE's success is largely attributable to its use of cluster-based distributions rather than the divergence metric itself. The study ultimately proposes that cluster-based metrics, which capture higher-level syntactic and coherence features, may serve as more effective evaluation tools for contemporary language generators.\n\n# Strength And Weaknesses\nOne of the paper's strengths lies in its thorough examination of the MAUVE metric, providing valuable insights into its underlying mechanics and limitations. The identification of cluster-based metrics as potentially more reliable alternatives is a significant contribution to the field, addressing a critical gap in the current evaluation landscape. However, the paper may not sufficiently address the potential vulnerabilities of cluster-based metrics, which raises questions about their robustness in diverse language generation scenarios. Additionally, the reliance on specific methodologies and assumptions could limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers with varying levels of expertise in the field. The quality of the analysis is high, supported by empirical evidence that strengthens the authors' claims. The novelty of the approach, particularly in the re-evaluation of the MAUVE metric and the focus on cluster-based evaluations, is commendable. However, the reproducibility of the results could be enhanced with more detailed descriptions of the experimental setup and the datasets used for evaluation.\n\n# Summary Of The Review\nOverall, the paper provides a significant contribution to the evaluation of text generation metrics by revealing the strengths and limitations of existing methods, particularly MAUVE. It advocates for the adoption of cluster-based metrics as a promising alternative, although further exploration of their robustness is warranted.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation\" by Tiago Pimentel, Clara Meister, and Ryan Cotterell addresses the critical need for effective automatic evaluation metrics in language generation. It introduces the MAUVE metric, which aims to approximate an uncomputable divergence measure, and investigates its performance despite theoretical limitations. The authors conclude that the success of MAUVE is largely due to its cluster-based approach, which outperforms classical divergence metrics by better capturing syntactic and coherence features, leading to stronger correlations with human judgments.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its comprehensive exploration of various divergence measures and the introduction of cluster-based metrics, which significantly improve the evaluation of language generation systems. The empirical results are robust, showing a clear advantage of cluster-based metrics over traditional methods, which is a valuable contribution to the field. However, a weakness lies in the reliance on the MAUVE metric's theoretical framework, which could limit the generalizability of the findings. Additionally, while the probing analysis is insightful, further exploration of potential limitations and the robustness of the cluster-based methods is warranted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers in the field. The methodology is sound, with thorough explanations of the divergence measures and experimental setups, which contribute to its reproducibility. The novelty of the approach, particularly in utilizing cluster-based metrics, is significant and addresses a gap in existing literature. However, the theoretical implications of the MAUVE metric could be more explicitly articulated to enhance understanding.\n\n# Summary Of The Review\nOverall, this paper provides a valuable contribution to the field of text generation evaluation by demonstrating the efficacy of cluster-based metrics over traditional divergence approaches. The findings are clearly presented and suggest important avenues for future research, although some theoretical aspects could benefit from further clarification.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper presents a novel evaluation metric called MAUVE, designed to assess the quality of text generated by machines in a way that correlates more closely with human judgments. The authors outline a comprehensive methodology based on divergence metrics and provide a theoretical framework to evaluate these metrics effectively. Through a series of experiments, they compare MAUVE against existing metrics, demonstrating its superior performance in capturing the nuances of human-like text generation. The results indicate that MAUVE not only aligns well with human evaluations but also addresses several limitations present in traditional metrics.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its rigorous approach to developing and validating MAUVE, which is grounded in both theoretical underpinnings and empirical analysis. The experiments are well-structured, allowing for a clear comparison of various evaluation methods across different models and datasets. However, a notable weakness is the potential bias introduced by cluster-based approximations, which the authors acknowledge but do not fully explore. Additionally, while the results are promising, further investigation into the robustness of MAUVE against adversarial examples would strengthen its validity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings in a clear and logical manner. Technical terms are adequately explained, and figures and tables enhance the readability of the results. The novelty of MAUVE is evident, as it provides a fresh perspective on evaluating text generation, highlighting its significance in the field. The methodology is sufficiently detailed to allow for reproducibility, although more explicit details on the experimental setup would benefit readers attempting to replicate the results.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of text generation evaluation by introducing MAUVE, a metric that aligns closely with human judgments. The methodology is sound and the empirical analysis is thorough, although some aspects could benefit from deeper exploration. The findings raise important considerations for future research on automatic evaluation metrics.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the efficacy of MAUVE, a novel automatic evaluation metric for language generation, which measures the information-theoretic divergence between the true natural language distribution and that of a language generator. The authors propose that the computational intractability of this divergence leads to approximations through cluster-based methods utilizing embeddings from a pre-trained language model (PLM). The findings reveal that while MAUVE shows a high correlation with human judgment, this correlation is primarily due to its cluster-based substitutions rather than the divergence metric itself. The authors further suggest that classical divergence measures may outperform MAUVE in evaluating text quality.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough theoretical grounding and empirical analysis, providing a comprehensive evaluation of MAUVE against traditional divergence metrics. The use of cluster-based approximations to handle the complexities of text distributions is a notable contribution, as it addresses a significant limitation in existing metrics. However, a weakness is the paper's heavy reliance on empirical validation, which, while insightful, could benefit from broader testing across various language models and datasets to strengthen its claims about the generalizability of findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical underpinnings of the proposed metric while also detailing the methodology and experimental setup. The writing quality is high, with rigorous mathematical formulations that enhance understanding. The novelty of the approach is significant, particularly in its integration of clustering techniques with language model evaluation. Reproducibility is adequately supported through detailed descriptions of experimental setups, yet additional sharing of code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of automatic text evaluation by critically assessing the MAUVE metric and its theoretical implications. While the findings underscore the importance of cluster-based approaches, the paper could improve by expanding its empirical scope. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes MAUVE, a new metric for evaluating text generated by language models. The authors claim that MAUVE improves upon existing metrics by leveraging cluster-based approximations of divergence measures. The methodology involves using pre-trained language models to derive embeddings, which are then evaluated through K-means clustering. However, the authors acknowledge a lack of understanding regarding the reasons for MAUVE's performance and note that further validation across different languages is necessary. The findings suggest that while MAUVE may correlate with human judgments, its theoretical foundations and practical implications remain questionable.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to innovate evaluation metrics for language generation, addressing a significant challenge in the field. However, the weaknesses are pronounced: the lack of a compelling narrative as to why MAUVE is effective undermines the validity of the proposed metric. The approximation method used is criticized for being insufficiently rigorous, and the dependence on K-means clustering raises concerns about reliability and reproducibility. The limited experiments focusing only on English text further restrict the generalizability of the findings, while the potential for bias introduced by pre-trained models casts doubt on the validity of the results. \n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity suffers from an incomplete narrative that does not convincingly explain MAUVE's effectiveness. The overall quality of the methodology is undermined by the lack of rigor in the theoretical foundations and the questionable choice of clustering methods. While the attempt at introducing novelty in evaluation metrics is commendable, the limitations in empirical validation and the potential for biased results significantly impact reproducibility.\n\n# Summary Of The Review\nOverall, the paper attempts to contribute to the field of language generation evaluation metrics through the introduction of MAUVE; however, it falls short in establishing a solid theoretical basis and fails to provide sufficient empirical validation. The concerns regarding methodology and the generalizability of findings limit the impact of the research.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThis paper presents significant advancements in the evaluation of language generators by introducing the MAUVE metric, which effectively measures the information-theoretic divergence between generated text and the true natural language distribution. The authors demonstrate that using cluster-based approximations instead of traditional string distributions leads to improved correlation with human text quality assessments. Their empirical findings indicate that these cluster-based metrics exhibit lower variance and higher correlation with human judgments, emphasizing the importance of higher-level linguistic features such as syntax and coherence over superficial attributes.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative introduction of the MAUVE metric and the use of cluster-based distributions, which represent a substantial improvement over traditional methods for evaluating language generation. The theoretical framework provided for comparing evaluation metrics is another commendable aspect, likely to inspire future research. However, the paper could benefit from further exploration of the limitations of cluster-based evaluations and potential scalability issues when applied to diverse datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its contributions clearly. The methodology is described in a detailed manner, allowing for reproducibility of the results. The novelty of the work is evident, with a fresh approach to language generation evaluation that prioritizes meaningful linguistic features. However, clarification on specific experimental settings and dataset characteristics would enhance the reproducibility aspect.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the field of natural language processing by introducing the MAUVE metric and demonstrating the advantages of cluster-based evaluation methods. Its findings have the potential to reshape how language generation quality is assessed, paving the way for future innovations in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for the evaluation of automatic metrics in language generation, focusing on the newly proposed metric MAUVE. MAUVE aims to measure the information-theoretic divergence between probability distributions over generated strings and is underpinned by theoretical properties that enhance its predictive capabilities regarding human judgments. The authors delve into the clustering approach that approximates uncomputable divergences, examining the implications of using clustering-based distributions and advocating for a rigorous theoretical foundation for future metrics.\n\n# Strength And Weaknesses\nStrengths of the paper include its thorough exploration of the theoretical basis for evaluation metrics, which is often overlooked in empirical studies. The introduction of MAUVE as a theoretically grounded metric offers a novel perspective on evaluating language generation quality. However, the paper's reliance on clustering methods raises concerns about potential biases and the loss of information regarding surface-level text features, which are not fully addressed. Furthermore, while the theoretical discussions are robust, there is a lack of empirical validation of MAUVE against established metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured arguments and a logical flow that facilitates understanding of complex theoretical concepts. The quality of the writing is high, although some sections could benefit from more detailed examples to illustrate the theoretical implications. The novelty of the proposed MAUVE metric is significant, as it introduces a fresh theoretical perspective to an area with much empirical focus. However, reproducibility may be limited due to the absence of empirical results or clear guidelines on implementing MAUVE.\n\n# Summary Of The Review\nOverall, the paper contributes valuable theoretical insights into the evaluation of language generation metrics, particularly through the introduction of MAUVE. While it excels in theoretical rigor and clarity, the lack of empirical validation and potential biases in clustering methods warrant further investigation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"ON THE USEFULNESS OF EMBEDDINGS, CLUSTERS AND STRINGS FOR TEXT GENERATOR EVALUATION\" critiques the MAUVE metric for evaluating text generation by highlighting its approximation of an uncomputable divergence through clustering embeddings from pre-trained language models (PLMs). The authors implement a comprehensive experimental setup that includes comparing automatic metrics using both string- and cluster-based divergences, utilizing various sizes of GPT-2 models to generate text. Their findings suggest that cluster-based metrics correlate more closely with human judgments and can capture higher-level textual features better than traditional string-based metrics.\n\n# Strength And Weaknesses\nThe paper presents a significant contribution by providing a thorough critique of the MAUVE metric and advocating for cluster-based evaluation methods, which enhances the understanding of how different evaluation metrics relate to human judgment. The methodology is sound, involving rigorous clustering and probing analyses that effectively demonstrate the advantages of their proposed approach. However, the paper could benefit from a more extensive discussion on the limitations of cluster-based metrics, particularly regarding their sensitivity to the choice of clustering parameters and the potential impact of the underlying PLM on the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the rationale behind the proposed methodologies, making the contributions accessible to the reader. The quality of the experiments is commendable, with robust evaluation metrics and a solid experimental design. The novelty lies in the application of clustering techniques to text generation evaluation, an area that has not been extensively explored. The availability of the code enhances reproducibility, allowing other researchers to validate and extend the findings.\n\n# Summary Of The Review\nOverall, the paper makes a strong case for the use of cluster-based metrics in text generation evaluation, providing valuable insights into the limitations of current methods like MAUVE. While the paper is well-executed and offers significant contributions, further exploration of the robustness and limitations of the proposed methods is needed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper critiques the MAUVE metric introduced by Pillutla et al. (2021), positing that MAUVE's success can be attributed more to cluster-based approximations than to the proposed divergence metric. The authors argue that classical divergences, when combined with cluster-based approaches, may outperform MAUVE in correlating with human judgments. The paper investigates biases that arise from employing cluster-based distributions, suggesting that these biases may undermine the validity of MAUVE as an evaluation tool for language generation. Ultimately, the authors conclude that cluster-based metrics might be better suited for evaluating state-of-the-art language generators, while also emphasizing the limitations of cluster-based approaches.\n\n# Strength And Weaknesses\nThe strength of the paper lies in its rigorous exploration of cluster-based approximations and their performance relative to MAUVE. The authors provide a detailed probing analysis that highlights potential biases in the MAUVE methodology. However, the critique may be perceived as overly dismissive of the innovative contributions of MAUVE itself, failing to appreciate its historical and contextual significance in language evaluation metrics. Furthermore, the comparisons drawn could be seen as unfair, as they do not adequately consider the original intent behind MAUVE's design.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly articulated and structured, allowing for a straightforward understanding of the authors' arguments and findings. The quality of the analysis is high, with well-supported claims and a thorough investigation into the biases introduced by alternative metrics. However, the novelty of the contributions may be undermined by a lack of acknowledgment of previous advancements, particularly those introduced by MAUVE. Reproducibility is not directly addressed in the paper, which is a notable gap given the critical nature of the claims made regarding methodologies.\n\n# Summary Of The Review\nThe paper presents a compelling critique of MAUVE, emphasizing the advantages of cluster-based metrics, yet it risks being perceived as unbalanced by not sufficiently acknowledging the historical contributions of MAUVE. While the arguments are well-supported and clear, they could benefit from a broader context that recognizes the evolution of language evaluation metrics.\n\n# Correctness\nRating: 3/5\nThe paper's claims are generally sound, but some arguments may oversimplify the complexities of evaluating language generation, potentially leading to misleading conclusions.\n\n# Technical Novelty And Significance\nRating: 3/5\nWhile the examination of cluster-based metrics is relevant, the novelty is somewhat diminished by the lack of acknowledgment of prior work, particularly MAUVE's contributions, which were significant at the time of their introduction.\n\n# Empirical Novelty And Significance\nRating: 3/5\nThe empirical findings regarding the performance of cluster-based metrics are noteworthy, yet the significance is lessened by the potential bias against MAUVE and the absence of a balanced discussion of its merits.",
    "# Summary Of The Paper\nThe paper presents a novel framework for evaluating language generation models using divergence metrics. The authors propose a set of metrics that quantify the differences between generated text and reference text, thereby providing a structured approach to assess the quality of generated content. The methodology involves extensive experimentation across multiple datasets, demonstrating that the proposed metrics correlate well with human judgments of text quality. The findings indicate that these divergence metrics outperform traditional evaluation methods, offering a more reliable tool for model assessment.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous empirical validation of the proposed metrics, which are shown to have a strong correlation with human evaluators. This addresses a significant issue in the field, where existing evaluation methods often fail to capture nuanced differences in text quality. However, a weakness is that the paper could benefit from a more thorough discussion on the limitations of the proposed metrics, particularly in edge cases where human judgment may still diverge from the metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the proposed methodology and findings. The quality of the writing is high, making complex ideas accessible. The novelty of the approach is significant, as it introduces new metrics that could potentially change how language generation is evaluated. Reproducibility is addressed through detailed descriptions of experiments and datasets used, although the availability of code and data should be explicitly stated for full reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of language generation evaluation by introducing effective divergence metrics. While the methodology is robust and the findings are compelling, further exploration of the limitations of the proposed approach would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper primarily investigates the effectiveness of the MAUVE metric in evaluating language generation quality. It employs a methodology that includes clustering-based approximations to analyze textual coherence and syntactic properties, correlating these with human judgments. The findings suggest that while MAUVE provides useful insights into certain aspects of text quality, its limitations, particularly concerning multilingual applicability and robustness under adversarial conditions, remain underexplored.\n\n# Strength And Weaknesses\nThe paper presents valuable contributions, particularly in its analysis of clustering-based approximations and their relationship with linguistic attributes. However, it fails to adequately address the limitations of existing metrics beyond those discussed, which could provide a more comprehensive evaluation landscape. Additionally, the focus on English text limits the generalizability of the findings, and there is a lack of empirical studies on the potential for metric manipulation. The absence of discussions on scalability and computational efficiency further detracts from its practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, though it could benefit from a more thorough exploration of the implications of its findings. The novelty is present in the specific analysis of clustering and linguistic attributes; however, the reproducibility of results may be hampered by the narrow scope of languages and contexts examined. The suggestion for future research directions is vague, which detracts from the overall clarity of the paper's contributions.\n\n# Summary Of The Review\nOverall, the paper offers insightful analysis and contributes to the understanding of language evaluation metrics, particularly MAUVE. However, its limitations in addressing broader applicability, alternative methods, and practical implications significantly reduce its potential impact in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation\" presents a thorough investigation into the effectiveness of automatic evaluation metrics for language generation, particularly focusing on the MAUVE metric. The authors critique MAUVE's reliance on an uncomputable divergence and propose a framework that utilizes multinomial distributions over clusters derived from pre-trained language model embeddings. Through empirical evaluation, the paper finds that cluster-based metrics show stronger correlations with human judgments than traditional string-based measures, suggesting that clustering approaches may significantly enhance the assessment of language generation quality.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its robust statistical methodology and comprehensive analysis of various divergence measures, offering valuable insights into the relationship between automatic metrics and human evaluations. The incorporation of both forward and backward KL divergences, along with novel approaches like the area under the curve divergence, provides a nuanced understanding of evaluation metrics. However, the reliance on correlation coefficients, while informative, may not fully capture the complexity of human judgments, and the limitations of the clustering methods used could be better addressed. Additionally, the theoretical critiques of MAUVE could benefit from a more detailed exploration of potential alternatives.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making complex statistical concepts accessible. The methodology is detailed, allowing for reproducibility of the experiments. However, some sections could be further clarified, especially concerning the assumptions made regarding independence and the implications of using clustered data. The novelty of the approach is significant, particularly in framing the evaluation of language generators through the lens of statistical divergence measures, which could inspire further research in this area.\n\n# Summary Of The Review\nOverall, the paper provides a critical and insightful analysis of text generation evaluation metrics, emphasizing the importance of statistical methods in correlating automatic metrics with human judgments. While the contributions are notable, certain areas could benefit from deeper exploration, particularly regarding the limitations of the proposed methodologies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces MAUVE, a novel metric aimed at evaluating language generation models by leveraging clustering methods to assess text quality. Through empirical analysis, the authors demonstrate that MAUVE provides a stronger correlation with human judgments compared to traditional string-based metrics when applied to English text. However, the paper does not provide a comprehensive explanation of the underlying mechanisms that contribute to MAUVE's performance, leaving critical questions unanswered.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to innovate upon existing metrics by utilizing clustering techniques, which shows promise in enhancing the correlation with human judgments. However, several weaknesses are apparent. The theoretical and practical limitations of the approximation method used in MAUVE raise concerns about the reliability of the results. Additionally, the study's focus on English text limits its generalizability across different languages. The analysis does not adequately address potential biases introduced by pre-trained language models and fails to explore alternative evaluation approaches. Furthermore, the susceptibility of the proposed metrics to manipulation and the lack of thorough investigation into linguistic attributes that influence text quality are significant drawbacks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is moderately clear, though it could benefit from a more detailed exploration of the theoretical foundations of MAUVE. While the novel approach of using clustering for metric evaluation is commendable, the reproducibility of the findings may be hindered due to the limited scope of datasets and languages analyzed. The gaps in understanding the metric's performance and the lack of an exhaustive investigation into various clustering algorithms further complicate reproducibility.\n\n# Summary Of The Review\nOverall, while the paper presents an interesting approach to improving language evaluation metrics through clustering, it suffers from significant limitations regarding its theoretical grounding, generalizability, and empirical analysis. The findings, while promising, require further validation and exploration to establish their robustness and applicability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"ON THE USEFULNESS OF EMBEDDINGS, CLUSTERS AND STRINGS FOR TEXT GENERATOR EVALUATION\" investigates automatic evaluation metrics for text generation, with a particular focus on the MAUVE metric, which the authors argue is based on an uncomputable divergence. They propose that traditional divergence metrics, such as KL divergence, may provide better correlations with human judgments than MAUVE's cluster-based approximation. Through experiments, the authors demonstrate that clustering metrics correlated more effectively with human evaluations of text quality, while also highlighting the sensitivity of these metrics to coherence and syntax.\n\n# Strength And Weaknesses\nThe paper effectively critiques existing metrics by highlighting their limitations and proposing alternatives, which is a valuable contribution to the field. However, much of the analysis appears to reiterate well-known issues in the evaluation of text generation, with limited new insights. The experimental results, while confirming the authors' hypotheses, do not present groundbreaking findings, as they align with existing expert opinions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably well-organized, though it sometimes delves into excessive detail about established concepts, which may detract from clarity. The writing is generally accessible, but the overall novelty of the findings is marginal, as they do not introduce significantly new methodologies or insights. Reproducibility issues are not addressed, as the paper does not provide comprehensive experimental details or code availability.\n\n# Summary Of The Review\nThis paper provides a critique of existing text generation evaluation metrics and suggests that traditional divergence metrics may offer better correlations with human judgments. However, the contributions are largely reiterative, lacking significant novelty or insight. Overall, while the paper is competent, it does not advance the field in a substantial way.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper discusses the limitations of existing automatic evaluation metrics for text generation, emphasizing the necessity for more effective measures that capture the nuances of language quality. The authors critique the MAUVE metric and propose using classical divergences paired with cluster-based approximations as a potential improvement. Their probing analysis reveals that current embedding-based metrics tend to focus on syntactic and coherence-level features while neglecting surface-level attributes. Empirical results demonstrate that cluster-based metrics outperform string-based ones, suggesting a need for further investigation into alternative clustering methods and the integration of traditional NLP metrics with embedding-based measures to provide a comprehensive assessment of text quality.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough critique of existing metrics and the innovative proposal of combining classical divergence measures with clustering approaches to enhance evaluation metrics. The probing analysis effectively highlights the shortcomings of current metrics, particularly their insensitivity to surface-level features, and opens avenues for further exploration into clustering algorithms. However, the paper could benefit from a more detailed empirical evaluation of the proposed methods and a clearer discussion on how the suggested hybrid metrics can be implemented in practice.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its arguments clearly, making it accessible to the reader. The quality of the analysis is commendable, as it synthesizes theoretical insights with empirical findings. The novelty of the proposed approach lies in the integration of classical and modern metrics, which is an underexplored area in the literature. However, reproducibility may be a concern, as the paper does not provide enough details on the experimental setup or the specific algorithms used in the clustering methods.\n\n# Summary Of The Review\nOverall, the paper presents a compelling argument for the need to rethink automatic evaluation metrics in text generation. It successfully identifies gaps in existing methodologies and proposes promising avenues for future research, although it could improve in empirical validation and reproducibility of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a comprehensive empirical evaluation of MAUVE, a novel metric designed for assessing the quality of language generation systems. The authors demonstrate that MAUVE, which relies on cluster-based divergences, exhibits a strong correlation with human judgments of text quality, outperforming traditional string-based divergences. Through a series of experiments across various language generation models (specifically different sizes of GPT-2) and decoding strategies, the findings indicate that cluster-based metrics not only correlate better with human evaluations but also capture important linguistic features, such as coherence, more effectively.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its rigorous empirical analysis, which convincingly shows that cluster-based divergences provide superior performance in evaluating language generation quality. The detailed comparisons across different models and decoding strategies lend robustness to the findings. However, a potential weakness is the reliance on specific model architectures (GPT-2) without exploring a wider range of models, which may limit the generalizability of the results. Additionally, the probing of linguistic features, while insightful, could benefit from a more thorough exploration of how these features directly influence the observed performance metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers. The quality of the experiments is high, with a focus on reproducibility, as the authors provide sufficient detail about their methodologies. The novelty of the MAUVE metric and its reliance on cluster-based divergences is significant, offering a fresh perspective in the field of language generation evaluation. Overall, the work appears reproducible, provided the necessary models and data are accessible.\n\n# Summary Of The Review\nOverall, the paper offers compelling evidence for the superiority of cluster-based divergences in language generation evaluation through the MAUVE metric. Its rigorous empirical approach and clear presentation make it a valuable contribution to the field, although further exploration of generalizability across different models would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to evaluating language generation models through the introduction of divergence metrics. The authors propose a framework that quantitatively assesses the quality of generated text by comparing it against reference texts using various divergence measures. Their methodology includes extensive experiments on benchmark datasets, demonstrating that their metrics correlate better with human judgments than existing evaluation methods. The findings show that the proposed metrics not only provide a more reliable assessment of language generation quality but also highlight areas where models can be improved.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to language generation evaluation, which addresses a critical gap in the field by proposing metrics that align more closely with human assessments. The empirical validation across multiple datasets lends credibility to the claims made. However, the paper does have weaknesses, including the dense language and occasional jargon that may alienate less technical readers. Furthermore, some sections suffer from repetition, which could lead to a lack of clarity in the overall argument.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the overall quality of the research is commendable, clarity suffers due to inconsistent formatting and the use of jargon. The novelty of the proposed metrics is significant, as they provide a new lens through which to evaluate language generation models. However, reproducibility could be enhanced by providing clearer explanations of the methodology and results, as well as ensuring that all figures and tables are properly referenced and explained.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of language generation evaluation through the introduction of new divergence metrics. While it presents innovative ideas backed by empirical evidence, improvements in clarity and structure are necessary for broader accessibility and understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.548381283646263,
    -1.6241952520940288,
    -1.9057533876715813,
    -1.7287042555219077,
    -1.8184759608900156,
    -1.6208012144090516,
    -1.729875435154316,
    -1.9522982125794315,
    -1.8208171739783368,
    -1.8662669358052488,
    -1.8655460036088172,
    -1.466297192554947,
    -1.615794377929004,
    -1.8417136685402482,
    -1.6033191503526705,
    -1.8826078240766162,
    -1.791025790245367,
    -1.8874550602048796,
    -1.670074011936656,
    -1.8285101526296377,
    -1.7344156099539714,
    -1.443799952772587,
    -1.9488649481472822,
    -1.9622368768019405,
    -1.8245956682140296,
    -1.8852849518241297,
    -1.8233600735499669,
    -1.5272384043562799,
    -1.668226026220043
  ],
  "logp_cond": [
    [
      0.0,
      -2.349135767899459,
      -2.36715487198071,
      -2.367916433469426,
      -2.3854698951873083,
      -2.380170845733389,
      -2.431379908220392,
      -2.409158966943177,
      -2.372723457456209,
      -2.3935960618125622,
      -2.3438890537322377,
      -2.4052046985994084,
      -2.3790363750498056,
      -2.3756377911928723,
      -2.401890460516954,
      -2.383099536354107,
      -2.407966382281281,
      -2.3810273097846397,
      -2.403454969024824,
      -2.357413313590171,
      -2.3516073524061953,
      -2.411667289976138,
      -2.3939023575160903,
      -2.346577849920223,
      -2.403041062983795,
      -2.382248414665003,
      -2.373583860645479,
      -2.3857107597086746,
      -2.402806499752615
    ],
    [
      -1.2696855090443036,
      0.0,
      -1.2194453880871419,
      -1.1783894353750184,
      -1.3195082393240911,
      -1.2692702280912134,
      -1.432246964043354,
      -1.245062049486183,
      -1.285297453577299,
      -1.3273654104786934,
      -1.2478840440209231,
      -1.4036294323140746,
      -1.328513441743511,
      -1.192318953773587,
      -1.3229275509204297,
      -1.2826996795789252,
      -1.342565985212267,
      -1.2905391708598666,
      -1.3596363333708021,
      -1.2356370530769811,
      -1.2312074162645708,
      -1.4146103728685262,
      -1.3495354175119283,
      -1.223878602129388,
      -1.375424133051993,
      -1.2466297875700296,
      -1.312076600675336,
      -1.3217130753119135,
      -1.387535452128759
    ],
    [
      -1.5890706038775952,
      -1.3822543394048863,
      0.0,
      -1.380845131824619,
      -1.5277978458508121,
      -1.4573111880627834,
      -1.6379263931424497,
      -1.4877031330898418,
      -1.4358150910221041,
      -1.5033265162150884,
      -1.4753958329458607,
      -1.6327722705575998,
      -1.5156439673096407,
      -1.358374300687855,
      -1.5959018880377283,
      -1.5380342101792501,
      -1.5080397505701781,
      -1.5556321801288477,
      -1.5654414886391648,
      -1.4074080313771575,
      -1.4563657686704294,
      -1.6557663860398033,
      -1.5572467594607493,
      -1.4371874461854899,
      -1.5770712301279104,
      -1.4547609226136073,
      -1.5635825380291937,
      -1.528730683184071,
      -1.6317059283278132
    ],
    [
      -1.382033986227784,
      -1.188041729477468,
      -1.1937364720882528,
      0.0,
      -1.3951579609724318,
      -1.2280537938879836,
      -1.450141878064666,
      -1.2567622356911978,
      -1.3044840438892975,
      -1.3167332788233872,
      -1.2971896156536746,
      -1.4338098130388506,
      -1.366286625975873,
      -1.2094212706274121,
      -1.3866381008536124,
      -1.3126469182141884,
      -1.329547466591414,
      -1.337336878859469,
      -1.3331349833113442,
      -1.2180443727069445,
      -1.32722931979196,
      -1.4780432176807823,
      -1.3548262013639634,
      -1.183213137888522,
      -1.420843660507371,
      -1.2190218608322174,
      -1.3504602836016728,
      -1.3411536182386041,
      -1.434190250726854
    ],
    [
      -1.469759001461182,
      -1.4237666429289988,
      -1.4470094186918085,
      -1.4863157348193983,
      0.0,
      -1.5020512631382252,
      -1.5748970378445015,
      -1.4321446689571673,
      -1.452979318250598,
      -1.4420875655943548,
      -1.4124144278723698,
      -1.5510607324713646,
      -1.4623443260112319,
      -1.4932308529632332,
      -1.488943056616538,
      -1.474461987566043,
      -1.4713148525447053,
      -1.4256313746963696,
      -1.478392496704431,
      -1.4662788788662022,
      -1.434832661433416,
      -1.5671512707421686,
      -1.398459919427972,
      -1.4591182659240325,
      -1.3733373503564408,
      -1.4738763369479762,
      -1.4098783616976462,
      -1.461222545908097,
      -1.5478490307987205
    ],
    [
      -1.294819140799873,
      -1.1554373525123764,
      -1.0883990021166567,
      -1.0875878209728769,
      -1.2853252785174925,
      0.0,
      -1.3668282846648465,
      -1.1071712760324746,
      -1.2476885631560428,
      -1.2559817380563822,
      -1.2106735208852408,
      -1.3463296107698,
      -1.2313851449438264,
      -1.0269543634245422,
      -1.3006533016331112,
      -1.2609730918784838,
      -1.2426446864276077,
      -1.2577907179442653,
      -1.2497951358030146,
      -1.1216594494990313,
      -1.242462334881299,
      -1.349913375263826,
      -1.254378848559277,
      -1.0962929445274303,
      -1.2837776151663756,
      -1.0897260949677134,
      -1.2298803249252357,
      -1.2331574965797452,
      -1.3096048881655922
    ],
    [
      -1.408792500918508,
      -1.3701733440300403,
      -1.4025062140975288,
      -1.4102939424905478,
      -1.347273664012115,
      -1.3840704368390362,
      0.0,
      -1.340774294238901,
      -1.3558685017046315,
      -1.3772247507730009,
      -1.329471683543035,
      -1.3912389519809016,
      -1.349274326158069,
      -1.3439669453208012,
      -1.3506546794359156,
      -1.3620041305044723,
      -1.4105058232666436,
      -1.3578509131991807,
      -1.386671034492344,
      -1.3458345740490645,
      -1.3968565509937185,
      -1.3686367220757913,
      -1.3615293184994448,
      -1.3380895014062626,
      -1.3697757276575018,
      -1.3886652766800367,
      -1.298340778475509,
      -1.344364687862541,
      -1.3545154085707365
    ],
    [
      -1.606304006752461,
      -1.4242727544469642,
      -1.464404364094239,
      -1.460276413469695,
      -1.5980941572099836,
      -1.474237833698241,
      -1.6495988464424125,
      0.0,
      -1.55206471801398,
      -1.6181227597701682,
      -1.5380206644892196,
      -1.6486232225509934,
      -1.575138799761074,
      -1.4269344464783973,
      -1.600149387345695,
      -1.5683674203840843,
      -1.6198738082268196,
      -1.5454182318035383,
      -1.6375001769413389,
      -1.4124066140136327,
      -1.5612741346277073,
      -1.6418592797762506,
      -1.5892738491529683,
      -1.3950422431569565,
      -1.628197809848384,
      -1.4630341477701643,
      -1.5567360871972562,
      -1.5822383846820858,
      -1.6063854534142414
    ],
    [
      -1.4432664850993098,
      -1.3423776117772135,
      -1.374365550001099,
      -1.3754562899830527,
      -1.4036676939951278,
      -1.4508432620152012,
      -1.5260354228242026,
      -1.4130277893987637,
      0.0,
      -1.387558206720619,
      -1.34180153270851,
      -1.551562978738327,
      -1.3746918065594105,
      -1.3192163746230252,
      -1.43323139503294,
      -1.3204642730809741,
      -1.4458862557057142,
      -1.4051907034811553,
      -1.4244469196077632,
      -1.3566338731073908,
      -1.3602384193302486,
      -1.516188416653561,
      -1.42672511684345,
      -1.3441282522219007,
      -1.4399821053508046,
      -1.412105461677211,
      -1.3689539987885477,
      -1.3912424596200668,
      -1.529656768877875
    ],
    [
      -1.527615777284557,
      -1.542073412325309,
      -1.5409042415644822,
      -1.5240434432726757,
      -1.503452730869632,
      -1.6052287002034735,
      -1.6182015349113843,
      -1.562953524379099,
      -1.513717798014016,
      0.0,
      -1.5228749935246833,
      -1.6133788312094337,
      -1.5320836306075958,
      -1.548424910727913,
      -1.5116257450240382,
      -1.5015646181251934,
      -1.4804326540923367,
      -1.5378116601340537,
      -1.555161426892705,
      -1.529441129381274,
      -1.558566071422631,
      -1.6163777698320596,
      -1.4990749936355043,
      -1.5136655711672347,
      -1.4844230508123093,
      -1.5464267354093022,
      -1.522938675986831,
      -1.5097294410723077,
      -1.6216643494878316
    ],
    [
      -1.4066182060260919,
      -1.355672178206168,
      -1.4304636902119856,
      -1.43548783513647,
      -1.4153970148534543,
      -1.418980630327031,
      -1.532042732596233,
      -1.4546993793357506,
      -1.4243331975766882,
      -1.4503277135936357,
      0.0,
      -1.589980250545248,
      -1.4372366194695583,
      -1.3748812354878939,
      -1.4518650445456673,
      -1.384011517945174,
      -1.4877421485542053,
      -1.428483592626438,
      -1.4598022329510174,
      -1.3804842693885346,
      -1.4404504874942785,
      -1.5475042855950116,
      -1.490507234029664,
      -1.3859682906355666,
      -1.508067794306582,
      -1.425935970787762,
      -1.3575580043525266,
      -1.441576346241278,
      -1.5328127131625038
    ],
    [
      -1.231686977072476,
      -1.204296251738354,
      -1.2355180756857764,
      -1.2140798723841257,
      -1.2094500041964726,
      -1.2279179420584883,
      -1.247471290129637,
      -1.2043072025184565,
      -1.2120652110356653,
      -1.2079181934539047,
      -1.2255199542185244,
      0.0,
      -1.2135144179615742,
      -1.2008356933080642,
      -1.2402349840173499,
      -1.2185664378938812,
      -1.185958002576987,
      -1.2255891928840765,
      -1.2191501849244826,
      -1.2363321178287332,
      -1.2252584556747819,
      -1.2340649441836575,
      -1.20666483315677,
      -1.2212113462617984,
      -1.2114840521410415,
      -1.2115495885671743,
      -1.2339649034314535,
      -1.2242416394043312,
      -1.2181739279025914
    ],
    [
      -1.2745651796227613,
      -1.2242395494380212,
      -1.2060659203050093,
      -1.240828809697672,
      -1.251532429451412,
      -1.2636612167182366,
      -1.3195061294986263,
      -1.2305122245798765,
      -1.2198189111974636,
      -1.2480708014306878,
      -1.2472330028836776,
      -1.3572270325545,
      0.0,
      -1.174991250604402,
      -1.2864708719893232,
      -1.2023017533221871,
      -1.2959142616392458,
      -1.2028795834277592,
      -1.2721233099344398,
      -1.248853362165471,
      -1.1903841239435438,
      -1.3424716161208938,
      -1.2399199860606225,
      -1.2189357482580743,
      -1.2561675798099308,
      -1.250031840893908,
      -1.2096163698741371,
      -1.254367691037679,
      -1.3251666433615406
    ],
    [
      -1.4973035838587778,
      -1.312998976107181,
      -1.2379501114855511,
      -1.2891057735960296,
      -1.470028597441692,
      -1.2672922640325313,
      -1.5052585114664254,
      -1.351724285394877,
      -1.3503844108003735,
      -1.4424661128460605,
      -1.3590127363394806,
      -1.5401588385808418,
      -1.3665561972714828,
      0.0,
      -1.4821699029507411,
      -1.3903266771251583,
      -1.4526598375609177,
      -1.4256957191562476,
      -1.4430662016776572,
      -1.2721829275921301,
      -1.401246796800344,
      -1.5689685196842793,
      -1.4598742005555465,
      -1.226411360173947,
      -1.5002624969110265,
      -1.2434390015509915,
      -1.3565042054686693,
      -1.4102616907771206,
      -1.5214189695524847
    ],
    [
      -1.1975913038102977,
      -1.2125240507450434,
      -1.2280602872819444,
      -1.2070210778562416,
      -1.2222840503103902,
      -1.228162832641458,
      -1.2740012325718157,
      -1.1922795995157152,
      -1.2080006141797486,
      -1.1476335224568441,
      -1.1465617791640736,
      -1.3025863217936051,
      -1.2066789675201788,
      -1.2072990115374536,
      0.0,
      -1.1966137165376114,
      -1.2423392982379766,
      -1.2020960868150747,
      -1.2182668667203573,
      -1.1920863409834028,
      -1.2400410259821544,
      -1.282448816717961,
      -1.203343127609926,
      -1.1905500028131497,
      -1.206611741396459,
      -1.2081886985119967,
      -1.1857926981592475,
      -1.2228465977971628,
      -1.2058581101756347
    ],
    [
      -1.5245685870227275,
      -1.4182108393231827,
      -1.5260676688525325,
      -1.4558464452219102,
      -1.481732465120586,
      -1.4688386700973064,
      -1.5825233110064814,
      -1.4751429258597435,
      -1.3440122947224982,
      -1.4500061364838042,
      -1.4127604414765833,
      -1.615670105019206,
      -1.431783441473891,
      -1.420137153461104,
      -1.4753687348675633,
      0.0,
      -1.5065072629157987,
      -1.4380894087788552,
      -1.454913225674093,
      -1.446080676508879,
      -1.4507874445216762,
      -1.563540523820091,
      -1.486408471788084,
      -1.4395422364323178,
      -1.4937256518791184,
      -1.4702662467338805,
      -1.4544341389987232,
      -1.4737567309494535,
      -1.5820909840778037
    ],
    [
      -1.476328984779059,
      -1.4588462451690005,
      -1.397859531531266,
      -1.3879415507409232,
      -1.4086948426157422,
      -1.4239954786740685,
      -1.5378580936221025,
      -1.4102312094968406,
      -1.3899756095089637,
      -1.3783351213042618,
      -1.429195391828453,
      -1.4549876021838184,
      -1.4419146868491375,
      -1.364778315088711,
      -1.4233134474844022,
      -1.413780824625446,
      0.0,
      -1.4421575161891016,
      -1.4159807227322636,
      -1.4247226455736246,
      -1.4228994116336888,
      -1.512380883025325,
      -1.4049159071919408,
      -1.4203545359705994,
      -1.3468252271607726,
      -1.3953285395362105,
      -1.4196013974875314,
      -1.4503083871250009,
      -1.471601314532663
    ],
    [
      -1.4263124596928953,
      -1.356621238446563,
      -1.3881281015455047,
      -1.3936449263840684,
      -1.4283715733226579,
      -1.4246419496140124,
      -1.5439661548588508,
      -1.3960610616066547,
      -1.3941891996001705,
      -1.4563304169136098,
      -1.3558906761105314,
      -1.5848834917178194,
      -1.3721562090335881,
      -1.3900751873450286,
      -1.483319275305546,
      -1.4271383660490293,
      -1.5295103430812624,
      0.0,
      -1.4338257817249154,
      -1.4161946105663603,
      -1.4074944196875554,
      -1.5785078242910575,
      -1.447759198600625,
      -1.371534459532591,
      -1.493140292905026,
      -1.4339179213035376,
      -1.3758933025584954,
      -1.4131780416571362,
      -1.5702586528075861
    ],
    [
      -1.3337223917630476,
      -1.2501385416978863,
      -1.246105671955928,
      -1.246258541719276,
      -1.3081196157211827,
      -1.2511476722653332,
      -1.4028255350562948,
      -1.317185006460526,
      -1.2163075297782813,
      -1.2888924255450638,
      -1.2287359277343088,
      -1.3915400576114312,
      -1.2993299948905213,
      -1.2228567095859093,
      -1.2896052842630705,
      -1.2896751451938557,
      -1.3077824615496234,
      -1.307762365413204,
      0.0,
      -1.2637542032438225,
      -1.2738109050313677,
      -1.3729631256069352,
      -1.294082061032629,
      -1.226945352351522,
      -1.338318813796247,
      -1.2073110101184497,
      -1.2492554573711152,
      -1.3028854013438473,
      -1.3665729498622066
    ],
    [
      -1.4768861110725129,
      -1.247068672626915,
      -1.2298276461286726,
      -1.2878256450651262,
      -1.4904835953666067,
      -1.3181485459006084,
      -1.51336173724094,
      -1.272528553461877,
      -1.3919500862613297,
      -1.4322851404337382,
      -1.368829560566138,
      -1.5379766993666588,
      -1.3822813944642018,
      -1.2826045255828655,
      -1.4730295262458142,
      -1.4129439089195392,
      -1.4653775396703488,
      -1.4421764719551422,
      -1.4883858032282664,
      0.0,
      -1.4419689897031465,
      -1.5287974556499637,
      -1.4224734847033,
      -1.2012380678412704,
      -1.4990144258323004,
      -1.2683590071071806,
      -1.3824056243181218,
      -1.4045086544196785,
      -1.5046096247442737
    ],
    [
      -1.4455114336823403,
      -1.3449030674498628,
      -1.4222236672870916,
      -1.404009717258982,
      -1.4472301662232834,
      -1.4679769400949239,
      -1.5150277173389302,
      -1.4363489576957829,
      -1.3563593074066536,
      -1.4580814731266212,
      -1.3968802142959547,
      -1.52209155450532,
      -1.442057340632293,
      -1.4026555117023352,
      -1.476023741097141,
      -1.439385442099324,
      -1.4829209381723818,
      -1.457976331250582,
      -1.4631713091662992,
      -1.440422188258694,
      0.0,
      -1.5350382169985797,
      -1.4637900008787403,
      -1.4472981139235237,
      -1.4874531737825558,
      -1.4228558615477407,
      -1.438637664180186,
      -1.456139872245003,
      -1.5050881640458675
    ],
    [
      -1.1789535535133675,
      -1.069672855317496,
      -1.13092954597848,
      -1.1053297705340595,
      -1.0588328575941106,
      -1.0851148332799947,
      -1.057653874702234,
      -1.0185173366028397,
      -1.073002877680009,
      -1.0658106178826967,
      -1.046054348343568,
      -1.108453535112668,
      -1.0488871835361935,
      -1.059440972551422,
      -1.058149830232697,
      -1.054026941996974,
      -1.1278703676508568,
      -1.0885493333420082,
      -1.1146664438374474,
      -1.107172107918815,
      -1.077659446628028,
      0.0,
      -1.067829604720844,
      -1.0462520829216697,
      -1.0888940099068063,
      -1.093608093748422,
      -1.0823288183982986,
      -1.0657023184051426,
      -0.9544790667478367
    ],
    [
      -1.5745174175989818,
      -1.5550911327529964,
      -1.5844097778612105,
      -1.5709540388625252,
      -1.465760695026782,
      -1.5765874814417236,
      -1.6091327104158557,
      -1.5270181183457148,
      -1.5382329980549412,
      -1.48953429518831,
      -1.5391720838759964,
      -1.6305314217436182,
      -1.5236409259562416,
      -1.5643200904070997,
      -1.5799054665245087,
      -1.5096117456813531,
      -1.5496892258939319,
      -1.537163145207074,
      -1.5705095985700306,
      -1.5609872783701124,
      -1.5470386622359371,
      -1.6080342693281575,
      0.0,
      -1.5356612051558902,
      -1.5153541701998845,
      -1.5568851790624036,
      -1.5309364174300624,
      -1.5577685211888461,
      -1.5879237896494722
    ],
    [
      -1.518813832725461,
      -1.4224275376608224,
      -1.4760944857955036,
      -1.4645913815883813,
      -1.5585842621782324,
      -1.4749166442050932,
      -1.6659153672119202,
      -1.4307965712399517,
      -1.4625181088841197,
      -1.5138877467398126,
      -1.4938090896123082,
      -1.6730028976918887,
      -1.520802817231582,
      -1.4147646420143973,
      -1.5609684748852841,
      -1.4562934817588795,
      -1.6056082662149778,
      -1.6102684863457049,
      -1.5651736971541281,
      -1.4076157599104608,
      -1.5630044562185275,
      -1.6637945342060656,
      -1.5702360922630338,
      0.0,
      -1.5821588708880496,
      -1.3942248750719417,
      -1.5715351471299657,
      -1.5513879249344649,
      -1.6190207762548054
    ],
    [
      -1.4664118816115668,
      -1.4443023236135129,
      -1.4296664996075643,
      -1.4773841540543171,
      -1.3181680656943529,
      -1.4703123251499044,
      -1.5419216838478902,
      -1.445631142813966,
      -1.4206620784785986,
      -1.3666529984648736,
      -1.4365408102809845,
      -1.5350548782113809,
      -1.4500892427482959,
      -1.4507375488972418,
      -1.4636680906607367,
      -1.429357411715112,
      -1.3455777712458945,
      -1.4342396610520238,
      -1.4720680619546664,
      -1.4397021674635473,
      -1.4384134916238625,
      -1.566750234000468,
      -1.3880904402647205,
      -1.4036373981480597,
      0.0,
      -1.4704563452700332,
      -1.398803497892535,
      -1.4024841240888815,
      -1.5131410028483114
    ],
    [
      -1.5049639591645743,
      -1.3866580029865672,
      -1.389164068435484,
      -1.3738826638730255,
      -1.5692009492153989,
      -1.4389260881094708,
      -1.6356413743611269,
      -1.3962258362731401,
      -1.4979464978280974,
      -1.5280437377298401,
      -1.4901587922549198,
      -1.6028308548956745,
      -1.4494929472681581,
      -1.3418206003615178,
      -1.5534885380511274,
      -1.4962256806383452,
      -1.5321580902865144,
      -1.5383468919072396,
      -1.5415376245986598,
      -1.3316505106719347,
      -1.4909744976598651,
      -1.6176376914564201,
      -1.51562237667724,
      -1.3495178557822918,
      -1.626143958589496,
      0.0,
      -1.5196927963242106,
      -1.5819395868385788,
      -1.6267650720562379
    ],
    [
      -1.4352699222010616,
      -1.4334954963648956,
      -1.479552713622871,
      -1.4523160415556566,
      -1.4166004453458738,
      -1.5176869182053256,
      -1.5087790174553235,
      -1.4313378004109574,
      -1.3994031553251771,
      -1.4285100855359665,
      -1.3828019876009252,
      -1.5671062829116544,
      -1.4479807530637905,
      -1.4113934774786145,
      -1.4795889789958563,
      -1.480303284597685,
      -1.4509823494284815,
      -1.441974650299637,
      -1.4588915611801123,
      -1.426311752837808,
      -1.414067872502319,
      -1.5510243710984128,
      -1.4507776041628486,
      -1.4324893828146752,
      -1.4570509073904123,
      -1.4391533123944344,
      0.0,
      -1.4487932120948623,
      -1.5269712043789774
    ],
    [
      -1.2254260715923282,
      -1.1620285597573605,
      -1.1605114700511125,
      -1.155214184117751,
      -1.1607943379280194,
      -1.1539074446691708,
      -1.2296507589469354,
      -1.1687114661868196,
      -1.148971791919918,
      -1.1377350005807545,
      -1.1315458348412708,
      -1.266573334676022,
      -1.1758882348485609,
      -1.089066691845482,
      -1.1870199030350737,
      -1.1512701156339153,
      -1.1827045208070641,
      -1.1225887501446472,
      -1.1923209819621783,
      -1.1398073883699864,
      -1.1898930216792416,
      -1.2506172485195155,
      -1.2002519459923533,
      -1.1363292796743558,
      -1.162298628098052,
      -1.1872495707298543,
      -1.1405164871810527,
      0.0,
      -1.2132792290020076
    ],
    [
      -1.3541267379643955,
      -1.3128716325613121,
      -1.3440164449156142,
      -1.3258030677206618,
      -1.2936474360003385,
      -1.3221216481477165,
      -1.2862827458690704,
      -1.2301624848999002,
      -1.2720765199957713,
      -1.2725256748648803,
      -1.2704455253263973,
      -1.2668948218674285,
      -1.274594413130162,
      -1.2733723547309572,
      -1.2876201503035676,
      -1.3184806358908563,
      -1.2990410151321397,
      -1.2913581093551825,
      -1.3295083442443243,
      -1.3245314870877491,
      -1.2899628665862428,
      -1.2392634901403305,
      -1.2888220043294318,
      -1.2842696816958967,
      -1.2688182057137336,
      -1.3066642835903581,
      -1.304872096868528,
      -1.285448931901964,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.19924551574680383,
      0.18122641166555287,
      0.1804648501768371,
      0.1629113884589546,
      0.1682104379128737,
      0.11700137542587097,
      0.1392223167030857,
      0.17565782619005388,
      0.1547852218337007,
      0.20449222991402527,
      0.1431765850468545,
      0.16934490859645734,
      0.1727434924533906,
      0.14649082312930872,
      0.16528174729215594,
      0.14041490136498203,
      0.1673539738616232,
      0.144926314621439,
      0.190967970056092,
      0.1967739312400676,
      0.1367139936701247,
      0.15447892613017267,
      0.20180343372603993,
      0.14534022066246788,
      0.16613286898126,
      0.17479742300078405,
      0.1626705239375883,
      0.14557478389364809
    ],
    [
      0.35450974304972527,
      0.0,
      0.40474986400688695,
      0.44580581671901043,
      0.3046870127699377,
      0.35492502400281545,
      0.19194828805067488,
      0.3791332026078458,
      0.33889779851672985,
      0.29682984161533543,
      0.3763112080731057,
      0.22056581977995426,
      0.2956818103505179,
      0.43187629832044183,
      0.30126770117359913,
      0.34149557251510365,
      0.2816292668817617,
      0.33365608123416224,
      0.2645589187232267,
      0.3885581990170477,
      0.39298783582945807,
      0.20958487922550262,
      0.2746598345821005,
      0.4003166499646409,
      0.24877111904203586,
      0.3775654645239992,
      0.3121186514186929,
      0.3024821767821153,
      0.23665979996526976
    ],
    [
      0.31668278379398607,
      0.523499048266695,
      0.0,
      0.5249082558469622,
      0.3779555418207692,
      0.44844219960879794,
      0.2678269945291316,
      0.4180502545817395,
      0.46993829664947717,
      0.4024268714564929,
      0.43035755472572057,
      0.27298111711398154,
      0.39010942036194063,
      0.5473790869837263,
      0.30985149963385306,
      0.3677191774923312,
      0.3977136371014032,
      0.35012120754273357,
      0.34031189903241654,
      0.4983453562944238,
      0.4493876190011519,
      0.249987001631778,
      0.348506628210832,
      0.46856594148609143,
      0.3286821575436709,
      0.45099246505797397,
      0.34217084964238764,
      0.3770227044875103,
      0.27404745934376806
    ],
    [
      0.3466702692941237,
      0.5406625260444398,
      0.5349677834336548,
      0.0,
      0.3335462945494758,
      0.500650461633924,
      0.27856237745724166,
      0.4719420198307098,
      0.42422021163261014,
      0.41197097669852045,
      0.431514639868233,
      0.2948944424830571,
      0.3624176295460346,
      0.5192829848944955,
      0.34206615466829526,
      0.4160573373077192,
      0.39915678893049367,
      0.3913673766624386,
      0.3955692722105635,
      0.5106598828149631,
      0.40147493572994764,
      0.25066103784112537,
      0.3738780541579443,
      0.5454911176333856,
      0.30786059501453655,
      0.5096823946896902,
      0.37824397192023484,
      0.3875506372833035,
      0.29451400479505363
    ],
    [
      0.3487169594288335,
      0.3947093179610168,
      0.3714665421982071,
      0.33216022607061735,
      0.0,
      0.3164246977517904,
      0.2435789230455141,
      0.3863312919328483,
      0.3654966426394177,
      0.3763883952956608,
      0.4060615330176458,
      0.26741522841865106,
      0.35613163487878374,
      0.3252451079267824,
      0.3295329042734776,
      0.34401397332397265,
      0.34716110834531033,
      0.39284458619364604,
      0.34008346418558455,
      0.3521970820238134,
      0.3836432994565997,
      0.251324690147847,
      0.4200160414620435,
      0.35935769496598313,
      0.44513861053357484,
      0.34459962394203947,
      0.4085975991923694,
      0.35725341498191865,
      0.2706269300912951
    ],
    [
      0.3259820736091785,
      0.4653638618966751,
      0.5324022122923948,
      0.5332133934361747,
      0.3354759358915591,
      0.0,
      0.25397292974420504,
      0.513629938376577,
      0.37311265125300874,
      0.3648194763526693,
      0.41012769352381073,
      0.2744716036392516,
      0.3894160694652251,
      0.5938468509845094,
      0.32014791277594035,
      0.35982812253056773,
      0.37815652798144384,
      0.3630104964647862,
      0.37100607860603696,
      0.49914176491002027,
      0.37833887952775247,
      0.27088783914522563,
      0.3664223658497745,
      0.5245082698816212,
      0.337023599242676,
      0.5310751194413381,
      0.39092088948381587,
      0.3876437178293064,
      0.3111963262434594
    ],
    [
      0.3210829342358079,
      0.35970209112427565,
      0.3273692210567871,
      0.3195814926637681,
      0.38260177114220095,
      0.3458049983152798,
      0.0,
      0.38910114091541503,
      0.3740069334496845,
      0.3526506843813151,
      0.40040375161128106,
      0.3386364831734143,
      0.3806011089962469,
      0.3859084898335148,
      0.3792207557184004,
      0.36787130464984363,
      0.3193696118876723,
      0.37202452195513525,
      0.3432044006619719,
      0.3840408611052515,
      0.33301888416059744,
      0.3612387130785246,
      0.36834611665487116,
      0.39178593374805337,
      0.3600997074968142,
      0.34121015847427927,
      0.43153465667880697,
      0.3855107472917749,
      0.3753600265835795
    ],
    [
      0.3459942058269705,
      0.5280254581324673,
      0.48789384848519246,
      0.4920217991097364,
      0.3542040553694479,
      0.4780603788811906,
      0.302699366137019,
      0.0,
      0.40023349456545154,
      0.3341754528092633,
      0.4142775480902119,
      0.3036749900284381,
      0.3771594128183575,
      0.5253637661010342,
      0.3521488252337366,
      0.3839307921953472,
      0.3324244043526119,
      0.40687998077589316,
      0.3147980356380926,
      0.5398915985657988,
      0.3910240779517242,
      0.3104389328031809,
      0.36302436342646316,
      0.557255969422475,
      0.32410040273104745,
      0.4892640648092672,
      0.3955621253821753,
      0.3700598278973457,
      0.34591275916519004
    ],
    [
      0.377550688879027,
      0.47843956220112327,
      0.4464516239772378,
      0.44536088399528406,
      0.41714947998320895,
      0.36997391196313556,
      0.2947817511541342,
      0.40778938457957303,
      0.0,
      0.43325896725771784,
      0.47901564126982676,
      0.2692541952400098,
      0.4461253674189263,
      0.5016007993553115,
      0.3875857789453967,
      0.5003529008973626,
      0.3749309182726226,
      0.4156264704971815,
      0.3963702543705736,
      0.46418330087094595,
      0.4605787546480882,
      0.3046287573247757,
      0.3940920571348867,
      0.47668892175643607,
      0.38083506862753214,
      0.4087117123011257,
      0.451863175189789,
      0.42957471435827,
      0.2911604051004617
    ],
    [
      0.3386511585206917,
      0.3241935234799398,
      0.3253626942407666,
      0.3422234925325731,
      0.3628142049356169,
      0.2610382356017753,
      0.24806540089386453,
      0.30331341142614976,
      0.35254913779123287,
      0.0,
      0.3433919422805656,
      0.2528881045958151,
      0.33418330519765305,
      0.3178420250773357,
      0.3546411907812106,
      0.36470231768005545,
      0.3858342817129121,
      0.32845527567119515,
      0.31110550891254385,
      0.33682580642397486,
      0.3077008643826178,
      0.24988916597318922,
      0.3671919421697445,
      0.35260136463801417,
      0.38184388499293953,
      0.31984020039594663,
      0.34332825981841775,
      0.35653749473294116,
      0.24460258631741727
    ],
    [
      0.4589277975827253,
      0.5098738254026491,
      0.43508231339683157,
      0.43005816847234724,
      0.4501489887553629,
      0.44656537328178625,
      0.33350327101258426,
      0.4108466242730666,
      0.441212806032129,
      0.4152182900151815,
      0.0,
      0.2755657530635691,
      0.4283093841392589,
      0.4906647681209233,
      0.41368095906314983,
      0.4815344856636432,
      0.3778038550546119,
      0.4370624109823791,
      0.40574377065779976,
      0.48506173422028254,
      0.42509551611453866,
      0.31804171801380554,
      0.3750387695791533,
      0.4795777129732506,
      0.35747820930223506,
      0.43961003282105526,
      0.5079879992562906,
      0.4239696573675391,
      0.3327332904463134
    ],
    [
      0.23461021548247096,
      0.2620009408165931,
      0.23077911686917063,
      0.2522173201708213,
      0.2568471883584744,
      0.2383792504964588,
      0.21882590242531008,
      0.2619899900364906,
      0.2542319815192817,
      0.25837899910104234,
      0.24077723833642262,
      0.0,
      0.25278277459337284,
      0.2654614992468829,
      0.22606220853759718,
      0.24773075466106587,
      0.28033918997796015,
      0.24070799967087053,
      0.2471470076304645,
      0.22996507472621386,
      0.2410387368801652,
      0.23223224837128953,
      0.2596323593981771,
      0.24508584629314867,
      0.25481314041390557,
      0.25474760398777274,
      0.23233228912349357,
      0.2420555531506159,
      0.24812326465235568
    ],
    [
      0.34122919830624276,
      0.3915548284909829,
      0.4097284576239948,
      0.374965568231332,
      0.36426194847759197,
      0.35213316121076743,
      0.2962882484303777,
      0.3852821533491275,
      0.3959754667315405,
      0.3677235764983162,
      0.3685613750453265,
      0.2585673453745041,
      0.0,
      0.44080312732460203,
      0.32932350593968085,
      0.4134926246068169,
      0.3198801162897582,
      0.4129147945012448,
      0.34367106799456426,
      0.3669410157635331,
      0.4254102539854603,
      0.2733227618081102,
      0.37587439186838156,
      0.3968586296709298,
      0.3596267981190733,
      0.3657625370350961,
      0.4061780080548669,
      0.36142668689132496,
      0.2906277345674635
    ],
    [
      0.3444100846814704,
      0.5287146924330672,
      0.6037635570546971,
      0.5526078949442186,
      0.3716850710985562,
      0.574421404507717,
      0.3364551570738228,
      0.4899893831453712,
      0.4913292577398747,
      0.3992475556941877,
      0.4827009322007676,
      0.3015548299594064,
      0.4751574712687654,
      0.0,
      0.35954376558950707,
      0.45138699141508987,
      0.38905383097933055,
      0.4160179493840006,
      0.398647466862591,
      0.5695307409481181,
      0.4404668717399043,
      0.2727451488559689,
      0.3818394679847017,
      0.6153023083663012,
      0.3414511716292217,
      0.5982746669892567,
      0.48520946307157886,
      0.4314519777631276,
      0.3202946989877635
    ],
    [
      0.4057278465423728,
      0.39079509960762704,
      0.3752588630707261,
      0.3962980724964289,
      0.38103510004228025,
      0.3751563177112125,
      0.32931791778085473,
      0.4110395508369553,
      0.3953185361729219,
      0.45568562789582634,
      0.45675737118859683,
      0.30073282855906536,
      0.3966401828324917,
      0.3960201388152169,
      0.0,
      0.4067054338150591,
      0.36097985211469386,
      0.40122306353759574,
      0.38505228363231314,
      0.41123280936926765,
      0.3632781243705161,
      0.3208703336347094,
      0.39997602274274446,
      0.41276914753952076,
      0.3967074089562115,
      0.39513045184067375,
      0.417526452193423,
      0.3804725525555077,
      0.39746104017703576
    ],
    [
      0.35803923705388874,
      0.4643969847534335,
      0.35654015522408367,
      0.426761378854706,
      0.4008753589560301,
      0.4137691539793098,
      0.30008451307013484,
      0.4074648982168727,
      0.538595529354118,
      0.43260168759281203,
      0.4698473826000329,
      0.26693771905741026,
      0.45082438260272517,
      0.46247067061551217,
      0.40723908920905294,
      0.0,
      0.37610056116081747,
      0.44451841529776104,
      0.4276945984025231,
      0.43652714756773725,
      0.43182037955494,
      0.3190673002565252,
      0.3961993522885321,
      0.4430655876442984,
      0.38888217219749777,
      0.4123415773427357,
      0.428173685077893,
      0.4088510931271627,
      0.3005168399988125
    ],
    [
      0.31469680546630796,
      0.3321795450763665,
      0.39316625871410094,
      0.40308423950444383,
      0.38233094762962483,
      0.3670303115712985,
      0.25316769662326455,
      0.38079458074852646,
      0.4010501807364033,
      0.4126906689411052,
      0.361830398416914,
      0.3360381880615486,
      0.3491111033962295,
      0.42624747515665606,
      0.36771234276096476,
      0.37724496561992105,
      0.0,
      0.3488682740562654,
      0.3750450675131034,
      0.3663031446717424,
      0.3681263786116782,
      0.2786449072200421,
      0.3861098830534262,
      0.37067125427476766,
      0.44420056308459444,
      0.3956972507091565,
      0.37142439275783556,
      0.34071740312036614,
      0.31942447571270405
    ],
    [
      0.4611426005119843,
      0.5308338217583166,
      0.49932695865937493,
      0.4938101338208112,
      0.4590834868822218,
      0.46281311059086727,
      0.3434889053460288,
      0.49139399859822497,
      0.4932658606047091,
      0.43112464329126987,
      0.5315643840943483,
      0.30257156848706024,
      0.5152988511712915,
      0.4973798728598511,
      0.40413578489933366,
      0.4603166941558503,
      0.3579447171236172,
      0.0,
      0.45362927847996426,
      0.4712604496385193,
      0.47996064051732423,
      0.3089472359138221,
      0.43969586160425456,
      0.5159206006722887,
      0.3943147672998537,
      0.45353713890134206,
      0.5115617576463842,
      0.4742770185477434,
      0.3171964073972935
    ],
    [
      0.3363516201736083,
      0.41993547023876965,
      0.4239683399807279,
      0.42381547021738,
      0.3619543962154732,
      0.41892633967132276,
      0.26724847688036113,
      0.35288900547613,
      0.4537664821583747,
      0.38118158639159216,
      0.4413380842023471,
      0.27853395432522476,
      0.3707440170461347,
      0.44721730235074664,
      0.3804687276735854,
      0.3803988667428002,
      0.36229155038703253,
      0.36231164652345194,
      0.0,
      0.40631980869283346,
      0.3962631069052882,
      0.2971108863297207,
      0.37599195090402704,
      0.44312865958513403,
      0.331755198140409,
      0.4627630018182063,
      0.42081855456554074,
      0.3671886105928086,
      0.3035010620744494
    ],
    [
      0.3516240415571248,
      0.5814414800027228,
      0.5986825065009651,
      0.5406845075645115,
      0.338026557263031,
      0.5103616067290293,
      0.31514841538869764,
      0.5559815991677606,
      0.436560066368308,
      0.39622501219589945,
      0.45968059206349965,
      0.29053345326297886,
      0.44622875816543583,
      0.5459056270467721,
      0.3554806263838235,
      0.4155662437100984,
      0.3631326129592889,
      0.38633368067449547,
      0.34012434940137126,
      0.0,
      0.3865411629264912,
      0.29971269697967395,
      0.40603666792633764,
      0.6272720847883673,
      0.3294957267973373,
      0.560151145522457,
      0.4461045283115159,
      0.42400149820995914,
      0.32390052788536394
    ],
    [
      0.2889041762716311,
      0.3895125425041086,
      0.3121919426668798,
      0.3304058926949893,
      0.287185443730688,
      0.2664386698590475,
      0.2193878926150412,
      0.2980666522581885,
      0.3780563025473178,
      0.2763341368273502,
      0.3375353956580167,
      0.2123240554486514,
      0.2923582693216784,
      0.3317600982516362,
      0.2583918688568303,
      0.2950301678546474,
      0.2514946717815896,
      0.27643927870338936,
      0.27124430078767214,
      0.2939934216952773,
      0.0,
      0.1993773929553917,
      0.27062560907523103,
      0.28711749603044767,
      0.24696243617141556,
      0.3115597484062307,
      0.29577794577378547,
      0.2782757377089684,
      0.22932744590810383
    ],
    [
      0.2648463992592196,
      0.3741270974550912,
      0.31287040679410705,
      0.33847018223852765,
      0.3849670951784765,
      0.3586851194925924,
      0.38614607807035317,
      0.4252826161697474,
      0.3707970750925782,
      0.3779893348898904,
      0.3977456044290191,
      0.335346417659919,
      0.3949127692363936,
      0.3843589802211651,
      0.38565012253989006,
      0.389773010775613,
      0.3159295851217303,
      0.3552506194305789,
      0.3291335089351397,
      0.33662784485377206,
      0.36614050614455906,
      0.0,
      0.3759703480517431,
      0.3975478698509174,
      0.3549059428657808,
      0.35019185902416505,
      0.36147113437428846,
      0.3780976343674445,
      0.48932088602475043
    ],
    [
      0.3743475305483004,
      0.3937738153942858,
      0.36445517028607166,
      0.377910909284757,
      0.4831042531205001,
      0.3722774667055586,
      0.33973223773142647,
      0.4218468298015674,
      0.410631950092341,
      0.45933065295897224,
      0.4096928642712858,
      0.31833352640366397,
      0.4252240221910406,
      0.38454485774018243,
      0.3689594816227735,
      0.439253202465929,
      0.3991757222533503,
      0.4117018029402082,
      0.3783553495772516,
      0.38787766977716975,
      0.40182628591134506,
      0.34083067881912466,
      0.0,
      0.41320374299139195,
      0.4335107779473977,
      0.3919797690848785,
      0.4179285307172198,
      0.39109642695843605,
      0.36094115849780994
    ],
    [
      0.4434230440764795,
      0.5398093391411181,
      0.48614239100643686,
      0.49764549521355916,
      0.40365261462370805,
      0.4873202325968473,
      0.29632150959002024,
      0.5314403055619887,
      0.49971876791782077,
      0.44834913006212784,
      0.4684277871896323,
      0.2892339791100518,
      0.44143405957035853,
      0.5474722347875431,
      0.40126840191665636,
      0.505943395043061,
      0.3566286105869627,
      0.3519683904562356,
      0.39706317964781235,
      0.5546211168914796,
      0.399232420583413,
      0.2984423425958749,
      0.39200078453890663,
      0.0,
      0.38007800591389085,
      0.5680120017299988,
      0.3907017296719748,
      0.4108489518674756,
      0.34321610054713503
    ],
    [
      0.35818378660246286,
      0.3802933446005168,
      0.3949291686064653,
      0.3472115141597125,
      0.5064276025196768,
      0.35428334306412523,
      0.28267398436613944,
      0.37896452540006353,
      0.40393358973543103,
      0.457942669749156,
      0.38805485793304517,
      0.28954079000264876,
      0.37450642546573376,
      0.3738581193167878,
      0.36092757755329297,
      0.39523825649891764,
      0.4790178969681351,
      0.3903560071620058,
      0.3525276062593632,
      0.38489350075048234,
      0.3861821765901672,
      0.25784543421356165,
      0.4365052279493091,
      0.42095827006597,
      0.0,
      0.3541393229439964,
      0.42579217032149463,
      0.4221115441251482,
      0.3114546653657182
    ],
    [
      0.38032099265955543,
      0.49862694883756253,
      0.4961208833886457,
      0.5114022879511042,
      0.3160840026087308,
      0.44635886371465894,
      0.2496435774630028,
      0.4890591155509896,
      0.3873384539960323,
      0.35724121409428955,
      0.39512615956920993,
      0.28245409692845524,
      0.43579200455597156,
      0.5434643514626118,
      0.3317964137730023,
      0.38905927118578454,
      0.3531268615376153,
      0.3469380599168901,
      0.34374732722546986,
      0.553634441152195,
      0.3943104541642646,
      0.2676472603677096,
      0.3696625751468896,
      0.5357670960418379,
      0.2591409932346338,
      0.0,
      0.3655921554999191,
      0.30334536498555087,
      0.2585198797678918
    ],
    [
      0.38809015134890523,
      0.3898645771850713,
      0.3438073599270959,
      0.37104403199431024,
      0.40675962820409306,
      0.30567315534464123,
      0.3145810560946434,
      0.3920222731390095,
      0.42395691822478976,
      0.3948499880140004,
      0.44055808594904167,
      0.25625379063831244,
      0.3753793204861764,
      0.41196659607135233,
      0.3437710945541106,
      0.343056788952282,
      0.3723777241214854,
      0.38138542325033,
      0.36446851236985456,
      0.3970483207121589,
      0.40929220104764785,
      0.27233570245155403,
      0.37258246938711825,
      0.3908706907352917,
      0.36630916615955456,
      0.38420676115553243,
      0.0,
      0.37456686145510454,
      0.2963888691709895
    ],
    [
      0.3018123327639517,
      0.36520984459891936,
      0.36672693430516734,
      0.3720242202385289,
      0.36644406642826044,
      0.37333095968710905,
      0.29758764540934446,
      0.35852693816946024,
      0.37826661243636184,
      0.3895034037755254,
      0.3956925695150091,
      0.26066506968025793,
      0.351350169507719,
      0.4381717125107978,
      0.34021850132120623,
      0.37596828872236454,
      0.34453388354921577,
      0.4046496542116327,
      0.3349174223941016,
      0.3874310159862935,
      0.3373453826770383,
      0.2766211558367644,
      0.3269864583639266,
      0.39090912468192407,
      0.3649397762582278,
      0.3399888336264256,
      0.3867219171752272,
      0.0,
      0.3139591753542723
    ],
    [
      0.3140992882556475,
      0.3553543936587309,
      0.3242095813044288,
      0.3424229584993812,
      0.3745785902197045,
      0.3461043780723265,
      0.3819432803509726,
      0.4380635413201428,
      0.3961495062242717,
      0.3957003513551627,
      0.3977805008936457,
      0.4013312043526145,
      0.3936316130898809,
      0.3948536714890858,
      0.38060587591647543,
      0.3497453903291867,
      0.36918501108790336,
      0.37686791686486054,
      0.3387176819757187,
      0.34369453913229386,
      0.3782631596338002,
      0.42896253607971246,
      0.3794040218906112,
      0.3839563445241463,
      0.3994078205063094,
      0.3615617426296849,
      0.3633539293515151,
      0.38277709431807905,
      0.0
    ]
  ],
  "row_avgs": [
    0.16457872841757912,
    0.32365120995506064,
    0.3908565367586338,
    0.4055548635366506,
    0.35130419727447304,
    0.3983979500135359,
    0.36397455360873454,
    0.4007321405966118,
    0.410854837413213,
    0.3254148850420392,
    0.4209427673237308,
    0.24676056053313886,
    0.3638709065068219,
    0.44368763615601375,
    0.38982744392970886,
    0.40607881610919133,
    0.3651288822574771,
    0.4484213053383554,
    0.3810065062951243,
    0.42967634913404706,
    0.28521710687015017,
    0.36759128744812325,
    0.39542309593193714,
    0.433229154372806,
    0.3810269063674831,
    0.3879043252421598,
    0.36726669707658777,
    0.35501796675660835,
    0.37474021154736764
  ],
  "col_avgs": [
    0.34987957020652477,
    0.42543355345747746,
    0.40498716309773747,
    0.4070921591822441,
    0.36881421518691626,
    0.3826270915699378,
    0.28428511329142986,
    0.40319491222048665,
    0.4008667262990096,
    0.3798080148944422,
    0.4049866187832353,
    0.28194561213909897,
    0.38110200879538575,
    0.4322753573328167,
    0.34422106765871974,
    0.3831677885036639,
    0.3494924178530826,
    0.366673405657964,
    0.346745140239626,
    0.412420557808275,
    0.3830543871530909,
    0.28064688398373694,
    0.36252673398329666,
    0.4302984915697383,
    0.3451312657458874,
    0.40495462564948514,
    0.3879926515954186,
    0.368279915239087,
    0.30523437871554887
  ],
  "combined_avgs": [
    0.25722914931205193,
    0.3745423817062691,
    0.39792184992818563,
    0.40632351135944733,
    0.36005920623069465,
    0.3905125207917368,
    0.32412983345008217,
    0.4019635264085492,
    0.4058607818561113,
    0.3526114499682407,
    0.41296469305348305,
    0.2643530863361189,
    0.37248645765110383,
    0.4379814967444152,
    0.36702425579421427,
    0.3946233023064276,
    0.35731065005527984,
    0.4075473554981597,
    0.3638758232673751,
    0.42104845347116104,
    0.3341357470116205,
    0.32411908571593007,
    0.3789749149576169,
    0.43176382297127214,
    0.36307908605668526,
    0.39642947544582247,
    0.3776296743360032,
    0.36164894099784767,
    0.33998729513145826
  ],
  "gppm": [
    612.0947598853403,
    622.196408147276,
    631.0631724774522,
    633.993426422675,
    650.594989805476,
    647.5495097253989,
    691.046117415606,
    634.7125084969339,
    635.6133236253099,
    645.2055829563658,
    633.8158124594873,
    689.6127648061258,
    648.3520859271915,
    621.66012632069,
    664.9863057252143,
    643.805909020405,
    659.2455414396046,
    653.6930729006278,
    663.5500232892415,
    630.8334633451449,
    636.9273265759633,
    697.6162654023336,
    655.1132070350753,
    621.2839281336363,
    662.2682847949321,
    635.3002750111108,
    642.6506158254872,
    654.9218095484035,
    683.6537961769056
  ],
  "gppm_normalized": [
    1.4413224433057295,
    1.5276591284800818,
    1.5531259169227363,
    1.5630635961359158,
    1.5961974738626414,
    1.594102959418386,
    1.6972644695142716,
    1.5693030008987117,
    1.5630731086941707,
    1.5891014803673897,
    1.556338574196188,
    1.7129063766867891,
    1.5925424648859665,
    1.5284276978482094,
    1.639335011059732,
    1.5840013159903301,
    1.6228115223801716,
    1.6084645226845862,
    1.631323644374333,
    1.552189742437785,
    1.5691023581209917,
    1.7140874839378164,
    1.6105769755477428,
    1.5339403825852886,
    1.6301443266938438,
    1.5645540600600103,
    1.5783974950216364,
    1.6114506722715214,
    1.6808561749544555
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377,
    333,
    535,
    441,
    415,
    440,
    532,
    467,
    476,
    390,
    400,
    461,
    506,
    501,
    483,
    418,
    403,
    393,
    415,
    434,
    464,
    456,
    391,
    393,
    426,
    444,
    420,
    400,
    420,
    534,
    448,
    422,
    431,
    410,
    445,
    437,
    448,
    356,
    418,
    400,
    408,
    567,
    423,
    428,
    389,
    424,
    419,
    417,
    416,
    470,
    368,
    384,
    403,
    423,
    394,
    366,
    382,
    442,
    412,
    532,
    444,
    478,
    502,
    430,
    533,
    483,
    440,
    480,
    379,
    427,
    503,
    458,
    484,
    371,
    423,
    429,
    400,
    409,
    416,
    406,
    403,
    340,
    427,
    412,
    405,
    403,
    478,
    337,
    881,
    516,
    435,
    467,
    432,
    482,
    427,
    429,
    438,
    480,
    526,
    521,
    442,
    442,
    405,
    427,
    443,
    406,
    409,
    416,
    406,
    404,
    382,
    465,
    409,
    391,
    417,
    484,
    418,
    1833,
    463,
    409,
    431,
    428,
    476,
    461,
    399,
    432,
    424,
    452,
    363,
    494,
    429,
    456,
    471,
    385,
    410,
    444,
    447,
    419,
    436,
    403,
    412,
    406,
    410,
    428,
    387,
    466,
    526,
    485,
    454,
    438,
    400,
    422,
    425,
    397,
    393,
    505,
    445,
    462,
    393,
    394,
    428,
    499,
    360,
    369,
    419,
    488,
    392,
    493,
    368,
    405,
    433,
    442,
    426,
    399,
    369,
    841,
    559,
    462,
    436,
    436,
    428,
    402,
    420,
    440,
    435,
    424,
    530,
    439,
    440,
    430,
    421,
    436,
    366,
    413,
    433,
    585,
    372,
    369,
    432,
    411,
    394,
    425,
    443,
    374
  ],
  "response_lengths": [
    4030,
    3226,
    2641,
    2438,
    2519,
    2405,
    2436,
    2407,
    2549,
    2527,
    2514,
    3022,
    2494,
    2441,
    2414,
    2426,
    2428,
    2213,
    2333,
    2429,
    3162,
    2150,
    2116,
    2604,
    2406,
    2126,
    2498,
    2453,
    2178
  ]
}