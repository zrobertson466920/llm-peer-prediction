{
  "example_idx": 33,
  "reference": "Under review as a conference paper at ICLR 2023\n\nIMPROVING LANGUAGE MODEL PRETRAINING WITH TEXT STRUCTURE INFORMATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nInter-sentence pretraining tasks learn from sentence relationships and facilitate high-level language understanding that cannot be directly learned in word-level pretraining tasks. However, we have found experimentally that existing intersentence methods for general-purpose language pretraining improve performance only at a relatively small scale but not at larger scales. For an alternative, we propose Text Structure Prediction (TSP), a more sophisticated inter-sentence task that uses text structure to provide more abundant self-supervised learning signals to pretraining models at larger scales. TSP classifies sentence pairs over six designed text structure relationships and it can be seen as an implicit form of learning high-level language understanding by identifying key concepts and relationships in texts. Experiments show that TSP provides improved performance on language understanding tasks for models at various scales. Our approach thus serves as an initial attempt to demonstrate that the exploitation of text structure can facilitate language understanding.\n\n1\n\nINTRODUCTION\n\nGeneral-purpose pretrained language models have been widely applied in natural language processing (NLP). The most representative model of these models is BERT (Devlin et al., 2019), which is pretrained simultaneously on two pretraining tasks: a masked language model (MLM) task, and a next sentence prediction (NSP) task. While MLM masks words and requires models to fill clozes, NSP is an inter-sentence task of predicting whether two texts are continuous. Inter-sentence tasks learn relationships between sentences and facilitate high-level language understanding that is not directly learned by word-level pretraining tasks (Devlin et al., 2019). However, the representative inter-sentence task, NSP, has been found to fail to improve performance (Liu et al., 2019; Yang et al., 2019). Although a few successors of NSP have been proposed, they are still not widely adopted and researched. In this paper, we show that the existing inter-sentence methods for general-purpose language pretraining are actually suboptimal; to improve on those methods’ weaknesses, we then propose an alternative that redefines what is learned from sentence relations.\n\nThe existing general-purpose inter-sentence pretraining tasks include NSP (Devlin et al., 2019), which discriminates whether two texts come from different documents; sentence order prediction (SOP) (Lan et al., 2020), which discriminates whether two texts are swapped; and sentence structure objective (SSO) (Wang et al., 2020), which discriminates whether two texts come from different documents or are swapped. To investigate claims of improved performance, we experimented these methods at three different scales (Small, Base, and Large), which mainly followed the setting of BERT (Devlin et al., 2019). The model size and the amount of consumed data increased from Small to Base and then to Large scale (see Appendix A for the details). As seen in Figure 1, our experimental results show that the existing methods improved performance only at the Small-scale whereas they undermined or failed to improve performance for models at larger scales.\n\nIn investigating the reason for little improvement by the existing methods at larger scales, we noticed that all of the aforementioned methods split an input text into two segments and learn from only the relationship between the two segments, thus ignoring the text’s underlying structure. As illustrated in Figure 2, text structure can be seen as the organization of information in texts. Without text structure, a text becomes a long continuous word sequence, which is hard to read and makes it difficult to identify key concepts and logical relationships for humans. This intuitive understand-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Experimental results on the language understanding benchmark SuperGLUE (Wang et al., 2019) at the Small, Base, and Large scales. The experiment compared the effect on the performance of using different inter-sentence tasks (NSP, SOP, SSO, and our proposed task, TSP) when learned concurrently with the word-based MLM pretraining task. By taking advantage of text structure information, TSP outperforms pure MLM at different scales, whereas the other inter-sentence baselines (NSP, SOP, and SSO) failed to improve the performance at larger scales in our experiments.\n\nFigure 2: Example of text structure in real text. The document comprises multiple paragraphs, where each paragraph is composed of multiple sentences. Each paragraph conveys a specific concept constructed by the sentences that the paragraph is composed of, while the order of concepts comes with a clear intention and forms a logical flow. This suggests that the combination of hierarchy and order in text structure hides valuable clues to high-level language comprehension.\n\ning of human reading ability inspires us to explore the possibility that text structure can provide models’ language understanding ability abundant learning signals of high-level semantics and their interaction, especially for models at larger scales. Hence, our goal in this paper is to demonstrate that learning from text structure can improve general-purpose language model pretraining. We thus propose a new inter-sentence task that better exploits text structure to examine our hypothesis.\n\nThe proposed task, Text Structure Prediction (TSP), is our initial attempt to integrate text structure information into general-purpose language pretraining. Regarding the task design, we view text structure in terms of two axes: ordering and hierarchy. Hierarchies are nested groupings of sentences, such as paragraphs or sections, where each group conveys a specific concept and its su-\n\n2\n\nSuperGLUE5861646770737679SmallBaseLargeMLMMLM + NSPMLM + SOPMLM + SSOMML + TSPThe game resolves around Qubits, the basic building block of a quantumcomputer. It's pretty straightforward (you won't need to learn any quantumentanglement math or physics) with the goal of increasing the number ofQubits while keeping them cool. The more Qubits you have, the more difficultit gets. Eventually, you'll \"discover new upgrades, complete big researchprojects and hopefully become a little more curious about how we're buildingquantum computers,\" wrote Google Quantum head of education Abe Asfaw.The goal is to draw attention to quantum computing, because it seems there'sa dearth of people working in the field. To that end, Google is bringing thegame to the classroom, hoping to encourage educators to talk about thesubject and expand access to quantum computing research. \"We need more students pursuing careers building or using quantumcomputers, and understanding what it would be like to be a quantum scientistor engineer,\" wrote Asfaw. \"For me, that’s what World Quantum Day is allabout: showing everyone what quantum computing really is and how they canget involved.\"Paragraph I (3 sentences): Brief introduction of The Qubit Game Paragraph II (4 sentences) The details of the game. Paragraph III (2 sentences) Google's intention behind the game.Paragraph IV (2 sentences)The meaning of World Quantum Day.World Quantum Day was apparently yesterday, and Google feted the occasionwith the launch of The Qubit Game, as spotted by 9to5Google. Created inpartnership with Doublespeak games, it's a “playful journey to building aquantum computer, one qubit at a time,\" Google said. It also hopes the game,and World Quantum Day, will help generate some interest in the field.Further describe ConcludeExplain the reason Under review as a conference paper at ICLR 2023\n\nperordinate group conveys a more global concept. The order of sentences represents the flow of context and connections between information such as chronological process or cause-effect relationship (Danes, 2015). Thus, the solution of the TSP task requires two abilities: understanding of the semantics of sentences to group them into nested concepts (reflected by hierarchical relationships), and identification of connective relations between them (reflected by ordering relationships). Specifically, we propose six inter-sentence relations for learning from text structure information, which combine three hierarchical levels and two ordering directions. As a result, the TSP task requires a model to classify sentence pairs over the six defined text structure relationships. More details of TSP are given in Section 3. Through experiments we have shown the following: (1) Our proposed method continues to improve performance on a language understanding benchmark at the Small, Base, and Large scales, whereas the inter-sentence baselines fail to do so. (2) Our proposed method is comparable to or better than the baselines on almost all language understanding tasks in the benchmarks. Accordingly, the results have shown the effectiveness and potential of exploiting text structure information for general-purpose language pretraining.\n\nIn summary, the contributions of this paper are as follows\n\n1. We find that existing inter-sentence tasks for general-purpose language pretraining did not bring significant improvement in our settings. In our experiments, these approaches perform well at a relatively small scale, but fail to improve or even degrade performance at larger scales.\n\n2. We propose the use of text structure information to provide more valuable learning signals for general-purpose language pretraining. For our initial attempt at this, we designed the Text Structure Prediction (TSP) task and show that TSP improves performance at all scales in our experiments. We thus demonstrate that the exploitation of text structure information is a promising research direction for facilitating general-purpose language understanding.\n\n2 RELATED WORK\n\n2.1 WORD-LEVEL PRETRAINING\n\nSelf-supervised word representation learning has rapidly developed in the past several years. GPT (Radford et al., 2018) predicts the next token with the information from the previous tokens. ELMo (Peters et al., 2018) uses a bidirectional framework to produce contextualized word representations by fusing the final hidden states of two LSTM networks processing information in different directions. BERT (Devlin et al., 2019) achieves deep bidirectional pretraining via the MLM task, which masks certain words in the input and then predicts the masked words. Since BERT’s success, transfer learning has become mainstream in NLP. Many research initialize their models with pretrained models to reuse learned representations for their target tasks. Also, the development of word-level pretraining tasks has become prosperous. For example, ELECTRA (Clark et al., 2020) detects replaced tokens that come from incorrect predictions by another MLM model, while PMI-Masking (Levine et al., 2021) samples and masks collocations instead of words. In addition to encoder-based BERT and decoder-based GPT, word-level pretraining methods that are based on encoder-decoder such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) are also developed, which gain great success on text generation tasks. Our proposed approach is learned simultaneously with a word-level pretraining task, and it can be easily combined with any word-level pretraining task that is based on transformer encoder, with negligible additional computational and memory costs.\n\n2.2 SENTENCE-LEVEL PRETRAINING\n\nApproaches for learning sentence representations mainly fall into three categories. The first type is sentence bottleneck, including Skip-Thought (Kiros et al., 2015) and CMLM (Yang et al., 2021), aims to generate single or multiple sentence representations that are useful for word prediction objectives. The sentence bottleneck methods compress the encoded sentence into bottleneck representations and thus benefiting from information bottleneck (Tishby et al., 1999). The second type is contrastive learning, including COCO-LM (Meng et al., 2021) and CLEAR (Wu et al., 2020), which makes representations of positive sentence pairs closer and vice versa. Contrastive learning methods improve uniformity and ease the anisotropy problem that limits the expressiveness of embeddings (Gao et al., 2021). The third type is inter-sentence learning, including NSP and SOP\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ndescribed in Section 1, which learns sentence representations by capturing relationships between sentences. Since inter-sentence methods learn from interactions of sentences, they inherently also encode relationships between sentences and facilitate contextualization of multiple sentences. There is currently no research comparing the three kinds of methods with each other, and each type benefits from different principles. However, the inter-sentence approach is the only one that explicitly encodes and contextualizes multiple sentences. Our proposed approach belongs to the inter-sentence category.\n\n2.3\n\nINTER-SENTENCE TASKS\n\nIn terms of the kinds of sentence relationships to be learned, most inter-sentence tasks are sentence ordering tasks that rearrange a set of sentences into their original order (Cui et al., 2020; Kumar et al., 2020), whereas our proposed sentence relationships (Section 3.2) include ordering relationships but are not restricted to them. Regarding the creation of sentence representations, instead of encoding each sentence individually (Prabhumoye et al., 2020), our proposed framework (Section 3.1) adopts an approach of concatenating sentences into an input sequence, whereby information flows across sentences; thus, the created sentence representations are contextualized. In terms of semantic units, approaches such as NSP and SOP capture relationships between segments that are two halves of a text sequence that may include many sentences. In contrast, our approach processes sentences as in many other approaches. Finally, for task objectives, some approaches aim to identify coherent text (Barzilay & Lapata, 2008; Mesgar & Strube, 2018) or learning discourse representations (Iter et al., 2020), whereas our baselines (NSP, SOP, SSO) and our proposed method are for creating general-purpose language models.\n\n3 PROPOSED METHOD\n\nWe explain our proposed method in two parts. Section 3.1 introduces the overall proposed framework for creating sentence embeddings and then performing six-class classification over sentence pairs using created sentence embeddings. Figure 3 shows an overview of this framework. Section 3.2 further elaborates on the six inter-sentence relationships that we designed for classification and learning over sentence pairs. Overall, the implementation of TSP comes with several advantages: (1) It is a self-supervised task that does not require external resources or human labeling. (2) It can easily be combined with any pretraining task that outputs contextual word representations. An example is provided in Appendix F. (3) It can be learned concurrently with other tasks in one forward pass. (4) It only requires constructing an output head on top of a text encoder, with negligible extra parameters and computations.\n\n3.1 PROPOSED FRAMEWORK\n\n3.1.1\n\nINPUT SEQUENCE\n\nGiven a text sequence, we mark the scope of each sentence in the sequence by using the NLTK toolkit (Bird et al., 2009), which is based on Punkt (Kiss & Strunk, 2006). The result is an sequence that can be described as x =< x1,1, x1,2, ..., x1,n1 , ..., xk,1, xk,2, ..., xk,nk >, where xi,t is the t-th token in sentence i, ni is the length of sentence i, and k is the number of sentences in x. Next, the sequence’s sentences are shuffled to enable the model to learn to identify their correct order. We have found that shuffling all of a sequence’s sentences introduces noise into the model; thus, we shuffle only a limited percentage of the sentences (15% in this work, as explained in Appendix C), while keeping the other sentences at their original positions. Additionally, to learn simultaneously from the MLM task, we follow the same procedure used in BERT to corrupt the sequence: 15% tokens are selected; then, 80% of them are replaced with a [MASK] token, 10% are replaced with random tokens, and 10% are unchanged. Finally, we add sentinel tokens [CLS] and [SEP] to the sequence as in BERT, and a masked and partially shuffled input sequence is thus obtained for the model.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Overview of the proposed Text Structure Prediction (TSP) task. (left) Before an input sequence is sent to the encoder, certain randomly selected sentences in the sequence are shuffled. After encoding, a sentence embedding is obtained by averaging the encoded states of the tokens belonging to that sentence. (right) The TSP loss is calculated for performing six-class classification over sentence pairs by their sentence embeddings, where the six classes are shown in Figure 4.\n\n3.1.2 SENTENCE EMBEDDING\n\nTo obtain sentence representations, we first calculate contextualized token hidden states:\n\nh = fθ(x′),\n\n(1)\n\nwhere fθ is the transformer encoder function and x′ is a corrupted and partially shuffled input token sequence. By averaging the hidden state vectors of tokens that belong to the sentence, we obtain its sentence embedding:\n\nsi =\n\nΣn\n\nt=1hi,t ni\n\n,\n\n(2)\n\nwhere hi,t is the hidden state vector of the t-th token in the i-th sentence and ni is the length of the i-th sentence. Additionally, through the transformer encoder’s attention mechanism, each token interacts with all tokens in the input sequence rather than only the tokens in the sentence it belongs to. As a result, the sentence embeddings are contextualized and aware of other sentences in the same sequence. This means that when we compare sentences for their structure relationships, not only the sentences themselves but also their context are considered. Since identifying structure relationships using only information from the compared sentences can be very hard, it is intuitive that models are motivated to encode high-level interactions between sentences in the context into sentence representations.\n\n3.1.3 PAIRWISE CLASSIFICATION\n\nNext, we perform six-class classification over sentence pairs to learn the text structure relationship between sentences. Given a pair of sentences, we pass their sentence embeddings to a classifier to obtain prediction logits zi,j = W2(GELU (W1(si ⊕ sj))). ⊕ denotes vector concatenation, sk is the embedding of the k-th sentence, and GELU is the activation function proposed by Hendrycks & Gimpel (2016). Finally, the loss in the Text Structure Prediction (TSP) task is the cross-entropy loss:\n\nLtsp[i, j] = −log\n\nexp(zi,j,yi,j ) c=1exp(zi,j,c)\n\nΣ6\n\n,\n\n(3)\n\nwhere yi,j is the correct label for the relationship between the i-th and j-th sentences, and zi,j,c is the c-th element of the logit vector for the sentence pair. The final TSP loss Ltsp is the average over the TSP losses for all sentence pairs in a mini-batch, where the sentences in each sentence pair are in the same sequence. Overall, a model is optimized by minimizing the sum of MLM loss Lmlm and our TSP loss Ltsp:\n\nL = Ltsp + Lmlm.\n\n(4)\n\n5\n\nTransformer EncoderSentence 1Sentence 3Sentence 2Input SequenceSentence 1Sentence 2Sentence 3Average S3Partially Shuffled Input Sequence Token Hidden States Sentence Embeddings S1S2S36-Class ClassificationTSP LossAverage S2Average S1Under review as a conference paper at ICLR 2023\n\nFigure 4: Inter-sentence relationship sets for (left) our baselines and (right) TSP. An inter-sentence task classifies pairs of texts into one of the relations in its own defined sets. For example, if two sentences are in the same paragraph (but not adjacent) and in reverse order, the TSP task should classify the pair as Same Paragraph & Reverse Order. Note that we leave Different Document for future work because of its lack of difficulty (Lan et al., 2020) and complexity of implementation. By comparing the defined relations, we can see that TSP takes better advantage of the text structure and is a more complex task that is expected to provide more valuable knowledge.\n\n3.2 PROPOSED TEXT STRUCTURE RELATIONSHIPS\n\nThe key part of the training objective is our design of six classes to represent six text structure relationships between sentences. As illustrated on the right in Figure 4, these relationships are the set of combination of three hierarchical relationships {neighbor, same paragraph, same document} and two ordering relationships {normal order, reverse order}. Concretely, these relationships are defined as follows. The hierarchical relationships include (1) neighbor (two sentences are adjacent), (2) same paragraph (two sentences are in the same paragraph but not adjacent), and (3) same document (two sentences are in the same document but not adjacent or in the same paragraph). The ordering relationships include (1) normal order (two sentences are in the forward order), and (2) reversed order (two sentences are in the reversed order).\n\n3.3 LEARNING EXPECTATION FROM HIERARCHY AND ORDER\n\nThe text hierarchy describes the semantics hierarchy and the interactions between the semantics. As shown in Figure 2, the semantics of sentences are included in higher-level paragraph semantics, and paragraph semantics are included in higher-level document semantics. Relationships between semantics at different levels and the same level can also be seen in the example. To group sentences nestedly, models are expected to summarize semantics at different levels from the context, while capturing relationships and interactions between these semantics. Intuitively, it can provide unique learning signals for models’ understanding ability on and between texts at different levels of granularity.\n\nThe order of sentences depends on the logical relations that connect the different concepts described in the sentences. These relations can be chronological relations, cause-effect relations, and so on. Textual coherence is also an important clue for sentence order. A model must recognize relations between concepts and identify coherence to rearrange the order of sentences. Moreover, learning from sentence order has long been shown to be useful for many downstream applications (Cui et al., 2020).\n\n6\n\nSame Document Reverse OrderSame DocumentNormal OrderSame Paragraph Reverse OrderSame Paragraph Normal OrderNeighbor Reverse OrderNeighbor Normal OrderNeighbor Reverse OrderNeighbor Normal OrderDifferent DocumentNeighborOrderHierarchyText Structure Prediction (TSP)Sentence Order Prediction (SOP)Next Sentence Prediction (NSP)Different DocumentNeighbor Reverse OrderNeighbor Normal OrderSentence Structure Objective (SSO)Under review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\n4.1 SETUP\n\n4.1.1 PRETRAINING\n\nAll models in this paper were pretrained on OpenWebText (Gokaslan & Cohen, 2019), an opensource recreation of the WebText corpus described in GPT2 (Radford et al., 2019). It includes 38GB of texts from 8,013,769 documents extracted from Reddit posts. Note that we did not use Wikipedia+BookCorpus because of the public unavailability of official BookCorpus (Zhu et al., 2015). In preprocessing the pretraining data, we delimited paragraphs by blank lines and marked the scopes of sentences with the sentence splitter of NLTK (Bird et al., 2009) for the use of TSP.\n\nFor this paper, we experimented with pretraining models on word-based MLM with different intersentence tasks or on MLM only. In particular, we mainly followed BERT’s pretraining setting and referred to the setting of ELECTRA (Clark et al., 2020), another popular pre-trained model. We trained with a sequence length of 512 and 1 million training steps at the Base and Large scales, or with a length of 128 and 1.45 million steps at the Small scale. The batch sizes vary with the scale, as described along with other details in Appendix A. To ensure fair comparisons, all models at the same scale had almost the same computational cost and number of parameters in both training and inference. Note that we did not perform any hyperparameter search, although we think it could help our proposed method to perform better.\n\n4.1.2 EVALUATION\n\nOur pretrained models were evaluated on the widely adopted SuperGLUE benchmark (Wang et al., 2019) for evaluating general language understanding systems. SuperGLUE contains tasks covering natural language inference tasks RTE (Dagan et al., 2006) and CB, multiple-choice reasoning task COPA (Melissa et al., 2011), question-answering tasks BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), and word sense disambiguation task WiC (Pilehvar & Camacho-Collados, 2019). The metrics used here for these tasks are the same as those the ones described in the SuperGLUE paper.\n\nBecause some of the evaluation datasets are small and the scores on them could vary substantially depending on the random seed, we followed the ELECTRA setting and finetuned 10 runs for each task from the same pretrained checkpoint for every model. Here, we took the median score of 10 runs for our development set results (Appendix D), and we used the best-performing model on the development set for evaluation on the test set. In contrast to certain related papers, we did not apply tricks such as intermediate finetuning or ensembling during the stage of finetuning. Similarly, we excluded the WSC task because it involves intermediate finetuning and publicly unavailable training data to obtain decent results (Kocijan et al., 2019). More details about our finetuning are given in Appendix B.\n\n4.2 RESULTS\n\nTable 1 lists the averaged SuperGLUE scores. TSP improved on pure MLM and outperformed the inter-sentence baselines at all scales. These results show the effectiveness and potential of incorporating text structure information into learning. The results also reveal two findings. First, NSP undermined or made little difference in the performance, which matches the claims of Liu et al. (2019); Yang et al. (2019). Second, while SOP has been claimed to improve NSP on the GLUE benchmark (Wang et al., 2018; Lan et al., 2020), we found that it failed to outperform NSP in our experiment.\n\nNext, we analyze the scaling of the four inter-sentence tasks. Although the inter-sentence baselines (NSP, SOP, and SSO) improved on pure MLM at the Small scale, at the larger scales they failed to improve on pure MLM by a significant margin or even undermined the performance. We suspect the reason to be the task difficulty. While the Small-scale models learned from the inter-sentence baseline tasks, those tasks may not have been difficult enough to provide valuable learning signals to models at larger scales. We can also see that SSO (the fusion of NSP and SOP) is slightly more complicated than NSP and SOP, and it slightly outperformed both NSP and SOP at Base\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Experiment results on the SuperGLUE test set at different scales. Overall, TSP outperformed MLM at the different scales, whereas the other inter-sentence baselines (NSP, SOP, and SSO) failed to achieve improvement. Note that because the sizes and settings for the naming of scales (e.g., Small, Base, and Large) differ greatly in different papers, results may not be directly comparable between papers.\n\nScale Model\n\nSmall\n\nBase\n\nLarge\n\nMLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours)\n\nAvg.\n\n58.8 61.1 59.9 60.8 61.5 71.0 70.8 71.0 71.6 73.6 76.5 76.5 76.4 76.6 79.0\n\nCB\n\nCOPA\n\nMultiRC\n\nRTE\n\nWIC\n\nBoolQ\n\nReCoRD\n\nAvg. F1/Acc.\n\n75.7/81.6 76.2/82.0 77.7/80.4 74.4/81.6 83.0/86.0 84.8/90.4 84.5/90.8 83.5/90.4 84.8/89.6 84.2/91.2 87.4/92.4 83.6/89.6 84.8/90.4 81.8/88.4 93.5/94.8\n\nAcc.\n\n57.6 58.4 59.0 57.2 59.4 63.2 66.0 70.0 71.2 72.4 73.6 76.0 74.6 74.6 77.4\n\nF1a/EM\n\n60.3/13.6 61.1/14.7 61.7/14.5 61.1/14.3 62.4/14.3 73.2/28.6 73.8/30.6 71.7/25.4 73.0/28.5 75.8/33.5 77.8/37.5 78.2/38.4 79.3/40.9 79.6/40.9 80.5/43.2\n\nAcc.\n\n58.3 65.5 63.0 66.1 66.1 71.3 73.2 70.8 72.3 74.1 76.6 77.4 77.1 78.5 79.1\n\nAcc.\n\n60.9 65.5 60.4 66.1 60.2 66.1 68.8 68.1 68.0 66.6 68.7 71.0 68.6 70.6 72.4\n\nAcc.\n\n70.4 71.5 71.7 71.8 71.8 79.4 70.9 78.3 76.9 80.6 82.7 81.7 82.2 82.1 82.7\n\nF1/Acc.\n\n49.4/48.6 50.1/49.3 48.3/47.4 49.3/48.6 50.5/49.8 79.1/78.2 77.9/76.9 74.9/74.0 75.0/74.0 79.2/78.5 86.5/85.7 85.1/84.4 85.2/84.5 85.1/84.3 85.8/85.2\n\nand Large scales. On the other hand, with the better use of text structure information, TSP was designed as more complicated (six inter-sentence relations instead of two or three inter-sentence relations for the inter-sentence baseline tasks, as shown in Figure 4) and more difficult. TSP provides valuable knowledge for models at all scales, especially for Large-scale models, which have the largest capability and consumed data to better exploit the difficult TSP task, thus yielding the largest improvement over other baselines as compared to the other scales. Encouraged by this observation, we expect that models with even larger model sizes and training data would also benefit significantly from learning text structure information.\n\nWhile TSP provided competitive or better performance on most of the tasks, we also found that it performed well on the reasoning (CB, RTE, and COPA) and question-answering (MultiRC and BoolQ) tasks. We can view a question and an answer as two texts that share a similar concept and can be placed in the same hierarchical group, such as a paragraph or a pair of consecutive sentences. Likewise, we can view reasoning in terms of whether two texts are coherent. From these viewpoints, the results are consistent with our interpretation that TSP facilitates language understanding by strengthening recognition and comparison of concepts and reasoning relationships (Section 3.2). On the other hand, although ReCoRD is a QA task, it is more like prediction of a masked entity in a paragraph given the context, which may explain why pure MLM was the leading model for ReCoRD at the Large scale. Regarding WiC, a word sense disambiguation task, TSP showed an interesting phenomenon. Compared to other baselines in the WiC task, TSP performed poorly at the Small scale, got better at the Base scale, and became the best-performing model at the Large scale. We can relate this result to our surmise on the scaling effect mentioned above. Learning of the complicated TSP stretches models’ capabilities, and small models may not have enough capability to encode information that is useful for word-level tasks like WiC, whereas large models have enough capability that complicated sentence-level learning can even help with word-level understanding. While more elaborate approaches will be needed to analyze the effect of learning text structure information on different types of downstream tasks, which is outside the scope of this paper, we hope to have a good starting point by providing interesting results and explanations for those who follow.\n\nIn addition to the main results, we also reported other experiment results. First, we reported scores on the SuperGLUE development set in Appendix D, which showed similar results as in Table 1. Second, we reported results on the GLUE benchmark in Appendix E. Finally, to demonstrate that our proposed approach is generally useful, we did an experiment showing that TSP can be easily combined with another word-based pretraining task (see Appendix F for details). Additionally, this\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Ablation results for the designed text structure relations.\n\nModel\n\n#total classes\n\nMLM+TSP MLM+TSP - paragraph MLM+TSP - hierarchy MLM+TSP - order MLM\n\n6 4\n2 3\n-\n\n#ordering relationship 2\n2 2\n- -\n\n#hierarchical relationship 3\n2 -\n3 -\n\nSuperGLUE\n\n73.9 73.2 72.5 71.9 71.2\n\nexperiment also shows that TSP still improves performance in its preliminary results, which suggests that learning text structure information may generalize to pretraining tasks other than MLM.\n\n5 ABLATION STUDY\n\nTo investigate whether the proposed TSP text structure relations are helpful, we experimented with different TSP variations in which some text structure relations were ablated. Specifically, MLM+TSP -hierarchy adopted only the 2 ordering relationships (normal, reversed), MLM+TSP - order adopted only the 3 hierarchical relationships (neighbor, paragraph, document), and MLM+TSP -paragraph combined the 2 ordering relationships with only the 2 hierarchical relationships (neighbor, document) , with no discrimination of whether two sentences were in the same paragraph.\n\nThe results on the SuperGLUE development set of models at the Base scale are shown in Table 2. First, we found that the models pretrained with TSP benefitted from both the hierarchy relationships and the ordering relationships, as both MLM+TSP - hierarchy and MLM+TSP - order underperformed MLM+TSP and outperformed MLM. Second, the performance slightly dropped when it was not discriminated whether two sentences were in the same paragraph, which shows that the adjacency of sentences is not the only important factor, and that discrimination of hierarchical groups at different levels matters. Although other design elements such as discrimination of sections and documents remain unexplored, we will encourage future work to explore further, whereas in this paper we focus on uncovering the potential for integrating text structure information.\n\n6 CONCLUSION\n\nDespite their stated usefulness for high-level language understanding and continued development, inter-sentence tasks have not gained enough attention as expected. Our experiments have shown that existing inter-sentence approaches for general-purpose language pretraining can not improve performance or even degrade performance at scales larger than Small. Furthermore, these existing approaches ignore a large part of text structure information and oversimplify complicated text structures into one relationship between two lengthy parts of a text sequence. The goal of this paper was thus to explore the potential of utilizing text structure information to provide more valuable learning signals and maintain the improvement at larger scales. For an initial attempt to achieve this goal, we proposed Text Structure Prediction (TSP), a self-supervised inter-sentence pretraining task that can easily be combined with any encoder-based pretraining task. TSP redefines what is learned from sentence pairs via text structure relationships. Specifically, these text structure relationships combine hierarchical and ordering relationships, which can be seen as implicit learning signals for language understanding via recognition of concepts and reasoning relationships respectively. In this paper, we have shown that (1) inter-sentence baselines (NSP, SOP, SSO) were not helpful for performance at larger scales in our settings, whereas TSP improved the overall performance of models at all scales in our experiments, which again demonstrates that inter-sentence tasks for learning from inter-sentence relationships can help language understanding. Moreover, (2) the good scaling and significant improvement at the Large scale showed that the exploitation of text structure information can result in a sophisticated task that provides valuable knowledge for models with a large capability and data size. Finally, although there may still be much room for different ways to exploit text structure information, we have shown the effectiveness and potential of exploiting such information, and we expect that this work will encourage more research on learning from text structure information.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRegina Barzilay and Mirella Lapata. Modeling local coherence: An entity-based approach. Com-\n\nputational Linguistics, 34(1):1–34, 2008.\n\nSteven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing\n\ntext with the natural language toolkit. ” O’Reilly Media, Inc.”, 2009.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936. Association for Computational Linguistics, June 2019.\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2020.\n\nBaiyun Cui, Yingming Li, and Zhongfei Zhang. BERT-enhanced relational sentence ordering network. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6310–6320. Association for Computational Linguistics, November 2020.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Joaquin Qui ̃nonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch ́e Buc (eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pp. 177–190. Springer Berlin Heidelberg, 2006.\n\nFrantisek Danes. FUNCTIONAL SENTENCE PERSPECTIVE AND THE ORGANIZATION OF THE\n\nTEXT. De Gruyter Mouton, 2015.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186. Association for Computational Linguistics, June 2019.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6894–6910. Association for Computational Linguistics, November 2021.\n\nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/\n\nOpenWebTextCorpus, 2019.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.\n\nDan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky. Pretraining with contrastive sentence objectives improves discourse performance of language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4859–4870. Association for Computational Linguistics, July 2020.\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262. Association for Computational Linguistics, June 2018.\n\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.\n\nTibor Kiss and Jan Strunk. Unsupervised multilingual sentence boundary detection. Computational\n\nLinguistics, 32(4):485–525, 2006.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz.\n\nA surprisingly robust trick for winograd schema challenge. CoRR, abs/1905.06290, 2019.\n\nPawan Kumar, Dhanajit Brahma, Harish Karnick, and Piyush Rai. Deep attentive ranking networks for learning to order sentences. Proceedings of the AAAI Conference on Artificial Intelligence, 34 (05):8115–8122, April 2020.\n\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.\n\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin Leyton-Brown, Moshe Tennenholtz, and Yoav Shoham. {PMI}-masking: Principled masking of correlated spans. In International Conference on Learning Representations, 2021.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880. Association for Computational Linguistics, July 2020.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.\n\nRoemmele Melissa, Bejan Cosmin, Adrian, and Gordon Andrew, S. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning, pp. 21–23, Stanford University, March 2011.\n\nYu Meng, Chenyan Xiong, Payal Bajaj, saurabh tiwary, Paul Bennett, Jiawei Han, and XIA SONG. Coco-lm: Correcting and contrasting text sequences for language model pretraining. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 23102–23114. Curran Associates, Inc., 2021.\n\nMohsen Mesgar and Michael Strube. A neural local coherence model for text quality assessment. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4328–4339. Association for Computational Linguistics, October-November 2018.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227–2237. Association for Computational Linguistics, June 2018.\n\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267–1273. Association for Computational Linguistics, June 2019.\n\nShrimai Prabhumoye, Ruslan Salakhutdinov, and Alan W Black. Topological sort for sentence ordering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2783–2792. Association for Computational Linguistics, July 2020.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-\n\nstanding by generative pre-training. 2018.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nNaftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp. 368–377, 1999.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355. Association for Computational Linguistics, November 2018.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and Luo Si. Structbert: Incorporating language structures into pre-training for deep language understanding. In International Conference on Learning Representations, 2020.\n\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive\n\nlearning for sentence representation, 2020.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nZiyi Yang, Yinfei Yang, Daniel Cer, Jax Law, and Eric Darve. Universal sentence representation learning with conditional masked language model. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6216–6228. Association for Computational Linguistics, November 2021.\n\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension, 2018.\n\nY. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision (ICCV), pp. 19–27. IEEE Computer Society, dec 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PRETRAINING DETAILS\n\nThe details given in Table 3 apply to all the models in this paper. In most cases, we used the same hyperparameters as for BERT and ELECTRA. At the Large scale, we adopted a setting closer to BERT-Large because ELECTRA’s setting for the Large scale requires much higher memory and computational cost than our computational resource could handle. For the Large scale, we also slightly increased the batch size to 288, which we found that avoids divergence in our experiments. Additionally, inspired by BERT, we trained with a sequence length of 128 for the first half of the training steps and then with a sequence length of 512 for the rest while keeping the same number of tokens in a batch to speed up pretraining in our Large-scale experiments. Note that we did not perform any hyperparameter search, although we think it could help our proposed method to perform better.\n\nTable 3: Pretraining hyperparameters.\n\nHyperparameter Number of layers Hidden size FFN inner hidden size Attention head size Attention heads Embedding size Mask percent Learning rate decay Warmup steps Learning rate Adam ε Adam β1 Adam β2 Dropout Weight decay Batch size Training steps (MLM/ELECTRA)\n\nLarge 24 1024 4096 16 64 1024 15\n\nSmall 12 256 1024 4\n64 128 15 Linear 10000 5e-4 1e-6 0.9 0.999 0.1 0.01 128 1.45M/1M 1M/-\n\nBase 12 768 3072 12 64 768 15 Linear Linear 10000 10000 1e-4 2e-4 1e-6 1e-6 0.9 0.9 0.999 0.999 0.1 0.1 0.01 0.01 288 256 1M/-\n\nB FINETUNING DETAILS\n\nOur finetuning setting (see Table 4 for details) mostly followed that of ELECTRA. Most tasks were finetuned for 3 epochs. However, inspired by ELECTRA, which increases the number of epochs to 10 for the RTE task, we also found that increasing the number of epochs to 10 helped stabilize the finetuning performance for CB and BoolQ. Except for this, we did not perform any hyperparameter search for finetuning.\n\nTable 4: Finetuning hyperparameters.\n\nHyperparameter Learning rate Adam epsilon Adam beta 1 Adam beta 2 Layerwise LR decay Learning rate decay Warmup fraction Dropout Weight decay Batch size Training epochs\n\nValue 3e-4 for Small, 1e-4 for Base, 5e-5 for Large 1e-6 0.9 0.999 0.8 for Base/Small, 0.9 for Large Linear 0.1 0.1 0.1 32 10 for RTE, CB, BoolQ; 3 for other tasks.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFor the WiC and ReCoRD tasks, in which span embeddings for words or entities are required to compare spans with each other, we created span embeddings by applying average pooling to the final hidden representations of the tokens constructing a span and we compared spans by passing the concatenated span embeddings to the classifiers.\n\nC SHUFFLING RATE\n\nTable 5: Comparison of different sentence shuffling rates in terms of the SuperGLUE development set results for Small-scale MLM-based models. We choose 15% as our final choice for shuffling rate. The results show that TSP is not sensitive to the extent of shuffling, but shuffling is necessary for TSP.\n\nShuffling rate 0% 15% 30% 50% 100%\n\nSuperGLUE 59.7 60.3 60.3 60.2 60.1\n\nAmong the sentences in a given input sequence, we randomly shuffled 15% of them while other sentences stay at their original positions, to prevent complete leakage of the original ordering and hierarchical information. We also experimented with different rates for such kind of partial shuffling, and the results are listed in Table 5. The case of no shuffling and complete shuffling gave the worst results, while there was little difference in performance for shuffling of 15 ∼ 50%. Accordingly, the results indicate that shuffling is needed, but the proposed task is not sensitive to the partial shuffling rate. We chose 15% as the partial shuffling rate in this paper because it was the lowest shuffling rate in this analysis that gave a good performance with minimal corruption of inputs.\n\nD RESULTS ON THE SUPERGLUE DEVELOPMENT SET\n\nFor reference, Table 6 lists our experimental results on the SuperGLUE development set.\n\nE RESULTS ON THE GLUE BENCHMARK\n\nThe GLUE benchmark is the predecessor of the SuperGLUE benchmark. Compared to the old GLUE benchmark, SuperGLUE comes with a new set of more difficult language understanding tasks and improved resources, which encourages us to evaluate models on SuperGLUE as our main results. At the same time, we also provide GLUE results for readers’ reference. As shown in Table 8, the proposed task, which learns from both hierarchical and ordering relationships, also outperform other baselines on the GLUE benchmark at Small, Base, and Large scales.\n\nF COMBINATION WITH OTHER WORD-LEVEL PRETRAINING METHOD\n\nBecause the proposed method can easily be added on top of other word-level pretraining methods, we conduct an experiment to evaluate whether the proposed method improves performance when it is not combined with MLM. ELECTRA (Clark et al., 2020), another word-level pretraining method, was adopted as the alternative to MLM in this experiment. We chose ELECTRA because it is a discriminative pretraining method that performs binary classification to detect replaced words among all words, making it largely different from MLM, which is generative and predicts masked words. Although ELECTRA comprises a generator to generate inputs for discrimination and a discriminator to detect replaced tokens in generated inputs, the generator is discarded after pretraining, and only the discriminator is used for finetuning. This is why, when combining TSP with ELECTRA, we applied TSP only to the discriminator and the generation stage remained unchanged.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Results of MLM-based models on the SuperGLUE development set.\n\nScale Model\n\nSmall\n\nBase\n\nLarge\n\nMLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours)\n\nAvg.\n\n58.7 60.4 60.0 60.0 60.7 71.2 71.6 70.2 70.8 73.9 76.4 76.7 77.0 77.3 78.0\n\nCB\n\nCOPA\n\nMultiRC\n\nRTE\n\nWIC\n\nBoolQ\n\nReCoRD\n\nAvg. F1/Acc. 83.9/77.6 70.7/80.4 71.7/78.6 65.0/75.0 72.9/80.4 90.6/92.9 89.9/92.9 83.7/87.5 86.3/91.1 88.1/92.9 93.0/94.6 93.7/94.6 93.2/94.6 93.6/94.1 94.3/96.4\n\nAcc.\n\n55.0 58.0 55.0 56.0 59.0 63.0 63.0 66.0 66.0 72.0 70.0 70.5 71.0 71.5 72.0\n\nF1a/EM\n\n67.1/12.8 67.0/14.5 67.2/14.8 67.3/14.9 66.6/15.2 75.1/27.3 75.8/28.7 75.1/26.8 75.5/26.7 79.0/34.1 80.3/35.8 81.5/41.6 82.0/43.0 82.9/43.1 83.3/45.4\n\nAcc.\n\n59.6 66.4 68.6 66.4 68.6 73.7 73.8 72.6 72.2 77.6 80.7 81.2 80.9 82.0 82.0\n\nAcc.\n\n61.0 65.8 63.5 68.8 62.1 66.9 70.1 68.7 70.3 68.3 70.1 69.7 69.0 70.2 70.5\n\nAcc.\n\n70.7 73.0 73.2 73.2 73.5 80.1 79.9 79.1 78.9 80.6 83.3 82.5 83.3 83.3 83.9\n\nF1/Acc.\n\n39.0/48.2 38.6/48.1 39.1/48.2 39.3/49.1 39.7/49.1 64.9/78.0 64.1/76.9 62.2/74.6 62.4/74.9 65.2/78.2 71.7/85.5 70.7/84.2 71.1/85.0 70.6/84.2 71.4/85.0\n\nTable 7: Results of MLM-based models on the GLUE development set. ”Pearson” stands for Pearson correlation. ”Spearman” stands for Spearman correlation.\n\nScale Model\n\nAvg.\n\nSmall\n\nBase\n\nLarge\n\n77.3 MLM 79.1 MLM+NSP (BERT) 78.4 MLM+SOP MLM+SSO 79.2 MLM+TSP (ours) 79.8 84.6 MLM 84.3 MLM+NSP (BERT) 83.5 MLM+SOP MLM+SSO 83.9 MLM+TSP (ours) 85.5 86.5 MLM 86.2 MLM+NSP (BERT) 86.5 MLM+SOP MLM+SSO 86.5 MLM+TSP (ours) 87.4\n\nCoLA\n\nSST\n\nMRPC\n\nSTS\n\nQQP\n\nMultiNLI\n\nQNLI\n\nRTE\n\nMatthew’s Corr\n\nAcc.\n\nF1/Acc\n\nPearson/Spearman\n\nF1/Acc.\n\nmatched/unmatched\n\nAcc.\n\n47.0 44.4 44.5 45.1 46.8 61.5 60.4 58.1 60.3 62.5 62.5 60.2 61.7 61.5 64.7\n\n90.0 89.8 89.6 89.6 89.7 93.5 93.1 92.0 92.5 93.9 95.0 94.5 94.6 94.5 95.1\n\n84.6/88.5 83.6/83.2 89.4/85.0 88.4/88.0 85.4/89.1 90.0/85.8 85.1/88.8 85.3/84.8 89.2/84.8 85.6/89.2 88.2/87.8 90.1/85.8 85.6/89.3 91.0/87.2 87.1/86.7 87.9/91.0 89.7/89.5 92.2/89.1 87.9/91.0 90.4/90.1 91.5/88.2 87.7/90.9 89.9/89.6 91.7/88.5 91.6/88.2 87.5/90.8 90.7/90.3 92.5/89.7 90.7/90.4 87.8/91.0 89.0/91.7 90.8/90.7 92.0/88.8 88.8/91.7 91.8/91.4 92.2/89.2 88.9/91.7 91.6/91.4 92.6/89.7 91.8/91.6 92.2/88.8 88.8/91.6 93.4/90.9 91.8/91.6 89.1/91.9\n\n79.2/80.3 79.9/81.0 78.8/79.3 80.3/81.2 80.2/81.2 86.6/86.7 85.3/85.7 84.9/85.2 85.0/85.5 86.3/86.6 88.7/89.1 88.0/88.3 88.3/88.6 88.4/88.5 88.3/89.0\n\n85.2 88.3 86.6 88.7 88.9 91.8 91.7 91.0 91.5 92.8 93.3 93.0 93.0 93.0 94.1\n\nAcc.\n\n59.6 66.4 68.6 66.4 68.6 73.7 73.8 72.6 72.2 77.6 80.7 81.2 80.9 82.0 82.0\n\nNote that, as in the previous experiments, TSP added negligible parameters and computation to the ELECTRA pretraining.\n\nThe experimental results are listed in Table 9. When combined with ELECTRA, the proposed method could still improve the SuperGLUE score. Because of limited computational resources and time, we did not perform these experiments beyond the Small-scale, but we expect that the proposed method would perform well in such cases. Overall, this experiment shows the capability of the proposed method to be combined with other word-level pretraining methods, thus suggesting the opportunity to improve such methods via text structure learning in the future.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Results of MLM-based models on the GLUE development set. ”Pearson” stands for Pearson correlation. ”Spearman” stands for Spearman correlation.\n\nScale Model\n\nSmall\n\nBase\n\nLarge\n\nMLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours)\n\nAvg.\n\n77.3 79.1 78.4 79.2 79.8 84.6 84.3 83.5 83.9 85.5 86.5 86.2 86.5 86.5 87.4\n\nCoLA\n\nSST\n\nMRPC\n\nSTS\n\nMatthew’s Corr\n\nAcc.\n\nF1/Acc\n\nPearson/Spearman\n\n47.0 44.4 44.5 45.1 46.8 61.5 60.4 58.1 60.3 62.5 62.5 60.2 61.7 61.5 64.7\n\n90.0 89.8 89.6 89.6 89.7 93.5 93.1 92.0 92.5 93.9 95.0 94.5 94.6 94.5 95.1\n\n89.4/85.0 90.0/85.8 89.2/84.8 90.1/85.8 91.0/87.2 92.2/89.1 91.5/88.2 91.7/88.5 91.6/88.2 92.5/89.7 92.0/88.8 92.2/89.2 92.6/89.7 92.2/88.8 93.4/90.9\n\n83.6/83.2 88.4/88.0 85.3/84.8 88.2/87.8 87.1/86.7 89.7/89.5 90.4/90.1 89.9/89.6 90.7/90.3 90.7/90.4 90.8/90.7 91.8/91.4 91.6/91.4 91.8/91.6 91.8/91.6\n\nQQP\n\nF1/Acc.\n\n84.6/88.5 85.4/89.1 85.1/88.8 85.6/89.2 85.6/89.3 87.9/91.0 87.9/91.0 87.7/90.9 87.5/90.8 87.8/91.0 89.0/91.7 88.8/91.7 88.9/91.7 88.8/91.6 89.1/91.9\n\nMultiNLI\n\nQNLI\n\nRTE\n\nmatched/unmatched\n\nAcc.\n\n79.2/80.3 79.9/81.0 78.8/79.3 80.3/81.2 80.2/81.2 86.6/86.7 85.3/85.7 84.9/85.2 85.0/85.5 86.3/86.6 88.7/89.1 88.0/88.3 88.3/88.6 88.4/88.5 88.3/89.0\n\n85.2 88.3 86.6 88.7 88.9 91.8 91.7 91.0 91.5 92.8 93.3 93.0 93.0 93.0 94.1\n\nAcc.\n\n59.6 66.4 68.6 66.4 68.6 73.7 73.8 72.6 72.2 77.6 80.7 81.2 80.9 82.0 82.0\n\nTable 9: Results of ELECTRA-based models on the SuperGLUE development set. TSP performs well even when combined with another word-level pretraining method that is vastly different from MLM.\n\nScale Model\n\nSmall\n\nELECTRA ELECTRA+NSP ELECTRA+SOP ELECTRA+SSO ELECTRA+TSP\n\nSuperGLUE 57.4 58.6 58.5 60.6 60.7\n\n16",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a new way to pre-train language models. The main idea is to include sentence-level hierarchy information during the pre-training. Instead of considering only neighbor sentences, they consider more relations between sentences such as if they are in the same paragraph or if they are in the same document. By training this classification loss along with the original masked token loss, they have better performance on the SuperGLUE downstream tasks.\n\n# Strength And Weaknesses\n\nStrength\n- The motivation is clear and reasonable.\n- The experiments support the claim and seem to be promising\n\nWeaknesses\n- To test the generalizability, I suggest to test on GLUE tasks as well.\n- It's not clear what size of models is using for the ablation study in Table 2 \n- What will happen if we do not shuffle the sentences?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe proposed method is clearly described. The novelty is enough for me. They propose additional self-supervised loss to improve the language models.\n\n# Summary Of The Review\n\nThe motivation is clear and reasonable. Although the proposed method is simple, it's very effective. Ablation study supports their claim.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents a novel task called **Text Structure Prediction (TSP)**, aimed at enhancing language model pretraining by incorporating text structure information. TSP classifies sentence pairs based on six defined relationships concerning hierarchy and ordering, addressing limitations of existing inter-sentence tasks such as Next Sentence Prediction (NSP) and Sentence Order Prediction (SOP). The methodology includes pretraining on OpenWebText with varying model sizes and concurrent execution of the masked language modeling (MLM) task. The findings reveal that TSP significantly outperforms existing methods across multiple scales in various language understanding tasks, indicating its potential for improving general-purpose language models.\n\n# Strength And Weaknesses\nThe paper effectively identifies and addresses the shortcomings of traditional inter-sentence tasks in language model pretraining, presenting TSP as a more informative alternative. The comprehensive evaluation across different model sizes and benchmarks (SuperGLUE) strengthens the validity of the findings. However, the paper could benefit from a deeper exploration of the implications of TSP's hierarchical and ordering relationships, and a more detailed discussion on potential limitations or edge cases where TSP may not perform optimally.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers. The experimental design is robust, with adequate detail for reproducibility, including the use of multiple runs to stabilize results. The novelty of TSP is significant, as it introduces a new paradigm for leveraging text structure in language modeling, which has not been extensively explored in prior work.\n\n# Summary Of The Review\nOverall, the paper provides a valuable contribution to the field of language model pretraining by introducing TSP, which effectively utilizes text structure information to enhance performance. The experimental results support the claims made, and the methodology is sound, though further exploration of the task's limitations would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel inter-sentence pretraining task called Text Structure Prediction (TSP), aimed at addressing the limitations of existing methods such as Next Sentence Prediction (NSP) and Sentence Order Prediction (SOP). TSP leverages text structure information by classifying sentence pairs into six distinct categories based on their structural relationships, promoting a deeper understanding of language semantics. Experimental results demonstrate that TSP outperforms traditional inter-sentence tasks across various model scales on the SuperGLUE benchmark, particularly excelling in reasoning and question-answering tasks, which indicates its potential for enhancing language understanding in larger models.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to incorporating text structure into pretraining, which has shown robust performance improvements, particularly for larger models. TSP's design enhances scalability and can be integrated with existing pretraining tasks without significant computational overhead. However, the complexity of implementing TSP may necessitate careful tuning and understanding of text structures. Additionally, the exploration of relationships is limited, and the dependency on the quality of the pretraining dataset raises concerns about generalizability across different domains. The absence of thorough hyperparameter tuning may also restrict the performance potential of TSP.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The experimental design is robust, with adequate evaluation metrics and ablation studies to support the claims. The novelty of TSP is significant, offering a fresh perspective on inter-sentence tasks for language models. However, the reproducibility may be hindered by the complexity of the task and the lack of detailed hyperparameter tuning, which could affect the consistency of results across different implementations.\n\n# Summary Of The Review\nOverall, the paper successfully introduces TSP as a compelling alternative to existing inter-sentence pretraining tasks, demonstrating its advantages in enhancing language understanding, particularly in larger models. While the approach is innovative and yields promising results, the complexity and potential limitations in hyperparameter tuning and dataset generalizability should be addressed for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Improving Language Model Pretraining with Text Structure Information\" introduces a novel pretraining method termed Text Structure Prediction (TSP), aimed at enhancing the self-supervised learning signals for language models. The authors propose TSP as a solution to the limitations of existing inter-sentence tasks like Next Sentence Prediction (NSP), which show diminishing returns at larger model scales. By classifying sentence pairs based on six designed text structure relationships, TSP provides a richer understanding of language, demonstrated through experiments across various model sizes. Results show that TSP outperforms traditional methods, especially in larger models, highlighting its potential for improving high-level language understanding.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to pretraining by incorporating text structure, which addresses the inadequacies of existing inter-sentence tasks. The experimental design is robust, testing across multiple scales and utilizing consistent datasets, leading to convincing empirical results that demonstrate the efficacy of TSP. However, the paper could benefit from a more detailed exploration of the underlying mechanisms of how TSP improves understanding, as well as a broader discussion on its limitations in practical applications. Additionally, while the ablation study provides insight into the importance of various relationships, more extensive analyses could further substantiate the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, making it accessible to readers with varying levels of expertise. The quality of the writing is high, with careful explanations of concepts and methods. The novelty of TSP is significant, as it introduces a new paradigm in sentence-level pretraining that builds on the limitations of NSP and other methods. Reproducibility is supported by detailed descriptions of the experimental setup, including hyperparameters and training procedures, thereby allowing other researchers to replicate the findings with relative ease.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in language model pretraining by introducing TSP, which effectively leverages text structure information to enhance model performance. The empirical results are robust and demonstrate the method's superior capabilities, particularly in larger models. While the paper is well-written and reproducible, further discussions on limitations and underlying mechanisms could strengthen the contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel inter-sentence task called Text Structure Prediction (TSP), which aims to leverage text structure information to enhance the pretraining of language models. The authors demonstrate that TSP outperforms existing inter-sentence tasks like Next Sentence Prediction (NSP), Sentence Order Prediction (SOP), and Sentence Sequence Order (SSO) across various scales. The methodology focuses on self-supervised learning, allowing TSP to be integrated seamlessly with existing word-level pretraining approaches, while emphasizing scalability in complex language understanding tasks.\n\n# Strength And Weaknesses\nThe paper's contributions are significant in proposing TSP as an innovative approach to enhance language model pretraining. The consistent performance improvements over traditional inter-sentence tasks highlight its effectiveness. However, the generalizability of TSP remains a concern, as it has not been thoroughly validated across diverse datasets or tasks. Additionally, while the integration potential with other methods is promising, the lack of exploration regarding hyperparameter sensitivity and potential biases in self-supervised learning could undermine the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers. The methodological framework is solid, although the novelty of TSP could benefit from a broader comparative analysis with recent advancements in inter-sentence tasks. The reproducibility of results is somewhat supported by consistent methodologies; however, the absence of hyperparameter tuning limits the robustness of the claims. Overall, the quality of the work is high, but the novelty and reproducibility aspects could be improved through more comprehensive evaluations.\n\n# Summary Of The Review\nThe paper presents a compelling new task for language model pretraining, demonstrating notable performance improvements over existing methods. However, the generalizability of the findings and the lack of detailed analysis on hyperparameter sensitivity and broader task evaluations warrant further investigation. Overall, the contributions are promising but could benefit from deeper exploration in future research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach termed Structural Text Analysis (STA) aimed at enhancing the pretraining of general-purpose language models by leveraging the structural properties of text. The authors critique existing inter-sentence tasks, such as Next Sentence Prediction and Sentence Order Prediction, for their limited effectiveness at larger model scales. STA proposes a new framework that categorizes sentence pairs according to four distinct structural relationships, emphasizing hierarchical and sequential organization. Experimental results demonstrate that STA outperforms traditional methods across various scales, particularly on the SuperGLUE benchmark, indicating its potential to improve language model performance significantly.\n\n# Strength And Weaknesses\nThe paper's primary strengths lie in its clear articulation of the limitations of existing inter-sentence pretraining methods and the innovative introduction of STA as a solution. The methodology is well-conceived, offering a fresh perspective that incorporates structural understanding into language model training. The empirical results are robust, showing consistent improvements over traditional methods, especially at larger scales. However, the paper could enhance its claims by providing a more detailed analysis of the specific structural relationships employed in STA and by including comparisons with a broader range of baseline methods to establish a clearer context for its superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its goals, methodology, and findings. The novelty of the proposed STA approach is evident, as it moves beyond conventional inter-sentence tasks to incorporate a richer understanding of text structure. The quality of the experimental design is strong, allowing for reproducibility of results, although additional information on the selection of structural relationships would further enhance clarity and understanding.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in language model pretraining through the introduction of Structural Text Analysis. By addressing the limitations of existing methods and demonstrating strong empirical results, it lays a solid foundation for future research in this area. However, further exploration of the specific structural relationships and additional baseline comparisons would strengthen its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to adversarial training through the introduction of Structured Attack Prediction (SAP), which aims to integrate structured information about potential adversarial attacks into the training process. The authors highlight the limitations of existing adversarial training methods, which struggle with scalability and robustness as model complexity increases. By employing a multi-class classification framework for attack categorization, the SAP task enhances the learning signals for models, leading to improved resilience against adversarial examples. Experimental results show that models trained using SAP achieve significant performance gains across various benchmarks, demonstrating the efficacy of the proposed method in addressing the challenges of adversarial training.\n\n# Strength And Weaknesses\n**Strengths**:\n1. The introduction of the SAP task provides a fresh perspective on adversarial training, emphasizing the importance of understanding attack structures.\n2. Thorough experimental validation across multiple datasets and model architectures supports the claims of improved robustness.\n3. The self-supervised nature of the SAP task allows for effective learning without requiring additional labeled data, enhancing the practicality of the approach.\n\n**Weaknesses**:\n1. The paper could benefit from a deeper exploration of the limitations of SAP, particularly in relation to edge cases or complex adversarial scenarios.\n2. While the focus on structured information is valuable, the methodology may oversimplify certain attack types that do not fit neatly into the proposed classification schema.\n3. More extensive comparisons with other state-of-the-art methods would strengthen the argument for the superiority of the SAP approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The writing quality is high, with a logical flow that guides the reader through the rationale, implementation, and results. The novelty of the approach is significant, as it introduces a conceptual framework that differs from traditional adversarial training techniques. The reproducibility of the experiments is adequately supported by the provided details, although additional information regarding implementation specifics and hyperparameter settings would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper makes a compelling contribution to the field of adversarial training by introducing the Structured Attack Prediction task, which enhances model robustness through structured attack information. The experimental validation supports the effectiveness of the proposed approach, although some aspects could be further explored to strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Text Structure Prediction (TSP), a novel inter-sentence task aimed at enhancing language model pretraining. TSP categorizes sentence pairs based on six predefined relationships, purportedly providing a deeper understanding of text structure. The authors claim that TSP leads to significant performance improvements across various scales in language understanding tasks, positioning it as a transformative methodology that renders previous inter-sentence tasks obsolete.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its proposition of TSP as a method to leverage text structure, which could potentially enrich the training data for language models. Additionally, the reported empirical results suggest that TSP improves performance in larger models. However, the paper significantly overstates its contributions, mischaracterizing existing methods (such as NSP and SOP) as ineffective and obsolete, which undermines the credibility of its claims. The dramatic assertions regarding TSP's superiority may lead to skepticism regarding its true impact on the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents its methodology with a clear structure, the claims made about TSP's novelty and effectiveness are inflated. The exaggeration of previous works' limitations detracts from the paper's overall quality and clarity. Novelty is present in the introduction of TSP, but the lack of a balanced perspective on prior research raises questions about the reproducibility of the claims, as the paper does not acknowledge or compare adequately with existing literature.\n\n# Summary Of The Review\nThe paper proposes TSP as a novel approach to inter-sentence tasks in language model pretraining, showing some empirical promise. However, the contributions are overstated, dismissing prior work and exaggerating the performance improvements. Overall, while the idea is interesting, the claims lack sufficient grounding in the existing research landscape.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel approach to language model pretraining called Text Structure Prediction (TSP), which leverages text structure information to enhance self-supervised learning signals. TSP classifies sentence pairs based on six text structure relationships, aiming to improve high-level language understanding, particularly in larger model scales. The authors report significant improvements in performance on the SuperGLUE benchmark across various model sizes (Small, Base, and Large), demonstrating that TSP consistently outperforms existing inter-sentence tasks like Next Sentence Prediction (NSP).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to incorporating text structure into the pretraining process, which addresses the limitations of traditional inter-sentence tasks that show diminishing returns at larger scales. The thorough experimental evaluation further supports the claims, showcasing TSP's effectiveness across multiple benchmarks. However, the paper could benefit from a more detailed explanation of the theoretical underpinnings of the chosen relationships and additional qualitative analysis of TSP’s impact on specific tasks beyond mere performance metrics. \n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly presents its contributions and findings. The methodology is straightforward, and the experimental design is robust, making it relatively easy to reproduce the results. However, the novelty of the proposed relationships could be elaborated upon to highlight their uniqueness compared to existing methods. While the results are compelling, further insights into the practical implications of TSP in real-world applications would enhance its impact.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in language model pretraining by introducing TSP, which effectively utilizes text structure to improve performance across various scales. The experimental results are robust and demonstrate significant gains over traditional methods, although some areas could benefit from deeper theoretical exploration and practical implications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach for enhancing language understanding by emphasizing the importance of text structure through a method called Text Structure Prediction (TSP). The authors argue that existing inter-sentence tasks are insufficient for larger models due to their simplicity and propose TSP as a more complex alternative that captures hierarchical relationships among sentences. The findings suggest that TSP performs well across various scales and tasks, demonstrating improved language understanding capabilities when compared to traditional methods.\n\n# Strength And Weaknesses\nThe paper makes several significant contributions, including the introduction of TSP, which addresses limitations in current inter-sentence tasks and provides a framework for modeling hierarchical relationships in text. However, the authors' assumptions regarding the universal importance of text structure and the necessity of complexity in learning signals may limit the generalizability of their findings. Furthermore, the reliance on the SuperGLUE benchmark raises questions about the robustness of their conclusions across diverse datasets and linguistic contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear articulation of the methodology and findings. However, some assumptions, such as the efficacy of TSP and the critical role of inter-sentence relationships, could benefit from more empirical support. While the novelty of the TSP approach is commendable, the reproducibility of the results may be affected by the specific evaluation metrics used, which may not capture all aspects of language understanding.\n\n# Summary Of The Review\nOverall, the paper offers a compelling argument for the role of text structure in language understanding and presents a novel method in TSP. Nevertheless, the assumptions made regarding the necessity of complexity and the limitations of existing benchmarks warrant further scrutiny to ensure the robustness and generalizability of the approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces Text Structure Prediction (TSP), a novel self-supervised pretraining task designed to improve the performance of language models at larger scales by leveraging the hierarchical and ordering relationships inherent in text structures. The methodology involves classifying sentence pairs based on defined structural relationships, which allows for the generation of contextualized sentence embeddings. The authors demonstrate that TSP consistently outperforms traditional inter-sentence pretraining tasks, such as Next Sentence Prediction (NSP), particularly in evaluations against the SuperGLUE benchmark, suggesting that richer learning signals can significantly enhance model performance.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its identification of the limitations of existing inter-sentence tasks like NSP, particularly for larger models, and its innovative approach to address these shortcomings through TSP. The empirical results are compelling, showing clear improvements in performance metrics across multiple scales. However, a potential weakness is the reliance on a specific dataset (OpenWebText) for pretraining, which may limit the generalizability of the findings to other domains or text corpora. Additionally, while the ablation study provides insights into the contributions of hierarchical and ordering relationships, further exploration into the specific mechanisms by which TSP improves performance would enhance the paper's robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, making it accessible for readers familiar with language model pretraining. The quality of the methodology is high, and the experiments are clearly outlined, facilitating reproducibility. The novelty of TSP as a task that integrates text structure into self-supervised learning is significant, addressing a critical gap in the current literature. However, the paper could benefit from a more detailed discussion on the reproducibility of the results across different datasets and model architectures.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the area of language model pretraining by introducing Text Structure Prediction, which effectively utilizes text structure to enhance learning signals. The empirical results indicate that TSP outperforms existing methods, particularly for larger models, though further exploration into its applicability across diverse datasets would strengthen the contributions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving the performance of neural networks through a new regularization technique designed to enhance generalization. The proposed method, termed Adaptive Regularization through Loss Shaping (ARLS), modifies the loss landscape during training to encourage exploration of more diverse solutions. The authors demonstrate the effectiveness of ARLS across several benchmark datasets, reporting consistent improvements over standard regularization methods, particularly in scenarios with limited training data.\n\n# Strength And Weaknesses\nThe primary contribution of this paper lies in the introduction of ARLS, which offers a fresh perspective on regularization in neural networks. The methodology is well-founded, combining established principles with innovative modifications that appear to enhance model robustness. However, the paper's weaknesses include a lack of extensive empirical comparisons with state-of-the-art techniques and limited discussion on the theoretical underpinnings of the proposed method. Additionally, while the results are promising, the evaluation could benefit from more comprehensive metrics beyond accuracy, such as robustness to adversarial examples.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is generally good, though certain sections are dense with technical jargon that may hinder understanding for readers less familiar with the concepts. The quality of the figures and tables is adequate, but some visualizations could be improved to better illustrate key results. The novelty of ARLS is significant in the context of regularization techniques, although its practical implications require further exploration. The reproducibility of the results is somewhat compromised due to the lack of detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper introduces a novel regularization technique that shows promise in improving neural network performance. While the contributions are noteworthy, the work would benefit from a more thorough empirical evaluation and clearer presentation of its theoretical foundations. Further refinement in these areas could enhance the impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel method called Text Structure Prediction (TSP) designed to enhance pretraining for language models by incorporating text structure information. TSP involves classifying sentence pairs based on six distinct text structure relationships, which are posited to provide richer self-supervised learning signals compared to existing inter-sentence tasks like next sentence prediction (NSP). The authors demonstrate that TSP leads to improved performance across various model scales, highlighting the potential benefits of utilizing text structural information in language modeling.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to leveraging text structure information, a dimension that has been somewhat overlooked in current inter-sentence pretraining tasks. By addressing the limitations of existing methods, especially NSP, the proposed TSP method presents a significant advancement in enhancing high-level language understanding. However, a potential weakness is the lack of extensive empirical comparisons with other state-of-the-art methods beyond NSP, which could limit the assessment of TSP's relative effectiveness in broader contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The quality of the writing is high, making it accessible to a broad audience. The novelty of TSP is notable, as it introduces a fresh perspective on inter-sentence relationships in language modeling. However, the reproducibility of the results could be enhanced by providing more detailed experimental setups and parameter settings, which would allow other researchers to replicate the findings more easily.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to improving language model pretraining by incorporating text structure information through the novel TSP method. While it demonstrates promising results and addresses key limitations of existing methods, further comparisons and detailed methodological disclosures could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces Text Structure Prediction (TSP) as a novel inter-sentence pretraining task aimed at enhancing the self-supervised learning capabilities of language models by leveraging text structure information. The methodology involves classifying sentence pairs based on six distinct text structure relationships, focusing on both hierarchical and ordering relationships. Experimental findings demonstrate that TSP outperforms traditional tasks like Next Sentence Prediction (NSP) and Sentence Order Prediction (SOP) across various model scales, indicating its effectiveness in improving language comprehension.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to incorporating structural information into language model pretraining, which addresses limitations identified in existing inter-sentence tasks. The comprehensive evaluation across multiple scales further strengthens the findings, showcasing TSP's robustness. However, a potential weakness is the lack of detailed exploration regarding the specific mechanisms by which TSP enhances model performance, which could benefit from further empirical investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The quality of the writing is high, making complex ideas accessible. The novelty of TSP is significant, as it presents a fresh perspective on language model pretraining. Reproducibility is facilitated by the use of established benchmarks and detailed descriptions of the experimental setup, although additional information regarding hyperparameters and training procedures could enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of language model pretraining through the introduction of TSP, which effectively utilizes text structure information to improve model performance. While the findings are promising, additional exploration into the underlying mechanisms could strengthen the contribution further.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Improving Language Model Pretraining with Text Structure Information\" introduces a novel method called Text Structure Prediction (TSP) aimed at enhancing language model pretraining by integrating text structure information. The methodology involves identifying hierarchical and ordering relationships within text, which are utilized to inform the pretraining process. The authors present experimental results indicating that models incorporating TSP outperform traditional baselines across various scales on the SuperGLUE benchmark, demonstrating improved language understanding capabilities.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to integrating text structure into language model pretraining, which addresses limitations of existing methods such as Next Sentence Prediction (NSP) and Sentence Order Prediction (SOP) at larger scales. The thorough experimental design, including ablation studies, adds credibility to the findings by validating the effectiveness of different components of TSP. However, a potential weakness is that while the results are promising, the paper could benefit from a more detailed analysis of the limitations of TSP and its applicability to a broader range of tasks beyond those evaluated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with a logical flow that facilitates understanding of the proposed methodology and results. The quality of the writing, along with comprehensive tables and figures, enhances the clarity of the presented findings. The novelty of TSP is significant as it introduces a new perspective on pretraining that leverages text structure, a relatively underexplored area in the current literature. The reproducibility of the results appears strong, given the detailed description of the experimental setup, datasets, and evaluation metrics.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in language model pretraining by introducing TSP, which effectively leverages text structure information to enhance model performance. The experimental validation supports the claims made, although further exploration of TSP's limitations and potential applications would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Text Structure Prediction (TSP), a novel self-supervised pretraining task designed to enhance language model performance by incorporating inter-sentence structural information. TSP operates through a six-class classification framework that captures hierarchical and ordering relationships between sentences, thus facilitating a deeper semantic understanding. Empirical evaluations demonstrate that TSP consistently outperforms traditional pretraining tasks such as masked language modeling (MLM) and next sentence prediction (NSP) across various model scales, particularly in larger configurations where existing methods falter.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to leveraging text structure to improve language model pretraining, addressing the limitations of traditional methods which struggle with scalability. The methodological framework is well-structured, utilizing sentence embeddings and a robust classification paradigm that enhances model comprehension. However, the paper could benefit from a more detailed exploration of the implications of TSP beyond the specific tasks evaluated, as well as potential limitations in the relational classes used.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clear and methodologically sound, with a well-organized structure that facilitates understanding. The novelty of TSP is significant, as it proposes a fresh perspective on inter-sentence relationships for language model training. However, reproducibility could be improved by providing more extensive details on the training procedures and the datasets used for evaluations. This would enable other researchers to more easily replicate the experiments and validate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in language model pretraining through the introduction of TSP, demonstrating significant empirical improvements over traditional methods. The approach is novel and well-supported by empirical evidence, though additional details on reproducibility would enhance the overall quality of the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel task called Text Structure Prediction (TSP) aimed at improving language model pretraining by incorporating text structure information. The authors argue that existing inter-sentence tasks are ineffective and present TSP as a solution. However, their methodology lacks rigorous evidence to support this claim and relies on a specific dataset (OpenWebText) that raises concerns about generalizability. The findings indicate marginal performance improvements, which do not convincingly justify the complexity of the proposed task.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to innovate by introducing TSP, which seeks to enhance language model pretraining. However, the paper is weakened by its reliance on an unsubstantiated claim regarding the ineffectiveness of existing inter-sentence tasks. Additionally, the complexity of TSP raises practical concerns, and the lack of rigorous hyperparameter tuning could undermine the validity of the findings. The limited evaluation scope and absence of comparative analysis with other methods further diminish the strength of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by the complexity of the proposed task and the lack of detailed explanations regarding its implementation. While TSP is presented as a novel concept, the novelty is undermined by its limited empirical validation and anecdotal support for its effectiveness. Reproducibility is also a concern, given the reliance on specific datasets and the absence of thorough hyperparameter tuning, which could lead to inconsistent results across different settings.\n\n# Summary Of The Review\nOverall, the paper presents an interesting but ultimately unconvincing attempt to innovate in language model pretraining through the introduction of TSP. The lack of solid evidence for the proposed method's efficacy, alongside concerns regarding its complexity and generalizability, limits its impact and relevance in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing language model pretraining through a new task called Text Structure Prediction (TSP). TSP leverages inter-sentence relationships to provide self-supervised learning signals that improve language understanding. The authors demonstrate that TSP outperforms traditional inter-sentence methods (Next Sentence Prediction, Sentence Order Prediction, and Sentence Shuffling Order) across various model scales, particularly benefiting larger models. The findings suggest that by emphasizing hierarchical and ordering relationships, TSP leads to more contextualized sentence embeddings and mimics human comprehension, which can be seamlessly integrated with existing pretraining tasks without additional computational costs.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its introduction of TSP, which significantly enhances the performance of language models by effectively capturing text structure information. This innovation is both theoretically sound and empirically validated, showcasing notable performance improvements across different model sizes. However, a potential weakness is the lack of extensive evaluations on diverse datasets beyond the standard benchmarks, which could limit the generalizability of the findings. Additionally, while the integration of TSP with existing tasks is highlighted, more detailed implementation guidelines or examples would enhance its practicality for the research community.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the methodology and results, making it accessible to a wide audience. The novelty of TSP is a significant contribution to the field, providing a fresh perspective on how to leverage text structure for language modeling. The reproducibility of the findings could be improved by including more comprehensive experimental setups and hyperparameter settings, which would allow other researchers to replicate the results more easily.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling new task for language model pretraining that successfully enhances performance by utilizing text structure information. While the empirical evidence is strong, further exploration of the task's applicability across diverse datasets would strengthen its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the Text Structure Prediction (TSP) task as a novel approach to enhancing language model pretraining by focusing on the inter-sentence relationships that are critical for understanding complex linguistic structures. The authors critique existing methods such as Next Sentence Prediction (NSP), Sentence Order Prediction (SOP), and Sentence Structure Objective (SSO), arguing that they fail to adequately capture the hierarchical and ordering structures inherent in text. TSP aims to provide richer self-supervised learning signals by integrating text structure information, thereby improving models' understanding of semantics and coherence in language comprehension.\n\n# Strength And Weaknesses\nThe main strengths of the paper include a well-structured theoretical framework that identifies the limitations of existing methods and proposes a more sophisticated alternative that addresses these shortcomings. The dual-axis approach focusing on hierarchy and order offers a compelling foundation for deeper investigations into language understanding. However, a potential weakness is the lack of empirical validation of TSP's effectiveness compared to existing methods, as the paper primarily emphasizes theoretical contributions without extensive experimental results to support its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, presenting complex ideas in an accessible manner while maintaining a high level of academic rigor. The novelty of the proposed TSP task is significant as it seeks to redefine the learning paradigm in NLP by emphasizing structural information. However, the reproducibility of findings could be questioned due to the absence of empirical results validating the theoretical assertions made throughout the paper.\n\n# Summary Of The Review\nOverall, the paper presents a theoretically robust framework that critiques existing language model pretraining methods and introduces a promising new task, Text Structure Prediction. While it offers significant theoretical insights, it would benefit from empirical validation to substantiate its claims about the effectiveness of TSP.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a self-supervised inter-sentence pretraining task called Text Structure Prediction (TSP), which aims to enhance the understanding of text structure in natural language processing tasks. The methodology involves sentence segmentation using the NLTK toolkit, selective shuffling of sentences, and token masking to prepare input sequences. The model employs a transformer encoder to generate sentence embeddings, followed by a six-class classification task over sentence pairs. The findings indicate that TSP improves performance on the SuperGLUE benchmark, with ablation studies revealing that both hierarchical and ordering relationships among sentences contribute positively to the model's effectiveness.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to leveraging text structure information through self-supervised learning, which is a pertinent area of research. The comprehensive methodology, including the careful design of the input sequence and the classification mechanism, is well-articulated. However, a notable weakness is the lack of hyperparameter tuning, which could potentially enhance performance further. Additionally, the absence of code availability raises concerns regarding reproducibility and accessibility for future researchers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with a logical flow that aids in understanding the proposed methodology. The novelty of integrating text structure into pretraining tasks is significant, yet the paper focuses more on methodological details rather than broader implications or applications of TSP. The lack of code availability detracts from the reproducibility aspect, making it challenging for others to build upon the work or validate the results independently.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of NLP through the introduction of TSP, demonstrating improvements in performance on established benchmarks. While the methodology is robust, concerns about hyperparameter tuning and reproducibility could limit the broader impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel task, Text Structural Prediction (TSP), aimed at improving upon existing inter-sentence tasks such as Next Sentence Prediction (NSP), Sentence Order Prediction (SOP), and Sentence Similarity Order (SSO). The authors claim that TSP offers superior performance at larger scales and provides better learning signals due to its focus on hierarchical relationships within text. The experimental results suggest that TSP outperforms these previous methods, although the justification for its superiority remains vague and unsubstantiated in the context of existing literature.\n\n# Strength And Weaknesses\nWhile the paper attempts to present TSP as a more sophisticated method for improving language model performance, it largely fails to clearly differentiate its contributions from previous models. The authors do not adequately address the shortcomings of NSP, SOP, and SSO, instead relying on broad statements about their failures without engaging deeply with prior research. Furthermore, the empirical results showing improvements in performance are not compelling enough to establish TSP as a significant advancement, as similar improvements have been reported for other models like BERT and ELECTRA. The lack of a thorough critique of existing methodologies and insufficient engagement with the broader research context weaken the paper's arguments and overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by its vague assertions regarding the superiority of TSP. While the methodology is presented, the lack of robust comparisons with existing methods and insufficient justification for the claims made diminish its quality. The novelty of the proposed TSP is questionable, as it does not convincingly articulate how it is distinct from past approaches. The reproducibility of the reported results is not adequately supported due to the absence of a detailed discussion on experimental setups and parameter choices, which could hinder future research efforts building on this work.\n\n# Summary Of The Review\nOverall, the paper presents an interesting idea in TSP but fails to convincingly demonstrate its novelty and superiority over existing methods. The claims made are often generic and lack sufficient engagement with prior work, making it difficult to appreciate the true contributions of this research.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Improving Language Model Pretraining with Text Structure Information\" presents a novel approach to enhance language model performance by incorporating text structure information. The authors propose a method called Text Structure Prediction (TSP), which aims to utilize structural cues from the text to improve understanding and downstream task performance. The methodology involves pretraining models on data enriched with text structure annotations, followed by fine-tuning on standard benchmarks. The findings demonstrate that models trained with TSP significantly outperform baseline models on various language understanding tasks, highlighting the effectiveness of incorporating structural information.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of text structure information into language model pretraining, which is a relatively unexplored area. The empirical results are compelling, showing substantial improvements on benchmark datasets, which underscores the significance of the proposed method. However, a notable weakness is the lack of detailed exploration into the types of text structures that are most beneficial, as well as a limited discussion on the potential limitations or drawbacks of the TSP approach. Additionally, some of the experimental setups could benefit from further elaboration to enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but several clarity issues detract from its overall quality. For instance, the abstract could be more concise and focused, while figure captions need to provide additional context for better comprehension. The introduction and subsequent sections would benefit from consistent formatting and terminology usage. Regarding reproducibility, while the authors provide a hyperparameter table, clearer descriptions of experimental setups and metrics are necessary to facilitate replication of results.\n\n# Summary Of The Review\nOverall, the paper presents a promising and novel approach to enhancing language model pretraining through the integration of text structure information. While the empirical results are impressive, clarity and consistency issues need to be addressed to improve the paper's overall quality and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents Text Structure Processing (TSP), a novel approach aimed at enhancing language understanding by integrating structured elements of text into existing models. The authors evaluate TSP primarily through the SuperGLUE benchmark, demonstrating improvements in performance across various tasks. However, they do not explore the implications of TSP in specialized domains, nor do they assess its effectiveness in real-world applications, which limits the practicality of their findings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of TSP, which offers a potentially valuable perspective on incorporating text structure into language modeling. However, several weaknesses undermine its contributions. Notably, the lack of discussion on how TSP can adapt to specialized domains, the absence of a performance evaluation across different languages, and the omission of multimodal integration are significant gaps. Additionally, the paper fails to provide a systematic hyperparameter optimization study and does not address ethical considerations related to bias in language models. The authors also neglect to explore the broader applicability of TSP to other NLP tasks, such as summarization or sentiment analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents its methodology and findings. However, the novelty of TSP is somewhat diminished by the limited exploration of its application and implications. The reproducibility of results is hampered by the lack of detailed comparisons with other state-of-the-art models and insufficient discussion of hyperparameter optimization. Moreover, the absence of practical evaluations raises concerns about the robustness of the findings.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting concept in language modeling but falls short in providing a comprehensive analysis of TSP's implications and practical applications. The lack of consideration for specialized domains, linguistic variation, and ethical concerns detracts from its potential impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper proposes an innovative inter-sentence task called Text Structure Prediction (TSP) to enhance language model pretraining. The authors conduct experiments across three model scales—Small, Base, and Large—comparing TSP with existing tasks such as Next Sentence Prediction (NSP), Sentence Order Prediction (SOP), and Sentence Similarity Order (SSO). The findings indicate that TSP consistently outperforms these baselines, particularly at larger scales where traditional methods show diminishing returns. The paper employs statistical analyses to support claims of TSP's efficacy, highlighting its ability to provide robust learning signals through text structure relationships.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous experimental design and comprehensive statistical analysis, which convincingly demonstrate the advantages of TSP over existing methods. The use of the SuperGLUE benchmark adds credibility to the evaluation metrics. However, a potential weakness is the reliance on specific performance metrics without a thorough exploration of the underlying reasons for the observed improvements, which could provide deeper insights into the mechanisms at play.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings, making it accessible to readers. The quality of the experiments is high, with a robust statistical framework supporting the results. In terms of novelty, TSP introduces a fresh perspective on inter-sentence tasks, which is a significant contribution to the field. The reproducibility is bolstered by detailed appendices and comprehensive performance metrics, although the specific implementations of TSP need to be made more explicit for complete reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the adoption of Text Structure Prediction in language model pretraining, backed by strong empirical evidence and statistical analysis. The novel approach addresses existing limitations in inter-sentence tasks, particularly at larger scales, and offers a pathway for future research in this domain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a new method called Text Structure Prediction (TSP) aimed at improving inter-sentence pretraining tasks in natural language processing. It proposes TSP as a way to exploit text structure information, with evaluations conducted on the SuperGLUE benchmark. The findings suggest that TSP has potential but do not provide conclusive evidence of significant improvements over existing methods, particularly when considering larger scale applications.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of TSP, which represents an initial step towards better leveraging text structure. However, the paper has notable weaknesses, including a lack of thorough analysis of the limitations of existing methods and insufficient exploration of various approaches to utilize text structure. The evaluation is limited to the SuperGLUE benchmark, which may not fully represent the breadth of language understanding tasks, and the authors acknowledge the absence of hyperparameter optimization, potentially limiting the effectiveness of TSP. Additionally, the paper leaves many aspects of the proposed method unexplored, such as the impact of noise from partial shuffling and the complexity of hierarchical relationships.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, but the novelty of the proposed method is somewhat diminished by the lack of comprehensive exploration of related techniques and limitations. The reproducibility of the results is hindered due to the absence of hyperparameter tuning and the reliance on a single benchmark, which raises questions about the broader applicability of the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method for inter-sentence pretraining but falls short in its exploration of the topic, limiting its contributions to the field. The lack of detailed analysis and the narrow focus on a single benchmark weaken its impact and raise concerns about reproducibility and generalizability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper proposes a method for improving language model pretraining by incorporating Text Structure Prediction (TSP) to enhance inter-sentence understanding. It identifies six inter-sentence relationships and demonstrates how text structure can be leveraged to improve comprehension in language models. The authors conduct experiments using the OpenWebText dataset and report improved performance across various scales, suggesting that TSP provides a beneficial framework for capturing hierarchical and ordering relationships in text.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its focus on text structure as an integral component of language understanding, highlighting a potentially overlooked area in NLP research. However, the methodology appears to lack novelty, as many of the concepts presented, such as the importance of context and structure, are well-established in previous literature. Additionally, the experimental results, while indicating improved performance, are based on comparisons with baselines that may not represent the current state-of-the-art, weakening the argument for TSP's superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the language could benefit from simplification to enhance accessibility. The novelty of the technical contributions is limited, as the ideas surrounding text structure and inter-sentence relationships are not new to the field. Reproducibility is supported by the use of established datasets and methodologies, although the reliance on a specific corpus raises questions about the generalizability of the results.\n\n# Summary Of The Review\nOverall, while the paper presents a relevant approach to language model pretraining by emphasizing text structure, it does not provide sufficiently novel insights or robust empirical evidence to significantly advance the field. The contributions feel more like a rehashing of existing ideas rather than groundbreaking research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a new inter-sentence task called Text Structure Prediction (TSP) aimed at addressing the limitations of existing tasks such as Next Sentence Prediction (NSP), Sentence Order Prediction (SOP), and Sentence Similarity Order (SSO) when applied at larger scales. The authors propose that TSP enhances model performance by providing richer learning signals through a focus on hierarchical and ordering relationships among sentences. Experimental results demonstrate that TSP significantly outperforms these traditional methods across various scales, indicating its potential for improving language model understanding.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of TSP, which fills a critical gap in inter-sentence tasks by leveraging text structure. This innovative approach shows promising empirical results, suggesting a solid foundation for future research. However, the paper could benefit from a more thorough exploration of complex hierarchical structures and the potential of multi-task learning frameworks. Additionally, while the findings are compelling, a lack of comparative analysis with emerging techniques like contrastive learning limits the robustness of the claims regarding TSP's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making it accessible to a broad audience. The methodology is sound, though more detail on experimental setups would enhance reproducibility. The novelty of TSP is significant, as it proposes a fresh perspective on inter-sentence relationships, yet further exploration of the task's complexities could strengthen its contribution to the field. Additionally, aspects such as fairness and bias in NLP are mentioned but require more in-depth discussion.\n\n# Summary Of The Review\nOverall, the paper provides a noteworthy contribution to the field of language modeling by introducing TSP, which shows improved performance over existing inter-sentence tasks. While the findings are promising, the paper would benefit from deeper investigations into the complexities of text structure and potential biases. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel inter-sentence pretraining task called Text Structure Prediction (TSP), designed to enhance the performance of general-purpose language models by incorporating text structure information. The methodology includes benchmarking TSP against existing inter-sentence tasks (Next Sentence Prediction, Sentence Order Prediction, and Sentence Structure Order) on the SuperGLUE dataset. The findings reveal that TSP consistently outperforms these methods across various model scales (Small, Base, Large), achieving significant improvements in performance metrics, particularly in reasoning and question-answering tasks. An ablation study supports the effectiveness of TSP by demonstrating its advantages from hierarchical and ordering relationships.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its clear demonstration of TSP's effectiveness across multiple scales, indicating its robustness and adaptability in various contexts. The rigorous benchmarking against established methods provides strong empirical support for its claims. However, a potential weakness is the lack of extensive discussion regarding the limitations of TSP or scenarios where it might not perform as well, which could provide a more balanced view of its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings, enhancing its overall quality. The novelty of introducing TSP as a pretraining task is significant, as it addresses previous shortcomings in inter-sentence tasks. Additionally, the reproducibility of the results seems feasible, given the detail provided in the experimental setup and the availability of benchmark datasets.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of language model pretraining through the introduction of TSP, demonstrating significant performance improvements over existing methods across various scales and tasks. The findings are robust and offer valuable insights for future research in natural language processing.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to inter-sentence tasks within the realm of language model pretraining. The authors propose a new framework that integrates multiple inter-sentence methods to enhance text understanding and generation. Through a series of experiments, they demonstrate significant improvements in performance metrics compared to baseline models, showcasing the effectiveness of their proposed methodology in various NLP applications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its systematic approach to combining inter-sentence tasks, which addresses a gap in the existing literature. The empirical results are compelling, indicating that the proposed framework outperforms traditional methods. However, the paper suffers from several clarity issues, including dense language and inconsistent terminology, which may hinder reader understanding. Additionally, the redundancy in phrasing diminishes the impact of the authors' contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the methodological rigor is commendable, the clarity of the presentation falls short. The abstract and section titles could be more concise, and there are abrupt transitions that disrupt the flow of the narrative. The novelty of the approach is significant; however, the paper could benefit from improved reproducibility through clearer descriptions of experimental setups and results. The figures, while informative, require more descriptive captions to enhance understanding.\n\n# Summary Of The Review\nOverall, this paper introduces a noteworthy advancement in inter-sentence tasks for language models, although its clarity and presentation require significant improvement. The contributions are relevant and impactful, but the paper's readability issues may limit its accessibility to a broader audience.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.211391382278411,
    -1.707292759128563,
    -1.6546006289630133,
    -1.6871419772951635,
    -1.8079636485438506,
    -1.6805963687270693,
    -1.478838155358349,
    -1.8654172489788747,
    -1.6853091891340144,
    -1.6709500242963953,
    -1.6016942683571462,
    -1.4533009999037254,
    -1.5340305243488648,
    -1.5382255009915922,
    -1.4561322791272033,
    -1.6968123106038224,
    -1.6090338302481124,
    -1.6774899031459305,
    -1.8287372120792909,
    -1.790328594496778,
    -1.8871607735243228,
    -1.5745209750563987,
    -1.7104342278032876,
    -1.6326178356998122,
    -1.7762260938334928,
    -1.791341971355423,
    -1.8153317719309854,
    -1.6952252659695037,
    -1.8285099449261657
  ],
  "logp_cond": [
    [
      0.0,
      -1.8533382517664994,
      -1.8459050252818534,
      -1.857193099321979,
      -1.8859205500017582,
      -1.8566249685749427,
      -1.9539489905443264,
      -1.9189812372868464,
      -1.8480642879489988,
      -1.8964806610212734,
      -1.8474015511308408,
      -1.9512247308849466,
      -1.874675920493835,
      -1.8947238719942876,
      -1.8415255303027425,
      -1.8669649615274797,
      -1.8862038928207336,
      -1.8832201196335032,
      -1.907312356728278,
      -1.8198327264961502,
      -1.8806762160927628,
      -1.9066377251708886,
      -1.901056434526536,
      -1.8719059746626876,
      -1.837928716932911,
      -1.885839651717695,
      -1.8881873051539404,
      -1.8588296822623327,
      -1.9176659947684451
    ],
    [
      -1.338799112758666,
      0.0,
      -1.2042265778156402,
      -1.2125671765488233,
      -1.231841345013087,
      -1.217609507485386,
      -1.363950273379203,
      -1.2328059934582913,
      -1.2002602245138911,
      -1.2306286498437085,
      -1.1757985006505507,
      -1.3969799257833406,
      -1.2118458060619413,
      -1.1677250591166095,
      -1.188609751143588,
      -1.2634648868020493,
      -1.2189770750959301,
      -1.2159413673171984,
      -1.2449486468254818,
      -1.265284475938413,
      -1.2485346015625283,
      -1.2944387554018097,
      -1.345585948867194,
      -1.2191459424181714,
      -1.2640493900919472,
      -1.2523026924556333,
      -1.2303464299062026,
      -1.1416357533477834,
      -1.3430237999702836
    ],
    [
      -1.3081887692497065,
      -1.1775504697324017,
      0.0,
      -1.182683682981358,
      -1.2364547738033744,
      -1.2256691148050842,
      -1.3861697934847286,
      -1.2677937663008076,
      -1.1694899310801823,
      -1.2176535114611873,
      -1.193120347552094,
      -1.389130510930438,
      -1.2281458622644381,
      -1.2265168075631887,
      -1.2593122865899697,
      -1.3004703934462478,
      -1.225186466982086,
      -1.240972083706375,
      -1.2892296058966712,
      -1.2239537846504065,
      -1.2826750898864696,
      -1.3548506054797105,
      -1.331894725327869,
      -1.225119346107177,
      -1.23427705571144,
      -1.2944501739865828,
      -1.256404790481513,
      -1.2222911607612232,
      -1.319571749823964
    ],
    [
      -1.3942902173614184,
      -1.2214719920479487,
      -1.2282942456626096,
      0.0,
      -1.2744898712330595,
      -1.2552349193211505,
      -1.3777649855950422,
      -1.2393337161923426,
      -1.1571884274305615,
      -1.2937616626162896,
      -1.2325386688570965,
      -1.4009780702058279,
      -1.169069588360397,
      -1.2315288510921467,
      -1.2589815101496526,
      -1.2714700250775934,
      -1.2785851615488881,
      -1.2549865021914208,
      -1.3072414134594865,
      -1.3277004105214194,
      -1.268990842430972,
      -1.3078232944606212,
      -1.342807372921426,
      -1.2503274620637093,
      -1.310433932092889,
      -1.3021110445049202,
      -1.243220998956228,
      -1.2919507449269392,
      -1.3323654260109388
    ],
    [
      -1.4844457618941336,
      -1.4003969885032745,
      -1.340878418234155,
      -1.4585570680592068,
      0.0,
      -1.4122238711449793,
      -1.5581454473750134,
      -1.3515919841199013,
      -1.4486113455076726,
      -1.410943601946145,
      -1.4353442085130956,
      -1.5353909838376893,
      -1.4668095451296081,
      -1.3946493507918267,
      -1.4585342456996042,
      -1.452032823774917,
      -1.3464563828670362,
      -1.3664236692234057,
      -1.4170743901140739,
      -1.4367595156345871,
      -1.3943212369100786,
      -1.475604496122989,
      -1.4581793320439271,
      -1.3712516963534005,
      -1.3594679656943554,
      -1.4360235681656768,
      -1.3787144980792807,
      -1.3904518540994915,
      -1.4497728250929476
    ],
    [
      -1.3738925621969775,
      -1.2046741487573618,
      -1.2096944781691041,
      -1.2512727159366883,
      -1.3081097047839008,
      0.0,
      -1.3601006980679884,
      -1.2911274915464968,
      -1.2193913312166353,
      -1.2952436740391717,
      -1.2135307912002675,
      -1.3940915260504243,
      -1.2872988973125612,
      -1.283933745292984,
      -1.3010514153634931,
      -1.3279118892642752,
      -1.301170004133822,
      -1.3225185389574816,
      -1.337068872089166,
      -1.3358266791641766,
      -1.2832847084644694,
      -1.400686593381479,
      -1.3351471002384008,
      -1.2701080967563616,
      -1.2976912732788437,
      -1.3596470471775184,
      -1.2792799687171734,
      -1.2745252604812527,
      -1.3616380831397674
    ],
    [
      -1.240252107201511,
      -1.1958651766386468,
      -1.202672349905991,
      -1.1701283689628934,
      -1.2415771271561498,
      -1.2135889804290205,
      0.0,
      -1.2667363385386576,
      -1.180314685590693,
      -1.2226450928603645,
      -1.2062516285736362,
      -1.2342785859257255,
      -1.2159822703735552,
      -1.2158501179837504,
      -1.207815193921163,
      -1.2142523996853387,
      -1.2150681826667014,
      -1.2265090593850123,
      -1.1912055182757197,
      -1.207347955457623,
      -1.2017920608220611,
      -1.2419714220465456,
      -1.240115844439502,
      -1.1692548804656613,
      -1.2255039649907524,
      -1.243930739231961,
      -1.2139074678685173,
      -1.2105055764155295,
      -1.235117939913026
    ],
    [
      -1.555193192717134,
      -1.3431060503476324,
      -1.3640808283021897,
      -1.3758630506529144,
      -1.3466397497747191,
      -1.3942149942126192,
      -1.5452241459518081,
      0.0,
      -1.3955556620074774,
      -1.4344722792000817,
      -1.4302460620776916,
      -1.5736305803016652,
      -1.4070482980706769,
      -1.393956905527335,
      -1.4183866989797698,
      -1.4431973852546114,
      -1.4365974249205429,
      -1.4059136125266738,
      -1.4855031083199592,
      -1.453620762286864,
      -1.4268775273503524,
      -1.44169199283187,
      -1.5115982028623767,
      -1.4057008674026064,
      -1.462500008597988,
      -1.427043432954774,
      -1.4195305233140485,
      -1.428024170920795,
      -1.4993181346553988
    ],
    [
      -1.3383208282703054,
      -1.1675107253314239,
      -1.2015475109300076,
      -1.042060228825869,
      -1.2561352720963057,
      -1.188800486825941,
      -1.3948294553928153,
      -1.2207086083741259,
      0.0,
      -1.2387785275061236,
      -1.162475214937839,
      -1.3894648436600778,
      -1.1221974947261542,
      -1.165234826048612,
      -1.2156771856984983,
      -1.2346431835925098,
      -1.2811681355202458,
      -1.2161992985052767,
      -1.3011958593587591,
      -1.2977596106948337,
      -1.2410463236037061,
      -1.316677875403184,
      -1.3437300426769705,
      -1.1776268371772836,
      -1.3117972064690135,
      -1.2744565691108947,
      -1.2315179440089732,
      -1.2181431506796223,
      -1.2973118596462088
    ],
    [
      -1.3390806918680307,
      -1.2262043339746973,
      -1.181655057942159,
      -1.2510661198496922,
      -1.239909455119657,
      -1.2422925229923816,
      -1.3584587741525314,
      -1.2758246700698763,
      -1.2357208819033336,
      0.0,
      -1.2226824829653102,
      -1.3805630640121773,
      -1.2484390984876956,
      -1.2663630982164495,
      -1.2409260494089707,
      -1.2928087263895667,
      -1.2640430563561562,
      -1.197563368628507,
      -1.2802470283385863,
      -1.2433588541998786,
      -1.224446190886597,
      -1.327508377291385,
      -1.2590616986697516,
      -1.193583241483474,
      -1.2088672759403793,
      -1.2659142307718567,
      -1.1772278591185563,
      -1.2260752778797017,
      -1.3134917028357211
    ],
    [
      -1.2481583062369213,
      -1.070619944458665,
      -1.1344640100090881,
      -1.0926163347233095,
      -1.1906760659331055,
      -1.1478203701564296,
      -1.2908623322566182,
      -1.1824656192582117,
      -1.0899878838564916,
      -1.154062492307623,
      0.0,
      -1.3192239912669106,
      -1.1319146979734,
      -1.1021379495773478,
      -1.1259428484142995,
      -1.1363716204296095,
      -1.1777589149036602,
      -1.164960991037379,
      -1.2001939630629173,
      -1.1438278258809207,
      -1.1798307882294183,
      -1.271354325464122,
      -1.2905555898679328,
      -1.160164728977134,
      -1.173392969727462,
      -1.1910013421291148,
      -1.1433247031489904,
      -1.1267111489844368,
      -1.2764196036902768
    ],
    [
      -1.1879026636988166,
      -1.1413076181252997,
      -1.142729064675005,
      -1.116544537428545,
      -1.1526704109751817,
      -1.120685347260898,
      -1.1491513550856456,
      -1.1965394804730982,
      -1.1383118155570628,
      -1.1307463649023244,
      -1.1443167629454205,
      0.0,
      -1.1666868683890943,
      -1.1385510073892284,
      -1.1248981033000671,
      -1.1382521746645757,
      -1.1497379004231483,
      -1.1633942060946698,
      -1.180978311662563,
      -1.1375858236759555,
      -1.1278381995860376,
      -1.147050460475558,
      -1.094359382794452,
      -1.1488429113506795,
      -1.1080174457889989,
      -1.1000454355877145,
      -1.1254309967501361,
      -1.16649381263036,
      -1.0701554258207657
    ],
    [
      -1.1742708901598191,
      -1.0572341631431161,
      -1.0386386953983375,
      -0.9180280773144569,
      -1.0589696038312695,
      -1.1011517521847713,
      -1.220617725530149,
      -1.0832915197997046,
      -0.9604789267462384,
      -1.0666355114397492,
      -1.0372072828624561,
      -1.2270085578852805,
      0.0,
      -0.9898937130370489,
      -1.0915198566683983,
      -1.0617945038668788,
      -1.0891273615540535,
      -1.0543194529632305,
      -1.0635140255530577,
      -1.1065059836190003,
      -1.08705925998521,
      -1.1885652398789763,
      -1.1811124190432483,
      -1.0991838362286324,
      -1.0999799198602307,
      -1.098858337149577,
      -1.0957853844541676,
      -1.0838379595182492,
      -1.1627941497730703
    ],
    [
      -1.174793790361994,
      -0.9295647809433988,
      -1.0039988952245666,
      -0.9253943760998145,
      -1.035522855135566,
      -1.0462956558267664,
      -1.1778290721802818,
      -1.0550945199682362,
      -0.9650986088749542,
      -1.0585341940767248,
      -1.009209463873768,
      -1.191301983339261,
      -0.9885841936661869,
      0.0,
      -0.9893237357584356,
      -1.0760170692832827,
      -1.0496885853244553,
      -1.0137583305804472,
      -1.0381607236690793,
      -1.069725257726275,
      -1.0747728201842943,
      -1.1343787241365089,
      -1.1828660322093665,
      -1.0608133843831518,
      -1.0957111314920145,
      -1.0559835048377633,
      -1.0263322026410282,
      -0.9901036090844592,
      -1.1617333785738455
    ],
    [
      -1.1452838998880153,
      -0.9821407880586145,
      -1.0371809474476315,
      -1.013171784756092,
      -1.0393419571259552,
      -1.0578471502351146,
      -1.1525341134022458,
      -1.0705132839739755,
      -0.9794185807447069,
      -1.0490193696892594,
      -1.005649740302083,
      -1.1626949194823692,
      -1.0480631330458814,
      -1.0123116328785737,
      0.0,
      -1.0526031121725732,
      -1.0479853648150221,
      -1.041482014881434,
      -1.0424709041094093,
      -1.0574132750207996,
      -1.037609994148678,
      -1.085058812577259,
      -1.1007482937960396,
      -1.0180744470538665,
      -1.0503676630427914,
      -1.0781568037175746,
      -0.9869727032259279,
      -1.027402248419346,
      -1.135873280670455
    ],
    [
      -1.3094612498836031,
      -1.1906151417958744,
      -1.2059039996153922,
      -1.0976264613069375,
      -1.2264850653263644,
      -1.2089714512086163,
      -1.3323781319472812,
      -1.1855764698265732,
      -1.1478100240926805,
      -1.2059351290136358,
      -1.1281825025654904,
      -1.3565523897670733,
      -1.2194762859169193,
      -1.190345746313005,
      -1.174112210242386,
      0.0,
      -1.2237697542840011,
      -1.200547291697349,
      -1.2499035927295588,
      -1.189552240155996,
      -1.2056124810918156,
      -1.261889794202367,
      -1.3309225456454306,
      -1.2080495500147657,
      -1.2296896494087524,
      -1.1898029403269303,
      -1.2192554361550982,
      -1.250694596118684,
      -1.2763014439234204
    ],
    [
      -1.3337530653432699,
      -1.1321914187214512,
      -1.1389257481687953,
      -1.140144098005117,
      -1.1903171411972213,
      -1.1786944253302947,
      -1.3157454168886862,
      -1.2175071371001236,
      -1.1587038951302238,
      -1.1797627157712554,
      -1.134040449933639,
      -1.297660386347272,
      -1.1570500716852938,
      -1.1648956238160684,
      -1.1620884492840524,
      -1.203585814028902,
      0.0,
      -1.1687908847891404,
      -1.2090635815748343,
      -1.1919927847521505,
      -1.188760794167086,
      -1.2350214932741979,
      -1.210613584054546,
      -1.1968491994973913,
      -1.1103484116578162,
      -1.1827450362373748,
      -1.1662246322391117,
      -1.2027083213234275,
      -1.2086297245372217
    ],
    [
      -1.3833327451185862,
      -1.2142314786555917,
      -1.2087983363964607,
      -1.2074438094483493,
      -1.165701519933874,
      -1.2632550338422937,
      -1.3461781262205548,
      -1.256043038422896,
      -1.2204901099926395,
      -1.2513646430500167,
      -1.2024339887024336,
      -1.343317030650803,
      -1.1499699318467658,
      -1.200561709247873,
      -1.2615029830927373,
      -1.205735064160727,
      -1.2385099238388206,
      0.0,
      -1.2084878999776894,
      -1.2182505264907402,
      -1.2189252018432588,
      -1.293280844836804,
      -1.3290389202592106,
      -1.2072639061699268,
      -1.2494404545751527,
      -1.2820899672605117,
      -1.163551728705633,
      -1.2390850370825575,
      -1.282034408392456
    ],
    [
      -1.5023457714065818,
      -1.355944547084917,
      -1.4263621919658056,
      -1.4022668396463198,
      -1.3977280315010197,
      -1.388135423101185,
      -1.537050052099428,
      -1.4857039363812048,
      -1.458547282696292,
      -1.445617270795956,
      -1.4251223863758395,
      -1.573876360670936,
      -1.4468468656940454,
      -1.3779855595563908,
      -1.3754340043703908,
      -1.4708490378701526,
      -1.4714019866207193,
      -1.383173595236015,
      0.0,
      -1.4629383794601778,
      -1.4093533449178788,
      -1.5122465276826462,
      -1.486991812973011,
      -1.3772757408953549,
      -1.450830297509355,
      -1.4636329593183655,
      -1.3851164268645617,
      -1.3533874551891223,
      -1.5348549774736266
    ],
    [
      -1.4237909482886466,
      -1.3437203137009264,
      -1.350866586927844,
      -1.406336174815097,
      -1.3629218016396487,
      -1.4073454856613279,
      -1.5040085294635879,
      -1.424324751680846,
      -1.3854730533184172,
      -1.3664846362204335,
      -1.3066091014782497,
      -1.494436854184717,
      -1.382892457209742,
      -1.3396341517664616,
      -1.3409098743062529,
      -1.3097702431134393,
      -1.3688927349025988,
      -1.3872858552586755,
      -1.3596781216584828,
      0.0,
      -1.4246519546195715,
      -1.4391674101963687,
      -1.4021351114310519,
      -1.3823178909066436,
      -1.3028944398386875,
      -1.375891103032324,
      -1.362539113512688,
      -1.313605893666795,
      -1.4116479891771634
    ],
    [
      -1.604432547940945,
      -1.4816632303558945,
      -1.5034857629158156,
      -1.5207340715313264,
      -1.5194145219526294,
      -1.4722461913821838,
      -1.6264162124097323,
      -1.4541344070109492,
      -1.5241375895383475,
      -1.5052038017482183,
      -1.4864950887783224,
      -1.6202705609357322,
      -1.5392184750780524,
      -1.5056823279497604,
      -1.4909457656689544,
      -1.5375313450990153,
      -1.5224633628678805,
      -1.4846769006459097,
      -1.5076621954804101,
      -1.515262075271854,
      0.0,
      -1.6046664431507993,
      -1.5482002251970444,
      -1.4382065638065131,
      -1.5113791106610266,
      -1.5457477466125837,
      -1.3750290239361815,
      -1.4615455354775235,
      -1.5909999056021824
    ],
    [
      -1.2733342477209935,
      -1.1980664579659548,
      -1.2200352786996222,
      -1.1986893977749014,
      -1.198832530801685,
      -1.221305950573998,
      -1.2865957638986856,
      -1.1959961909064816,
      -1.2360908046220143,
      -1.2533113250695114,
      -1.2340873765490787,
      -1.2818519285876901,
      -1.2289330420863913,
      -1.2426634662137184,
      -1.1767528924500354,
      -1.2395789322711759,
      -1.2256575618917147,
      -1.2181826648573937,
      -1.2653512909282134,
      -1.2002772568321516,
      -1.2375138702344592,
      0.0,
      -1.245524400968733,
      -1.2180591885759562,
      -1.232176381732221,
      -1.2245616020597527,
      -1.2133169849602206,
      -1.1930737356590673,
      -1.1889148094170163
    ],
    [
      -1.3935447888772492,
      -1.3485778590651654,
      -1.3088087064630285,
      -1.3614323546786304,
      -1.34341030545742,
      -1.3319630367106445,
      -1.4113344016371874,
      -1.372112885816316,
      -1.3543122249094175,
      -1.329781702820393,
      -1.36536693657544,
      -1.4167807417188334,
      -1.3887981178100173,
      -1.3870883774695681,
      -1.3611949321872725,
      -1.3927379589284887,
      -1.355971037867832,
      -1.3731842291590042,
      -1.4042813838368544,
      -1.353769497024693,
      -1.3578898042302892,
      -1.4055041246805873,
      0.0,
      -1.3480361643768608,
      -1.3275985939155666,
      -1.3998573632358144,
      -1.3563188459506794,
      -1.3638066035973353,
      -1.397024164035338
    ],
    [
      -1.330783604863408,
      -1.193019054675065,
      -1.1892580067602208,
      -1.18944527038565,
      -1.1849036236185768,
      -1.1706530032626263,
      -1.326539543779235,
      -1.2219556284199402,
      -1.1914071893254101,
      -1.200047545703471,
      -1.1580441155870198,
      -1.3554565032452583,
      -1.1985300640515304,
      -1.2200190881201534,
      -1.2187812997664058,
      -1.2406003565273054,
      -1.2255521324864032,
      -1.1848079977344468,
      -1.2272078462548306,
      -1.2435221724009904,
      -1.1487198520031896,
      -1.2999665916719219,
      -1.310547801642209,
      0.0,
      -1.2231437412157238,
      -1.2558817637121404,
      -1.1459122518959586,
      -1.196995941275047,
      -1.284530967984163
    ],
    [
      -1.4300846168676544,
      -1.3508808902719842,
      -1.2611512931199222,
      -1.3613969294917267,
      -1.3951348123370835,
      -1.3495381459904157,
      -1.4865690934680011,
      -1.400137439613916,
      -1.3660907046157185,
      -1.299005971382553,
      -1.345541342241039,
      -1.481952984717065,
      -1.4023383367838385,
      -1.3634512787917068,
      -1.3698180235992734,
      -1.419589241973881,
      -1.3662391219141623,
      -1.3708193321870004,
      -1.4059127124903277,
      -1.2890595866798278,
      -1.3494963642432736,
      -1.465679314979603,
      -1.4064398988867721,
      -1.333994655581554,
      0.0,
      -1.4109851035615582,
      -1.2995530734689145,
      -1.3745316058871404,
      -1.4298256638107116
    ],
    [
      -1.4562243059685802,
      -1.281684008485406,
      -1.3625793926456835,
      -1.3251022393718952,
      -1.3623382181412147,
      -1.3523820508740907,
      -1.4629893593276133,
      -1.312287215682959,
      -1.3506600643104127,
      -1.3520385097954821,
      -1.2937819875213452,
      -1.4655886110434266,
      -1.368495231022513,
      -1.294971540058815,
      -1.3355890168970566,
      -1.3333272577791677,
      -1.296732797859885,
      -1.3494219312570432,
      -1.2884955009545236,
      -1.3798565871184925,
      -1.3008734127708885,
      -1.4269978374712309,
      -1.3878973894426632,
      -1.3426422121949457,
      -1.3111861155420175,
      0.0,
      -1.3293447761568618,
      -1.3276714749160574,
      -1.4047610739326368
    ],
    [
      -1.5096892438052265,
      -1.3126166836698565,
      -1.3294128069589468,
      -1.3145767625402105,
      -1.2744070636698304,
      -1.3407425606337324,
      -1.5059000436915964,
      -1.3796574438935987,
      -1.3877582421625259,
      -1.3257697075494745,
      -1.3093346653734117,
      -1.500612867988349,
      -1.3607696408639522,
      -1.3214297288719568,
      -1.3543823770489187,
      -1.3574352927219657,
      -1.3873581729851776,
      -1.3011705536378242,
      -1.376578758164082,
      -1.3748554477955577,
      -1.2351644733378866,
      -1.4532528522142383,
      -1.4268700656502158,
      -1.287788685292481,
      -1.357231994088259,
      -1.3579182614094667,
      0.0,
      -1.3047053033029685,
      -1.42663615140413
    ],
    [
      -1.3429048861818838,
      -1.0781259208524674,
      -1.1723011584860275,
      -1.2484527927329283,
      -1.1779767625408497,
      -1.1659885134154022,
      -1.3652913850133814,
      -1.254458908978721,
      -1.172075400154434,
      -1.2230989050886558,
      -1.1418238894640045,
      -1.3833275850988742,
      -1.2403799375447102,
      -1.1509092355490873,
      -1.1936731186807672,
      -1.2502969480176158,
      -1.2328118584154462,
      -1.152790702776941,
      -1.195442039459134,
      -1.2092688844266513,
      -1.1610333996499445,
      -1.3186551544248806,
      -1.3273332544723753,
      -1.1401480009224598,
      -1.2443302696370737,
      -1.2606679477712583,
      -1.1586393535080355,
      0.0,
      -1.2947182733144593
    ],
    [
      -1.4989656513788383,
      -1.3846782201353596,
      -1.392400850760396,
      -1.3960861915731682,
      -1.3646505080584208,
      -1.3623325665155994,
      -1.488345247268686,
      -1.3573075829229095,
      -1.3850328981307078,
      -1.4041586021674626,
      -1.3633102559482473,
      -1.4364632776113992,
      -1.3942861705377345,
      -1.3957451919449149,
      -1.4033529928608617,
      -1.3813488595766021,
      -1.3801375107861298,
      -1.3580607843575754,
      -1.4208623682350519,
      -1.4249008527313975,
      -1.3718891469844308,
      -1.3781193142679888,
      -1.427547361634195,
      -1.3855514833328995,
      -1.4093265686599374,
      -1.3974432047806842,
      -1.366970522929535,
      -1.391579052822822,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.3580531305119117,
      0.3654863569965576,
      0.3541982829564321,
      0.32547083227665285,
      0.3547664137034683,
      0.2574423917340847,
      0.2924101449915646,
      0.36332709432941224,
      0.3149107212571376,
      0.3639898311475702,
      0.2601666513934644,
      0.336715461784576,
      0.31666751028412343,
      0.36986585197566857,
      0.34442642075093133,
      0.3251874894576774,
      0.3281712626449078,
      0.3040790255501331,
      0.3915586557822608,
      0.3307151661856482,
      0.30475365710752245,
      0.3103349477518751,
      0.33948540761572343,
      0.3734626653455,
      0.32555173056071607,
      0.32320407712447063,
      0.35256170001607834,
      0.2937253875099659
    ],
    [
      0.3684936463698969,
      0.0,
      0.5030661813129227,
      0.49472558257973964,
      0.475451414115476,
      0.48968325164317683,
      0.3433424857493599,
      0.4744867656702716,
      0.5070325346146718,
      0.4766641092848545,
      0.5314942584780122,
      0.3103128333452223,
      0.4954469530666217,
      0.5395677000119534,
      0.5186830079849749,
      0.44382787232651366,
      0.4883156840326328,
      0.49135139181136456,
      0.46234411230308115,
      0.44200828319014995,
      0.45875815756603466,
      0.4128540037267532,
      0.36170681026136897,
      0.4881468167103915,
      0.44324336903661576,
      0.4549900666729296,
      0.47694632922236035,
      0.5656570057807795,
      0.3642689591582793
    ],
    [
      0.3464118597133068,
      0.47705015923061156,
      0.0,
      0.47191694598165523,
      0.41814585515963887,
      0.4289315141579291,
      0.2684308354782847,
      0.38680686266220565,
      0.485110697882831,
      0.436947117501826,
      0.46148028141091935,
      0.26547011803257536,
      0.4264547666985752,
      0.4280838213998246,
      0.39528834237304356,
      0.3541302355167655,
      0.4294141619809273,
      0.4136285452566384,
      0.36537102306634206,
      0.4306468443126068,
      0.3719255390765437,
      0.2997500234833028,
      0.3227059036351443,
      0.4294812828558363,
      0.4203235732515733,
      0.3601504549764305,
      0.3981958384815003,
      0.43230946820179006,
      0.3350288791390492
    ],
    [
      0.292851759933745,
      0.4656699852472148,
      0.4588477316325539,
      0.0,
      0.412652106062104,
      0.43190705797401296,
      0.30937699170012123,
      0.4478082611028209,
      0.529953549864602,
      0.3933803146788739,
      0.45460330843806696,
      0.2861639070893356,
      0.5180723889347665,
      0.45561312620301675,
      0.4281604671455108,
      0.41567195221757003,
      0.40855681574627534,
      0.43215547510374264,
      0.37990056383567694,
      0.35944156677374406,
      0.41815113486419153,
      0.3793186828345423,
      0.3443346043737374,
      0.4368145152314542,
      0.3767080452022744,
      0.38503093279024325,
      0.4439209783389355,
      0.3951912323682243,
      0.3547765512842247
    ],
    [
      0.32351788664971703,
      0.40756666004057607,
      0.4670852303096955,
      0.3494065804846438,
      0.0,
      0.39573977739887134,
      0.24981820116883724,
      0.4563716644239493,
      0.35935230303617804,
      0.3970200465977056,
      0.372619440030755,
      0.27257266470616126,
      0.34115410341424246,
      0.4133142977520239,
      0.3494294028442464,
      0.3559308247689337,
      0.46150726567681444,
      0.44153997932044486,
      0.39088925842977673,
      0.37120413290926346,
      0.413642411633772,
      0.33235915242086156,
      0.3497843164999235,
      0.4367119521904501,
      0.44849568284949526,
      0.37194008037817383,
      0.4292491504645699,
      0.41751179444435915,
      0.35819082345090303
    ],
    [
      0.30670380653009177,
      0.4759222199697075,
      0.47090189055796516,
      0.429323652790381,
      0.37248666394316854,
      0.0,
      0.3204956706590809,
      0.38946887718057255,
      0.46120503751043396,
      0.38535269468789757,
      0.4670655775268018,
      0.286504842676645,
      0.3932974714145081,
      0.3966626234340853,
      0.3795449533635762,
      0.35268447946279413,
      0.3794263645932474,
      0.3580778297695877,
      0.3435274966379034,
      0.34476968956289267,
      0.3973116602625999,
      0.27990977534559036,
      0.3454492684886685,
      0.4104882719707077,
      0.3829050954482256,
      0.3209493215495509,
      0.4013164000098959,
      0.4060711082458166,
      0.3189582855873019
    ],
    [
      0.23858604815683804,
      0.2829729787197022,
      0.2761658054523579,
      0.30870978639545554,
      0.2372610282021992,
      0.2652491749293284,
      0.0,
      0.2121018168196913,
      0.298523469767656,
      0.25619306249798446,
      0.27258652678471273,
      0.24455956943262347,
      0.26285588498479373,
      0.2629880373745985,
      0.27102296143718596,
      0.26458575567301024,
      0.2637699726916476,
      0.25232909597333664,
      0.2876326370826292,
      0.271490199900726,
      0.2770460945362878,
      0.2368667333118033,
      0.238722310918847,
      0.30958327489268767,
      0.2533341903675965,
      0.234907416126388,
      0.2649306874898316,
      0.2683325789428195,
      0.24372021544532285
    ],
    [
      0.31022405626174065,
      0.5223111986312423,
      0.501336420676685,
      0.48955419832596037,
      0.5187774992041556,
      0.47120225476625555,
      0.3201931030270666,
      0.0,
      0.46986158697139735,
      0.43094496977879304,
      0.43517118690118317,
      0.2917866686772095,
      0.45836895090819785,
      0.47146034345153964,
      0.44703054999910496,
      0.42221986372426334,
      0.42881982405833186,
      0.45950363645220094,
      0.3799141406589155,
      0.41179648669201074,
      0.4385397216285223,
      0.4237252561470046,
      0.35381904611649806,
      0.4597163815762684,
      0.4029172403808867,
      0.43837381602410064,
      0.44588672566482623,
      0.4373930780580797,
      0.36609911432347597
    ],
    [
      0.346988360863709,
      0.5177984638025905,
      0.48376167820400684,
      0.6432489603081455,
      0.4291739170377087,
      0.4965087023080734,
      0.29047973374119906,
      0.4646005807598885,
      0.0,
      0.4465306616278908,
      0.5228339741961754,
      0.29584434547393657,
      0.5631116944078602,
      0.5200743630854023,
      0.46963200343551614,
      0.4506660055415046,
      0.4041410536137686,
      0.46910989062873765,
      0.38411332977525525,
      0.3875495784391807,
      0.44426286553030825,
      0.36863131373083036,
      0.3415791464570439,
      0.5076823519567308,
      0.37351198266500085,
      0.41085262002311973,
      0.4537912451250412,
      0.4671660384543921,
      0.38799732948780563
    ],
    [
      0.3318693324283646,
      0.444745690321698,
      0.4892949663542363,
      0.4198839044467031,
      0.4310405691767383,
      0.4286575013040137,
      0.3124912501438639,
      0.39512535422651895,
      0.43522914239306165,
      0.0,
      0.44826754133108504,
      0.29038696028421795,
      0.42251092580869964,
      0.40458692607994573,
      0.4300239748874246,
      0.3781412979068286,
      0.40690696794023906,
      0.4733866556678883,
      0.390702995957809,
      0.4275911700965167,
      0.4465038334097984,
      0.34344164700501034,
      0.4118883256266437,
      0.47736678281292133,
      0.4620827483560159,
      0.40503579352453856,
      0.49372216517783896,
      0.44487474641669356,
      0.35745832146067413
    ],
    [
      0.353535962120225,
      0.5310743238984812,
      0.4672302583480581,
      0.5090779336338367,
      0.41101820242404075,
      0.45387389820071666,
      0.31083193610052806,
      0.41922864909893454,
      0.5117063845006546,
      0.4476317760495232,
      0.0,
      0.28247027709023564,
      0.4697795703837462,
      0.49955631877979845,
      0.47575141994284675,
      0.46532264792753675,
      0.423935353453486,
      0.4367332773197672,
      0.401500305294229,
      0.45786644247622554,
      0.4218634801277279,
      0.33033994289302426,
      0.3111386784892134,
      0.44152953938001227,
      0.42830129862968414,
      0.41069292622803144,
      0.45836956520815586,
      0.47498311937270943,
      0.3252746646668694
    ],
    [
      0.2653983362049088,
      0.3119933817784257,
      0.3105719352287204,
      0.33675646247518043,
      0.30063058892854366,
      0.33261565264282744,
      0.3041496448180798,
      0.2567615194306272,
      0.31498918434666257,
      0.322554635001401,
      0.3089842369583049,
      0.0,
      0.2866141315146311,
      0.314749992514497,
      0.3284028966036583,
      0.3150488252391497,
      0.30356309948057714,
      0.2899067938090556,
      0.27232268824116246,
      0.3157151762277699,
      0.3254628003176878,
      0.3062505394281674,
      0.3589416171092734,
      0.3044580885530459,
      0.34528355411472655,
      0.3532555643160109,
      0.3278700031535893,
      0.2868071872733655,
      0.3831455740829597
    ],
    [
      0.35975963418904566,
      0.47679636120574864,
      0.49539182895052725,
      0.6160024470344079,
      0.4750609205175953,
      0.43287877216409343,
      0.31341279881871587,
      0.45073900454916016,
      0.5735515976026264,
      0.46739501290911556,
      0.49682324148640866,
      0.30702196646358426,
      0.0,
      0.5441368113118159,
      0.44251066768046643,
      0.472236020481986,
      0.44490316279481124,
      0.4797110713856343,
      0.47051649879580704,
      0.42752454072986446,
      0.44697126436365475,
      0.34546528446988845,
      0.3529181053056165,
      0.4348466881202324,
      0.4340506044886341,
      0.43517218719928774,
      0.43824513989469716,
      0.45019256483061554,
      0.3712363745757945
    ],
    [
      0.3634317106295981,
      0.6086607200481934,
      0.5342266057670255,
      0.6128311248917777,
      0.5027026458560262,
      0.49192984516482574,
      0.3603964288113104,
      0.48313098102335594,
      0.5731268921166379,
      0.4796913069148674,
      0.5290160371178241,
      0.3469235176523311,
      0.5496413073254053,
      0.0,
      0.5489017652331566,
      0.4622084317083095,
      0.48853691566713686,
      0.524467170411145,
      0.5000647773225129,
      0.46850024326531714,
      0.46345268080729785,
      0.4038467768550833,
      0.3553594687822257,
      0.4774121166084404,
      0.4425143694995777,
      0.4822419961538289,
      0.511893298350564,
      0.548121891907133,
      0.37649212241774666
    ],
    [
      0.31084837923918807,
      0.47399149106858884,
      0.4189513316795719,
      0.4429604943711114,
      0.4167903220012481,
      0.3982851288920888,
      0.30359816572495757,
      0.38561899515322784,
      0.47671369838249644,
      0.407112909437944,
      0.45048253882512035,
      0.2934373596448341,
      0.40806914608132194,
      0.4438206462486296,
      0.0,
      0.40352916695463015,
      0.4081469143121812,
      0.41465026424576923,
      0.413661375017794,
      0.3987190041064037,
      0.4185222849785253,
      0.37107346654994444,
      0.3553839853311638,
      0.4380578320733368,
      0.40576461608441194,
      0.3779754754096287,
      0.46915957590127544,
      0.4287300307078574,
      0.3202589984567483
    ],
    [
      0.3873510607202193,
      0.5061971688079481,
      0.49090831098843024,
      0.5991858492968849,
      0.47032724527745806,
      0.48784085939520616,
      0.36443417865654126,
      0.5112358407772493,
      0.5490022865111419,
      0.4908771815901867,
      0.5686298080383321,
      0.34025992083674916,
      0.47733602468690317,
      0.5064665642908175,
      0.5227001003614364,
      0.0,
      0.4730425563198213,
      0.49626501890647345,
      0.4469087178742637,
      0.5072600704478265,
      0.4911998295120068,
      0.4349225164014554,
      0.36588976495839187,
      0.48876276058905677,
      0.46712266119507007,
      0.5070093702768921,
      0.4775568744487242,
      0.4461177144851385,
      0.42051086668040205
    ],
    [
      0.27528076490484255,
      0.4768424115266612,
      0.4701080820793171,
      0.46888973224299546,
      0.4187166890508911,
      0.43033940491781775,
      0.29328841335942624,
      0.3915266931479888,
      0.45032993511788866,
      0.42927111447685706,
      0.47499338031447347,
      0.31137344390084043,
      0.4519837585628186,
      0.44413820643204405,
      0.44694538096406,
      0.40544801621921045,
      0.0,
      0.440242945458972,
      0.39997024867327813,
      0.4170410454959619,
      0.4202730360810265,
      0.37401233697391456,
      0.3984202461935664,
      0.4121846307507211,
      0.49868541859029625,
      0.4262887940107376,
      0.44280919800900076,
      0.4063255089246849,
      0.4004041057108907
    ],
    [
      0.2941571580273443,
      0.4632584244903388,
      0.46869156674946977,
      0.4700460936975812,
      0.5117883832120564,
      0.41423486930363684,
      0.3313117769253757,
      0.4214468647230345,
      0.456999793153291,
      0.4261252600959138,
      0.47505591444349693,
      0.3341728724951274,
      0.5275199712991647,
      0.4769281938980574,
      0.41598692005319315,
      0.47175483898520354,
      0.43897997930710986,
      0.0,
      0.46900200316824114,
      0.4592393766551903,
      0.4585647013026717,
      0.38420905830912644,
      0.34845098288671994,
      0.47022599697600365,
      0.42804944857077776,
      0.3953999358854188,
      0.5139381744402975,
      0.43840486606337303,
      0.3954554947534745
    ],
    [
      0.3263914406727091,
      0.4727926649943739,
      0.40237502011348525,
      0.42647037243297103,
      0.4310091805782712,
      0.44060178897810576,
      0.29168715997986294,
      0.34303327569808606,
      0.3701899293829989,
      0.3831199412833348,
      0.4036148257034513,
      0.2548608514083548,
      0.38189034638524544,
      0.45075165252290006,
      0.4533032077089001,
      0.3578881742091382,
      0.35733522545857155,
      0.44556361684327594,
      0.0,
      0.36579883261911306,
      0.41938386716141207,
      0.31649068439664463,
      0.34174539910627977,
      0.451461471183936,
      0.37790691456993586,
      0.3651042527609254,
      0.44362078521472914,
      0.47534975689016856,
      0.2938822346056642
    ],
    [
      0.36653764620813134,
      0.4466082807958516,
      0.43946200756893394,
      0.38399241968168085,
      0.42740679285712924,
      0.38298310883545006,
      0.2863200650331901,
      0.366003842815932,
      0.40485554117836076,
      0.42384395827634447,
      0.48371949301852823,
      0.2958917403120609,
      0.40743613728703587,
      0.45069444273031634,
      0.4494187201905251,
      0.4805583513833387,
      0.4214358595941792,
      0.4030427392381024,
      0.43065047283829516,
      0.0,
      0.36567663987720644,
      0.3511611843004092,
      0.3881934830657261,
      0.40801070359013436,
      0.4874341546580905,
      0.41443749146445397,
      0.42778948098408986,
      0.476722700829983,
      0.37868060531961456
    ],
    [
      0.28272822558337785,
      0.40549754316842823,
      0.38367501060850717,
      0.3664267019929963,
      0.3677462515716934,
      0.41491458214213894,
      0.26074456111459043,
      0.43302636651337356,
      0.3630231839859752,
      0.38195697177610444,
      0.4006656847460004,
      0.2668902125885906,
      0.34794229844627034,
      0.38147844557456234,
      0.39621500785536834,
      0.3496294284253074,
      0.36469741065644223,
      0.40248387287841303,
      0.3794985780439126,
      0.3718986982524688,
      0.0,
      0.28249433037352345,
      0.3389605483272784,
      0.44895420971780964,
      0.3757816628632962,
      0.341413026911739,
      0.5121317495881412,
      0.42561523804679924,
      0.29616086792214036
    ],
    [
      0.3011867273354052,
      0.37645451709044386,
      0.3544856963567764,
      0.3758315772814973,
      0.3756884442547137,
      0.3532150244824006,
      0.2879252111577131,
      0.3785247841499171,
      0.3384301704343844,
      0.3212096499868873,
      0.3404335985073199,
      0.2926690464687085,
      0.34558793297000734,
      0.3318575088426803,
      0.3977680826063632,
      0.33494204278522277,
      0.34886341316468394,
      0.35633831019900497,
      0.30916968412818524,
      0.37424371822424707,
      0.3370071048219394,
      0.0,
      0.3289965740876657,
      0.35646178648044247,
      0.34234459332417755,
      0.3499593729966459,
      0.3612039900961781,
      0.3814472393973314,
      0.3856061656393823
    ],
    [
      0.3168894389260384,
      0.3618563687381222,
      0.4016255213402591,
      0.34900187312465714,
      0.36702392234586756,
      0.37847119109264304,
      0.2990998261661002,
      0.3383213419869715,
      0.3561220028938701,
      0.3806525249828945,
      0.34506729122784763,
      0.2936534860844542,
      0.3216361099932703,
      0.32334585033371943,
      0.3492392956160151,
      0.31769626887479885,
      0.35446318993545556,
      0.3372499986442834,
      0.30615284396643316,
      0.3566647307785946,
      0.3525444235729984,
      0.3049301031227003,
      0.0,
      0.3623980634264268,
      0.382835633887721,
      0.31057686456747313,
      0.35411538185260816,
      0.3466276242059523,
      0.31341006376794955
    ],
    [
      0.30183423083640415,
      0.43959878102474725,
      0.44335982893959147,
      0.44317256531416227,
      0.4477142120812354,
      0.4619648324371859,
      0.3060782919205771,
      0.410662207279872,
      0.4412106463744021,
      0.4325702899963413,
      0.4745737201127924,
      0.2771613324545539,
      0.43408777164828183,
      0.41259874757965886,
      0.4138365359334064,
      0.39201747917250684,
      0.40706570321340907,
      0.44780983796536544,
      0.40540998944498163,
      0.38909566329882184,
      0.4838979836966226,
      0.33265124402789037,
      0.32207003405760326,
      0.0,
      0.40947409448408845,
      0.3767360719876718,
      0.48670558380385365,
      0.4356218944247652,
      0.34808686771564923
    ],
    [
      0.3461414769658384,
      0.4253452035615086,
      0.5150748007135706,
      0.4148291643417661,
      0.38109128149640936,
      0.4266879478430772,
      0.2896570003654917,
      0.37608865421957693,
      0.41013538921777437,
      0.4772201224509398,
      0.4306847515924539,
      0.2942731091164279,
      0.3738877570496544,
      0.41277481504178604,
      0.40640807023421943,
      0.3566368518596119,
      0.40998697191933053,
      0.4054067616464925,
      0.37031338134316516,
      0.48716650715366505,
      0.42672972959021926,
      0.3105467788538898,
      0.3697861949467207,
      0.4422314382519388,
      0.0,
      0.3652409902719347,
      0.47667302036457837,
      0.40169448794635243,
      0.3464004300227812
    ],
    [
      0.33511766538684284,
      0.5096579628700171,
      0.42876257870973955,
      0.46623973198352786,
      0.42900375321420836,
      0.43895992048133237,
      0.32835261202780974,
      0.479054755672464,
      0.4406819070450103,
      0.43930346155994093,
      0.49755998383407785,
      0.3257533603119964,
      0.42284674033291014,
      0.49637043129660796,
      0.4557529544583665,
      0.45801471357625534,
      0.494609173495538,
      0.4419200400983798,
      0.5028464704008995,
      0.4114853842369306,
      0.49046855858453453,
      0.3643441338841922,
      0.4034445819127599,
      0.44869975916047733,
      0.4801558558134056,
      0.0,
      0.4619971951985613,
      0.46367049643936564,
      0.38658089742278623
    ],
    [
      0.30564252812575887,
      0.5027150882611289,
      0.48591896497203857,
      0.5007550093907749,
      0.540924708261155,
      0.47458921129725296,
      0.309431728239389,
      0.4356743280373867,
      0.4275735297684595,
      0.48956206438151084,
      0.5059971065575737,
      0.31471890394263635,
      0.45456213106703314,
      0.49390204305902863,
      0.46094939488206665,
      0.45789647920901966,
      0.4279735989458078,
      0.5141612182931612,
      0.43875301376690334,
      0.4404763241354277,
      0.5801672985930988,
      0.3620789197167471,
      0.3884617062807696,
      0.5275430866385045,
      0.4580997778427265,
      0.4574135105215187,
      0.0,
      0.5106264686280169,
      0.38869562052685547
    ],
    [
      0.35232037978762,
      0.6170993451170363,
      0.5229241074834763,
      0.44677247323657543,
      0.517248503428654,
      0.5292367525541015,
      0.3299338809561223,
      0.4407663569907827,
      0.5231498658150697,
      0.4721263608808479,
      0.5534013765054993,
      0.3118976808706295,
      0.4548453284247935,
      0.5443160304204164,
      0.5015521472887365,
      0.4449283179518879,
      0.4624134075540576,
      0.5424345631925627,
      0.49978322651036966,
      0.4859563815428525,
      0.5341918663195593,
      0.37657011154462317,
      0.36789201149712847,
      0.5550772650470439,
      0.45089499633243,
      0.4345573181982454,
      0.5365859124614683,
      0.0,
      0.4005069926550444
    ],
    [
      0.3295442935473274,
      0.4438317247908061,
      0.4361090941657697,
      0.43242375335299754,
      0.4638594368677449,
      0.4661773784105663,
      0.3401646976574797,
      0.47120236200325616,
      0.44347704679545785,
      0.4243513427587031,
      0.4651996889779184,
      0.39204666731476645,
      0.4342237743884312,
      0.43276475298125083,
      0.425156952065304,
      0.44716108534956356,
      0.4483724341400359,
      0.4704491605685903,
      0.40764757669111384,
      0.40360909219476815,
      0.45662079794173493,
      0.45039063065817686,
      0.40096258329197076,
      0.44295846159326624,
      0.41918337626622826,
      0.4310667401454815,
      0.46153942199663067,
      0.4369308921033437,
      0.0
    ]
  ],
  "row_avgs": [
    0.3314531524552156,
    0.4601024852152289,
    0.39498539110420283,
    0.40767978596327087,
    0.38335446729626227,
    0.37774217961356055,
    0.2627509755110022,
    0.42881954711021125,
    0.440772935381458,
    0.4144006246623567,
    0.42609350542993923,
    0.31368586106403606,
    0.44483823472570844,
    0.48177582672530905,
    0.4019397713171428,
    0.4748328972261081,
    0.41700474793182796,
    0.4342642471382389,
    0.3872722454593873,
    0.40889171656903905,
    0.3693803810598303,
    0.34778042740251164,
    0.34220254412343304,
    0.4063238014723729,
    0.3981826102993278,
    0.4393448242646049,
    0.45197370583363394,
    0.47176367716312984,
    0.43133661496495307
  ],
  "col_avgs": [
    0.322847993439937,
    0.4572272232040038,
    0.44592138615215177,
    0.45080834550166066,
    0.42415040605009946,
    0.4275873506221642,
    0.30653175147268424,
    0.4039723982538814,
    0.4405308714640502,
    0.4155185565258079,
    0.4476790930075967,
    0.2978301539310097,
    0.42028138711677737,
    0.4346310786762536,
    0.42676717982586926,
    0.3991143517214925,
    0.4097989274715785,
    0.4274318008476177,
    0.39688022981496685,
    0.4062970656964572,
    0.4246376761551297,
    0.3494067245668794,
    0.3527621659199937,
    0.4359553905697857,
    0.4132452722899451,
    0.39079729006900416,
    0.43904885528808624,
    0.4293235154787853,
    0.35396474334963407
  ],
  "combined_avgs": [
    0.3271505729475763,
    0.45866485420961634,
    0.42045338862817727,
    0.42924406573246576,
    0.40375243667318084,
    0.40266476511786237,
    0.2846413634918432,
    0.4163959726820463,
    0.44065190342275407,
    0.4149595905940823,
    0.436886299218768,
    0.3057580074975229,
    0.4325598109212429,
    0.45820345270078133,
    0.41435347557150604,
    0.4369736244738003,
    0.41340183770170325,
    0.4308480239929283,
    0.3920762376371771,
    0.4075943911327481,
    0.39700902860748,
    0.34859357598469554,
    0.3474823550217134,
    0.4211395960210793,
    0.40571394129463645,
    0.41507105716680454,
    0.4455112805608601,
    0.45054359632095753,
    0.39265067915729357
  ],
  "gppm": [
    563.7980698399637,
    509.86909071273476,
    513.429881718064,
    509.81326641909425,
    521.1293683265686,
    521.9312837722529,
    571.3217807068912,
    530.1658631772658,
    517.4222894351261,
    529.0513535307929,
    513.4103751277338,
    579.5271898073538,
    528.0678742586522,
    523.0008569649883,
    525.9420949346897,
    535.0883094857146,
    531.5192265783754,
    522.4359812062766,
    534.1984433592804,
    530.1226519142192,
    518.7263981785751,
    556.0387915166558,
    553.6569775692909,
    519.1140155154783,
    528.0104408944535,
    538.0191353230388,
    515.6859954539028,
    522.1499636178393,
    554.2228145072319
  ],
  "gppm_normalized": [
    1.2553338540086663,
    1.1693973314940134,
    1.1844350132207484,
    1.1676937924529458,
    1.1930396434060537,
    1.1925558684276603,
    1.309967333835821,
    1.2045509437147948,
    1.1823103663069343,
    1.2082220507188872,
    1.1680604047200096,
    1.3226086650458733,
    1.2089034863780834,
    1.1943493279763187,
    1.1993092786179511,
    1.217682598934459,
    1.2130661110856686,
    1.1946587958832886,
    1.217849497900019,
    1.2135683434874178,
    1.1790430686238873,
    1.262337725564424,
    1.2577189133113562,
    1.182997939765449,
    1.2047625170307674,
    1.2229915029160345,
    1.175874929392238,
    1.1921548699350992,
    1.258330518536214
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344
  ],
  "response_lengths": [
    1587,
    2299,
    2597,
    2762,
    2415,
    2575,
    2926,
    2140,
    2314,
    2160,
    2672,
    2376,
    2255,
    2208,
    2409,
    2271,
    2155,
    2471,
    2280,
    2303,
    2505,
    2454,
    2247,
    2316,
    2169,
    2130,
    2289,
    2246,
    1966
  ]
}