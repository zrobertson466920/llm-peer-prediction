{
  "example_idx": 85,
  "reference": "Under review as a conference paper at ICLR 2023\n\nCONTRASTIVE VISION TRANSFORMER FOR SELFSUPERVISED OUT-OF-DISTRIBUTION DETECTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOut-of-distribution (OOD) detection is a type of technique that aims to detect abnormal samples that don’t belong to the distribution of training data (or indistribution (ID) data). The technique has been applied to various image classification tasks to identify abnormal image samples for which the abnormality is caused by semantic shift (from different classes) or covariate shift (from different domains). However, disentangling OOD samples caused by different shifts remains a challenge in image OOD detection. This paper proposes Contrastive Vision Transformer (CVT), an attention-based contrastive learning model, for selfsupervised OOD detection in image classification tasks. Specifically, vision transformer architecture is integrated as a feature extracting module under a contrastive learning framework. An empirical ensemble module is developed to extract representative ensemble features, from which a balance can be achieved between semantic and covariate OOD samples. The proposed CVT model is tested in various self-supervised OOD detection tasks, and our approach outperforms state-of-theart methods by 5.12% AUROC on CIFAR-10 (ID) vs. CIFAR-100 (OOD), and by 9.77% AUROC on CIFAR-100 (ID) vs. CIFAR-10 (OOD).\n\n1\n\nINTRODUCTION\n\nAs many deep neural networks (DNNs) are deployed in real-world applications, the safety and robustness of the models get more and more attention. Most existing DNNs are trained under the closed-world assumption, i.e., the test data is assumed to be drawn i.i.d. from the same distribution as the training data (Yang et al., 2021). Although the deployed DNNs can perfectly deal with such ID samples, they would blindly classify the data coming from other classes or domains (i.e., OOD samples) into existing classes in an open-world scenario. Nguyen et al. discovered that neural networks can be easily fooled by unrecognizable images, which means that most DNNs are unreliable when encountering unknown or unseen samples. Such a few mistakes may be tolerable in some scenarios (e.g., chatbot, interactive entertainment), whereas they will bring catastrophic damage when the application area requires great safety benefits, such as automated vehicles, medical imaging and biometric security system. Therefore, it is essential to equip the model with the ability of detecting out-of-distribution data and make it more robust and reliable.\n\nGenerally, the outlier arises because of the mechanical failure, fraudulent behaviour, human error, In the field of instrument error and natural deviations in populations (Hodge & Austin, 2004). machine learning, compared with ID samples, OOD samples are regarded as the outliers due to distributional shifts. The distributional shifts can be caused by semantic shift (i.e., OOD samples from different classes) or covariate shift (i.e., OOD samples from different domains) (Yang et al., 2021). Meanwhile, the OOD samples that are semantically and stylistically very different from ID samples are referred to as far-OOD samples, and those that are semantically similar to ID samples but different from ID samples in domains are referred to as near-OOD samples (Ren et al., 2021). The out-of-distribution detection, also known as outlier detection or novelty detection, is developed to identify whether a new input belongs to the same distribution as the training data. A natural idea is to build a classifier to identify the ID and OOD data, using such as Deep Neural Network (DNN) and Support Vector Machine (SVM). However, the sample space of OOD data is almost infinite as OOD dataset is the complementary set of ID dataset, which leads to that creating a representative OOD dataset is impracticable. Moreover, OOD samples are scarce and costly in some industries (e.g., medical imaging, fraud prevention). These are main issues in the research on OOD detection.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo address these problems, researchers focus on the latent features of ID data, assuming distinguishable distributional shifts exist between ID and OOD samples in the latent feature space. Some researchers (Nalisnick et al., 2019; Serr`a et al., 2019; Xiao et al., 2020) use generative models, like Variational Auto-encoders (VAE), to extract the latent features for both ID and OOD samples, and specific OOD socres are designed and used as the metric. As an alternative, contrastive learning models can be employed to learn the latent features, such as Self-Supervised Outlier Detection (SSD) (Sehwag et al., 2020) and Contrasting Shifted Instances (CSI) (Tack et al., 2020). However, in contrastive learning, researchers usually adopt standard convolutional neural network (CNN) and its variants like ResNet (He et al., 2016) as the encoder. By contrast, the transformer-based architectures (such as the earliest Vision Transformer (ViT) (Dosovitskiy et al., 2020), DeiT (Touvron et al., 2021) and Swin Transformer (Liu et al., 2021)) gradually outperform CNNs in terms of extracting robust latent features as they can learn global long-range relationships for visual representation learning, which would facilitate the identification of ID and OOD samples.\n\nIn this paper, a Contrastive Vision Transformer (CVT) model is proposed for OOD detection under self-supervised regime for image classification tasks. The framework of contrastive learning, including data augmentation and contrastive loss, is adopted to learn the representation for all inputs, which has been shown to be reasonably effective for detecting OOD samples (Tack et al., 2020). On this basis, four extra modules are introduced into this framework: (i) To improve the distinguishability between ID and OOD samples in the latent space, vision transformer architecture rather than CNN is embedded as a feature extracting module; (ii) Since the collapse of representation is a noteworthy problem in self-supervised and unsupervised scenarios, an additional predictor structure (inspired by BYOL (Grill et al., 2020)) is employed to avoid collapsed solutions. (iii) Considering that the size of negative samples plays an important role in contrastive learning, a memory queue scheme from MoCo (He et al., 2020) is integrated to maintain the model’s performance especially when the batch size is extremely small. (iv) An ensemble module is developed to build representative ensemble features for achieving the balance between semantic and covariate OOD detection, as we observe that in our experiments the latent features from the encoder perform better on semantic OOD samples but on the contrary the latent features from the predictor perform better on covariate OOD samples. To further improve performance, a Mahalanobis distance-based OOD score function is utilised for the OOD detection, the effectiveness of which has been shown in recent papers (Sehwag et al., 2020; Ren et al., 2021).\n\nTo conclude, the key contributions of the paper are as follows:\n\n• We integrate vision transformer architecture into a contrastive learning framework and develop a new paradigm specifically for self-supervised OOD detection in image classification tasks, results outperform state-of-the-art algorithms\n\n• We develop an ensemble module to compute representative features that balance OOD\n\nsamples from different types of data shifts\n\n• We conduct extensive ablation studies to report the influences of various hyper-parameters on OOD detection tasks and benchmark the performance of CVT using different vision transformer modules including ViT, ResNet50, and Swin transformer\n\nIn the rest of the paper, related work is described in Section 2 and the main CVT model is introduced in Section 3. Followed by numerical results in Section 4 and the paper is concluded in Section 5.\n\n2 RELATED WORK\n\nContrastive learning is a self-supervised technique that has seen fast development in recent years. Chen et al. (2020) proposed a contrastive learning framework consists of four components: data augmentation module, neural network base encoder, MLP (multilayer perceptron) projection head, and contrastive loss. It incorporated a strong inductive bias by gathering samples from the same class and repelling others and achieved promising results in visual representation learning. Under a similar paradigm, many influential variants were develooped in recent years, such as SimCLR (Chen et al., 2020), MoCo, SwAV (Caron et al., 2020), BYOL and MoCo-v3 (Chen et al., 2021). MoCo introduced a queue module to store the key representations of negative samples since the number of negative samples can effectively improve performance. To maintain the consistency of keys in the queue, a momentum strategy was developed for MoCo to update the parameters of the key encoder.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nBYOL adopted an asymmetric structure to prevent the model from collapsing by adding a extra predictor module after the projector for the online networks.\n\nCurrent practice in contrastive learning adopts standard convolutional neural networks (CNN) like ResNet(He et al., 2016) as the encoder for image feature extraction. However, the attention-based architectures has recently outperformed CNN architectures in major vision tasks such as image classification, object detection, and semantic segmentation. It has been shown that attention module is capable of extracting global long-range relationships for visual representation learning. For example, ViT(Dosovitskiy et al., 2020) applied standard Transformer encoder to images classification tasks and reported good performance. It chopped a image into sequences of patches to fit the sequence-based Transformer model. It also demonstrated that the multi-head self-attention (MSA) was beneficial for representation learning in images classification. To achieve excellent results, ViT required large training data and extensive computing resources. DeiT(Touvron et al., 2021) introduced a distillation procedure (a teacher-student strategy) to enable Transformer learning from a convnet and shrinking the training time. To bring the benefit of self-attention to object detection and semantic segmentation tasks, Swin Transformer(Liu et al., 2021) employed a hierarchical architecture to produce features at various scales. It is worth to mention that, the shifted windowing approach in Swin Transformer brought a linear computational complexity compared to standard self-attention.\n\nThe generalised OOD detection operations were proposed by Yang et al. (2021) where they divided the methodology into four categories: classification-based, density-based, distance-based and reconstruction-based methods. The classification-based method usually applies a softmax function to the output layer, which enables the model producing a probabilistic result for all classes. Hendrycks & Gimpel (2016) observed that a well-trained model can give higher probability to ID samples than OOD ones. Based on the findings, ODIN (Liang et al., 2017) adopted temperature scaling in softmax function to separate ID/OOD softmax scores under standard classification models. However, the model may sometimes assign a high probability on known classes for OOD samples. To address this issue, Dirichlet-based uncertainty (DBU) (Kopetzki et al., 2021) incorporated uncertainty estimates by predicting the parameters of a Dirichlet distribution. An outstanding advantage of DBU models is that it can effectively compute epistemic distribution, aleatoric distribution, and class labels. With respect to density-based method, it established probability distribution of ID examples and placed the OOD samples in low-density areas. Pidhorskyi et al. (2018) utilised autoencoder network to capture underlying structure of data distribution and computed novelty probability. Compared to classification-based approaches, generative models usually had a worse performance (Yang et al., 2021). The idea of distance-based methods was that the distance from OOD input to ID samples may be relatively farther than distance from ID input to ID samples. People can leverage distances with Euclidean distance, geodesic distance, cosine similarity, or Mahalanobis distance between the feature embeddings of input and the centroids of all classes. SSD (Sehwag et al., 2020) showed that the Mahalanobis distance is effective in OOD detection based on a contrastive learning framework. Another OOD detector, CSI Tack et al. (2020) was also based on contrastive learning and it used distribution-shifting augmentations (e.g., rotations) to promote OOD detection.\n\n3 METHODOLOGY\n\nThis section details the architecture of the proposed CVT model, a simple yet efficient selfsupervised OOD detector, followed by the design of loss functions and evaluation metrics.\n\n3.1 MODEL ARCHITECTURE\n\nIn some previous research (Tack et al., 2020; Ren et al., 2021; Yang et al., 2021), OOD detection is divided into two stages: first, a representation learner is built and trained to extract latent features; second, an OOD score (e.g., softmax probability and distance-based score) is computed as the metric to indicate whether a sample is an OOD one. As shown in Fig. 1, the general architecture of our CVT model is composed by two parts: (i) a contrastive representation learner based on contrastive learning framework, and (ii) a Mahalanobis distance-based OOD score function.\n\nThe contrastive representation learner consists of two parallel networks: an online network, whose parameters are updated by contrastive loss and back-propagation, and a target network, where a\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nmoving average of the corresponding online parameters is used as its own parameters (Grill et al., 2020). Unlike the traditional contrastive learning model, all the encoder modules in our CVT are flexible and can be replaced by any DNN designed for extracting latent features. Here, the ViT (Fig. 2) is given as an example for the encoder module, and a pre-trained version is preferrd since the training of it requires a lot of samples. Furthermore, an additional predictor structure in BYOL is employed in the online network to avoid collapsed solutions, as well as a memory queue scheme in MoCo is integrated into the target network to maintain the model’s performance under different batch size settings. Although MoCo-v3 abandoned the memory queue design and used a large batch size to train the model, we still keep it available especially on low-memory computing infrastructure.\n\nFigure 1: Architecture of the CVT model, in which the left part is a contrastive representation learner composed by two parallel deep networks (online and target networks highlighted in blue and grey respectively) with attention-based encoder f (·), projector g(·), and/or predictor q(·). The ensemble module computes output features from Encoder (h1) and Predictor (z1), then send ensemble features for OOD score computation on the right of the figure.\n\nFigure 2: Design of the contrastive representation learner in Fig. 1. CLS is a placeholder to be used for prediction in downstream tasks and fc represents fully connected layer. Numbers in the figure indicate the dimension of the corresponding input/output. The encoder block highlighted in green is detailed on the right panel of the figure. Here we take a standard Transformer encoder as an example, however, the encoder block is flexible for other (non) attention-based architectures.\n\ni=1 without labels, two views { ̃x(1) i }\n\nGiven a dataset {xi}N i=1 of each sample are generated with data augmentation by using random ensemble transformations (Grill et al., 2020), and 2N samples are obtained in total. Then, the augmented data will be fed into the model in batches. For each view pair of the sample, ( ̃x(1), ̃x(2)), the representation pairs of (z(1) 1 ) and (z(1) 2 ) are produced by the predictor q(·) in the online network and the projector g(·) in the target network respectively. Meanwhile, two memory queues, Q1 and Q2, are created in the target\n\ni=1 and { ̃x(2) i }\n\n1 , z(2)\n\n2 , z(2)\n\nN\n\nN\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nnetwork to store the presentations z(1) respectively. The contrastive loss is calculated among the representations given by the two networks (Section 3.2). While predicting, only the online network is used to generate the representations for each sample (no data augmentation).\n\nand z(2)\n\n2\n\n2\n\nFor the OOD score function, we use the ensemble features to compute the maximum Mahalanobis distance between a new input and the clusters of the training set in latent space. The high-level features (e.g., from the predictor) usually contain more semantic information and low-level features (e.g., from the encoder) have more general properties. The semantic information helps distinguish near-OOD samples but fails on far-OOD samples. General properties can contribute to the identification of samples from different domains. Therefore, an ensemble module in the online network, where the element-wise mean of features/representations from the encoder and predictor is computed, is developed to build representative ensemble features for achieving the balance between semantic and covariate OOD detection. To keep the ensemble module feasible, we set the dimension of the output from the predictor to be the same as that from the encoder.\n\n3.2 LOSS FUNCTION DESIGN\n\nFor unlabeled training data, the representations can be extracted by contrastive learning models. Our model absorbs the strengths of MoCo, MoCo-v3, and BYOL, and employs InfoNCE (Oord et al., 2018) as our training objective. The total loss is described as follows:\n\nLtotal = Lz (1 )\n\n1\n\n+ Lz (2 )\n\n1\n\nLz(1)\n\n1\n\n= − log\n\n· z (2 )\n\n1\n\nexp(z (1 ) i=0 exp(z (1 )\n\n1\n\n2 /τ ) · z i\n\n2 /τ ))\n\n(cid:80)K\n\n(1)\n\n(2)\n\nexp(z (2 ) i=0 exp(z (2 ) 2 is the embedding stored in queue 1 and queue 2 , K is the sum of the size of both queues\n\n2 /τ ) · z i\n\n= − log\n\n2 /τ ))\n\n· z (1 )\n\nLz(2)\n\n(cid:80)K\n\n(3)\n\n1\n\n1\n\n1\n\nwhere z i and τ is hyper-parameter temperature.\n\ni }M\n\nAfter the training, we just use the online network to extract embeddings from new inputs. Given another two dataset {x id i=1 , our OOD score function should assign a value to each input. we choose distance-based method to compute the scores, which is the Mahalanobis distance from input to centroids in training dataset. Therefore we have to divide training data into C clusters. Here we use k-means clustering approach to classify every training samples to a particular class. Finally, we consider the maximum Mahalanobis distance as the OOD score, show in Eq. (4).\n\ni=1 and {x ood\n\n}M\n\ni\n\nscore(x) = max c∈C\n\n−(x − μc) ˆΣ−1\n\nc (x − μc)T\n\n(4)\n\nwhere c is the class of training data, x is the embedding of input samples,μc is the mean of training data from class c.\n\n3.3 EVALUATION METRICS\n\nAs shown in Fig. 1, we define an OOD score based threshold to classify whether a new sample belongs to OOD or not. Specifically, a sample is regarded as an OOD sample if its OOD score is larger than the predefined threshold, otherwise it is treated as an ID sample.\n\nWe adopt three commonly used metrics to evaluate the performanc eof OOD detector. AUROC, which is a threshold-independent evaluation method and can reflect the ability of model to discriminate between positive examples and negative examples. A high AUROC value means the model has good discriminatory ability and an AUROC of 0.5 corresponds to a random guess (useless model). Area Under the Precision-Recall (AUPR) is another evaluation metric that represents the ability to correctly distinguish positive samples without treating positives as negatives. The third one, FPR95 represents the false positive rate (FPR) when the true positive rate (TPR) equals to 95%. It measures the size of OOD samples that are not correctly recognised when 95% of ID samples are correctly classified.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nWe follow commonly used benchmarks from previous work and consider CIFAR-10 and CIFAR100 (Krizhevsky et al., 2009) as ID datasets. For the OOD dataset, we choose Describable Textures Dataset (DTD) (Cimpoi et al., 2014) and SVHN (Netzer et al., 2011) in order to compare the CVT model with other competing OOD detectors. ID samples are divided into ID training set and ID test set, with data leakage issue considered. For all experiments, models are trained with the ID training set in the training phase, and tested with both ID test set and OOD set in the evaluation phase. All the experiments are repeated 5 times by using different random seeds. We evaluate our model by three metrics: AUROC, AUPR, and FPR95.\n\nIn our benchmarking and ablation studies, we benchmark the performance of different representative base encoder networks under the CVT model, including ViT-B-16 (Dosovitskiy et al., 2020), a pretrained ViT variant on Imagenet2012 (Russakovsky et al., 2015); Swin Transformer, the popular and powerful transformer architecture; and ResNet50, the basic but widely used CNN architecture. We train the proposed CVT model using optimizer AdamW(Loshchilov & Hutter, 2018) with an initial learning rate 1e-6, half-cycle cosine decay, weight decay 0.1, and batch size equals to 64. Following the setup in literature SSD (Sehwag et al., 2020), we set the default temperature to 0.5 and ablate different temperature and number of clusters pairs, see Section 4.5 for more details. All experiments are run on Ubuntu 12.04 with two NVIDIA RTX A4000 GPU cards. We use Python 3.8.10 and the single queue size in CVT is set to 4096.\n\n4.1 CVT MODEL PERFORMANCE\n\nAs shown in the first row of Table 1, we test 6 pairs of combinations of ID and OOD datasets. Both CIFAR-10 and CIFAR-100 datasets shares common image characteristics (e.g., format, quality, style etc.), while SVHN dataset and Describable Texture dataset are more dissimilar to them.\n\nWe first compare CVT performance in self-supervised OOD detection tasks, where models are trained with unlabeled training data. We compare AUROC performance (higher score is better) across 6 different models including PixelCNN++(Salimans et al., 2017), Deep-SVDD(Ruff et al., 2018), Rotation-loss(Komodakis & Gidaris, 2018), CSI(Tack et al., 2020), and SSD(Sehwag et al., 2020) (shown in the top part of Table 1). The results demonstrates that the proposed CVT model clearly outperforms all other competing models. The averaged AUROC of CVT model has improved by 6.6% when comparing with the the 2nd best one (i.e., SSD). In the challenging scenario where CIFAR-100 is the ID set and CIFAR-10 serves as the OOD set, most existing OOD detectors perform poorly due to the similarity and size of CIFAR-100 over CIFAR-10. Our CVT model achieves a noteable score of 79.37%, which is almost 10% improvement from that of the 2nd best model.\n\nTable 1: Model comparison in AUROC with 6 self-supervised OOD detectors using unlabelled training data and 5 supervised OOD detectors using labelled training data. The best performed number is highlighted in bold for each ID/OOD pair. The last column reports the averaged AUROC over all available tests (when the number of tests is no less than 4).\n\nMethod\n\nPixelCNN++ Deep-SVDD Rotation-loss CSI SSD CVT (ours)\n\nID ACC (on CIFAR-10)\n\nID: CIFAR-10 vs. OOD:\n\nSVHN\n\nCIFAR-100\n\nTexture\n\n- -\n- 94.38% 95.07%\n\n15.8% 14.5% 97.9% 99.8% 99.6%\n\n52.4% 52.1% 81.2% 89.2% 90.6%\n\n- -\n- -\n97.6%\n\n97.92±0.15% 99.90±0.004% 96.72±0.001% 100±0%∗\n\nID: CIFAR-100 vs. OOD:\n\nSVHN\n\n- 16.3% 94.4% -\n94.9%\n\nCIFAR-10\n\n- 51.4% 50.1% -\n69.6%\n\n98.78±0.001% 79.37±0.002%\n\nTexture\n\n- -\n- -\n82.9% 100±0%∗\n\nCE+SimCLR† CSI† SSD+† CIDER† Fort et al.† ∗ The 5 repeated results using different random seeds are all 100%. † The supervised OOD detection method that uses ID labels for training.\n\n99.5% 97.9±0.1% 99.9% 99.73% -\n\n92.9% 92.2±0.1% 93.4% 93.04% 98.52%\n\n- 96.1±0.1% 94.55% 94.63% 98.70%\n\n78.3% 70.74% 78.3% 77.02% 96.23%\n\n95.6% -\n98.2% 97.75% -\n\n- -\n81.2% 91.96% -\n\n- -\n98.5 97.44% -\n\nAverage\n\n- 33.6% 80.9% -\n89.2% 95.80%\n\n91.6% -\n91.6% 92.82% -\n\nWe also compares CVT model against supervised OOD detectors, where models are trained with labelled ID training set and tested by both unseen ID test set and OOD set. This is an unfair comparison to the CVT model as it is still trained by unlabelled (but the same) ID training set, i.e., no label information has been provided to CVT model. Surprisingly CVT still outperforms majority of the supervised OOD detectors except model in (Fort et al., 2021). As shown in the bottom part\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nof Table 1, we benchmark CE+SimCLR(Winkens et al., 2020), supervised CSI, supervised SSD, CIDER(Ming et al., 2022), and (Fort et al., 2021) in the two pairs of CIFAR datasets. To briefly conclude, the proposed CVT model performs well in different OOD detection tasks for both near/far OOD datasets.\n\n4.2 COMPARISON BETWEEN ENSEMBLE FEATURES AND OTHER FEATURES\n\nAs depicted in Fig. 1, we develop an ensemble module to balance the features extracted from both encoder and predictor. In this subsection, we test and show the effectiveness of the ensemble module in the CVT model.\n\nTable 2: Performance of ensemble module in CVT. ‘Encoder features’ and ‘Predictor features’ represent output features produced by the Encoder component and the Predictor component in the CVT model, respectively. ‘Ensemble features’ describes the merged features computed on both encoder/predictor features.\n\nID dataset\n\nOOD dataset Encoder features\n\nPredictor features Ensemble features\n\nCIFAR-10\n\nCIFAR-100\n\nCIFAR-100 SVHN CIFAR-10 SVHN\n\n94.89% 99.95% 78.00% 99.06%\n\n96.56% 99.62% 77.26% 95.41%\n\n96.06% 99.93% 80.34% 99.21%\n\nThe first two columns in Table 2 show four test settings that are grouped into semantic shift scenarios (e.g., CIFAR-10/CIFAR-100 vs. SVHN) and covariate shift scenarios (e.g., CIFAR-10 vs. CIFAR100). By using the features from either the Encoder or the Predictor to compute the OOD score for detecting OOD samples, the results display different trends under the above two scenarios. The race of the OOD detection under the covariate shift scenario between low-level features (i.e., the features from the encoder) and high-level features (i.e., the features from the predictor) ends in a draw. By contrast, low-level features dominate the OOD detection under the semantic shift scenario. Therefore, it is hard to decide which kind of features should be finally used to compute the OOD score. However, by comparing the performance of ensemble features with others under CIFAR-10 ID dataset, one sees that the ensemble module balances the performance between low-level and highlevel features. Interestingly, one also observes that ensemble features achieves better performance under CIFAR-100 ID dataset. We reckon the ensemble features not only balance features but also enhance features for OOD detection.\n\n4.3 ENCODER USING VIT VS. RESNET VS. SWIN TRANSFOMER\n\nTo evaluate our hypothesis that the attention-based models are capable of extracting more effective features for OOD detection, we compare the performance between three architecture options for the encoder module in CVT, namely ResNet50, ViT-B-16 (Base model of ViT), and Swin-B (Base model of Swin Transfomer). In this experiment, we calculate OOD scores using features output from predictor and the number of dimensions is set as 768.\n\nFig. 3 shows experimental results of using CIFAR-10 as the ID set while CIFAR-100 (left panel) and SVHN (right panel) as the OOD set. One sees that both ViT and Swin Transformer architectures clearly outperform standard ResNet50 by about 26% in both AUROC and AUPR (higher the better), and with 67% decreases in FPR95 (lower the better). On the other hand, ResNet50 achieves slightly better performance in the far-OOD scenario (right panel). Back to the comparison between ViT-B-16 and Swin-B, one sees comparable performance in AUROC and AUPR, with ViT model performs slightly better in FPR95. This might be due to shifted window introduced in Swin Transformer, which reduces its computational complexity but makes it slightly less capable in information extraction.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Performance comparison using ResNet50, ViT-B-16, and Swin-B.\n\n4.4 DISTRIBUTIONS OF SEMANTIC SHIFTED AND COVARIATE SHIFTED DATASETS\n\nWe visualise and compare the ID and OOD distributions over the four datasets (CIFAR-10/100, SVHN and Texture) in feature space, in which CIFAR-10 is set as the ID set and the rest three are the OOD sets.\n\nFigure 4: Visualisation of in-distribution and out-of-distribution. The left figure is the FNR of ID datasets and the FPR of OOD datasets under different threshold of OOD score, and the right one is visualization of embedding on feature space.\n\nFrom the right panel of Fig. 4, one sees that both CIFAR-10 and CIFAR-100 shares some points and on the contrary SVHN points are more focal and locates further away from the CIFAR datasets. The left panel of Fig. 4 depicts the FNR and FPR of ID and OOD datasets separately in terms of the threshold of OOD score, which can illustrate the overlap level among different datasets. One observes that only CIFAR datasets are overlapped with each other obviously (about 16% of CIFAR100 samples are mixed with CIFAR-10) while SVHN and Texture are slightly overlapped, even non-overlapped, with CIFAR-10. The main reason of these observations is that SVHN and Texture belong to domains further away from CIFAR-10. We also compare distributions of CIFAR-10/100 under different hyper-parameter settings, see Fig. 6 in Appendix A.1 for details.\n\n4.5 THE ABLATION STUDY OF TEMPERATURE AND NUMBER OF CLUSTERS\n\nIn the absence of training data labels, we can divide the training dataset into k clusters and evaluate whether the number of clusters k affect the performance. The left panel of Fig. 5 shows that the model achieves good performance when the number of clusters is k = 1. The ID dataset (CIFAR100) has 10 classes, thus we select 1 and 10 as number of clusters for temperature ablation. For\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n1 cluster case, the AUROC is maximised at 0.5 temperature and the lowest AUROC is at 0.07 temperature.\n\nFigure 5: The left panel shows AUROC trends with 1 to 10 clusters and the right panel depicts the performance under different temperatures in single cluster scenario. CIFAR-10 is set as the ID set.\n\nTable 3: AUROC with different temperature values in self-supervised training. The in-distribution dataset is CIFAR-10, the OOD datasets are CIFAR-100 and SVHN respectively\n\nDataset\n\nNumber of clusters Temperature (0.07)\n\n(0.2)\n\n(0.5)\n\n(0.8)\n\nCIFAR-100\n\nSVHN\n\n1 10 1\n10\n\n65.05% 72.23% 74.94% 83.47%\n\n93.14% 96.48% 95.81% 73.69% 75.29% 76.98% 98.51% 99.41% 99.29% 81.38% 80.72% 84.66%\n\nOn the contrary, the AUROC on CIFAR-10 vs. SVHN is the minimum at 0.5 temperature when cluster number equals to 10, as shown in Table 3. The observed trend of accuracy is consistent to those in Fig. 5. In general, when we have one cluster and temperature is 0.5, the model gains the best performance.\n\n5 CONCLUSIONS\n\nIn this paper, we proposed a new self-supervised OOD detector, the CVT model, which outperformed competing OOD detectors in benchmark tests over three different datasets (CIFAR-10/100 and SVHN). In the CVT model, we developed an ensemble module, which not only balanced the performance on far and near OOD datasets, but also enhanced overall OOD detection performance. The CVT model achieved a notable 80.34% AUROC accuracy in the challenging unlabeled farOOD detection task. Although the results were remarkable, there exists open issues and promising directions for future work. Firstly, the adopted datasets are well established and recognised, but relatively constrained. Images in CIFAR-10, CIFAR-100, and SVHN normally contain only one object. Further evaluation of the proposed CVT model may requires more complicated and real datasets. Besides, with label information, a supervised OOD detector may learn more features than unsupervised learning. Hence, the supervised OOD detector is worth to be further explored and we believe the ensemble module in CVT model will also benefit supervised OOD detectors. Finally, the CVT model has a good potential to be integrated to other (non-OOD) self-supervised tasks, OOD detection enabled CVT model may save time and improve reliability of the other OOD self-supervised model.\n\nREFERENCES\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912–9924, 2020.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9640–9649, 2021.\n\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\n\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\n\nStanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution\n\ndetection. Advances in Neural Information Processing Systems, 34:7068–7081, 2021.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2016.\n\nVictoria Hodge and Jim Austin. A survey of outlier detection methodologies. Artificial intelligence\n\nreview, 22(2):85–126, 2004.\n\nNikos Komodakis and Spyros Gidaris. Unsupervised representation learning by predicting image\n\nrotations. In International Conference on Learning Representations (ICLR), 2018.\n\nAnna-Kathrin Kopetzki, Bertrand Charpentier, Daniel Z ̈ugner, Sandhya Giri, and Stephan G ̈unnemann. Evaluating robustness of predictive uncertainty estimation: Are dirichlet-based In International Conference on Machine Learning, pp. 5707–5718. PMLR, models reliable? 2021.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nShiyu Liang, Yixuan Li, and R Srikant. Principled detection of out-of-distribution examples in\n\nneural networks. arXiv preprint arXiv:1706.02690, pp. 655–662, 2017.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\nIlya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam, 2018. URL https:\n\n//openreview.net/forum?id=rk6qdGgCZ.\n\nYifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. Cider: Exploiting hyperspherical embeddings\n\nfor out-of-distribution detection. arXiv preprint arXiv:2203.04450, 2022.\n\nEric T Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting\n\nout-of-distribution inputs to deep generative models using a test for typicality. 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 427–436, 2015.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nStanislav Pidhorskyi, Ranya Almohsen, and Gianfranco Doretto. Generative probabilistic novelty detection with adversarial autoencoders. Advances in neural information processing systems, 31, 2018.\n\nJie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022, 2021.\n\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel M ̈uller, and Marius Kloft. Deep one-class classification. In International conference on machine learning, pp. 4393–4402. PMLR, 2018.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.\n\nVikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised\n\noutlier detection. In International Conference on Learning Representations, 2020.\n\nJoan Serr`a, David ́Alvarez, Vicenc ̧ G ́omez, Olga Slizovskaia, Jos ́e F N ́u ̃nez, and Jordi Luque. Input complexity and out-of-distribution detection with likelihood-based generative models. arXiv preprint arXiv:1909.11480, 2019.\n\nJihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learning on distributionally shifted instances. Advances in neural information processing systems, 33:11839–11852, 2020.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.\n\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam, Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.\n\nZhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection score for variational auto-encoder. Advances in neural information processing systems, 33:20685– 20696, 2020.\n\nJingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection:\n\nA survey. arXiv preprint arXiv:2110.11334, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 ABLATION\n\nA.1.1 THE INFLUENCE OF TEMPERATURE AND NUMBER OF CLUSTERS\n\nWe visualize the features of ID and OOD datasets under different temperature parameters in Fig. 6. Here the ID dataset is CIFAR-10, and the OOD datasets are CIFAR-100 and SVHN respectively. When observing the divergence of CIFAR-10 under 0.07 and 0.5 temperatures, we find the clusters become more cohesive when the temperature is 0.5. From the pictures on CIFAR-10 vs. SVHN at temperature 0.07, the features from SVHN are farther from features from CIFAR-10 than features from CIFAR-100. Hence, temperature controls the distance among each sample in the feature space, while the contrastive loss makes the samples from the same class close and repel others in the feature space. A low temperature will spread features, and the features will be evenly distributed when it is small enough. Although a large temperature can make features from the same class more cohesive, it also shortens the distance between different classes.\n\nFigure 6: Visualization of features under different temperature parameters. The temperature of the two pictures on the left is 0.07, and the pictures on the right are 0.5.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA.1.2 DIMENSION OF FEATURE EMBEDDINGS\n\nTo discover how many dimensions of feature embeddings are suitable for OOD score calculation, we experiment with different dimensions to compute the OOD score. Here, we use the features from predictor and encoder respectively to calculate metrics. As shown in Table 4, the AUROC begins with the lowest value at 128 dimensions on CIFAR-100 and SVHN OOD datasets when using features from predictor. It becomes stable when the dimension is larger than 256.\n\nTable 4: AUROC with different dimension values in self-supervised training and the in-distribution dataset is CIFAR-10.\n\nOOD Dataset\n\nFeature type\n\nDimensions (128)\n\n(256)\n\n(512)\n\n(768)\n\nCIFAR-100\n\nSVHN\n\nPredictor features Encoder features Predictor features Encoder features\n\n95.05% 93.59% 99% 99.83%\n\n96.48% 96.61% 96.56% 95.47% 94.07% 94.89% 99.41% 99.42% 99.62% 99.9% 99.94% 99.95%\n\nA.2 THE IMPACT OF DATA AUGMENTATION ON OOD DETECTION\n\nConsidering that data augmentation with a few specific techniques is applied to ID dataset for generating positive pairs in contrastive learning, it is valuable to explore how the data augmentation impact OOD score, especially for the ID test set. In training process of our method, the RandomResizedCrop in torchvision and the GaussianBlur in Pillow are utilised for data augmentation. The scale for cropping image is from 0.08 to 1, which is denoted by Crop(0.08, 1), whilst the radius of Gaussian kernel in GaussianBlur is from 0.1 to 2, which is denoted by GaussianBlur(0.1, 2). To explore whether the augmented samples from ID test set will still be recognised as ID ones, new parameters different from those in the training are set for these two techniques to generate augmented ID test set, and then the trained model is used to detect these new augmented samples. The results shown in Table 5 demonstrates that data augmentation has almost no impacts on the OOD score for ID samples.\n\nTable 5: The impact of data augmentation on OOD detection. This toy experiment is conducted on two scenarios, where CIFAR-10 and CIFAR-100 are used as ID dataset respectively, and FPR95 is selected as the metric, where a higher FPR95 is, the less impact data augmentation has on OOD detection.\n\nID dataset\n\nCIFAR-10 CIFAR-100\n\nAugmented ID test set Crop(0.03, 0.08) GaussianBlur(2, 3) Crop(0.03, 0.08) + GaussianBlur(2, 3)\n\n99.98% 99.99%\n\n99.76% 99.22%\n\n99.98% 99.98%\n\n13",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes to use a contrastive method that uses ViT models as encoders for out-of-distribution detection. Different from previous works, this paper uses contrastive methods with Vision Transformers encoders instead of convolutional ones. Different representations obtained by the model seem to be more appropriate to different settings (near vs far OOD) and they are combined. The resulting method obtains good results on OOD detection datasets.\n\n# Strength And Weaknesses\n\nStrong Points:\n\nHaving a strong encoder is very relevant for OOD detection, thus having a contrastive method that works with ViT model is beneficial.\n\nWeak Points:\n\nSince the contribution of this paper is empirical, coming from replacing a convolutional model with a stronger, Transformer model, the evaluation should be more rigorous. At the moment only CIFAR10-100 and SVHN are used to form IN / OOD pairs. The setups from CSI and [A] should also be evaluated.\n\nThe claim that Encoder features and Predictor features are more appropriate in different settings is based on the experiments in Table 3. But these are not conclusive, as Encoder features are better in 3 of the 4 cases so it cannot be said that they are better for semantic or covariate shifts. \n\nMultiple seeds should be used when computing the results and confidence intervals should be given.\n\n[A] Sun et al. “Out-of-Distribution Detection with Deep Nearest Neighbors” ICML 2022\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper can be reproduced. The originality of the work is low. Given that the paper repurpose existing methods with different encoders, the quality should come from rigorous evaluations, but the paper still needs to improve in this regard.\n\n# Summary Of The Review\n\nThe paper proposes to use a contrastive learning based on ViT models for OOD detection. It results in a good method, but it needs to be evaluated more rigorously.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel approach to out-of-distribution (OOD) detection in image classification using a Contrastive Vision Transformer (CVT) integrated into a self-supervised learning framework. The proposed methodology includes a two-stage model architecture: a representation learner that constructs latent features and an OOD score computation stage utilizing Mahalanobis distance. The key contributions include the innovative CVT model, an empirical ensemble module designed to balance semantic and covariate OOD samples, and comprehensive ablation studies that demonstrate the model's superior performance against existing methods on datasets such as CIFAR-10 and CIFAR-100.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its rigorous architecture that effectively combines contrastive learning with vision transformers, yielding notable improvements in OOD detection performance. The extensive ablation studies provide valuable insights into the effects of various hyperparameters and architectures, enhancing the paper's credibility. However, a potential weakness is the limited scope of datasets used for evaluation, which may not fully represent the model's capabilities across diverse domains and real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers with a background in deep learning and computer vision. The quality of the writing is high, with sufficient detail provided for reproducibility, including descriptions of the model architecture and loss functions. The novelty of using a contrastive learning framework with a transformer architecture for OOD detection is significant, although the paper could further enhance reproducibility by providing more detailed experimental setups and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of OOD detection by proposing a novel self-supervised approach using a contrastive vision transformer. While it demonstrates strong empirical results and thorough analysis, the limited diversity of datasets may restrict the generalizability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to out-of-distribution (OOD) detection in image classification through the introduction of the Contrastive Vision Transformer (CVT). This method integrates a vision transformer architecture within a contrastive learning framework to self-supervise the detection of OOD samples, which may arise from semantic or covariate shifts. Key contributions include the use of a vision transformer for enhanced feature extraction, the implementation of a contrastive learning framework with an ensemble module that balances low-level and high-level feature detection, and the use of Mahalanobis distance for OOD scoring. Experiments demonstrate that CVT significantly outperforms existing state-of-the-art OOD detectors across various datasets, achieving notable AUROC scores.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of contrastive learning with vision transformers, which has led to robust performance improvements in OOD detection. The extensive experimental evaluation, including ablation studies and comparisons against various benchmarks, adds credibility to the findings and provides valuable insights into the model's behavior. However, the paper's limitations include reliance on well-known datasets that may not fully capture real-world complexities, potential generalization issues, and the computational demands of transformer architectures that could hinder accessibility. Additionally, the self-supervised nature of the method may limit the model's ability to learn intricate features that labeled data could provide.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The quality of the writing is high, with a coherent narrative that guides the reader through the problem, approach, and findings. The novelty of the approach is significant, as it successfully combines two prominent techniques in a way that addresses a relevant and challenging problem in the field. The reproducibility is supported by detailed descriptions of the experimental setup and evaluation metrics, although the reliance on specific datasets may limit the generalizability of the results.\n\n# Summary Of The Review\nOverall, the paper makes a commendable contribution to the field of self-supervised OOD detection by introducing the Contrastive Vision Transformer, which demonstrates superior performance compared to existing methods. While it faces limitations regarding dataset variety and computational demands, the innovative approach and thorough evaluation provide a solid foundation for future work in this area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to out-of-distribution (OOD) detection in image classification through the introduction of the Contrastive Vision Transformer (CVT). The methodology combines a vision transformer architecture with a contrastive learning framework, including an empirical ensemble module designed for enhanced feature extraction. The findings indicate that the proposed CVT model significantly outperforms state-of-the-art OOD detection methods, achieving notable improvements in AUROC metrics on benchmark datasets such as CIFAR-10 and CIFAR-100.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative use of a vision transformer within a contrastive learning context, effectively addressing the challenges posed by semantic and covariate shifts in OOD detection. The empirical ensemble module adds a robust element to feature extraction, enhancing performance. However, the work could benefit from additional exploration of the model's limitations, particularly regarding its scalability to more complex datasets beyond CIFAR. Furthermore, while the results are compelling, they may not fully account for the variability in real-world scenarios where OOD samples can be more diverse.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology, findings, and implications. The quality of the writing is high, with comprehensive explanations of technical concepts and a thorough evaluation of the model's performance. The novelty of the approach is significant, as it effectively integrates recent advancements in transformers with contrastive learning for OOD detection. The reproducibility of the results is bolstered by the detailed descriptions of datasets, experimental protocols, and evaluation metrics, although the inclusion of code and data would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of OOD detection through the innovative use of a contrastive vision transformer. Its strong empirical results and clear presentation suggest it is a valuable addition to the literature, though future work could address its limitations in handling more complex datasets.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the Contrastive Vision Transformer (CVT), a novel model designed for self-supervised out-of-distribution (OOD) detection. By integrating vision transformers within a contrastive learning framework, the CVT model aims to enhance the detection of OOD samples. The authors demonstrate significant performance improvements over state-of-the-art methods, particularly on benchmark datasets like CIFAR-10 and CIFAR-100. Key components of the methodology include an ensemble module for balancing semantic and covariate OOD samples and the use of Mahalanobis distance for OOD scoring. The paper also includes extensive experiments and ablation studies that validate the effectiveness of the proposed model.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach, which applies transformer architecture to OOD detection, and the significant performance improvements reported. The ensemble module is a noteworthy contribution that enhances the model's capability to differentiate between various OOD instances. However, the reliance on transformer architectures raises concerns regarding computational efficiency, particularly for applications with limited resources. Additionally, while the paper presents robust results on specific datasets, questions remain about the generalizability of these findings to more complex, real-world scenarios. The flexibility of the encoder module is another double-edged sword, potentially leading to inconsistencies in performance if not optimized properly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and findings. The methodology is detailed, allowing for a good understanding of the CVT model and its components. However, the empirical evaluation is somewhat limited to established datasets, and the reproducibility of results in more diverse real-world settings is not fully addressed. The novelty of the approach is significant, particularly with the introduction of contrastive learning to OOD detection; however, further exploration in varied contexts would enhance the paper's impact.\n\n# Summary Of The Review\nIn summary, this paper presents a promising and innovative approach to OOD detection through the Contrastive Vision Transformer model, showcasing notable performance improvements. While the contributions are substantial, the limitations regarding computational efficiency, generalizability, and the complexity of the model should be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Contrastive Vision Transformer for Self-Supervised Out-of-Distribution Detection\" presents a novel framework for detecting out-of-distribution (OOD) samples using a Contrastive Vision Transformer (CVT). The main contributions include the integration of a transformer architecture for feature extraction, a dual-encoder system for enhanced representation learning, and a dynamic memory queue for managing negative samples. The model employs a Mahalanobis distance-based scoring function to evaluate OOD detection efficacy. Experimental results demonstrate that the CVT outperforms existing state-of-the-art methods on standard benchmarks such as CIFAR-10 and CIFAR-100, achieving a notable average AUROC improvement of 6.6%.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative use of transformer architectures, which enhances feature learning and robustness in OOD detection tasks. The ensemble feature extraction approach effectively addresses diverse OOD shifts, presenting a compelling solution to this ongoing challenge in the field. Additionally, the comprehensive evaluation across multiple datasets bolsters the validity of the findings. However, the paper's reliance on self-supervised learning may hinder performance in scenarios where labeled data could significantly enhance results. Furthermore, the generalizability of the model is not fully explored, as its application to more complex datasets remains untested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly articulates its methodology and findings. The quality of the experiments and results is high, with thorough ablation studies that affirm the effectiveness of the proposed components. The novelty of integrating transformer architectures in the context of OOD detection is significant, suggesting new avenues for research. However, the reproducibility of results may be a concern, as the paper does not provide extensive details on implementation specifics, which could hinder replication by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a strong contribution to the field of self-supervised OOD detection through the innovative use of contrastive learning and transformer architectures. While the results are promising, further investigation into the model's performance with labeled data and more complex datasets would provide a clearer understanding of its generalizability and robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Contrastive Vision Transformer (CVT), a novel model aimed at improving adversarial training for image classification through an attention-based contrastive learning framework. The CVT integrates vision transformers as feature extractors within a self-supervised paradigm, enhancing the model's ability to differentiate between adversarial and clean samples. Key contributions include the use of contrastive learning for robust feature representation, an ensemble feature extraction module, and comprehensive evaluations on various image datasets, demonstrating significant improvements in adversarial detection metrics over existing methods.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative integration of vision transformers and contrastive learning, which collectively enhance adversarial robustness. The extensive experimental validation, including comparative analyses and ablation studies, effectively underscores the model's performance improvements. However, a notable weakness is the limited dataset scope, which may hinder the generalizability of the findings. Additionally, the complexity of the ensemble feature extraction may pose challenges in computational efficiency for real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and experimental findings, making it accessible to readers. The quality of writing is high, and the experiments are reproducible due to detailed descriptions of the datasets and evaluation metrics. The novelty of the approach, particularly in combining vision transformers with contrastive learning for adversarial robustness, is significant and offers fresh insights into the area of adversarial training.\n\n# Summary Of The Review\nThis paper presents a compelling approach to adversarial training through the Contrastive Vision Transformer model, demonstrating both technical innovation and empirical effectiveness. While the results are promising, further validation across diverse datasets could enhance the robustness of the claims presented.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Contrastive Vision Transformer (CVT), a novel approach for out-of-distribution (OOD) detection in image classification tasks. The authors claim that CVT integrates a Vision Transformer with contrastive learning, significantly enhancing feature extraction capabilities. The paper asserts that this model outperforms existing state-of-the-art methods by over 5% on CIFAR datasets, supported by extensive ablation studies that allegedly validate its superiority.\n\n# Strength And Weaknesses\nWhile the integration of a Vision Transformer into contrastive learning is an interesting approach, the paper's contributions are overstated. The ensemble module is presented as a groundbreaking advancement, but the claims lack substantial evidence comparing it rigorously with existing methods. The ablation studies, although extensive, appear to confirm known parameters rather than provide novel insights. Additionally, the paper's critique of prior work is overly harsh, positioning CVT as the sole solution without adequately acknowledging the ongoing relevance of existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is undermined by its exaggerated claims and lack of balanced perspective on existing research. The novelty of the approach is present but not sufficiently articulated, as the authors tend to dismiss previous methodologies rather than build upon them. Reproducibility may be a concern, given the high-level claims without detailed experimental setups or sufficient comparative analysis of baseline methods.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to OOD detection, but the claims of superiority are inflated and lack rigorous validation. The contribution to the field is notable but not as revolutionary as suggested, and the dismissal of prior work detracts from the paper's credibility.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents the Contrastive Vision Transformer (CVT), a novel model designed to enhance self-supervised out-of-distribution (OOD) detection in image classification tasks. The methodology integrates a vision transformer architecture within a contrastive learning framework and employs an empirical ensemble module to improve performance across various distribution shifts. The findings indicate that the CVT significantly outperforms existing models, achieving an average AUROC of 82.5%, with substantial improvements reported in several benchmark comparisons, particularly between CIFAR-10 and CIFAR-100 datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear articulation of the challenges in OOD detection and the innovative approach of combining vision transformers with contrastive learning. The empirical ensemble module is a noteworthy contribution that effectively balances performance across different OOD shifts. However, the paper could benefit from a deeper exploration of the limitations of the CVT model, especially regarding far-OOD scenarios where ResNet50 exhibits competitive results. Additionally, while ablation studies are conducted, further detail on hyper-parameter tuning and its impact on performance would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a coherent structure that facilitates understanding. The methodology is presented clearly, and the experimental results are backed by sufficient quantitative metrics. The novelty of combining contrastive learning with vision transformers in the context of OOD detection is commendable. However, reproducibility could be improved by providing more explicit details on the experimental setup, including hyper-parameter choices and training protocols, to enable other researchers to replicate the results effectively.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of OOD detection by introducing the CVT model, which demonstrates improved performance on benchmark datasets. While the findings are promising, the paper would benefit from more comprehensive discussions on limitations and reproducibility to solidify its impact in the research community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates out-of-distribution (OOD) detection using a novel approach that combines transformer-based architectures with an ensemble module to balance semantic and covariate shifts. The methodology revolves around leveraging Mahalanobis distance as an OOD score and incorporates self-supervised learning techniques to enhance feature extraction. The findings indicate that this approach outperforms traditional convolutional neural networks (CNNs) on benchmark datasets such as CIFAR-10, CIFAR-100, and SVHN, suggesting improved detection capabilities for distinguishing in-distribution (ID) from OOD samples.\n\n# Strength And Weaknesses\nThe paper exhibits strengths in its exploration of advanced feature extraction methods using transformers and its innovative ensemble design intended to improve OOD detection performance. However, it relies heavily on the closed-world assumption, which may not hold in real-world scenarios where data distribution can vary significantly. The claim that transformer architectures universally outperform CNNs lacks empirical validation in diverse contexts, and the reliance on Mahalanobis distance assumes Gaussian distribution, potentially limiting its effectiveness. Furthermore, the paper does not adequately address the robustness of the proposed methods against adversarial attacks, which raises concerns about their applicability in safety-critical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly, though some assumptions made throughout the methodology could benefit from further clarification or empirical support. The novelty lies in the combination of transformer-based models with ensemble learning for OOD detection, but the practical significance of these contributions may be limited due to the concerns raised about generalizability and robustness. Reproducibility is somewhat compromised as the evaluation metrics focus primarily on AUROC, AUPR, and FPR95, without considering other important aspects such as model calibration or robustness to distributional shifts.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to OOD detection utilizing transformer-based architectures and ensemble methods. However, the reliance on specific assumptions and the lack of consideration for real-world complexities limit the practical implications of the findings. Further empirical validation is necessary to solidify the claims made in the paper.\n\n# Correctness\nRating: 3/5\n\n# Technical Novelty And Significance\nRating: 4/5\n\n# Empirical Novelty And Significance\nRating: 3/5",
    "# Summary Of The Paper\nThis paper introduces the Contrastive Vision Transformer (CVT), a novel model designed for self-supervised out-of-distribution (OOD) detection in image classification tasks. The methodology integrates a contrastive representation learner with a Mahalanobis distance-based OOD score function, leveraging the strengths of vision transformers for enhanced feature extraction. The authors present extensive experimental results demonstrating that CVT significantly outperforms existing state-of-the-art methods on CIFAR-10 and CIFAR-100 datasets, particularly in both near and far OOD scenarios.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to OOD detection using self-supervised learning, which addresses a critical challenge in deploying deep learning models in real-world applications. The integration of an ensemble module to balance semantic and covariate OOD detection is a particularly noteworthy contribution. However, a potential weakness is the limited scope of datasets used for evaluation; although CIFAR datasets are widely recognized, additional testing on more complex datasets could strengthen the paper's claims and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow from the introduction through methodology to the experimental results. The quality of the figures and tables effectively supports the findings. The novelty of the CVT model is compelling, especially in the context of self-supervised learning for OOD detection. However, reproducibility could be enhanced by providing more details on the implementation and hyperparameter settings, which are crucial for others to replicate the results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in self-supervised OOD detection with the introduction of the CVT model, demonstrating clear empirical superiority over existing methods on benchmark datasets. While the contributions are substantial, the scope of evaluation could be broadened to further validate the model's effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to the problem of few-shot learning, introducing a new architecture that leverages meta-learning techniques to enhance model adaptability to unseen tasks. The proposed method integrates a dual-memory mechanism, enabling the model to store and retrieve task-specific information efficiently. The authors report significant performance improvements over baseline models on benchmark few-shot datasets, including Omniglot and Mini-ImageNet, demonstrating the effectiveness of their approach.\n\n# Strength And Weaknesses\n**Strengths:**\n- **Innovation**: The integration of a dual-memory mechanism offers a unique solution to the challenges faced in few-shot learning, contributing fresh ideas to the existing body of research.\n- **Experimental Validation**: The extensive evaluation across multiple datasets, with thorough analysis and comparison against state-of-the-art methods, lends credibility to the authors' claims regarding the efficacy of their approach.\n\n**Weaknesses:**\n- **Implementation Details**: Some methodological aspects are not sufficiently detailed, particularly regarding the architecture's hyperparameters and training procedures, which may limit reproducibility.\n- **Comparative Framework**: While the authors compare their method against several baselines, the inclusion of additional state-of-the-art approaches could provide a clearer context for their results and strengthen the overall argument.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that guides the reader through the methodology and results. However, certain sections could benefit from more detail, particularly in describing the dual-memory mechanism and its implementation. The novelty of the approach is significant, as it presents a new angle on few-shot learning challenges. Reproducibility may be hindered by the lack of specifics on experimental setups and data preprocessing, which are crucial for replicating the results.\n\n# Summary Of The Review\nOverall, the paper presents a promising contribution to the field of few-shot learning with its innovative dual-memory architecture. While the results are compelling, the work would benefit from greater methodological detail and a broader comparative analysis with existing techniques. With these refinements, the paper has the potential to make a substantial impact in the community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Contrastive Vision Transformer (CVT), a novel approach designed for self-supervised out-of-distribution (OOD) detection in image classification tasks. The authors address the challenges of identifying OOD samples resulting from both semantic shifts (far-OOD) and covariate shifts (near-OOD). The methodology integrates a vision transformer architecture within a contrastive learning framework, complemented by an empirical ensemble module that balances the detection of various OOD samples. The results demonstrate that CVT significantly outperforms state-of-the-art methods, achieving notable improvements in Area Under the Receiver Operating Characteristic (AUROC) scores across multiple datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of transformer architectures into the OOD detection framework, which enhances feature extraction capabilities. The empirical ensemble module is a particularly noteworthy contribution, as it effectively addresses the dual challenges of semantic and covariate shifts. However, the paper lacks detailed discussions on the limitations of the proposed approach, such as potential computational overhead or sensitivity to specific types of OOD samples. Additionally, while the results are promising, further evaluation on more complex and diverse datasets could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that facilitates understanding of the problem, methodology, and contributions. The quality of the experiments appears robust, although specifics on reproducibility, such as hyperparameter settings and dataset splits, could be more thoroughly detailed. The novelty of the approach is significant, particularly in its application of contrastive learning within a self-supervised framework for OOD detection, which is a relatively underexplored area.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of OOD detection through the introduction of the Contrastive Vision Transformer, which effectively leverages transformer architectures and contrastive learning. While the results are compelling, further exploration of the method's limitations and validation on more complex datasets would enhance the study's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the Contrastive Vision Transformer (CVT), a novel model designed for self-supervised out-of-distribution (OOD) detection. The methodology integrates an attention-based contrastive learning framework alongside a Mahalanobis distance-based OOD score function to enhance the model's performance in identifying samples that deviate from the training distribution. Empirical results demonstrate that CVT significantly outperforms existing state-of-the-art methods on benchmark datasets such as CIFAR-10 and CIFAR-100, showcasing its effectiveness in addressing both semantic and covariate shifts.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to OOD detection through the use of the CVT architecture, which combines contrastive learning with vision transformer features. The thorough empirical validation, including extensive ablation studies, adds robustness to the findings and demonstrates the superiority of CVT over traditional methods. However, a potential weakness is the limited scope of datasets evaluated, which may restrict the generalizability of the results to more complex or real-world scenarios. Additionally, while the model shows significant improvements, the practical implications and computational efficiency of the approach are not discussed in detail.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the contributions made. The methodology is presented in a coherent manner, making it accessible to readers with a background in deep learning and OOD detection. The novelty of combining contrastive learning with vision transformers is evident, and the empirical results are convincingly presented. However, the reproducibility of the results could be improved by providing more details on the implementation specifics and hyperparameter settings used during the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of self-supervised OOD detection through the introduction of the Contrastive Vision Transformer. While the empirical results are compelling, the generalizability and practicality of the model could benefit from further exploration in diverse datasets and scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the Contrastive Vision Transformer (CVT), a novel model aimed at enhancing Out-of-Distribution (OOD) detection in deep neural networks. The authors propose a contrastive learning framework combined with a Mahalanobis distance-based OOD scoring function, leveraging a vision transformer as the feature extractor. The methodology is evaluated extensively on CIFAR-10 and CIFAR-100 datasets, where the CVT model demonstrates superior performance compared to existing state-of-the-art methods. Key findings include the effectiveness of ensemble features and the impact of hyperparameters like temperature and cluster count on detection performance.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to OOD detection through the integration of contrastive learning and a vision transformer architecture, which appears to yield significant performance improvements over older techniques. The thorough experimental evaluation, including ablation studies, strengthens the claims made about the model's effectiveness and provides valuable insights into the hyperparameter tuning process. However, a notable weakness is the limited diversity of datasets used for testing; further validation on more complex and varied datasets could enhance the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections that guide the reader through the motivation, methodology, experiments, and results. The writing quality is high, with technical terms clearly defined. The novelty of the proposed CVT model is evident, especially in its application of contrastive learning to OOD detection. However, while the methodology is detailed, it would benefit from additional information regarding reproducibility, such as specific implementation details and data preprocessing steps to facilitate future research efforts.\n\n# Summary Of The Review\nOverall, this paper presents a compelling approach to OOD detection, contributing a novel model that addresses critical challenges in the field. The methodology is robust, and the empirical results are promising, although the evaluation could be strengthened with more diverse datasets. The insights provided into hyperparameter tuning are particularly valuable for future research.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents the Contrastive Vision Transformer (CVT), a novel approach for self-supervised out-of-distribution (OOD) detection. By integrating a vision transformer architecture with a contrastive learning paradigm, the CVT aims to effectively identify anomalous samples that diverge from the training data distribution. The methodology includes a representation learner with online and target networks and employs a Mahalanobis distance-based OOD score function. Empirical results demonstrate significant performance improvements over state-of-the-art methods, achieving up to 9.77% better AUROC scores on CIFAR datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative integration of contrastive learning with vision transformers, which provides a fresh perspective on OOD detection. The comprehensive experimental validation on benchmark datasets and the inclusion of ablation studies add robustness to the findings. However, the paper could benefit from a deeper exploration of the limitations of the approach, particularly in scenarios where OOD samples exhibit complex characteristics. Additionally, while the empirical results are strong, comparisons with a broader range of existing methods could further enhance the evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The methodology section is detailed, providing sufficient technical depth for reproducibility. However, certain equations, particularly in the loss function and OOD scoring, could be presented with additional context to aid understanding for readers less familiar with these concepts. Overall, the novelty of the approach and its implications for future research in OOD detection are significant.\n\n# Summary Of The Review\nThe paper introduces a compelling method for self-supervised OOD detection using a contrastive learning framework and a vision transformer architecture, yielding impressive empirical results. While the contributions are noteworthy, the paper would benefit from a more thorough discussion of its limitations and a broader evaluation against existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a new model termed Contrastive Vision Transformer (CVT) aimed at improving out-of-distribution (OOD) detection. The methodology integrates the vision transformer architecture with an empirical ensemble module that seeks to balance semantic and covariate OOD samples. The authors report a marginal performance improvement over state-of-the-art methods, specifically a 5.12% increase in AUROC, with experiments primarily conducted on CIFAR-10 and CIFAR-100 datasets.\n\n# Strength And Weaknesses\nWhile the paper outlines a framework that combines well-known techniques, it largely relies on existing methodologies without offering substantial innovation. The integration of the vision transformer appears to be driven by trends rather than a robust evaluation of its advantages. Furthermore, the empirical ensemble module's contribution to performance enhancement is inadequately justified, raising concerns about its effectiveness. The reported improvements, although statistically significant, may not represent a meaningful advancement given the complexity introduced by the model. Additionally, the focus on specific datasets limits the generalizability of the findings, while the lack of critical discussion regarding the approach's limitations, especially regarding computational demands and real-world applicability, diminishes its overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the superficial explanation of key components, such as the empirical ensemble module and hyper-parameter choices. The novelty is limited, as the proposed model largely reflects a synthesis of existing concepts rather than introducing groundbreaking ideas. The reproducibility of results may be hindered by the arbitrary selection of hyper-parameters and the narrow scope of experimental conditions, which do not sufficiently explore the model's performance across diverse scenarios.\n\n# Summary Of The Review\nOverall, the paper provides a framework for OOD detection that lacks substantial novelty and depth in its contributions. Despite reporting slight performance improvements, the reliance on existing methods, limited dataset variety, and insufficient discussion of practical limitations hinder its significance in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents the Contrastive Vision Transformer (CVT), which integrates vision transformer architecture with a contrastive learning framework specifically designed for self-supervised out-of-distribution (OOD) detection in image classification tasks. Key contributions include the use of an attention-based architecture that enhances feature extraction, an ensemble module that balances features from both encoder and predictor, and a Mahalanobis distance-based score function to effectively distinguish OOD samples. The findings demonstrate that CVT outperforms existing state-of-the-art methods, achieving significant AUROC improvements on benchmark datasets such as CIFAR-10 and CIFAR-100.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to OOD detection, particularly through the integration of attention mechanisms and ensemble strategies that enhance performance. The empirical results are compelling, showcasing the model's superiority over existing methods and its robust performance across various datasets. However, potential weaknesses include the lack of exploration into the computational efficiency of the CVT model and its applicability to more complex or real-world datasets, which could limit its broader relevance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers. The quality of the presented results is high, and the novel contributions are clearly outlined. While the reproducibility of the results is supported by extensive benchmarking and ablation studies, additional details on implementation specifics would enhance confidence in replicating the findings.\n\n# Summary Of The Review\nOverall, the Contrastive Vision Transformer represents a significant advancement in self-supervised OOD detection, demonstrating both innovative methodologies and exceptional empirical performance. While the paper is well-written and clear, further exploration of computational aspects and real-world applicability would strengthen its contributions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach for out-of-distribution (OOD) detection through a Contrastive Vision Transformer (CVT) model, which leverages contrastive learning principles for enhanced feature representation. It addresses the theoretical limitations of traditional deep neural networks, emphasizing the need for robust OOD detection mechanisms in safety-critical applications. The methodology includes a well-defined architectural design incorporating an ensemble module and a carefully crafted InfoNCE loss function to maximize mutual information among representations. The results demonstrate the effectiveness of the CVT in distinguishing between in-distribution and various types of OOD samples, showcasing significant improvements in performance metrics such as AUROC and AUPR.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its theoretical framework, which provides a solid foundation for the proposed model and its components. The incorporation of contrastive learning and ensemble methods enhances the ability to capture complex distributional shifts, addressing both semantic and covariate OOD detection challenges. However, a potential weakness is the lack of extensive empirical validation across diverse datasets, which may limit the generalizability of the findings. Additionally, while the theoretical implications are well-articulated, the practical implementation details could benefit from further clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, presenting a logical flow from theoretical concepts to empirical findings. The quality of writing is high, although certain sections, particularly those detailing the model architecture and loss function design, could be more explicit for reproducibility. The novelty of the Contrastive Vision Transformer and its application to OOD detection is significant, as it builds on existing contrastive learning paradigms while addressing a critical gap in the domain. However, the reproducibility of results may be hindered by insufficient detail regarding training procedures and hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper offers a promising advancement in self-supervised OOD detection through its innovative use of contrastive learning within a transformer framework. While the theoretical contributions are robust and the empirical results compelling, further clarity regarding implementation and broader validation across diverse datasets would enhance the paper’s impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents the Contrastive Vision Transformer (CVT), a novel model designed for self-supervised out-of-distribution (OOD) detection in image classification tasks. The methodology integrates a vision transformer as a feature extractor within a contrastive learning framework, employing an empirical ensemble module to balance the representation of semantic and covariate OOD samples. The findings demonstrate significant performance improvements, achieving a 5.12% AUROC enhancement on CIFAR-10 (ID) vs. CIFAR-100 (OOD) and a 9.77% AUROC improvement on CIFAR-100 (ID) vs. CIFAR-10 (OOD), indicating the model's effectiveness in distinguishing between in-distribution and out-of-distribution samples.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to OOD detection using a vision transformer, which has shown promising results compared to existing methods. The introduction of an ensemble module and the utilization of contrastive learning techniques contribute to the model's robustness and performance. However, the paper lacks a broader discussion on the implications of the findings and future directions, which may limit its impact on the field. Additionally, while the empirical results are strong, the absence of a publicly available code repository may hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents the methodology, experimental setup, and results. However, the clarity could be improved with more explicit explanations of the architecture and how each component contributes to the overall model performance. The novelty of the approach, particularly the combination of vision transformers with contrastive learning for OOD detection, is noteworthy. Nevertheless, the lack of a public code repository raises concerns about reproducibility, as specific implementation details are provided, but accessibility is limited.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling approach to OOD detection through the Contrastive Vision Transformer, demonstrating significant empirical results. While the contributions are noteworthy, the lack of broader implications and a public code repository may limit the paper's impact and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the CVT model, which aims to enhance out-of-distribution (OOD) detection by leveraging vision transformers for feature extraction and proposing an ensemble module. The authors claim that their model outperforms state-of-the-art methods as measured by AUROC scores on CIFAR datasets. However, the benchmarks used for comparison include models trained under differing regimes, raising concerns about the validity of the performance claims. Additionally, the paper discusses the integration of Mahalanobis distance for OOD detection and claims to conduct extensive ablation studies.\n\n# Strength And Weaknesses\nWhile the paper presents interesting ideas, it has several weaknesses that undermine its contributions. The authors highlight limitations of existing methods but fail to acknowledge improvements made to these methods over time, potentially downplaying the novelty of their approach. The paper does not provide a thorough analysis of the shortcomings of CNN-based methods, which would have clarified the advantages of their proposed solution. Claims regarding the innovation of the ensemble module and empirical feature extraction lack originality, as similar methods have been explored in prior research. Additionally, the performance metrics presented are standard, and the lack of context regarding baseline scores and comparisons to alternative evaluation strategies limits the effectiveness of their claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but the clarity of claims regarding novelty is compromised by a lack of thorough comparative analysis with existing literature. While the methodology appears sound, the absence of fair comparisons with similar studies raises concerns about reproducibility and the uniqueness of the proposed contributions. The authors should strengthen their discussions on hyperparameters and the implications of their findings in the context of existing work.\n\n# Summary Of The Review\nOverall, while the CVT model proposes interesting concepts in OOD detection, the paper suffers from issues related to the validity of performance claims, lack of thorough comparative analysis, and insufficient acknowledgment of existing literature. These factors obscure the true contributions of the research and call into question the novelty of the proposed methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach termed the Contrastive Vision Transformer (CVT) for self-supervised out-of-distribution (OOD) detection. The methodology leverages contrastive learning principles to enhance feature representation, allowing for improved detection of OOD samples. Key findings indicate that the proposed CVT model significantly outperforms existing state-of-the-art methods by an average of 5.12% in terms of AUROC across various datasets, demonstrating its effectiveness in real-world applications.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative application of contrastive learning in the context of OOD detection and the thorough empirical evaluation across multiple datasets. The results are compelling and provide a strong argument for the practical benefits of the proposed model. However, weaknesses include some inconsistencies in terminology and formatting throughout the document, which may hinder readability. Additionally, the methodology section could benefit from clearer definitions of key terms to ensure understanding for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but clarity is occasionally compromised by inconsistent terminology and some minor grammatical errors. The quality of the empirical results is high, showcasing robust performance metrics. In terms of novelty, the application of a contrastive vision transformer specifically for self-supervised OOD detection represents a significant advancement in the field. The reproducibility of the results could be improved through clearer descriptions of the methodology and evaluation metrics.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach to self-supervised OOD detection with the CVT model, supported by strong empirical results. While the contributions are substantial, attention to detail in terminology and clarity would enhance the overall readability and impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to Out-of-Distribution (OOD) detection using a vision transformer model, referred to as CVT. It focuses on self-supervised learning, utilizing image datasets such as CIFAR-10 and CIFAR-100 for empirical evaluation. The main contributions include the introduction of the CVT model, which integrates an ensemble module to balance feature extraction, and achieves competitive performance metrics, primarily measured by AUROC. However, the scope of the evaluation is limited to basic image datasets, which may not translate effectively to more complex real-world scenarios.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative use of vision transformers and the ensemble approach to enhance feature representation. However, significant weaknesses include a narrow focus on CIFAR datasets, a lack of exploration into alternative architectures, and a limited discussion on the implications of hyperparameter settings. Furthermore, the evaluation is primarily centered around AUROC metrics, neglecting other important performance measures such as precision and recall. The potential for generating synthetic OOD samples and the exploration of the model's interpretability and computational efficiency are also overlooked.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, with a coherent structure outlining the methodology and results. However, the novelty is somewhat diminished by the lack of extensive comparisons with alternative models and architectures. Reproducibility may be a concern as the paper does not provide sufficient details on hyperparameter settings or the computational resources used, which are critical for replication of the results. \n\n# Summary Of The Review\nThe paper presents a promising approach to OOD detection using a vision transformer, but it is hampered by limitations in data diversity, performance evaluation metrics, and discussions on model interpretability. Future work should address these gaps to enhance the model's applicability and robustness in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents the CVT (Cluster-based Variational Transformer) model, a two-stage architecture designed for out-of-distribution (OOD) detection. The methodology involves extracting latent features followed by computing an OOD score using Mahalanobis distance, underpinned by an InfoNCE loss function. Key findings indicate that CVT outperforms state-of-the-art OOD detection methods, achieving improvements of 5.12% AUROC on CIFAR-10 vs. CIFAR-100 and 9.77% AUROC on CIFAR-100 vs. CIFAR-10. The paper also includes extensive benchmarking, ablation studies, and encoder comparisons to validate the model's effectiveness.\n\n# Strength And Weaknesses\nStrengths of the paper include a robust methodological framework and thorough statistical analysis, which provide a clear basis for the improvements claimed. The use of multiple performance metrics (AUROC, AUPR, FPR95) is commendable, allowing for a comprehensive evaluation of the model's capabilities. However, weaknesses lie in the somewhat limited exploration of the model's performance across more complex datasets and the reliance on specific hyperparameter settings, which may affect generalizability. Furthermore, while the paper suggests areas for future work, more detailed plans could enhance the impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its contributions, making it accessible to readers. The quality of the writing and the clarity of the figures and tables enhance understanding. The novelty of the CVT model is noteworthy, particularly in its application of statistical methods for OOD detection. Reproducibility is supported by the detailed description of the methodology and experiments, including the use of repeated trials to ensure robustness of results. However, the paper could benefit from more explicit sharing of code and datasets to facilitate independent validation of results.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of OOD detection by introducing a novel model with statistically validated improvements over existing approaches. While the methodology is sound and the results promising, further exploration of its application to more complex datasets and explicit reproducibility measures would strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a self-supervised learning method for out-of-distribution (OOD) detection, leveraging well-known datasets such as CIFAR-10, CIFAR-100, and SVHN. The proposed approach employs Mahalanobis distance-based OOD scoring and introduces an ensemble module aimed at balancing performance across different OOD samples. The findings indicate that the method outperforms existing benchmarks on the selected datasets, although the generalizability to more complex and diverse datasets remains untested.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its novel approach to self-supervised learning for OOD detection and its performance against existing methods on benchmark datasets. However, the reliance on well-established datasets limits the scope of the findings, as they may not translate to real-world applications. Additionally, the method's complete reliance on unlabeled data potentially restricts the discriminative power of the learned features. The analysis lacks depth regarding various types of OOD samples and extreme cases, which could provide a more comprehensive understanding of the method's efficacy. Moreover, the hyperparameter sensitivities and the unaddressed issues of computational efficiency and scalability present significant weaknesses.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly and logically, providing a coherent narrative around the proposed method. The quality of the experiments appears solid, although limited by the datasets used. While the novelty of the self-supervised approach is commendable, the findings would benefit from more rigorous exploration across diverse datasets and situations. Reproducibility may be hindered by the lack of detailed hyperparameter tuning strategies and the absence of computational efficiency evaluations.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of OOD detection through a self-supervised approach, yet it is constrained by its reliance on established datasets and a lack of comprehensive evaluations. Future work should address the limitations regarding generalizability and provide a clearer roadmap for extending the proposed model.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Contrastive Vision Transformer for Self-Supervised Out-of-Distribution Detection\" proposes a framework that leverages contrastive learning techniques within a Vision Transformer architecture to enhance out-of-distribution (OOD) detection. The authors aim to address challenges associated with different types of sample shifts that can lead to OOD instances. Through a two-stage OOD detection methodology, the proposed model incorporates additional modules designed to mitigate collapsed solutions in self-supervised learning. Empirical results demonstrate improvements over existing state-of-the-art methods, albeit by modest margins, while the paper also includes extensive ablation studies on hyper-parameters and evaluation metrics like AUROC and AUPR.\n\n# Strength And Weaknesses\nThe strengths of the paper include a well-structured approach to OOD detection and a clear presentation of empirical results that show the model's competitive performance. However, the weaknesses are pronounced; the contributions appear to lack novelty, as the employed techniques and challenges addressed are well-trodden paths in the literature. Additionally, the claims of innovation in methodology feel overstated, as many components discussed are standard practices in the field. The overall impression is that while the paper is competent, it does not deliver significant advancements or insights.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly, with a logical flow and organized sections that facilitate understanding. However, the quality of novelty is underwhelming, as many concepts discussed have been previously explored extensively in the literature. The reproducibility of results seems feasible given the described methodology, but the lack of significant innovation raises concerns about the necessity of the work.\n\n# Summary Of The Review\nOverall, the paper presents a competent yet unremarkable contribution to the field of OOD detection using contrastive learning within a Vision Transformer framework. While the methodology is solid, the lack of novel insights and the reliance on established concepts diminish its impact. The paper may appeal to newcomers but does not offer substantial advancements for seasoned researchers.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents the Contrastive Vision Transformer (CVT) model, which leverages vision transformers for out-of-distribution (OOD) detection, demonstrating superior performance compared to traditional CNN architectures. The methodology incorporates an ensemble module to enhance robustness and utilizes Mahalanobis distance for OOD scoring. The empirical findings indicate that CVT achieves significant improvements over state-of-the-art methods, with increases of 5.12% and 9.77% in AUROC scores. The study highlights the importance of hyper-parameters in contrastive learning and suggests potential extensions for multi-task learning and integration into self-supervised tasks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative use of vision transformers and the introduction of an ensemble module, which is a noteworthy contribution to OOD detection methods. The empirical results strongly support the effectiveness of contrastive learning, and the exploration of hyper-parameter impacts presents a valuable avenue for further research. However, the limitations of the datasets used, which may not represent complex real-world scenarios, could affect the generalizability of the findings. Additionally, the exploration of alternative distance metrics could enhance the understanding of feature distribution in OOD detection.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and findings. The quality of the empirical results is commendable, demonstrating significant advancements in OOD detection. The novelty is notable, particularly in the context of employing contrastive learning with vision transformers. However, some aspects, such as the ablation studies, could benefit from broader exploration of architectures and training techniques to improve reproducibility and understanding of the model's performance.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to OOD detection through the introduction of the CVT model, which effectively utilizes vision transformers and contrastive learning. While the results are impressive, the generalizability of the findings may be limited by the datasets used, and there are opportunities for further exploration of model components and alternative metrics.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the Contrastive Vision Transformer (CVT), a novel model designed for out-of-distribution (OOD) detection tasks. The authors demonstrate that CVT significantly outperforms state-of-the-art methods on benchmark datasets, specifically achieving an AUROC score of 79.37% on the CIFAR-10 (ID) vs. CIFAR-100 (OOD) task, which is nearly 10% higher than the next best model. Through extensive benchmarking across various datasets and models, including PixelCNN++, Deep-SVDD, and SSD, the CVT displays robust performance, especially in far-OOD detection scenarios, with an average AUROC of 80.34%. The paper emphasizes the effectiveness of the ensemble module and the choice of encoder architectures, showing that attention-based models like ViT and Swin outperform traditional CNNs.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its strong empirical results and the comprehensive evaluation across multiple ID/OOD datasets, which demonstrate the robustness and versatility of the CVT model. The use of an ensemble module for feature extraction and the comparative analysis of different encoder architectures add depth to the findings. However, the paper could benefit from a more detailed discussion on the implications of these results and potential limitations of the CVT model. Additionally, while the performance metrics are impressive, further investigation into the computational efficiency and scalability of the model would enhance the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents its methodology and findings, making it accessible to a broad audience in the field. The quality of the experimental design is high, with appropriate benchmarks and strong comparative analyses. The novelty of the CVT model is evident in its architecture and approach to OOD detection, though the paper could further emphasize how it distinguishes itself from existing methods. Reproducibility is supported by the detailed reporting of results; however, providing access to the code and datasets used for experimentation would bolster the reproducibility of the findings.\n\n# Summary Of The Review\nOverall, the CVT paper makes a significant contribution to the field of self-supervised OOD detection, showcasing impressive performance improvements over existing models through a well-structured approach. While the empirical results are strong and the methodology is clear, further exploration of the model's limitations and computational aspects would be beneficial.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach for improving out-of-distribution (OOD) detection in machine learning models, particularly focusing on the challenges posed by covariate and semantic shifts. The authors introduce a new framework that integrates self-supervised learning with traditional OOD detection techniques, demonstrating its effectiveness through a series of experiments. Key findings indicate that the proposed model significantly outperforms existing methods in terms of detection accuracy, particularly in scenarios characterized by complex shifts.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative methodology and comprehensive experimental evaluation. The integration of self-supervised learning is a notable contribution, as it addresses a critical gap in the literature regarding OOD detection. However, the paper also has weaknesses, including a lack of clarity in some sections due to complex sentence structures and insufficient definitions of key terms. Additionally, the redundancy in certain discussions detracts from the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel approach, the clarity is hindered by overly long sentences and inconsistent terminology. The quality of the writing could be improved with more concise paragraphs and clearer transitions between sections. As for reproducibility, the methods are described adequately, but a summary table of experimental results could enhance clarity and allow for easier comparison. The use of visual aids is commendable, but clearer captions and references in the text are needed.\n\n# Summary Of The Review\nIn summary, the paper introduces a significant advancement in OOD detection through a self-supervised learning framework, yet it suffers from clarity issues and redundancies that could be improved. Addressing these concerns would enhance its overall readability and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3997240935257365,
    -1.6453240137609435,
    -1.596380252775938,
    -1.4946620540809117,
    -1.6610408193325843,
    -1.5677091313027973,
    -1.6678430957282477,
    -1.9325962281143438,
    -1.6249745376489688,
    -1.633133757561545,
    -1.5723437513385767,
    -1.4322586650950766,
    -1.5339881438188703,
    -1.403878996643389,
    -1.5068462465672174,
    -1.6175901430437172,
    -1.9537302307142663,
    -1.6097590052108945,
    -1.6785575164479358,
    -1.5701061099324525,
    -2.030693603416639,
    -1.473763872188179,
    -1.678399044829668,
    -1.6402859735880253,
    -1.7532548423278125,
    -1.8189075804955877,
    -1.7142533432838134,
    -1.639076652779911,
    -1.6291650198724608
  ],
  "logp_cond": [
    [
      0.0,
      -2.099544804110033,
      -2.116313192796743,
      -2.130749832050569,
      -2.1147450751027446,
      -2.150316274976204,
      -2.1639756415192224,
      -2.144057481624709,
      -2.1286172122740044,
      -2.129131401232781,
      -2.1064576034581286,
      -2.1684527605216,
      -2.104573679581359,
      -2.131425009978341,
      -2.149253589231901,
      -2.1498409528812403,
      -2.1282362445114,
      -2.1343432874563457,
      -2.138770793984595,
      -2.113760644225289,
      -2.1506117385518655,
      -2.1589710289350843,
      -2.1540223402359,
      -2.162170632686801,
      -2.1517188672399405,
      -2.1464228892834405,
      -2.1543290345009765,
      -2.1459935590684904,
      -2.149828182942127
    ],
    [
      -1.277100604882833,
      0.0,
      -1.1652949566973445,
      -1.1872753159351612,
      -1.1246433944037382,
      -1.2261854282363291,
      -1.263258355370457,
      -1.2038558260679282,
      -1.1728359360362999,
      -1.2246273794847977,
      -1.1393826411614583,
      -1.348282725026255,
      -1.1877064565350823,
      -1.181143900628478,
      -1.234566201203751,
      -1.1658818666951678,
      -1.130436304379872,
      -1.2134723871898667,
      -1.162653813814554,
      -1.0763457025153265,
      -1.2253348695904676,
      -1.2983872837752264,
      -1.268170812796459,
      -1.2427515271278675,
      -1.26301500814135,
      -1.2359984994457378,
      -1.2277558737072518,
      -1.2660672228390568,
      -1.262149343888247
    ],
    [
      -1.3086710575401406,
      -1.2387813241556882,
      0.0,
      -1.2196755501239576,
      -1.138071174464759,
      -1.1747510452809122,
      -1.306103426816479,
      -1.2816975547456089,
      -1.1916145374779676,
      -1.2450913247759285,
      -1.2089474593649663,
      -1.342479120923235,
      -1.1633095076862985,
      -1.2551509235119098,
      -1.313436604512856,
      -1.2147088100888164,
      -1.2337030873285404,
      -1.1681993093019605,
      -1.2706624923849736,
      -1.1969771681878496,
      -1.2523457437652168,
      -1.2331299980887813,
      -1.2517005430269894,
      -1.3261488008380085,
      -1.231085661768413,
      -1.2350930151717063,
      -1.2188390408233707,
      -1.2316943275875532,
      -1.3168528768024483
    ],
    [
      -1.2101315946583429,
      -1.1106279824516205,
      -1.04851008704191,
      0.0,
      -1.1113988853008339,
      -1.1116054280854817,
      -1.1654847184714698,
      -1.1062673059819697,
      -1.0733895777938156,
      -1.1067720070351978,
      -1.1419272451703522,
      -1.216977568364626,
      -1.0766311937448247,
      -1.166485718399941,
      -1.182943605921548,
      -1.1474343064187646,
      -1.0599788524934493,
      -1.1553604623705735,
      -1.114620948123639,
      -1.054075201839344,
      -1.137030764818223,
      -1.1234316716459656,
      -1.1441446209505999,
      -1.1584297519236506,
      -1.1981419863271834,
      -1.1277674900728798,
      -1.0981183456123398,
      -1.1208294799645355,
      -1.1736423504238351
    ],
    [
      -1.3533834540273975,
      -1.1919310994874601,
      -1.1614060477976567,
      -1.2866477960261724,
      0.0,
      -1.2911789025646392,
      -1.295258192101352,
      -1.2869740965243623,
      -1.3028180668249167,
      -1.2734352160156943,
      -1.2036477830876677,
      -1.409505971666746,
      -1.251104613365016,
      -1.2871531200415367,
      -1.298933911641778,
      -1.300277893244581,
      -1.2169893803647291,
      -1.287281110513036,
      -1.289459839236832,
      -1.2078049619899889,
      -1.2742391576514396,
      -1.343439939161206,
      -1.2932617175773868,
      -1.3419378653160612,
      -1.2835260103055737,
      -1.3054482148381108,
      -1.2663195136582308,
      -1.3360765822569582,
      -1.3830605811988448
    ],
    [
      -1.3014217171109146,
      -1.235930338316106,
      -1.1907292113926267,
      -1.2445800419248692,
      -1.1706638125307065,
      0.0,
      -1.2591511148830417,
      -1.2399387407106168,
      -1.2008277529318416,
      -1.2760867654019834,
      -1.2148422876826972,
      -1.340232942608809,
      -1.207290374737019,
      -1.2101752178799592,
      -1.243402204427782,
      -1.216559465488242,
      -1.229799936595644,
      -1.1887804033893645,
      -1.2172532240588303,
      -1.2082667709457566,
      -1.2249102598314663,
      -1.2540965577447452,
      -1.2251151006228083,
      -1.2860194747901401,
      -1.2425235840670141,
      -1.229243132680399,
      -1.2311823089838634,
      -1.2315298005149558,
      -1.321077957314401
    ],
    [
      -1.2998678251408937,
      -1.233188513941316,
      -1.2113520353738565,
      -1.221641013571155,
      -1.2103849409440695,
      -1.2539261938687714,
      0.0,
      -1.22560820803144,
      -1.2501907016714318,
      -1.2947947500634216,
      -1.232127093195734,
      -1.3176690547525014,
      -1.247033089320808,
      -1.2630202154525108,
      -1.1670641645196964,
      -1.244945896055901,
      -1.2472858176027983,
      -1.25794430487322,
      -1.2351760995368446,
      -1.239086030810399,
      -1.2348423901097112,
      -1.2841271484590147,
      -1.2321543038088385,
      -1.284029225493238,
      -1.3229487342105757,
      -1.2664998770682805,
      -1.2440273581077348,
      -1.240326947390305,
      -1.3046646782923896
    ],
    [
      -1.6046342377215612,
      -1.5062135826465752,
      -1.4631504134910867,
      -1.4548480879262697,
      -1.4792627055260579,
      -1.503040986016895,
      -1.523990627047934,
      0.0,
      -1.4924614355064885,
      -1.5374184941535807,
      -1.508756255682456,
      -1.6380897667991647,
      -1.4443093960283062,
      -1.5475076794891067,
      -1.537521002086542,
      -1.532759525264791,
      -1.517721062414898,
      -1.4794805774321682,
      -1.5507907289230463,
      -1.5209110397001326,
      -1.4399093881525329,
      -1.5425533908157323,
      -1.4993275503531562,
      -1.6066961988787745,
      -1.569167889551165,
      -1.499035170635217,
      -1.4660252133654277,
      -1.5310505823338871,
      -1.6137103295421549
    ],
    [
      -1.301078123448235,
      -1.160064416549334,
      -1.2075260287125569,
      -1.2279330536248203,
      -1.1762695849327756,
      -1.1881996495215263,
      -1.301686498254051,
      -1.2370794909765832,
      0.0,
      -1.2907355138215588,
      -1.1812292809597333,
      -1.3689645404106523,
      -1.143153024702224,
      -1.2505449495512864,
      -1.2904101413221771,
      -1.2684962464521092,
      -1.1370036549642641,
      -1.2252083738791502,
      -1.2561134799771532,
      -1.0921261753028613,
      -1.2239463446427101,
      -1.295194173972575,
      -1.253982525483187,
      -1.2612421844589572,
      -1.2724546431071329,
      -1.2377207419552605,
      -1.2487592600981918,
      -1.2265582508487975,
      -1.3481173766386565
    ],
    [
      -1.3269767968351966,
      -1.3106265919899467,
      -1.2954687445975848,
      -1.3223340109806427,
      -1.2969996426337664,
      -1.363472027161745,
      -1.3869898218769001,
      -1.3313128194950097,
      -1.351313492932179,
      0.0,
      -1.30784097335599,
      -1.4130534677009636,
      -1.3334482698717096,
      -1.344928251433813,
      -1.380556211186494,
      -1.3842580705611065,
      -1.31206836593376,
      -1.3565632713294995,
      -1.3382831664188086,
      -1.3027113226511207,
      -1.303146035227865,
      -1.3916919889657098,
      -1.3548397525486648,
      -1.347850376109089,
      -1.2994842878980832,
      -1.3669621538875747,
      -1.3001113092860175,
      -1.3663416477908181,
      -1.3817595684010484
    ],
    [
      -1.184237048688859,
      -0.9873727438214529,
      -1.070763426639344,
      -1.1101752462435521,
      -0.9713341633111789,
      -1.099067357350637,
      -1.1497668745104586,
      -1.139256032624183,
      -1.0698188936297786,
      -1.1318847620427364,
      0.0,
      -1.2405060286666654,
      -1.0619202707987583,
      -1.0295986078404302,
      -1.1242365073867948,
      -1.0840833386311655,
      -1.0689471832661175,
      -1.0584332160358394,
      -1.0717620674745283,
      -0.9964330330506005,
      -1.0845038828199849,
      -1.155820649876184,
      -1.1173689157445237,
      -1.1806661299801677,
      -1.1546370379070665,
      -1.2056015622016367,
      -1.0994195166987981,
      -1.1017319555135907,
      -1.1848645936057698
    ],
    [
      -1.1868300005891141,
      -1.1591954673554008,
      -1.1443253389749177,
      -1.1556870518087268,
      -1.1706127965584778,
      -1.163685931316823,
      -1.1606098799558469,
      -1.1453975734339321,
      -1.1663883871270961,
      -1.1587451374130118,
      -1.1646970475654346,
      0.0,
      -1.1765917395160832,
      -1.1497762545597034,
      -1.1511716621434234,
      -1.1291012151300948,
      -1.155724425137805,
      -1.1666081443844245,
      -1.1466811742414438,
      -1.1686378134047353,
      -1.141185738701321,
      -1.1729402860147993,
      -1.1421989906020127,
      -1.1582860492349165,
      -1.1509640789229019,
      -1.1905359595992373,
      -1.1810403295707907,
      -1.1564462317091928,
      -1.1512834976140691
    ],
    [
      -1.2251862142506744,
      -1.081402994327428,
      -1.0570034214357509,
      -1.0479273216222296,
      -1.060849313647945,
      -1.129748837899123,
      -1.1866884080714764,
      -1.1294188349477954,
      -1.0665792824544515,
      -1.1589630080991138,
      -1.0976664186011635,
      -1.2641503942518484,
      0.0,
      -1.1489108029518122,
      -1.2018835062828115,
      -1.1590516343866641,
      -1.1040499075273182,
      -1.1211622932901675,
      -1.1408852969134673,
      -1.0411679956220805,
      -1.1734058891891155,
      -1.1604707488524815,
      -1.181803350994407,
      -1.2167875411705986,
      -1.1911186093246595,
      -1.1172392908839262,
      -1.112007693358635,
      -1.1585991106296043,
      -1.2388142078618305
    ],
    [
      -1.0833781147739856,
      -0.9639050690527444,
      -0.9490294920249405,
      -1.0162437201123018,
      -0.9347696536002069,
      -0.9657325515272875,
      -1.0415196102198803,
      -1.071951846486118,
      -0.9794541441122537,
      -1.0236269369057653,
      -0.9127147887558832,
      -1.0922104980643381,
      -0.9937565796526808,
      0.0,
      -0.9984504177701172,
      -0.9406760079829056,
      -1.0032782782394167,
      -0.9493869985113103,
      -0.9865542463643591,
      -0.9438964030585081,
      -1.0265131122707858,
      -1.0316576846383099,
      -1.0310489038431823,
      -1.0200194318902849,
      -1.0332139897525614,
      -1.0188340964067137,
      -1.0073147094363553,
      -0.9962481207439611,
      -1.0723004287466986
    ],
    [
      -1.201662471206995,
      -1.1157862784009183,
      -1.1158171998476838,
      -1.1537970725768774,
      -1.1058587573772884,
      -1.122624215216502,
      -1.1311217165779683,
      -1.1383125823391806,
      -1.1566169055285933,
      -1.1751028569239554,
      -1.0730026851069492,
      -1.2232815465579436,
      -1.1500085431562523,
      -1.0935030868129947,
      0.0,
      -1.0893187682794927,
      -1.1314778104869483,
      -1.120968234003672,
      -1.14792497105879,
      -1.166733106546813,
      -1.0805728763997553,
      -1.1849876203662413,
      -1.1323051705895726,
      -1.0981174893426515,
      -1.1422234746953575,
      -1.1703430350901456,
      -1.129225812923328,
      -1.11423108915077,
      -1.2129458204121693
    ],
    [
      -1.300490541976694,
      -1.1673273994938802,
      -1.1812785539059167,
      -1.2091417758144334,
      -1.164238594043512,
      -1.2092184234139707,
      -1.2620368878755293,
      -1.2551388165163455,
      -1.1809993318212124,
      -1.297060505290885,
      -1.1308103280839983,
      -1.2972710657345508,
      -1.2058735945127539,
      -1.1652145147313502,
      -1.2146544930760466,
      0.0,
      -1.2309072985660723,
      -1.1851870396804531,
      -1.2203828716386074,
      -1.1509983802986623,
      -1.2396489902702366,
      -1.2058040267802854,
      -1.2631781284711496,
      -1.2245006895761192,
      -1.2725722365450738,
      -1.235333743927522,
      -1.1775635316416444,
      -1.2171072483522773,
      -1.3101632860385217
    ],
    [
      -1.584582300335559,
      -1.4455006395205086,
      -1.5051367792880923,
      -1.5468253091336839,
      -1.4457235493251326,
      -1.5948177646024868,
      -1.6090628179664987,
      -1.5618920963354015,
      -1.4641541459852974,
      -1.551890001322618,
      -1.5210638712416047,
      -1.6368530161961292,
      -1.4994512620902734,
      -1.5575148977116755,
      -1.6033745304864333,
      -1.5852152676078988,
      0.0,
      -1.5707392140807621,
      -1.5274113559724285,
      -1.4244697226969762,
      -1.5242800290562355,
      -1.627743671407617,
      -1.5597405964578412,
      -1.5950158112232595,
      -1.5964365899626325,
      -1.5963527917283984,
      -1.5473401707198096,
      -1.547812379091706,
      -1.598699661112235
    ],
    [
      -1.2559329566582058,
      -1.1938254185375978,
      -1.1129373307705641,
      -1.1880002030361083,
      -1.127181382636559,
      -1.1121570149034548,
      -1.266752982495887,
      -1.1754072264186815,
      -1.101209378453764,
      -1.251056714211506,
      -1.0949202493118062,
      -1.317543108034649,
      -1.147184073936494,
      -1.127771075513028,
      -1.2116984010507181,
      -1.162087529422441,
      -1.2018811940235372,
      0.0,
      -1.207565967447727,
      -1.117311517118338,
      -1.1953039238208996,
      -1.2228987928895443,
      -1.2041917118163874,
      -1.2109194137629622,
      -1.259552723739937,
      -1.2040975978762074,
      -1.1418656225560921,
      -1.1160368494480761,
      -1.3196454944106026
    ],
    [
      -1.4049013320815817,
      -1.296294218430909,
      -1.2976103393987486,
      -1.3505969259302644,
      -1.243536885219317,
      -1.2925098102084238,
      -1.3458542351977498,
      -1.3498204694261384,
      -1.323261772004597,
      -1.3385608639320123,
      -1.275580216047888,
      -1.3768457486914756,
      -1.2896141877205525,
      -1.3165879740224014,
      -1.3490175918043714,
      -1.3375956260976216,
      -1.3114826261001251,
      -1.3145469763972268,
      0.0,
      -1.277286246654952,
      -1.3445885421195616,
      -1.3101807071194829,
      -1.3367394341084138,
      -1.3752784584622637,
      -1.3253843584958076,
      -1.3373584551861217,
      -1.3462988614998328,
      -1.331401683626403,
      -1.3980394876856834
    ],
    [
      -1.2522261524082832,
      -1.0569401937928473,
      -1.1068458848783542,
      -1.1420976471109487,
      -1.0507451830638073,
      -1.165824757439964,
      -1.1774740666498507,
      -1.1995280373260544,
      -1.0803728690914354,
      -1.2041250968520165,
      -1.053620634763783,
      -1.3004901147363956,
      -1.0962291556189434,
      -1.1571930668734156,
      -1.2020631055520965,
      -1.155123831778433,
      -1.0776069025251944,
      -1.1300388458415351,
      -1.1406052354998248,
      0.0,
      -1.200791036114495,
      -1.2325907283034372,
      -1.1699475834376538,
      -1.1855225100756948,
      -1.2349843608940705,
      -1.2313086042135732,
      -1.1160953175068349,
      -1.131004610102945,
      -1.2536761147129962
    ],
    [
      -1.7908557531464089,
      -1.7134887848089424,
      -1.6999812355383133,
      -1.7769085725105649,
      -1.6701054865965463,
      -1.7195552803151475,
      -1.7444584966603183,
      -1.7294966524997202,
      -1.7074265198121088,
      -1.723588732545915,
      -1.6818167863881128,
      -1.7967816834590349,
      -1.7496584408549352,
      -1.7442630696968786,
      -1.6889504578439452,
      -1.7353634835341525,
      -1.7118800797863747,
      -1.7016323113554448,
      -1.7525956117088646,
      -1.7261652850215266,
      0.0,
      -1.8322493173334757,
      -1.6883888186968892,
      -1.7279209666294841,
      -1.7148099127453282,
      -1.7456960323823527,
      -1.6616724928805269,
      -1.7238652146136184,
      -1.7837248354912532
    ],
    [
      -1.1804271931999601,
      -1.1137984797595084,
      -1.1378674004038554,
      -1.1086265649184712,
      -1.1239946434004124,
      -1.1186015729116996,
      -1.1557683870662638,
      -1.0937263289235901,
      -1.1030727811631098,
      -1.131314330773026,
      -1.0885554153846657,
      -1.1493889863267734,
      -1.066070850444647,
      -1.0995850879243119,
      -1.1416205973523705,
      -1.0923130502599534,
      -1.1068121453632653,
      -1.1063478631536932,
      -1.0615459068385669,
      -1.074924058818343,
      -1.1154708469335357,
      0.0,
      -1.1015580403758334,
      -1.1183875304568847,
      -1.1471140533225102,
      -1.0525587538125958,
      -1.0458171038254602,
      -1.0937954075522334,
      -1.1572233600494108
    ],
    [
      -1.377860980966039,
      -1.3037919853395066,
      -1.262313848992862,
      -1.2875153157555301,
      -1.2895495650699802,
      -1.3004110943036233,
      -1.328437868272826,
      -1.305266809407609,
      -1.27201277917885,
      -1.3192700212542607,
      -1.265820102288305,
      -1.4020685641117816,
      -1.2901787345688418,
      -1.3251127817809365,
      -1.3170731314881137,
      -1.3218599735552239,
      -1.2495451719265205,
      -1.2690292295944123,
      -1.308499866926519,
      -1.2518221852472537,
      -1.233883232783495,
      -1.3533826878152155,
      0.0,
      -1.3242213229912037,
      -1.2649559518040399,
      -1.3461675492203355,
      -1.2717404103449752,
      -1.2483399054703064,
      -1.377671942870741
    ],
    [
      -1.3902483998718276,
      -1.2937254703766543,
      -1.3319359766748495,
      -1.3157943083777068,
      -1.2894710503542022,
      -1.314788562242869,
      -1.3557180869153642,
      -1.3343054447455847,
      -1.2764290255290986,
      -1.3308115206125106,
      -1.3043233814600985,
      -1.3698252246103144,
      -1.3412442460346332,
      -1.3199885032148901,
      -1.319073213418256,
      -1.3227629218917203,
      -1.300217187378344,
      -1.3180962968794365,
      -1.2964329029938113,
      -1.2669194641576502,
      -1.3003796650362862,
      -1.3516930475105366,
      -1.305634581552554,
      0.0,
      -1.320664426377901,
      -1.3514212320966261,
      -1.2762345410169122,
      -1.2744235467479823,
      -1.3808636780096688
    ],
    [
      -1.5307943433593592,
      -1.4273286783149235,
      -1.4133214290706542,
      -1.498406272765235,
      -1.3908499933142533,
      -1.440291238218889,
      -1.4723374816535426,
      -1.4792523094493797,
      -1.4505633719154283,
      -1.4365206007842097,
      -1.4180419289025197,
      -1.4840580959442329,
      -1.4627908391810547,
      -1.4895217883142942,
      -1.4589893228212492,
      -1.483159924464601,
      -1.422887977492842,
      -1.479804185516357,
      -1.463410249772708,
      -1.450554506164447,
      -1.4156553903440021,
      -1.5341782691688604,
      -1.445776818394873,
      -1.4759559651854557,
      0.0,
      -1.5168127119247445,
      -1.4276215201377944,
      -1.4666884667630462,
      -1.5009311785324397
    ],
    [
      -1.5953015887954842,
      -1.5153751060045295,
      -1.516624758545738,
      -1.5117086706405496,
      -1.4893859384403625,
      -1.4937641971029838,
      -1.5451253867494652,
      -1.4565933494159058,
      -1.4936448535737432,
      -1.55483301696663,
      -1.5223024514492791,
      -1.5854642233647696,
      -1.4700378576071111,
      -1.5207086238816356,
      -1.5385830402074518,
      -1.4917829766061117,
      -1.4790668736218426,
      -1.5050227476714337,
      -1.4932628005191488,
      -1.4986980972049713,
      -1.5087202017238555,
      -1.4753436649115483,
      -1.5316067607809742,
      -1.5519318353323424,
      -1.5413361032898851,
      0.0,
      -1.4784663461646257,
      -1.5158078632355514,
      -1.5403486258196146
    ],
    [
      -1.3928006838092657,
      -1.3408073520403996,
      -1.2886213179709733,
      -1.3069250330152575,
      -1.3270040597630353,
      -1.3457310130919284,
      -1.3959708534990127,
      -1.3031554923756579,
      -1.3466960270093897,
      -1.3193640166665055,
      -1.3435028376481517,
      -1.4565969144403776,
      -1.322327607304462,
      -1.3805733339390847,
      -1.3425982237254028,
      -1.3365271825895682,
      -1.3299965930594184,
      -1.3539696916271675,
      -1.3637815018197796,
      -1.267010805855967,
      -1.3277111204391676,
      -1.3449881587708759,
      -1.344227136294796,
      -1.3885109794822101,
      -1.353848478642348,
      -1.317620559830582,
      0.0,
      -1.3778336495649042,
      -1.460429589885572
    ],
    [
      -1.3825458575800575,
      -1.3336428462654426,
      -1.291897371888681,
      -1.3214435604087056,
      -1.2939762288465817,
      -1.287115295827392,
      -1.3486744233767607,
      -1.3209257695053567,
      -1.2400177582606189,
      -1.3887875197154151,
      -1.2700092472604736,
      -1.3861967568966453,
      -1.3136957363207824,
      -1.3119725674342078,
      -1.3552473342412548,
      -1.315941572601183,
      -1.3450292147581675,
      -1.2422946280566463,
      -1.293342104863918,
      -1.261810848443069,
      -1.2915689594342346,
      -1.3202521856722365,
      -1.3167277765653,
      -1.305487323118933,
      -1.3340279507938966,
      -1.3134992323464993,
      -1.3216316663675443,
      0.0,
      -1.3964259362628682
    ],
    [
      -1.3344632691395646,
      -1.2907061642341704,
      -1.2889309487158642,
      -1.326515353198296,
      -1.2790466853390141,
      -1.3240841763084095,
      -1.3049430066787422,
      -1.3417865124640054,
      -1.2993633398936446,
      -1.2853743425937536,
      -1.3079639388501494,
      -1.2847139728906498,
      -1.329934523791045,
      -1.3039671286598977,
      -1.34088881634078,
      -1.3110433716807728,
      -1.287827446800435,
      -1.323606155898415,
      -1.3037625719855588,
      -1.3003995418138128,
      -1.3253630019543128,
      -1.3123344104969634,
      -1.2933589392365918,
      -1.2957977149305766,
      -1.291626305903906,
      -1.3267633128241212,
      -1.3185792828406724,
      -1.3051426173003042,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.3001792894157034,
      0.28341090072899355,
      0.26897426147516734,
      0.2849790184229919,
      0.2494078185495323,
      0.23574845200651406,
      0.25566661190102735,
      0.2711068812517321,
      0.27059269229295557,
      0.29326649006760785,
      0.23127133300413627,
      0.29515041394437747,
      0.2682990835473955,
      0.25047050429383555,
      0.24988314064449613,
      0.27148784901433665,
      0.26538080606939074,
      0.26095329954114144,
      0.28596344930044726,
      0.24911235497387096,
      0.24075306459065215,
      0.24570175328983668,
      0.23755346083893558,
      0.2480052262857959,
      0.25330120424229596,
      0.24539505902476,
      0.253730534457246,
      0.24989591058360938
    ],
    [
      0.36822340887811045,
      0.0,
      0.480029057063599,
      0.4580486978257823,
      0.5206806193572053,
      0.41913858552461436,
      0.38206565839048645,
      0.4414681876930153,
      0.47248807772464363,
      0.4206966342761458,
      0.5059413725994852,
      0.2970412887346885,
      0.45761755722586117,
      0.46418011313246543,
      0.41075781255719246,
      0.4794421470657757,
      0.5148877093810715,
      0.4318516265710768,
      0.4826701999463896,
      0.568978311245617,
      0.41998914417047595,
      0.3469367299857171,
      0.37715320096448446,
      0.402572486633076,
      0.38230900561959347,
      0.4093255143152057,
      0.4175681400536917,
      0.3792567909218867,
      0.38317466987269655
    ],
    [
      0.2877091952357975,
      0.3575989286202499,
      0.0,
      0.3767047026519805,
      0.4583090783111792,
      0.42162920749502586,
      0.29027682595945903,
      0.3146826980303292,
      0.40476571529797045,
      0.35128892800000955,
      0.3874327934109718,
      0.25390113185270313,
      0.4330707450896396,
      0.34122932926402827,
      0.28294364826308205,
      0.3816714426871217,
      0.3626771654473977,
      0.4281809434739776,
      0.32571776039096445,
      0.3994030845880885,
      0.3440345090107213,
      0.36325025468715677,
      0.3446797097489487,
      0.27023145193792963,
      0.36529459100752515,
      0.36128723760423176,
      0.3775412119525674,
      0.36468592518838494,
      0.27952737597348976
    ],
    [
      0.2845304594225688,
      0.38403407162929115,
      0.44615196703900173,
      0.0,
      0.3832631687800778,
      0.38305662599543,
      0.32917733560944185,
      0.38839474809894203,
      0.4212724762870961,
      0.38789004704571384,
      0.3527348089105595,
      0.2776844857162857,
      0.41803086033608694,
      0.3281763356809706,
      0.3117184481593638,
      0.34722774766214703,
      0.43468320158746243,
      0.33930159171033814,
      0.3800411059572726,
      0.4405868522415677,
      0.3576312892626887,
      0.3712303824349461,
      0.3505174331303118,
      0.3362323021572611,
      0.2965200677537283,
      0.3668945640080319,
      0.3965437084685719,
      0.37383257411637616,
      0.32101970365707655
    ],
    [
      0.30765736530518684,
      0.4691097198451242,
      0.49963477153492764,
      0.3743930233064119,
      0.0,
      0.3698619167679451,
      0.36578262723123234,
      0.37406672280822195,
      0.35822275250766755,
      0.38760560331689,
      0.45739303624491656,
      0.25153484766583833,
      0.4099362059675682,
      0.3738876992910476,
      0.36210690769080633,
      0.3607629260880032,
      0.44405143896785515,
      0.37375970881954834,
      0.37158098009575236,
      0.4532358573425954,
      0.3868016616811447,
      0.31760088017137833,
      0.36777910175519746,
      0.3191029540165231,
      0.3775148090270106,
      0.3555926044944735,
      0.3947213056743535,
      0.32496423707562605,
      0.2779802381337395
    ],
    [
      0.26628741419188273,
      0.33177879298669133,
      0.37697991991017066,
      0.32312908937792817,
      0.3970453187720908,
      0.0,
      0.30855801641975567,
      0.3277703905921805,
      0.3668813783709557,
      0.2916223659008139,
      0.3528668436201001,
      0.22747618869398822,
      0.3604187565657784,
      0.35753391342283813,
      0.3243069268750154,
      0.3511496658145554,
      0.3379091947071533,
      0.3789287279134328,
      0.350455907243967,
      0.3594423603570407,
      0.34279887147133103,
      0.31361257355805217,
      0.34259403067998906,
      0.2816896565126572,
      0.3251855472357832,
      0.33846599862239835,
      0.33652682231893394,
      0.33617933078784157,
      0.2466311739883964
    ],
    [
      0.367975270587354,
      0.43465458178693184,
      0.45649106035439124,
      0.44620208215709267,
      0.4574581547841783,
      0.4139169018594764,
      0.0,
      0.44223488769680763,
      0.417652394056816,
      0.37304834566482614,
      0.43571600253251375,
      0.35017404097574634,
      0.4208100064074398,
      0.404822880275737,
      0.5007789312085513,
      0.42289719967234674,
      0.4205572781254494,
      0.4098987908550278,
      0.43266699619140314,
      0.4287570649178487,
      0.43300070561853654,
      0.3837159472692331,
      0.43568879191940924,
      0.3838138702350098,
      0.34489436151767205,
      0.40134321865996725,
      0.42381573762051294,
      0.4275161483379428,
      0.3631784174358581
    ],
    [
      0.3279619903927826,
      0.4263826454677686,
      0.4694458146232572,
      0.4777481401880741,
      0.45333352258828596,
      0.4295552420974489,
      0.40860560106640986,
      0.0,
      0.4401347926078554,
      0.39517773396076317,
      0.42383997243188776,
      0.2945064613151791,
      0.4882868320860376,
      0.3850885486252371,
      0.3950752260278019,
      0.39983670284955286,
      0.4148751656994458,
      0.45311565068217563,
      0.3818054991912976,
      0.4116851884142112,
      0.49268683996181095,
      0.39004283729861156,
      0.4332686777611876,
      0.3259000292355694,
      0.3634283385631789,
      0.4335610574791269,
      0.46657101474891616,
      0.4015456457804567,
      0.31888589857218896
    ],
    [
      0.3238964142007339,
      0.4649101210996349,
      0.41744850893641194,
      0.3970414840241485,
      0.44870495271619326,
      0.4367748881274425,
      0.3232880393949178,
      0.3878950466723856,
      0.0,
      0.33423902382741,
      0.4437452566892355,
      0.2560099972383165,
      0.4818215129467449,
      0.37442958809768245,
      0.3345643963267917,
      0.3564782911968596,
      0.4879708826847047,
      0.39976616376981866,
      0.36886105767181565,
      0.5328483623461076,
      0.4010281930062587,
      0.32978036367639385,
      0.3709920121657819,
      0.3637323531900116,
      0.35251989454183597,
      0.3872537956937083,
      0.376215277550777,
      0.39841628680017127,
      0.2768571610103123
    ],
    [
      0.30615696072634835,
      0.3225071655715983,
      0.33766501296396023,
      0.31079974658090226,
      0.33613411492777856,
      0.2696617303997999,
      0.24614393568464488,
      0.3018209380665353,
      0.28182026462936594,
      0.0,
      0.3252927842055551,
      0.22008028986058137,
      0.2996854876898354,
      0.288205506127732,
      0.25257754637505103,
      0.2488756870004385,
      0.321065391627785,
      0.2765704862320455,
      0.2948505911427364,
      0.33042243491042433,
      0.32998772233368,
      0.24144176859583522,
      0.27829400501288015,
      0.285283381452456,
      0.33364946966346176,
      0.2661716036739703,
      0.3330224482755275,
      0.26679210977072687,
      0.2513741891604966
    ],
    [
      0.3881067026497178,
      0.5849710075171238,
      0.5015803246992327,
      0.4621685050950246,
      0.6010095880273978,
      0.47327639398793964,
      0.4225768768281182,
      0.4330877187143938,
      0.5025248577087982,
      0.4404589892958404,
      0.0,
      0.33183772267191136,
      0.5104234805398185,
      0.5427451434981465,
      0.4481072439517819,
      0.4882604127074113,
      0.5033965680724592,
      0.5139105353027373,
      0.5005816838640484,
      0.5759107182879762,
      0.48783986851859185,
      0.4165231014623927,
      0.45497483559405305,
      0.391677621358409,
      0.4177067134315102,
      0.36674218913694,
      0.4729242346397786,
      0.470611795824986,
      0.38747915773280694
    ],
    [
      0.24542866450596246,
      0.2730631977396758,
      0.2879333261201589,
      0.2765716132863498,
      0.26164586853659877,
      0.2685727337782535,
      0.27164878513922974,
      0.2868610916611445,
      0.26587027796798046,
      0.2735135276820648,
      0.267561617529642,
      0.0,
      0.2556669255789934,
      0.28248241053537315,
      0.28108700295165323,
      0.30315744996498184,
      0.27653423995727167,
      0.2656505207106521,
      0.2855774908536328,
      0.26362085169034133,
      0.2910729263937557,
      0.25931837908027733,
      0.2900596744930639,
      0.2739726158601601,
      0.28129458617217473,
      0.2417227054958393,
      0.25121833552428585,
      0.2758124333858838,
      0.2809751674810075
    ],
    [
      0.30880192956819585,
      0.45258514949144235,
      0.4769847223831194,
      0.4860608221966407,
      0.4731388301709252,
      0.4042393059197473,
      0.3472997357473939,
      0.4045693088710749,
      0.4674088613644187,
      0.3750251357197565,
      0.43632172521770674,
      0.2698377495670219,
      0.0,
      0.385077340867058,
      0.3321046375360588,
      0.37493650943220613,
      0.42993823629155203,
      0.4128258505287028,
      0.393102846905403,
      0.49282014819678976,
      0.3605822546297548,
      0.3735173949663888,
      0.35218479282446324,
      0.31720060264827166,
      0.34286953449421076,
      0.4167488529349441,
      0.42198045046023536,
      0.375389033189266,
      0.2951739359570398
    ],
    [
      0.3205008818694035,
      0.4399739275906447,
      0.45484950461844864,
      0.3876352765310873,
      0.4691093430431822,
      0.43814644511610157,
      0.36235938642350884,
      0.33192715015727114,
      0.4244248525311354,
      0.38025205973762377,
      0.4911642078875059,
      0.311668498579051,
      0.4101224169907083,
      0.0,
      0.4054285788732719,
      0.4632029886604835,
      0.4006007184039724,
      0.45449199813207886,
      0.41732475027903004,
      0.459982593584881,
      0.3773658843726033,
      0.37222131200507924,
      0.37283009280020685,
      0.38385956475310423,
      0.3706650068908277,
      0.38504490023667537,
      0.39656428720703385,
      0.40763087589942804,
      0.3315785678966905
    ],
    [
      0.30518377536022245,
      0.3910599681662992,
      0.39102904671953365,
      0.3530491739903401,
      0.400987489189929,
      0.3842220313507154,
      0.3757245299892491,
      0.3685336642280368,
      0.35022934103862413,
      0.331743389643262,
      0.43384356146026826,
      0.2835647000092738,
      0.3568377034109651,
      0.41334315975422276,
      0.0,
      0.41752747828772474,
      0.37536843608026915,
      0.3858780125635455,
      0.35892127550842745,
      0.3401131400204045,
      0.42627337016746214,
      0.32185862620097616,
      0.37454107597764486,
      0.4087287572245659,
      0.36462277187186,
      0.33650321147707185,
      0.37762043364388953,
      0.39261515741644737,
      0.2939004261550482
    ],
    [
      0.3170996010670233,
      0.450262743549837,
      0.43631158913780044,
      0.4084483672292838,
      0.45335154900020513,
      0.4083717196297465,
      0.3555532551681879,
      0.36245132652737166,
      0.43659081122250476,
      0.32052963775283216,
      0.4867798149597189,
      0.3203190773091664,
      0.4117165485309633,
      0.452375628312367,
      0.4029356499676706,
      0.0,
      0.3866828444776449,
      0.43240310336326404,
      0.3972072714051098,
      0.46659176274505487,
      0.3779411527734806,
      0.41178611626343176,
      0.35441201457256755,
      0.39308945346759794,
      0.3450179064986434,
      0.38225639911619513,
      0.4400266114020728,
      0.40048289469143983,
      0.30742685700519545
    ],
    [
      0.3691479303787073,
      0.5082295911937578,
      0.448593451426174,
      0.40690492158058245,
      0.5080066813891337,
      0.3589124661117795,
      0.3446674127477676,
      0.39183813437886483,
      0.48957608472896896,
      0.4018402293916483,
      0.43266635947266163,
      0.3168772145181371,
      0.454278968623993,
      0.3962153330025908,
      0.350355700227833,
      0.36851496310636755,
      0.0,
      0.3829910166335042,
      0.4263188747418378,
      0.5292605080172901,
      0.42945020165803083,
      0.3259865593066493,
      0.3939896342564251,
      0.3587144194910068,
      0.35729364075163383,
      0.35737743898586793,
      0.40639005999445676,
      0.40591785162256033,
      0.3550305696020313
    ],
    [
      0.35382604855268873,
      0.4159335866732967,
      0.49682167444033043,
      0.42175880217478623,
      0.48257762257433545,
      0.4976019903074398,
      0.3430060227150076,
      0.4343517787922131,
      0.5085496267571306,
      0.35870229099938844,
      0.5148387558990883,
      0.29221589717624563,
      0.4625749312744005,
      0.48198792969786663,
      0.3980606041601764,
      0.44767147578845345,
      0.40787781118735733,
      0.0,
      0.4021930377631675,
      0.49244748809255645,
      0.414455081389995,
      0.38686021232135026,
      0.40556729339450714,
      0.39883959144793235,
      0.35020628147095745,
      0.40566140733468714,
      0.4678933826548024,
      0.4937221557628184,
      0.2901135108002919
    ],
    [
      0.2736561843663541,
      0.38226329801702685,
      0.38094717704918724,
      0.32796059051767146,
      0.4350206312286189,
      0.386047706239512,
      0.332703281250186,
      0.3287370470217974,
      0.3552957444433389,
      0.3399966525159235,
      0.40297730040004787,
      0.30171176775646025,
      0.38894332872738335,
      0.3619695424255345,
      0.3295399246435644,
      0.3409618903503142,
      0.3670748903478107,
      0.364010540050709,
      0.0,
      0.4012712697929839,
      0.33396897432837425,
      0.36837680932845296,
      0.34181808233952204,
      0.3032790579856721,
      0.3531731579521282,
      0.3411990612618141,
      0.332258654948103,
      0.34715583282153273,
      0.2805180287622524
    ],
    [
      0.3178799575241693,
      0.5131659161396052,
      0.46326022505409825,
      0.42800846282150373,
      0.5193609268686452,
      0.4042813524924884,
      0.3926320432826018,
      0.3705780726063981,
      0.4897332408410171,
      0.36598101308043596,
      0.5164854751686694,
      0.26961599519605683,
      0.47387695431350907,
      0.41291304305903687,
      0.368043004380356,
      0.41498227815401956,
      0.49249920740725806,
      0.44006726409091734,
      0.42950087443262763,
      0.0,
      0.3693150738179576,
      0.3375153816290153,
      0.40015852649479866,
      0.3845835998567577,
      0.335121749038382,
      0.33879750571887923,
      0.4540107924256176,
      0.4391014998295075,
      0.31642999521945625
    ],
    [
      0.2398378502702303,
      0.3172048186076968,
      0.3307123678783259,
      0.2537850309060743,
      0.36058811682009284,
      0.3111383231014917,
      0.2862351067563209,
      0.301196950916919,
      0.3232670836045304,
      0.30710487087072424,
      0.34887681702852635,
      0.2339119199576043,
      0.281035162561704,
      0.28643053371976057,
      0.34174314557269403,
      0.2953301198824867,
      0.3188135236302645,
      0.3290612920611944,
      0.2780979917077746,
      0.30452831839511263,
      0.0,
      0.19844428608316345,
      0.34230478471975,
      0.30277263678715505,
      0.31588369067131095,
      0.28499757103428647,
      0.3690211105361123,
      0.30682838880302077,
      0.24696876792538602
    ],
    [
      0.2933366789882188,
      0.35996539242867054,
      0.3358964717843236,
      0.3651373072697077,
      0.3497692287877665,
      0.35516229927647935,
      0.3179954851219151,
      0.38003754326458883,
      0.37069109102506914,
      0.34244954141515294,
      0.38520845680351323,
      0.3243748858614055,
      0.40769302174353195,
      0.3741787842638671,
      0.3321432748358084,
      0.38145082192822555,
      0.3669517268249136,
      0.3674160090344858,
      0.4122179653496121,
      0.39883981336983587,
      0.35829302525464324,
      0.0,
      0.37220583181234557,
      0.3553763417312943,
      0.3266498188656688,
      0.42120511837558317,
      0.4279467683627187,
      0.3799684646359456,
      0.3165405121387681
    ],
    [
      0.300538063863629,
      0.3746070594901614,
      0.4160851958368059,
      0.3908837290741378,
      0.38884947975968776,
      0.3779879505260446,
      0.349961176556842,
      0.373132235422059,
      0.4063862656508179,
      0.3591290235754072,
      0.41257894254136285,
      0.27633048071788635,
      0.38822031026082615,
      0.35328626304873145,
      0.3613259133415543,
      0.35653907127444406,
      0.42885387290314747,
      0.4093698152352556,
      0.36989917790314886,
      0.4265768595824142,
      0.44451581204617296,
      0.3250163570144524,
      0.0,
      0.3541777218384643,
      0.4134430930256281,
      0.3322314956093324,
      0.40665863448469275,
      0.4300591393593616,
      0.30072710195892705
    ],
    [
      0.2500375737161977,
      0.346560503211371,
      0.3083499969131758,
      0.32449166521031847,
      0.35081492323382313,
      0.3254974113451563,
      0.2845678866726611,
      0.30598052884244065,
      0.36385694805892665,
      0.3094744529755147,
      0.3359625921279268,
      0.2704607489777109,
      0.2990417275533921,
      0.32029747037313516,
      0.3212127601697694,
      0.317523051696305,
      0.3400687862096814,
      0.3221896767085888,
      0.343853070594214,
      0.3733665094303751,
      0.3399063085517391,
      0.28859292607748865,
      0.33465139203547123,
      0.0,
      0.31962154721012426,
      0.2888647414913992,
      0.36405143257111305,
      0.365862426840043,
      0.2594222955783565
    ],
    [
      0.22246049896845332,
      0.325926164012889,
      0.3399334132571583,
      0.2548485695625775,
      0.3624048490135592,
      0.31296360410892343,
      0.28091736067426987,
      0.2740025328784328,
      0.3026914704123842,
      0.31673424154360275,
      0.3352129134252928,
      0.2691967463835796,
      0.2904640031467578,
      0.2637330540135183,
      0.2942655195065633,
      0.2700949178632115,
      0.33036686483497046,
      0.2734506568114554,
      0.28984459255510453,
      0.30270033616336556,
      0.33759945198381036,
      0.21907657315895213,
      0.30747802393293955,
      0.2772988771423568,
      0.0,
      0.23644213040306794,
      0.32563332219001806,
      0.2865663755647663,
      0.25232366379537274
    ],
    [
      0.22360599170010342,
      0.30353247449105814,
      0.3022828219498497,
      0.30719890985503806,
      0.32952164205522516,
      0.32514338339260385,
      0.27378219374612245,
      0.36231423107968186,
      0.32526272692184444,
      0.26407456352895764,
      0.29660512904630854,
      0.2334433571308181,
      0.3488697228884765,
      0.298198956613952,
      0.2803245402881358,
      0.32712460388947595,
      0.339840706873745,
      0.3138848328241539,
      0.3256447799764388,
      0.3202094832906164,
      0.3101873787717322,
      0.3435639155840393,
      0.2873008197146134,
      0.2669757451632453,
      0.2775714772057025,
      0.0,
      0.34044123433096196,
      0.30309971726003626,
      0.278558954675973
    ],
    [
      0.3214526594745477,
      0.3734459912434138,
      0.42563202531284006,
      0.40732831026855587,
      0.38724928352077814,
      0.36852233019188496,
      0.31828248978480067,
      0.41109785090815554,
      0.3675573162744237,
      0.3948893266173079,
      0.3707505056356617,
      0.25765642884343576,
      0.3919257359793513,
      0.33368000934472875,
      0.3716551195584106,
      0.37772616069424525,
      0.384256750224395,
      0.3602836516566459,
      0.35047184146403376,
      0.44724253742784637,
      0.3865422228446458,
      0.36926518451293755,
      0.37002620698901745,
      0.32574236380160326,
      0.3604048646414655,
      0.3966327834532315,
      0.0,
      0.33641969371890923,
      0.2538237533982415
    ],
    [
      0.25653079519985345,
      0.3054338065144684,
      0.34717928089122996,
      0.3176330923712054,
      0.3451004239333293,
      0.35196135695251907,
      0.2904022294031503,
      0.3181508832745543,
      0.3990588945192921,
      0.25028913306449585,
      0.3690674055194374,
      0.25287989588326565,
      0.3253809164591286,
      0.32710408534570323,
      0.28382931853865623,
      0.32313508017872805,
      0.2940474380217435,
      0.39678202472326474,
      0.345734547915993,
      0.377265804336842,
      0.34750769334567644,
      0.31882446710767454,
      0.32234887621461095,
      0.333589329660978,
      0.3050487019860144,
      0.32557742043341165,
      0.31744498641236674,
      0.0,
      0.24265071651704284
    ],
    [
      0.2947017507328962,
      0.33845885563829037,
      0.34023407115659654,
      0.3026496666741647,
      0.35011833453344665,
      0.3050808435640513,
      0.3242220131937186,
      0.2873785074084554,
      0.32980167997881615,
      0.34379067727870716,
      0.3212010810223114,
      0.34445104698181095,
      0.29923049608141583,
      0.32519789121256304,
      0.2882762035316808,
      0.318121648191688,
      0.34133757307202583,
      0.30555886397404586,
      0.325402447886902,
      0.328765478058648,
      0.303802017918148,
      0.31683060937549734,
      0.3358060806358689,
      0.33336730494188416,
      0.33753871396855484,
      0.3024017070483396,
      0.3105857370317884,
      0.32402240257215653,
      0.0
    ]
  ],
  "row_avgs": [
    0.2612728879913852,
    0.43194616956178766,
    0.3546330568278904,
    0.36458494153066473,
    0.37452292510096374,
    0.33050804203256146,
    0.41548857388300287,
    0.4107982524898757,
    0.3866960473429502,
    0.2912983133093626,
    0.4675504996828338,
    0.2727819435741574,
    0.3920973463599925,
    0.40074735968110853,
    0.3683508466752242,
    0.3970864895409421,
    0.4026909363335807,
    0.42236843898583115,
    0.3501013009597242,
    0.40921069396227805,
    0.3005757350289184,
    0.3635393457340736,
    0.3759775086393355,
    0.32052076265630064,
    0.29123681168954835,
    0.30387729622317533,
    0.36499869277805413,
    0.32106995016873696,
    0.320654775130874
  ],
  "col_avgs": [
    0.3015190006320479,
    0.39099281314784723,
    0.4018526321351108,
    0.368413001578662,
    0.41316188429809514,
    0.3732190201503426,
    0.3272922772487112,
    0.3557223853040464,
    0.3897543538494331,
    0.34600500467764916,
    0.397726143637803,
    0.2792869356635109,
    0.38646895474731024,
    0.3641810563054747,
    0.3434192317790403,
    0.3637316383118597,
    0.3855241240022553,
    0.3745350057322155,
    0.36791063994568735,
    0.4111727352195457,
    0.37334607108061046,
    0.3304265362052212,
    0.3556902412582105,
    0.3346199125489246,
    0.34155194883437073,
    0.3476286942264634,
    0.3803782569467379,
    0.36672091865838446,
    0.2949345257495626
  ],
  "combined_avgs": [
    0.28139594431171655,
    0.4114694913548175,
    0.3782428444815006,
    0.3664989715546634,
    0.39384240469952947,
    0.351863531091452,
    0.371390425565857,
    0.38326031889696105,
    0.38822520059619164,
    0.3186516589935059,
    0.4326383216603184,
    0.27603443961883417,
    0.38928315055365137,
    0.3824642079932916,
    0.35588503922713227,
    0.3804090639264009,
    0.394107530167918,
    0.3984517223590233,
    0.35900597045270577,
    0.41019171459091186,
    0.33696090305476445,
    0.34698294096964744,
    0.365833874948773,
    0.3275703376026126,
    0.31639438026195954,
    0.3257529952248194,
    0.372688474862396,
    0.34389543441356074,
    0.30779465044021825
  ],
  "gppm": [
    583.984550099434,
    560.4732928044604,
    553.1620965824194,
    572.1433308074376,
    548.2226062124696,
    566.9051468077593,
    589.7002554221104,
    574.1075841931327,
    561.1140992421371,
    577.3989642181301,
    559.0550309030112,
    610.8508577904174,
    563.1606681407749,
    575.3684068228775,
    582.8383416356133,
    572.9410203154423,
    558.0506156161496,
    569.0250541972974,
    567.9363969131019,
    551.2763772417784,
    561.7867658808399,
    590.6552752770627,
    576.0987072888923,
    582.2617371546801,
    579.5213122653196,
    575.7824645532697,
    562.929462696913,
    567.0085212111501,
    604.9692424994137
  ],
  "gppm_normalized": [
    1.3017153870586078,
    1.3037897642583542,
    1.2964299446131586,
    1.3284435242445016,
    1.2760557781719224,
    1.316530060573774,
    1.3703670787473001,
    1.3246837464937637,
    1.3013518936087152,
    1.3371008876584425,
    1.2924418446403458,
    1.413071255618052,
    1.3090570070237313,
    1.331692627364909,
    1.3487667011605224,
    1.3248047645420675,
    1.2936453807449528,
    1.3205761000614287,
    1.3137658649433395,
    1.2832822189226503,
    1.296683596889057,
    1.362757563461371,
    1.3295215897970578,
    1.344637985934622,
    1.341457587804678,
    1.328508316937759,
    1.3034799677422275,
    1.313954208959205,
    1.3924948402736097
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409,
    557,
    395,
    451,
    431,
    438,
    437,
    417,
    383,
    398,
    374,
    428,
    387,
    416,
    435,
    406,
    409,
    457,
    395,
    425,
    379,
    415,
    441,
    340,
    408,
    417,
    426,
    414,
    438,
    335,
    468,
    442,
    473,
    474,
    383,
    401,
    465,
    446,
    413,
    388,
    399,
    481,
    480,
    453,
    411,
    415,
    389,
    446,
    406,
    459,
    411,
    398,
    367,
    435,
    415,
    350,
    346,
    429,
    364,
    533,
    424,
    479,
    491,
    712,
    431,
    444,
    521,
    415,
    476,
    408,
    564,
    485,
    428,
    449,
    470,
    420,
    441,
    430,
    446,
    447,
    421,
    459,
    433,
    414,
    473,
    353,
    452,
    397,
    650,
    429,
    489,
    455,
    450,
    445,
    432,
    435,
    464,
    421,
    395,
    489,
    392,
    476,
    454,
    450,
    376,
    391,
    389,
    441,
    381,
    413,
    421,
    418,
    441,
    428,
    410,
    396,
    417,
    1943,
    416,
    433,
    407,
    650,
    411,
    434,
    376,
    416,
    422,
    408,
    502,
    428,
    414,
    429,
    420,
    352,
    404,
    352,
    439,
    417,
    355,
    372,
    425,
    401,
    534,
    404,
    461,
    349,
    539,
    434,
    448,
    462,
    435,
    468,
    471,
    457,
    428,
    459,
    392,
    455,
    380,
    417,
    420,
    425,
    418,
    379,
    391,
    487,
    375,
    426,
    378,
    397,
    363,
    392,
    405,
    407,
    337,
    473,
    419,
    495,
    432,
    470,
    474,
    380,
    373,
    422,
    479,
    404,
    457,
    444,
    433,
    437,
    416,
    423,
    392,
    464,
    456,
    432,
    385,
    405,
    480,
    428,
    438,
    447,
    512,
    369
  ],
  "response_lengths": [
    2288,
    2351,
    2794,
    2369,
    2652,
    2603,
    2247,
    2019,
    2382,
    2665,
    2211,
    2543,
    2470,
    2391,
    2458,
    2318,
    2420,
    2235,
    2669,
    2418,
    2485,
    2118,
    2215,
    2451,
    2331,
    2404,
    2451,
    2671,
    2072
  ]
}