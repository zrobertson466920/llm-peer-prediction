{
  "example_idx": 93,
  "reference": "Published as a conference paper at ICLR 2023\n\nTEMPERA: TEST-TIME PROMPT EDITING VIA REINFORCEMENT LEARNING\n\nTianjun Zhang1 Xuezhi Wang2 Denny Zhou2 Dale Schuurmans2, 3 1 UC Berkeley 2 Google Research, Brain Team 3 University of Alberta tianjunz@berkeley.edu\n\nJoseph E. Gonzalez1\n\nABSTRACT\n\nCareful prompt design is critical to the use of large language models in zeroshot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose TEst-tiMe Prompt Editing using Reinforcement leArning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries, and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a comprehensive set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks, including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods. Our code is available at https://github.com/tianjunz/TEMPERA.\n\n1\n\nINTRODUCTION\n\nWith the recent advances in pre-training large language models (Brown et al., 2020; Fedus et al., 2021; Raffel et al., 2020; Chowdhery et al., 2022), prompting, or in-context learning provides a dataefficient framework for performing NLU (Li & Liang, 2021; Shin et al., 2020b; Gao et al., 2020b). Such methods achieve impressive zero-shot and few-show performance in many downstream tasks.\n\nHowever, the prompt often has to be carefully tuned to achieve consistent performance for each task (Lu et al., 2021). For example, prompt tuning aims to optimize a continuous prefix embedding via gradient descent and directly takes generated output from the frozen pre-trained language model (Lester et al., 2021; Liu et al., 2021b;a). On the contrary, discrete prompt optimization focuses on constructing meaningful instructions, in-context exemplars and verbalizers (Brown et al., 2020; Gao et al., 2020b). Prior work often performs black-box optimization or applies RL-based methods for direct generation (Deng et al., 2022; Sun et al., 2022; Prasad et al., 2022). Recent works in the prompt tuning field have shown that, performing instance-dependent prompt tuning (Wu et al., 2022; Jiang et al., 2022) can improve the performance of some downstream tasks. The corresponding concept in the discrete prompt optimization domain is intriguing since it allows users to provide different instructions for different inputs and task. Unlike prompt tuning, such instructions can be more human interpretable. However, finding such query-dependent prompts is often overlooked and is not feasible given the inefficiency of black-box optimization.\n\nIn this paper, we investigate the importance of providing query-dependent discrete prompts and demonstrate how this can be achieved via efficient search. To this end, we propose the concept of test-time editing through reinforcement learning (RL) that allows the agent to perform different editing techniques at test time to construct query-dependent prompts efficiently.\n\nWe formulate discrete prompt optimization as an RL problem by sequentially editing an initial prompt, which only requires high-level guidance on which part to edit and what tools to use. Different from prior work, this formulation strikes a good balance between human prior knowledge, flexibility, feasibility and interpretability. The method allows easy incorporation of human knowledge since one can provide a manually chosen initial prompt and allow RL to perform editing on\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nit. It also achieves a balance between search flexibility and feasibility because by enabling different editing techniques, the prompt can be transformed to very different forms but the search space is more feasible compared to direct generation. The final prompt is also more interpretable since the editing tools we adopted usually do not change the semantic meaning of the sentence.\n\nTo summarize, we propose to construct querydependent prompts through test-time editing and formulate this as an RL problem. We carefully design the action space, enabling the agent to flexibly edit the instructions, in-context exemplars and verbalizers. To better train the RL agent, we propose using the score difference between consecutive prompts before and after editing as rewards and developing a set of techniques that help improve the final performance (e.g., reward normalization). We also adopt an attentionbased policy architecture to attend over possible candidates or design choices, and show this can be effective for RL training.\n\nFigure 1: Data Efficiency for TEMPERA: We comopare the data efficiency of TEMPERA and standard fine-tuning in a few-shot setting. Results are averaged across four tasks: SST2, AG News, RTE and MR. It shows that our method achieves comparable performance using 4x fewer examples.\n\nFollowing the standard few-shot text classification setting, we benchmark our algorithm extensively on multiple tasks (including those from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019)). We show that TEMPERA can achieve SoTA performance (e.g., 1.8% better in SST-2 and 3.9% better in CR) compared to few-shot finetuning, prompt tuning and discrete prompt optimization. We also show that TEMPERA is on 4x more data efficient (over the average of 4 tasks SST2, MR, AG News and RTE) compared with traditional finetuning methods (Figure 1). In addition, we perform extensive ablations on different aspects of the proposed algorithm. We demonstrate that TEMPERA is robust to the prompt pool size and the number of few-shot exemplars.\n\n2 RELATED WORK\n\nPrompting in language models and sensitivity to prompts. Recent research has shown that as language models scale up, new capabilities could be unlocked such as in-context learning (Brown et al., 2020), where the language model is prompted with a few in-context demonstrations and learns to perform a certain task in a sample-efficient way. However, several works have studied the incontext learning ability more closely and found that the task performance can be highly sensitive to how the in-context prompt is written. For example, Lu et al. (2022) found that the prompt order can have a large effect on the final task performance; Zhao et al. (2021) show that the choice of prompt format, training examples, and prompt order can cause the performance to vary quite significantly.\n\nAutomatic prompt generation and search. To address such sensitivity in language models, multiple approaches have been proposed for better prompt generation. In the continuous space, Lester et al. (2021) propose prompt-tuning to add tunable tokens for each task during the fine-tuning stage to improve task performance. Zhong et al. (2021) propose OptiPrompt that optimizes the prompts in the input embedding space directly for factual probing. More recently, Wu et al. (2022) found performing instance-independent prompt-tuning can further boost the performance. In the discrete space, Gao et al. (2020a) propose prompt-based fine-tuning and utilize pre-trained models to automatically generate prompt templates. Schick & Sch ̈utze (2021) and Schick et al. (2020) use a small amount of training data to automatically identify the best label words to use for few-shot classification. Shin et al. (2020a) propose AutoPrompt to perform gradient-guided search to find the best tokens in the prompt, although the best prompts found are usually not interpretable by humans. Jiang et al. (2020) propose mining-based and paraphrasing-based methods to generate meaningful and diverse prompts for factual knowledge probing. Related to our work, Deng et al. (2022) propose an RL-based framework to directly generate better prompts via black-box optimization. Different from existing work, our approach frames the problem as test-time prompt editing with an RL-based framework to perform efficient search in the editing space.\n\n2\n\n100200300400500Number of Training Examples0.650.700.750.800.85Classification PerformanceData Efficiency for TEMPERAFinetuningTEMPERAPublished as a conference paper at ICLR 2023\n\nFigure 2: Test-Time Editing via RL: The RL agent is trained to optimize the performance of a downstream task. At test-time, given a query, the agent adopts an attention-based policy to edit the instructions, in-context exemplars and verbalizers for T rounds.\n\nEfficient training exemplar retrieval as prompts. In addition, existing work has shown the choice of the exemplars can also be critical to the final performance. For example, Liu et al. (2022) propose to retrieve exemplars from a training pool that are semantically similar to a test example, and show it can significantly boost the performance. Rubin et al. (2022) trained a dense retriever to efficiently retrieve good training examples as prompts during test time. In this work, we show that an attention-based exemplar selection process over the embedding space can effectively choose performant training examples within our RL framework.\n\n3 TEST-TIME PROMPT EDITING\n\nWe formulate the task of test-time editing in this section. We give some background on the fewshot text classification and how to use prompts for downstream NLP tasks. Then we formalize a new setting called test-time editing where users are allowed to perform editing over a given prompt, depending on the given input and task during test time.\n\n3.1 BACKGROUND\n\nFew-Shot Text Classification. Following the standard few-shot language model classification setting (Brown et al., 2020), we assume that we are given a pretrained language model L and wish to perform classification on dataset D with label space Y. Assume we are given K samples per class from the training set, the new few-shot training set is given as Dtrain = {xi, yi}K×|Y| . In addition, there is a hold-out test dataset Dtest that we use for evaluation on downstream NLP tasks.\n\ni=1\n\nOptimizing Discrete Prompts. Prompt-based few-shot learning considers the following problem: given a piece of text p as a prompt, we use the generative distribution of the language model PL(y|p, x) to perform various NLP tasks without fine-tuning the model. In particular, for a given objective R, we propose to perform the desired optimization over the prompt by finding an optimal p∗ = arg minp∈V R(PL(y|p, x)). In this paper, we focus on restricting the prompt p as a piece of text instead of letting p to be any vector in the latent space. This not only provides more interpretability of the prompt, but also allows us to use existing natural language tools (e.g., NLTK (Bird et al., 2009)) to perform a discrete search for constructing better prompts.\n\nDifferent Forms of Discrete Prompts. We consider three popular forms of discrete prompts: (1) Instructions, which provide a segment of text describing how the task is performed, usually put at the beginning. (2) In-Context Demonstrations {e0, e1, ..., ek}, which selects several examples and their corresponding labels, usually placed before the query. (3) Verbalization, which aims to design how the task is asked and which keywords to select as labels. See Figure 2 for an example of different transformations that we perform when editing in our RL-based framework.\n\n3.2 TEST-TIME EDITING\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Test-Time Prompt Editing with TEMPERA\n\n1: Input: Language Model L, Initial Prompt p0, Training set Dtrain, Evaluation set Deval, Iteration\n\nN , Fix rounds T\n\nRandom sample batch B ∼ Dtrain, Set p0 for step t = 1, · · · , T do Get st = L(B, pt) Run editing policy at = πθ(st), Get new prompt pt+1 Get new state st+1 = L(B, pt+1) Add transition (st, at, st+1) to replay buffer\n\n2: Initialize πθ(· | s) to be uniform; 3: for episode n = 1, · · · , N do 4: 5: 6: 7: 8: 9: 10: 11: 12: end for 13: Evaluate policy πθ on evaluation dataset Deval\n\nend for Update policy parameter θ of πθ with the PPO loss\n\nPrior works have often attempted to identify a query-agnostic prompt (Deng et al., 2022; Sun et al., 2022) or attempted to directly generate a query-dependent prompt via hyper-networks learning (He et al., 2022). However, query-agnostic prompting fails to incorporate any query-related information into the prompts and directly generating prompts for each individual query is challenging (due to its difficulty to incorporate human prior knowledge or feedback). In addition, by permuting the order of in-context exemplars {e0, e1, ..., ek} (Lu et al., 2022) or searching for the k nearest neighbors of the current test instance (Liu et al., 2022) as in-context exemplars yields better performance. These reveal the importance of constructing query-dependent prompts.\n\nUnlike prior methods, we perform prompt editing at test-time. The procedure works as follows: at test time, one is given an initial prompt p0. We want to learn a function f that takes the initial prompt p0, query x and a pool of examples/verbalizers p′, and outputs a final prompt: pf = f (p0, x, p′). The overall framework of our algorithm is shown in Fig. 2. We allow f to make edits (e.g., editing verbalizers and/or swapping examples) over the original prompt to make it more suitable for the downstream task and query x. Since the editing function f can depend on the query x, we call it the test-time editing function. Note that we train the function f in a fixed training dataset and directly deploy it at test time without any addition training. This is different from the test-time optimization since we don’t have access to the ground truth label or a surrogate objective. Plese see Algorithm.1 for details.\n\n4 TEST-TIME EDITING VIA REINFORCEMENT LEARNING\n\nIn order to learn the test-time editing function f , we present a novel RL-based framework that naturally maps the editing process to an MDP. We will present our framework and discuss how we design the state space, action space and reward in this section.\n\nReinforcement Learning Formulation. We formulate test-time editing as a Markov Decision Process (MDP). Given an initial state, s = (p0, x), consisting of an initial prompt and a query, at each time step t, the RL agent selects one of the editing methods from the action space A. We can then define the transition function T : S × A → S to be the state of prompt before and after editing (pt, x) × at → (pt+1, x). That is, the transition dynamics are deterministic given the editing action. We can either define a fixed horizon H or design a termination function to stop editing and get the final prompt. The goal is to maximize the expected reward R = E[(cid:80)T k=0 γkrk] where rt is the reward and γ is the discount factor. We introduce in detail each component of the state representation, action space and rewards in the following subsections.\n\nState Representation. The RL framework is general and flexible about the representation of states. The only requirement is that such representation contains text information. Instead of directly using the raw text representation, we use the last hidden states of the pretrained language model st = L(pt, x) as the state representation and feed it into the policy network.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Effect of different editing techniques. For instruction, we tokenize it into phrases and perform swapping, addition or deletion. We also allow swapping in-context exemplars or changing different verbalizers.\n\nBefore Editing\n\nAfter Editing\n\nInstruction\n\nSwap Add Delete\n\n“Given text, classify whether it is good or bad.” “Given text, classify whether it is good or bad.” “Given text, classify whether it is good or bad.”\n\n“Classify whether it is good or bad, given text.” “Given text, given text, Classify whether it is good or bad.” “Classify whether it is good or bad.”\n\nExample\n\nPermute Swap\n\n{Example 1, Example 2, ..., Example k } {Example 1, Example 2, ..., Example k }\n\n{Example k, Example 3, ..., Example 1 } {Example k + 1, Example n, ..., Example 1 }\n\nVerbalizer Change\n\n{ “positive”, “negative”}\n\n{“great”, “terrible”}\n\nAction Space Design. We include most of the editing actions in our action space. At each stage, the RL agent can choose the editing objects from instruction, in-context exemplars or verbalizer. For editing the instruction, we provide the initial instruction from natural instructions (Wang et al., 2022). Then we tokenize the instruction into phrase level using NLTK (Bird et al., 2009) and perform swapping, deletion or addition of different phrases. Suppose we have l phrases, the action space size will become (l × (l − 1))/2 + 2l.\n\nFor the in-context exemplars, we keep an example pool of N , initialize our prompt by randomly choose n of them as the initial prompt. We then allow the agent to directly perform swapping one example from the current prompt with either another one from the current prompt or from the pool of examples that are not currently used. This results in an action space for the RL agent of n × N − (n × (n − 1))/2 since we do not allow swapping with the same example.\n\nFor the verbalizer, we allow the RL agent to freely choose which verbalizer to use for each incontext example from PromptSource (Bach et al., 2022). We also will enable the agent to freely choose which verbalizer to use for each query x. Interestingly we found that this helps boost the performance of our algorithm. We provide some examples of the editing process in Tab. 1.\n\nReward Design. We adopt the step reward proposed in RLPrompt (Deng et al., 2022). For each query x, we get the log probability of the output label from the language model log PL(ˆy|x, pt) given the proposed prompt pt with the correct label c, and we define the score difference s(c) as:\n\ns(c, x, pt) = λ1 log PL(ˆyc|x, pt) − λ2 arg max\n\nc′̸=c\n\nlog PL(ˆyc′|x, pt)\n\n(1)\n\nwhere we have introduceed the two balancing hyperparameters λ1 > 0 and λ2 > 0 for the positive and negative terms respectively. Intuitively, this score gives a negative reward when the prediction is not correct and a positive reward otherwise. The goal is to optimize the score for the final prompt.\n\nHowever, RL aims to optimize the accumulated reward during the MDP process while prompt design only cares about the performance of the final prompt. Thus, we propose to use the score difference between successive edits as the immediate reward:\n\nrt = s(c, x, pt) − s(c, x, pt−1)\n\n(2)\n\nIgnoring the discounting factor γ, this makes the accumulated reward from time 0 to T correspond to the score difference between the final and the initial prompt s(c, x, pT) − s(c, x, p0). Now the objective of RL is to maximize the score difference.\n\nAttention-Based Policy Architecture. We adopt an attention-based policy architecture for the reinforcement learning agent. We put attention over a graph of possible candidates and let the agent choose which editing technique to perform. We find that the attention-based architecture helps the agent to emphasize the important examples (e.g., examples that are more semantically similar to the test instance).\n\nWe use the PPO (Schulman et al., 2017) algorithm in our experiments. The detailed hyperparameter used can be found in Appendix. A. We list here a couple of very important techniques we used in our experiments. We found these techniques are crucial to the success of our RL-based framework.\n\nObservation Normalization: Since we take the last hidden states of the language model as observation, it might have very small variances between different samples. We keep a running mean\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nand standard deviation for the observation and normalize it before feeding it to the policy and value network. This is commonly used in RL and we found this boosts the performance of our method.\n\nReward Normalization: For different training samples, performing editing over prompts may result in significantly different reward scales. For some of the samples, different prompts might have very marginal effects on the final prediction, either due to the fact that the model is already confident about the prediction since it is too easy, or the task sample is too hard to predict and the model is confused regardless of what prompt it is fed. On the other hand, for other training samples, editing prompts might bring a huge difference in terms of the accuracy. Thus, we perform sample-wise reward normalization to ensure that the reward scale between samples is relatively consistent.\n\nConditioning Policy on Action History: Directly taking the observation from the language model can be inefficient since the policy has no clue about how it has reached the current state. This will bring a loop that the policy will edit prompts pA → pB and then pB → pA. To mitigate this effect, we build a policy that not only takes in the current hidden state, but also conditioned on the action history on how it gets to the current state. Thus, we break the loop between two prompts by considering how each state is reached.\n\n5 EXPERIMENTS\n\nOur experiments first reveal the effectiveness of TEMPERA in the few-shot setting. We compare TEMPERA with prior baselines like Finetuning (Devlin et al., 2019), Soft Prompt Tuning (Lester et al., 2021), Black-Box Tuning (Sun et al., 2022), RLPrompt (Deng et al., 2022) and other manually tuned prompt methods. On various tasks from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), our method achieves impressive performance comparing to prior baselines. This shows that only using a small amount of training examples is sufficient for RL and TEMPERA is sample efficient. We also illustrate the data efficiency of our method compared to finetuning, showing that TEMPERA can achieve same performance with 5.33x less data.\n\nIn addition to the performance gains, we aim to understand our method from different aspects. In Sec. 5.2, we study how much test-time editing helps compared to query-agnostic prompts. Our experiments demonstrate the importance of test-time editing and the necessity of query-dependent prompts. In Sec. 5.4, we show that how different editing techniques (e.g, instruction, in-context demonstration and verbalization) affect the final performance of the downstream task. We also ablate the number of in-context demonstrations used and the size of the example pool in Sec. 5.6 and Sec. 5.7. Finally, we show some example prompts after editing to illustrate the editing policy.\n\nTasks. We conduct our experiments from different categories including single-sentence tasks (e.g., sentiment analysis including SST-2, Yelp reviews, MR, CR, topic classification including AG News). For one-sentence tasks, the goal is to make a prediction based on the sentence. We also include tasks from different types like NLI (e.g., SST-2) and multiple choices (e.g., AG News). Most of the tasks are from the standard GLUE (Wang et al., 2018).\n\nTask Settings. To ensure a fair comparison, we follow the same setting from LM-BFF (Gao et al., 2020b) and RLPrompt (Deng et al., 2022), we test TEMPERA on few-shot text classification tasks. The setting is devised as follows: We randomly sample 16 training samples per class from the training dataset of each task and use them as the few-shot dataset. This will result in a total of 16 × |Y| training samples (please refer to Appendix. E for the number of classes in each task). We also randomly sample 16 samples per class as the validation dataset. For reporting the final performance, we use the standard test set and the detailed information can be found at Appendix E. In addition to the common setup, we also randomly select n examples from the training dataset as the in-context exemplar pool. We average our runs for 4 random seeds and report the average performance and corresponding standard deviation. For the language model, we use L = RoBERTalarge (Liu et al., 2019). For the details of these settings and tasks, please refer to Appendix. E. The initial instruction is taken from the Natural Instructions (Mishra et al., 2021). The initial in context demonstrations are randomly sampled from a fixed example pool of size 16 and the example pool is also randomly sampled from the training dataset, different from the few-shot dataset that used for training the RL policy.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nBaselines. We compare TEMPERA with several SoTA prompt tuning and discrete prompt optimization baselines (including finetuning).\n\n• Finetuning: it finetunes the entire language model with a classification head using the few-shot\n\ndataset.\n\n• Manual Prompt: we take the handcrafted prompt from (Bach et al., 2022).\n\n• Black-Box Tuning: it is a mixture of discrete and soft prompt. The soft part is trained using\n\ngradient descent and the discrete part is optimized using gradient-free tuner.\n\n• AutoPrompt: it adds the discrete trigger token and updates the prompts by iterative gradient\n\nsearch.\n\n• In-Context Demonstration: it randomly selects one training example and concatenates them\n\nwith the input query.\n\n• Instructions: Following Natural Instructions (Wang et al., 2022), prompts are manually created instruction for each task. Each prompt is concatenated with inputs. Details are in Appendix. D.\n\n• GrIPS: it performs phrase level editing on the instructions and selects the best one.\n\n• RLPrompt: it generates discrete prompts using RL framework.\n\n5.1 FEW-SHOT TEXT CLASSIFICATION\n\nFollowing the settings in existing work, we evaluate our model on some few-shot text classification tasks. In Tab. 2, We compare our method with various baselines including RLPrompt. We can see that on most tasks we tested, TEMPERA outperforms previous baselines by a large margin. For example, we have a 1.8% absolute gain on the SST-2 task (over RLPrompt), 3.9% gain on the CR task and the performance is almost comparable to finetuning the language model on the AG News task. We also see that our method results in a much smaller variance between runs than Soft Prompt Tuning and AutoPrompt, indicating that it is more stable across different few-shot datasets. Comparing to search-based methods (e.g., Black-Box Tuning or GrIPS), our method avoids the expensive run-time search if one wants to perform test-time editing using one of the black-box optimization methods with a surrogate reward. Note since the original Black-Box Tuning or GrIPS paper didn’t perform query-dependent search, this is our conjecture. Thus, out method achieves both test-time efficiency and good performances on downstream tasks.\n\nTable 2: Few-shot classification results. We compare against different baselines in this setting. Results show that TEMPERA surpasses various baselines including finetuning, prompt tuning and discrete prompt search. The standard deviations are shown in brackets.\n\nSST-2\n\nYelp P.\n\nMR\n\nCR\n\nAG News\n\nFinetuning\n\nFinetuning (few-shot)\n\n80.6 (3.9)\n\n88.7 (4.7)\n\n67.4 (9.7)\n\n73.3 (7.5)\n\n84.9 (3.6)\n\nContinuous Prompt\n\nDiscrete Prompt\n\nSoft Prompt Tuning Black-Box Tuning AutoPrompt\n\nManual Prompt In-Context Demo. Instructions GrIPS RLPrompt\n\n73.8 (10.9) 88.6 (2.1) 74.1 (14.6) 75.9 (11.8) 82.6 (0.9) 83.5 (0.9) 86.6 (1.3) 89.1 (0.9) 65.7 (1.9) 62.0 (0.8) 75.0 (7.6)\n\n93.2 (0.5) 79.8 (8.3)\n\n87.4 (1.0) 57.5 (5.8)\n\n82.8 85.9 (0.7) 89.0 87.1 (1.5) 90.1 (1.8)\n\n83.0 89.6 (0.4) 84.4 88.2 (0.1) 93.9 (1.8)\n\n80.9 80.6 (1.4) 85.2 86.1 (0.3) 86.7 (2.4)\n\n79.6 85.5 (1.5) 80.8 80.0 (2.5) 87.2 (1.7)\n\n76.9 74.9 (0.8) 54.8 65.4 (9.8) 77.2 (2.0)\n\nDiscrete Prompt\n\nTEMPERA (ours)\n\n91.9 (2.0)\n\n92.6 (1.7)\n\n88.0 (1.1)\n\n91.1 (1.6)\n\n85.5 (1.5)\n\n5.2\n\nIMPORTANCE OF TEST-TIME PROMPT EDITING\n\nTo illustrate the importance of test-time prompt editing, we compare our method with various baselines that do not perform test-time editing. In addition, we also construct another baseline where we\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Data Efficiency for TEMPERA: We compare data efficiency between TEMPERA and few-shot finetuning. Results show that we can achieve a good performance with significantly less data (varying from 4x to 8x).\n\ncreate a RL based method where the policy is not dependent on the input query x, denoted as “TEMPERA (No TTE)”. Results in Tab. 3 show that TEMPERA even without test-time editing can find better query-agnostic prompts comparing to manually construct prompts, in-context demonstration and GrIPS. However, adding test-time editing can further improve the performance when the task is harder: we got 0.8% improvement on MR task and 3.0% improvement at AG News task. On SST-2, the effect of test-time editing is not significant as we suspect that the task is too easy. We found on harder tasks like AG News, the gain of test-time editing is huge.\n\nTable 3: We compare our method against different methods which do not perform test-time editing. Results show that test-time editing is mostly helpful in harder tasks like AG News.\n\nSST-2 MR AG News\n\nManual Prompt In-Context Demo. Instructions GrIPS TEMPERA (No TTE)\n\n82.8 85.9 89.0 87.1 92.0\n\n80.9 80.6 85.2 87.1 87.4\n\nTEMPERA\n\n91.9\n\n88.2\n\n76.9 74.9 54.8 65.4 81.3\n\n84.3\n\n5.3 DATA EFFICIENCY FOR TEMPERA\n\nTable 4: Ablation on different editing techniques. Results show that adding verbalizer-edits helps all the tasks (especially MR and AG News). Adding instruction-edits marginally helps the performance in SST-2 and MR.\n\nSST-2 MR AG News\n\nTEMPERA (No Inst & Verb) TEMPERA (No Inst) TEMPERA\n\n91.2 91.9 92.4\n\n87.2 88.2 88.4\n\n82.2 84.3 85.5\n\nTo illustrate the data efficiency of our method, we compare the performance of TEMPERA with some few-shot standard finetuning results in Fig. 3. We see that in SST-2, we achieve similar performance using almost 8x fewer training data. In tasks like Yelp, the gain is about 4x. We see that with fewer examples, TEMPERA strictly dominates fine-tuning methods. This is critical when applying TEMPERA in the real-world application since labeled data is expensive to get.\n\n5.4 QUALITATIVE ANALYSIS OF THE EDITS\n\nWe also visualize our policy by taking a few examples from the final prompts after editing in Tab. 5. We see that our method mostly does example selection, verbalizer swapping and phrase-level instruction editing. Our editing techniques are flexible and the final prompt may take different combinations for each query. In addition, the resulting final prompt is still interpretable by human, showing that our method achieves flexibility and interpretability at the same time. Note that in the examples provided in Tab. 1, our policy choose to modify the example selection and verbalization.\n\n5.5 ABLATION: DIFFERENT EDITING TECHNIQUES\n\nWe ablate on the different editing techniques and study how adding or removing them can affect the performance. The results are shown in Tab. 4. We can see that adding each component (e.g., verbalizer, instruction) is helpful in terms of the final performance. We also find that verbalizer is especially helpful in some tasks like AG News, resulting in a 1.2% difference in the final performance. This indicates that adding more flexibility to some extent can help the performance.\n\n8\n\n100200300400500Number of Training Examples0.60.70.80.9Classification PerformanceData Efficiency for TEMPERA (SST2)FinetuningTEMPERARLPromptGrIPSBlack-Box TuningSoft Prompt TuningAutoPrompt100200300400500Number of Training Examples0.650.700.750.800.850.90Classification PerformanceData Efficiency for TEMPERA (AG_News)FinetuningTEMPERARLPromptGrIPSBlack-Box TuningSoft Prompt TuningAutoPrompt100200300400500Number of Training Examples0.8000.8250.8500.8750.9000.9250.950Classification PerformanceData Efficiency for TEMPERA (Yelp)FinetuningTEMPERARLPromptGrIPSBlack-Box TuningSoft Prompt TuningAutoPromptPublished as a conference paper at ICLR 2023\n\nTable 5: Qualitative results on the effect of the learned policy. We see that our method both enables the flexibility of various edits and interpretability of the final results. On the contrary, many prior methods produce non-readable prompts. Red text is prior to editing and blue text are the changes.\n\nSST-2\n\nBefore Edit\n\nAG News\n\nAfter Edit (better verbalizer)\n\nBefore Edit\n\nAfter Edit (better exemplar selection)\n\n“In this task, you are given sentences from movie reviews. The task is to classify a sentence as “positive” if the sentiment of the sentence is positive or as “negative” if the sentiment of the sentence is negative. Review: of saucy. Sentiment: positive. Review: cold movie. Sentiment: negative. Review: heroes. Sentiment: <mask>.” “In this task, you are given sentences from movie reviews. The task is to classify a sentence as ”great” if the sentiment of the sentence is positive or as “terrible” if the sentiment of the sentence is negative. Review: of saucy. Sentiment: great. Review: cold movie. Sentiment: terrible. Review: heroes. Sentiment: <mask>.”\n\n“Classify the news articles into the categories of World, Sports, Business, and Technology. Article: What’s in a Name? Well, Matt Is Sexier Than Paul (Reuters) Reuters - As Shakespeare said, a rose by any other name would smell as sweet. Right? Answer: Technology. Article: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street’s dwindling band of ultra-cynics, are seeing green again. Answer: <mask>.” “Classify the news articles into the categories of World, Sports, Business, and Technology. Article: Expansion slows in Japan Economic growth in Japan slows down as the country experiences a drop in domestic and corporate spending. Answer: Business. Article: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street’s dwindling band of ultra-cynics, are seeing green again. Answer: <mask>.”\n\nTable 6: Ablation on the number of in-context exemplars. Results show that increasing the number of examples results in a consistent increase of performance except for AG News (which is due to the length limit).\n\nTable 7: Ablation on the size of the prompt pool to select from. We see that the performance does not change too much when changing the size of the pool, indicating that the performance is relatively stable.\n\nSST-2 MR AG News\n\nSST-2 MR AG News\n\nTEMPERA (2 Examples) TEMPERA (4 Examples) TEMPERA (8 Examples)\n\n91.6 91.9 92.4\n\n87.9 88.2 88.4\n\n84.0 84.3 82.2\n\nTEMPERA (Pool Size 8) TEMPERA (Pool Size 16) TEMPERA (Pool Size 32)\n\n91.6 91.9 92.2\n\n87.9 88.2 88.4\n\n84.1 84.3 84.7\n\n5.6 ABLATION: NUMBER OF SHOTS\n\nWe also ablate on the number of examples used in the in-context demonstration part of our algorithm. We choose the size of 2, 4 and 8 for the analysis. We see that from Tab. 6, in all the tasks we tested (SST-2, MR and AG News), increasing the number of examples consistently improves the performance. However, the performance improvement is relatively limited. In addition, due to the input length limit constraint by the language model (512 for RoBERTa), longer sequences of input will be truncated. This results in the performance decrease when increasing the number of examples from 4 to 8 for AG News, where the input length is longer than 512.\n\n5.7 ABLATION: SIZE OF THE PROMPT POOL\n\nWe also ablate on the example size of the prompt pool where we keep the number of examplers of 4. Intuitively, allowing our method to choose in-context demonstrations from a large range of example pool can provide better prompts. From Table. 7, we can see that increasing the example pool size gives the algorithm more flexibility to choose in-context demonstrations, resulting in a slightly better final performance.\n\n6 CONCLUSION\n\nIn this paper we present TEMPERA, a test-time prompt editing method for large language models via reinforcement learning. We found that perform test-time editing can greatly improve the performance of downstream tasks for a pretrained language model. The proposed method only requires little guidance on high-level search space design and can easily incorporate prior human knowledge. It achieves SoTA performance on multiple benchmarks including those from GLUE. This intersection area of research between NLP and RL can inspire future research on designing better test-time editing algorithms for practical usage.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and repository for natural language prompts, 2022.\n\nSteven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing\n\ntext with the natural language toolkit. ” O’Reilly Media, Inc.”, 2009.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. 2022. doi: 10.48550/ARXIV.2205.12548. URL https://arxiv.org/abs/ 2205.12548.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\n\nmodels with simple and efficient sparsity, 2021.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\n\nlearners, 2020a. URL https://arxiv.org/abs/2012.15723.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\n\nlearners. arXiv preprint arXiv:2012.15723, 2020b.\n\nYun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, Yaguang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. HyperPrompt: Prompt-based task-conditioning of transformers. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8678–8690. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ he22f.html.\n\nYuezihan Jiang, Hao Yang, Junyang Lin, Hanyu Zhao, An Yang, Chang Zhou, Hongxia Yang, Zhi Yang, and Bin Cui. Instance-wise prompt tuning for pretrained language models. arXiv preprint arXiv:2206.01958, 2022.\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020. doi: 10.1162/tacl a 00324. URL https://aclanthology.org/2020.tacl-1.28.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\n\npreprint arXiv:2101.00190, 2021.\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100–114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022. deelio-1.10.\n\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021a.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021b.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086–8098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022. acl-long.556.\n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\n\nvia natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.\n\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2655–2671, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. naacl-main.191. URL https://aclanthology.org/2022.naacl-main.191.\n\nTimo Schick and Hinrich Sch ̈utze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https: //aclanthology.org/2021.eacl-main.20.\n\nTimo Schick, Helmut Schmid, and Hinrich Sch ̈utze. Automatically identifying words that can serve as labels for few-shot text classification. In Proceedings of the 28th International Conference on\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nComputational Linguistics, pp. 5569–5578, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.488. URL https://aclanthology.org/2020.coling-main.488.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In Empirical Methods in Natural Language Processing (EMNLP), 2020a.\n\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020b.\n\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for\n\nlanguage-model-as-a-service. arXiv preprint arXiv:2201.03514, 2022.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 4496bf24afe7fab6f046bf4923da8de6-Paper.pdf.\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, et al. Benchmarking\n\ngeneralization via in-context instructions on 1,600+ language tasks. arXiv, 2022.\n\nIDPG: An instance-dependent prompt generation method.\n\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Rui Hou, Yuxiao Dong, V.G.Vinod Vydiswaran, and In Proceedings of the Hao Ma. 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5507–5521, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.403. URL https: //aclanthology.org/2022.naacl-main.403.\n\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models, 2021. URL https://arxiv.org/abs/ 2102.09690.\n\nZexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [mask]: Learning vs. learning\n\nto recall. In North American Association for Computational Linguistics (NAACL), 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA TRAINING DETAIL\n\nWe provide the training details here. We use standard PPO algorithm to do online policy optimization with GAE. We provide all the hyperparameters here for a reference. We’ll specify our neural network architecture in the following section. Note that we perform additional observation normalization (i.e., keeping a running mean and std) and reward normalization. We also adopt the same number of parallel environment as the few-shot setting (e.g., 32 in our few-shot experiments). We found a large size of parallel environment helps boost the performance.\n\nTable 8: Hyperparameters used for TEMPERA in all the tasks.\n\nHyperparameter Value\n\nSteps per training Time limit Number Parallel Processes Learning rate Entropy Coefficient Value loss Coefficient Mini Batch Size Gamma GAE Lambda Number of in-context Exemplars Number of example pool Positive lambda coefficient (λ1) Negative lambda coefficient (λ2)\n\n8 8\n256 0.00005 0.005 0.5 32 0.99 0.95 4\n16 2.0 1.8\n\nB NETWORK ARCHITECTURE\n\nWe follow the GPT (Brown et al., 2020) architecture and use the encoder layer for our policy network. Note that our policy and baseline network shares the same attention-based encoder. The attention is flat over all the possible candidate examples. We use a 3-layer encoder block with 3 heads and 48 latent dimension. We build two different head with 2-layer MLP for each as the policy head and baseline head. We also don’t use dropout for the policy learning part. We found this boost up the performance.\n\nC ADDITIONAL EXPERIMENTS\n\nWe perform additional experiments on some more tasks like RTE, QNLI, SNLI, MNLI and MRPC. Results show that we are consistently better than most of the discrete prompt optimization methods and continuous prompt tuning methods. On several tasks, we are also better than finetuning the entire model.\n\nD NATURAL INSTRUCTIONS AND PROMPTSOURCE\n\nWe provide all the instructions we used in our experiments from Natural Instructions. Here we just provide a few examples. Please refer to the github for all the instruction they provided. We also provide all the verbalizers we used in our experiments from Promptsource. Here we only provide a few examples. Please also refer to their github for the full verbalization.\n\nE DATASET DETAIL\n\nFor the Finetuning, we use standard finetuning of the RoBERTa model from huggingface for 100 epochs, a learning rate of 0.0003 and the optimizer of Adam.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Few-shot classification results. We compare against different baselines in this setting. Results show that TEMPERA surpasses various baselines including finetuning, prompt tuning and discrete prompt search. The standard deviations are shown in brackets.\n\nRTE\n\nQNLI\n\nSNLI\n\nMNLI\n\nMRPC\n\nFinetuning\n\nFinetuning (few-shot)\n\n58.6 (3.9)\n\n60.2 (4.7)\n\n54.64 (9.7)\n\n47.8 (7.5)\n\n77.4 (3.6)\n\nContinuous Prompt\n\nDiscrete Prompt\n\nSoft Prompt Tuning Black-Box Tuning\n\nManual Prompt In-Context Demo.\n\n54.7 (10.9) 52.6 (0.9)\n\n51.6 60.4 (0.7)\n\n49.7 (0.2) 48.8 (0.6)\n\n50.8 53.8 (0.4)\n\n36.13 (14.6) 46.58 (1.3)\n\n31.11 47.11 (1.4)\n\n33.2 (0.0) 42.9 (2.0)\n\n51.7 53.4 (1.5)\n\n51.6 (0.9) 61.6 (0.9)\n\n67.4 45.8 (0.8)\n\nDiscrete Prompt\n\nTEMPERA (ours)\n\n60.3 (2.2)\n\n57.4 (1.5)\n\n56.4 (3.2)\n\n45.2 (2.0)\n\n74.0 (1.0)\n\nFigure 4: Data Efficiency for TEMPERA: We plot all the finetuning performance for 8 tasks we tested. We see that TEMPERA often achieves the better few-shot performance except for MRPC and QNLI.\n\nF COMPARISON OF DIFFERENT METHOD\n\nWe compare the different property of different prompting methods in this section in order to give a better understanding of different algorithms.\n\n14\n\n1002003004005006065707580859095Classification PerformanceData Efficiency for TEMPERA (SST2)Fine-TuningTEMPERA1002003004005008586878889Classification PerformanceData Efficiency for TEMPERA (Ag_News)Fine-TuningTEMPERA100200300400500848688909294Classification PerformanceData Efficiency for TEMPERA (Yelp)Fine-TuningTEMPERA10020030040050050.052.555.057.560.062.565.067.5Classification PerformanceData Efficiency for TEMPERA (RTE)Fine-TuningTEMPERA1002003004005006065707580Classification PerformanceData Efficiency for TEMPERA (QNLI)Fine-TuningTEMPERA10020030040050055606570758085Classification PerformanceData Efficiency for TEMPERA (MR)Fine-TuningTEMPERA100200300400500Number of Training Examples40506070Classification PerformanceData Efficiency for TEMPERA (MNLI)Fine-TuningTEMPERA100200300400500Number of Training Examples7476788082848688Classification PerformanceData Efficiency for TEMPERA (MRPC)Fine-TuningTEMPERAPublished as a conference paper at ICLR 2023\n\nTask\n\nSST-2\n\nAG News\n\nCR\n\nMR\n\nYelp\n\nRTE SNLI\n\nQNLI\n\nMNLI\n\nTask\n\nSST-2\n\nAG News\n\nCR\n\nMR\n\nYelp RTE\n\nSNLI\n\nQNLI\n\nMNLI\n\nMRPC\n\nTable 10: Natural instructions used for TEMPERA in all the tasks.\n\nNatural Instructions\n\n“In this task, you are given sentences from movie reviews. The task is to classify a sentence as “great” if the sentiment of the sentence is positive or as “terrible” if the sentiment of the sentence is negative.” “Classify the news articles into the categories of World, Sports, Business, and Technology.” “In this task, you are given sentences from customer reviews. The task is to classify a sentence as “great” if the sentiment of the sentence is positive or as “terrible” if the sentiment of the sentence is negative.” “In this task, you are given sentences from movie reviews. The task is to classify a sentence as “great” if the sentiment of the sentence is positive or as “terrible” if the sentiment of the sentence is negative.” “In this task, you are given sentences from Yelp reviews. The task is to classify a sentence as “great” if the sentiment of the sentence is positive or as “terrible” if the sentiment of the sentence is negative.” N/A “In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree (entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the letters Yes, Maybe, and No respectively.” “You are given two sentences(Sentence1 and Sentence2). Answer “yes” if these sentences are a paraphrase of one another, otherwise answer “no”.” “In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree (entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the letters Yes, Maybe, and No respectively.”\n\nTable 11: Verbalizers used for TEMPERA in all the tasks.\n\nNatural Instructions\n\nthis\n\nbest\n\nlabel\n\nnews\n\narticle?\n\n{{text}}\n\ndescribes\n\n‘Someone just said to me “{{sentence}}”. Do you think they are {{“sad”}} or {{“happy”}}? {{ answer choices[label]}}’ “What {{answer choices[label]}}” ‘Someone just said to me “{{sentence}}”. Do you think they are {{“sad”}} or {{“happy”}}? {{ answer choices[label]}}’ ‘{{text}} Did the reviewer find this movie {{“good or bad”}}? swer choices[label] }}’ ‘{{ text }} Overall, the experience is {{ answer choices[label] }}’ ‘Does the claim “{{sentence2}}” follow from the fact that “{{sentence1}}”? Please answer either {{“yes”}} or {{“no”}}. {{answer choices[label]}}’ ‘Suppose {{premise}} Can we infer that “{{hypothesis}}”? Yes, no, or maybe? {{ answer choices[label] }}’ ‘{{sentence}} Does that sentence have all you need to answer the question “{{question}}”? {{answer choices[label]}}’ ‘Suppose {{premise}} Can we infer that ”{{hypothesis}}”? Yes, no, or maybe? {{ answer choices[label] }} ’ ‘Does the sentence {{sentence1}} paraphrase (that is, mean the same thing as) this sentence? {{sentence2}} {{ answer choices[label] }}’\n\n{{ an-\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 12: Scaling results for TEMPERA in 512 training data per class. Results show that TEMPERA also scales and achieves better results comparing to finetuning.\n\nFinetuning\n\nFinetuning (few-shot)\n\nDiscrete Prompt\n\nTEMPERA (ours)\n\nSST2 MR AG News RTE\n\n93.4\n\n93.8\n\n87.0\n\n88.6\n\n89.5\n\n88.6\n\n67.9\n\n71.4\n\nTable 13: Details for the dataset including the type, size of training, evaluation and test. Note that here all the sizes are few-shot dataset.\n\nDataset\n\nSST2 AG News CR MR Yelp RTE SNLI QNLI MNLI\n\nType\n\nSentiment topic Sentiment Sentiment Sentiment NLI NLI NLI NLI\n\n|C|\n\n|Train| = |Dev|\n\n|Test|\n\n2 4\n2 2\n2 2\n3 3\n3\n\n32 64 32 32 32 32 48 48 48\n\n1.8k 7.6k 2k 2k 38k 0.3k 10k 9.8k 9.8k\n\nFigure 5: Comparison of Different Prompting Methods: We compare the different property of different algorithms. We can see that TEMPERA is gradient-free, the resulting prompt is interpretable and query-dependent.\n\n16",
  "translations": [
    "# Summary Of The Paper\n\n* This paper proposes a test-time prompt editing technique with reinforcement learning. Compare to prior methods, TEMPERA can efficiently leverage prior knowledge, adaptive to different queries, and provides an interpretable prompt for every query. This method achieves 5.33x on average improvement in sample efficiency when compared to traditional fine-tuning methods.\n\n# Strength And Weaknesses\n\nStrength:\n* This paper proposes a novel method: It formulates discrete prompt optimization as using RL to edit an initial prompt. It also proposes carefully designed action space, and a set of techniques to improve the final performance.\nWeakness:\n* It seems that this method mostly improves sample efficiency but not absolute performance (e.g. not in few-shot setting).\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* This paper is clear, novel and seems to provide a lot of implementation details (in appendices).\n\n# Summary Of The Review\n\nThis paper proposes to treat prompt tuning as a test-time editing problem with RL, which can give more flexibility and demonstrate promising empirical results. However, it's novel because it's the first to apply RL on prompt editing, but the RL technique itself is not much novel and seems like it mostly improves sample inefficiency but not absolute accuracy (e.g. not in few-shot setting). So, I'd recommend weak accept.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable\n\n# Details Of Ethics Concerns\n\nN/A",
    "# Summary Of The Paper\nThe paper introduces TEMPERA (TEst-tiMe Prompt Editing via Reinforcement Learning), a novel framework for optimizing prompt design for large language models (LLMs) in zero-shot and few-shot learning settings. TEMPERA reframes prompt optimization as a reinforcement learning (RL) problem, allowing for adaptive and interpretable prompt edits tailored to different queries. The methodology leverages a designed action space that enables flexible editing of prompts, including instructions, examples, and verbalizers. The results demonstrate significant performance improvements over existing methods, achieving a remarkable 5.33x enhancement in sample efficiency compared to traditional fine-tuning approaches across various NLP tasks.\n\n# Strength And Weaknesses\nThe main strengths of TEMPERA include its innovative application of RL to prompt optimization, which offers a structured and interpretable way to customize prompts dynamically. The extensive empirical evaluations across diverse tasks showcase its robustness and practical utility, as well as its superior performance compared to state-of-the-art techniques. However, a potential weakness lies in the complexity of the RL formulation, which may present challenges in terms of implementation and understanding, particularly for practitioners unfamiliar with reinforcement learning. Additionally, the reliance on an attention-based architecture, while effective, may limit accessibility for users with less computational resources.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and findings. The authors provide comprehensive descriptions of the proposed method and experimental design. The novelty of framing prompt optimization as an RL problem is a significant contribution to the field. The reproducibility is enhanced by the availability of the code, which allows other researchers to validate the results and build upon the proposed method. Overall, the clarity and quality of the writing are high, making the paper accessible to both experts and non-experts in the field.\n\n# Summary Of The Review\nTEMPERA presents a significant advancement in prompt design for large language models by integrating reinforcement learning for adaptive and interpretable prompt editing. The empirical results demonstrate its effectiveness and efficiency, establishing it as a promising approach for future research in automated prompt optimization. Despite its complexity, the paper is well-written and reproducible, contributing valuable insights to the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel approach for test-time prompt editing using reinforcement learning (RL) to optimize prompts for large language models (LLMs). The authors address the critical nature of prompt design, especially in zero-shot and few-shot learning scenarios, by proposing a flexible method to edit prompts dynamically during the test phase. Key contributions include a novel action space for editing prompts, a unique reward mechanism based on score differences, and comprehensive evaluations across various benchmarks. The results show TEMPERA achieving state-of-the-art performance and substantial improvements in sample efficiency compared to existing methods.\n\n# Strength And Weaknesses\nThe strengths of TEMPERA lie in its innovative approach to prompt editing, allowing for increased adaptability and interpretability, which is crucial for real-world applications where labeled data is limited. The method demonstrates impressive efficiency, achieving high performance with far fewer training examples, showcasing robustness across tasks. However, the complexity of the RL framework may pose implementation challenges, and the method’s dependence on the initial prompt quality could hinder its effectiveness. Additionally, scalability concerns with larger datasets and the potential need for further fine-tuning for specific tasks are notable limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates the methodology and findings, making it accessible to the reader. The novelty of the approach is significant, introducing a new dimension to prompt engineering via RL. However, the complexity of the RL-based framework may affect reproducibility, as practical implementation could require substantial expertise and effort. Overall, the quality of the experiments and the thorough benchmarking enhance the paper's credibility.\n\n# Summary Of The Review\nTEMPERA introduces a promising method for optimizing prompts in large language models through reinforcement learning, achieving notable improvements in performance and data efficiency. While the approach is innovative and well-validated, its complexity and reliance on initial conditions may limit practical applications. Future research could focus on simplifying the method and exploring broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces TEMPERA (TEst-tiMe Prompt Editing via Reinforcement learning), a novel approach for adapting prompts in large language models during test time to enhance performance on various tasks, particularly in low-data scenarios. The authors employ reinforcement learning to facilitate prompt editing, allowing the model to modify prompts based on specific queries effectively. TEMPERA demonstrates significant performance improvements over existing methods, achieving a 5.33x increase in sample efficiency compared to traditional fine-tuning techniques, as evidenced by experiments on several benchmarks such as GLUE and SuperGLUE.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of a significant problem in prompt optimization and the innovative application of reinforcement learning for test-time editing, which is a relatively unexplored area. The experimental results are robust, showing consistent improvements across a variety of tasks, which supports the proposed methodology. However, one weakness is the reliance on specific hyperparameters that may require fine-tuning for different tasks, which could limit the method's generalizability. Additionally, while the qualitative analysis provides insights into the editing process, further exploration of the interpretability of the learned strategies would strengthen the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a logical flow from the introduction to the experimental results. The methodology is articulated clearly, allowing readers to follow the RL framework and the design of the action space. The novelty lies in framing prompt editing as a reinforcement learning task, which is a fresh perspective in the context of NLP. The reproducibility is enhanced by the availability of code on GitHub, although it would benefit from more detailed explanations of the hyperparameter choices and their impact on different tasks.\n\n# Summary Of The Review\nTEMPERA presents a compelling approach to prompt optimization in large language models using reinforcement learning, achieving significant gains in efficiency and performance across various benchmarks. While the method shows promise, further exploration of hyperparameter sensitivity and interpretability could enhance its applicability and understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TEMPERA, a novel approach for efficient test-time prompt editing using reinforcement learning (RL). The main contributions include a flexible framework that allows for real-time adaptations of prompts based on varying queries, achieving state-of-the-art performance and significant improvements in sample efficiency (up to 5.33x better than traditional fine-tuning). The methodology emphasizes interpretable prompt modifications, robust integration of human knowledge, and an attention-based policy architecture, which collectively enhance the model's editing process.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative use of reinforcement learning for prompt editing, which demonstrates significant performance gains across specific tasks like sentiment analysis and topic classification. The interpretability of prompts and the incorporation of human-selected initial prompts further enhance usability and understanding. However, the complexity of the RL framework may deter practitioners, and the limited diversity of tasks in the evaluation restricts the generalizability of the findings. Additionally, the model's performance is heavily reliant on the quality of initial prompts, which poses a risk of poor outcomes if these prompts are not well-designed. While robustness to prompt variability is claimed, further empirical validation through ablation studies is necessary.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, though some sections could benefit from more detailed explanations, particularly regarding the RL framework. The novelty lies in the intersection of NLP and RL, which is a growing area of interest. However, the complexity may hinder reproducibility for those not familiar with RL techniques. A more thorough error analysis is needed to enhance the understanding of the model's limitations and provide insights for future applications.\n\n# Summary Of The Review\nTEMPERA presents a compelling approach to prompt editing using reinforcement learning, achieving notable performance improvements in specific NLP tasks. While the methodology shows promise, the paper could strengthen its contributions through broader task validation and deeper empirical analysis to support its claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents **TEMPERA: Test-Time Prompt Editing via Reinforcement Learning**, a novel framework designed to enhance large language models' performance through dynamic prompt modification during inference. The methodology introduces **Test-Time Prompt Modification via Adaptive Search (TMPMAS)**, which adapts prompts in real-time based on specific queries without requiring extensive retraining. Key contributions include the design of a unique action space for prompt modification, a reinforcement learning approach to optimize these modifications, and empirical validation showing substantial improvements in data efficiency across various natural language processing tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to prompt engineering, which allows for dynamic and interpretable modifications that enhance task performance. The empirical results demonstrate that TMPMAS significantly outperforms existing methods, highlighting its potential for real-world applications. However, weaknesses include potential scalability issues when applied to extremely large models and a need for further exploration of user-guided modifications to enhance adaptability and user experience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it easy to follow the methodology and findings. The quality of the empirical results is high, with rigorous evaluations and ablation studies supporting the claims. The novelty of the approach lies in its unique combination of reinforcement learning with adaptive prompt modification, which is a relatively unexplored area. Reproducibility is aided by the clear description of the methodology, though additional details regarding implementation and dataset specifics could further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in prompt engineering for language models through the TMPMAS framework, demonstrating strong empirical results and interpretability. While the approach shows great promise, addressing potential scalability and user adaptation issues could further strengthen its applicability in diverse scenarios.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"TEMPERA: Test-Time Prompt Editing via Reinforcement Learning\" introduces a novel framework aimed at enhancing the robustness of machine learning models against adversarial attacks during inference. The core methodology involves using reinforcement learning to adaptively modify input data at test time, allowing models to improve their performance when faced with adversarial examples. The findings demonstrate that TEMPERA significantly outperforms existing adversarial training approaches across various benchmarks, highlighting improvements in both robustness and overall performance. Additionally, the authors emphasize the interpretability of the modifications, which aids in building user trust and understanding of model behavior.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative integration of reinforcement learning with adversarial training, which presents a fresh perspective in a relatively under-explored area. The empirical results are compelling, showcasing significant improvements in model robustness, which is crucial for real-world applications. Furthermore, the focus on interpretability adds value, making the method more accessible to practitioners. However, the paper has weaknesses, particularly concerning the computational efficiency of the proposed method, which may need optimization for practical applications. Additionally, a more thorough discussion of the method's limitations, especially in handling extreme adversarial scenarios, would strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of writing is high, making it accessible to a broad audience. The novelty of the approach is notable, combining established concepts in a unique manner. However, the reproducibility of the results may be challenged by the computational demands of the reinforcement learning component, which could deter some researchers from implementing TEMPERA in their own work.\n\n# Summary Of The Review\nOverall, TEMPERA presents a significant advancement in the field of adversarial training, offering a novel and interpretable approach to enhance model robustness through adaptive input modifications. Although the empirical results are promising, attention to computational efficiency and limitations would further solidify the paper's contributions to the community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TEMPERA, a novel approach to prompt optimization in large language models, aiming to enhance performance in zero-shot and few-shot learning tasks through prompt editing using reinforcement learning (RL). The methodology proposes a new action space for prompt editing, which is only slightly more complex than existing methods. Although the authors claim state-of-the-art performance improvements, the empirical results indicate only marginal gains in sample efficiency and data efficiency, raising questions about the practical significance of the findings.\n\n# Strength And Weaknesses\nOne of the strengths of TEMPERA lies in its exploration of prompt optimization, which is a crucial aspect of leveraging large language models effectively. The incorporation of reinforcement learning offers a fresh perspective on prompt editing, albeit the actual advancements appear minimal compared to existing techniques. The extensive ablation studies provide insights into the impacts of different editing strategies, but the reported improvements are often trivial and unlikely to influence broader NLP research. A significant weakness is the exaggerated claims regarding data efficiency and performance improvements, which do not hold up under scrutiny, potentially misleading readers about the practical implications of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally acceptable; however, the presentation of results may downplay the actual significance and relevance of the findings. The quality of the methodology is adequate, but the novelty is questionable, as the proposed improvements do not substantially exceed existing methods. Reproducibility is not thoroughly addressed, particularly in light of the small sample sizes used in experiments, which may hinder other researchers from validating the claims effectively.\n\n# Summary Of The Review\nWhile TEMPERA proposes an interesting approach to prompt editing in NLP, its contributions appear to be less impactful than claimed, with only marginal improvements over existing methods and questionable empirical significance. The method's strengths are overshadowed by inflated claims and limited practical relevance, ultimately suggesting that further advancements in the area are still needed.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces TEMPERA, a novel method for test-time prompt editing in large language models utilizing reinforcement learning (RL) to enhance prompt design's adaptivity and sample efficiency. The authors propose a flexible action space for prompt modifications that includes instructions, exemplars, and verbalizers, asserting significant improvements over traditional prompt tuning and recent state-of-the-art (SoTA) approaches. Experimental results demonstrate that TEMPERA achieves better performance across various tasks, with claims of 3.5x improved data efficiency compared to traditional fine-tuning, although the magnitude of performance gains is modest in simpler tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to framing discrete prompt optimization as an RL problem, allowing for more adaptive and efficient prompt generation. The flexibility of the action space is a notable advancement in prompt design, potentially addressing existing limitations in prompt optimization methods. However, the reported performance gains, while statistically significant, appear modest, particularly in less complex tasks, which may limit the practical impact of the proposed method. Additionally, the paper acknowledges the need for further evaluation across a broader range of tasks, which is a weakness that may affect its overall applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings of the research. The writing quality is high, making it accessible to readers with varying backgrounds in machine learning and natural language processing. The novelty of the approach is evident, particularly in its combination of RL with prompt editing, which has not been extensively explored in prior work. However, the reproducibility of the results may be a concern, as the paper does not provide extensive details on the experimental setup or hyperparameter choices, which are crucial for replicating the findings.\n\n# Summary Of The Review\nTEMPERA presents a promising methodology for enhancing prompt editing in large language models through reinforcement learning, showcasing improvements in sample efficiency and performance. While the approach is innovative and well-articulated, the modest performance gains and the need for further validation across diverse tasks may limit its immediate applicability. Overall, the paper contributes valuable insights into prompt optimization but requires additional empirical support.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"TEMPERA: Test-Time Prompt Editing via Reinforcement Learning\" presents a novel approach to enhancing language model performance by dynamically editing prompts using reinforcement learning (RL). The authors frame prompt editing as a Markov Decision Process (MDP), allowing for query-dependent prompt adjustments. They demonstrate that TEMPERA achieves significant improvements in sample efficiency compared to traditional fine-tuning methods, claiming a 5.33x improvement. The method is evaluated across various tasks, and results suggest that it can effectively generalize learned editing strategies, although its reliance on pre-trained models raises questions about robustness and generalizability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of RL to the prompt editing problem, providing a structured framework for optimizing prompts based on specific queries. However, several weaknesses are noted: the assumption that prompt sensitivity is universally applicable may not hold for all tasks; the choice of RL may not be the most effective method for exploring prompt space compared to simpler techniques; and the comprehensive nature of the action space for prompt editing is questionable, potentially overlooking alternative strategies. Additionally, the dependence on human-selected initial prompts may introduce bias, and there are concerns regarding the scalability and computational overhead associated with the RL training process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-organized and clearly articulates its contributions; however, some assumptions could benefit from further clarification. The novelty of framing prompt editing as an MDP is significant, yet the reproducibility of the results may be hindered by the reliance on specific pre-trained models and the design of the reward function, which is critical for RL performance. The empirical evidence provided is compelling, but a more comprehensive comparison with recent methods could enhance the paper's contributions.\n\n# Summary Of The Review\nOverall, TEMPERA offers a promising approach to prompt editing through reinforcement learning, showing notable improvements in sample efficiency. Despite its strengths, the paper's assumptions regarding the generalizability and effectiveness of its approach should be critically considered, alongside a broader evaluation against contemporary methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces TEMPERA, a novel approach for test-time prompt editing utilizing reinforcement learning to optimize prompts for large language models (LLMs). The authors argue that effective prompt design is crucial for enhancing few-shot learning capabilities of LLMs and propose a framework that enables adaptive, interpretable, and query-dependent prompts. The methodology involves framing the editing process as a Markov Decision Process (MDP), where the RL agent learns to adjust prompts based on input queries. Extensive experiments on various NLP tasks demonstrate that TEMPERA significantly improves performance over baseline methods, showcasing its data efficiency and the effectiveness of different editing techniques.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative application of reinforcement learning for prompt optimization, which addresses a critical gap in the literature regarding adaptive prompt design. The authors provide a comprehensive experimental evaluation, clearly demonstrating the advantages of their approach across multiple tasks. However, a notable weakness is the lack of detailed qualitative analyses regarding the nature of the prompt edits made by TEMPERA, which could provide deeper insights into the underlying mechanisms driving performance improvements. Additionally, the computational complexity of the proposed method may limit its practical applicability in resource-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation and contributions of TEMPERA. The methodology is presented with sufficient detail, allowing for reproducibility; however, some sections could benefit from additional clarifications, particularly around the implementation specifics of the RL agent. The novelty of the approach is substantial, as it blends reinforcement learning with prompt engineering in a manner that has not been extensively explored in prior work. Overall, the quality of writing is high, with clear figures and explanations supporting the findings.\n\n# Summary Of The Review\nTEMPERA represents a significant advancement in the field of prompt optimization for large language models through the innovative use of reinforcement learning. While the paper exhibits strong empirical results and clarity, it could benefit from further qualitative insights into the prompt editing process and considerations regarding practical implementations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel algorithm for enhancing the performance of neural networks in image classification tasks. The authors propose a hybrid framework that combines techniques from transfer learning and data augmentation to improve accuracy and robustness. Through extensive experiments on benchmark datasets, the authors demonstrate that their approach outperforms existing methods, achieving state-of-the-art results while also addressing issues related to overfitting and generalization.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Framework:** The combination of transfer learning and data augmentation is novel and offers a unique perspective in tackling the challenges in image classification.\n2. **Robust Experimental Design:** The authors conduct a comprehensive set of experiments, comparing their method against a wide range of baselines, which strengthens the validity of their claims.\n3. **Significant Results:** The reported improvements in performance metrics are substantial, indicating the potential impact of the proposed method on real-world applications.\n\n**Weaknesses:**\n1. **Complexity of Implementation:** The proposed method may be difficult to implement for practitioners due to its complexity and the requirement of multiple hyperparameter tuning.\n2. **Limited Dataset Diversity:** While the experiments are thorough, they primarily focus on established datasets, and additional testing on more diverse datasets would enhance the generalizability of the results.\n3. **Insufficient Discussion of Limitations:** The paper lacks a thorough discussion of potential limitations and biases in the proposed approach, which is critical for transparency.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-written, with a clear structure and logical flow. However, certain sections, particularly the methodology, could benefit from more detailed explanations and examples to enhance clarity. The novelty of the approach is significant, as it offers a fresh perspective on integrating transfer learning and data augmentation. The reproducibility is somewhat compromised due to the complexity of the method and the lack of detailed implementation guidelines.\n\n# Summary Of The Review\nThis paper presents a promising approach that combines transfer learning and data augmentation to improve image classification performance. While the contributions are significant and the experimental results are compelling, there are areas that require refinement, particularly in the clarity of the methodology and the discussion of limitations.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel approach for optimizing prompts in large language models using reinforcement learning (RL) at test time. The primary contributions include the development of a flexible prompt editing mechanism that adapts to various queries while maintaining interpretability. The methodology involves leveraging prior knowledge to enhance sample efficiency, demonstrating an average improvement of 5.33x over traditional fine-tuning methods across tasks such as sentiment analysis, topic classification, natural language inference, and reading comprehension. The findings indicate that TEMPERA can effectively enhance the performance of pretrained language models with minimal guidance and incorporates human knowledge seamlessly.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative application of reinforcement learning to prompt optimization, which addresses a critical aspect of zero-shot and few-shot learning. The empirical results showcase significant improvements in performance and sample efficiency, establishing TEMPERA as a competitive method in the NLP domain. However, a potential weakness is the reliance on prior knowledge, which may limit the method's applicability in scenarios where such information is not readily available or easily integrated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its objectives, methodology, and findings. The quality of the experiments is commendable, providing robust evidence for the claims made. The novelty of combining RL with prompt editing in NLP tasks is notable, although the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nOverall, TEMPERA presents a significant advancement in the field of NLP by introducing a novel method for optimizing prompts using reinforcement learning. The approach demonstrates impressive empirical results and opens avenues for future research in automated prompt design. However, the integration of prior knowledge could be a limitation in broader applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TEMPERA, a novel approach for test-time prompt editing in large language models utilizing reinforcement learning (RL). It formulates the prompt editing process as a Markov Decision Process (MDP), allowing for dynamic adjustments based on query-specific needs. The authors demonstrate that TEMPERA significantly enhances model performance across multiple NLP tasks, achieving 5.33 times better sample efficiency compared to traditional fine-tuning methods. The findings underscore the importance of query-dependent prompts and the flexibility of RL-driven editing techniques.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative application of reinforcement learning to prompt optimization, which is a critical aspect of leveraging large language models effectively. The authors provide a thorough empirical evaluation, showcasing TEMPERA's superior performance over baseline methods including prompt tuning and fine-tuning. However, the paper could benefit from a more detailed discussion on the limitations of the proposed approach and potential scenarios where it may not outperform existing techniques. Additionally, the complexity of the RL framework may pose challenges for practitioners aiming to replicate the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the methodology and findings accessible to the reader. The quality of the experiments is high, with comprehensive benchmarking against established methods. The novelty of applying an RL framework to prompt editing is significant, pushing the boundaries of current prompt optimization techniques. However, the reproducibility of the results may be hindered by the complexity of the RL setup and the specific implementation details that may not be fully elaborated.\n\n# Summary Of The Review\nOverall, TEMPERA presents a compelling and innovative approach to prompt optimization through test-time editing via reinforcement learning, demonstrating significant performance improvements in NLP tasks. While the contributions are noteworthy and the methodology is sound, the paper could enhance its discussion of limitations and reproducibility challenges.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"TEMPERA: Test-Time Prompt Editing via Reinforcement Learning\" introduces a novel method for dynamically editing prompts at test time using reinforcement learning techniques. The authors formulate the problem of test-time prompt editing as a reinforcement learning task, detailing an action space, state representation, and reward structure designed specifically for this purpose. Through empirical evaluations on various natural language understanding tasks, TEMPERA demonstrates significant performance improvements over existing state-of-the-art methods while also being more data-efficient.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to prompt editing, which leverages reinforcement learning to optimize prompts dynamically during inference. This method addresses the limitations of traditional prompt tuning approaches and enhances performance across multiple benchmarks. However, a potential weakness is that the paper could elaborate further on the limitations of the proposed method, such as the complexity of the reinforcement learning setup and how it may affect real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to the reader. The methodology is described in sufficient detail to allow for reproducibility, with code availability noted, which is a positive aspect for the community. The novelty of applying reinforcement learning to prompt editing is significant, as it opens new avenues for research in the intersection of natural language processing and reinforcement learning.\n\n# Summary Of The Review\nOverall, \"TEMPERA\" presents a compelling and innovative approach to prompt editing that holds promise for enhancing the effectiveness of large language models. Its clear methodology and strong empirical results underscore its potential impact in the field, while the availability of code further supports reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TEMPERA (TEst-tiMe Prompt Editing via Reinforcement learning), a novel framework for optimizing prompts in large language models (LLMs) during test-time. By reformulating prompt optimization as a Reinforcement Learning (RL) problem, the methodology allows for dynamic adjustments to prompts, leveraging pre-existing knowledge and adapting to different queries. The authors present a well-defined action space that includes editing instructions, few-shot exemplars, and verbalizers. Empirical results demonstrate significant performance enhancements over existing methods, with a 5.33x improvement in sample efficiency compared to traditional fine-tuning, validated through experiments on GLUE and SuperGLUE benchmarks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to prompt editing through the lens of Reinforcement Learning, which allows for adaptable and interpretable modifications of prompts. The design of the action space is comprehensive, facilitating various editing operations that enhance the model's performance across tasks. However, the paper could benefit from a deeper exploration of the implications of the hyperparameters in the reward mechanism, and a clearer discussion on the scalability of the approach to larger and more complex datasets beyond GLUE and SuperGLUE.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly. The methodology is logically structured, making it relatively easy to follow the process of how TEMPERA operates. The novelty of the approach is significant, as it combines RL with prompt engineering in a way that has not been extensively explored in prior works. However, the reproducibility of results could be improved by providing more details on the implementation specifics, including the RL training process and hyperparameter tuning.\n\n# Summary Of The Review\nTEMPERA presents a substantial advancement in the field of prompt engineering for LLMs through the innovative application of Reinforcement Learning. While the framework demonstrates impressive empirical results, enhancing clarity and reproducibility details will strengthen the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for prompt editing in reinforcement learning, termed TEMPERA, which aims to enhance the interpretability and efficiency of model outputs. The authors employ an attention-based policy architecture to facilitate decision-making during the prompt editing process. The findings suggest significant improvements over existing methods, particularly in specific benchmark tasks; however, the real-world applicability of these improvements remains inadequately addressed.\n\n# Strength And Weaknesses\nThe paper makes a notable contribution by proposing a framework that integrates reinforcement learning with attention mechanisms, potentially offering a more interpretable approach to prompt editing. However, several weaknesses are apparent. The reliance on reinforcement learning introduces complexity and instability, which could hinder reproducibility. The tailored nature of the prompts raises concerns about generalizability, while the benchmarking choices may lead to biased assessments of the method's effectiveness. Moreover, the lack of thorough ablation studies limits the understanding of the framework's components and their necessity, and claims regarding data efficiency appear exaggerated without adequate comparative context.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a clear methodology and promises novelty in its approach, the complexity of the reinforcement learning model may obscure reproducibility. The focus on a specific architecture and its implications for interpretability could confuse readers regarding how decisions are derived. Additionally, the lack of rigorous validation in diverse settings diminishes the perceived quality of the findings, as real-world applicability remains uncertain.\n\n# Summary Of The Review\nOverall, the paper introduces a potentially valuable framework for prompt editing using reinforcement learning; however, significant concerns regarding its generalizability, reproducibility, and practical applicability limit its impact. Further validation and exploration of the trade-offs involved in its implementation are necessary to establish the robustness of the proposed method.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel approach for test-time prompt editing using reinforcement learning within the context of natural language processing (NLP). Its main contributions include a significant enhancement in sample efficiency—averaging 5.33x better than traditional fine-tuning methods—and state-of-the-art performance across various NLP tasks such as sentiment analysis and reading comprehension. The methodology allows for real-time editing of prompts to adapt to individual queries, while the empirical results demonstrate consistent improvements in task performance, reduced data requirements, and the generation of human-interpretable prompts.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of reinforcement learning for prompt editing, which offers adaptability and efficiency. The empirical results are compelling, showing strong performance across multiple benchmarks and significant data efficiency. However, potential weaknesses include a lack of detailed analysis regarding the limitations of the approach and how it performs in edge cases or with noisy data. Additionally, while the interpretability of prompts is a strong point, the paper could benefit from more extensive user studies to evaluate the practical implications of these interpretable prompts in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with clear explanations of its methodology and findings. The quality of the experimental results is high, with robust comparisons to existing methods. The novelty of using reinforcement learning for dynamic prompt editing is significant, presenting a fresh perspective in the field. However, reproducibility could be improved with additional details on the implementation and hyperparameter settings, ensuring that other researchers can readily replicate the results.\n\n# Summary Of The Review\nOverall, TEMPERA represents a significant advancement in the field of NLP by introducing a flexible and efficient method for prompt editing at test time. The combination of reinforcement learning and sample efficiency positions it as a promising approach for improving language model interactions, though further work on reproducibility and practical implications would strengthen the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel approach to test-time prompt editing for language models leveraging reinforcement learning. It proposes a framework that formulates prompt optimization as a Markov Decision Process (MDP) to systematically explore state-action spaces for dynamic prompt adjustment. The methodology includes a unique action space designed for prompt editing, an attention-based policy architecture, and a reward mechanism that evaluates prompt edits based on performance improvements. The findings indicate that query-dependent prompts significantly enhance model performance, thereby underscoring the importance of context-awareness in natural language processing.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical foundation, which effectively integrates reinforcement learning principles with prompt optimization in language models. The MDP formulation provides a structured approach to understanding prompt dynamics, while the novel action space and attention-based policy architecture contribute to its innovative nature. However, one potential weakness is the reliance on empirical results to validate theoretical constructs, which may not fully capture the robustness of the proposed methods in diverse real-world applications. Additionally, the interpretability aspect, although emphasized, could benefit from more concrete examples or case studies demonstrating its impact on user trust.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulate, presenting complex concepts in an accessible manner. The quality of writing is high, with clear definitions and explanations of key terms. The novelty of the approach is significant, particularly in the integration of reinforcement learning with prompt design. However, reproducibility may be a concern, as the specifics of the empirical experiments are not detailed extensively, leaving room for ambiguity regarding the implementation of the proposed techniques.\n\n# Summary Of The Review\nOverall, TEMPERA offers a compelling framework for enhancing prompt optimization in language models through reinforcement learning. Its theoretical contributions are significant, though the empirical validation of the proposed methods could be elaborated further to strengthen claims regarding effectiveness and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"TEMPERA: Test-Time Prompt Editing via Reinforcement Learning\" introduces a novel approach to prompt editing for language models, leveraging reinforcement learning (RL) to adapt prompts based on different queries. The methodology is framed as a Markov Decision Process (MDP) where an action space is defined to facilitate flexible modifications of prompts, such as swapping, adding, or deleting phrases. The authors demonstrate that TEMPERA achieves data efficiency, outperforming traditional fine-tuning methods by utilizing significantly fewer training examples while still maintaining robust performance across various tasks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to prompt editing through an RL framework, which provides a structured and interpretable method for adapting prompts. The inclusion of ablation studies adds credibility to the claims of effectiveness across different editing techniques. However, the paper could benefit from more comprehensive comparisons with existing state-of-the-art methods to better contextualize its contributions. Additionally, while the theoretical framework is solid, the practical implications and potential limitations of the proposed method warrant further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the contributions and findings. The methodology is described in sufficient detail to allow for reproducibility, with code made available for public access. The novelty of the approach is evident in the integration of RL with prompt editing, though the overall significance of this innovation in the broader context of natural language processing could be emphasized further. The clarity of the writing and the quality of the figures and tables enhance the overall presentation.\n\n# Summary Of The Review\nOverall, TEMPERA presents a compelling approach to prompt editing that leverages reinforcement learning to enhance the adaptability and efficiency of language models. While the contributions are significant, a deeper exploration of comparative performance and limitations would strengthen the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TEMPERA, a method that employs reinforcement learning for test-time prompt editing, claiming to surpass previous approaches such as RLPrompt and AutoPrompt in performance. The authors argue that TEMPERA provides enhanced interpretability and sample efficiency, while achieving state-of-the-art performance across various benchmarks, particularly in sentiment analysis and topic classification tasks. However, the paper's claims regarding superiority and innovation may not be thoroughly substantiated against the latest advancements in the field.\n\n# Strength And Weaknesses\nThe key strengths of TEMPERA lie in its application of reinforcement learning for prompt editing and its focus on interpretability, which are noteworthy contributions to the field. However, the paper exhibits several weaknesses, including a lack of robust comparisons with the latest methods, potential overstatements regarding sample efficiency and interpretability, and a narrow evaluation scope that does not account for broader applications. Furthermore, the authors' critique of prior methods appears selective and fails to address the practical complexities of their own approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably clear in its presentation, but the claims of novelty and significance are undermined by the authors' reliance on prior work and the lack of comprehensive comparisons. While the methodology is described adequately, the reproducibility of results may be challenged by the selective nature of the evaluation and the assumptions made regarding the benefits of human knowledge in prompt initialization. Overall, the quality of the work is acceptable, but the novelty and reproducibility aspects require further strengthening.\n\n# Summary Of The Review\nTEMPERA presents a reinforcement learning-based approach to prompt editing that claims to improve upon existing methods in terms of interpretability and efficiency. However, the paper's contributions may not be as novel or significant as suggested, given the similarities to prior work and the selective critique of existing methods. A more thorough evaluation against the latest advancements would enhance the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"TEMPERA: Test-Time Prompt Editing via Reinforcement Learning\" proposes a novel approach for optimizing prompt designs in natural language processing (NLP) tasks using reinforcement learning (RL). The authors present a methodology that leverages test-time prompt editing to enhance model performance without the need for extensive training data. The findings demonstrate that their approach achieves a significant average improvement of 5.33 times over existing state-of-the-art (SoTA) methods, showing promise in few-shot and zero-shot learning scenarios.\n\n# Strength And Weaknesses\nThe paper's primary contribution lies in its innovative use of RL for prompt optimization, which addresses a critical gap in existing prompt engineering techniques. However, the paper suffers from several weaknesses, including inconsistent formatting and typographical errors that detract from its professionalism. The explanations of certain terms, such as \"ablations,\" may not be accessible to all readers, and the lack of clear subsection titles in the introduction reduces the overall readability of the paper. \n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel approach, the clarity of the writing could be improved through better organization and consistent terminology. The overall quality is hindered by formatting inconsistencies and typographical errors that require a thorough proofreading pass. The reproducibility of the results is difficult to assess due to the unclear notation in certain sections and the inconsistent citation formatting.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach to prompt optimization via reinforcement learning, demonstrating significant empirical improvements. However, the clarity and professionalism of the document are compromised by several formatting and typographical issues, which need to be addressed for a stronger presentation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel method for test-time prompt editing aimed at improving model performance across various tasks. The authors employ a reinforcement learning (RL) framework to facilitate flexible edits to prompts based on specific input queries. Results demonstrate that TEMPERA achieves state-of-the-art performance on selected benchmarks, though the evaluation is largely confined to few-shot scenarios. The paper discusses the action space for prompt modifications but does not fully explore the integration of advanced natural language understanding techniques or the implications of user feedback in the prompt editing process.\n\n# Strength And Weaknesses\nTEMPERA's main strengths lie in its innovative approach to prompt editing and the comprehensive analysis of different editing techniques. However, the paper's weaknesses include a limited scope of evaluation, as it primarily focuses on few-shot settings without exploring zero-shot or many-shot scenarios. Additionally, there is a lack of discussion on how the method could be adapted for novel or unseen queries, as well as the ethical implications of automated prompt editing which are crucial for sensitive applications. The paper also falls short in addressing the computational efficiency and memory requirements relevant for real-world deployment.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and findings. However, the lack of in-depth discussions on hyperparameter settings and their effects on performance hinders reproducibility. While the technical novelty is notable, the empirical evaluation could be strengthened by including a wider array of tasks and complexities. The absence of case studies or illustrative examples makes it difficult to fully grasp the implications of the editing process on model performance.\n\n# Summary Of The Review\nOverall, TEMPERA introduces a promising approach to test-time prompt editing, showcasing significant potential through its RL framework. However, the paper would benefit from a broader evaluation, enhanced discussion of practical implications, and a deeper exploration of the editing process to improve clarity and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel approach to test-time prompt editing utilizing reinforcement learning (RL) for enhancing model performance on natural language processing tasks. The authors claim significant improvements over state-of-the-art methods such as prompt tuning and AutoPrompt, achieving a 5.33x increase in sample efficiency. The methodology involves formulating prompt optimization as an RL problem, with extensive experiments conducted on GLUE and SuperGLUE datasets, demonstrating statistically significant gains in performance metrics.\n\n# Strength And Weaknesses\nThe key strength of TEMPERA lies in its innovative application of RL to prompt optimization, which is a fresh perspective in the field. The extensive benchmarking and statistical significance testing bolster the credibility of the results. However, a notable weakness is the limited discussion on the potential limitations of the RL approach, including the computational complexity and scalability of the method. Additionally, while the paper emphasizes statistical validation, it could benefit from more comprehensive analyses to further substantiate the performance claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers. The use of statistical methods to validate results enhances the quality of the work. In terms of novelty, the approach is relatively new, but the practical implications and ease of reproducibility could be better addressed, particularly concerning the RL framework's implementation details.\n\n# Summary Of The Review\nOverall, TEMPERA demonstrates a promising method for prompt optimization that leverages reinforcement learning, showing significant potential in improving model performance efficiently. However, more rigorous statistical validation and implementation details are needed to address potential limitations and enhance reproducibility.\n\n# Correctness\n4/5 - The methodology and results appear to be sound, but further statistical validation is required to fully substantiate performance claims.\n\n# Technical Novelty And Significance\n4/5 - The application of reinforcement learning to prompt editing is innovative, contributing to the advancement of methodologies in natural language processing.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results are compelling, showcasing significant improvements over existing methods, though further validation is necessary to confirm robustness across varied tasks and datasets.",
    "# Summary Of The Paper\nThe paper introduces TEMPERA, a novel approach for few-shot text classification that leverages reinforcement learning to edit prompts dynamically. The methodology involves a flexible editing mechanism that allows for the modification of prompts, which aims to enhance model performance on benchmark datasets such as SST-2 and AG News. The findings indicate that TEMPERA outperforms traditional fine-tuning methods in terms of efficiency, although the evaluation is limited to a narrow set of tasks and lacks comprehensive exploration of prompt diversity.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative use of reinforcement learning for prompt editing, which adds a layer of adaptability to few-shot learning scenarios. However, the paper exhibits significant weaknesses, including a lack of comprehensive evaluation across a variety of tasks, limited exploration of diverse prompt types, and a dependency on the quality of initial prompts. Additionally, the sensitivity to hyperparameter tuning and scalability concerns raise questions about the practical applicability of the approach. The absence of an ablation study on editing methods further obscures the understanding of their individual contributions. Lastly, the paper does not adequately address potential biases in training data, which could impact the fairness of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a coherent presentation of the methodology and findings. However, the reproducibility is hindered by the lack of guidance on prompt selection and hyperparameter tuning, which are critical for achieving optimal performance. The novelty of the approach is notable, particularly in its application of reinforcement learning for prompt editing, but it could be strengthened by a more thorough exploration of diverse tasks and prompt formats.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to few-shot text classification through TEMPERA, but it is limited by its narrow focus and lack of comprehensive evaluation. While the methodological innovation is promising, the practical implications and generalizability of the findings warrant further investigation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"TEMPERA: Test-Time Prompt Editing via Reinforcement Learning\" by Zhang et al. proposes a method to enhance prompt design for large language models through automated, reinforcement learning-driven prompt editing at test time. The authors claim that their approach leverages prior knowledge to generate interpretable prompts, achieving a 5.33x improvement in sample efficiency compared to existing methods. The methodology involves a carefully designed action space for prompt adjustments, targeting query-dependent prompts to optimize performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its focus on optimizing prompt design, which is indeed a critical aspect of working with large language models. The use of reinforcement learning to facilitate test-time editing offers a systematic approach that could benefit practitioners in the field. However, the weaknesses are notable: the concepts presented feel derivative, lacking in originality, and the improvement claims require thorough validation. The paper does not sufficiently differentiate itself from previous works in the area, and the authors' interpretation of \"state-of-the-art\" could be seen as a low bar.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is average; while the methodology is described, it may not be easily reproducible due to a lack of detailed explanations and examples. The quality of the writing is acceptable, but the novelty is questionable, as the ideas presented are closely aligned with established concepts in the field. Overall, the paper does not break new ground, and the reproducibility of the results hinges on transparent sharing of code and datasets, which is not clearly addressed.\n\n# Summary Of The Review\nIn conclusion, while the paper presents a structured approach to prompt editing via reinforcement learning, it struggles with originality and fails to provide substantial new insights into the field. The claimed improvements, although impressive, require further scrutiny to ascertain their validity.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel framework that leverages reinforcement learning (RL) for prompt editing in large language models. The main contributions include a flexible action space for prompt modifications, the incorporation of human prior knowledge, and significant improvements in sample efficiency (5.33x) compared to traditional fine-tuning methods. The authors evaluate TEMPERA's performance across various NLP tasks, demonstrating state-of-the-art results while highlighting the varying impacts of different editing techniques on model performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to prompt design through RL, which addresses a critical aspect of enhancing the adaptability and effectiveness of large language models. The detailed evaluation across tasks showcases the framework's robustness. However, there are areas for improvement, such as the need for a more granular analysis of specific editing actions' effects on performance, the exploration of alternative architectures for the RL agent, and the establishment of a systematic framework for integrating human knowledge into the prompt editing process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to a broad audience. The methodology is sound and reproducible, though the incorporation of additional contextual features and a more diverse array of editing techniques could enhance the framework's applicability. The novelty of using RL for prompt editing is significant, yet exploring meta-learning and hybrid models could further push the boundaries of this research.\n\n# Summary Of The Review\nOverall, the paper offers a substantial contribution to the field of NLP by introducing a reinforcement learning-based approach to prompt editing. While it establishes a strong foundation, there is potential for further exploration of human knowledge integration and alternative architectures to enhance model performance and adaptability in real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel methodology for few-shot text classification that achieves state-of-the-art (SoTA) performance across multiple benchmarks. TEMPERA's main contributions include significant improvements in performance metrics, such as a 1.8% absolute gain on the SST-2 task and a 3.9% gain on the CR task, alongside competitive results when compared to fine-tuning methods on datasets like AG News. The authors demonstrate TEMPERA's data efficiency, requiring approximately 5.33 times fewer training examples than traditional fine-tuning approaches, while also showing stability across various few-shot datasets and outperforming a range of baseline methods through the use of test-time editing and query-dependent prompts.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its impressive empirical results, especially in demonstrating superior performance on challenging tasks and its data efficiency. The comparative analysis against numerous baselines highlights the effectiveness of TEMPERA, particularly its innovative approach to test-time editing which enhances interpretability and flexibility in prompt design. However, the paper could benefit from a deeper discussion on the practical implications of using TEMPERA in real-world applications and potential limitations, such as scalability or computational overhead associated with the editing process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, presenting its methodology and results in an accessible manner. The quality of the experiments is high, with thorough comparisons and ablation studies that reinforce the claims made. In terms of novelty, TEMPERA introduces a fresh perspective on few-shot learning through its focus on editing techniques, which is a relatively underexplored area. However, the reproducibility of the results would benefit from more detailed descriptions of experimental setups and hyperparameter settings used in the evaluations.\n\n# Summary Of The Review\nOverall, TEMPERA represents a significant advancement in the field of few-shot text classification, showcasing both strong empirical performance and methodological innovation. While the paper is well-written and provides valuable insights, it could improve in discussing real-world applicability and reproducibility details.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents TEMPERA, a novel approach for test-time prompt editing using reinforcement learning. The methodology integrates prompt tuning strategies with reinforcement learning to adaptively modify prompts for improved model performance during inference. The findings demonstrate that TEMPERA achieves significant gains in data efficiency and overall model accuracy compared to baseline methods, suggesting its potential for real-world applications in natural language processing tasks.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative integration of reinforcement learning with prompt tuning, which addresses a pertinent challenge in adapting models during test time. However, the paper suffers from several weaknesses, including a convoluted methodology section that may hinder comprehension, grammatical errors, and inconsistent formatting that detracts from its professional presentation. Furthermore, the abstract and title could be clearer in conveying the significance of the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by grammatical issues and inconsistent terminology, which could confuse readers. While the novel approach is noteworthy, the complexity of the methodology could pose challenges for reproducibility. The inclusion of more structured explanations and a clearer presentation of results would enhance the overall quality and accessibility of the work.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting and potentially impactful method for prompt editing via reinforcement learning. Nonetheless, it requires significant improvements in clarity, grammatical accuracy, and consistency to strengthen its contribution and ensure that readers can fully grasp and replicate the proposed methodology.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.6717415285843162,
    -1.7281382148676132,
    -1.7992847548523314,
    -1.6982553465014778,
    -2.0692803634737214,
    -1.8329694768935467,
    -1.6320772102585948,
    -1.9598458249260624,
    -1.722543663113515,
    -1.843715515420489,
    -1.6281951062412898,
    -1.2659401925235745,
    -1.7285516959600986,
    -1.60875065649708,
    -1.6839230535232255,
    -1.8340585963795402,
    -2.0280051923824614,
    -1.7438054000856544,
    -1.8690303502422536,
    -1.7009402650002072,
    -1.8525468733255663,
    -1.8452361340220502,
    -1.9320451570714008,
    -1.766528445776042,
    -1.6025827969631283,
    -1.9235555430655102,
    -1.7617538698748847,
    -1.6957604242401318,
    -1.8081115752207677
  ],
  "logp_cond": [
    [
      0.0,
      -2.1483945725301314,
      -2.2388767124099327,
      -2.167192405735277,
      -2.1847438835480424,
      -2.2223691998630595,
      -2.2697058206877143,
      -2.2550129909314482,
      -2.1525267519119353,
      -2.173692494073902,
      -2.2158043302357973,
      -2.3891912628647276,
      -2.133303557612185,
      -2.1742653702967014,
      -2.2332410438340795,
      -2.132154759160068,
      -2.3084384734115497,
      -2.1588112356453,
      -2.247865128934118,
      -2.2190595898085634,
      -2.2676757768738334,
      -2.240310954787004,
      -2.244240998917677,
      -2.1918526173373567,
      -2.2759140842365526,
      -2.1217536395762115,
      -2.194419917954947,
      -2.222468348594279,
      -2.2902000213657265
    ],
    [
      -1.2652296840866901,
      0.0,
      -1.2817945921423382,
      -1.182408178341955,
      -1.2491538692478446,
      -1.2919787648089271,
      -1.302014308008263,
      -1.3008457799133266,
      -1.2024131943372658,
      -1.267493820263969,
      -1.25854576186655,
      -1.4236915376139423,
      -1.1939983005587929,
      -1.273457212841355,
      -1.301002605003637,
      -1.1202000597414963,
      -1.3188495322424767,
      -1.266519969855961,
      -1.2696389457935713,
      -1.2224844621947037,
      -1.3464666266222682,
      -1.3037415369548593,
      -1.3106141281497485,
      -1.3549302614461018,
      -1.3630227706915332,
      -1.2141319239973665,
      -1.2784360222163829,
      -1.3293428393491578,
      -1.34734921116605
    ],
    [
      -1.425867992408747,
      -1.3293057721434862,
      0.0,
      -1.3224312383437276,
      -1.3506370437229094,
      -1.336427948109483,
      -1.3968811472880236,
      -1.327381909754721,
      -1.3010900731165467,
      -1.4101614434918799,
      -1.3173787071775223,
      -1.4537192725324777,
      -1.3661558169865675,
      -1.340572384428698,
      -1.3581474069041684,
      -1.3082729688849022,
      -1.3477463081161307,
      -1.3443293752919148,
      -1.3821090890891226,
      -1.346938481202909,
      -1.3584245392910654,
      -1.3956706474935614,
      -1.3728276783475084,
      -1.4249676734019086,
      -1.4149455726463034,
      -1.337256421557176,
      -1.4033330645129376,
      -1.350386317747525,
      -1.3707061160701328
    ],
    [
      -1.2123113099150442,
      -1.0946704340321143,
      -1.2475644110802169,
      0.0,
      -1.2017209845870007,
      -1.2134340293551429,
      -1.2303515427161495,
      -1.2291964685777643,
      -1.154864622624627,
      -1.1766393560831125,
      -1.1624028460238398,
      -1.372508634917293,
      -1.1931978888902162,
      -1.189574327741213,
      -1.2190144697137346,
      -1.0542605660354403,
      -1.3020417985790815,
      -1.2468690468887953,
      -1.2475359722025827,
      -1.1585960909297928,
      -1.2566118868826228,
      -1.2309654045207647,
      -1.1886452725175711,
      -1.2518691597497078,
      -1.3401522878809202,
      -1.1449750596019646,
      -1.2100762470850868,
      -1.2663179209631326,
      -1.2737208925090384
    ],
    [
      -1.6123445266669052,
      -1.5908997698325027,
      -1.6470184070464706,
      -1.6059846252042185,
      0.0,
      -1.6859041567607576,
      -1.6802469552633996,
      -1.6584396402950017,
      -1.6176584195259263,
      -1.5585067356381672,
      -1.655749085116832,
      -1.732199116107914,
      -1.5947252597937134,
      -1.6012622319248992,
      -1.6925072441302864,
      -1.6154448214722184,
      -1.6614642206043055,
      -1.6210036809097659,
      -1.65755306695765,
      -1.631984830734945,
      -1.5663545736227367,
      -1.6211284161431023,
      -1.6543908082378798,
      -1.6366608939644978,
      -1.5978949839679295,
      -1.621737313038441,
      -1.6165537751875885,
      -1.6595542275470543,
      -1.6502305770795132
    ],
    [
      -1.4135069098135298,
      -1.4085642867482808,
      -1.3955610824398894,
      -1.3656072650782674,
      -1.4050357808095881,
      0.0,
      -1.3242653272891172,
      -1.3937409013432291,
      -1.3900206186005029,
      -1.396340082229995,
      -1.34753196326226,
      -1.5102237403532903,
      -1.3772157803231828,
      -1.4284438802795458,
      -1.389793435973234,
      -1.349833898780357,
      -1.4537223364526255,
      -1.3665159154708388,
      -1.3936847214456456,
      -1.3386692518291778,
      -1.4515774970422253,
      -1.4009964179031047,
      -1.407066527542733,
      -1.4574353736840517,
      -1.4673018513315317,
      -1.3592842584018403,
      -1.4322984386474087,
      -1.3845702450012638,
      -1.426476105792151
    ],
    [
      -1.2603279576294326,
      -1.2338978662448388,
      -1.2343130778471239,
      -1.2150798350628296,
      -1.1947995964895926,
      -1.180419571820482,
      0.0,
      -1.1958080820815211,
      -1.191796490435973,
      -1.2487934904125668,
      -1.1914857512373311,
      -1.305167277885897,
      -1.2140999690910739,
      -1.2277831384688995,
      -1.2383574798811081,
      -1.2038218669059477,
      -1.2160941461703656,
      -1.1985258291998862,
      -1.1752541119344073,
      -1.2010208960508302,
      -1.2230809973520635,
      -1.2539555787811065,
      -1.2037822197511945,
      -1.254240494388158,
      -1.2368653611712284,
      -1.1984489172958956,
      -1.2536909145791082,
      -1.2359418171924454,
      -1.2572736287998834
    ],
    [
      -1.5377278638739287,
      -1.4399939556307528,
      -1.4437388448784192,
      -1.462773957481287,
      -1.516560680410118,
      -1.5157074732622093,
      -1.5335851692932174,
      0.0,
      -1.4600965068180511,
      -1.549705925596388,
      -1.477607925882829,
      -1.6722065483575717,
      -1.4810535628397434,
      -1.5222402794865235,
      -1.5187614121081932,
      -1.4834522479947039,
      -1.4994379172407066,
      -1.521498279072747,
      -1.5127013766508624,
      -1.4963850487572645,
      -1.546292613462837,
      -1.5963736237694874,
      -1.5335524483922778,
      -1.5493151131053857,
      -1.5847564557947893,
      -1.5129205304548616,
      -1.4758758352445758,
      -1.5718556427885686,
      -1.5882231113417609
    ],
    [
      -1.3201738350141552,
      -1.3077305678699194,
      -1.3516173359882717,
      -1.3284396223697044,
      -1.357621188675028,
      -1.3903536167461046,
      -1.3595750580260182,
      -1.3166700541675802,
      0.0,
      -1.3814439222251516,
      -1.3574551203362486,
      -1.4619267824390265,
      -1.3883214371877575,
      -1.3896479806366124,
      -1.3777194298206314,
      -1.3094996203653995,
      -1.3712699340652634,
      -1.4004076923137883,
      -1.3321199588318589,
      -1.3625957849893924,
      -1.348357753313079,
      -1.4404918865334493,
      -1.3337797476737383,
      -1.3944712589511237,
      -1.4309358739101619,
      -1.3329161868273642,
      -1.3773426891428262,
      -1.4100153182886797,
      -1.398218452474248
    ],
    [
      -1.4411001549037366,
      -1.395137251359075,
      -1.465641454544022,
      -1.3818430905764039,
      -1.3386089086211828,
      -1.458572308456756,
      -1.4819452659169545,
      -1.4752724009676224,
      -1.4362826261137915,
      0.0,
      -1.4366775644535092,
      -1.5709003578419964,
      -1.4325382762460068,
      -1.3332561562381544,
      -1.488145291874753,
      -1.4110783166670633,
      -1.457673396035064,
      -1.4365528688681775,
      -1.4456758441419493,
      -1.3780549306565941,
      -1.4001719531864187,
      -1.43295553808428,
      -1.4259905624676763,
      -1.4330173147677931,
      -1.443286674803527,
      -1.342012631060791,
      -1.4062965237952494,
      -1.498966756979143,
      -1.4675041748935087
    ],
    [
      -1.246082996523654,
      -1.127743021552717,
      -1.2197004019514428,
      -1.175636156171227,
      -1.203523854844936,
      -1.2052583089046847,
      -1.2215752076499007,
      -1.197590008596253,
      -1.170818143947808,
      -1.246751330614109,
      0.0,
      -1.322306872042336,
      -1.2400900377014545,
      -1.221048780908862,
      -1.1954252839925903,
      -1.1834599820756042,
      -1.2630310513658392,
      -1.2738275188171626,
      -1.2131284542433896,
      -1.1445779473498803,
      -1.240193705383774,
      -1.2499496053394177,
      -1.204676301587405,
      -1.3203325036015967,
      -1.3066328827668263,
      -1.1842339520854783,
      -1.2375466669409059,
      -1.2551723308036185,
      -1.260918809654554
    ],
    [
      -1.0239714366685662,
      -1.0296447472825268,
      -1.0449407599730773,
      -1.0077397925454976,
      -0.9631211157951897,
      -1.0170355820342438,
      -0.9885190857494612,
      -1.0227807252518357,
      -1.009473896829404,
      -0.9951341028034658,
      -1.0023024601233843,
      0.0,
      -1.0320882022113196,
      -0.9724060769624921,
      -1.018073833588755,
      -0.999270865195343,
      -1.0159989728105383,
      -1.0293083810813848,
      -1.0170074922479455,
      -0.9991047619382388,
      -0.987171428937997,
      -0.9648974721889699,
      -1.0325260626191064,
      -1.0061611538615196,
      -0.9815811173252801,
      -0.9847111967027349,
      -1.0193914601345708,
      -1.0271892519062564,
      -0.9682931563264563
    ],
    [
      -1.16507963858707,
      -1.1503578722715324,
      -1.188721269128147,
      -1.1703985467007663,
      -1.14365932140212,
      -1.2544157166659673,
      -1.2321244453440048,
      -1.2173665078648481,
      -1.1964058513832367,
      -1.215886324364485,
      -1.2141224091058058,
      -1.4144042386759665,
      0.0,
      -1.1975583357356008,
      -1.2650743120925663,
      -1.1766587163534448,
      -1.2817433992673548,
      -1.1134127255258484,
      -1.2499505743353823,
      -1.2300844101440132,
      -1.2390132266953173,
      -1.2581041763183547,
      -1.2590170801944363,
      -1.2618410719556854,
      -1.2999234814355867,
      -1.1891751898978193,
      -1.1512801688090801,
      -1.2112495995025967,
      -1.2855428632127044
    ],
    [
      -1.124263449135023,
      -1.0993617259761437,
      -1.161199289735437,
      -1.085468807637289,
      -1.070472274644675,
      -1.1757609049961046,
      -1.1880357543093119,
      -1.1582832684156386,
      -1.1472548714399005,
      -1.079123895413718,
      -1.118907124445971,
      -1.2521658951786547,
      -1.1381212303416608,
      0.0,
      -1.1750995797733883,
      -1.0931755513067702,
      -1.2249836538911651,
      -1.1408014887226416,
      -1.13987716794713,
      -1.127663570766651,
      -1.1700686043571225,
      -1.1775219559367673,
      -1.2009315248472487,
      -1.1327374116875562,
      -1.2134033664526636,
      -1.070775893359622,
      -1.1781527162308159,
      -1.2013471483783436,
      -1.1993644363151612
    ],
    [
      -1.223606036888964,
      -1.1618305461292557,
      -1.2028950238942007,
      -1.150758242446913,
      -1.1845224519095228,
      -1.1264625030688273,
      -1.195432456362319,
      -1.2008810665437515,
      -1.149576869686433,
      -1.19757924487917,
      -1.1723375855734324,
      -1.3178345775942273,
      -1.2184471943470854,
      -1.1876308136385454,
      0.0,
      -1.1269256284680262,
      -1.1953200793820575,
      -1.1565044907938777,
      -1.168300874690924,
      -1.1566628968156332,
      -1.2236567453946232,
      -1.2335282172202597,
      -1.2083999261656215,
      -1.2095000324755705,
      -1.2730913483636246,
      -1.1723522708852754,
      -1.190397039896487,
      -1.2578839498627383,
      -1.1932383767677675
    ],
    [
      -1.327266520230083,
      -1.243070306439275,
      -1.3829089738009188,
      -1.234776526871141,
      -1.348524461942259,
      -1.3765313095701834,
      -1.4258760412543248,
      -1.404363182007817,
      -1.3034700830474126,
      -1.3630795101477415,
      -1.374571376966312,
      -1.538898764139991,
      -1.3614441240185937,
      -1.35675065599554,
      -1.3966854262074406,
      0.0,
      -1.4323470151177904,
      -1.390336819578682,
      -1.3887901137417638,
      -1.3664549084233943,
      -1.4128941882015347,
      -1.4091707518869645,
      -1.3886455094654384,
      -1.4065722236960576,
      -1.468242802718193,
      -1.307004151400468,
      -1.3442867692457374,
      -1.4039448365685714,
      -1.4618435504281868
    ],
    [
      -1.6189673751225682,
      -1.636357745361878,
      -1.6318795438368263,
      -1.631527180433914,
      -1.6146804807301474,
      -1.6390839926018193,
      -1.634171186170514,
      -1.5686545321111567,
      -1.5884751927083878,
      -1.6126276222813247,
      -1.6335413992157979,
      -1.7626049199322587,
      -1.6300568953848646,
      -1.6440755390453268,
      -1.6368599649718576,
      -1.6643862301921244,
      0.0,
      -1.641442728977643,
      -1.6004136039309353,
      -1.6191098621345539,
      -1.5900967289307462,
      -1.6744542994844838,
      -1.6338704042174732,
      -1.6716745740630883,
      -1.6269198853011995,
      -1.612470630236328,
      -1.6313417039951703,
      -1.6695511524347997,
      -1.6522252880951231
    ],
    [
      -1.305802767955371,
      -1.3027430319699325,
      -1.3228881474546232,
      -1.2657439218832154,
      -1.2379923697700432,
      -1.3268728088584993,
      -1.3071731328088048,
      -1.3616217132016353,
      -1.2774352983383073,
      -1.2991261603781534,
      -1.3294279964930196,
      -1.4732768147020563,
      -1.265366363843386,
      -1.2848292650629085,
      -1.3376617172574048,
      -1.2671690790478092,
      -1.3342309469267628,
      0.0,
      -1.277803988941791,
      -1.2810729915249193,
      -1.3450608138619036,
      -1.4103953379795924,
      -1.3539077304955929,
      -1.3672256723478033,
      -1.4007755127641464,
      -1.2947591102623721,
      -1.2858625966138375,
      -1.2894513565827659,
      -1.3826786544843663
    ],
    [
      -1.4696018268218942,
      -1.4363185263475748,
      -1.4594777700243229,
      -1.468783825213726,
      -1.4586662501552856,
      -1.4498495293911233,
      -1.4323985408349824,
      -1.4540617823997348,
      -1.4568534051249187,
      -1.468949094275302,
      -1.4318803894798198,
      -1.5971911873276168,
      -1.482093286148393,
      -1.4581822108271711,
      -1.4994669850881444,
      -1.4095070650038057,
      -1.4446532570926414,
      -1.4565476035104439,
      0.0,
      -1.4679570276276983,
      -1.4660662522492098,
      -1.4928930178392639,
      -1.5142309416089565,
      -1.5084080128464041,
      -1.550761569684444,
      -1.3799350883391708,
      -1.500362836098211,
      -1.5274705246098552,
      -1.5264525024796136
    ],
    [
      -1.2720011268646205,
      -1.2317576138026458,
      -1.3123933215778263,
      -1.261948710643279,
      -1.239010493105344,
      -1.258373309918676,
      -1.2231220642635556,
      -1.2481853398173683,
      -1.2390577840848616,
      -1.206583870629305,
      -1.2390358423527614,
      -1.3962284286641637,
      -1.3018344803022,
      -1.3012293355243776,
      -1.3104140131595345,
      -1.2478614486109174,
      -1.264230907299235,
      -1.2874245288245394,
      -1.2346965772513085,
      0.0,
      -1.2457618398394064,
      -1.3193483429337918,
      -1.228954727312837,
      -1.318405311952014,
      -1.3194128637578033,
      -1.239312422493991,
      -1.2813131789228827,
      -1.2989282963786613,
      -1.3026120358270574
    ],
    [
      -1.4861954323349917,
      -1.4551593821922824,
      -1.436087129692754,
      -1.465692581266901,
      -1.352115991048323,
      -1.5031147982063058,
      -1.4919325261808407,
      -1.4608499868281695,
      -1.4594994370450558,
      -1.4093410020804946,
      -1.4732021342647974,
      -1.543019504578407,
      -1.4711217283097968,
      -1.4378546386691702,
      -1.4922921928786694,
      -1.4575444336216226,
      -1.4654870649343805,
      -1.4920532365054853,
      -1.4894776376754781,
      -1.4613660984624286,
      0.0,
      -1.4789172430324344,
      -1.457935635240581,
      -1.4828890159262769,
      -1.4459492214576215,
      -1.4787345577532374,
      -1.4629725458455392,
      -1.522328136289944,
      -1.467720118363187
    ],
    [
      -1.3582320282664806,
      -1.3445115578531472,
      -1.402072548859615,
      -1.3855674906983886,
      -1.355790528074167,
      -1.3787811129240053,
      -1.374710503748927,
      -1.39715234522085,
      -1.395147916254922,
      -1.330147587898439,
      -1.3945012432010098,
      -1.496271302024743,
      -1.313987859924087,
      -1.320150677622805,
      -1.4074128938028432,
      -1.357143176965998,
      -1.3915776720646764,
      -1.3971352286879763,
      -1.3828978804619183,
      -1.3908489617912734,
      -1.3565860873308129,
      0.0,
      -1.393843955891946,
      -1.3173652158891727,
      -1.3719970080937607,
      -1.3253412894807657,
      -1.3760023942229151,
      -1.404977580968889,
      -1.3149790028525208
    ],
    [
      -1.538351510689044,
      -1.4696512762911198,
      -1.5005165854503215,
      -1.4641033909699712,
      -1.48798140825622,
      -1.501552700521557,
      -1.4944987996914667,
      -1.5141307096749546,
      -1.5035081340486844,
      -1.5024935010510243,
      -1.4735874408643828,
      -1.614923835671844,
      -1.5487065283015364,
      -1.5023338449860588,
      -1.5240128826142796,
      -1.4624954269829045,
      -1.508714572783413,
      -1.5173720697398527,
      -1.5060891713708258,
      -1.4526435460704823,
      -1.5129978232442374,
      -1.5435725752233427,
      0.0,
      -1.5813131179817603,
      -1.4857876264079173,
      -1.4970048753624947,
      -1.4869506086351911,
      -1.5548353571757638,
      -1.5562950056439375
    ],
    [
      -1.3624037611813575,
      -1.3603567260776397,
      -1.3942822614544408,
      -1.3109300811488944,
      -1.333409591409901,
      -1.3785675292195108,
      -1.3438706368727942,
      -1.3589875443480592,
      -1.3165939218516183,
      -1.3273825238038957,
      -1.3830380807389897,
      -1.448676371742832,
      -1.3734951009719876,
      -1.2966122876438717,
      -1.3364807439010757,
      -1.3008077653371908,
      -1.3576138012763421,
      -1.3268080187282143,
      -1.3464477239557695,
      -1.3775699463088609,
      -1.2938393794235463,
      -1.3515255411327718,
      -1.350832919594275,
      0.0,
      -1.3899220622382493,
      -1.2916043359613463,
      -1.3682303696846871,
      -1.363743479223441,
      -1.3418040922634664
    ],
    [
      -1.205119141755974,
      -1.219309873676261,
      -1.2292705569815734,
      -1.2699287169366704,
      -1.174107229679616,
      -1.237677828919288,
      -1.2395088985134137,
      -1.22719427921173,
      -1.239975611683145,
      -1.181628775015289,
      -1.21844131980127,
      -1.3092759478132108,
      -1.2292990713024388,
      -1.1842803921018175,
      -1.2535293310500315,
      -1.2392773285984753,
      -1.214421299991401,
      -1.228281283639082,
      -1.2430052593129186,
      -1.216118296271797,
      -1.1830819332358893,
      -1.2121012365989727,
      -1.1875536505599658,
      -1.2476439684415805,
      0.0,
      -1.2452722584227052,
      -1.1983138563915734,
      -1.1934743412181905,
      -1.207660895117626
    ],
    [
      -1.4643765726400106,
      -1.4162239366755653,
      -1.497634200177728,
      -1.472822747546448,
      -1.4578845618080158,
      -1.4782465497233124,
      -1.4970577212129914,
      -1.4668035016864094,
      -1.4682138616793372,
      -1.4265839307888215,
      -1.4691617736921954,
      -1.6310928435253658,
      -1.4796965491890408,
      -1.4670197230510398,
      -1.5170500638158475,
      -1.425202736432955,
      -1.5219607178841619,
      -1.4845677749580848,
      -1.4706392922348337,
      -1.4671629807165092,
      -1.4636868216314602,
      -1.4811441003926424,
      -1.5012604158408784,
      -1.5048877440807886,
      -1.5551624349325033,
      0.0,
      -1.448137559984569,
      -1.511411636921062,
      -1.5229275190387361
    ],
    [
      -1.2829256946893566,
      -1.2649845588822248,
      -1.321353223676178,
      -1.2830607606471536,
      -1.3038664018381072,
      -1.3267317546003932,
      -1.3623022605039834,
      -1.2988102844793024,
      -1.27223189405986,
      -1.306293057055326,
      -1.3141326518490126,
      -1.4634816177712315,
      -1.2918654206719904,
      -1.3249506830314408,
      -1.3448907282958351,
      -1.285290451632716,
      -1.3445868309519262,
      -1.3139202679972963,
      -1.3295552020906132,
      -1.2835552815853135,
      -1.3509838583069145,
      -1.370206811672208,
      -1.2975121845177882,
      -1.3739971466105,
      -1.3797524093647988,
      -1.2621325758104778,
      0.0,
      -1.3440282445153702,
      -1.3644294988135381
    ],
    [
      -1.3294605464594222,
      -1.3652605668112983,
      -1.3983658305123778,
      -1.3565931566025453,
      -1.3851868363644342,
      -1.3328876998667325,
      -1.3907084442120239,
      -1.3371166476354446,
      -1.353932355990662,
      -1.3716423764990688,
      -1.3539683180521191,
      -1.460412634012304,
      -1.3616113210790581,
      -1.343057173499161,
      -1.3886359809233524,
      -1.3592108847048818,
      -1.3616982467861578,
      -1.3343102297654021,
      -1.3570882510886018,
      -1.3672824528394498,
      -1.363891452767913,
      -1.3788938278559655,
      -1.3821981664694292,
      -1.3456177237936244,
      -1.3760480200893002,
      -1.3137509506715197,
      -1.3367343442222321,
      0.0,
      -1.4019921783399372
    ],
    [
      -1.3737441497334915,
      -1.3880453266921198,
      -1.3761574151180542,
      -1.35751729921585,
      -1.2932809488965178,
      -1.3446312508159926,
      -1.3347824574725313,
      -1.3778328555534467,
      -1.3348328493702712,
      -1.3056497147928152,
      -1.3642008021094623,
      -1.406333684543655,
      -1.4092245120539462,
      -1.3120961311195527,
      -1.3275803609038885,
      -1.3751302391643037,
      -1.3290338186106943,
      -1.3438237312255221,
      -1.325057349453765,
      -1.3426992222067486,
      -1.2903203951657287,
      -1.2513148955998887,
      -1.3615761827671948,
      -1.294887740724991,
      -1.3543592798692772,
      -1.3558692992872492,
      -1.3895268209972262,
      -1.4107103511276278,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.5233469560541848,
      0.43286481617438355,
      0.5045491228490393,
      0.48699764503627385,
      0.44937232872125676,
      0.4020357078966019,
      0.416728537652868,
      0.5192147766723809,
      0.49804903451041405,
      0.455937198348519,
      0.28255026571958863,
      0.5384379709721312,
      0.4974761582876148,
      0.43850048475023673,
      0.5395867694242482,
      0.3633030551727665,
      0.512930292939016,
      0.4238763996501982,
      0.4526819387757528,
      0.40406575171048287,
      0.4314305737973121,
      0.4275005296666392,
      0.47988891124695954,
      0.3958274443477636,
      0.5499878890081047,
      0.4773216106293692,
      0.44927317999003735,
      0.3815415072185897
    ],
    [
      0.4629085307809231,
      0.0,
      0.44634362272527506,
      0.5457300365256581,
      0.47898434561976866,
      0.43615945005868606,
      0.4261239068593503,
      0.4272924349542866,
      0.5257250205303474,
      0.46064439460364426,
      0.4695924530010631,
      0.3044466772536709,
      0.5341399143088204,
      0.45468100202625816,
      0.42713560986397625,
      0.6079381551261169,
      0.4092886826251365,
      0.46161824501165216,
      0.4584992690740419,
      0.5056537526729095,
      0.381671588245345,
      0.42439667791275393,
      0.4175240867178647,
      0.37320795342151136,
      0.36511544417608,
      0.5140062908702467,
      0.44970219265123035,
      0.39879537551845545,
      0.3807890037015631
    ],
    [
      0.37341676244358446,
      0.4699789827088452,
      0.0,
      0.47685351650860386,
      0.44864771112942203,
      0.46285680674284846,
      0.4024036075643078,
      0.4719028450976104,
      0.49819468173578474,
      0.38912331136045153,
      0.48190604767480916,
      0.34556548231985373,
      0.4331289378657639,
      0.45871237042363333,
      0.44113734794816306,
      0.4910117859674292,
      0.4515384467362007,
      0.45495537956041665,
      0.41717566576320886,
      0.45234627364942237,
      0.440860215561266,
      0.40361410735877,
      0.426457076504823,
      0.37431708145042286,
      0.38433918220602803,
      0.46202833329515536,
      0.3959516903393938,
      0.44889843710480637,
      0.4285786387821986
    ],
    [
      0.4859440365864336,
      0.6035849124693635,
      0.45069093542126093,
      0.0,
      0.4965343619144771,
      0.4848213171463349,
      0.46790380378532825,
      0.4690588779237135,
      0.5433907238768507,
      0.5216159904183653,
      0.535852500477638,
      0.3257467115841848,
      0.5050574576112616,
      0.5086810187602648,
      0.4792408767877432,
      0.6439947804660375,
      0.39621354792239627,
      0.4513862996126825,
      0.4507193742988951,
      0.539659255571685,
      0.441643459618855,
      0.46728994198071305,
      0.5096100739839067,
      0.44638618675177,
      0.3581030586205576,
      0.5532802868995133,
      0.48817909941639104,
      0.43193742553834524,
      0.4245344539924394
    ],
    [
      0.45693583680681615,
      0.47838059364121865,
      0.42226195642725073,
      0.4632957382695029,
      0.0,
      0.3833762067129638,
      0.38903340821032173,
      0.41084072317871967,
      0.4516219439477951,
      0.5107736278355541,
      0.4135312783568894,
      0.33708124736580736,
      0.474555103680008,
      0.4680181315488221,
      0.37677311934343494,
      0.453835542001503,
      0.4078161428694158,
      0.4482766825639555,
      0.41172729651607143,
      0.4372955327387764,
      0.5029257898509847,
      0.448151947330619,
      0.4148895552358416,
      0.4326194695092236,
      0.47138537950579185,
      0.4475430504352804,
      0.4527265882861329,
      0.40972613592666707,
      0.4190497863942082
    ],
    [
      0.41946256708001695,
      0.4244051901452659,
      0.4374083944536573,
      0.46736221181527937,
      0.4279336960839586,
      0.0,
      0.5087041496044296,
      0.4392285755503176,
      0.44294885829304387,
      0.4366293946635518,
      0.48543751363128673,
      0.3227457365402564,
      0.4557536965703639,
      0.4045255966140009,
      0.4431760409203127,
      0.48313557811318963,
      0.3792471404409212,
      0.4664535614227079,
      0.43928475544790113,
      0.4943002250643689,
      0.38139197985132145,
      0.431973058990442,
      0.42590294935081374,
      0.375534103209495,
      0.36566762556201504,
      0.4736852184917064,
      0.40067103824613803,
      0.4483992318922829,
      0.40649337110139583
    ],
    [
      0.37174925262916214,
      0.398179344013756,
      0.3977641324114709,
      0.4169973751957652,
      0.4372776137690022,
      0.4516576384381128,
      0.0,
      0.43626912817707364,
      0.44028071982262174,
      0.38328371984602794,
      0.4405914590212636,
      0.32690993237269783,
      0.4179772411675209,
      0.40429407178969523,
      0.39371973037748664,
      0.42825534335264703,
      0.41598306408822916,
      0.4335513810587086,
      0.45682309832418744,
      0.4310563142077646,
      0.4089962129065312,
      0.37812163147748823,
      0.4282949905074003,
      0.37783671587043677,
      0.3952118490873664,
      0.4336282929626991,
      0.37838629567948656,
      0.3961353930661493,
      0.3748035814587114
    ],
    [
      0.42211796105213373,
      0.5198518692953096,
      0.5161069800476432,
      0.4970718674447754,
      0.4432851445159445,
      0.44413835166385307,
      0.426260655632845,
      0.0,
      0.49974931810801126,
      0.4101398993296743,
      0.48223789904323344,
      0.28763927656849075,
      0.478792262086319,
      0.4376055454395389,
      0.44108441281786925,
      0.47639357693135853,
      0.4604079076853558,
      0.4383475458533155,
      0.44714444827520006,
      0.4634607761687979,
      0.41355321146322543,
      0.36347220115657497,
      0.42629337653378463,
      0.4105307118206767,
      0.3750893691312731,
      0.4469252944712008,
      0.48396998968148663,
      0.3879901821374938,
      0.37162271358430154
    ],
    [
      0.4023698280993597,
      0.41481309524359555,
      0.3709263271252432,
      0.39410404074381056,
      0.3649224744384869,
      0.33219004636741034,
      0.36296860508749673,
      0.4058736089459347,
      0.0,
      0.34109974088836337,
      0.3650885427772663,
      0.2606168806744884,
      0.33422222592575745,
      0.3328956824769025,
      0.3448242332928835,
      0.41304404274811546,
      0.3512737290482515,
      0.32213597079972667,
      0.39042370428165607,
      0.35994787812412254,
      0.37418590980043587,
      0.28205177658006564,
      0.3887639154397766,
      0.32807240416239125,
      0.29160778920335306,
      0.3896274762861507,
      0.3452009739706887,
      0.31252834482483527,
      0.3243252106392669
    ],
    [
      0.4026153605167524,
      0.448578264061414,
      0.378074060876467,
      0.4618724248440851,
      0.5051066067993062,
      0.38514320696373305,
      0.3617702495035344,
      0.3684431144528666,
      0.4074328893066974,
      0.0,
      0.4070379509669797,
      0.2728151575784925,
      0.4111772391744821,
      0.5104593591823345,
      0.35557022354573586,
      0.43263719875342566,
      0.3860421193854249,
      0.4071626465523115,
      0.3980396712785397,
      0.4656605847638948,
      0.4435435622340702,
      0.41075997733620895,
      0.41772495295281264,
      0.4106982006526958,
      0.40042884061696205,
      0.501702884359698,
      0.43741899162523956,
      0.344748758441346,
      0.37621134052698024
    ],
    [
      0.3821121097176359,
      0.5004520846885727,
      0.408494704289847,
      0.45255895007006286,
      0.42467125139635375,
      0.42293679733660516,
      0.40661989859138914,
      0.43060509764503685,
      0.4573769622934818,
      0.3814437756271809,
      0.0,
      0.3058882341989537,
      0.3881050685398353,
      0.4071463253324279,
      0.43276982224869953,
      0.4447351241656856,
      0.36516405487545067,
      0.3543675874241272,
      0.41506665199790027,
      0.48361715889140955,
      0.3880014008575159,
      0.37824550090187214,
      0.4235188046538849,
      0.3078626026396931,
      0.3215622234744635,
      0.4439611541558115,
      0.39064843930038395,
      0.3730227754376714,
      0.3672762965867358
    ],
    [
      0.24196875585500832,
      0.23629544524104773,
      0.22099943255049714,
      0.25820039997807687,
      0.3028190767283848,
      0.24890461048933066,
      0.27742110677411325,
      0.24315946727173876,
      0.25646629569417057,
      0.27080608972010867,
      0.2636377324001902,
      0.0,
      0.2338519903122549,
      0.29353411556108233,
      0.24786635893481956,
      0.26666932732823145,
      0.24994121971303618,
      0.2366318114421897,
      0.24893270027562897,
      0.26683543058533565,
      0.27876876358557745,
      0.30104272033460455,
      0.23341412990446808,
      0.2597790386620549,
      0.2843590751982944,
      0.2812289958208396,
      0.24654873238900366,
      0.23875094061731805,
      0.2976470361971182
    ],
    [
      0.5634720573730285,
      0.5781938236885662,
      0.5398304268319516,
      0.5581531492593323,
      0.5848923745579786,
      0.4741359792941313,
      0.49642725061609383,
      0.5111851880952505,
      0.5321458445768619,
      0.5126653715956135,
      0.5144292868542928,
      0.31414745728413207,
      0.0,
      0.5309933602244978,
      0.46347738386753234,
      0.5518929796066538,
      0.4468082966927438,
      0.6151389704342503,
      0.47860112162471635,
      0.49846728581608546,
      0.4895384692647813,
      0.4704475196417439,
      0.46953461576566236,
      0.4667106240044132,
      0.42862821452451194,
      0.5393765060622793,
      0.5772715271510185,
      0.5173020964575019,
      0.4430088327473942
    ],
    [
      0.48448720736205697,
      0.5093889305209363,
      0.44755136676164287,
      0.523281848859791,
      0.5382783818524048,
      0.4329897515009753,
      0.4207149021877681,
      0.4504673880814414,
      0.4614957850571795,
      0.529626761083362,
      0.489843532051109,
      0.3565847613184252,
      0.47062942615541914,
      0.0,
      0.43365107672369163,
      0.5155751051903097,
      0.3837670026059148,
      0.4679491677744383,
      0.4688734885499499,
      0.481087085730429,
      0.4386820521399575,
      0.4312287005603126,
      0.4078191316498312,
      0.4760132448095238,
      0.3953472900444164,
      0.5379747631374578,
      0.4305979402662641,
      0.4074035081187364,
      0.4093862201819187
    ],
    [
      0.4603170166342614,
      0.5220925073939697,
      0.4810280296290248,
      0.5331648110763125,
      0.4994006016137027,
      0.5574605504543981,
      0.48849059716090637,
      0.483041986979474,
      0.5343461838367924,
      0.48634380864405546,
      0.5115854679497931,
      0.3660884759289982,
      0.4654758591761401,
      0.4962922398846801,
      0.0,
      0.5569974250551992,
      0.48860297414116793,
      0.5274185627293477,
      0.5156221788323014,
      0.5272601567075923,
      0.46026630812860225,
      0.45039483630296573,
      0.47552312735760394,
      0.474423021047655,
      0.4108317051596009,
      0.5115707826379501,
      0.49352601362673854,
      0.42603910366048714,
      0.490684676755458
    ],
    [
      0.5067920761494571,
      0.5909882899402652,
      0.45114962257862135,
      0.5992820695083991,
      0.4855341344372812,
      0.4575272868093567,
      0.4081825551252154,
      0.4296954143717231,
      0.5305885133321275,
      0.4709790862317986,
      0.45948721941322823,
      0.2951598322395492,
      0.47261447236094645,
      0.47730794038400015,
      0.43737317017209953,
      0.0,
      0.4017115812617498,
      0.4437217768008581,
      0.4452684826377764,
      0.4676036879561458,
      0.4211644081780055,
      0.42488784449257566,
      0.44541308691410175,
      0.42748637268348255,
      0.3658157936613471,
      0.5270544449790722,
      0.48977182713380274,
      0.4301137598109688,
      0.37221504595135335
    ],
    [
      0.4090378172598932,
      0.3916474470205833,
      0.39612564854563503,
      0.39647801194854737,
      0.413324711652314,
      0.3889211997806421,
      0.3938340062119474,
      0.45935066027130467,
      0.4395299996740736,
      0.41537757010113663,
      0.39446379316666347,
      0.2654002724502027,
      0.3979482969975967,
      0.3839296533371346,
      0.39114522741060376,
      0.36361896219033696,
      0.0,
      0.38656246340481837,
      0.427591588451526,
      0.4088953302479075,
      0.4379084634517152,
      0.3535508928979776,
      0.39413478816498815,
      0.35633061831937307,
      0.4010853070812619,
      0.41553456214613327,
      0.39666348838729104,
      0.3584540399476617,
      0.3757799042873382
    ],
    [
      0.43800263213028345,
      0.44106236811572197,
      0.4209172526310312,
      0.47806147820243905,
      0.5058130303156112,
      0.4169325912271551,
      0.4366322672768497,
      0.38218368688401916,
      0.46637010174734717,
      0.444679239707501,
      0.41437740359263486,
      0.27052858538359814,
      0.47843903624226836,
      0.4589761350227459,
      0.40614368282824964,
      0.47663632103784526,
      0.4095744531588916,
      0.0,
      0.4660014111438635,
      0.46273240856073516,
      0.39874458622375086,
      0.33341006210606206,
      0.38989766959006156,
      0.3765797277378511,
      0.3430298873215081,
      0.4490462898232823,
      0.45794280347181693,
      0.45435404350288855,
      0.3611267456012881
    ],
    [
      0.3994285234203594,
      0.43271182389467877,
      0.40955258021793073,
      0.40024652502852764,
      0.410364100086968,
      0.4191808208511303,
      0.4366318094072712,
      0.4149685678425188,
      0.41217694511733494,
      0.40008125596695154,
      0.43714996076243384,
      0.27183916291463683,
      0.3869370640938605,
      0.41084813941508247,
      0.36956336515410926,
      0.4595232852384479,
      0.42437709314961225,
      0.41248274673180974,
      0.0,
      0.4010733226145553,
      0.4029640979930438,
      0.37613733240298974,
      0.35479940863329706,
      0.3606223373958495,
      0.3182687805578097,
      0.48909526190308283,
      0.36866751414404253,
      0.3415598256323984,
      0.34257784776264
    ],
    [
      0.4289391381355867,
      0.4691826511975614,
      0.38854694342238094,
      0.43899155435692827,
      0.4619297718948632,
      0.4425669550815312,
      0.4778182007366516,
      0.4527549251828389,
      0.4618824809153457,
      0.4943563943709022,
      0.4619044226474458,
      0.3047118363360435,
      0.3991057846980073,
      0.39971092947582965,
      0.3905262518406727,
      0.4530788163892898,
      0.4367093577009722,
      0.41351573617566784,
      0.4662436877488987,
      0.0,
      0.45517842516080087,
      0.38159192206641546,
      0.4719855376873703,
      0.3825349530481932,
      0.3815274012424039,
      0.46162784250621614,
      0.4196270860773246,
      0.402011968621546,
      0.3983282291731498
    ],
    [
      0.3663514409905746,
      0.397387491133284,
      0.41645974363281235,
      0.3868542920586653,
      0.5004308822772434,
      0.34943207511926055,
      0.3606143471447256,
      0.39169688649739687,
      0.39304743628051053,
      0.4432058712450717,
      0.37934473906076893,
      0.30952736874715936,
      0.3814251450157695,
      0.41469223465639615,
      0.3602546804468969,
      0.3950024397039438,
      0.3870598083911858,
      0.36049363682008106,
      0.3630692356500882,
      0.39118077486313774,
      0.0,
      0.3736296302931319,
      0.3946112380849853,
      0.36965785739928947,
      0.40659765186794483,
      0.37381231557232897,
      0.3895743274800272,
      0.33021873703562243,
      0.3848267549623794
    ],
    [
      0.4870041057555696,
      0.500724576168903,
      0.44316358516243515,
      0.45966864332366164,
      0.48944560594788333,
      0.46645502109804493,
      0.47052563027312333,
      0.44808378880120014,
      0.45008821776712815,
      0.5150885461236112,
      0.45073489082104046,
      0.3489648319973073,
      0.5312482740979632,
      0.5250854563992453,
      0.43782324021920704,
      0.48809295705605216,
      0.4536584619573738,
      0.4481009053340739,
      0.4623382535601319,
      0.45438717223077685,
      0.48865004669123735,
      0.0,
      0.45139217813010424,
      0.5278709181328776,
      0.47323912592828954,
      0.5198948445412845,
      0.4692337397991351,
      0.4402585530531613,
      0.5302571311695294
    ],
    [
      0.3936936463823568,
      0.462393880780281,
      0.43152857162107927,
      0.46794176610142957,
      0.44406374881518085,
      0.4304924565498438,
      0.4375463573799341,
      0.41791444739644623,
      0.42853702302271635,
      0.4295516560203765,
      0.45845771620701803,
      0.31712132139955673,
      0.38333862876986435,
      0.42971131208534197,
      0.4080322744571212,
      0.4695497300884963,
      0.42333058428798775,
      0.41467308733154806,
      0.425955985700575,
      0.4794016110009185,
      0.4190473338271634,
      0.3884725818480581,
      0.0,
      0.35073203908964046,
      0.4462575306634835,
      0.4350402817089061,
      0.44509454843620966,
      0.37720979989563697,
      0.37575015142746326
    ],
    [
      0.4041246845946844,
      0.40617171969840227,
      0.37224618432160117,
      0.45559836462714753,
      0.43311885436614106,
      0.3879609165565312,
      0.4226578089032478,
      0.4075409014279827,
      0.4499345239244237,
      0.4391459219721463,
      0.38349036503705225,
      0.31785207403321003,
      0.3930333448040544,
      0.46991615813217025,
      0.4300477018749662,
      0.4657206804388512,
      0.40891464449969983,
      0.43972042704782766,
      0.42008072182027245,
      0.3889584994671811,
      0.47268906635249563,
      0.41500290464327017,
      0.41569552618176697,
      0.0,
      0.37660638353779263,
      0.4749241098146957,
      0.3982980760913548,
      0.4027849665526009,
      0.42472435351257554
    ],
    [
      0.39746365520715443,
      0.38327292328686724,
      0.37331223998155494,
      0.3326540800264579,
      0.4284755672835123,
      0.3649049680438403,
      0.3630738984497146,
      0.37538851775139825,
      0.36260718527998326,
      0.4209540219478394,
      0.3841414771618583,
      0.29330684914991756,
      0.37328372566068957,
      0.4183024048613109,
      0.3490534659130968,
      0.36330546836465305,
      0.3881614969717273,
      0.3743015133240464,
      0.3595775376502097,
      0.3864645006913314,
      0.41950086372723905,
      0.3904815603641556,
      0.4150291464031626,
      0.3549388285215478,
      0.0,
      0.35731053854042316,
      0.40426894057155494,
      0.4091084557449378,
      0.39492190184550235
    ],
    [
      0.4591789704254996,
      0.5073316063899449,
      0.42592134288778216,
      0.4507327955190623,
      0.4656709812574944,
      0.44530899334219787,
      0.42649782185251883,
      0.45675204137910086,
      0.45534168138617304,
      0.49697161227668873,
      0.4543937693733149,
      0.2924626995401445,
      0.44385899387646943,
      0.45653582001447046,
      0.40650547924966274,
      0.4983528066325553,
      0.40159482518134837,
      0.43898776810742546,
      0.45291625083067655,
      0.45639256234900105,
      0.45986872143405,
      0.4424114426728678,
      0.4222951272246318,
      0.4186677989847216,
      0.36839310813300696,
      0.0,
      0.4754179830809413,
      0.4121439061444483,
      0.4006280240267741
    ],
    [
      0.4788281751855281,
      0.49676931099265986,
      0.44040064619870667,
      0.4786931092277311,
      0.4578874680367775,
      0.4350221152744915,
      0.3994516093709013,
      0.46294358539558234,
      0.48952197581502466,
      0.45546081281955875,
      0.44762121802587207,
      0.29827225210365316,
      0.4698884492028943,
      0.4368031868434439,
      0.4168631415790496,
      0.47646341824216876,
      0.4171670389229585,
      0.44783360187758836,
      0.4321986677842715,
      0.4781985882895712,
      0.4107700115679702,
      0.39154705820267677,
      0.46424168535709653,
      0.3877567232643846,
      0.3820014605100859,
      0.4996212940644069,
      0.0,
      0.4177256253595145,
      0.39732437106134655
    ],
    [
      0.3662998777807096,
      0.3304998574288336,
      0.297394593727754,
      0.33916726763758653,
      0.31057358787569767,
      0.36287272437339935,
      0.30505198002810796,
      0.35864377660468727,
      0.3418280682494699,
      0.324118047741063,
      0.3417921061880127,
      0.2353477902278278,
      0.3341491031610737,
      0.35270325074097086,
      0.30712444331677946,
      0.33654953953525,
      0.33406217745397404,
      0.3614501944747297,
      0.3386721731515301,
      0.328477971400682,
      0.33186897147221894,
      0.31686659638416637,
      0.3135622577707027,
      0.3501427004465074,
      0.3197124041508317,
      0.38200947356861215,
      0.3590260800178997,
      0.0,
      0.2937682459001947
    ],
    [
      0.43436742548727625,
      0.42006624852864793,
      0.4319541601027135,
      0.45059427600491775,
      0.5148306263242499,
      0.46348032440477516,
      0.4733291177482364,
      0.430278719667321,
      0.47327872585049646,
      0.5024618604279525,
      0.44391077311130545,
      0.4017778906771128,
      0.39888706316682154,
      0.496015444101215,
      0.4805312143168792,
      0.43298133605646405,
      0.47907775661007346,
      0.4642878439952456,
      0.48305422576700274,
      0.46541235301401906,
      0.517791180055039,
      0.556796679620879,
      0.4465353924535729,
      0.5132238344957767,
      0.45375229535149053,
      0.4522422759335185,
      0.41858475422354147,
      0.39740122409313994,
      0.0
    ]
  ],
  "row_avgs": [
    0.4548313163293833,
    0.44814693274416567,
    0.4352107402072579,
    0.481466456051336,
    0.43551599337462765,
    0.4317093378268015,
    0.4090726972528379,
    0.4382600981407031,
    0.35357515921413707,
    0.41103127990187466,
    0.40207967347638174,
    0.26023145712730444,
    0.5059598576397507,
    0.4571677078669879,
    0.48908175030375645,
    0.4548174926969753,
    0.3933080258859503,
    0.4227927107352607,
    0.39513676779762047,
    0.42846032871038703,
    0.3846592515153815,
    0.47255281076929817,
    0.4211014311533797,
    0.41703428157979083,
    0.37991306188306023,
    0.43898339048474905,
    0.43811702144913983,
    0.33120483074318824,
    0.4606037507710602
  ],
  "col_avgs": [
    0.42497819470864673,
    0.45920543549081,
    0.416036367884202,
    0.4531485616789856,
    0.45575765557238146,
    0.4213286246572444,
    0.4160259021208189,
    0.4232961747671376,
    0.4544686743612205,
    0.4423098863242488,
    0.43671352561153515,
    0.3082535379967129,
    0.42805220627479856,
    0.4405661801078968,
    0.4039255021500708,
    0.46373494625730377,
    0.40431416655535596,
    0.4287305645215917,
    0.4272777873602579,
    0.4438753011483682,
    0.4244371732626315,
    0.4006217028449169,
    0.41687015567217706,
    0.39894374924208637,
    0.3814211293166334,
    0.46156216978557346,
    0.42643901043478244,
    0.3987248497902378,
    0.39100647773392205
  ],
  "combined_avgs": [
    0.439904755519015,
    0.45367618411748784,
    0.42562355404572993,
    0.4673075088651608,
    0.44563682447350456,
    0.4265189812420229,
    0.4125492996868284,
    0.4307781364539204,
    0.4040219167876788,
    0.4266705831130617,
    0.41939659954395847,
    0.2842424975620087,
    0.46700603195727464,
    0.4488669439874423,
    0.44650362622691364,
    0.45927621947713954,
    0.39881109622065314,
    0.4257616376284262,
    0.41120727757893916,
    0.4361678149293776,
    0.4045482123890065,
    0.4365872568071075,
    0.4189857934127784,
    0.40798901541093857,
    0.3806670955998468,
    0.45027278013516125,
    0.43227801594196114,
    0.36496484026671305,
    0.4258051142524911
  ],
  "gppm": [
    567.1839742136948,
    558.2573572461172,
    577.4877563754137,
    562.8415534939189,
    556.5973162726709,
    576.8883424378548,
    579.4156007713781,
    572.459614360999,
    560.2813512713273,
    564.6620012131782,
    570.6733694144231,
    628.5524341262745,
    574.8840508510527,
    570.4320161952995,
    587.1197867580112,
    556.4776323399966,
    581.0293135335942,
    573.6936468485245,
    572.2715821064519,
    568.2107250177054,
    573.6183835587888,
    586.1529879403131,
    575.5533572361711,
    584.8035601498156,
    594.7787443876983,
    557.6523318059399,
    575.4909662844183,
    585.0211174842963,
    592.4795091624161
  ],
  "gppm_normalized": [
    1.3132786290175953,
    1.2804452330172693,
    1.3235656089296377,
    1.2865708096065098,
    1.269696305032771,
    1.3227222491559107,
    1.3311692175454133,
    1.3088035790837524,
    1.279820147206159,
    1.292885305798995,
    1.3069656025230858,
    1.4409858748763038,
    1.316476451430685,
    1.3026944047132332,
    1.3429266049395232,
    1.2760575504905507,
    1.3272114503881418,
    1.306922154916714,
    1.306282796845818,
    1.3038522074382635,
    1.3025367981211708,
    1.3402103492090653,
    1.30970792984826,
    1.338304881646618,
    1.3575647376147058,
    1.2764882056376858,
    1.315858192404042,
    1.3342413306276524,
    1.3552184838054584
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364,
    668,
    476,
    491,
    435,
    399,
    469,
    505,
    387,
    435,
    421,
    416,
    456,
    451,
    447,
    366,
    429,
    444,
    439,
    432,
    365,
    436,
    383,
    403,
    437,
    433,
    512,
    444,
    453,
    398,
    556,
    436,
    455,
    520,
    439,
    403,
    443,
    430,
    421,
    442,
    402,
    469,
    440,
    412,
    457,
    437,
    417,
    397,
    430,
    478,
    440,
    351,
    389,
    390,
    417,
    413,
    412,
    418,
    373,
    589,
    405,
    404,
    436,
    416,
    426,
    434,
    447,
    434,
    406,
    406,
    524,
    449,
    391,
    456,
    425,
    488,
    400,
    372,
    431,
    460,
    423,
    389,
    472,
    413,
    395,
    393,
    447,
    357,
    756,
    505,
    437,
    451,
    455,
    446,
    567,
    465,
    461,
    428,
    451,
    350,
    461,
    430,
    486,
    497,
    420,
    414,
    408,
    431,
    401,
    373,
    409,
    440,
    446,
    377,
    441,
    425,
    432,
    719,
    452,
    465,
    403,
    440,
    476,
    406,
    461,
    445,
    427,
    419,
    561,
    443,
    439,
    402,
    436,
    435,
    404,
    421,
    436,
    431,
    405,
    404,
    434,
    436,
    444,
    457,
    417,
    387,
    351,
    464,
    488,
    447,
    414,
    460,
    562,
    459,
    454,
    404,
    403,
    553,
    453,
    418,
    415,
    449,
    402,
    424,
    399,
    466,
    446,
    424,
    385,
    459,
    452,
    465,
    385,
    416,
    392,
    342,
    477,
    430,
    439,
    431,
    390,
    446,
    418,
    476,
    458,
    445,
    482,
    400,
    421,
    367,
    433,
    391,
    423,
    418,
    406,
    419,
    375,
    418,
    446,
    434,
    405,
    391,
    455,
    335
  ],
  "response_lengths": [
    1707,
    2709,
    2462,
    2478,
    2454,
    2323,
    2581,
    2430,
    2698,
    2597,
    2610,
    2717,
    2287,
    2356,
    2130,
    2366,
    2314,
    2438,
    2480,
    2313,
    2353,
    2067,
    2367,
    2587,
    2373,
    2194,
    2191,
    2473,
    1951
  ]
}