{
  "example_idx": 16,
  "reference": "Published as a conference paper at ICLR 2023\n\nLEARNING ZERO-SHOT COOPERATION WITH HUMANS, ASSUMING HUMANS ARE BIASED\n\nChao Yu1∗, Jiaxuan Gao1,3∗, Weilin Liu1, Botian Xu3, Hao Tang1, Jiaqi Yang2, Yu Wang1†, Yi Wu1,3† 1 Tsinghua University, 2 UC Berkeley, 3 Shanghai Qi Zhi Institute zoeyuchao@gmail.com\n\nABSTRACT\n\nThere is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.\n\n1\n\nINTRODUCTION\n\nBuilding intelligent agents that can interact with, cooperate and assist humans remains a longstanding AI challenge with decades of research efforts (Klien et al., 2004; Ajoudani et al., 2018; Dafoe et al., 2021). Classical approaches are typically model-based, which (repeatedly) build an effective behavior model over human data and plan with the human model (Sheridan, 2016; Carroll et al., 2019; Bobu et al., 2020). Despite great successes, this model-based paradigm requires an expensive and time-consuming data collection process, which can be particularly problematic for complex problems tackled by today’s AI techniques (Kidd & Breazeal, 2008; Biondi et al., 2019) and may also suffer from privacy issues (Pan et al., 2019).\n\nRecently, multi-agent reinforcement learning (MARL) has become a promising approach for many challenging decision-making problems. Particularly in competitive settings, AIs developed by MARL algorithms based on self-play (SP) defeated human professionals in a variety of domains (Silver et al., 2018; Vinyals et al., 2019; Berner et al., 2019). This empirical evidence suggests a new direction of developing strong AIs that can directly cooperate with humans in a similar “model-free” fashion, i.e., via self-play.\n\nDifferent from zero-sum games, where simply adopting a Nash equilibrium strategy is sufficient, an obvious issue when training cooperative agents by self-play is convention overfitting. Due to the existence of a large number of possible optimal strategies in a cooperative game, SP-trained agents can easily converge to a particular optimum and make decisions solely based on a specific behavior pattern, i.e., convention (Lowe et al., 2019; Hu et al., 2020), of its co-trainers, leading to poor generalization ability to unseen partners. To tackle this problem, recent works proposed a two-staged framework by first developing a diverse policy pool consisting of multiple SP-trained policies, which possibly cover different conventions, and then further training an adaptive policy against this policy pool (Lupu et al., 2021; Strouse et al., 2021; Zhao et al., 2021).\n\nDespite the empirical success of this two-staged framework, a fundamental drawback exists. Even though the policy pool prevents convention overfitting, each SP-trained policy in the pool remains a solution, which is either optimal or sub-optimal, to a fixed reward function specified by the underlying cooperative game. This implies a crucial generalization assumption that any test-time partner\n\n∗Equal Contribution †Equal Advising\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nwill be precisely optimizing the specified game reward. Such an assumption results in a pitfall in the case of cooperation with humans. Human behavior has been widely studied in cognitive science (Griffiths, 2015), economics (Wilkinson & Klaes, 2017) and game theory (Fang et al., 2021). Systematic research has shown that humans’ utility functions can be substantially biased even when a clear objective is given (Pratt, 1978; Selten, 1990; Camerer, 2011; Barberis, 2013), suggesting that human behaviors may be subject to an unknown reward function that is very different from the game reward (Nguyen et al., 2013). This fact reveals an algorithmic limitation of the existing SP-based methods.\n\nIn this work, we propose Hidden-Utility Self-Play (HSP), which extends the SP-based two-staged framework to the assumption of biased humans. HSP explicitly models the human bias via an additional hidden reward function in the self-play training objective. Solutions to such a generalized formulation are capable of representing any non-adaptive human strategies. We further present a tractable approximation of the hidden reward function space and perform a random search over this approximated space when building the policy pool in the first stage. Hence, the enhanced pool can capture a wide range of possible human biases beyond conventions (Hu et al., 2020; Zhao et al., 2021) and skill-levels (Dafoe et al., 2021) w.r.t. the game reward. Accordingly, the final adaptive policy derived in the second phase can have a much stronger adaptation capability to unseen humans.\n\nWe evaluate HSP in a popular human-AI cooperation benchmark, Overcooked (Carroll et al., 2019), which is a fully observable two-player cooperative game. We conduct comprehensive ablation studies and comparisons with baselines that do not explicitly model human biases. Empirical results show that HSP achieves superior performances when cooperating with behavior models learned from human data. In addition, we also consider a collection of manually scripted biased strategies, which are ensured to be sufficiently distinct from the policy pool, and HSP produces an even larger performance improvement over the baselines. Finally, we conduct real human studies. Collected feedbacks show that the human participants consistently feel that the agent trained by HSP is much more assistive than the baselines. We emphasize that, in addition to algorithmic contributions, our empirical analysis, which considers learned models, script policies and real humans as diverse testing partners, also provides a more thorough evaluation standard for learning human-assistive AIs.\n\n2 RELATED WORK\n\nThere is a broad literature on improving the zero-shot generalization ability of MARL agents to unseen partners (Kirk et al., 2021). Particularly for cooperative games, this problem is often called ad hoc team play (Stone et al., 2010) or zero-shot cooperation (ZSC) (Hu et al., 2020). Since most existing methods are based on self-play (Rashid et al., 2018; Yu et al., 2021), how to avoid convention overfitting becomes a critical challenge in ZSC. Representative works include improved policy representation (Zhang et al., 2020; Chen et al., 2020), randomization over invariant game structures (Hu et al., 2020; Treutlein et al., 2021), population-based training (Long* et al., 2020; Lowe* et al., 2020; Cui et al., 2021) and belief modeling for partial observable settings (Hu et al., 2021; Xie et al., 2021). Fictitious co-play (FCP) (Strouse et al., 2021) proposes a two-stage framework by first creating a pool of self-play policies and their previous versions and then training an adaptive policy against them. Some techniques improves the diversity of the policy pool (Garnelo et al., 2021; Liu et al., 2021; Zhao et al., 2021; Lupu et al., 2021) for a stronger adaptive policy (Knott et al., 2021).\n\nWe follow the FCP framework and augment the policy pool with biased strategies. Notably, techniques for learning a robust policy in competitive games, such as policy ensemble (Lowe et al., 2017), adversarial training (Li et al., 2019) and double oracle (Lanctot et al., 2017), are complementary to our focus.\n\nBuilding AIs that can cooperate with humans remains a fundamental challenge in AI (Dafoe et al., 2021). A critical issue is that humans can be systematically biased (Camerer, 2011; Russell, 2019). Hence, great efforts have been made to model human biases, such as irrationality (Selten, 1990; Bobu et al., 2020; Laidlaw & Dragan, 2022), risk aversion (Pratt, 1978; Barberis, 2013), and myopia (Evans et al., 2016). Many popular models further assume humans have hidden subject utility functions (Nguyen et al., 2013; Hadfield-Menell et al., 2016; Eckersley, 2019; Shah et al., 2019). Conventional methods for human-AI collaboration require an accurate behavior model over human data (Ajoudani et al., 2018; Kwon et al., 2020; Kress-Gazit et al., 2021; Wang et al., 2022), while we consider the setting of no human data. Hence, we explicitly model human biases as a hidden utility function in the self-play objective to reflect possible human biases beyond conventions w.r.t. optimal rewards. We prove that such a hidden-utility model can represent any strategy of nonadaptive humans. Notably, it is also feasible to generalize our model to capture higher cognitive hierarchies (Camerer et al., 2004), which we leave as a future direction.\n\nWe approximate the reward space by a linear function space over event-based features. Such a linear representation is typical in inverse reinforcement learning (Ng & Russell, 2000), policy trans-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nfer (Barreto et al., 2017b), evolution computing (Cully et al., 2015) and game theory (Winterfeldt & Fischer, 1975; Kiekintveld et al., 2013). Event-based rewards are also widely adopted as a general design principle in robot learning (Fu et al., 2018; Zhu et al., 2019; Ahn et al., 2022). We perform randomization over feature weights to produce diverse biased strategies. Similar ideas have been adopted in other settings, such as generating adversaries (Paruchuri et al., 2006), emergent teamformation (Baker, 2020), and searching for diverse Nash equilibria in general-sum games (Tang et al., 2020). In our implementation, we use multi-reward signals as an approximate metric to filter out duplicated policies, which is inspired by the quality diversity method (Pugh et al., 2016). There are also some works utilizing model-based methods to solve zero-shot cooperation Wu et al. (2021). Their focus is orthogonal to our approach since they focus more on constructing an adaptive agent, while our approach aims to find more diverse strategies. Besides, we adopt an end-to-end fashion to train an adaptive agent, which is more general. Lastly, our final adaptive agent assumes a zero-shot setting without any data from its testing partner. This can be further extended by allowing metaadaptation at test time (Charakorn et al., 2021; Gupta et al., 2021; Nekoei et al., 2021), which we leave as a future direction.\n\n3 PRELIMINARY\n\nTwo-Player Human-AI Cooperative Game: A human-AI cooperative game is defined on a world model, i.e., a two-player Markov decision process denoted by M = ⟨S, A, P, R⟩, with one player with policy πA being an AI and the other with policy πH being a human. S is a set of world states. A is a set of possible actions for each player. P is a transition function over states given the actions from both players. R is a global reward function. A policy πi produces an action a(i) t ∈ A given a world state st ∈ S at the time step t. We use the expected discounted return J(πA, πH ) = E as the objective. Note that J(πH , πA) can be similarly defined, and we use J(πA, πH ) for conciseness without loss of generality. Let PH : Π → [0, 1] be the unknown distribution of human policies. The goal is to find a policy πA that maximizes the expected return with an unknown human, i.e., EπH ∼PH [J(πH , πA)]. In practice, many works construct or learn a policy distribution ˆPH to approximate real-world human behaviors, leading to an approximated objective for πA, i.e., E\n\nt γtR(st, a(A)\n\n[J(πA, ˆπH )].\n\n, a(H)\n\nst,a(i)\n\n(cid:104)(cid:80)\n\n(cid:105) )\n\nt\n\nt\n\nt\n\nˆπH ∼ ˆPH\n\nSelf-Play for Human-AI Cooperation: Self-play (SP) optimizes J(π1, π2) with two parametric policies π1 and π2 and takes π1 as πA without use of human data. However, SP suffers from poor generalization since SP converges to a specific optimum and overfits the resulting behavior convention. Population-based training (PBT) improves SP by representing πi as a mixture of K individual policies {π(k) k=1 and runs cross-play between policies by optimizing the expected return (Long* et al., 2020; Lowe* et al., 2020; Cui et al., 2021). PBT can be further improved by adding a diversity bonus over the population (Garnelo et al., 2021; Liu et al., 2021; Lupu et al., 2021).\n\ni }K\n\n2 )}K\n\n1 , π(k)\n\n1 , π(k)\n\n2 , denoted by ̃π(k)\n\nk=1 by optimizing J(π(k)\n\nFictitious Co-Play (FCP): FCP (Strouse et al., 2021) is a recent work on zero-shot human-AI cooperation with strong empirical performances. FCP extends PBT via a two-stage framework. In the first stage, FCP trains K individual policy pairs {(π(k) 1 , π(k) 2 ) for each k. Each policy pair (π(k) 2 ) may quickly converge to a distinct local optimum. Then FCP constructs a policy pool Π2 = { ̃π(k) k=1 with two past versions of each converged SP policy π(k) 2 . In the second stage, FCP constructs a human proxy distribution ˆPH by randomly sampling from Π2 and trains πA by optimizing E [J(πA, ˆπH )]. We remark that, for a better cooperation, the adaptive policy πA should condition on the state-action history in an episode to infer the intention of its partner. Individual SP policies ensure ˆPH contains diverse conventions while using past versions enables ˆPH to cover different skill levels. So, the final policy πA can be forced to adapt to humans with unknown conventions or sub-optimalities. Maximum Entropy Population-based Training (MEP) (Zhao et al., 2021) is the latest variant of FCP, which adopts the population entropy as a diversity bonus in the first stage to improve the generalization of the learned πA.\n\n2 , π(k)\n\n2 }K\n\nˆπH ∼ ˆPH\n\n4 COOPERATING WITH HUMANS IN Overcooked: A MOTIVATING EXAMPLE\n\nOvercooked Game: Overcooked (Carroll et al., 2019) is a fully observable two-player cooperative game developed as a testbed for human-AI cooperation. In Overcooked, players cooperatively accomplish different soup orders and serve the soups for rewards. Basic game items include onions, tomatoes, and dishes. An agent can move in the game or “interact” to trigger some events, such as grabbing/putting an item, serving soup, etc., depending on the game state. To finish an order, players\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Layouts in Overcooked. From left to right are Asymmetric Advantages, Coordination Ring, Counter Circuit, Distant Tomato and Many Orders respectively, with orders shown below.\n\nshould put a proper amount of ingredients into the pot and cook for some time. Once a soup is finished, players should pick up the soup with a dish and serve it to get a reward. Different orders have different cooking times and different rewards. Fig. 1 demonstrates five layouts we consider, where the first three onion-only layouts are adopted from (Carroll et al., 2019), while the latter two, Distant Tomato and Many Orders, are newly introduced to include tomato orders to make the problem more challenging: an AI needs to carefully adapt its behavior to either cook onions or tomatoes according to the other player’s actions.\n\nA Concrete Example of Human Preference: Fig. 2 illustrates a motivating example in Distant Tomato (the 4th layout in Fig. 1). There are two orders: one requires three onions, and the other requires three tomatoes. We run FCP on this multi-order scenario, and all the policies in the FCP policy pool converge to the specific pattern of only cooking onion soup (Fig. 2a). Hence, the final adaptive policy by FCP only learns to grab onions and cook onion soups. Cooking tomato soup is a sub-optimal strategy that requires many extra moves, so the onion-only policy pool is exactly the solution to the FCP self-play objective under the environment reward. However, it is particularly reasonable for a human to dislike onions and accordingly only grab tomatoes in a game. To be an assistive AI, the policy should adapt its strategy to follow the human preference for tomatoes. On the contrary, as shown in Fig. 2b, the FCP policy completely ignores human moves for tomatoes and even results in poor cooperation by producing valueless wrong orders of mixed onions and tomatoes. Thus, to make an FCP agent human-assistive, the first-stage policy pool should not only contain optimal strategies (i.e., onion soups) of different conventions but also cover diverse human preferences (e.g., tomatoes) even if these preferences are sub-optimal under the environment reward.\n\n(a) FCP-FCP\n\n(b) FCP’s failure case when cooperating with a human player\n\nFigure 2: Motivating example. (a) FCP converges to the optimal onion soup strategy. (b) A failure case of FCP with a human partner: FCP agent corrupts the human’s plan of cooking tomato soups.\n\n5 METHODOLOGY\n\nWe introduce a general formulation to model human preferences and develop a tractable learning objective (Sec. 5.2). The algorithm, Hidden-Utility Self-Play (HSP), is summarized in Sec. 5.3.\n\n5.1 HIDDEN-UTILITY MARKOV GAME\n\nThe key insight from Sec. 4 is that humans may not truthfully behave under the environment reward. Instead, humans are biased and driven by their own utility functions, which are formulated below.\n\nDefinition: A two-player hidden utility Markov game is defined as ⟨S, A, P, Rw, Rt⟩. ⟨S, A, P, Rt⟩ corresponds to the original game MDP with Rt being the task reward function. Rw denotes an additional hidden reward function. There are two players, πa, whose goal is to maximize the task reward Rt, and πw, whose goal is to maximize the hidden reward Rw. Rw is only visible to πw.\n\na, π′\n\na, π∗ w|Rw) ≥ J(π∗\n\nLet J(π1, π2|R) denote the expected return under reward R with a policy π1 and π2. During self-play, πa optimizes J(πa, πw|Rt) while πw optimizes J(πa, πw|Rw). A solution policy profile (π∗ w) to the hidden utility Markov game is now defined by a Nash equilibrium (NE): □\na, π∗ J(π∗\n\na, π∗ Intuitively, with a suitable hidden reward function Rw, we can obtain any possible (non-adaptive and consistent) human policy by solving the hidden-utility game induced by Rw. Lemma 5.1. Given an MDP M = ⟨S, A, P, Rt⟩, for any policy π : S × A → [0, 1], there exists a hidden reward function Rw such that the two-player hidden utility Markov game M ′ = ⟨S, A, P, Rw, Rt⟩ has a Nash equilibrium (π∗\n\nw|Rt) ≥ J(π′\n\nw|Rt), ∀π′ a.\n\nw and J(π∗\n\nw|Rw), ∀π′\n\nw) where π∗\n\nw = π.\n\na, π∗\n\na, π∗\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nLemma 5.1 connects any human behavior to a hidden reward function. Then the objective of the adaptive policy πA in Eq. (3) can be formulated under the hidden reward function space R as follows. Theorem 5.1. For any ε > 0, there exists a mapping ̃πw where ̃πw(Rw) denotes the derived policy π∗ w in the NE of the hidden utility Markov game Mw = ⟨S, A, P, Rw, Rt⟩ induced by Rw, and a distribution PR : R → [0, 1] over the hidden reward space R, such that, for any adaptive policy πA ∈ arg maxπ′ ERw∼PR [J(π′, ̃πw(Rw))], πA approximately maximizes the ground-truth objective with at most an ε gap, i.e., EπH ∼PH [J(πA, πH )] ≥ maxπ′ EπH ∼PH [J(π′, πH )] − ε.\n\nTheorem 5.1 indicates that it is possible to derive diverse human behaviors by properly designing a hidden reward distribution ˆPR, which can have a much lower intrinsic dimension than the policy distribution. In Overcooked, human preferences can be typically described by a few features, such as interaction with objects or certain type of game events, like finishing an order or delivering a soup. By properly approximating the hidden reward distribution as ˆPR, the learning objective becomes,\n\nπA = arg max\n\nπ′\n\nE\n\nRw∼ ˆPR\n\n[J(π′, ̃πw(Rw))]\n\n(1)\n\nEq. (1) naturally suggests a two-staged solution by first constructing a policy pool { ̃πw(R) : R ∼ ˆPR} from ˆPR and then training πA to maximize the game reward w.r.t. the induced pool.\n\n5.2 CONSTRUCT A POLICY POOL OF DIVERSE PREFERENCES\n\nEvent-based Reward Function Space: The fundamental question is how to design a proper hidden reward function space R. Inspired by the fact that human preferences are often event-centric, we formulate R as linear functions over event features, namely R = {Rw : Rw(s, a1, a2) = φ(s, a1, a2)T w, ||w||∞ ≤ Cmax}. Cmax is a bound on the feature weight w while φ : S × A × A → Rm specifies occurrences of different game events when taking joint action (a1, a2) at state s.\n\nIn general, a valid reward space is intractably large.\n\nDerive a Diverse Set of Biased Policies: We simply perform a random search over the feature weight w to derive a set of diverse behaviors. We first draw N samples {w(i)}i∈[N ] for the feature weight w where w(i) is sampled uniformly from a set of values Cj, leading to a set of hidden reward w (s, a1, a2) = φ(s, a1, a2)T w(i)}i∈[N ]. For each hidden reward function R(i) w : R(i) functions {R(i) w , we find an approximated NE, π(i) w through self-play. The above process produces a policy pool {π(i) w }i∈[N ] that can cover a wide range of behavior preferences.\n\na , of the hidden utility Markov game induced by R(i)\n\nw , π(i)\n\nj\n\nAlgorithm 1: Greedy Policy Selection\n\nS ← {i0} where i0 ∼ [N ]; for i = 1 → K − 1 do\n\nPolicy Filtering: We notice that the derived pool often contains a lot of similar policies. This is because the same policy can be optimal under a set of reward functions, which is typical in multi-objective optimization (Chugh et al., 2019; Tabatabaei et al., 2015). Duplicated policies simply slow down training without any help to learn πA. For more efficient training, we adopt a behavior metric, i.e., event-based diversity, to only keep distinct ones from the initial pool. For each biased policy π(i) w , let EC(i) denote the expected event count, i.e. E[(cid:80)T a ]. We define event-based diversity for a subset S ⊆ [N ] by normalized pairwise EC differences, i.e., ED(S) = (cid:80) k |, where ck is a frequency normalization constant. Finding a subset S∗ of size K with the optimal ED can be expensive. We simply adopt a greedy method in Algo. 1 to select policies incrementally.\n\nk′ ← arg maxk′ /∈S ED(S ∪ {k′}); S ← S ∪ {k′};\n\nt=1 φ(st, at)|π(i)\n\nk ck · |EC(i)\n\nk − EC(j)\n\nw , π(i)\n\ni,j∈S\n\nend\n\n(cid:80)\n\n5.3 HIDDEN-UTILITY SELF-PLAY\n\nGiven the filtered policy pool, we train the final adaptive policy πA over rollout games by πA and randomly sampled policies from the pool, which completes our overall algorithm HSP in Algo. 2.\n\nWe implement HSP using MAPPO (Yu et al., 2021) as the RL algorithm. In the first stage, we use MLP policies for fast SP convergence. In practice, we use\n\n5\n\nAlgorithm 2: Hidden-Utility Self-Play for i = 1 → N do Train π(i)\n\na under sampled R(i) w ;\n\nw and π(i)\n\nend Run Algo. 1 to only keep K policies; Initial policy πA; repeat\n\nRollout with πA and sampled π(i) w ; Update πA;\n\nuntil enough iterations;\n\nPublished as a conference paper at ICLR 2023\n\nhalf of the policy pool to train biased policies and the other half to train MEP policies (Zhao et al., 2021) under the game reward. This increases the overall pool towards the game reward, leading to improved empirical performances. For the final adaptive training, as suggested in (Tang et al., 2020), we add the identity of each biased policy as an additional feature to the critic. For eventbased features for the reward space, we consider event types, including interactions with basic items and events causing non-zero rewards in Overcooked. Full implementation details can be found in Appendix D and E.\n\n6 EXPERIMENTS\n\nBaselines. We compare HSP with other SP-based baselines, including Fictitious Co-Play (FCP), Maximum Entropy Population-based training (MEP), and Trajectory Diversity-based PBT (TrajDiv). All methods follow a two-stage framework with a final pool size of 36, which we empirically verified to be sufficiently large to avoid performance degradation for all methods. More analysis on pool size can be found in Appendix F.2.1. The implementation details of baselines can be found in Appendix D.2. Each policy is trained for 100M timesteps for convergence over 5 random seeds. Full training details with hyper-parameter settings can be found in Appendix E.1.\n\nEvaluation. We aim to examine whether HSP can cooperate well with (1) learned human models, (2) scripted policies with strong preferences, and (3) real humans. We use both game reward and human feedback as evaluation metrics. We remark that since a biased human player may play a suboptimal strategy, the game reward may not fully reflect the performance gap between the baselines and HSP. Our goal is to ensure the learned policy is effective for biased partners/humans. Therefore, we consider human feedback as the fundamental metric. Ablation studies are also performed to investigate the impact of our design choices in HSP. In tables, maximum returns or comparable returns within a threshold of 5 are marked in bold. Full results can be found in Appendix F.\n\n6.1 COOPERATION WITH LEARNED HUMAN MODELS IN ONION-ONLY LAYOUTS\n\nFor evaluation with learned human models, we adopted the models provided by (Carroll et al., 2019), which only support onion-only layouts, including Asymm. Adv., Coord. Ring and Counter Circ.. The results are shown in Tab. 1. For a fair comparison, we reimplement all the baselines, labeled MEP, FCP, and TrajDiv, with the same training steps and policy pool size as HSP. We additionally take the best performance ever reported in the existing literature, labeled Existing SOTA in Tab. 1. Our implementation achieves substantially higher scores than Existing SOTA when evaluated with the same human proxy models. HSP further outperforms other reimplementations in Asymm. Adv. and is comparable with the best baseline in the rest. Full results of the evaluation with learned human models can be found in Appendix F.1. We emphasize that the improvement is marginal because the learned human models have limited representation power to imitate natural human behaviors, which typically cover many behavior modalities. Fig.8 in Appendix F.1.1 shows trajectories induced by the learned human models only cover a narrow subspace of trajectories played by human players. Further analysis of the learned human models can be found in Appendix F.1.1. Furthermore, our implementation of baselines achieves substantially better results than the original papers (Carroll et al., 2019; Zhao et al., 2021), which also makes the improvement margin smaller.\n\nPos. Asymm. Adv. Coord. Ring Counter Circ.\n\nExisting SOTA\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\n1 2\n1 2\n1 2\n1 2\n1 2\n\n141.1(12.5) 84.6(16.3) 282.8(9.4) 203.8(8.2) 291.7(4.6) 203.4(2.0) 289.3(8.8) 194.2(0.7) 300.3(2.2) 217.1(3.3)\n\n92.7(7.4) 107.3(6.4) 161.3(1.6) 161.0(2.7) 161.8(0.7) 164.2(2.1) 150.8(3.1) 142.1(2.3) 160.0(2.6) 160.6(3.3)\n\n54.5(2.3) 55.8(3.6) 95.9(2.0) 92.7(1.3) 108.8(4.2) 111.1(0.7) 60.1(5.0) 53.7(12.4) 107.4(3.5) 106.6(3.0)\n\nTable 1: Comparison of average episode reward and standard deviation when cooperating with learned human models. The Pos. column indicates the roles played by AI policies. Existing SOTA is the best performance ever reported in the existing literature. HSP achieves substantially higher scores than Existing SOTA. And HSP further outperforms other methods in Asymm. Adv. and is comparable with the best baseline in the rest.\n\n6.2 ABLATION STUDIES\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nWe investigate the impact of our design choices, including the construction of the final policy pool and the batch size for training the adaptive policy.\n\nPolicy Pool Construction: HSP has two techniques for the policy pool, i.e., (1) policy filtering to remove duplicated biased policies and (2) the use of MEP policies under the game reward for half of the pool size. We measure the performance with human proxies by turning these options off. For “HSP w.o. Filtering”, we keep all policies by random search in the policy pool, resulting in a larger pool size of 54 (18 MEP policies and a total of 36 random search ones). For“HSP w.o. MEP”, we exclude MEP policies from the policy pool and keep all biased policies without filtering, which leads to the same pool size of 36. The results are shown in Fig. 3 and the detailed numbers can be found in Appendix F.2.2. By excluding MEP policies, the HSP variant (HSP w.o. MEP) performs worse in the more complicated layout Counter Circ. while remaining comparable in the other two simpler ones. So we suggest including a few MEP policies when possible. With policy filtering turned off, even though the policy pool size grows, the performance significantly decays in both Coord. Ring and Counter Circ. layouts, suggests that duplicated biased policies can hurt policy generalization.\n\nFigure 3: Performance of different pool construction strategies. Results suggest that it is beneficial to incorporate MEP policies and filter duplicated policies.\n\nBatch Size: We measure the training curves of the final adaptive policy under the game reward using different numbers of parallel rollout threads in MAPPO. More parallel threads indicate a larger batch size. The results in all five layouts are reported in Fig. 4. In general, we observe that a larger batch size often leads to better training performance. In particular, when the batch size is small, i.e., using 50 or 100 parallel threads, training becomes significantly unstable and even breaks in three layouts. Note that the biased policies in the HSP policy pool have particularly diverse behaviors, which cause a high policy gradient variance when training the final adaptive policy. Therefore, a sufficiently large training batch size can be critical to stable optimization. We adopt 300 parallel threads in all our experiments for a fair comparison.\n\nFigure 4: Average game reward by using different numbers of parallel rollout threads in MAPPO to train the final adaptive policy. More parallel threads imply a larger training batch size.\n\nPractical Remark: Overall, we suggest using a pool size of 36 and including a few MEP policies for the best empirical performance. Besides, a sufficiently large training batch size can help stable optimization, and we use the same batch size for all methods for a fair comparison.\n\n6.3 COOPERATION WITH SCRIPTED POLICIES WITH STRONG BEHAVIOR PREFERENCES\n\nWe empirically notice that human models learned by imitating the entire human trajectories cannot well capture a wide range of behavior modalities. So, we manually designed a set of script policies to encode some particular human preferences: Onion/Tomato Placement, which continuously places onions or tomatoes into the pot, Onion/Dish Everywhere, which keeps putting onions or dishes on the counters, Tomato/Onion Placement and Delivery, which puts tomatoes/onions into the pot in half of the time and tries to deliver soup in the other half of the time. For a fair comparison, we ensure that all scripted policies are strictly different from the HSP policy pool. More details about scripted policies and a full evaluation can be found in Appendix D.3.\n\nWe remark that scripted policies are only used for evaluation but not for training HSP. Tab. 2 shows the average game reward of all the methods when paired with scripted policies, where HSP significantly outperforms all baselines. In particular, in Distant Tomato, when cooperating with a strong tomato preference policy (Tomato Placement), HSP achieves a 10× higher score than other baselines, suggesting that the tomato-preferred behavior is well captured by HSP.\n\n6.4 COOPERATION WITH HUMAN PARTICIPANTS\n\nWe recruited 60 volunteers (28.6% female, 71.4% male, age between 18–30) by posting the experiment advertisement on a public platform and divided them into 5 groups for 5 layouts. They are provided with a detailed introduction to the basic gameplay and the experiment process. Vol-\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nScripts\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\nAsymm. Adv.\n\nCoord. Ring\n\nCounter Circ.\n\nDistant Tomato\n\nMany Orders\n\nOnion Placement Onion Place.&Delivery Onion Everywhere Dish Everywhere Onion Everywhere Dish Everywhere Tomato Placement Tomato Place.&Delivery Tomato Placement Tomato Place.&Delivery\n\n334.8(13.0) 297.7(3.4) 109.1(7.9) 94.4(3.8) 63.7(9.2) 57.0(5.3) 15.6(5.2) 177.9(6.1) 282.6(16.2) 329.1(5.3)\n\n330.5(14.2) 298.5(3.4) 124.0(3.4) 100.2(5.3) 88.9(5.1) 53.0(1.8) 20.1(10.6) 180.4(8.7) 225.8(60.8) 328.1(12.6)\n\n323.6(17.0) 290.0(4.7) 116.9(8.9) 107.3(5.3) 82.0(12.8) 57.2(2.2) 23.3(9.5) 164.8(19.6) 259.2(7.9) 295.7(2.4)\n\n376.8(9.9) 300.1(4.1) 121.2(12.6) 115.4(7.4) 107.5(3.5) 78.5(4.1) 277.9(14.3) 234.6(15.1) 317.8(9.3) 324.5(3.9)\n\nTable 2: Average episode reward and standard deviation with unseen testing scripted policies. HSP significantly outperforms all baselines.\n\nunteers are fully aware of all their rights and experiments are approved with the permission of the department. A detailed description of the human study can be found in Appendix F.4. The experiment has two stages:\n\n• Warm-up Stage: Participants could play the game freely to explore possible AI behaviors. They are asked to rank AI policies according to the degree of assistance during free plays.\n\n• Exploitation Stage: Participants are instructed to achieve a score\n\nas high as possible.\n\nWe note that our user study design differs from that of the original Overcooked paper (Carroll et al., 2019). The additional warm-up stage allows for diverse human behaviors under any possible preference, suggesting a strong testbed for human-assistive AIs.\n\n6.4.1 RESULTS OF THE WARM-UP STAGE\n\nThe warm-up stage is designed to test the performance of AI policies in the face of diverse human preferences. Fig. 5 visualizes the human preference for different methods reported in the warm-up stage. The unit represents the difference between the percentage of human players who prefer row partners over column partners and human players who prefer column partners over row partners. The detailed calculation method can be found in Appendix F.4.3. HSP is preferred by humans with a clear margin. Since humans can freely explore any possible behavior, the results in Fig. 5 imply the strong generalization capability of HSP. We also summarize feedback from human participants in Appendix F.4.2.\n\n6.4.2 RESULTS OF THE EXPLOITATION STAGE\n\nFigure 5: Human preference in the warm-up stage. The unit denotes the difference between the percentage of human players who prefer row partners over column partners and human players who prefer column partners over row partners. HSP is consistently preferred by human participants with a clear margin.\n\nThe exploitation stage is designed to test the scoring capability of different AIs. Note that it is possible that a human player simply adapts to the AI strategy when instructed to have high scores. So, in addition to final rewards, we also examine the emergent human-AI behaviors to measure the human-AI cooperation level. The experiment layouts can be classified into two categories according to whether the layout allows diverse behavior modes. The first category contains simple onion-only layouts that are taken from (Carroll et al., 2019), including Asymm. Adv., Coord. Ring and Counter Circ.. The second category contains newly introduced layouts with both onions and tomatoes, Distant Tomato and Many Orders, which allow for a much wider range of behavior modes.\n\nOnion-only Layouts: Fig. 6a shows the average reward in onion-only layouts for different methods when paired with humans. Among these onion-only layouts, all methods have comparable episode reward in simpler ones (Asymm. Adv. and Coord. Ring), while HSP is significantly better in the most complex Counter Circ. layout. Fig. 6b shows the frequency of successful onion passing between the human player and the AI player. The learned HSP policy is able to use the middle counter for passing onions, while the baseline policies are less capable of this strategy.\n\nLayouts with Both Onions and Tomatoes: The results and behavior analysis in Distant Tomato and Many Orders are shown as follows,\n\n• Distant Tomato: In Distant Tomato, the optimal strategy is always cooking onion soups, while it is suboptimal to cook tomato soups due to the much more time spent on moving. Interestingly, our human-AI experiments found that humans may have diverse biases over onions and tomatoes. However, all learned baseline policies tend to have a strong bias towards onions and often place onions into a pot with tomatoes in it already. Tab. 3 reports the average number of such Wrong\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) Onion-Only Layouts\n\n(b) Counter Circ.\n\nFigure 6: (a) Average episode reward in onion-only layouts of different methods when paired with humans in the exploitation stage. HSP has comparable performance with the baselines in Asymm. Adv. and Coord. Ring, and is significantly better in the most complex Counter Circ. layout. (b) The onion passing frequency in Counter Circ. shows that HSP is the most capable, among other baselines, of passing onions via the counter, suggesting better capabilities to assist humans.\n\nPlacements made by different AI players. HSP makes the lowest number of wrong placements and is the only method that can correctly place additional tomatoes into a pot partially filled with tomatoes, labeled Correct Placements. This suggests that HSP is the only effective method to cooperate with biased human strategies, e.g., preferring tomatoes. In addition, as shown in Tab. 3, even when humans play the optimal strategy of cooking onion soups, HSP still achieves comparable performance with other methods.\n\n• Many Orders: In Many Orders, an effective strategy is to utilize all three pots to cook soups. Our experiments found that baseline policies tend to ignore the middle pot. Tab. 4 shows the average number of soups picked up from the middle pot by different AI players. The learned HSP policy is much more active in taking soups from the middle pot, leading to more soup deliveries. Furthermore, HSP achieves a substantially higher episode reward than other methods, as shown in Tab. 4.\n\nFCP\n\nMEP\n\nTrajDiv HSP\n\nOnion-Preferred Episode Reward ↑ Wrong Placements ↓ Correct Placements ↑\n\n343.65 0.37 0.0\n\n325.08 0.41 0.0\n\n334.73 0.38 0.0\n\n340.3 0.21 1.41\n\nTable 3: Average onion-preferred episode reward and frequency of different emergent behaviors in Distant Tomato during the exploitation stage. Onion-Preferred Episode Reward is the average episode reward when humans prefer onions. Wrong Placements and Correct Placements are the average numbers of wrong and correct placements into a pot partially filled with tomatoes. HSP makes the lowest number of wrong placements and is the only method that can place tomatoes correctly, suggesting that HSP is effective at cooperating with biased human strategies.\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\nEpisode Reward ↑ Number of Soups Picked Up from the Middle Pot ↑\n\n316.81 1.93\n\n320.61 2.03\n\n323.52 1.33\n\n382.52 5.64\n\nTable 4: Average episode reward and average number of picked-up soups from the middle pot by different AI players in Many Orders during the exploitation stage. HSP achieves significantly better performance and is much more active in taking soups from the middle pot than baselines.\n\n7 CONCLUSION\n\nWe developed Hidden-Utility Self-Play (HSP) to tackle the problem of zero-shot human-AI cooperation by explicitly modeling human biases as an additional reward function in self-play. HSP first generates a pool of diverse strategies and then trains an adaptive policy accordingly. Experiments verified that agents trained by HSP are more assistive for humans than baselines in Overcooked. Although our work suggests a new research direction on this fundamentally challenging problem, there are still limitations to be addressed. HSP requires domain knowledge to design a suitable set of events. There exists some work on learning reward functions rather than assuming event-based rewards (Shah et al., 2019; Zhou et al., 2021). So a future direction is to utilize learning-based methods to design rewards automatically. Another major limitation is the computation needed to obtain a diverse policy pool. Possible solutions include fast policy transfer and leveraging a prior distribution of reward functions extracted from human data (Barreto et al., 2017a). Learning and inferring the policy representations of partners could also provide further improvement. We leave these issues as our future work.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis research was supported by National Natural Science Foundation of China (No.U19B2019, 62203257, M-0248), Tsinghua University Initiative Scientific Research Program, Tsinghua-Meituan Joint Institute for Digital Life, Beijing National Research Center for Information Science, Technology (BNRist), and Beijing Innovation Center for Future Chips and 2030 Innovation Megaprojects of China (Programme on New Generation Artificial Intelligence) Grant No. 2021AAA0150000.\n\nREFERENCES\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n\nArash Ajoudani, Andrea Maria Zanchettin, Serena Ivaldi, Alin Albu-Schäffer, Kazuhiro Kosuge, and Oussama Khatib. Progress and prospects of the human-robot collaboration. Autonomous Robots, 42(5):957–975, 2018.\n\nBowen Baker. Emergent reciprocity and team formation from randomized uncertain social prefer-\n\nences. Advances in Neural Information Processing Systems, 33:15786–15799, 2020.\n\nNicholas C Barberis. Thirty years of prospect theory in economics: A review and assessment.\n\nJournal of Economic Perspectives, 27(1):173–96, 2013.\n\nAndre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van HasIn selt, and David Silver. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017a. URL https://proceedings.neurips.cc/paper/2017/file/ 350db081a661525235354dd3e19b8c05-Paper.pdf.\n\nSuccessor features for transfer in reinforcement\n\nlearning.\n\nAndré Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017b.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D ̨ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nFrancesco Biondi, Ignacio Alvarez, and Kyeong-Ah Jeong. Human–vehicle cooperation in auInternational Journal of Human–\n\ntomated driving: A multidisciplinary review and appraisal. Computer Interaction, 35(11):932–946, 2019.\n\nAndreea Bobu, Dexter RR Scobee, Jaime F Fisac, S Shankar Sastry, and Anca D Dragan. Less is more: Rethinking probabilistic models of human behavior. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction, pp. 429–437, 2020.\n\nColin F Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton univer-\n\nsity press, 2011.\n\nColin F Camerer, Teck-Hua Ho, and Juin-Kuan Chong. A cognitive hierarchy model of games. The\n\nQuarterly Journal of Economics, 119(3):861–898, 2004.\n\nMicah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-AI coordination. Advances in neural information processing systems, 32, 2019.\n\nRujikorn Charakorn, Poramate Manoonpong, and Nat Dilokthanakul. Learning to cooperate with In Proceedings of the 20th International\n\nunseen agents through meta-reinforcement learning. Conference on Autonomous Agents and MultiAgent Systems, pp. 1478–1479, 2021.\n\nShuo Chen, Ewa Andrejczuk, Zhiguang Cao, and Jie Zhang. Aateam: Achieving the ad hoc teamwork by employing the attention mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7095–7102, 2020.\n\nTinkle Chugh, Karthik Sindhya, Jussi Hakanen, and Kaisa Miettinen. A survey on handling computationally expensive multiobjective optimization problems with evolutionary algorithms. Soft Computing, 23(9):3137–3166, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBrandon Cui, Hengyuan Hu, Luis Pineda, and Jakob Foerster. K-level reasoning for zero-shot\n\ncoordination in hanabi. Advances in Neural Information Processing Systems, 34, 2021.\n\nAntoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like\n\nanimals. Nature, 521(7553):503–507, 2015.\n\nAllan Dafoe, Yoram Bachrach, Gillian Hadfield, Eric Horvitz, Kate Larson, and Thore Graepel.\n\nCooperative AI: machines must learn to find common ground, 2021.\n\nPeter Eckersley.\n\nImpossibility and uncertainty theorems in ai value alignment (or why your agi\n\nshould not have a utility function). In SafeAI@ AAAI, 2019.\n\nOwain Evans, Andreas Stuhlmüller, and Noah Goodman. Learning the preferences of ignorant,\n\ninconsistent agents. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nFei Fang, Shutian Liu, Anjon Basak, Quanyan Zhu, Christopher D Kiekintveld, and Charles A Kamhoua. Introduction to game theory. Game Theory and Machine Learning for Cyber Security, pp. 21–46, 2021.\n\nJustin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework for data-driven reward definition. Advances in neural information processing systems, 31, 2018.\n\nMarta Garnelo, Wojciech Marian Czarnecki, Siqi Liu, Dhruva Tirumala, Junhyuk Oh, Gauthier Gidel, Hado van Hasselt, and David Balduzzi. Pick your battles: Interaction graphs as populationlevel objectives for strategic diversity. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1501–1503, 2021.\n\nThomas L Griffiths. Manifesto for a new (computational) cognitive revolution. Cognition, 135:\n\n21–23, 2015.\n\nAbhinav Gupta, Marc Lanctot, and Angeliki Lazaridou. Dynamic population-based meta-learning for multi-agent communication with natural language. Advances in Neural Information Processing Systems, 34, 2021.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\nDylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse\n\nreinforcement learning. Advances in neural information processing systems, 29, 2016.\n\nHengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. “other-play” for zero-shot coordination. In International Conference on Machine Learning, pp. 4399–4410. PMLR, 2020.\n\nHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, and Jakob Foerster. Off-belief\n\nlearning. In International Conference on Machine Learning, pp. 4369–4379. PMLR, 2021.\n\nCory D Kidd and Cynthia Breazeal. Robots at home: Understanding long-term human-robot inIn 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.\n\nteraction. 3230–3235. IEEE, 2008.\n\nChristopher Kiekintveld, Towhidul Islam, and Vladik Kreinovich. Security games with interval In Proceedings of the 2013 international conference on Autonomous agents and\n\nuncertainty. multi-agent systems, pp. 231–238, 2013.\n\nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A survey of generalisation in\n\ndeep reinforcement learning. arXiv preprint arXiv:2111.09794, 2021.\n\nGlen Klien, David D Woods, Jeffrey M Bradshaw, Robert R Hoffman, and Paul J Feltovich. Ten challenges for making automation a \"team player\" in joint human-agent activity. IEEE Intelligent Systems, 19(6):91–95, 2004.\n\nPaul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, Anca Dragan, and Rohin Shah. Evaluating the robustness of collaborative agents. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1560–1562, 2021.\n\nHadas Kress-Gazit, Kerstin Eder, Guy Hoffman, Henny Admoni, Brenna Argall, Ruediger Ehlers, Christoffer Heckman, Nils Jansen, Ross Knepper, Jan Kˇretínsk`y, et al. Formalizing and guaranteeing human-robot interaction. Communications of the ACM, 64(9):78–84, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMinae Kwon, Erdem Biyik, Aditi Talati, Karan Bhasin, Dylan P Losey, and Dorsa Sadigh. When humans aren’t optimal: Robots that collaborate with risk-aware humans. In 2020 15th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 43–52. IEEE, 2020.\n\nCassidy Laidlaw and Anca Dragan. The boltzmann policy distribution: Accounting for systematic suboptimality in human models. In International Conference on Learning Representations, 2022.\n\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Pérolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. Advances in neural information processing systems, 30, 2017.\n\nShihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent In Proceedings of the\n\nreinforcement learning via minimax deep deterministic policy gradient. AAAI Conference on Artificial Intelligence, volume 33, pp. 4213–4220, 2019.\n\nXiangyu Liu, Hangtian Jia, Ying Wen, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhipeng Hu, and Yaodong Yang. Towards unifying behavioral and response diversity for open-ended learning in zero-sum games. Advances in Neural Information Processing Systems, 34:941–952, 2021.\n\nQian Long*, Zihan Zhou*, Abhinav Gupta, Fei Fang, Yi Wu†, and Xiaolong Wang†. Evolutionary population curriculum for scaling multi-agent reinforcement learning. In International Conference on Learning Representations, 2020.\n\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.\n\nRyan Lowe, Jakob Foerster, Y-Lan Boureau, Joelle Pineau, and Yann Dauphin. On the pitfalls of In Proceedings of the 18th International Conference on\n\nmeasuring emergent communication. Autonomous Agents and MultiAgent Systems, pp. 693–701, 2019.\n\nRyan Lowe*, Abhinav Gupta*, Jakob Foerster, Douwe Kiela, and Joelle Pineau. On the interacIn International Confertion between supervision and self-play in emergent communication. ence on Learning Representations, 2020. URL https://openreview.net/forum?id= rJxGLlBtwH.\n\nAndrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot coordination. In International Conference on Machine Learning, pp. 7204–7213. PMLR, 2021.\n\nHadi Nekoei, Akilesh Badrinaaraayanan, Aaron Courville, and Sarath Chandar. Continuous coorIn International Conference on Machine\n\ndination as a realistic scenario for lifelong learning. Learning, pp. 8016–8024. PMLR, 2021.\n\nAndrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In in Proc. 17th\n\nInternational Conf. on Machine Learning. Citeseer, 2000.\n\nThanh Nguyen, Rong Yang, Amos Azaria, Sarit Kraus, and Milind Tambe. Analyzing the effecIn Proceedings of the AAAI Conference on\n\ntiveness of adversary modeling in security games. Artificial Intelligence, volume 27, pp. 718–724, 2013.\n\nXinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, and Dawn Song. How you act In Proceedings of the 18th\n\ntells a lot: Privacy-leaking attack on deep reinforcement learning. International Conference on Autonomous Agents and MultiAgent Systems, pp. 368–376, 2019.\n\nPraveen Paruchuri, Milind Tambe, Fernando Ordónez, and Sarit Kraus. Security in multiagent systems by policy randomization. In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pp. 273–280, 2006.\n\nJohn W Pratt. Risk aversion in the small and in the large. In Uncertainty in economics, pp. 59–79.\n\nElsevier, 1978.\n\nJustin K Pugh, Lisa B Soros, and Kenneth O Stanley. Quality diversity: A new frontier for evolu-\n\ntionary computation. Frontiers in Robotics and AI, 3:40, 2016.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.\n\nStuart Russell. Human compatible: Artificial Intelligence and the problem of control. Penguin,\n\n2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nReinhard Selten. Bounded rationality.\n\nJournal of Institutional and Theoretical Economics\n\n(JITE)/Zeitschrift für die gesamte Staatswissenschaft, 146(4):649–658, 1990.\n\nRohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather In International Conference on Machine\n\nthan assuming, human biases for reward inference. Learning, pp. 5670–5679. PMLR, 2019.\n\nThomas B Sheridan. Human-robot interaction: status and challenges. Human factors, 58(4):525–\n\n532, 2016.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140– 1144, 2018.\n\nPeter Stone, Gal A Kaminka, Sarit Kraus, and Jeffrey S Rosenschein. Ad hoc autonomous agent teams: Collaboration without pre-coordination. In Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.\n\nDJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with humans without human data. Advances in Neural Information Processing Systems, 34, 2021.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nMohammad Tabatabaei, Jussi Hakanen, Markus Hartikainen, Kaisa Miettinen, and Karthik Sindhya. A survey on handling computationally expensive multiobjective optimization problems using surrogates: non-nature inspired methods. Structural and Multidisciplinary Optimization, 52(1):1–25, 2015.\n\nZhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Shaolei Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. In International Conference on Learning Representations, 2020.\n\nJohannes Treutlein, Michael Dennis, Caspar Oesterheld, and Jakob Foerster. A new formalism, In International Conference on Machine\n\nmethod and open issues for zero-shot coordination. Learning, pp. 10413–10423. PMLR, 2021.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nChen Wang, Claudia Pérez-D’Arpino, Danfei Xu, Li Fei-Fei, Karen Liu, and Silvio Savarese. Cogail: Learning diverse strategies for human-robot collaboration. In Conference on Robot Learning, pp. 1279–1290. PMLR, 2022.\n\nNick Wilkinson and Matthias Klaes. An introduction to behavioral economics. Macmillan Interna-\n\ntional Higher Education, 2017.\n\nDetlof Von Winterfeldt and Gregory W Fischer. Multi-attribute utility theory: models and assess-\n\nment procedures. Utility, probability, and human decision making, pp. 47–85, 1975.\n\nSarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration. Topics in Cognitive Science, 13(2):414–432, 2021.\n\nMarkus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforce-\n\nment learning. arXiv preprint arXiv:1507.04888, 2015.\n\nAnnie Xie, Dylan Losey, Ryan Tolsma, Chelsea Finn, and Dorsa Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, pp. 575–588. PMLR, 2021.\n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nTianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E Gonzalez, and Yuandong Tian. Multi-agent collaboration via reward attribution decomposition. arXiv preprint arXiv:2010.08531, 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nRui Zhao, Jinming Song, Hu Haifeng, Yang Gao, Yi Wu, Zhongqian Sun, and Yang Wei. Maximum entropy population based training for zero-shot human-ai coordination. arXiv preprint arXiv:2112.11701, 2021.\n\nZihan Zhou, Wei Fu, Bingliang Zhang, and Yi Wu. Continuously discovering novel strategies via reward-switching policy optimization. In Deep RL Workshop NeurIPS 2021, 2021. URL https: //openreview.net/forum?id=2AJtG_ZIV2.\n\nHenry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. The ingredients of real world robotic reinforcement learning. In International Conference on Learning Representations, 2019.\n\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse\n\nreinforcement learning. In Aaai, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nWe would suggest visiting https://sites.google.com/view/hsp-iclr for more information.\n\nA THEOREM PROOFS\n\nFor simplicity, we assume state space and action space in our analysis are both discrete and finite, which is exactly the case for Overcooked, and the rewards r are bounded: |r(s, a)| ≤ Rmax, ∀s ∈ S, a ∈ A.\n\nLemma 5.1. Given an MDP M = ⟨S, A, P, Rt⟩, for any policy πw : S × A → [0, 1], there exists a hidden reward function Rw such that the two-player hidden utility Markov game M ′ = ⟨S, A, P, Rw, Rt⟩ has a Nash equilibrium (π∗\n\nw) where π∗\n\nw = πw.\n\na, π∗\n\nProof. Our analysis is based on the maximum entropy reinforcement learning framework (Haarnoja et al., 2018; Ziebart et al., 2008; Wulfmeier et al., 2015). Given a reward function R and policies of the two players π1 and π2, we consider following maximum entropy RL objective for policy πi(1 ≤ i ≤ 2),\n\nJi(π1, π2|R) = Eτ\n\n(cid:34)\n\n(cid:88)\n\nt\n\nγt(R(st, a(1)\n\nt\n\n, a(2)\n\n(cid:12) (cid:12) t ) + αH(πi(·|st)))\n\n(cid:12)a(i)\n\nt ∼ πi(·|st)\n\n(cid:35)\n\nWe shall first constructs πa given policy πw to satisfy J2(πw, πa|Rt) ≥ J2(πw, π′ secondly constructs Rw such that J1(πw, πa|Rw) ≥ J1(π′ w is satisfied.\n\nw, πa|Rw), ∀π′\n\na|Rt), ∀π′\n\na and\n\nStep 1: Construct πa given πw.\n\nGiven πw, let πa ∈ arg maxπ J2(πw, π|Rt).\n\nStep 2: Construct Rw such that J1(πw, πa|Rw) ≥ J1(π′ πa.\n\nw, πa|Rw), ∀π′\n\nw is satisfied given πw and\n\nGiven a fixed partner πa, by regarding πa as part of the environment dynamics, we could consider the dynamics for πw in a single-agent MDP M ′ = ⟨S, A, P ′, Rw, γ⟩ where S is the state space, A is the action space, P ′ denotes the transition probability and Rw is the reward function to construct. More specifically, P ′ is defined as,\n\nP ′(s′|s, a) =\n\n(cid:88)\n\n ̃a\n\nP (s′|s, a, ̃a) · πa( ̃a|s)\n\nIn M ′, given reward Rw, the objective of πw becomes,\n\nmax π\n\nEτ\n\n(cid:34)\n\n(cid:88)\n\nt\n\n(cid:12) γt(Rw(st, at) + αH(π(st))) (cid:12) (cid:12)at ∼ π(st)\n\n(cid:35)\n\nThe value function and the Q function could be defined as,\n\nV (s) = Eτ\n\n(cid:34)\n\n(cid:88)\n\nt\n\n(cid:12) γt(Rw(st, at) + αH(πw(st))) (cid:12) (cid:12)at ∼ πw(st), s0 = s\n\n(cid:35)\n\n(cid:88)\n\n=\n\na\n\nπw(a|s)(Rw(s, a) + γEs′[V (s′)|s, a]) + αH(πw(s))\n\nQ(s, a) = Rw(s, a) + γ · Es′[V (s′)|s, a]\n\n(2)\n\n(3)\n\n(4)\n\n(5)\n\n(6)\n\nIt is sufficient to construct Rw such that V (s) is a stable point of the Bellman backup operator (Sutton & Barto, 2018) T ∗ under some Rw:\n\n(T ∗V )(s) = max\n\nd:(cid:80)\n\na d(a)=1\n\nαH(d) +\n\n(cid:88)\n\na\n\nd(a)(Rw(s, a) + γEs′[V (s′)|s, a])\n\n(7)\n\nNow we assume V (s) is a stable point for Eq. 7 and construct Rw. For all s ∈ S, πw(·|s) should be a solution to the following maximization problem,\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nmax d\n\ns.t.\n\nαH(d) +\n\n(cid:88)\n\na\n\nd(a)Q(s, a)\n\nd(a) = 1\n\n(cid:88)\n\na\n\nApplying KKT conditions over the above optimization problem indicates that,\n\nπw(·|s) ∝ exp(Q(s, ·)/α), ∀s\n\n(8)\n\n(9)\n\n(10)\n\nw(s) = arg maxa πw(a|s), V ∗(s) = maxa Q(s, a), A(s, a) = Q(s, a) − V ∗(s). By Eq. 10,\n\nLet π∗ we also have\n\nA(s, a) = α(log πw(a|s) − log πw(π∗\n\nw(s)|s))\n\nBy definition of value function V (s),\n\nπw(a|s)Q(s, a) + αH(πw(s))\n\nπw(a|s)(A(s, a) + V ∗(s)) + αH(πw(s))\n\nπw(a|s)A(s, a) + V ∗(s) + αH(πw(s))\n\nV (s) =\n\n=\n\n=\n\n=\n\n(cid:88)\n\na (cid:88)\n\na (cid:88)\n\na (cid:88)\n\na\n\nπw(a|s)A(s, a) + Rw(s, π∗\n\nw(s)) + γEs′[V (s′)|s′ ∼ P ′(s, π∗\n\nw(s))] + αH(πw(s))\n\n= Eτ\n\n(cid:34)\n\n(cid:88)\n\nγt\n\n(cid:32)\n\n(cid:88)\n\nt\n\na′\n\nπw(a′|s)A(s, a′) + Rw(st, at) + αH(πw(st))\n\n(cid:33)\n\n(cid:35)\n\n(cid:12) (cid:12)at = π∗ (cid:12)\n\nw(st)\n\nLet b(s) = Rw(s, π∗\n\nw(s)). Then V (s) is determined given πw and b,\n\nV (s) = Eτ\n\n(cid:34)\n\n(cid:88)\n\nγt\n\n(cid:32)\n\n(cid:88)\n\nt\n\na′\n\nπw(a′|s)A(s, a′) + b(st) + αH(πw(st))\n\n(cid:33)\n\n(cid:35)\n\n(cid:12) (cid:12)at = π∗ (cid:12)\n\nw(st)\n\nBy A(s, a) = α(log πw(a|s) − log πw(π∗\n\nw(s)|s)) = Q(s, a) − V ∗(s), w(s)|s)) = Rw(s, a) + γEs′[V (s′)|s′ ∼ P ′(s, a)] − V ∗(s)\n\nα(log πw(a|s) − log πw(π∗\n\nRw(s, a) = α log\n\n(cid:18) πw(a|s) πw(π∗\n\nw(s)|s)\n\n(cid:19)\n\n− γEs′[V (s′)|s′ ∼ P ′(s, a)] + V ∗(s)\n\n(19)\n\nTo summarize, for policy πw, we can construct a valid hidden reward function Rw via following process,\n\n1. Choose a function b : S ′ → R.\n\n2. Compute A(s, a) by Eq. 11.\n\n3. Compute V (s) and V ∗(s) by Eq. 17.\n\n4. Construct Rw(s, a) by Rw(s, π∗\n\nw(s)) = b(s) and Eq. 19.\n\nNow we show that, for any b : S ′ → R, under Rw constructed by the above process, V (s) is a stable point of the Bellman backup operator T ∗. This is straightforward. First, constructed Rw ensures\n\n16\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\nPublished as a conference paper at ICLR 2023\n\nthat α(log πw(a|s) − log πw(π∗ exp(Q(s, a)/α), which means πw is a solution for the maximization problem 8. So\n\nw(s)|s)) = Q(s, a) − V ∗(s) (Eq. 19) and therefore πw(a|w) ∝\n\n(T ∗V )(s) = max\n\nd\n\nαH(d) +\n\n(cid:88)\n\na\n\nd(a)(Rw(s, a) + γEs′[V (s′)])\n\n(cid:88)\n\n=\n\na\n\nπw(a|s)Q(s, a) + αH(πw(s)) = V (s).\n\n(20)\n\n(21)\n\nTheorem 5.1. For any ε > 0, there exists a mapping ̃πw where ̃πw(Rw) denotes the derived policy π∗ w in the NE of the hidden utility Markov game Mw = ⟨S, A, P, Rw, Rt⟩ induced by Rw, and a distribution PR : R → [0, 1] over the hidden reward space R, such that, for any adaptive policy πA ∈ arg maxπ′ ERw∼PR [J(π′, ̃πw(Rw))], πA approximately maximizes the ground-truth objective with at most an ε gap, i.e., EπH ∼PH [J(πA, πH )] ≥ maxπ′ EπH ∼PH [J(π′, πH )] − ε.\n\nProof. Let K(K > |A|) be a large positive integer. We construct a discretization of the policy space K where i ∈ [K], ∀s ∈ S, a ∈ A and (cid:80) Π by ΠK = {π : π(a|s) = i a π(a|s) = 1, ∀s ∈ S}. Note that ΠK is finite, i.e. |ΠK| ≤ (K + 1)|S|·|A|. Let M = |ΠK| and π1, π2, · · · , πM be an ordering of the policies in ΠK. For simplicity of notation, let δ = |A| K .\n\nGiven the discretization ΠK, it’s straightforward to specify the nearest policy ˆπ ∈ ΠK for any policy π ∈ Π. Formally, for any policy π ∈ Π, let G(π) = arg mini=1,...,M s,a |π(a|s) − πi(a|s)|. An obvious property of G is that, ∀s ∈ S, ||π(·|s) − G(π)(·|s)||∞ ≤ |A|\n\n(cid:80)\n\nK = δ.\n\nFor two policies π1 and π2, consider π1 playing with π2 and G(π2) respectively. Since the action distribution of π2 and G(π2) at each state differ at most δ, we have follows,\n\n|J(π1, π2) − J(π1, G(π2))| ≤\n\nγt · (1 − δ)t · δ ·\n\n2Rmax 1 − γ\n\n≤\n\n2δRmax (1 − γ)2\n\n(cid:88)\n\nt\n\n(22)\n\nWe can then derive a discretized approximation of the ground-truth policy distribution PH as follows,\n\nˆPH (π) = Prπ′∼PH [π = G(π′)]\n\n(23)\n\nWe could show that the difference between the objective under the ground-truth policy distribution PH and that under the approximated policy distribution ˆPH is bounded. By Eq. 22, for any adaptive policy πA,\n\n(cid:12) (cid:12)E\n\nπH ∼ ˆPH\n\n[J(πA, πH )] − EπH ∼PH [J(πA, πH )](cid:12)\n\n(cid:12) = (cid:12)\n\n(cid:12)EπH ∼PH [J(πA, G(πH )) − J(πA, πH )](cid:12) 2δRmax (1 − γ)2\n\n(cid:12) (24)\n\n(25)\n\n≤\n\nOn the other hand, consider following an iterative process to find hidden reward functions for policies in ΠK. For i = 1..M , we find hidden reward function R(i) w |1 ≤ j ≤ i − 1} and R(i) w could be constructed from πi as in Lemma 5.1. Notice that, by construction rule in Lemma 5.1, such R(i)\n\nw must exists since we can specify arbitrary b : S → R.\n\nw where R(i)\n\nw /∈ {R(j)\n\nw ) = πi, ∀i = 1 . . . M and the hidden reward distribution PR be PR(R(i)\n\nLet ̃πw(R(i) w ) = ˆPH (πi), ∀i = 1 · · · M . We immediately see that, for any adaptive policy πA, the objective is equivalent under the approximated policy distribution ˆPH and hidden reward function distribution PR,\n\nERw∼PR [J(πA, ̃πw(Rw))] = E\n\nπH ∼ ˆPH\n\n[J(πA, πH )]\n\n(26)\n\nFinally, for any adaptive policy πA ∈ arg maxπ′ ERw∼PR [J(π′, ̃πw(Rw))] and any policy π′ ∈ Π,\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nEπH ∼PH [J(πA, πH )] ≥ E\n\nπH ∼ ˆPH\n\n[J(πA, πH )] −\n\n2δRmax (1 − γ)2\n\n= ERw∼PR [J(πA, ̃πw(Rw))] −\n\n≥ ERw∼PR [J(π′, ̃πw(Rw))] −\n\n2δRmax (1 − γ)2 2δRmax (1 − γ)2\n\n= E\n\nπH ∼ ˆPH\n\n[J(π′, πH )] −\n\n≥ EπH ∼PH [J(π′, πH )] −\n\n2δRmax (1 − γ)2 4δRmax (1 − γ)2\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\nLet K ≥ 4|A|Rmax\n\nε(1−γ)2 and we have EπH ∼PH [J(πA, πH )] ≥ maxπ′ EπH ∼PH [J(π′, πH )] − ε.\n\nB ENVIRONMENT DETAILS\n\nFigure 7: All 5 layouts used in our work (from left to right): Asymmetric Advantage, Coordination Ring, Counter Circuit, Distant Tomato, and Many Orders, each featuring specific cooperation patterns we want to study.\n\nB.1 DESCRIPTION\n\nThe Overcooked Environment, first introduced in (Carroll et al., 2019), is based on the popular video game Overcooked where multiple players cooperate to finish as many orders as possible within a time limit. In this simplified version of the original game, two chiefs, each controlled by a player (either human or AI), work in grid-like layouts. Chiefs can move between non-table tiles and interact with table tiles by picking up or placing objects. Ingredients (e.g., onions and tomatoes) and empty dishes can be picked up from the corresponding dispenser tiles and placed on empty table tiles or into the pots. The typical pipeline for completing an order is (1) players put appropriate ingredients into a pot; (2) a pot starts cooking automatically once filled and takes a certain amount of time (depending on the recipe) to finish; (3) a player harvests the cooked soup with an empty dish and deliver it to the serving area.\n\nThe observation for an agent includes the whole layout, items on the counter and pots, player positions, orders, and time. The possible actions are up, down, left, right, no-op, and \"interacting\" with the tile the player is facing. Reward is given to both agents upon successful soup delivery, with the amount varying with the type of soup. An episode of the game terminates when the time limit is reached.\n\nThe environment used in (Carroll et al., 2019) has only onions as ingredients and onion soups as In our work, we evaluate all methods in three of them, namely Asymmetric Advantage, orders. Coordination Ring, and Counter Circuit, each designed to enforce a specific cooperation pattern.\n\nOur work introduces two new layouts: Distant Tomato and Many orders, with new ingredients and order types to make cooperation more challenging. In Distant Tomato, a dish of onion soup takes 20 ticks to finish and gives 20 rewards when delivered, while a tomato soup takes 10 ticks and gives the same reward but needs more movements to get the ingredient. The two players need to agree on which type of soup to cook in order to reach a high score. Failure in cooperation may result in tomato-onion soups that give no reward. In many orders, there are three types of orders: onion,\n\n18\n\nPublished as a conference paper at ICLR 2023\n\ntomato, and 1-onion-2-tomato. To fully utilize the three pots, the players need to work seamlessly in filling not just the pots near each of them but also the pot in the middle.\n\nWe show all the layouts in Fig.7. and conclude the cooperation pattern of our interest as follows.\n\n• Asymmetric Advantage tests whether the players can choose a strategy to their strengths. • Coordination Ring requires the players not to block each other when traveling between the\n\ntwo corners.\n\n• Counter Circuit embeds a non-trivial but efficient strategy of passing onions through the\n\nmiddle counter, which needs close cooperation.\n\n• Distant Tomato and Many Orders both encourage the players to reach an agreement on the\n\nfly in order to achieve a high reward.\n\nB.2 EVENTS\n\nIn Overcooked, we consider the following events for random search in HSP and reward shaping during training of all methods:\n\n• putting an onion/tomato/dish/soup on the counter, • picking up an onion/tomato/dish/soup from the counter, • picking up an onion from the onion dispenser, • picking up a tomato from tomato dispenser, • picking up a dish from the dish dispenser, • picking up a ready soup from the pot with a dish, • placing an onion/tomato into the pot, • valid placement: after the placement, we can finish an order with a positive reward by\n\nplacing other ingredients,\n\n• optimal placement: the placement is optimal if the maximum order reward we can achieve\n\nfor this particular pot is not decreased after the placement,\n\n• catastrophic placement: the placement is catastrophic if the maximum order reward we can\n\nachieve for this particular pot decreases from positive to zero after the placement,\n\n• useless placement: the placement is useless if the maximum order reward we can achieve\n\nfor this particular pot is already zero before the placement,\n\n• useful dish pickup: picking up a dish is useful when there are no dishes on the counter, and the number of dishes already taken by players is less than the total number of unready and ready soups,\n\n• delivering a soup to the serving area.\n\nAdditionally, in Distant Tomato, we consider the following events only for reward shaping,\n\n• placing a tomato into an empty pot, • optimal tomato placement: the placement is optimal and a tomato placement, • useful tomato pickup: the agent picks up a tomato when the partner isn’t holding a tomato,\n\nand there is a pot that is not full but only has tomatoes in it.\n\nC OVERCOOKED VERSION\n\nIn our experiments, we use two versions of Overcooked for a fair comparison with prior works and introduce challenging layouts. One version, in which we tested Asymmetric Advantages, Coordination Ring and Counter Circuit, is consistent with the \"neurips2019\" branch in the released GitHub repository of (Carroll et al., 2019). We remark that MEP (Zhao et al., 2021) also follows this version. Following this also allows us to perform an evaluation with human proxy models provided in the released code of (Carroll et al., 2019). The other version is an up-to-date version of Overcooked, which supports tomatoes and user-defined orders. We notice that a pot automatically starts cooking soup once there are three items in it in the former version, while it requires an additional \"interact\" action to start cooking in the latter version. This additional \"interact\" is required in the latter version since it supports orders with different amounts of ingredients. However, having an additional \"interact\" significantly influences a human player’s interactive experience. Therefore, we make modifications on the latter version to restrict orders to 3 items and support auto-cooking when there are 3 items. For more details, please refer to the released code.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nD IMPLEMENTATION DETAILS\n\nD.1 HSP\n\nAlgorithm 3: Hidden-Utility Self-Play for i = 1 → N do Train π(i)\n\na under sampled R(i) w ;\n\nw and π(i)\n\nend Run greedy policy selection to only keep K policies; Initial policy πA; repeat\n\nRollout with πA and sampled π(i) w ; Update πA;\n\nuntil enough iterations;\n\nThe pseudocode of HSP is shown in Algo. 3. We implemented HSP on top of MAPPO (Yu et al., 2021). Following the standard practice, we use multiprocessing to collect trajectories in parallel and then update the models. In the first stage, we use MLP policies, which empirically yield better results. In the second stage, we use RNN policies so that the adaptive policy could infer the intention of its partner by observing the history of its partner and make decisions accordingly for better adaptation. As suggested in(Tang et al., 2020), we add the identities of the policies in the policy pool as an additional feature to the critic. For better utilization of the computation resources, each environment sub-process loads a uniformly sampled policy and performs inference on CPUs, while the inference of the adaptive policy is batched across sub-processes in a GPU.\n\nD.2 BASELINES\n\nFor a fair comparison, we implement all baselines to be two-staged and train layout-specific agents.\n\nWe remark that our implementation of MEP achieves substantially higher scores than reported in the original paper (Zhao et al., 2021) when evaluated with the same human proxy models as MEP. All baselines are implemented with techniques stated above: loading policies from the pool per subprocess and the additional feature of identities of policies in the policy pool. We detail the baselines here and point out the difference with the original papers,\n\nFCP(Strouse et al., 2021): We list the differences between our implementation and the original FCP as follows,\n\n1. The original FCP uses image-based egocentric observations, while we use feature-based\n\nobservations as provided in Overcooked.\n\n2. The original FCP uses a pool size of 96 while we use 36. We empirically found 36 a sufficiently large pool size in our experiments. As shown in Table 21, in the three layouts that have human proxy models, there is no significant difference between using a pool size of 36 and of 72.\n\nMEP(Zhao et al., 2021): We list the differences between our implementation and the original MEP as follows,\n\n1. While the released code of MEP uses MLP policy in the second training stage, we found RNN policy to work better. Intuitively, for better cooperation, the adaptive policy should infer the intention of its partner by observing the state-action history.\n\n2. MEP uses a pool size of 15 while we use 36. 3. MEP uses prioritized sampling in the second stage, which favors weak policies in the pool, while we adopt uniform sampling for MEP since we found prioritized sampling not helpful with our carefully tuned implementation (shown in Table 5).\n\n4. In the released code of MEP, the policy updates are performed on data against only one policy from the pool, while we perform policy updates on data against many policies from the pool. This avoids the update from being biased towards some specific policies.\n\nTrajDiv(Lupu et al., 2021): While the original TrajDiv is tested in hand-crafted MDPs and Hanabi, we test TrajDiv in Overcooked. Although (Lupu et al., 2021) suggests training the adaptive policy and the policy pool together in a single stage, we choose to follow MEP and FCP to have a twostaged design that trains the adaptive policy in the second stage.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nUniform Sampling\n\nPrioritized Sampling\n\nPos. Asy. Adv. Coor. Ring Coun. Circ.\n\n1 2\n1 2\n\n291.7(4.6) 203.4(2.0) 284.6(3.2) 218.8(2.4)\n\n161.8(0.7) 164.2(2.1) 161.2(1.4) 167(4.5)\n\n108.8(4.2) 111.1(0.7) 94.4(2.3) 99.8(1.8)\n\nTable 5: Average episode reward and standard deviation (over 5 seeds) with different sampling methods of MEP. The \"1\" and \"2\" indicates the roles played by AI policies.\n\nBiased\n\nScripted\n\nAsymm. Adv. Coord. Ring Counter Circ. Dist. Tomato Many Orders\n\n0.56 0.59 0.56 0.70 0.55\n\n0.85 0.72 0.73 1.90 1.00\n\nTable 6: The average event-based difference of biased and scripted policies respectively.\n\nD.3 SCRIPTED POLICIES\n\nTo evaluate all methods with policies that have strong preferences, we consider the following scripted policies,\n\n• Onion/Tomato/Dish Everywhere continuously tries to put onions, tomatoes or dishes over\n\nthe counter.\n\n• Onion/Tomato Placement always tries to put onion or tomato into the pot.\n\n• Delivery delivers a ready soup to the serving area whenever possible.\n\n• Onion/Tomato Placement and Delivery puts tomatoes/onions into the pot in half of the time\n\nand tries to deliver soup in the other half of the time.\n\nFor Counter Circuit, we additionally consider a scripted policy, named Onion to Middle Counter, which keeps putting onions randomly over the counter in the middle of the layout.\n\nInput to these scripted policies is the ground-truth state of the game, which is accessible via the game simulator. When a scripted policy is unable to finish the event of its interest at some state, the scripted policy would walk to a random empty grid. For example, Onion Placement would choose a random walk when all pots are full. We ensure that these scripted policies are strictly different from policies in the policy pool of HSP. For more details, please refer to the released code.\n\ns\n\n}m∈M be the set of scripted policies. For convenience, let Π = {π(n)\n\nWe also provide evidence to show scripted policies are sufficiently different from those in the training pool. We use the expected event count of scripted and biased policies to support our claim. Recall that expected event count for a pair of policy πa, πb is EC(πa, πb) = E[(cid:80)T t=1 φ(st, at)|πa, πb]. Let πHSP be the HSP adaptive policy, {π(n) w }n∈[N ] be the set of biased policies in the training pool, and {π(m) }m∈M be the union of biased policies and scripted policies. For each policy π′ ∈ Π, we measure how the event-based difference close it is to the rest of policies in Π in the expected event count, i.e. (cid:80) EventDiffΠ(π′) = minπ′′∈Π\\{π′} k ck · |ECk(π′, πHSP ) − ECk(π′′, πHSP )| where ck is a frequency normalization constant. Then a large event-based difference indicates that π′ is sufficiently different from other policies in Π. We calculate the event-based difference for all biased and scripted policies. Table. 6 reports the average event-based difference between biased and scripted policies, respectively. Scripted policies consistently have a larger average event-based difference, indicating scripted policies are sufficiently different from biased policies, which are used for training the HSP adaptive policy.\n\nw }n∈[N ] ∪ {π(m)\n\ns\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nE TRAINING DETAILS\n\nE.1 HYPERPARAMETERS\n\nHSP and baselines are all two-staged solutions by first constructing a policy pool and then training an adaptive policy πA to maximize the game reward w.r.t. the induced pool.\n\nThe network architecture in both two stages is composed of 3 convolution layers with max pooling. Hyperparameters of these layers are listed in Table 7. Each layer is followed by a max pooling layer with a kernel size of 2. For MLP policies, we add two linear layers after the convolution. For RNN policies, we add a 1-layer GRU after the convolution and two linear layers after the GRU layer. The hidden sizes for these linear layers and the GRU layer are all 64. We use ReLU as the activation function between layers and LayerNorm after GRU and linear layers except the last one. The output is a 6-dim vector denoting the categorical action distribution.\n\nCommon hyperparameters for all methods in 5 layouts are listed in Table 8 and Table 9. Specifically, for MEP, we use the suggested hyperparameters from the original paper (Zhao et al., 2021). Detailed hyperparameters of MEP are shown in Table 10, where population entropy coef. adjusts the importance of the population entropy term. Detailed hyperparameters of TrajDiv are shown in Table 11, where traj. gamma is the discounting factor used in local action kernel and diversity coef. adjusts the importance of the diversity term. For each one of MEP, FCP and TrajDiv, we train 12 policies in the first stage and, following the convention of MEP (Zhao et al., 2021) and FCP (Strouse et al., 2021), take the init/middle/final checkpoints for each policy to build up the policy pool, leading to a pool size of 36. For HSP, we use a random search to first train 36 biased policies and then filter out 18 biased policies from them. We then combine these biased policies and past checkpoints of 6 policies in the policy pool of MEP to build up the policy pool of HSP, again leading to a pool size of 36.\n\nLayer Out Channels Kernel Size\n\nStride\n\nPadding\n\n1 2\n3\n\n32 64 32\n\n3 3\n3\n\n1 1\n1\n\n1 1\n1\n\nTable 7: CNN feature extractor hyperparameters.\n\ncommon hyperparameters\n\nvalue\n\nentropy coef. gradient clip norm GAE lambda gamma value loss huber delta mini batch size optimizer optimizer epsilon weight decay network initialization use reward normalization use feature normalization learning rate parallel environment threads ppo epoch environment steps episode length reward shaping horizon\n\n0.01 10.0 0.95 0.99 huber loss 10.0 batch size / mini-batch Adam 1e-5 0\nOrthogonal True True 5e-4 100 15 10M 400 100M\n\nTable 8: Common hyperparameters in the first stage.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nE.2 CONSTRUCTING THE POLICY POOL FOR HSP\n\nTo construct the policy pool for HSP, we perform a random search over possible hidden reward functions. Each reward function is formulated as a linear function over the event-based features, i.e. R = {Rw : Rw(s, a1, a2) = φ(s, a1, a2)T w, ||w||∞ ≤ Cmax} where φ : S × A × A → Rm specifies occurrences of different events when taking joint action (a1, a2) at state s. To perform random search, instead of directly sampling each wj from the section [−Cmax, Cmax], we sample each wj from a set of possible values Cj. We detail the Cj for each event on each layout here. Tab. 12 shows Cj in Asymmetric Advantages, Coordination Ring and Counter Circuit. Tab. 13 and Tab. 14 show Cj in Distant Tomato and Many Orders respectively. A detailed description of the events is shown in Sec. B.2. Note that in addition to events, we also include order reward as one element in a random search.\n\nTo filter out duplicated policies, we define an event-based diversity for a subset S, i.e. ED(S) = (cid:80) k − EC(j) k is the expected number of occurrences of event type k w . The coefficient ck balances the importance of different kinds of events. We\n\nk ck · |EC(i) for biased policy π(i)\n\nk | where ECi\n\ni,j∈S\n\n(cid:80)\n\nsimply set ck as a normalization constant, i.e. ck =\n\n(cid:16)\n\nmaxi∈[N ] EC(i)\n\nk\n\n(cid:17)−1\n\n.\n\ncommon hyperparameters\n\nvalue\n\nentropy coef. gradient clip norm GAE lambda gamma value loss huber delta mini batch size optimizer optimizer epsilon weight decay network initialization use reward normalization use feature normalization learning rate parallel environment threads ppo epoch environment steps episode length reward shaping horizon policy pool size\n\n0.01 10.0 0.95 0.99 huber loss 10.0 batch size / mini-batch Adam 1e-5 0\nOrthogonal True True 5e-4 300 15 100M 400 100M 36\n\nTable 9: Common hyperparameters in the second stage.\n\nhyperparameters\n\nvalue\n\npopulation entropy coef.\n\n0.01\n\nTable 10: MEP hyperparameters in the first stage.\n\nhyperparameters\n\nvalue\n\ntraj. gamma diversity coef.\n\n0.5 0.1\n\nTable 11: TrajDiv hyperparameters in the first stage.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nPicking up an onion from onion dispenser Picking up a dish from dish dispenser Picking up a ready soup from the pot Placing an onion into the pot Delivery Order reward\n\nCj\n\n-10, 0, 10 0, 10 -10, 0, 10 -10, 0, 10 -10, 0 0, 1\n\nTable 12: Cj for random search in Asymmetric Advantages, Coordination Ring and Counter Circuit.\n\nEvent\n\nPicking up an onion from onion dispenser Picking up a tomato from tomato dispenser Picking up a dish from dish dispenser Picking up a soup Viable placement Optimal placement Catastrophic placement Placing an onion into the pot Placing a tomato into the pot Delivery Order reward\n\nCj\n\n-5, 0, 5 0, 10, 20 0, 10 -5, 0, 5 -10, 0, 10 -10, 0, 10 0, 10 -10, 0, 10 -10, 0, 10 -10, 0 0, 1\n\nTable 13: Cj for random search in Distant Tomato.\n\nE.3 REWARD SHAPING\n\nWe use reward shaping during training in all layouts, detailed as follows,\n\n• In the first stage, the reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit is shown in Table. 15 and that for Distant Tomato and Many Orders is shown in Table. 17. Note that we do not use reward shaping when training biased policies for HSP in the first stage.\n\n• In the second stage, the reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit is shown in Table. 16. Reward shaping for Many Orders is shown in Table. 18 and that for Distant Tomato is shown in Table. 19. The factor of shaped reward anneals from 1 to 0 during the whole course of training in all layouts except Distant Tomato, in which the factor anneals from 1 to 0.5.\n\nF FULL RESULTS\n\nF.1 COOPERATION WITH LEARNED HUMAN MODELS\n\nTable 20 shows average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. All values within 5 standard deviations of the maximum episode return are marked in bold. These three simple layouts may not fully reflect the performance gap between the baselines and HSP. The results with learned human models are reported for a fair comparison with existing SOTA methods. Besides, our implementation of the baselines achieves substantially better results than their original papers with the same human proxy models, making the improvement margin look smaller. We also remark that the learned human models have limited representation power to imitate natural human behaviors that typically cover many behavior modalities. Here we give empirical evidence of the learned human models failing to fully reflect human behaviors.\n\nF.1.1 EMPIRICAL EVIDENCE\n\nThe original Overcooked paper (Carroll et al., 2019) collected human-play trajectories. We then collect game trajectories played by the learned human models and compare them with human-play\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nPicking up an onion from onion dispenser Picking up a tomato from tomato dispenser Picking up a dish from dish dispenser Picking up a soup Viable placement Optimal placement Catastrophic placement Placing an onion into the pot Placing a tomato into the pot Delivery Order reward\n\nCj\n\n-5, 0, 5 0, 10, 20 0, 5 -5, 0, 5 -10, 0, 10 -10, 0 0, 10 -3, 0, 3 -3, 0, 3 -10, 0 0, 1\n\nTable 14: Cj for random search in Many Orders.\n\nEvent\n\nValue\n\nOptimal placement Picking up a dish from dish dispenser Picking up a ready soup from the pot\n\n3 3\n5\n\nTable 15: Reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit in the first stage.\n\n(a) Asymmetric Advantages\n\n(b) Coordination Ring\n\n(c) Counter Circuit\n\nFigure 8: Trajectories induced by the learned human models and human players in Asymmetric Advantages, Coordination Ring and Counter Circuit. Each point or triangle denotes a trajectory with the X-axis coordinate being the self-cooking ratio, which is the ratio of onions the player places in the pot to the total amount of placements in the trajectory, and the Y-axis coordinate being the selfdelivery ratio, which is the ratio of deliveries given by the player to the total number of deliveries in the trajectory. Triangles and points denote trajectories induced by human players and learned human models, respectively. Different colors stand for different player indices. \"BC\" represents the learned human models, and \"Human\" denotes human players. Clearly, trajectories induced by the learned human models can not fully cover those by human players.\n\ntrajectories by measuring self-delivery ratio, i.e., the ratio of deliveries by the specific player to the total delivery number in a trajectory, and self-cooking ratio, which is the ratio of onions that the player places in the pot to the total pot placement number in a trajectory. The distributions of these trajectories are demonstrated in Fig. 8. From the figure, we can observe that the learned human models can not fully cover human behaviors. This suggests that evaluation results with the learned human models can not provide a comprehensive comparison among different methods.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nValue\n\nOptimal placement Picking up a dish from dish dispenser Picking up a ready soup from the pot\n\n3 3\n5\n\nTable 16: Reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit in the second stage.\n\nEvent\n\nValue\n\nPicking up a dish from dish dispenser Picking up a ready soup from the pot\n\n3 5\n\nTable 17: Reward shaping for Distant Tomato and Many Orders in the first stage.\n\nF.2 ABLATION STUDIES\n\nF.2.1 POOL SIZE\n\nTable 21 shows the average episode reward on 3 layouts with different sizes of the final policy pool for training the adaptive policy. Since increasing the pool size to 72 gives little improvement as suggested by the result, we use 36 in our experiments for computation efficiency.\n\nF.2.2 POLICY POOL CONSTRUCTION\n\nHSP has two techniques for the policy pool, i.e., (1) policy filtering to remove duplicated biased policies and (2) the use of MEP policies under the game reward for half of the pool size. We measure the performance with human proxies by turning these options off. For “HSP w.o. Filtering”, we keep all the policies by random search in the policy pool, which results in a larger pool size of 54 (18 MEP policies and a total of 36 random search ones). For“HSP w.o. MEP”, we exclude MEP policies in the policy pool and keep all the biased policies without filtering, which leads to the same pool size of 36. The results are shown in Table. 22.\n\nF.3 COOPERATION WITH SCRIPTED POLICIES WITH STRONG BEHAVIOR PREFERENCES\n\nTable 23 illustrates average episode reward and standard deviation (over 5 seeds) in all layouts with scripted policies. All values within a difference of 5 from the maximum value are marked in bold.\n\nF.4 HUMAN-AI EXPERIMENT\n\nF.4.1 EXPERIMENT SETTING\n\nWe recruited 60 volunteers by posting the experiment advertisement on a public platform. They are provided with a detailed introduction to the basic gameplay and the experiment process. The Overcooked game was deployed remotely on a server that the volunteers could access with their browsers. According to the feedback, over 90 percent of volunteers had no prior experience with Overcooked. We uniformly divided 60 volunteers into 5 groups assigned to each of the 5 layouts. We designed the experiment to last around 30 minutes for each volunteer to ensure the validity of the data. Due to availabilities of volunteers, experiments are conducted within two consecutive days. The experiment has two stages. In the first stage, which is called the warm-up stage, the participants are encouraged to explore the behaviors of 4 given AI agents without a time limit. After the first stage, they are required to comment on their game experience, e.g., whether the AI agents are cooperative and comfortable to play with, and rank the agents accordingly. In the second stage, each participant is instructed to achieve scores as high as they could in 24 games (4 AI agents × 2 player positions × 3 repeats).\n\nWe remark that, on the environment side, different from human-AI experiments performed by prior works (Zhao et al., 2021; Carroll et al., 2019) in Overcooked, we slow down the AI agents so that the AI agents have similar speed with human players. More specifically, 7 idle steps are inserted before each step of the AI agent. Such an operation is necessary since, in our prior user studies, we find that human players commonly feel uncomfortable if the AI agent is much faster and human players could contribute little to the score.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nValue\n\nPicking up a dish from the dish dispenser Picking up a ready soup from the pot\n\n3 5\n\nTable 18: Reward shaping for Many Orders in the second stage.\n\nEvent\n\nPicking up a dish from the dish dispenser Picking up a ready soup from the pot Useful tomato pickup Optimal tomato placement Placing a tomato into an empty pot\n\nValue\n\n3 5\n10 5\n-15\n\nTable 19: Reward shaping for Distant Tomato in the second stage.\n\nF.4.2 HUMAN FEEDBACK\n\nWe collected and analyzed the feedback from the participants to see how they felt playing with AI agents. Here we summarize the typical reflections.\n\n1. In Coordination Ring, the most annoying thing reported is players blocking each other during movement. To effectively maneuver in the ring-like layout, players must reach a temporary agreement on either going clockwise or counterclockwise. HSP is the only AI able to make way for the other player, while others can not recover by themselves once stuck. For example, both FCP and TrajDiv players tend to take a plate and wait next to the pot immediately after one pot is filled. But they can neither take a detour when blocked on their way to the dish dispenser nor yield their position to the human player trying to pass through. The video recorded in the human study can be found in Part 4.2 of https://sites.google.com/view/hsp-iclr.\n\n2. In Counter Circuit, one efficient strategy is passing onion via the counter in the middle of the room: a player at the bottom fetches onions and places them on the counter, while another player at the top picks up the onions and puts them into pots. HSP is the only AI player capable of this strategy in both top and bottom places and performs the highest onion passing frequency cooperating with human players as shown in Figure. 6b.\n\n3. In Distant Tomato, one critical thing is that mixed (onion-tomato) soups give no reward, which means two players need to agree on the soup to cook. All AI agents perform well when the other player focuses on onion soups. However, all AI agents except for HSP fail to deal with tomato-preferring partners as shown in Table. 3. FCP, MEP, or TrajDiv agents never actively choose to place tomatoes and keep placing onions even when a pot has tomatoes in it, resulting in invalid orders. On the contrary, HSP chooses to place tomatoes when there are tomatoes in the pot. Participants commonly agree that the HSP agent is the best partner to play with in this layout. The video recorded in the human study can be found in Part 4.2 of https://sites.google.com/view/hsp-iclr.\n\n4. In Many Orders, most participants claim that HSP is able to pick up soups from all three pots, while other AI agents only concentrate on the pot in front of them and ignore the middle pot even if the human player attempts to use it. Table. 4 shows that HSP agent picks up most soups from the middle pot and meanwhile gets the highest average episode reward.\n\nF.4.3 HUMAN PREFERENCE ON DIFFERENT AI AGENTS\n\nFigure. 9 illustrates human preference for different AI agents. In all layouts except a relatively restricted and simple layout, Coordination Ring, human players strongly prefer HSP over other AI agents. In Coordination Ring, though human players rank MEP above HSP, HSP is still significantly better than FCP and TrajDiv.\n\nCalculation Method: Human preference for different methods is computed as follows. Assume we are comparing human preference between method A and method B. Let N be the total number of human players attending the experiments in one layout, NA be the number of human players who\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nrank A over B, and NB be the number of those who rank B over A. \"Human preference for method A over method B\" is computed as NA\n\nN − NB N .\n\n(a) Asy. Adv.\n\n(b) Coor. Ring\n\n(c) Coun. Circ.\n\n(d) Dis. Toma.\n\n(e) Many Ord.\n\nFigure 9: Human preference for row partner over column partner in all layouts.\n\nF.4.4 SCORES IN THE SECOND STAGE\n\nTable 24 shows average reward per episode during the second stage in all layouts. All methods have comparable episode rewards in Asymm. Adv and Coord. Ring. There is no room for improvement since all the methods have reached the highest possible rewards. In Counter Circ., the most complex layout in this category, HSP achieves a better performance than baselines: HSP has a 155+ reward while the most competitive baseline MEP has a reward of 134+. We remark that the reward difference between HSP and MEP is around 20, which is exactly the value of 1 onion soup delivery. This implies that the HSP agent can, on average, deliver one more soup than all the baselines per game episode with humans, which is a significant improvement.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nPos. Asy. Adv. Coor. R. Coun. Circ.\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\n1 2\n1 2\n1 2\n1 2\n\n282.8(9.4) 203.8(8.2) 291.7(4.6) 203.4(2.0) 289.3(8.8) 194.2(0.7) 300.3(2.2) 217.1(3.3)\n\n161.3(1.6) 161.0(2.7) 161.8(0.7) 164.2(2.1) 150.8(3.1) 142.1(2.3) 160.0(2.6) 160.6(3.3)\n\n95.9(2.0) 92.7(1.3) 108.8(4.2) 111.1(0.7) 60.1(5.0) 53.7(12.4) 107.4(3.5) 106.6(3.0)\n\nTable 20: Average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. All values within 5 standard deviations of the maximum episode return are marked in bold. The Pos. column indicates the roles played by AI policies.\n\nPos. Asy. Adv. Coor. R. Coun. Circ. Asy. Adv. Coor. R. Coun. Circ.\n\npolicy pool size = 36\n\npolicy pool size = 72\n\nFCP\n\nMEP\n\nTrajDiv\n\n1 2\n1 2\n1 2\n\n282.8(9.4) 203.8(8.2) 291.7(4.6) 203.4(2.0) 289.3(8.8) 194.2(0.7)\n\n161.3(1.6) 161.0(2.7) 161.8(0.7) 164.2(2.1) 150.8(3.1) 142.1(2.3)\n\n95.9(2.0) 92.7(1.3) 108.8(4.2) 111.1(0.7) 60.1(5.0) 53.7(12.4)\n\n278.3(16.0) 200.9(13.2) 298.2(5.4) 207.8(7.3) 270.8(2.5) 192.8(8.7)\n\n158.9(0.6) 156.9(4.7) 157.3(2.7) 158.9(3.0) 142.5(2.8) 137.3(4.9)\n\n91.9(7.5) 90.7(4.8) 104.6(5.0) 105.0(2.2) 70.1(6.7) 63.8(8.2)\n\nTable 21: Average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. The Pos. column indicates the roles played by AI policies.\n\nHSP w.o. MEP (pool size = 36)\n\nHSP w.o. Filtering (pool size = 54)\n\nHSP (pool size = 36)\n\nPos. Asy. Adv. Coor. R. Cou. Circ.\n\n1 2\n1 2\n1 2\n\n308.5(4.4) 219.6(15.9) 311.3(8.1) 209.3(4.0) 300.3(2.2) 217.1(3.3)\n\n157.5(3.0) 157.7(2.5) 139.2(5.6) 138.5(3.1) 160.0(2.6) 160.6(3.3)\n\n94.0(2.7) 100.4(1.1) 80.1(4.6) 88.7(0.9) 107.4(3.5) 106.6(3.0)\n\nTable 22: Average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. The Pos. column indicates the roles played by AI policies.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nm o\nr f\n\n5\n\nf o\n\ne c\nn e\nr e\nf f\ni\n\nd\n\na\n\nn\n\ni\n\nh\n\nt i\n\nW\n\nn\n\ni\n\nh\n\nt i\n\nw\n\ns e\nu\n\nl a\nv\n\nl l\n\nA\n\n. s\ne i\nc i\nl\n\no p\n\nd e\nt\n\np\n\ni r\nc s\n\nh\n\nt i\n\nw\n\ns t\n\nu o\ny a\nl\n\nl l\na\n\nn\n\ni\n\n) s\nd e\ne s\n\n5\n\nr e\nv o\n(\n\nn o\n\ni t\na i\n\nv e\nd\n\nd r\na d\nn a\nt s\n\nd n\na\n\nd r\na w\ne r\n\ne d\no s\ni\n\np e\n\ne g\na r\ne v\nA\n\n:\n\n3 2\n\ne l\n\nb a\nT\n\n. s\ne i\nc i\nl\n\no p\n\nI\n\nA y\nb\n\nd e\ny a\nl\n\np\n\ns e\nl\n\no r\n\ne h\n\nt\n\ns e\nt a\nc i\n\nd n\n\ni\n\n\" 2\n\"\n\nd n\na\n\n\" 1\n\"\n\ne h\nT\n\n.\n\nd\n\nl\n\no b\n\nn\n\ni\n\nd e\nk r\na\n\nm\n\ne r\na\n\nd r\na w\ne r\n\nm u\nm x\na\n\ni\n\nm\n\n) 4\n\n.\n\n7 1\n(\n\n3\n\n.\n\n6 9\n3\n\n) 9\n\n.\n\n4 (\n\n9\n\n.\n\n9 2\n2\n\n) 9\n\n.\n\n0 1\n(\n\n7\n\n.\n\n1 2\n1\n\n) 2\n\n.\n\n7 (\n\n) 4\n\n.\n\n3 (\n\n0\n\n.\n\n6 1\n1\n\n4\n\n.\n\n4 0\n1\n\n) 7\n\n.\n\n4 (\n\n8\n\n.\n\n7 7\n\n) 2\n\n.\n\n3 (\n\n1\n\n.\n\n7 4\n1\n\n) 8\n\n.\n\n8 1\n(\n\n9\n\n.\n\n8 7\n2\n\n) 0\n\n.\n\n2 2\n(\n\n9\n\n.\n\n1 3\n2\n\n) 5\n\n.\n\n6 (\n\n) 9\n\n.\n\n3 (\n\n9\n\n.\n\n9 1\n3\n\n3\n\n.\n\n2 2\n3\n\n) 4\n\n.\n\n2 (\n\n) 3\n\n.\n\n3 (\n\n3\n\n.\n\n7 5\n3\n\n4\n\n.\n\n1 7\n3\n\n) 3\n\n.\n\n4 1\n(\n\n8\n\n.\n\n0 2\n1\n\n) 7\n\n.\n\n7 (\n\n) 7\n\n.\n\n3 (\n\n9\n\n.\n\n4 1\n1\n\n5\n\n.\n\n0 1\n1\n\n) 5\n\n.\n\n3 (\n\n3\n\n.\n\n9 7\n\n) 1\n\n.\n\n3 (\n\n) 7\n\n.\n\n9 (\n\n) 2\n\n.\n\n8 (\n\n3\n\n.\n\n9 4\n1\n\n7\n\n.\n\n7 7\n2\n\n2\n\n.\n\n7 3\n2\n\n) 1\n\n.\n\n2 1\n(\n\n7\n\n.\n\n5 1\n3\n\n) 0\n\n.\n\n4 (\n\n7\n\n.\n\n6 2\n3\n\n) 9\n\n.\n\n8 (\n\n) 4\n\n.\n\n3 (\n\n) 7\n\n.\n\n6 (\n\n) 3\n\n.\n\n5 (\n\n7\n\n.\n\n0 5\n3\n\n8\n\n.\n\n1 2\n2\n\n1\n\n.\n\n8 1\n1\n\n1\n\n.\n\n7 0\n1\n\n) 4\n\n.\n\n3 1\n(\n\n1\n\n.\n\n8 7\n\n) 5\n\n.\n\n1 (\n\n0\n\n.\n\n5 5\n\n) 5\n\n.\n\n5 (\n\n9\n\n.\n\n7 0\n1\n\n) 8\n\n.\n\n4 (\n\n2\n\n.\n\n7 1\n\n) 6\n\n.\n\n7 1\n(\n\n3\n\n.\n\n4 5\n1\n\n) 7\n\n.\n\n5 (\n\n) 5\n\n.\n\n1 (\n\n6\n\n.\n\n4 5\n2\n\n6\n\n.\n\n2 9\n2\n\n) 2\n\n.\n\n5 2\n(\n\n5\n\n.\n\n6 9\n2\n\n) 0\n\n.\n\n6 (\n\n2\n\n.\n\n8 5\n3\n\n) 1\n\n.\n\n1 1\n(\n\n7\n\n.\n\n5 1\n1\n\n) 2\n\n.\n\n5 (\n\n5\n\n.\n\n7 0\n1\n\n) 1\n\n.\n\n2 1\n(\n\n0\n\n.\n\n6 8\n\n) 9\n\n.\n\n2 (\n\n3\n\n.\n\n9 5\n\n) 6\n\n.\n\n5 (\n\n5\n\n.\n\n9 0\n1\n\n) 1\n\n.\n\n4 1\n(\n\n6\n\n.\n\n9 2\n\n) 6\n\n.\n\n1 2\n(\n\n) 1\n\n.\n\n0 1\n(\n\n3\n\n.\n\n5 7\n1\n\n8\n\n.\n\n3 6\n2\n\n) 2\n\n.\n\n3 (\n\n8\n\n.\n\n8 9\n2\n\n) 8\n\n.\n\n2 (\n\n9\n\n.\n\n5 6\n3\n\n) 3\n\n.\n\n5 (\n\n) 2\n\n.\n\n3 (\n\n3\n\n.\n\n0 3\n2\n\n4\n\n.\n\n3 2\n1\n\n) 3\n\n.\n\n7 (\n\n) 2\n\n.\n\n6 (\n\n) 4\n\n.\n\n2 (\n\n0\n\n.\n\n9 9\n\n4\n\n.\n\n7 8\n\n5\n\n.\n\n2 5\n\n) 8\n\n.\n\n5 (\n\n7\n\n.\n\n0 1\n1\n\n) 6\n\n.\n\n3 (\n\n3\n\n.\n\n4 1\n\n) 4\n\n.\n\n7 (\n\n8\n\n.\n\n4 7\n1\n\n) 5\n\n.\n\n0 6\n(\n\n) 4\n\n.\n\n2 1\n(\n\n6\n\n.\n\n7 2\n2\n\n2\n\n.\n\n7 2\n3\n\n) 7\n\n.\n\n5 2\n(\n\n0\n\n.\n\n5 9\n2\n\n) 5\n\n.\n\n1 (\n\n) 6\n\n.\n\n3 (\n\n) 1\n\n.\n\n3 (\n\n8\n\n.\n\n6 6\n3\n\n6\n\n.\n\n4 2\n1\n\n4\n\n.\n\n1 0\n1\n\n) 9\n\n.\n\n3 (\n\n) 1\n\n.\n\n1 (\n\n6\n\n.\n\n0 9\n\n5\n\n.\n\n3 5\n\n) 5\n\n.\n\n6 (\n\n1\n\n.\n\n2 1\n1\n\n) 6\n\n.\n\n7 1\n(\n\n9\n\n.\n\n5 2\n\n) 9\n\n.\n\n9 (\n\n9\n\n.\n\n5 8\n1\n\n) 2\n\n.\n\n1 6\n(\n\n) 7\n\n.\n\n2 1\n(\n\n1\n\n.\n\n4 2\n2\n\n1\n\n.\n\n9 2\n3\n\n) 3\n\n.\n\n5 (\n\n) 9\n\n.\n\n3 (\n\n) 2\n\n.\n\n7 (\n\n7\n\n.\n\n5 6\n3\n\n5\n\n.\n\n0 3\n2\n\n1\n\n.\n\n2 1\n1\n\n) 4\n\n.\n\n3 (\n\n0\n\n.\n\n7 9\n\n) 5\n\n.\n\n9 (\n\n) 2\n\n.\n\n5 (\n\n7\n\n.\n\n2 6\n\n1\n\n.\n\n6 5\n\n) 6\n\n.\n\n2 1\n(\n\n4\n\n.\n\n2 7\n\n) 6\n\n.\n\n7 (\n\n8\n\n.\n\n7 1\n\n) 3\n\n.\n\n7 (\n\n3\n\n.\n\n8 5\n1\n\n) 8\n\n.\n\n2 1\n(\n\n8\n\n.\n\n9 7\n2\n\n) 9\n\n.\n\n4 (\n\n2\n\n.\n\n4 2\n3\n\nP S\nH\n\ni\n\nv D\nj a\nr\n\nT\n\nP E\nM\n\nP C\nF\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n) 8\n\n.\n\n0 2\n(\n\n0\n\n.\n\n4 0\n3\n\n.\n\na l\n\nP\n\n. i\n\nn O\n\ns t\n\np\n\ni r\nc S\n\n) 9\n\n.\n\n2 (\n\n9\n\n.\n\n4 6\n3\n\n) 7\n\n.\n\n8 (\n\n0\n\n.\n\n6 0\n1\n\n) 1\n\n.\n\n4 (\n\n9\n\n.\n\n1 9\n\n) 8\n\n.\n\n8 (\n\n0 8\n\n.\n\n4 6\n\n) 3\n\n.\n\n5 (\n\n9\n\n.\n\n7 5\n\n) 5\n\n.\n\n1 1\n(\n\n0\n\n.\n\n3 7\n\n) 6\n\n.\n\n2 (\n\n5\n\n.\n\n3 1\n\n) 9\n\n.\n\n4 (\n\n5\n\n.\n\n7 9\n1\n\n) 5\n\n.\n\n9 1\n(\n\n4\n\n.\n\n5 8\n2\n\n) 6\n\n.\n\n5 (\n\n1\n\n.\n\n4 3\n3\n\n. i\nl e\nD &\n\n.\n\na l\n\nP\n\n. i\n\nn O\n\n.\n\ny r\ne v\nE\n\n. i\n\nn O\n\n.\n\ny r\ne v\nE\n\nh s\ni\n\nD\n\n.\n\ny r\ne v\nE\n\n. i\n\nn O\n\n.\n\ny r\ne v\nE\n\nh s\ni\n\nD\n\n.\n\nu o\nC\n\n.\n\nd\n\ni\n\nM\n\n2\n\n. i\n\nn O\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n. i\nl e\nD &\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n. i\nl e\nD &\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n.\n\nv d\nA\n\n.\n\ny s\n\nA\n\ng n\n\ni\n\nR\n\n. r\no o\nC\n\n.\n\nc r\ni\n\nC\n\n.\n\nn u\no C\n\n.\n\na\n\nm o\nT\n\n. s\ni\n\nD\n\n.\n\nd r\n\nO y\nn a\n\nM\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nPos. Asy. Adv.\n\nCoor. Ring Cou. Circ. Dis. Toma. Many Ord.\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\n1 2\n1 2\n1 2\n1 2\n\n339.3(38.17) 321.3(34.80) 329.7(45.97) 324.2(39.93) 329.0(43.18) 318.8(48.97) 336.0(35.55) 318.8(48.97)\n\n185.0(19.73) 180.7(22.98) 193.3(22.11) 183.6(26.75) 184.7(28.60) 176.8(31.33) 185.5(38.92) 188.9(22.00)\n\n127.7(28.14) 118.3(29.20) 136.9(27.00) 134.5(28.63) 112.6(25.78) 105.0(31.05) 158.0(28.56) 155.2(23.43)\n\n351.3(82.25) 320.5(66.49) 341.9 (65.07) 313.9(78.29) 327.1(71.04) 316.0(77.65) 331.6(61.33) 305.9(58.61)\n\n312.4(58.73) 321.3(61.12) 322.0(50.53) 319.2(52.98) 312.9(62.82) 334.2(57.99) 384.3(47.50) 380.7(62.27)\n\nTable 24: Average reward per episode in all layouts with human players in the second stage.\n\n31",
  "translations": [
    "# Summary Of The Paper\n\nThe authors train agents with self play that have diverse preferences that differ from the original task and show that having a richer pool of agents generated with this technique yields policies that are better adapted to human coordination.\n\n# Strength And Weaknesses\n\nThe empirical results are excellent. In particular, I think the combination of ablations, human experiments, scripted bots, and imitation-trained policies go beyond most any other works and clarify key issues that were not carefully analyzed in previous works. For instance, the authors clearly show that the techniques used in most evaluations (imitation-learned policies and human interaction) are highly confounded since people adapt, and the imitation-learned policies don’t show much diversity. Their method only weakly improves over baselines in these tasks. In contrast, the use of specific scripted probes and a more qualitative evaluation revealed large discrepancies between actual coordination performance in the most important edge cases. \n\nThe algorithmic contribution is a weakness as it depends on significant hand-tuning of custom features specific for these specific Overcooked environments. I do not see how this approach could be easily adapted to a new tasks (or even an Overcooked level with different dynamics). On its own, I do not think this algorithm is a sufficient contribution to literature. I would have also liked to see comparisons or thoughts on more model-based towards generating diversity in Overcooked for example: Wu, Sarah A., et al. \"Too Many Cooks: Bayesian Inference for Coordinating Multi‐Agent Collaboration.\" Topics in Cognitive Science 13.2 (2021): 414-432.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written with significant details available in the appendix. The work is original. The units in Figure 5 should be explained in the text\n\n# Summary Of The Review\n\nBased on the new method alone I would not accept this paper. However, the thoroughness of evaluation sets a new standard and I feel that I learned something new and important from these empirical analyses. I would like to cite this paper in the future and that should be sufficient for acceptance. I would raise my score further if the authors can more greatly emphasize these contributions in their work\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Hidden-Utility Self-Play (HSP) aimed at training agents to cooperate with humans in a zero-shot manner, without relying on human data. The authors argue that traditional reinforcement learning approaches often fail to optimize for human preferences due to a misalignment between environment reward functions and human biases. HSP models these biases as hidden reward functions, facilitating the creation of a diverse policy pool. The method is evaluated in the Overcooked benchmark, where it outperforms baseline methods in terms of rewards and human feedback, demonstrating its effectiveness in fostering cooperation under varied human behaviors.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to modeling human biases as hidden reward functions, which is a significant advancement in multi-agent reinforcement learning (MARL). The empirical results show that HSP can effectively improve cooperation in complex scenarios that require adaptation to diverse human behaviors. The thorough evaluation on the Overcooked benchmark and inclusion of human participant studies lend credibility to the findings. However, the paper could benefit from clearer explanations of the algorithm's implementation and the specific parameters used in the experiments, as these details are essential for reproducibility. Additionally, while the approach is promising, its generalizability beyond the tested environment remains uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that guides the reader through the proposed method and its evaluations. The quality of the experiments is high, though some technical details regarding the implementation of the policy pool generation could be elaborated for better reproducibility. The novelty of the approach lies in its assumption of human biases, which distinguishes it from existing methods. However, the paper could enhance its reproducibility by providing more comprehensive descriptions of the experimental setup and parameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to enhancing AI cooperation with humans by modeling biases in a zero-shot context. The methodology is innovative, and the empirical results are promising, although some aspects regarding clarity and reproducibility could be improved.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Learning Zero-Shot Cooperation with Humans, Assuming Humans are Biased\" introduces a novel framework called Hidden-Utility Self-Play (HSP) that enables intelligent agents to cooperate with humans without prior human data. The methodology focuses on modeling human biases as hidden reward functions during the self-play training process. The results demonstrate that HSP significantly improves the performance of agents in cooperative tasks, particularly in the Overcooked game, where agents showed enhanced adaptability to diverse human strategies compared to several baselines. The evaluation included rigorous testing against learned human models, scripted policies, and real human participants, all of which highlighted the effectiveness of the proposed approach.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative introduction of HSP, which addresses a critical gap in existing frameworks by modeling human biases, thereby improving the adaptability of AI agents. The comprehensive evaluation across different types of partners ensures the robustness of the findings. Additionally, the positive feedback from human participants underscores the practical relevance of the research. However, weaknesses include the potential limitations in applicability to domains lacking clear event features, the computational intensity involved in generating a diverse policy pool, and the dependency on the accuracy of learned human models, which may affect generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of the methodology and results. The quality of the experimental design is high, providing a thorough evaluation of the proposed method. The novelty is substantial, as the paper presents a unique approach to zero-shot cooperation modeling. However, reproducibility may be hindered by the complexity and computational demands of the HSP framework, which could pose challenges for implementation in other contexts.\n\n# Summary Of The Review\nThe paper offers significant advancements in the field of human-AI cooperation through the introduction of the HSP framework, which effectively models human biases to enhance agent adaptability. Despite some limitations regarding computational costs and domain applicability, the empirical results are compelling and demonstrate the framework's practical utility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Zero-Shot Cooperation with Humans, Assuming Humans are Biased\" presents a novel approach called **Hidden-Utility Self-Play (HSP)** designed to train agents for effective cooperation with humans while accounting for human biases. The authors critique existing multi-agent reinforcement learning (MARL) frameworks for their assumption that human objectives align with environmental rewards. HSP introduces a framework that incorporates hidden reward functions to model these biases, and it is evaluated on the Overcooked benchmark. The findings indicate that HSP significantly improves cooperative behaviors with learned human models, scripted policies, and real human participants, outperforming various baseline methods.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative methodology that addresses a significant gap in current MARL frameworks by explicitly modeling human biases. The theoretical foundations provided—specifically the formulation of the Hidden-Utility Markov Game and the accompanying theorems—are well-articulated and contribute to a deeper understanding of the problem space. The experimental results are robust, showing clear advantages over existing methods in diverse scenarios. However, a potential weakness is the reliance on the Overcooked environment, which may limit the generalizability of the findings to other complex real-world scenarios. Additionally, while the paper discusses future work, it could further elaborate on potential challenges and limitations in automating reward function design.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, providing a clear progression from problem definition to methodology and results. The theoretical contributions are presented coherently, and the experimental design is detailed enough to allow for reproducibility. The novelty of the HSP approach in modeling human biases is significant, although the empirical evaluation could benefit from additional metrics or environments to strengthen the findings further. Overall, the quality of the research is high, with a clear contribution to the field of human-AI interaction.\n\n# Summary Of The Review\nThis paper presents a compelling method for improving agent cooperation with humans by acknowledging and modeling human biases through the Hidden-Utility Self-Play framework. The results demonstrate a significant advancement in the field, although further exploration in diverse environments could enhance the applicability of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel Hidden-Utility Self-Play (HSP) framework designed to improve human-AI cooperation in multi-agent reinforcement learning (MARL) settings by modeling human biases. The methodology involves generating hidden reward functions to better align AI behavior with human partners, addressing the challenge of convention overfitting. Empirical findings demonstrate that HSP outperforms existing models on the Overcooked benchmark when cooperating with both learned human models and actual human players, showcasing its effectiveness across diverse cooperative scenarios.\n\n# Strength And Weaknesses\nThe paper makes several noteworthy contributions, particularly in its introduction of the HSP framework, which effectively captures human biases in cooperative tasks. However, the complexity involved in implementing hidden reward functions could pose challenges for practitioners lacking expertise. While the empirical validation is robust, relying on learned human models may limit the generalizability of the findings. The approach to generating a diverse policy pool is innovative, yet it demands substantial computational resources, potentially restricting its practical application. Additionally, the evaluation's reliance on human feedback raises concerns about bias, and while the paper encourages future research directions, it does not provide immediate solutions to existing challenges.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear introduction that effectively motivates the need for the HSP framework. The algorithmic contributions are articulated with sufficient detail, enhancing understanding of the proposed method's advancements in MARL. However, the depth of related work could be improved to better contextualize HSP's unique contributions within the broader field. Reproducibility may be impacted by the complexity of the implementation and the need for significant computational resources.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of human-AI cooperation through the introduction of the HSP framework. While it showcases strong empirical results and addresses key challenges, the complexities of implementation and potential biases in human feedback may limit its immediate applicability and generalizability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework called **Hidden-Preference Self-Play (HPSP)** designed to improve collaboration between AI agents and human partners in zero-shot environments. It innovatively incorporates hidden reward functions to model the diverse and often biased preferences of humans, which allows for the generation of a diverse policy pool. The methodology includes defining a hidden-preference Markov game and utilizing random search for policy diversity, demonstrating superior performance in the **Overcooked** benchmark compared to traditional methods like Fictitious Co-Play (FCP) and Maximum Entropy Population-based training (MEP).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to human-AI cooperation by introducing hidden preferences, addressing a significant gap in existing self-play frameworks. The empirical validation is robust, employing both scripted and real human interactions, which enhances the study's credibility. However, the complexity of implementing HPSP may pose challenges, particularly regarding computational intensity. Additionally, the framework's dependency on human models could limit its applicability in real-world scenarios if those models do not accurately represent human behavior.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the framework's contributions and methodology. The quality of the empirical results is high, providing a comprehensive evaluation of HPSP's effectiveness. The novelty of the approach is significant, addressing the real challenges posed by human biases in collaborative settings. However, the reproducibility might be affected by the complexity of the model and the reliance on specific human behavior simulations, which may not be easily replicable in different contexts.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of human-AI cooperation through its innovative HPSP framework, effectively addressing the complexities of human preferences. While the empirical results are compelling, the practical implementation and generalizability of the model may require further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Learning Robust Adversarial Policies via Hidden-Utility Self-Play\" presents a novel framework known as Hidden-Utility Self-Play (HSP) aimed at improving adversarial training within multi-agent reinforcement learning (MARL). The main contributions include the development of HSP, which models adversarial strategies through hidden reward functions, allowing for a more diverse exploration of adversarial behaviors during training. The methodology involves generating a broad pool of policies optimized against these hidden rewards, followed by training an adaptive policy to maximize expected returns. Experimental results demonstrate that agents trained with HSP outperform traditional adversarial training methods, achieving higher rewards and exhibiting better cooperation in dynamic environments.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to modeling adversarial biases and the comprehensive empirical evaluation conducted on the Overcooked benchmark, which strengthens the validity of the findings. The framework's potential to enhance robustness in multi-agent systems is a significant contribution to the field. However, the implementation complexity of hidden utility functions poses a challenge for real-world applications, particularly in high-dimensional action spaces. Additionally, the computational cost associated with the random search for policy generation raises scalability concerns.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical structure that guides the reader through the contributions, methodology, and results. The quality of the experiments is high, and the authors provide sufficient detail for reproducibility, although the computational demands of the approach may limit accessibility for some practitioners. The novelty of the hidden utility functions represents a significant advancement in the field, addressing limitations in existing adversarial training models.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful advancement in adversarial training through the introduction of the Hidden-Utility Self-Play framework. It establishes a novel approach to modeling adversarial behaviors that enhances the robustness of agents in multi-agent environments, although practical implementation challenges may arise. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the Hidden-Utility Self-Play (HSP) framework, which is claimed to be a novel method for training AI agents to cooperate with humans without requiring human data. The authors propose that modeling human biases as hidden reward functions enhances the adaptability of AI agents and introduce a linear function approximation of the reward space. Empirical evaluations on the Overcooked benchmark are conducted to demonstrate the superiority of HSP over traditional self-play methods, with claims of improved generalization capabilities to unseen human behaviors.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its attempt to address the challenge of AI-human cooperation through a structured framework. However, the contributions are largely overstated. The HSP framework introduces minimal advancements over existing models, with its methodology being incremental rather than transformative. The empirical evaluations show only marginal improvements, which could be attributed to random fluctuations rather than the proposed method's effectiveness. Furthermore, the paper fails to contextualize its contributions adequately within the existing literature, limiting its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but the clarity is compromised by overstatements regarding the novelty and impact of the proposed method. The quality of the empirical results lacks rigorous statistical backing, raising concerns about their reproducibility. The novelty of the HSP framework is questionable, as it builds upon established concepts in reinforcement learning without offering substantial new insights.\n\n# Summary Of The Review\nOverall, while the paper attempts to introduce a new framework for AI-human cooperation, its contributions are minimal and primarily reiterate existing methods with slight modifications. The empirical results do not convincingly demonstrate the claimed advantages, leading to concerns about the actual significance of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Hidden-Utility Self-Play (HSP), a novel framework aimed at enhancing multi-agent reinforcement learning (MARL) for zero-shot cooperation with humans by modeling human biases. HSP consists of a two-stage methodology: first, it constructs a diverse pool of biased policies, and subsequently, it develops an adaptive policy capable of interacting effectively with humans without relying on extensive human data. Empirical evaluations in the Overcooked game demonstrate that HSP significantly outperforms baseline methods when cooperating with learned human models, scripted policies, and real human participants, showcasing its potential for practical applications in human-AI collaboration.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing the limitations of traditional human-AI cooperation frameworks, particularly the reliance on extensive human data. By modeling human biases, HSP enhances the adaptability of AI agents, making it a significant contribution to the field of MARL. However, a notable weakness is the variability of the results presented in the ablation studies, particularly regarding the necessity of the MEP policies. The revised findings suggest that the impact of MEP may be less critical than initially claimed, raising questions about the robustness of the reported results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with a logical flow from the introduction through to the methodology and results. The novelty of HSP is evident in its unique approach to policy construction and adaptation. However, the reproducibility of the findings may be undermined by the modifications made to experimental results, which could affect the perceived consistency of the claims. While the methodology is described adequately, further details on the implementation and evaluation process would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in zero-shot cooperation in MARL through the HSP framework, demonstrating strong empirical results. However, the variability in results and claims regarding the importance of certain policies raises concerns about the robustness of the findings. The paper contributes significantly to the field, yet it requires further clarification and validation of its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for zero-shot cooperation with human partners, assuming that human behavior is influenced by inherent biases. The methodology involves the introduction of a hidden utility function to model these biases, while leveraging self-play to generate a diverse policy pool for interaction. The findings indicate that the proposed approach can facilitate cooperation between AI agents and humans, although the authors acknowledge several limitations related to the assumptions made around human behavior and the evaluation metrics employed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to modeling human biases through a hidden utility function, which adds depth to the understanding of human-AI interaction. However, the paper is weakened by several fundamental assumptions, such as the alignment of human and environment reward functions, the oversimplification of human motivations, and the assumption of uniformity among human players. Additionally, the reliance on game rewards and human feedback for evaluation may not accurately reflect real-world dynamics and user experiences.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly; however, the clarity of the underlying assumptions could be improved to enhance reader understanding. The novelty of the proposed approach is significant, as it attempts to address the complex dynamics of human biases in AI cooperation. Nonetheless, the reproducibility of the results may be limited due to the controlled experimental setup and the assumptions made about human behavior and feedback mechanisms.\n\n# Summary Of The Review\nWhile the paper introduces a promising approach to understanding zero-shot cooperation with humans, it is hampered by several critical assumptions that may limit its real-world applicability. The framework's reliance on simplified models of human behavior and the experimental context raises concerns regarding the robustness and generalizability of the findings.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Learning Zero-Shot Cooperation with Humans, Assuming Humans are Biased\" by Chao Yu et al. introduces a novel framework called Hidden-Utility Self-Play (HSP) aimed at enhancing multi-agent reinforcement learning (MARL) for effective cooperation with humans in the absence of prior human data. The methodology involves a two-stage process where a diverse pool of policies is generated, incorporating human biases as hidden reward functions to improve adaptability and generalization. Experimental results demonstrate that HSP significantly outperforms baseline methods in cooperative tasks, such as those in the Overcooked benchmark, highlighting its efficacy in improving assistive capabilities.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to modeling human biases, which addresses the limitations of traditional self-play methods that often fail to account for the inherent variability in human behavior. By framing biases as hidden rewards, HSP offers a promising solution to the issue of convention overfitting in MARL. However, the paper does acknowledge certain weaknesses, including the reliance on domain knowledge for reward design and the computational demands associated with generating a diverse policy pool. Furthermore, while the empirical results are compelling, the applicability of the method to more complex environments remains to be fully established.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of writing is high, with sufficient detail provided to understand the proposed framework and its implications. The novelty of the approach is significant, particularly in its consideration of human biases within the context of MARL. However, reproducibility could be improved by providing more comprehensive details on implementation specifics and the experimental setups used.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of multi-agent reinforcement learning by introducing the Hidden-Utility Self-Play framework, which effectively models human biases to enhance cooperation with humans in zero-shot scenarios. While the methodology is robust and the results are promising, some limitations in terms of computational efficiency and reliance on prior knowledge are noted.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing classification performance in imbalanced datasets through a hybrid methodology that integrates oversampling techniques with deep learning architectures. The authors propose a framework that leverages generative adversarial networks (GANs) to create synthetic minority class samples, which are then used to fine-tune a convolutional neural network (CNN). The findings indicate that the proposed method significantly outperforms traditional models in terms of accuracy, F1-score, and area under the curve (AUC) metrics when tested on multiple benchmark datasets.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The focus on imbalanced datasets is highly pertinent, as these scenarios are common in real-world applications, making the research timely and impactful.\n2. **Innovation:** The integration of GANs with CNNs for oversampling adds a unique twist to existing methodologies, potentially setting a new standard in the field.\n3. **Comprehensive Evaluation:** The paper includes a thorough evaluation across diverse datasets, demonstrating the versatility of the proposed approach.\n4. **Clear Results:** The presentation of results is clear and well-illustrated, allowing for straightforward interpretation of the performance improvements.\n\n**Weaknesses:**\n1. **Limited Comparison:** While the results are promising, the comparisons to existing state-of-the-art methods could be more exhaustive. Including more baselines would strengthen the claims of superiority.\n2. **Reproducibility Issues:** Certain methodological details regarding the GAN training process and hyperparameter settings are not sufficiently detailed, which may hinder reproducibility.\n3. **Scalability Concerns:** The scalability of the proposed approach in terms of computational resources and time is not adequately addressed, leaving questions regarding its applicability to larger datasets or real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a logical structure that facilitates understanding of the proposed methodology and results. The novelty of combining GANs with CNNs for the specific problem of imbalanced datasets is commendable, contributing to the paper's overall quality. However, the reproducibility is somewhat compromised due to a lack of detailed descriptions of experimental setups and parameters.\n\n# Summary Of The Review\nThis paper offers an innovative solution to the challenge of imbalanced datasets by combining GANs with CNNs, demonstrating significant performance improvements. However, the paper would benefit from more comprehensive comparisons, clearer methodological details, and a discussion on scalability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel framework, Hidden-Utility Self-Play (HSP), aimed at enhancing the capacity of intelligent agents to cooperate with humans by addressing the biases present in human decision-making. The authors argue that traditional methods in multi-agent reinforcement learning (MARL) often fail to account for the divergence between human reward structures and those assumed by agents. By modeling human biases as hidden reward functions, HSP generates a diverse set of policies that better reflect human strategies. The evaluation of HSP on the Overcooked benchmark shows that agents trained with this framework achieve higher rewards and are perceived as more assistive by human players compared to baseline approaches.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to zero-shot cooperation, addressing a significant gap in existing AI frameworks by explicitly accounting for human biases. The introduction of hidden reward functions represents a meaningful advancement in the modeling of human-agent interactions. However, a notable weakness is the reliance on domain knowledge to define these hidden reward functions, which may limit the applicability of HSP across diverse scenarios. Additionally, the computational challenges associated with constructing a diverse policy pool could impact the scalability of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, with a logical flow from motivation to methodology and evaluation. The introduction effectively frames the problem and situates the research within the broader context of AI and human cooperation. The methodology is described in sufficient detail to allow for reproducibility, although the specifics of how hidden reward functions are derived may need further elaboration. The novelty of the approach is significant, as it challenges conventional assumptions in multi-agent learning and introduces a fresh perspective on human-agent interaction.\n\n# Summary Of The Review\nOverall, the paper makes a compelling contribution to the field of AI by introducing a novel framework that enhances cooperation between agents and biased human players. While it presents a strong methodological foundation and promising empirical results, the reliance on domain knowledge and computational challenges pose limitations that warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Zero-Shot Cooperation with Humans, Assuming Humans are Biased\" by Chao Yu et al. presents a novel approach to multi-agent reinforcement learning (MARL) that addresses the challenge of cooperative AI agents interacting with humans who may have biased reward functions. The proposed methodology, Hidden-Utility Self-Play (HSP), models human biases using hidden reward functions and generates a diverse policy pool through self-play. The authors evaluated HSP using the Overcooked benchmark and demonstrated that it consistently outperforms traditional approaches in terms of rewards and adaptability to human preferences, thus providing a significant advancement in the field of human-AI collaboration.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to modeling human biases, which is a critical factor in enhancing human-AI cooperation. The two-staged training approach for generating a policy pool is particularly noteworthy, as it allows for greater flexibility and adaptability in the learned policies. The empirical evaluations against various baselines provide robust evidence of HSP's effectiveness. However, a potential weakness is the reliance on the Overcooked benchmark; while it is a well-established testing ground, the generalizability of the findings to other tasks or environments remains unclear. Additionally, the computational overhead associated with policy pool generation may limit practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results. The logical flow from problem statement to solution and empirical validation is commendable. The authors provide sufficient details about their methods, allowing for reproducibility, although further clarification on the implementation specifics could enhance transparency. In terms of novelty, the introduction of hidden utility functions to capture human biases is a significant contribution, marking a departure from traditional MARL approaches that often overlook these biases.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of MARL by addressing the complexities of human biases in cooperative settings. The proposed HSP framework demonstrates both theoretical and empirical advancements, although further exploration of its applicability in diverse contexts is necessary. The clarity and quality of the writing facilitate understanding, while the novelty of the approach positions it as a noteworthy advancement in human-AI interaction research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Zero-Shot Cooperation with Humans, Assuming Humans are Biased\" presents a novel approach to facilitating cooperation between AI agents and humans in scenarios where human biases are prevalent. The authors introduce the Hidden-Utility Self-Play (HSP) framework, which models human biases using a hidden utility function. The methodology consists of a two-stage process that generates a diverse policy pool based on biased strategies. Experimental evaluations on the Overcooked benchmark demonstrate that the HSP framework significantly improves cooperation performance, with empirical results indicating higher average episode rewards and positive human feedback compared to baseline methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to incorporating human biases into the design of AI systems, which is a critical consideration for effective human-AI collaboration. The comprehensive evaluation across various scenarios and partner types further bolsters the paper's contributions, showcasing the practical relevance and robustness of the proposed framework. However, a notable weakness is the requirement for substantial domain knowledge for event design, which may limit the framework's applicability in diverse real-world settings. Additionally, the computational intensity involved in generating diverse policy pools could hinder scalability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem of zero-shot cooperation with humans, as well as the proposed solution. The methodology is presented in a logical manner, allowing readers to follow the development of the HSP framework easily. The empirical results are well-documented, although the computational demands may pose challenges for reproducibility. Overall, the paper demonstrates a high level of quality and clarity, with a significant novel contribution to the field of human-AI cooperation.\n\n# Summary Of The Review\nThis paper offers a meaningful advancement in the understanding of human-AI cooperation through the introduction of the HSP framework, effectively addressing the challenge of human biases. With strong empirical evidence supporting its claims, the work is both innovative and relevant, although it may face limitations in terms of domain knowledge requirements and computational efficiency.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to multi-agent reinforcement learning (MARL) aimed at facilitating zero-shot cooperation between AI agents and humans by addressing the biases inherent in human decision-making. It introduces the Hidden-Utility Self-Play (HSP) framework, which models human biases as hidden reward functions, thereby enriching the policy pool generated through self-play. Empirical evaluations in the Overcooked benchmark demonstrate that the HSP framework significantly enhances reward acquisition and agent assistiveness compared to traditional methods, highlighting its potential for more effective human-AI collaboration.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to incorporating human biases into the training of AI agents, which is a critical issue in cooperative scenarios. The introduction of the hidden utility framework allows for a more realistic modeling of human behavior, thereby improving the diversity and effectiveness of the policy pool. However, the paper could benefit from a more detailed discussion on the limitations of the HSP framework, such as potential computational costs or scalability issues with larger environments or more complex human behaviors. Additionally, while the empirical results are promising, further validation in diverse settings would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to readers familiar with MARL concepts. The quality of the writing is high, with appropriate use of technical language and clear explanations of complex ideas. The novelty of the HSP framework is significant, as it addresses a gap in the existing literature regarding human biases in cooperative environments. However, reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameters used in the Overcooked benchmark.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of multi-agent reinforcement learning by introducing a framework that explicitly accounts for human biases, improving agent performance in cooperative tasks. The findings are promising, though further exploration of scalability and reproducibility would bolster the overall impact of the research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces Hidden-Utility Self-Play (HSP) as a new method aimed at addressing human biases in AI cooperation. The authors propose that existing self-play methods are fundamentally flawed due to their inability to account for hidden utility functions that humans may possess. They present empirical evaluations demonstrating that HSP achieves higher rewards than baseline methods in certain benchmarks, particularly in the game Overcooked. However, the findings are limited by the narrow focus of the evaluation and raised questions about the overall significance and applicability of the results.\n\n# Strength And Weaknesses\nWhile HSP presents an intriguing concept regarding hidden utility functions, the paper fails to convincingly critique existing self-play methods, lacking depth and comprehensive evidence for its claims. The assumption of hidden utility functions is overly simplistic and not empirically supported, undermining the proposed modeling of biases as hidden rewards. The empirical evaluations appear biased due to the restricted selection of benchmarks, which may not generalize well to diverse environments. Furthermore, the marginal improvements reported raise doubts about the transformative nature of HSP. The comparison to learned human models is problematic due to the limited representation power of these models. Additionally, the ablation studies are insufficiently rigorous, and the authors' acknowledgment of HSP's reliance on domain knowledge and computational challenges for policy diversity raises concerns about its scalability and accessibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hindered by a lack of rigorous statistical analysis in the ablation studies, making it challenging to assess the significance of the findings. While the idea of HSP is novel, the execution lacks the depth needed to substantiate its claims. The discussions on potential extensions are vague and lack concrete proposals, diminishing the overall quality of the work. Furthermore, the paper does not adequately address its limitations, which affects its reproducibility and reliability.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to addressing human biases in AI cooperation through HSP. However, it suffers from a lack of rigorous evidence, limited empirical validation, and insufficient critical analysis of its own limitations, which ultimately questions the utility and significance of the proposed method.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents the Hidden-Utility Self-Play (HSP) framework, a novel approach in multi-agent reinforcement learning (MARL) that enables AI agents to cooperate effectively with humans by modeling human biases as hidden reward functions. The methodology involves training agents to adaptively understand and respond to diverse human preferences, leading to improved cooperation in various scenarios. Empirical findings indicate that HSP outperforms existing baselines, particularly in the Overcooked benchmark, and demonstrates robust performance with real human participants, highlighting its potential for real-world applications in sectors such as healthcare and customer service.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative framework, which sets a new standard for human-AI collaboration by focusing on the understanding of human biases. The comprehensive evaluation, including interactions with real users, adds significant credibility to the findings, showcasing the effectiveness and acceptability of HSP-trained agents. However, a potential weakness could be the limited exploration of the HSP framework's applicability across a broader range of tasks beyond gaming environments, which may affect its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the contributions of the HSP framework, making it accessible to readers. The quality of the empirical evaluation is high, with diverse testing conditions that reinforce the findings. The novelty of the approach is significant, as it breaks new ground in the domain of cooperative AI. Reproducibility is supported by the thorough description of methodologies and empirical setups, although more explicit details on the implementation could enhance confidence in replicating results.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of AI by introducing the HSP framework, which enhances cooperation between AI agents and humans by effectively modeling human biases. Its empirical results are promising, indicating significant practical applications, though further exploration of its versatility in various contexts would strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel framework termed **Hidden-Utility Self-Play (HSP)**, which challenges traditional multi-agent reinforcement learning (MARL) paradigms that assume aligned reward functions between agents and humans. The authors propose that human biases diverge from environmental rewards, necessitating a new theoretical approach to agent training. Key contributions include the introduction of a hidden utility Markov game to model human preferences and biases, insights into Nash equilibria under non-standard human strategies, and the suggestion of performing random searches over hidden reward spaces to enhance policy diversity. The paper emphasizes the importance of explicitly modeling human biases to improve human-agent collaboration in complex environments.\n\n# Strength And Weaknesses\nThe paper presents significant theoretical advancements, particularly in addressing the limitations of traditional MARL frameworks that overlook human bias. It effectively articulates the necessity of differentiating between environmental rewards and human preferences. However, the computational demands associated with the proposed methods are a notable limitation, as is the reliance on domain knowledge for designing event-based rewards. These weaknesses may hinder the practicality of the proposed framework in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its theoretical contributions clearly. The mathematical formulations, including the definition of the hidden utility Markov game and the Nash equilibrium representation, are articulated effectively. The novelty of the HSP framework is substantial, offering fresh insights into the interplay between human biases and AI training. However, the reproducibility of the proposed methods may be challenged by the computational complexity and the requirement for in-depth domain knowledge, which could limit accessibility for practitioners.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical framework that significantly advances the understanding of human-agent collaboration by modeling human biases through the Hidden-Utility Self-Play approach. While the contributions are noteworthy, practical implementation may be hindered by computational challenges and the need for extensive domain knowledge.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"Learning Zero-Shot Cooperation with Humans, Assuming Humans are Biased\" presents a novel approach to enhance AI cooperation with human partners, particularly in scenarios where human actions may be biased. The authors develop the Hidden-Utility Self-Play (HSP) algorithm, implemented using Multi-Agent Proximal Policy Optimization (MAPPO), to construct a diverse policy pool from which adaptive policies can be trained. The methodology involves a two-stage training framework: the first stage focuses on constructing a policy pool using random searches over hidden reward functions, while the second stage involves training adaptive policies based on this pool. Experiments conducted in the Overcooked game environment demonstrate the effectiveness of the proposed method, with human participants providing feedback on the perceived cooperation and effectiveness of the AI agents.\n\n# Strength And Weaknesses\nStrengths of the paper include a well-structured methodology that effectively addresses the challenges of AI-human cooperation, particularly in the presence of human biases. The use of diverse policy pools and adaptive training strategies contributes to the robustness of the approach. However, weaknesses include a lack of broader implications or discussions on the significance of AI-human interaction beyond the specific task framework. The paper could also benefit from a more thorough exploration of potential limitations and future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-organized, detailing the algorithm's implementation and experimental setup comprehensively. The quality of the writing is high, with appropriate technical terminology used throughout. In terms of novelty, while the approach offers interesting insights into adaptive policy training for biased human interactions, it builds on existing methodologies without introducing fundamentally new ideas. Reproducibility is supported by the availability of implementation details and code, which enhances the transparency of the research.\n\n# Summary Of The Review\nOverall, the paper contributes valuable insights into AI-human cooperation through innovative policy training methods, though it could benefit from a broader discussion of implications and limitations. The clarity and reproducibility of the work are commendable, but the novelty is somewhat constrained by the reliance on existing frameworks.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces HSP (Human-Supportive Policy), a method designed to enhance cooperation between agents in multi-agent reinforcement learning (MARL) environments by modeling human biases. The authors claim that HSP achieves superior performance in collaborative tasks, particularly in the game Overcooked. However, the paper lacks a thorough comparison with the latest state-of-the-art (SOTA) methods and does not provide robust statistical validation for its findings. Moreover, HSP's ability to generalize to unseen human strategies and effectively capture a range of human biases remains inadequately substantiated.\n\n# Strength And Weaknesses\nThe strengths of the paper include its attempt to address the crucial challenge of cooperation in MARL, as well as the introduction of HSP as a potential framework for modeling human biases. However, the paper's weaknesses are significant; it fails to convincingly demonstrate that HSP outperforms existing methods like Fictitious Co-Play (FCP) and Maximum Entropy Population-based Training (MEP) in a meaningful way. The empirical results presented appear to be marginal and potentially influenced by the experimental setup, raising concerns about their reliability. The lack of comprehensive comparisons with relevant methods further diminishes the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but the clarity of its claims is undermined by the insufficient empirical validation of its results. The novelty of HSP is questionable, as similar approaches have been proposed in the literature without explicit claims of superiority. Reproducibility is also a concern, as the authors acknowledge issues with their reimplementation of baseline methods, which could affect the integrity of their results.\n\n# Summary Of The Review\nOverall, while the paper presents HSP as an innovative method for enhancing cooperation in MARL, its contributions appear to be incremental rather than groundbreaking. The empirical evidence supporting HSP's effectiveness is not robust enough to distinguish it from existing methods, and the claims of novelty regarding human bias modeling are not adequately substantiated.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased\" presents a novel approach to multi-agent reinforcement learning (MARL) in environments where human collaborators exhibit biases. The primary contribution is a framework that enables agents to learn cooperative strategies without prior exposure to human behaviors, relying instead on a theoretical foundation that assumes inherent human biases. The methodology involves a series of experiments in simulated environments where agents adapt their strategies based on observed human actions. The findings indicate that agents can successfully cooperate in zero-shot scenarios, demonstrating improved performance compared to traditional approaches that do not account for human biases.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to addressing human biases in cooperative scenarios, which is a significant gap in the existing literature. The experimental design is robust, providing clear evidence of the proposed framework's effectiveness. However, a notable weakness is the limited diversity of the simulated environments, which may not fully capture the complexities of real-world interactions with humans. Additionally, the assumptions regarding human biases could benefit from further empirical validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow; however, some sections could benefit from additional clarity, particularly in the methodology, where the steps could be better delineated for ease of understanding. The quality of the figures and tables is satisfactory, although some captions could be more descriptive. The novelty of the approach is commendable, offering new insights into human-agent collaboration. Reproducibility is somewhat limited due to the lack of detailed descriptions of the environments used in the experiments, which could pose challenges for other researchers attempting to replicate the study.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of MARL by addressing human biases in a zero-shot learning context. While the methodology is sound and the findings are promising, the paper would benefit from improved clarity in its explanations and a broader range of experimental settings to enhance its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to modeling human biases in multi-agent reinforcement learning (MARL) through a method termed Human-Sensitive Policy (HSP). The authors demonstrate the effectiveness of HSP using the Overcooked benchmark, illustrating improvements in cooperation with biased human agents. However, the scope of the evaluation is limited, with the main focus on static biases and scripted policies, leaving questions regarding broader applicability and dynamic human behavior unaddressed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative attempt to incorporate human biases into MARL, which is a critical aspect of developing AI that interacts with humans. The methodology presents a clear framework for how biases can be modeled and leveraged to enhance cooperation in multi-agent settings. However, the evaluation is too narrow, relying solely on the Overcooked game, which may not fully capture the complexity of human-AI interactions in diverse real-world scenarios. The dependency on domain knowledge for reward design and the static assumptions about human biases further limit the framework’s applicability and robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to a wide audience. The quality of the writing is high, and the methodology is described in sufficient detail to allow for reproducibility. However, the novelty is somewhat constrained due to the limited exploration of diverse environments and the absence of consideration for dynamic human biases. Future work could enhance the reproducibility aspect by including automated methods for reward design and a wider range of evaluation scenarios.\n\n# Summary Of The Review\nOverall, the paper offers a promising approach to integrating human biases into MARL, with clear contributions and methodological rigor. Nevertheless, its impact is diminished by a narrow evaluation scope and several unexplored avenues for future research, particularly regarding dynamic biases and diverse human behavior modeling.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework called Hidden-Utility Self-Play (HSP), which enhances traditional self-play techniques in multi-agent reinforcement learning (MARL) by incorporating human biases. The methodology includes rigorous statistical evaluations through empirical results and ablation studies, assessing HSP against several baseline methods. The findings reveal that HSP shows significant improvements in average episode rewards while addressing the limitations of existing models in capturing the complexity of human behavior.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to integrating human biases into MARL through the HSP framework, which is well-supported by comprehensive statistical analyses and ablation studies. The use of clear evaluation metrics, including average rewards and standard deviations, enhances the readability and reliability of the results. However, a notable weakness is the potential limitation of the learned human models, as the authors acknowledge their inability to fully capture the breadth of human behavior, which could impact the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The quality of the analysis is high, with a strong emphasis on statistical rigor in reporting results. The novelty of the HSP framework is significant, as it offers a fresh perspective on human-AI interaction in MARL settings. The reproducibility is bolstered by detailed descriptions of methodologies and comprehensive performance metrics, although practical implementation may require additional resources.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of multi-agent reinforcement learning by introducing the HSP framework, which effectively models human biases. The rigorous statistical analysis and thorough evaluation of results reinforce the claims made, although limitations in capturing human behavior should be noted.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes the Hidden-Utility Self-Play (HSP) framework, which aims to model human biases through a set of predefined hidden reward functions. The methodology involves constructing a policy pool based on these hidden rewards, relying on domain knowledge to design suitable event-based features. The findings indicate that while HSP shows performance improvements over baseline methods, the enhancements are marginal, particularly in certain scenarios, suggesting limitations in robustness and adaptability.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to modeling human biases and the empirical results that demonstrate its effectiveness over existing methods. However, significant weaknesses include the reliance on potentially oversimplified linear approximations of human preferences, which may hinder agent performance. The requirement for extensive domain knowledge to construct the policy pool limits the framework's applicability across diverse environments. Additionally, challenges concerning computational intensity and scalability are notable, as generating a diverse policy pool can be resource-intensive. The lack of exploration into adaptive learning methods and the representation of policy diversity further weaken the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation of the HSP framework, but it could benefit from a more robust discussion of the limitations and assumptions underlying the approach. The quality of the empirical evaluation is acceptable; however, the findings raise questions about reproducibility due to the specific domain knowledge required for implementation. The novelty of the approach is noteworthy, although the paper does not sufficiently explore advanced techniques for learning reward structures or meta-adaptation strategies, which could enhance its contributions.\n\n# Summary Of The Review\nOverall, the HSP framework presents an interesting approach to modeling human biases in AI interactions, but it is constrained by significant assumptions and limitations regarding the representation of human behavior. Future work should focus on enhancing adaptability and generalizability across diverse environments to fully realize the potential of this framework.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a framework called \"Hidden-Utility Self-Play (HSP)\" aimed at modeling human biases in cooperative settings by utilizing hidden reward functions. The authors propose that this framework allows AI systems to better adapt to human preferences in multi-agent environments. They evaluate HSP in the context of the cooperative video game Overcooked, reporting that their approach yields higher rewards compared to baseline methods.\n\n# Strength And Weaknesses\nWhile the paper attempts to address the intersection of AI and human cooperation through the lens of human biases, the contributions lack originality and depth. The concept of hidden motives and biases in human behavior is well-established in behavioral economics and has been previously explored in the AI domain. The methodology employed, relying on standard probabilistic models and random weight search, does not present a significant advancement in the field. Furthermore, the choice of Overcooked as a testing environment raises questions about the robustness of their findings due to its simplicity. The empirical results, although claiming improvements, may not be meaningful without a strong baseline for comparison.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, though the novelty of the ideas put forth is questionable. The quality of the writing is adequate, but the claims made regarding innovation and contributions appear overstated. In terms of reproducibility, while ablation studies are conducted, the reliance on basic methodologies could hinder the ability of other researchers to build upon this work meaningfully.\n\n# Summary Of The Review\nOverall, the paper presents a framework that lacks significant novelty and does not contribute meaningfully to the ongoing discourse in AI-human cooperation. The results are based on a simplistic evaluation environment, and the claims of innovation in methodology do not hold up to scrutiny.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Hidden-Utility Self-Play (HSP), a novel framework designed to accommodate human biases in zero-shot cooperation scenarios. The methodology involves generating biased policies through random search and approximating reward spaces using linear functions. The findings indicate that HSP can improve cooperation between AI agents and humans by incorporating insights from human feedback, although the authors acknowledge limitations in representation learning and policy diversity.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing human biases in AI cooperation scenarios and its relevant evaluation on the Overcooked benchmark. However, the paper could benefit from exploring more advanced representation learning techniques, such as variational autoencoders or generative adversarial networks, to enhance the diversity of the policy pool. Additionally, the use of linear functions to approximate reward spaces may limit the ability to capture complex human preferences, suggesting a need for more nuanced non-linear approximations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, presenting its contributions in a logical manner. However, the novelty of the proposed method could be further substantiated by incorporating more sophisticated optimization techniques for generating biased policies, such as Bayesian optimization. While the paper articulates future research directions, it lacks comprehensive details on the implementation aspects necessary for reproducibility.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of human-AI cooperation by introducing HSP to address human biases. However, the methodology could be strengthened by employing advanced representation learning techniques and optimizing policy generation. The exploration of ethical implications and the necessity for diverse policies further highlight the importance of this research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Hidden-Utility Self-Play (HSP) method, which aims to enhance cooperative multi-agent systems by integrating human-like behavior modeling into training. HSP demonstrates significant performance improvements across various benchmarks, particularly in the Overcooked environment, where it consistently outperforms existing state-of-the-art methods such as FCP, MEP, and TrajDiv. The methodology includes a combination of policy filtering, the incorporation of Maximum Entropy Population-based policies, and a dual evaluation approach using game rewards and human feedback. Findings indicate that HSP not only achieves higher average episode rewards but also receives favorable ratings from human participants, reinforcing its effectiveness as a human-assistive AI.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its robust empirical results and the clear demonstration of HSP's superiority over baseline methods across multiple layouts. The ablation studies provide valuable insights into the mechanisms that contribute to its performance, highlighting the importance of policy pool construction. However, a notable weakness is the acknowledgment of limitations in capturing natural human behaviors, which may affect the generalizability of the findings to diverse contexts. This limitation could hinder HSP's applicability in real-world scenarios where human behavior may vary significantly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The quality of the writing is high, with logical flow and clear explanations of complex concepts. The novelty of the HSP method is significant, particularly in its approach to integrating human feedback and behavior modeling. Reproducibility appears to be supported by thorough descriptions of experimental setups and metrics used for evaluation, though more detailed sharing of code and data would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to improving cooperative multi-agent systems through the HSP method. Its strong empirical results and positive human feedback indicate a promising direction for future research in human-assistive AI. However, the limitations in modeling human behavior should be addressed to ensure broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to multi-agent reinforcement learning (MARL) by introducing a framework that leverages self-play and a hidden reward function (HSP). The authors propose a methodology that integrates these components to enhance the cooperation and competition dynamics among agents. Empirical results demonstrate that the proposed framework outperforms existing methods on benchmark tasks, highlighting improvements in learning efficiency and overall performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative combination of self-play and hidden reward functions, which addresses some of the key challenges in MARL, such as coordination among agents. The empirical validation on benchmark tasks underscores the robustness of the proposed approach. However, the paper suffers from several weaknesses, including dense jargon that may alienate non-expert readers and a lack of clarity in the methodology section. Additionally, the paper could benefit from a more concise abstract and a better-structured conclusion that clearly outlines the implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the technical depth of the paper is commendable, the clarity is hampered by the use of jargon and long paragraphs that cover multiple ideas. Improvements in the organization and transitions between sections would enhance readability. The novelty of the research is evident in its approach to MARL, yet the reproducibility could be affected by the lack of detailed explanations regarding the methodology. Including visual aids or flowcharts could improve understanding of complex algorithms.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in multi-agent reinforcement learning through its innovative framework. However, improvements in clarity, organization, and accessibility are necessary for the work to reach a broader audience and facilitate reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.7758485989953607,
    -1.7785925247653196,
    -1.8465674809524313,
    -1.8360326544154517,
    -1.9560310967225776,
    -1.9166717169494918,
    -1.8349224447645398,
    -1.9007176866071158,
    -1.8555232244712463,
    -1.7682596960343777,
    -1.748915765450948,
    -1.2527101050112586,
    -1.7159658815115797,
    -1.6236286413969998,
    -1.7566366368362243,
    -1.529644630910038,
    -2.048042028862047,
    -1.7622651515686556,
    -1.968293626579978,
    -1.8507621984440679,
    -1.7977895613605097,
    -1.4701410582907348,
    -1.7401710989666241,
    -1.825177099593544,
    -2.090588758884738,
    -2.068350885990917,
    -2.0074532422841185,
    -1.9692596053241034,
    -1.650815727457978
  ],
  "logp_cond": [
    [
      0.0,
      -2.5547175894180096,
      -2.5588968357813084,
      -2.552643272823418,
      -2.556833541181104,
      -2.54778548485592,
      -2.5559407712497344,
      -2.5978338123292035,
      -2.558776384915066,
      -2.5900859122268924,
      -2.5650538558581535,
      -2.617120531950661,
      -2.5941560698399475,
      -2.549653977107001,
      -2.576487120521183,
      -2.5808014520061353,
      -2.5945598476409986,
      -2.5785716281304225,
      -2.596966858825392,
      -2.5736390073870243,
      -2.5840123698282174,
      -2.6121918068519188,
      -2.597445090877214,
      -2.6044038096992814,
      -2.5715912718919487,
      -2.5864457414118895,
      -2.567216905587814,
      -2.5742016439609197,
      -2.6144434003511337
    ],
    [
      -1.4393135805182347,
      0.0,
      -1.2060993473452426,
      -1.2384173454080933,
      -1.2975962208442133,
      -1.2927908188032502,
      -1.271078017166874,
      -1.2405784714820884,
      -1.2714303381003902,
      -1.3104115507521108,
      -1.1873393273887787,
      -1.4745463009651474,
      -1.201699151532481,
      -1.2161309254095907,
      -1.2257460913669629,
      -1.2516273100319053,
      -1.2976535522998975,
      -1.3166244878794005,
      -1.3231349688012417,
      -1.2514826250113513,
      -1.3700965837651982,
      -1.361355688834248,
      -1.365992967941903,
      -1.3436837314634078,
      -1.2986113862977864,
      -1.3005319290541304,
      -1.2864994285296538,
      -1.3548653444507741,
      -1.4283179528756331
    ],
    [
      -1.5308709544414474,
      -1.3049726812500002,
      0.0,
      -1.2653556705704434,
      -1.345955099668925,
      -1.3642938120494772,
      -1.435228388557634,
      -1.4364860032208122,
      -1.3012712284562393,
      -1.4341332628716423,
      -1.274015580539516,
      -1.5875660344068778,
      -1.3274177859550347,
      -1.2811184821416064,
      -1.194590651186205,
      -1.4049526795017744,
      -1.457698770603367,
      -1.382399365813111,
      -1.4147976051921511,
      -1.2844743897945237,
      -1.4926665388522824,
      -1.4502958555424104,
      -1.495462085921967,
      -1.4781627440326601,
      -1.3455684101698286,
      -1.4310227952195613,
      -1.3994147611759236,
      -1.4245388128277412,
      -1.5587012186945517
    ],
    [
      -1.5646809470232252,
      -1.341314001463768,
      -1.2199438176182127,
      0.0,
      -1.374250862894454,
      -1.4022956396631368,
      -1.424831685904909,
      -1.4188488887521071,
      -1.3138295937544526,
      -1.4292071090432912,
      -1.2913095652108768,
      -1.5593161397300748,
      -1.3876756363631035,
      -1.2995772676010602,
      -1.3074078504686473,
      -1.3987346949934103,
      -1.4216801385610647,
      -1.41821779013112,
      -1.3823891735641844,
      -1.3267453662956499,
      -1.500417049554873,
      -1.4420695599179394,
      -1.4668559397674663,
      -1.443424397456223,
      -1.4421569018429994,
      -1.4213147914959328,
      -1.4088166222033478,
      -1.4266605757405948,
      -1.4924523027867627
    ],
    [
      -1.6563096761923837,
      -1.446957502469523,
      -1.4536787320276334,
      -1.4857908604962538,
      0.0,
      -1.4986259405975737,
      -1.4865991656669768,
      -1.5086840884146477,
      -1.434945416568298,
      -1.5221178874043857,
      -1.4203296783429513,
      -1.6737143901946023,
      -1.480545923406637,
      -1.4825974797826647,
      -1.4670576960065085,
      -1.5027549030700187,
      -1.4476691369667773,
      -1.4975639710132473,
      -1.5690090890320203,
      -1.527320395270491,
      -1.5969533696833875,
      -1.6648345306477386,
      -1.6253537358333632,
      -1.5156463098576671,
      -1.5009643291283157,
      -1.5086108202914306,
      -1.4879309174445214,
      -1.5517459273393601,
      -1.5683610198410736
    ],
    [
      -1.5512040719819198,
      -1.3462885348017988,
      -1.3349253365813514,
      -1.3693713361280586,
      -1.3488664816166045,
      0.0,
      -1.4367122140641602,
      -1.4033385954943,
      -1.397769358734657,
      -1.4299091047003314,
      -1.3844228370225524,
      -1.6066674746461633,
      -1.3743047375833612,
      -1.39347206338681,
      -1.3871064152188408,
      -1.4059038274303408,
      -1.4626483433867958,
      -1.4220864216698998,
      -1.417753403761026,
      -1.421811429717432,
      -1.4464524348827745,
      -1.4684801971075496,
      -1.478609516075207,
      -1.4491995553110801,
      -1.4727169193747884,
      -1.4495328688631306,
      -1.4252356757295026,
      -1.4515308904647886,
      -1.5372658204783884
    ],
    [
      -1.5342202516963626,
      -1.3186543282008776,
      -1.3803577071705413,
      -1.394273876104334,
      -1.2702712654097728,
      -1.3662150204789172,
      0.0,
      -1.3922153482812216,
      -1.347922807546794,
      -1.454305227561109,
      -1.3472973894517895,
      -1.5419055207178782,
      -1.313194417986712,
      -1.3540057535205439,
      -1.3559876552809997,
      -1.3459325338102663,
      -1.3640389571281832,
      -1.3846304987835225,
      -1.3977090694878156,
      -1.3645730441959605,
      -1.5006673082488815,
      -1.5497603795071446,
      -1.467848840752187,
      -1.4126696682991091,
      -1.3824599595564464,
      -1.357196945001343,
      -1.3461123510524164,
      -1.4123638425568763,
      -1.4517657575433482
    ],
    [
      -1.5248022210143908,
      -1.299580999745063,
      -1.334398980625794,
      -1.3564088226077418,
      -1.3774642489146318,
      -1.3600692402989674,
      -1.4234378318506085,
      0.0,
      -1.3094512514528303,
      -1.4703704184176511,
      -1.3190269586575194,
      -1.6172894054412252,
      -1.3348402210949633,
      -1.3259393246676778,
      -1.3612177561483607,
      -1.3590284735522604,
      -1.3614462570674228,
      -1.3085038003814335,
      -1.3726122479783809,
      -1.374521758615632,
      -1.3880764448855054,
      -1.5140166766274041,
      -1.451305351417737,
      -1.3559792679020497,
      -1.3443495426520908,
      -1.332154554776081,
      -1.3064143325439737,
      -1.4298683480214693,
      -1.4881847584093595
    ],
    [
      -1.5475950480082337,
      -1.385531210127318,
      -1.3203873587745958,
      -1.3695755509555345,
      -1.3816986222784144,
      -1.459545782780514,
      -1.4423256760384948,
      -1.4409995561998599,
      0.0,
      -1.4774132360947982,
      -1.401208132806059,
      -1.6116341002247179,
      -1.4031740891229827,
      -1.4136724339562574,
      -1.3925114887569425,
      -1.4370692463318637,
      -1.4533822284905056,
      -1.4148828142968581,
      -1.4753960482396975,
      -1.3897675907875588,
      -1.481797692722,
      -1.5416060978116652,
      -1.4807951934885972,
      -1.4543900887628645,
      -1.4455048066975293,
      -1.4447748741618287,
      -1.402827922649285,
      -1.3970124356368712,
      -1.5439077573737607
    ],
    [
      -1.5192252468695544,
      -1.3413514011768948,
      -1.3429897941696098,
      -1.3687096740661568,
      -1.3629788954371187,
      -1.3902881357247785,
      -1.4026981335500859,
      -1.387155554183238,
      -1.3615015547055058,
      0.0,
      -1.3630339928445536,
      -1.529119060743486,
      -1.3686627524713586,
      -1.3293165172717794,
      -1.302204767598771,
      -1.341146992348146,
      -1.3713199559532776,
      -1.4419146793844964,
      -1.4190816409916658,
      -1.3700810885495063,
      -1.494328506158127,
      -1.4820707474842936,
      -1.5045644949932073,
      -1.4433438941320411,
      -1.3700571190401247,
      -1.4173528316065693,
      -1.3401633755352458,
      -1.436732399905338,
      -1.4779263500392767
    ],
    [
      -1.4458540534982662,
      -1.1814595062677309,
      -1.14777730416131,
      -1.2095722677672096,
      -1.207304885719731,
      -1.2984983698343446,
      -1.3199219082879963,
      -1.287889593760219,
      -1.2038392029719773,
      -1.3213688887161628,
      0.0,
      -1.4995893191984904,
      -1.2296557594635116,
      -1.1343397823250283,
      -1.1720566002841561,
      -1.1918793225725175,
      -1.2958545536031576,
      -1.3262711227380588,
      -1.3121689765418822,
      -1.2225674604852597,
      -1.369677115989759,
      -1.3619882337464269,
      -1.383520996212936,
      -1.3102692891036292,
      -1.285380776774438,
      -1.3065588299166166,
      -1.271206361880446,
      -1.325560012923829,
      -1.424252371540163
    ],
    [
      -1.0577916133208207,
      -1.018586372695271,
      -1.0553264233170887,
      -1.033347220688284,
      -1.0227036022911562,
      -1.0514398152767224,
      -1.0180854291252905,
      -1.0480734085160606,
      -1.0325325670869008,
      -1.0065839871418927,
      -1.0416950601951194,
      0.0,
      -1.0505005903647748,
      -1.0446215637411658,
      -1.0601508283506589,
      -1.0094873232160042,
      -1.0082492922318305,
      -1.0513596817038047,
      -1.0302335752291947,
      -1.0479973849662325,
      -1.0361918368210434,
      -1.0229942961745022,
      -1.026715378151832,
      -1.0387389262290156,
      -1.0297537331694442,
      -1.0194849915529967,
      -1.0406400527906554,
      -1.0405426260749415,
      -0.9938922454387773
    ],
    [
      -1.4359473585156814,
      -1.2208850463451169,
      -1.2204174066605467,
      -1.2331929979396177,
      -1.2686118789319532,
      -1.2544236393175436,
      -1.245404809529785,
      -1.2869732342627567,
      -1.2622764659033943,
      -1.3017922013493102,
      -1.176166917840124,
      -1.456831999154642,
      0.0,
      -1.2102902541953036,
      -1.181998251945948,
      -1.2283030497714746,
      -1.246838176463376,
      -1.3026997403252762,
      -1.2876755730976184,
      -1.2153479837777574,
      -1.3613556361853258,
      -1.3533454033936008,
      -1.3150283161295961,
      -1.3247188340897664,
      -1.2558089714746001,
      -1.2441229410876724,
      -1.2363341434859716,
      -1.3046386037518378,
      -1.3706470467773568
    ],
    [
      -1.3829012056204404,
      -1.1156599640012141,
      -1.108203465493301,
      -1.059804547536889,
      -1.1821827256803514,
      -1.2307374216364335,
      -1.2364535307512226,
      -1.2111838495705638,
      -1.1918949372755037,
      -1.2414457804882828,
      -1.0656482873609816,
      -1.3844578765992037,
      -1.1279197570764739,
      0.0,
      -1.0504319703461482,
      -1.172413868419296,
      -1.2562230569870818,
      -1.2141444369346228,
      -1.257543259995039,
      -1.096776401434191,
      -1.3090140345802366,
      -1.2765195396569935,
      -1.2959073908466359,
      -1.2505801552354814,
      -1.2342146659056468,
      -1.208162309628999,
      -1.2038432871514044,
      -1.2707241844224113,
      -1.3316603135208882
    ],
    [
      -1.4866133099531345,
      -1.2310630817647,
      -1.0911051850587719,
      -1.1903908590267633,
      -1.2699937393264684,
      -1.2725042129078041,
      -1.291389378173044,
      -1.3040474922625707,
      -1.2475137189548917,
      -1.2676445931838527,
      -1.0967847052501594,
      -1.4589057015963405,
      -1.1995722622707277,
      -1.1181052187323794,
      0.0,
      -1.2431823586185293,
      -1.3186418771794242,
      -1.308973462232489,
      -1.2923625582006348,
      -1.2108989439165907,
      -1.375190012117508,
      -1.3320337797597392,
      -1.373380491262812,
      -1.2845191013080888,
      -1.2846496305331596,
      -1.304258336241333,
      -1.2631230347331874,
      -1.3018876712870422,
      -1.4387546839812124
    ],
    [
      -1.2914695106462555,
      -0.9759387309820234,
      -1.0976476537636852,
      -1.057998919414874,
      -1.064510018430284,
      -1.0777673429574384,
      -1.0498697313490433,
      -1.102738381554692,
      -1.0717428813080483,
      -1.0866978206711044,
      -0.9895157078340729,
      -1.270506694909915,
      -0.9568760368776822,
      -1.011302993878139,
      -1.021459805451081,
      0.0,
      -1.0751784121092096,
      -1.1502140109312131,
      -1.1250067504018342,
      -1.0593153924304999,
      -1.253823258079694,
      -1.2201545103930223,
      -1.2374204052049866,
      -1.1687279443949028,
      -1.1245797064475485,
      -1.1031034638623383,
      -1.0455964712074097,
      -1.1829518241025234,
      -1.20483455305605
    ],
    [
      -1.794571599664132,
      -1.6495807463776055,
      -1.6540026001481365,
      -1.6981918197767332,
      -1.594943756663697,
      -1.7042769769010204,
      -1.6535031943963854,
      -1.6938379883012167,
      -1.6466228499710212,
      -1.7002101226975803,
      -1.6383499687133518,
      -1.7860972581004546,
      -1.6233129593196138,
      -1.670283310528937,
      -1.6770623116849566,
      -1.6403952517705758,
      0.0,
      -1.6991314617131623,
      -1.6977916758009968,
      -1.7204198362074867,
      -1.7205902566224762,
      -1.8135419354874855,
      -1.7737764954157718,
      -1.69998725700899,
      -1.6852559730222014,
      -1.6471310459956128,
      -1.6470448009562912,
      -1.6988420849114132,
      -1.7243469030239233
    ],
    [
      -1.3974152360015397,
      -1.1707196106340196,
      -1.1665128316118432,
      -1.1911794865181942,
      -1.1983758503864337,
      -1.2614234554676287,
      -1.2604622580705114,
      -1.187870814172171,
      -1.205151134776041,
      -1.3413783887656372,
      -1.214969328863758,
      -1.4420571087203866,
      -1.1752904897353595,
      -1.2176140224578826,
      -1.236022973756853,
      -1.22795441732052,
      -1.2777515759572613,
      0.0,
      -1.2988503367002917,
      -1.2325818168794909,
      -1.3205748890876787,
      -1.3941927927255642,
      -1.363785395937449,
      -1.279642161121603,
      -1.2459699014591568,
      -1.2416037979859735,
      -1.219599942682377,
      -1.2425605288357697,
      -1.3662007159983547
    ],
    [
      -1.6472572189397412,
      -1.5040454206589366,
      -1.5073070001410616,
      -1.4350273873407315,
      -1.5268252210133886,
      -1.4894905101182803,
      -1.5449823127133797,
      -1.5287988684287523,
      -1.5143703771606072,
      -1.560634402083611,
      -1.4792968181437167,
      -1.7191769882500523,
      -1.478362454561164,
      -1.5293102527724722,
      -1.4826616451723396,
      -1.5133702671884537,
      -1.5509379079203132,
      -1.5379709493984677,
      0.0,
      -1.5042321515026822,
      -1.5899755630637562,
      -1.604980009537817,
      -1.573830257249067,
      -1.533004763895576,
      -1.4682025298006962,
      -1.5225708315013788,
      -1.4933152279179942,
      -1.5728434476906161,
      -1.6542063862875218
    ],
    [
      -1.5703549721094665,
      -1.3973423782134924,
      -1.3368374533448042,
      -1.3806683584843964,
      -1.4687159395666802,
      -1.4676443941589137,
      -1.4752168802405266,
      -1.4519850486284933,
      -1.397757287792812,
      -1.4325420731223313,
      -1.3535588598119361,
      -1.6185614849948546,
      -1.4362391015342069,
      -1.3250560983452797,
      -1.3620315163503052,
      -1.4678927895538958,
      -1.493089632276027,
      -1.4777108514579924,
      -1.4626861776834228,
      0.0,
      -1.5184790820497214,
      -1.487899741359986,
      -1.5238394855737958,
      -1.503578475987492,
      -1.4259179981303505,
      -1.42675609553779,
      -1.4089650820508803,
      -1.4575121760038825,
      -1.5453944946122073
    ],
    [
      -1.4597236872049946,
      -1.400785456973704,
      -1.401278591288749,
      -1.42806846150801,
      -1.4187530805734554,
      -1.362978921351448,
      -1.4718678638448763,
      -1.3553519386491584,
      -1.3254609291452035,
      -1.4471824769545827,
      -1.3999143703179435,
      -1.5267234083390318,
      -1.4080509326222723,
      -1.4339979886105403,
      -1.4258291981365487,
      -1.4594479736512973,
      -1.397567337513693,
      -1.4112861003868182,
      -1.4273341396118115,
      -1.4417025857645125,
      0.0,
      -1.4652173356867393,
      -1.4087598794785707,
      -1.4101715319008274,
      -1.398987711974052,
      -1.383611010739489,
      -1.4028523299830113,
      -1.3746783200594153,
      -1.4916318168039497
    ],
    [
      -1.261149432383493,
      -1.1836692022993807,
      -1.1236528158963197,
      -1.105431714206248,
      -1.2023473344521713,
      -1.20287757159285,
      -1.2015391171604959,
      -1.2082323784128772,
      -1.1828125516617118,
      -1.1841439075627593,
      -1.0682244922011164,
      -1.1907509320880931,
      -1.1659441408159648,
      -1.0893478096058764,
      -1.1121109669437994,
      -1.197069909382503,
      -1.2078417689533325,
      -1.2212315739896984,
      -1.1526513351181271,
      -1.091567567205585,
      -1.1950492265572012,
      0.0,
      -1.1762647951409124,
      -1.1815246764303997,
      -1.1736306827776901,
      -1.1851905376794676,
      -1.1856085349827485,
      -1.1590814259560207,
      -1.1489403540823417
    ],
    [
      -1.4923949352112533,
      -1.4184449204237273,
      -1.4212463736926422,
      -1.361608028426661,
      -1.4262016186950905,
      -1.390275762422351,
      -1.3979630236154486,
      -1.3930455060181621,
      -1.3994553964514291,
      -1.3969955652139614,
      -1.3745573549140102,
      -1.499030867899777,
      -1.3926956986076744,
      -1.3745347248636992,
      -1.3394600056224395,
      -1.4227512831046523,
      -1.4014551419752048,
      -1.4420880128282791,
      -1.4010153282789748,
      -1.369826562414219,
      -1.3968422086383843,
      -1.4447424850987953,
      0.0,
      -1.4078702843973028,
      -1.3584722783582852,
      -1.3683839502504247,
      -1.359071338665642,
      -1.4480491365010022,
      -1.4315992643741666
    ],
    [
      -1.4707802331784432,
      -1.3470847336272354,
      -1.3342550814253853,
      -1.3426399443311643,
      -1.3122257218111777,
      -1.3467232296295335,
      -1.3252376121715503,
      -1.3140410646204281,
      -1.3043652096414686,
      -1.3449684176037853,
      -1.3441768925934336,
      -1.4945551210302563,
      -1.331857255498184,
      -1.335519213494173,
      -1.2938092548185878,
      -1.3809699268238709,
      -1.3262973815287897,
      -1.3049108258364537,
      -1.2995080122845524,
      -1.3614243253581058,
      -1.356017683930011,
      -1.4136217209970319,
      -1.4037811808652654,
      0.0,
      -1.2888011132537003,
      -1.2774311991238692,
      -1.331545518446221,
      -1.2951589885859662,
      -1.4116894629186783
    ],
    [
      -1.7481807312086965,
      -1.6024045578957904,
      -1.5688874035654805,
      -1.6630020580163911,
      -1.5992768824305585,
      -1.6401983893766021,
      -1.5797649392524051,
      -1.5857326269870178,
      -1.5971157968262784,
      -1.6255250180665608,
      -1.5982455740863013,
      -1.823840714551799,
      -1.5839589174302435,
      -1.6273164861744953,
      -1.556920084164089,
      -1.6536597654143526,
      -1.6003904034668675,
      -1.624294897045728,
      -1.6055818988490471,
      -1.6109877092754346,
      -1.6973969887612248,
      -1.7682304323690472,
      -1.7173610759243136,
      -1.6376226215278453,
      0.0,
      -1.6493168395746542,
      -1.5563812358760347,
      -1.6381210854105301,
      -1.729567875038141
    ],
    [
      -1.7098332109494356,
      -1.535865105226444,
      -1.562465716455808,
      -1.575831100814957,
      -1.5988308771183342,
      -1.571658607431001,
      -1.641144034079019,
      -1.5594847509917507,
      -1.5609717665610274,
      -1.6534072509919848,
      -1.5585403503450739,
      -1.7836798759195214,
      -1.5636918995387257,
      -1.5379244061463473,
      -1.564503872130043,
      -1.617503870336626,
      -1.5460623682196084,
      -1.5705160482561593,
      -1.5978743648327167,
      -1.5568906855735938,
      -1.6413770013762956,
      -1.7152511348459294,
      -1.653455243147463,
      -1.5868392016333148,
      -1.5673817572708446,
      0.0,
      -1.5419615230928885,
      -1.5939462736235912,
      -1.722488635665899
    ],
    [
      -1.6756257422627139,
      -1.4591790868024426,
      -1.5061679170658886,
      -1.4792419230744305,
      -1.4799066508885523,
      -1.496083127870758,
      -1.5081554559544312,
      -1.455501485986323,
      -1.4678556248011554,
      -1.500713279890893,
      -1.4840054156088065,
      -1.671087222842596,
      -1.4846999233714306,
      -1.4904205365474443,
      -1.4392335611768505,
      -1.5364024207750144,
      -1.5329347276684508,
      -1.5468309922941674,
      -1.5390086411793502,
      -1.4753730364454833,
      -1.5593268837789667,
      -1.6660441725164095,
      -1.5735529168503932,
      -1.5690592996304968,
      -1.481637389673254,
      -1.5111434208628014,
      0.0,
      -1.5264673913685611,
      -1.6530266435604581
    ],
    [
      -1.6448322760792573,
      -1.5082808009329103,
      -1.5420603833378075,
      -1.5701433398523166,
      -1.570685868275868,
      -1.5034934383278922,
      -1.5798091766466316,
      -1.5594738644909767,
      -1.4422569905384364,
      -1.5629001236539586,
      -1.561166642372455,
      -1.712191801448126,
      -1.5327469753062537,
      -1.5569276618349481,
      -1.522786463330417,
      -1.568974556095224,
      -1.562634043858647,
      -1.5466831495395172,
      -1.580861717693207,
      -1.5101964861470039,
      -1.5026127095962103,
      -1.648821989743719,
      -1.6208805491718308,
      -1.506405745007505,
      -1.5399534218706903,
      -1.551984733952446,
      -1.5168766777659513,
      0.0,
      -1.6684891160486468
    ],
    [
      -1.3958193495310534,
      -1.3043318852017818,
      -1.3376825443844544,
      -1.3412851471408942,
      -1.2499599311873315,
      -1.2838436541682943,
      -1.2605879365489479,
      -1.3035666847498821,
      -1.3061363944506,
      -1.2748621572967607,
      -1.3245081975025752,
      -1.3068614489238897,
      -1.3119876396726649,
      -1.3324192594503095,
      -1.339557851664595,
      -1.26128544142583,
      -1.2894238518175507,
      -1.3050817930628988,
      -1.2969091376020296,
      -1.3411341883953047,
      -1.3720360642524774,
      -1.3805701167725513,
      -1.347756792635976,
      -1.305842828540787,
      -1.278904667497067,
      -1.3094245789607146,
      -1.3227462999652182,
      -1.3393860340651627,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2211310095773511,
      0.2169517632140523,
      0.22320532617194289,
      0.21901505781425668,
      0.2280631141394407,
      0.21990782774562634,
      0.17801478666615722,
      0.21707221408029476,
      0.1857626867684683,
      0.21079474313720725,
      0.15872806704469955,
      0.18169252915541323,
      0.22619462188835993,
      0.19936147847417773,
      0.19504714698922543,
      0.18128875135436218,
      0.19727697086493823,
      0.17888174016996894,
      0.2022095916083364,
      0.19183622916714338,
      0.16365679214344198,
      0.17840350811814654,
      0.17144478929607931,
      0.20425732710341205,
      0.1894028575834712,
      0.20863169340754695,
      0.20164695503444108,
      0.1614051986442271
    ],
    [
      0.33927894424708493,
      0.0,
      0.572493177420077,
      0.5401751793572263,
      0.48099630392110626,
      0.4858017059620694,
      0.5075145075984455,
      0.5380140532832312,
      0.5071621866649294,
      0.4681809740132088,
      0.5912531973765409,
      0.3040462238001722,
      0.5768933732328385,
      0.5624615993557289,
      0.5528464333983567,
      0.5269652147334143,
      0.48093897246542205,
      0.46196803688591914,
      0.4554575559640779,
      0.5271098997539683,
      0.40849594100012143,
      0.41723683593107164,
      0.41259955682341665,
      0.4349087933019118,
      0.4799811384675332,
      0.4780605957111892,
      0.4920930962356658,
      0.4237271803145455,
      0.35027457188968647
    ],
    [
      0.3156965265109839,
      0.5415947997024311,
      0.0,
      0.5812118103819879,
      0.5006123812835064,
      0.4822736689029541,
      0.41133909239479727,
      0.41008147773161907,
      0.545296252496192,
      0.41243421808078895,
      0.5725519004129154,
      0.25900144654555346,
      0.5191496949973966,
      0.5654489988108249,
      0.6519768297662263,
      0.4416148014506569,
      0.38886871034906423,
      0.4641681151393202,
      0.43176987576028014,
      0.5620930911579076,
      0.3539009421001489,
      0.3962716254100209,
      0.35110539503046434,
      0.36840473691977116,
      0.5009990707826026,
      0.41554468573286996,
      0.4471527197765077,
      0.4220286681246901,
      0.2878662622578796
    ],
    [
      0.27135170739222647,
      0.4947186529516838,
      0.616088836797239,
      0.0,
      0.46178179152099763,
      0.43373701475231496,
      0.41120096851054266,
      0.4171837656633446,
      0.5222030606609991,
      0.40682554537216054,
      0.5447230892045749,
      0.27671651468537695,
      0.4483570180523482,
      0.5364553868143915,
      0.5286248039468044,
      0.4372979594220414,
      0.41435251585438704,
      0.4178148642843318,
      0.45364348085126727,
      0.5092872881198018,
      0.33561560486057873,
      0.3939630944975123,
      0.3691767146479854,
      0.39260825695922863,
      0.39387575257245233,
      0.41471786291951895,
      0.4272160322121039,
      0.4093720786748569,
      0.34358035162868905
    ],
    [
      0.2997214205301939,
      0.5090735942530547,
      0.5023523646949442,
      0.4702402362263238,
      0.0,
      0.4574051561250039,
      0.46943193105560077,
      0.44734700830792984,
      0.5210856801542796,
      0.4339132093181919,
      0.5357014183796263,
      0.28231670652797525,
      0.47548517331594065,
      0.4734336169399129,
      0.4889734007160691,
      0.4532761936525589,
      0.5083619597558002,
      0.45846712570933024,
      0.3870220076905573,
      0.42871070145208656,
      0.35907772703919005,
      0.291196566074839,
      0.3306773608892144,
      0.44038478686491045,
      0.4550667675942619,
      0.447420276431147,
      0.46810017927805614,
      0.40428516938321746,
      0.387670076881504
    ],
    [
      0.365467644967572,
      0.570383182147693,
      0.5817463803681404,
      0.5473003808214332,
      0.5678052353328873,
      0.0,
      0.4799595028853316,
      0.5133331214551917,
      0.5189023582148349,
      0.4867626122491604,
      0.5322488799269394,
      0.31000424230332846,
      0.5423669793661305,
      0.5231996535626817,
      0.529565301730651,
      0.510767889519151,
      0.45402337356269595,
      0.49458529527959194,
      0.4989183131884658,
      0.49486028723205977,
      0.47021928206671726,
      0.44819151984194217,
      0.4380622008742847,
      0.46747216163841165,
      0.44395479757470335,
      0.4671388480863612,
      0.4914360412199892,
      0.46514082648470323,
      0.37940589647110334
    ],
    [
      0.30070219306817725,
      0.5162681165636622,
      0.4545647375939985,
      0.44064856866020574,
      0.564651179354767,
      0.4687074242856226,
      0.0,
      0.4427070964833182,
      0.48699963721774586,
      0.3806172172034308,
      0.48762505531275036,
      0.2930169240466616,
      0.5217280267778277,
      0.48091669124399594,
      0.47893478948354007,
      0.48898991095427347,
      0.47088348763635657,
      0.4502919459810173,
      0.4372133752767242,
      0.4703494005685793,
      0.3342551365156583,
      0.2851620652573952,
      0.3670736040123528,
      0.4222527764654307,
      0.4524624852080934,
      0.4777254997631968,
      0.48881009371212336,
      0.4225586022076635,
      0.38315668722119156
    ],
    [
      0.37591546559272504,
      0.6011366868620529,
      0.5663187059813217,
      0.5443088639993741,
      0.523253437692484,
      0.5406484463081485,
      0.47727985475650736,
      0.0,
      0.5912664351542856,
      0.43034726818946467,
      0.5816907279495964,
      0.2834282811658906,
      0.5658774655121526,
      0.574778361939438,
      0.5394999304587551,
      0.5416892130548554,
      0.539271429539693,
      0.5922138862256823,
      0.5281054386287349,
      0.5261959279914838,
      0.5126412417216104,
      0.3867010099797117,
      0.44941233518937884,
      0.5447384187050661,
      0.556368143955025,
      0.5685631318310349,
      0.5943033540631422,
      0.4708493385856465,
      0.41253292819775633
    ],
    [
      0.3079281764630126,
      0.46999201434392823,
      0.5351358656966505,
      0.4859476735157118,
      0.47382460219283185,
      0.39597744169073223,
      0.41319754843275147,
      0.4145236682713864,
      0.0,
      0.37810998837644805,
      0.45431509166518724,
      0.24388912424652842,
      0.4523491353482636,
      0.4418507905149889,
      0.46301173571430376,
      0.41845397813938257,
      0.4021409959807407,
      0.44064041017438815,
      0.3801271762315488,
      0.46575563368368744,
      0.3737255317492463,
      0.31391712665958105,
      0.37472803098264906,
      0.4011331357083818,
      0.41001841777371695,
      0.4107483503094176,
      0.45269530182196127,
      0.4585107888343751,
      0.31161546709748555
    ],
    [
      0.2490344491648233,
      0.42690829485748294,
      0.4252699018647679,
      0.39955002196822087,
      0.405280800597259,
      0.3779715603095992,
      0.36556156248429184,
      0.3811041418511396,
      0.4067581413288719,
      0.0,
      0.4052257031898241,
      0.2391406352908918,
      0.3995969435630191,
      0.43894317876259836,
      0.4660549284356068,
      0.42711270368623167,
      0.3969397400811001,
      0.32634501664988136,
      0.34917805504271193,
      0.39817860748487144,
      0.27393118987625065,
      0.28618894855008414,
      0.2636952010411704,
      0.3249158019023366,
      0.398202576994253,
      0.3509068644278084,
      0.42809632049913193,
      0.33152729612903964,
      0.29033334599510097
    ],
    [
      0.3030617119526817,
      0.5674562591832171,
      0.601138461289638,
      0.5393434976837383,
      0.5416108797312169,
      0.4504173956166033,
      0.4289938571629517,
      0.4610261716907289,
      0.5450765624789706,
      0.4275468767347852,
      0.0,
      0.2493264462524576,
      0.5192600059874364,
      0.6145759831259197,
      0.5768591651667918,
      0.5570364428784305,
      0.45306121184779036,
      0.42264464271288915,
      0.4367467889090657,
      0.5263483049656883,
      0.379238649461189,
      0.3869275317045211,
      0.36539476923801195,
      0.4386464763473188,
      0.4635349886765099,
      0.4423569355343313,
      0.47770940357050207,
      0.423355752527119,
      0.324663393910785
    ],
    [
      0.19491849169043785,
      0.2341237323159875,
      0.19738368169416987,
      0.21936288432297446,
      0.23000650272010237,
      0.2012702897345362,
      0.23462467588596803,
      0.204636696495198,
      0.2201775379243578,
      0.2461261178693659,
      0.2110150448161392,
      0.0,
      0.20220951464648373,
      0.2080885412700928,
      0.1925592766605997,
      0.24322278179525436,
      0.2444608127794281,
      0.2013504233074539,
      0.2224765297820639,
      0.20471272004502605,
      0.21651826819021514,
      0.22971580883675635,
      0.22599472685942668,
      0.21397117878224292,
      0.22295637184181438,
      0.2332251134582619,
      0.21207005222060316,
      0.21216747893631704,
      0.25881785957248127
    ],
    [
      0.28001852299589824,
      0.49508083516646284,
      0.49554847485103304,
      0.482772883571962,
      0.44735400257962654,
      0.46154224219403606,
      0.47056107198179475,
      0.428992647248823,
      0.45368941560818543,
      0.4141736801622695,
      0.5397989636714557,
      0.2591338823569378,
      0.0,
      0.5056756273162761,
      0.5339676295656317,
      0.48766283174010505,
      0.46912770504820367,
      0.41326614118630345,
      0.4282903084139613,
      0.5006178977338223,
      0.35461024532625385,
      0.36262047811797893,
      0.40093756538198355,
      0.39124704742181327,
      0.46015691003697956,
      0.4718429404239073,
      0.4796317380256081,
      0.41132727775974187,
      0.34531883473422287
    ],
    [
      0.24072743577655942,
      0.5079686773957857,
      0.5154251759036987,
      0.5638240938601109,
      0.4414459157166484,
      0.3928912197605663,
      0.38717511064577725,
      0.412444791826436,
      0.43173370412149614,
      0.382182860908717,
      0.5579803540360182,
      0.2391707647977961,
      0.49570888432052596,
      0.0,
      0.5731966710508516,
      0.45121477297770385,
      0.367405584409918,
      0.409484204462377,
      0.3660853814019609,
      0.5268522399628088,
      0.31461460681676323,
      0.3471091017400063,
      0.32772125055036394,
      0.3730484861615184,
      0.38941397549135304,
      0.41546633176800074,
      0.4197853542455954,
      0.3529044569745885,
      0.2919683278761116
    ],
    [
      0.27002332688308983,
      0.5255735550715244,
      0.6655314517774524,
      0.566245777809461,
      0.48664289750975587,
      0.48413242392842015,
      0.4652472586631804,
      0.4525891445736536,
      0.5091229178813326,
      0.4889920436523716,
      0.6598519315860649,
      0.2977309352398838,
      0.5570643745654966,
      0.6385314181038448,
      0.0,
      0.513454278217695,
      0.4379947596568001,
      0.4476631746037354,
      0.46427407863558945,
      0.5457376929196336,
      0.38144662471871627,
      0.42460285707648504,
      0.38325614557341225,
      0.4721175355281355,
      0.47198700630306467,
      0.45237830059489137,
      0.49351360210303685,
      0.4547489655491821,
      0.31788195285501186
    ],
    [
      0.2381751202637825,
      0.5537058999280146,
      0.43199697714635277,
      0.471645711495164,
      0.46513461247975396,
      0.4518772879525996,
      0.4797748995609947,
      0.42690624935534593,
      0.4579017496019897,
      0.44294681023893356,
      0.540128923075965,
      0.2591379360001229,
      0.5727685940323558,
      0.518341637031899,
      0.5081848254589569,
      0.0,
      0.4544662188008284,
      0.3794306199788249,
      0.4046378805082038,
      0.4703292384795381,
      0.275821372830344,
      0.30949012051701574,
      0.29222422570505135,
      0.3609166865151352,
      0.40506492446248954,
      0.42654116704769973,
      0.48404815970262827,
      0.3466928068075146,
      0.32481007785398797
    ],
    [
      0.25347042919791507,
      0.39846128248444157,
      0.3940394287139106,
      0.34985020908531395,
      0.4530982721983501,
      0.3437650519610267,
      0.39453883446566174,
      0.35420404056083044,
      0.40141917889102596,
      0.3478319061644668,
      0.4096920601486953,
      0.2619447707615925,
      0.4247290695424333,
      0.37775871833311014,
      0.37097971717709055,
      0.40764677709147135,
      0.0,
      0.3489105671488848,
      0.35025035306105035,
      0.3276221926545604,
      0.3274517722395709,
      0.23450009337456157,
      0.27426553344627536,
      0.3480547718530571,
      0.3627860558398457,
      0.40091098286643434,
      0.40099722790575587,
      0.3491999439506339,
      0.32369512583812377
    ],
    [
      0.3648499155671159,
      0.591545540934636,
      0.5957523199568124,
      0.5710856650504614,
      0.5638893011822219,
      0.5008416961010269,
      0.5018028934981442,
      0.5743943373964846,
      0.5571140167926145,
      0.4208867628030184,
      0.5472958227048976,
      0.320208042848269,
      0.586974661833296,
      0.544651129110773,
      0.5262421778118025,
      0.5343107342481357,
      0.4845135756113943,
      0.0,
      0.4634148148683639,
      0.5296833346891647,
      0.44169026248097687,
      0.3680723588430914,
      0.3984797556312065,
      0.4826229904470527,
      0.5162952501094988,
      0.5206613535826821,
      0.5426652088862787,
      0.5197046227328859,
      0.3960644355703009
    ],
    [
      0.3210364076402368,
      0.4642482059210413,
      0.4609866264389164,
      0.5332662392392464,
      0.4414684055665894,
      0.4788031164616977,
      0.42331131386659826,
      0.43949475815122563,
      0.4539232494193708,
      0.40765922449636705,
      0.48899680843626125,
      0.2491166383299257,
      0.489931172018814,
      0.43898337380750574,
      0.4856319814076384,
      0.4549233593915243,
      0.4173557186596648,
      0.4303226771815103,
      0.0,
      0.4640614750772958,
      0.37831806351622177,
      0.3633136170421609,
      0.394463369330911,
      0.43528886268440203,
      0.5000910967792818,
      0.44572279507859913,
      0.47497839866198377,
      0.39545017888936185,
      0.31408724029245616
    ],
    [
      0.28040722633460136,
      0.4534198202305755,
      0.5139247450992637,
      0.47009383995967147,
      0.38204625887738763,
      0.3831178042851542,
      0.3755453182035413,
      0.39877714981557455,
      0.4530049106512559,
      0.4182201253217366,
      0.49720333863213173,
      0.23220071344921323,
      0.414523096909861,
      0.5257061000987882,
      0.4887306820937627,
      0.38286940889017207,
      0.3576725661680409,
      0.37305134698607545,
      0.38807602076064507,
      0.0,
      0.3322831163943465,
      0.36286245708408194,
      0.3269227128702721,
      0.3471837224565759,
      0.4248442003137174,
      0.4240061029062778,
      0.44179711639318753,
      0.39325002244018537,
      0.3053677038318605
    ],
    [
      0.33806587415551514,
      0.3970041043868058,
      0.3965109700717606,
      0.3697210998524998,
      0.3790364807870543,
      0.43481064000906167,
      0.3259216975156334,
      0.44243762271135134,
      0.4723286322153062,
      0.350607084405927,
      0.3978751910425662,
      0.27106615302147796,
      0.3897386287382374,
      0.3637915727499694,
      0.371960363223961,
      0.3383415877092124,
      0.4002222238468167,
      0.3865034609736915,
      0.37045542174869817,
      0.3560869755959972,
      0.0,
      0.3325722256737704,
      0.389029681881939,
      0.38761802945968227,
      0.3988018493864578,
      0.4141785506210207,
      0.39493723137749837,
      0.42311124130109445,
      0.30615774455656
    ],
    [
      0.20899162590724174,
      0.28647185599135416,
      0.3464882423944151,
      0.36470934408448685,
      0.2677937238385635,
      0.2672634866978849,
      0.26860194113023894,
      0.2619086798778576,
      0.287328506629023,
      0.2859971507279755,
      0.40191656608961845,
      0.2793901262026417,
      0.30419691747477007,
      0.3807932486848584,
      0.3580300913469354,
      0.2730711489082318,
      0.2622992893374023,
      0.24890948430103643,
      0.3174897231726077,
      0.3785734910851499,
      0.2750918317335336,
      0.0,
      0.2938762631498224,
      0.2886163818603351,
      0.2965103755130447,
      0.28495052061126724,
      0.2845325233079863,
      0.3110596323347141,
      0.3212007042083931
    ],
    [
      0.24777616375537082,
      0.32172617854289687,
      0.31892472527398197,
      0.3785630705399632,
      0.3139694802715336,
      0.3498953365442732,
      0.3422080753511756,
      0.347125592948462,
      0.340715702515195,
      0.34317553375266274,
      0.36561374405261393,
      0.24114023106684712,
      0.34747540035894975,
      0.36563637410292493,
      0.40071109334418464,
      0.3174198158619719,
      0.3387159569914193,
      0.298083086138345,
      0.33915577068764935,
      0.3703445365524052,
      0.3433288903282399,
      0.2954286138678288,
      0.0,
      0.33230081456932137,
      0.38169882060833893,
      0.37178714871619944,
      0.38109976030098225,
      0.29212196246562194,
      0.3085718345924575
    ],
    [
      0.3543968664151007,
      0.47809236596630855,
      0.4909220181681586,
      0.4825371552623796,
      0.5129513777823662,
      0.4784538699640104,
      0.49993948742199357,
      0.5111360349731158,
      0.5208118899520753,
      0.48020868198975863,
      0.48100020700011026,
      0.33062197856328757,
      0.4933198440953599,
      0.489657886099371,
      0.5313678447749561,
      0.44420717276967303,
      0.4988797180647542,
      0.5202662737570902,
      0.5256690873089915,
      0.46375277423543815,
      0.4691594156635328,
      0.41155537859651203,
      0.42139591872827853,
      0.0,
      0.5363759863398436,
      0.5477459004696748,
      0.49363158114732286,
      0.5300181110075777,
      0.41348763667486566
    ],
    [
      0.3424080276760415,
      0.48818420098894766,
      0.5217013553192575,
      0.42758670086834694,
      0.4913118764541795,
      0.45039036950813593,
      0.5108238196323329,
      0.5048561318977203,
      0.4934729620584597,
      0.46506374081817725,
      0.49234318479843675,
      0.2667480443329391,
      0.5066298414544945,
      0.46327227271024274,
      0.5336686747206492,
      0.4369289934703855,
      0.4901983554178706,
      0.46629386183901,
      0.48500686003569093,
      0.4796010496093035,
      0.39319177012351325,
      0.32235832651569085,
      0.3732276829604244,
      0.45296613735689273,
      0.0,
      0.4412719193100838,
      0.5342075230087033,
      0.4524676734742079,
      0.36102088384659714
    ],
    [
      0.3585176750414816,
      0.5324857807644732,
      0.5058851695351092,
      0.4925197851759602,
      0.46952000887258305,
      0.49669227855991616,
      0.4272068519118981,
      0.5088661349991666,
      0.5073791194298898,
      0.4149436349989324,
      0.5098105356458433,
      0.28467101007139584,
      0.5046589864521915,
      0.5304264798445699,
      0.5038470138608742,
      0.4508470156542912,
      0.5222885177713088,
      0.4978348377347579,
      0.4704765211582005,
      0.5114602004173234,
      0.4269738846146216,
      0.3530997511449878,
      0.41489564284345426,
      0.4815116843576024,
      0.5009691287200726,
      0.0,
      0.5263893628980287,
      0.474404612367326,
      0.34586225032501816
    ],
    [
      0.3318275000214046,
      0.5482741554816759,
      0.5012853252182299,
      0.528211319209688,
      0.5275465913955661,
      0.5113701144133604,
      0.49929778632968724,
      0.5519517562977956,
      0.5395976174829631,
      0.5067399623932254,
      0.523447826675312,
      0.33636601944152256,
      0.5227533189126878,
      0.5170327057366741,
      0.568219681107268,
      0.47105082150910405,
      0.47451851461566763,
      0.46062224998995105,
      0.46844460110476827,
      0.5320802058386351,
      0.4481263585051518,
      0.341409069767709,
      0.43390032543372525,
      0.43839394265362164,
      0.5258158526108645,
      0.4963098214213171,
      0.0,
      0.48098585091555734,
      0.35442659872366034
    ],
    [
      0.3244273292448461,
      0.4609788043911931,
      0.4271992219862959,
      0.3991162654717868,
      0.3985737370482354,
      0.4657661669962112,
      0.3894504286774718,
      0.4097857408331267,
      0.527002614785667,
      0.4063594816701448,
      0.4080929629516483,
      0.2570678038759773,
      0.4365126300178497,
      0.41233194348915525,
      0.4464731419936865,
      0.4002850492288794,
      0.40662556146545636,
      0.42257645578458614,
      0.38839788763089644,
      0.4590631191770995,
      0.4666468957278931,
      0.3204376155803843,
      0.3483790561522726,
      0.46285386031659836,
      0.4293061834534131,
      0.4172748713716574,
      0.4523829275581521,
      0.0,
      0.3007704892754566
    ],
    [
      0.25499637792692464,
      0.3464838422561962,
      0.31313318307352356,
      0.3095305803170838,
      0.4008557962706465,
      0.36697207328968373,
      0.39022779090903015,
      0.34724904270809587,
      0.3446793330073781,
      0.37595357016121733,
      0.3263075299554028,
      0.34395427853408833,
      0.33882808778531315,
      0.3183964680076685,
      0.31125787579338304,
      0.3895302860321479,
      0.3613918756404273,
      0.34573393439507916,
      0.35390658985594836,
      0.30968153906267326,
      0.2787796632055006,
      0.27024561068542674,
      0.3030589348220021,
      0.34497289891719096,
      0.371911059960911,
      0.3413911484972634,
      0.3280694274927598,
      0.31142969339281534,
      0.0
    ]
  ],
  "row_avgs": [
    0.19683159919150675,
    0.47774768746817714,
    0.4500163499289416,
    0.43151750049392007,
    0.4352213505443472,
    0.485472221727577,
    0.43818831171699146,
    0.5149762760439648,
    0.412295114343546,
    0.36542685471529884,
    0.4615485202264746,
    0.2192201112304914,
    0.4373202785936171,
    0.41053391910571635,
    0.47601201541360433,
    0.4197535976011247,
    0.35685979881986035,
    0.49877546361759306,
    0.43018694192095747,
    0.39811098669492706,
    0.37853186925069876,
    0.30021653130719245,
    0.3355254897893506,
    0.4789843736854253,
    0.45168579429309774,
    0.46515870982754565,
    0.48000021047167124,
    0.4087192230770014,
    0.33567601756984944
  ],
  "col_avgs": [
    0.297614019870823,
    0.4643746945950314,
    0.4701678316979704,
    0.4554490779986687,
    0.4432670325354456,
    0.4228877998733603,
    0.413237353881356,
    0.4207532801455111,
    0.45832948526496387,
    0.4003060346014777,
    0.47408931435265517,
    0.2724744264572662,
    0.4568135488738531,
    0.4656190849805916,
    0.4707406263815542,
    0.4284013674991493,
    0.4170810036683147,
    0.40631139677414296,
    0.4051275584945961,
    0.447905693469798,
    0.3613210899276161,
    0.33638595001837757,
    0.35365562386313915,
    0.39359268448035806,
    0.42691808980262697,
    0.4202946741812709,
    0.44003505110831576,
    0.39800168527141516,
    0.3293576386008205
  ],
  "combined_avgs": [
    0.24722280953116488,
    0.47106119103160426,
    0.460092090813456,
    0.4434832892462944,
    0.43924419153989636,
    0.45418001080046866,
    0.4257128327991737,
    0.4678647780947379,
    0.4353122998042549,
    0.38286644465838826,
    0.4678189172895649,
    0.2458472688438788,
    0.4470669137337351,
    0.43807650204315396,
    0.47337632089757925,
    0.424077482550137,
    0.38697040124408755,
    0.452543430195868,
    0.4176572502077768,
    0.42300834008236254,
    0.3699264795891574,
    0.318301240662785,
    0.34459055682624484,
    0.4362885290828917,
    0.4393019420478623,
    0.4427266920044083,
    0.46001763078999347,
    0.4033604541742083,
    0.332516828085335
  ],
  "gppm": [
    651.1089218977297,
    600.664310239073,
    596.771896197446,
    602.1608896947129,
    607.8553194203593,
    618.5058423408351,
    622.8296408918319,
    621.9760680254506,
    601.8068025739457,
    631.6323345954553,
    596.3382979047218,
    688.7738103234805,
    605.2126956201455,
    600.8351180692176,
    598.6080327213592,
    620.2714125251832,
    616.1711461066905,
    627.7831415581617,
    625.30832312425,
    606.5537394463173,
    645.9810850487518,
    661.1271387970066,
    651.3168325337907,
    633.9573936393982,
    614.6182431654076,
    619.2154011604,
    612.5246498277082,
    626.5187059369601,
    664.2266043900366
  ],
  "gppm_normalized": [
    1.5907281473586399,
    1.4163749175627962,
    1.3963317330964173,
    1.4137543990351955,
    1.4238544577044947,
    1.4541245053987335,
    1.469681646343119,
    1.460855030313587,
    1.4079161540614087,
    1.4818198200350157,
    1.3935585692049468,
    1.6335522546467867,
    1.421975687843062,
    1.4090899419996465,
    1.4040811574736467,
    1.456646460844347,
    1.4491502003932701,
    1.4650018968661056,
    1.4577091965913016,
    1.428248887112059,
    1.5158797139674234,
    1.5514229292919084,
    1.531204980588333,
    1.490771587917844,
    1.44316604161013,
    1.4527229411697016,
    1.4374800423815859,
    1.477377306029999,
    1.5531030369180996
  ],
  "token_counts": [
    844,
    485,
    390,
    428,
    396,
    426,
    466,
    423,
    385,
    389,
    374,
    530,
    438,
    406,
    419,
    417,
    428,
    341,
    345,
    457,
    385,
    373,
    405,
    429,
    420,
    401,
    409,
    456,
    333,
    354,
    486,
    423,
    408,
    496,
    475,
    436,
    410,
    490,
    437,
    391,
    468,
    467,
    443,
    440,
    469,
    405,
    446,
    490,
    431,
    391,
    548,
    393,
    414,
    417,
    434,
    401,
    501,
    406,
    1114,
    512,
    435,
    467,
    410,
    462,
    427,
    423,
    415,
    432,
    412,
    496,
    369,
    381,
    392,
    428,
    443,
    380,
    408,
    476,
    446,
    405,
    409,
    442,
    440,
    407,
    402,
    408,
    383,
    511,
    450,
    448,
    474,
    435,
    417,
    434,
    376,
    457,
    390,
    456,
    537,
    447,
    489,
    442,
    447,
    468,
    423,
    421,
    448,
    437,
    436,
    410,
    389,
    414,
    384,
    366,
    451,
    383
  ],
  "response_lengths": [
    2631,
    2546,
    2545,
    2679,
    2467,
    2325,
    2502,
    2148,
    2521,
    2205,
    2537,
    2854,
    2573,
    2756,
    2529,
    2515,
    2621,
    2375,
    2480,
    2565,
    2333,
    2518,
    2234,
    2203,
    2443,
    2097,
    2139,
    2520,
    2100
  ]
}