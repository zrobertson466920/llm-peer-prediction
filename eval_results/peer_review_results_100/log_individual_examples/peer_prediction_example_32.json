{
  "example_idx": 32,
  "reference": "Published as a conference paper at ICLR 2023\n\nDISPARATE IMPACT IN DIFFERENTIAL PRIVACY FROM GRADIENT MISALIGNMENT\n\nMaria S. Esipova, Atiyeh Ashari Ghomi, Yaqiao Luo & Jesse C. Cresswell Layer 6 AI {maria, atiyeh, emily, jesse}@layer6.ai\n\nABSTRACT\n\nAs machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.\n\n1\n\nINTRODUCTION\n\nThe increasingly widespread use of machine learning throughout society has brought into focus social, ethical, and legal considerations surrounding its use. In highly regulated industries, such as healthcare and banking, regional laws and regulations require data collection and analysis to respect the privacy of individuals.1 Other regulations focus on the fairness of how models are developed and used.2 As machine learning is progressively adopted in highly regulated industries, the privacy and fairness aspects of models must be considered at all stages of the modelling lifecycle.\n\nThere are many privacy enhancing technologies including differential privacy (Dwork et al., 2006), federated learning (McMahan et al., 2017), secure multiparty computation (Yao, 1986), and homomorphic encryption (Gentry, 2009) that are used separately or jointly to protect the privacy of individuals whose data is used for machine learning (Choquette-Choo et al., 2020; Adnan et al., 2022; Kalra et al., 2021). The latter three technologies find usage in sharing schemes and can allow data to be analysed while preventing its exposure to the wrong parties. However, the procedures usually return a trained model which itself can leak private information (Carlini et al., 2019). On the other hand, differential privacy (DP) focuses on quantifying the privacy cost of disclosing aggregated information about a dataset, and can guarantee that nothing is learned about individuals that could not be inferred from population-level correlations (Jagielski et al., 2019). Hence, DP is often used when the results of data analysis will be made publicly available, for instance when exposing the outputs of a model, or the results of the most recent US census (Abowd, 2018).\n\nNot only must privacy be protected for applications in regulated industries, models must be fair. While there is no single definition that captures what it means to be fair, with regards to modelbased decision making fairness may preclude disparate treatment or disparate impact (Mehrabi et al., 2021). Disparate treatment is usually concerned with how models are applied across populations, whereas disparate impact can arise from biases in datasets that are amplified by the greedy nature of loss minimization algorithms (Buolamwini & Gebru, 2018). Differences in model performance across protected groups can result in a significant negative monetary, health, or societal impact for individuals who are discriminated against (Chouldechova & Roth, 2020).\n\n1Examples of laws governing data privacy include the General Data Protection Regulation in Europe, Health Insurance Portability and Accountability Act in the USA, and Personal Information Protection and Electronic Documents Act in Canada.\n\n2In the USA, fair lending laws including the Fair Housing Act, and Equal Credit Opportunity Act prohibit\n\ndiscrimination based on protected characteristics such as race, age, and sex.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nUnfortunately, it has been observed that disparate impact can be exacerbated by applying DP in machine learning (Bagdasaryan et al., 2019). Applications of DP always come with a privacy-utility tradeoff, where stronger guarantees of privacy negatively impact the usefulness of results - model performance in this context (Dwork & Roth, 2014). Underrepresented groups within the population can experience disparity in the cost of adding privacy, hence, fairness concerns are a major obstacle to deploying models trained with DP.\n\nThe causes of unfairness in DP depend on the techniques used, but are not fully understood. For the most widely used technique, differentially private stochastic gradient descent (DPSGD), two sources of error are introduced that impact model utility. Per-sample gradients are clipped to a fixed upper bound on their norm, then noise is added to the averaged gradient. Disparate impact from DPSGD was initially hypothesized to be rooted in unbalanced datasets (Bagdasaryan et al., 2019), though counterexamples were found by Xu et al. (2021). Recent research claims disparate impact to be caused by incommensurate clipping errors across groups, in turn effected by a large difference in average group gradient norms (Xu et al., 2021; Tran et al., 2021a).\n\nIn this work we highlight the disparate impact of gradient misalignment. In particular, we claim that the most significant cause of disparate impact is the difference in the direction of the unclipped and clipped gradients, which in turn can be caused by aggressive clipping and imbalances of gradient norms between groups. Our analysis of direction errors leads to a variant of DPSGD with properly aligned gradients. We explore this alternate method in relation to disparate impact and show that it not only significantly reduces the cost of privacy across all protected groups, it also reduces the difference in cost of privacy for all groups. Hence, it removes disparate impact and is more effective than previous proposals in doing so. On top of this, it is the only approach which does not require access to protected group labels, and thereby avoids disparate treatment of groups. In summary we:\n\n• Conduct a more fine-grained analysis of disparate impact in DPSGD, and demonstrate\n\ngradient misalignment to be the most significant cause;\n\n• Identify an existing algorithm, previously undiscussed in the fairness context, which prop-\n\nerly aligns gradients, and show it reduces disparate impact and disparate treatment;\n\n• Improve the utility of said algorithm via two alterations; • Experimentally verify that aligning gradients is more successful at mitigating disparate\n\nimpact than previous approaches.\n\n2 RELATED WORK\n\nPrivacy and Fairness: While privacy and fairness have been extensively studied separately, recently their interactions have come into focus. Ekstrand et al. (2018) considered the intersection of privacy and fairness for several definitions of privacy. This research gained new urgency when Bagdasaryan et al. (2019) observed that DPSGD exacerbated existing disparity in model accuracy on underrepresented groups. Disparate impact due to DP was further observed in Pujol et al. (2020) and Farrand et al. (2020) for varying levels of group imbalance. Using an adversarial definition of privacy, Jaiswal & Mower Provost (2020) found that overrepresented groups can incur higher privacy costs. Similar examples were shown in Xu et al. (2021) for DPSGD, and disparate impact was linked to groups having larger gradient norms.\n\nOther fairness-aware learning research has evaluated the fairness of a private model’s outcomes on protected groups. In this context fairness might refer to a statistical condition of non-discrimination with respect to groups (Mozannar et al., 2020; Tran et al., 2021b), for example, equalized odds (Jagielski et al., 2019), equality of opportunity (Cummings et al., 2019), or demographic parity (Xu et al., 2019; Farrand et al., 2020). Chang & Shokri (2021) empirically found that imposing fairness constraints on private models could lead to higher privacy loss for certain groups. We consider crossmodel fairness where the cost of adding privacy to a non-private model must be fairly distributed between groups.\n\nAdaptive Clipping: Many variations on the clipping procedure in DPSGD have been proposed to improve properties other than fairness. Adaptive clipping comes in many forms, but usually tunes the clipping threshold during training to provide better privacy-utility tradeoffs and convergence (Andrew et al., 2021; Pichapati et al., 2019). The convergence of DPSGD connects to the symmetry properties of the distribution of gradients (Chen et al., 2020) which are affected by clipping.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n3 BACKGROUND\n\n3.1 SETTING AND DEFINITIONS\n\nWe begin by laying out the problem setting and review the relevant definitions for discussing fairness in privacy. For concreteness we consider a binary classification problem on a dataset D which consists of n points of the form (xi, ai, yi), where xi ∈ Rd is a feature vector, yi ∈ {0, 1} is a binary label, and ai ∈ [K] refers to a protected group attribute which partitions the data. The group label ai can optionally be an attribute in xi, the label value yi, or some distinct auxiliary value. The goal is to train a model fθ : Rd → [0, 1] with parameter vector θ that is simultaneously useful and private, and in which the application of privacy is fair. Utility in the empirical risk minimization (ERM) problem is governed by the per-sample loss l : [0, 1] × {0, 1} → R, with the optimal model minimizing the objective L(θ; D) = 1 i∈D l(fθ(xi), yi), which happens for optimal parameters θ∗ = arg minθ L(θ; D). The requirement of privacy is applied to the model through its parameters; private parameters ̃θ must be obtained while exposing a minimal amount of private information in D. For this we apply the framework of differential privacy, recounted in the next section.\n\n(cid:80)\n\nn\n\nFairness of the privacy methodology can be measured in terms of the disparate impact that applying privacy has on the protected groups. As in Bagdasaryan et al. (2019), we use a version of accuracy parity, the difference in classification accuracy across protected groups after adding privacy. We denote a subset of the data containing all points belonging to group k as Dk = {(xi, ai, yi) ∈ D | ai = k}. A private model has accuracy parity for subset Dk if it minimizes the privacy cost\n\nπ(θ, Dk) = acc(θ∗; Dk) − E ̃θ[acc( ̃θ; Dk)], where the expectation is over the randomness involved in acquiring private model parameters. Of course, metrics other than classification accuracy could be used as required by the problem setting. Alternatively, fairness for privacy can be measured at the level of the loss function as in Tran et al. (2021a), which is more amenable to analyzing the causes of unfairness. The excessive risk over the course of training experienced by a group is\n\n(1)\n\nR(θ, Dk) = E ̃θ[L( ̃θ; Dk)] − L(θ∗; Dk). When the model is clear from context we denote R(θ; Dk) as Rk, and similarly for privacy cost πk. For both accuracy and loss we consider the gap between disparate impact values across groups. The privacy cost gap is πa,b = |πa − πb| for groups a, b ∈ [K], and the excessive risk gap refers to Ra,b = |Ra − Rb|. The goal of a fair private classifier is to minimize the privacy cost and/or excessive risk for all values of the protected group attribute, while maintaining small fairness gaps.\n\n(2)\n\n3.2 DIFFERENTIAL PRIVACY\n\nDifferential privacy (DP) (Dwork et al., 2006) is a widely used framework for quantifying the privacy consumed by a data analysis procedure. Formally, let D represent a set of data points, and M a probabilistic function, or mechanism, acting on datasets. We say that the mechanism is (ε, δ)-differentially private if for all subsets of possible outputs S ⊆ Range(M ), and for all pairs of databases D and D′ that differ by the addition or removal of one element, Pr[M (D) ∈ S] ≤ exp(ε) Pr[M (D′) ∈ S] + δ. (3)\n\nAlgorithm 1 DPSGD\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, noise multiplier σ, learning rates ηt\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, yi) in B do\n\ngi ← ∇θl(fθt(xi), yi) 1, C0 ̄gi ← gi · min ∥gi∥\n\n(cid:16)\n\n(cid:17)\n\n(cid:0)(cid:80)\n\n ̃gB ← 1 |B| θt+1 ← θt − ηt ̃gB\n\nFor the ERM problem, there are several ways to train a differentially private model (Chaudhuri et al., 2011). In this work we consider models that can be trained with stochastic gradient descent (SGD), such as neural networks, and focus on the most successful approach, DPSGD (Abadi et al., 2016), in which the Gaussian mechanism (Dwork & Roth, 2014) is applied to gradient updates as in Alg. 1. Since per-sample gradients gi generally do not have finite sensitivity, defined as ∆h = maxD,D′ ∥h(D) − h(D′)∥ for a function h, they are first clipped to have norm upper bounded by a fixed hyperparameter C0. Clipped gradients ̄gi in a batch B ⊂ D are aggregated into ̄gB and noise is added to produce ̃gB used in the parameter update.\n\ni∈B ̄gi + N (0, σ2C 2\n\n0\n\nI)(cid:1)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3.3 FAIRNESS CONCERNS FROM CLIPPING AND NOISE IN DPSGD\n\nThe two most significant steps in DPSGD, clipping and adding noise, can impact the learning process disproportionately across groups, but the exact conditions where disparate impact will occur have been debated (Bagdasaryan et al., 2019; Farrand et al., 2020; Xu et al., 2021; Tran et al., 2021a). The most concrete connection so far appears in (Tran et al., 2021a), where the expected loss L(θ; Da) is decomposed into terms contributing to the excessive risk at a single iteration for group a, Ra:\n\nProposition 1 (Tran et al. (2021a)). Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The expected loss E[L(θt+1; Da)] of group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as:\n\nE[L(θt+1; Da)] ≈ L(θt; Da) − ηt⟨gDa , gD⟩ + η2\n\nt 2\n\nE[gT\n\n+ ηt⟨gDa , gD − ̄gD⟩ + η2 + η2 l )C 2\n\n2 Tr(H a\n\n0 σ2.\n\nt 2\n\nt\n\nBH a l gB] l ̄gB] − E[gT\n\n(cid:0)E[ ̄gT\n\nBH a\n\nBH a\n\nl gB](cid:1)\n\n(non-private term)\n\n(Rclip a )\n\n(Rnoise a\n\n)\n\nThe expectation is taken over the randomness of the DP mechanisms, and batches of data.\n\nTerms in the first line appear for ordinary SGD, and do not contribute to the excessive risk Eq. (2). The terms in the second line, Rclip a , are caused by clipping since they cancel when ̄gB = gB for every batch. They involve gradients gDa and Hessians H a l , averaged over datapoints belonging to group a. The final term, Rnoise , depends on the scale of noise added in Alg. 1, as well as the trace of the Hessian, also called the Laplacian, averaged over Da. Based on Prop. 1, Tran et al. (2021a) argue that clipping causes excessive risk to groups with large gradient norms, which can result from large input norms ∥xi∥. Whether or not a group is underrepresented has less influence. In the next section we provide a new perspective on Rclip\n\nand the underlying causes of unfairness in DPSGD.\n\na\n\na\n\n4 DISPARATE IMPACT IS CAUSED BY GRADIENT MISALIGNMENT\n\nClipping in DPSGD introduces two types of error to the clipped batch gradient ̄gB. It will generally have different norm than ∥gB∥, and be misaligned compared to the SGD batch gradient, gB. At a high level, gradient misalignment poses a more serious problem to the convergence of DPSGD than magnitude error, as illustrated in Fig. 1. Changing only the norm means gradient descent will still step towards the (local) minimum of the loss function, and any norm error could be completely compensated for by adapting the learning rate ηt. In contrast, a misaligned gradient could result in a step towards significantly worse regions of the loss landscape causing catastrophic failures of convergence. Misaligned gradients add bias which compounds over training, as underrepresented or complex groups are systematically clipped. For comparison, adding noise to the aggregated gradient does not add bias, so noise errors tend to cancel out over training. We aim to quantify the relative impact of these effects and how they contribute to the excessive risk.\n\nFigure 1: Direction errors from clipping are more severe than magnitude errors over the course of training and can lead to suboptimal convergence.\n\n(cid:16) ∥ ̄gB ∥\n\n(cid:17)\n\nWe can distinguish the effects of clipping by rewriting the clipped batch gradient as\n\n∥gB ∥ gB\n\nfor an orthogonal matrix MB such that ̄gB and MBgB are colinear. As a ̄gB=MB proof of concept that gradient misalignment is the more severe error we compared models trained by taking steps ∥ ̄gB ∥ ∥gB ∥ gB vs. MBgB with no noise added. These represent magnitude errors and direction errors from clipping, respectively. The models were trained on MNIST with class 8 undersampled, and the results compare the typical class 2 to the underrepresented class 8; full details are provided in App. B. As seen in Table 1, direction error is more detrimental to performance than magnitude error. In particular, it disproportionately increases loss and decreases accuracy on the underrepresented class 8.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Effect of direction vs. magnitude error on MNIST with class 8 undersampled. The results compare accuracy and loss on the typical class 2 to the underrepresented class 8.\n\nTYPE OF ERROR ACC 2 ACC 8\n\nLOSS 2\n\nLOSS 8\n\nMAGNITUDE DIRECTION\n\n99.0 96.8\n\n93.5 84.1\n\n0.002 0.076\n\n0.005 0.518\n\nOur first theoretical result quantifies the excessive risk from the two types of errors, and follows from a Taylor expansion of the expected loss using ̄gB in the gradient descent update compared to gB. The excessive risk from magnitude error comes from comparing gB to ∥ ̄gB ∥ ∥gB ∥ gB, while that of\n\ngradient misalignment is isolated by comparing ̄gB=MB\n\n(cid:16) ∥ ̄gB ∥\n\n∥gB ∥ gB\n\n(cid:17)\n\nto ∥ ̄gB ∥\n\n∥gB ∥ gB (see Fig. 1).\n\nProposition 2. Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The excessive risk due to clipping experienced by group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as\n\nRclip\n\na ≈ ηt (cid:68)\n\n(cid:68)\n\ngDa , E (cid:104) ∥ ̄gB ∥\n\n(cid:104)(cid:16)\n\n1 − ∥ ̄gB ∥ ∥gB ∥\n\n(cid:17)\n\n(cid:105)(cid:69)\n\n+ η2\n\nt 2\n\nE\n\ngB (cid:105)(cid:69)\n\n(cid:104)(cid:16) ∥ ̄gB ∥2\n\n(cid:17)\n\n∥gB ∥2 − 1 (cid:0)(MBgB)T H a\n\nBH a gT\n\nl gB\n\n+ ηt\n\ngDa , E\n\n∥gB ∥ (gB−MBgB)\n\n(cid:104) ∥ ̄gB ∥2 ∥gB ∥2 where gDa , ̄gDa denote the average non-clipped and clipped gradients over group a at iteration t, H a l refers to the Hessian over group a, and MB is an orthogonal matrix such that ̄gB and MBgB are colinear. The expectations are taken over batches of data.\n\nl (MBgB)−gT\n\n, (Rdir a )\n\nBH a\n\n+ η2\n\nl gB\n\n(cid:1)(cid:105)\n\nE\n\nt 2\n\n(cid:105)\n\n(Rmag a )\n\nWe provide a derivation in App. A. Note that when the magnitude error is zero for all batches, ∥gB∥=∥ ̄gB∥, we have that Rmag a =0 as expected. As well, when there is no gradient misalignment then MB is the identity matrix for every batch, and so Rdir\n\na = 0.\n\na > Rdir\n\nTo determine the characteristics of groups that will have unfair outcomes from clipping in DPSGD we can distill a simpler condition for when Rdir b . Tran et al. (2021a) already provide such a condition for clipping overall, however it does not effectively account for the danger of gradient misalignment. Their condition is sufficient, but not necessary, and some of its looseness stems from the inequality xT y ≥ −∥x∥∥y∥ used to convert all terms in Rclip into expressions involving group gradient norms. This approach loses information about gradient direction. We instead propose a tighter analysis of Rdir Proposition 3. Assume the loss l is twice continuously differentiable and convex with respect to the model parameters. As well, assume that ηt ≤ (maxk∈[K] λk)−1 where λk is the maximum eigenvalue of the Hessian H k\n\nb using xT y = ∥x∥∥y∥ cos θ, where θ = ∠(x, y).\n\nl . For groups a, b ∈ [K], Rdir\n\na > Rdir\n\na − Rdir\n\nif\n\na\n\nb\n\nE (cid:2)∥ ̄gB∥(cos θa B = ∠(gDk , gB) and ̄θk\n\nB − cos ̄θa\n\nwhere θk\n\nB)(cid:3) > ∥gDb ∥\n\n∥gDa ∥\n\nE (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥ ̄gB ∥2] ∥gDa ∥ ,\n\n(4)\n\nB = ∠(gDk , ̄gB) for a group k ∈ [K]. Furthermore, the bound is tight.\n\nApp. A contains our proof. Prop. 3 shows that if the clipping operation disproportionately and sufficiently increases the direction error for group a relative to group b, then group a incurs larger excessive risk due to gradient misalignment.\n\nb\n\na − Rdir\n\nThe lower bound for Rdir inferred from Eq. 4 is tight, and in our experiments we empirically show that it is close to saturation in a typical case. Hence, when the direction errors for groups a and b are small (i.e. we expect that θi b ≈ 0 regardless of the size of ∥gDa ∥ relative to ∥gDb ∥. It follows that clipping does not negatively impact excessive risk if gradients remain aligned. On the other hand if direction error is not close to zero, large group gradient norms do exacerbate the error in direction, as the dominant term of Rdir a scales with ∥gDa ∥.\n\nB for i = a, b), we have that Rdir\n\na − Rdir\n\nB ≈ ̄θi\n\nThe excessive risk in Eq. 2 is evaluated at the end of training, whereas Props. 1 and 2 estimate it per-iteration. Fig. 1 demonstrates that the full impact of clipping errors may not be felt per-iteration, but only at convergence. Indeed what matters to the end user is how fair the final model is, not how fair any intermediate training step is. However, it is not possible to attribute overall excessive risk to the per-iteration terms Rdir i used in the expansions of Props. 1 and 2 differ at each iteration, and do not equal the overall optimal θ∗. Still, Table 1 demonstrates that gradient misalignment is the main cause of disparate impact, so we seek a method to prevent it.\n\n, since the optimal θ∗\n\na , and Rnoise\n\na , Rmag\n\na\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) Left: Per-sample gradients colored based on group membership. Top: Local clipping in DPSGD Bottom: Global scaling in DPSGD-Global.\n\n(b) In DPSGD-Global-Adapt scaling alone does not guarantee finite sensitivity, so gradients with norm above Z are clipped to C0 (DPSGD-Global clips large gradients to 0 rather than C0).\n\nFigure 2: Illustration of privatization steps in DPSGD, DPSGD-Global, and DPSGD-Global-Adapt\n\n5 PREVENTING GRADIENT MISALIGNMENT IN DPSGD\n\nOur results so far show that gradient misalignment due to clipping is the most significant cause of unfairness in DPSGD. Logically, Rdir a would be minimized if privatization left the direction of gB unchanged. A promising avenue is to scale down all per-sample gradients in a batch by the same amount. This is the approach taken by DPSGD-Global (Bu et al., 2021), which was recently proposed to improve the convergence of DPSGD, and has not been discussed in the context of fairness before. Our theoretical results suggest that global scaling will reduce disparate impact.\n\nAlgorithm 2 DPSGD-Global(-Adapt)\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, strict clipping bound Z ≥ C0, noise multipliers σ1, (σ2), learning rates ηt, (clipping learning rate ηZ, threshold τ ≥ 0)\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, yi) in B do\n\nγi ←\n\ngi ← ∇θl(fθt(xi), yi) (cid:40) C0 Z , 0 ( C0\n\n ̄gi ← γigi (cid:0)(cid:80)\n\n∥gi∥ ≤ Z ∥gi∥ ), ∥gi∥ > Z\n\nDPSGD-Global (Alg. 2) aims to preserve privacy by scaling gradients as ̄gi = γgi, 0 < γ < 1. Of course, scaling alone is insufficient to ensure persample gradients have bounded sensitivity. However, supposing that there were a strict upper bound Z ≥ ∥gi∥ ∀ i ∈ D, then scaling all gradients by γ = C0/Z would guarantee bounded sensitivity of C0 for each ̄gi (Fig. 2a). Given sufficient smoothness of the loss function, for any sample of data there will be such an upper bound maxi∈D ∥gi∥, but determining it exactly cannot be done in a differentially private manner. DPSGD-Global sets Z as a hyperparameter without looking at the data, in the same way C0 is chosen in DPSGD. If Z fails to be a strict upper bound, any gradients with ∥gi∥ > Z are discarded to guarantee a bound on sensitivity. When Z is chosen sufficiently large, no gradients are discarded and gradient misalignment is avoided. The drawback of a large Z is that the scaled gradients ̄gi will become small and convergence of gradient descent may be hindered.\n\n ̃gB ← 1 |B| θt+1 ← θt − ηt ̃gB (Adaptively set Z): bt ← |{i : ∥gi∥ > τ · Z}| ̃bt ← 1 2)) Z ← Z · exp(−ηZ + ̃bt)\n\n|B| (bt + N (0, σ2\n\ni∈B ̄gi + N (0, σ2\n\n1C 2\n\nI)(cid:1)\n\n0\n\nIn addition to identifying that DPSGD-Global has the potential to reduce disparate impact, we propose two modifications to improve its utility. First, we note that discarding gradients with ∥gi∥ > Z can exacerbate disparate impact as it is often underrepresented groups that have large gradient norms (Xu et al., 2021). Instead, we clip large gradients to have norm C0, which preserves more information while maintaining finite sensitivity (Fig. 2b). Second, rather than choosing Z as a hyperparameter, we adaptively update Z to upper-bound maxi∈B ∥gi∥. When Z is larger than all gradients it should be reduced to scale down gradients less, but if gradients are being clipped, Z should be increased. Z can be updated each iteration by privately estimating bt, the number of gradients in B that are larger than Z times a tolerance threshold τ ≥ 0. Since bt is a unit sensitivity quantity we can estimate it privately as ̃bt = 1 2)). Then, we use the geometric update rule Z ← Z · exp(−ηZ + ̃bt) with a learning rate ηZ (cf. (Andrew et al., 2021)). When all samples\n\n|B| (bt + N (0, σ2\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nhave gradient norm less than or equal to τ · Z, then in expectation ̃bt = 0 and Z is decreased by a factor of exp(−ηZ). Alternatively, Z is increased when ̃bt > ηZ, which occurs with probability 0.977 when bt |B| . As a result, with high probability the algorithm will not have more than |B|ηZ + 2σ2 gradients with norm exceeding τ · Z.\n\n|B| ≥ ηZ + 2σ2\n\nWe call the method with our two alterations DPSGD-Global-Adapt, shown in Alg. 2 in red parentheses. We empirically find in Sec. 6 that both global approaches improve fairness compared to prior methods, and that DPSGD-Global-Adapt has improved utility over DPSGD-Global. While the alterations are minor, our main contributions are elucidating that gradient misalignment is the main cause of disparate impact, and identifying that global scaling can prevent this problem.\n\nBoth global methods apply the sampled Gaussian mechanism (Mironov et al., 2019) to gradient norms with a sensitivity of C0, and hence are amenable to the same DP analysis as DPSGD itself. In DPSGD-Global-Adapt, the additional step of privately estimating the number of gradients with norm larger than τ · Z must be accounted for in the overall DP guarantee via a composition of sampled Gaussian mechanisms. From the analysis in (Mironov et al., 2019), DPSGD-Global-Adapt is (ε, δ)-DP for any σ1, σ2 > 0, where ε can be determined numerically given δ. However, our adaptive method is empirically not sensitive to the exact count bt, so a relatively large amount of noise can be used, see (Andrew et al., 2021) for comparison. In practice we used σ2 ≈ 10σ1 which produced a negligible additional cost in the overall privacy budget.\n\nFinally, we note that other approaches for mitigating unfairness, specifically DPSGD-F (Xu et al., 2021) and that of Tran et al. (2021a), require protected group labels for the training set. Collecting such labels may expose individuals to additional privacy risks in the case of security breaches, or may be prohibited in practice. Both global methods have the advantage of not requiring protected group labels for training data, and treat all training examples on an equal footing, thereby avoiding disparate treatment, while disparate impact is mitigated by reducing gradient misalignment.\n\n6 EXPERIMENTS\n\nIn our experiments we provide evidence that gradient misalignment is the most significant cause of unfairness, and demonstrate that global scaling can effectively reduce unfairness by aligning gradients. Our code for reproducing the experiments is provided as supplementary material.\n\n6.1 EXPERIMENT SETTINGS\n\nFor all experiments, full details are provided in App. B. We use an artificially unbalanced MNIST training dataset where class 8 only constitutes about 1% of the dataset on average, and protected groups are the classes. We also use two census datasets popular in the ML fairness literature, Adult and Dutch (van der Laan, 2000), preprocessed as in Le Quy et al. (2022). For both datasets, “sex” is the protected group attribute which is balanced between males and females. Finally, we use the CelebA dataset (Liu et al., 2015) for binary classification on the gender label. The protected group attribute is whether the image contains eyeglasses. Images with eyeglasses comprise 12% of male images but only 2% of female images, and are more difficult to classify accurately.\n\nWe compare both global scaling techniques (Alg. 2) against two methods designed to reduce unfairness, DPSGD-F (Xu et al., 2021) (Alg. 3) and the Fairness-Lens method (Tran et al., 2021a) (Alg. 4), both of which are reviewed in App. B.5. Each method’s effectiveness in removing disparate impact is measured using privacy cost πa (Eq. 1), and excessive risk Ra (Eq. 2) per group, as well as the privacy cost gap πa,b, and excessive risk gap Ra,b between groups. For MNIST, the underrepresented group 8 is compared to group 2 (Xu et al., 2021). All experiments were run for 5 random seeds, and results are given as means ± standard errors.\n\nFor MNIST and CelebA, all methods train a convolutional neural network with two layers of 32 and 16 channels, 3x3 kernels, and tanh activations. Adult uses an MLP model with two hidden layers of 256 units, while Dutch uses a logistic regression model. For all private methods, we use an RDP accountant (Mironov, 2017) with δ = 10−6. As a baseline, for DPSGD we set σ = 1, C0 = 0.5 for Adult, σ = 1, C0 = 0.1 for Dutch, and σ = 0.8, C0 = 1 for image datasets. With this, training 20 epochs for tabular datasets, 60 epochs for MNIST and 30 epochs for CelebA gives ε = 3.41 for Adult, ε = 2.27 for Dutch, ε = 5.90 for MNIST, and ε = 2.49 for CelebA. DPSGD-F has negligibly\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nhigher ε, while our method achieves the same ε guarantees to three significant digits. Complete hyperparameters are given in App. B.2.\n\n6.2 RESULTS\n\nTable 2: Performance and Fairness metrics for MNIST\n\nMETHOD\n\nACC 2\n\nACC 8\n\nπ2\n\nπ8\n\nπ2,8\n\nLOSS 2\n\nLOSS 8\n\nR2\n\nR8\n\nR2,8\n\n98.0±0.1 84.3±1.1 -\nNON PRIVATE 89.0±0.1 26.3±0.4 8.9±0.1 57.9±1.3 48.9±1.3 0.67±0.01 2.56±0.04 0.61±0.01 2.24±0.03 1.63±0.03 DPSGD 89.5±0.1 59.3±0.4 8.5±0.1 24.9±1.3 16.4±1.3 0.65±0.01 1.47±0.04 0.59±0.01 1.16±0.03 0.56±0.04 DPSGD-F 90.6±0.2 62.0±2.6 7.4±0.1 22.2±2.6 14.8±2.7 0.34±0.01 1.31±0.04 0.28±0.01 0.99±0.03 0.71±0.04 DPSGD-G. DPSGD-G.-A. 92.0±0.2 65.5±1.2 6.0±0.2 18.8±0.9 12.8±0.8 0.35±0.01 1.20±0.04 0.29±0.01 0.89±0.03 0.60±0.03\n\n- 0.06±0.00 0.32±0.01\n\n-\n\n-\n\n-\n\n-\n\nTable 3: Performance and Fairness metrics for CelebA\n\nMETHOD\n\nACC W/O ACC W πW/O\n\nπW πW/O, W LOSS W/O LOSS W\n\nRW/O\n\nRW RW/O, W\n\nNON PRIVATE DPSGD DPSGD-F DPSGD-G. DPSGD-G.-A.\n\n-\n\n95.8±0.1 89.7±0.4 -\n86.5±0.2 74.0±0.6 9.3±0.3 15.7±0.6 6.4±0.7 91.8±0.2 79.7±0.5 4.0±0.2 10.0±0.6 6.0±0.6 7.2±0.6 4.5±0.5 93.1±0.3 82.5±0.5 2.7±0.3 5.2±0.5 3.6±0.4 94.2±0.1 84.5±0.2 1.6±0.2\n\n-\n\n-\n\n0.11±0.00 0.24±0.01 -\n0.60±0.01 1.34±0.05 0.49±0.01 1.10±0.05 0.61±0.05 0.32±0.01 0.97±0.04 0.21±0.01 0.73±0.04 0.52±0.04 0.21±0.01 0.57±0.05 0.10±0.01 0.33±0.05 0.24±0.04 0.17±0.00 0.45±0.01 0.06±0.00 0.21±0.01 0.15±0.01\n\n-\n\nTables 2 and 3 display the accuracy and loss, along with privacy cost and excessive risk metrics respectively for MNIST on classes 2 and 8 and CelebA on group W with eyeglasses, and group W/O without.3 Recall that higher is better for accuracy, but for all other metrics lower is better. According to the one-sided Wilcoxon signed rank test, both global methods have statistically significant (p < 0.05) improvement over DPSGD on accuracy, loss, privacy cost gap, and excessive risk gap. Similarly, DPSGD-Global-Adapt has statistically significant improvement over DPSGD-Global and DPSGD-F on accuracy and loss. The same conclusions hold for the Adult dataset, and also for Dutch with the exception of DPSGD-Global being comparable to DPSGD in loss, see Tables 4 and 5 in App. B.7. We infer that the global scaling technique mitigates unfairness, while our modifications further improve utility.\n\nNot only are final model metrics improved, we see that DPSGD-Global-Adapt trains more similarly to non-private SGD in Fig. 3 for Dutch (cf. Figs. 8, 9, and 10 in App. B.7 for Adult, MNIST, and CelebA). This shows the average train loss per iteration, and average norm of the batched gradient. The difference in loss for groups in DPSGD-Global-Adapt resembles that of the non-private method more closely than other methods. Consider Fig. 3 (bottom), where the group M average norm does not converge to 0 in DPSGD, a problem which is somewhat improved in DPSGD-F, while for FairLens the group F norms become much larger. In DPSGD-Global-Adapt the norms for both groups remain small, but importantly the gap between groups is reduced.\n\n3The FairLens method (Tran et al., 2021a) is not compared for MNIST and CelebA because the author-\n\nprovided code only handles binary classification problems, and does not scale to image datasets.\n\nFigure 3: Dutch dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Adult dataset. Top Rdir Rmag\n\na , excessive risk due to magnitude error per group. See Prop. 2 for definitions.\n\na , excessive risk due to gradient misalignment per group. Bottom\n\na , and magnitude error Rmag Fig. 4 shows the excessive risk terms due to gradient misalignment Rdir for Adult at each iteration (see Figs. 11, 12, and 13 in App. B.7 for Dutch, MNIST, and CelebA). We see that global clipping almost completely removes direction errors as intended, but as a tradeoff increases magnitude error. However, we have argued that direction error is the more severe cause of disparate impact over the course of training, which is borne out by the results in Tables 1, 2 and 3, as well as 4, and 5 in App. B.7. Direction errors introduce bias which accumulates, whereas magnitude errors do not alter the convergence path, and noise errors add zero bias and tend to cancel out.\n\na\n\n6.3 TIGHTNESS OF LOWER BOUNDS\n\nb\n\na −Rdir\n\na − Rclip\n\nIn Fig. 5 we compare the usefulness of the lower bound of Rclip given in the proof of Theorem 3 in Tran et al. (2021a), to the lower bound we give in Prop. 3 for Rdir b . We see that while group 0 experiences disparate impact due to clipping, the lower bound from Tran et al. (2021a) is negative for each iteration, failing to capture that Rclip 1 . On the other hand, the true values of Rdir 1 are closely lower-bounded in our version, such that disparate impact due to direction error is accurately predicted. The assumptions of Prop. 3 are discussed in App. B.6.\n\n0 > Rclip\n\n0 − Rdir\n\nFigure 5: Comparison of excessive risk gaps R0,1 to lower bounds on Adult. Left: R0,1 due to clipping error, and bound from Tran et al. (2021a). Right: R0,1 due to direction error, and bound from our Prop. 3.\n\n7 DISCUSSION\n\nIn this paper we identified a core cause of disparate impact in DPSGD, gradient misalignment, and proposed a mitigating solution, global scaling. We empirically verified that global scaling is successful in improving fairness in terms of accuracy and loss over DPSGD and other fair baselines on several datasets. Our method has additional advantages over other fair baselines in that it does not require the collection of protected group data for training, does not involve disparate treatment, and it removes disparate impact for all groups simultaneously.\n\nIt is important to note that while global scaling is effective at reducing disparate impact by aligning gradients, it does not resolve the privacy-utility trade-off, which exists in any private mechanism fundamentally. Nor does it ensure that the model is non-discriminatory towards subgroups, only that adding privacy does not exacerbate unfairness. For example, biases in data collection or discriminatory modelling assumptions can cause disparate impact within the non-private model, which overlaying global scaling will not cure. Any models trained with global scaling should still be validated for fairness independently; failure to do so could unknowingly cause additional unfairness.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and In Proceedings of the 2016 ACM SIGSAC\n\nLi Zhang. Deep learning with differential privacy. conference on computer and communications security, pp. 308–318, 2016.\n\nJohn M. Abowd. The U.S. Census Bureau Adopts Differential Privacy. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2867, 2018. ISBN 9781450355520.\n\nMohammed Adnan, Shivam Kalra, Jesse C. Cresswell, Graham W. Taylor, and Hamid R. Tizhoosh. Federated learning and differential privacy for medical image analysis. Scientific reports, 12(1): 1–10, 2022.\n\nGalen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially Private Learning with Adaptive Clipping. In Advances in Neural Information Processing Systems, volume 34, pp. 17455–17466, 2021.\n\nEugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model accuracy. In Advances in Neural Information Processing Systems, volume 32, pp. 15479–15488, 2019.\n\nZhiqi Bu, Hua Wang, Qi Long, and Weijie J. Su. On the Convergence of Deep Learning with\n\nDifferential Privacy. arXiv preprint arXiv:2106.07830, 2021.\n\nJoy Buolamwini and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81, pp. 77–91, 2018.\n\nNicholas Carlini, Chang Liu, ́Ulfar Erlingsson, Jernej Kos, and Dawn Song. The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. In 28th USENIX Security Symposium, pp. 267–284, 2019.\n\nHongyan Chang and Reza Shokri. On the privacy risks of algorithmic fairness.\n\nIn 2021 IEEE\n\nEuropean Symposium on Security and Privacy, pp. 292–303, 2021.\n\nKamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially Private Empirical\n\nRisk Minimization. Journal of Machine Learning Research, 12(29):1069–1109, 2011.\n\nXiangyi Chen, Steven Z. Wu, and Mingyi Hong. Understanding Gradient Clipping in Private SGD: A Geometric Perspective. In Advances in Neural Information Processing Systems, volume 33, pp. 13773–13782, 2020.\n\nChristopher A Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha, Nicolas Papernot, and Xiao Wang. CaPC Learning: Confidential and Private Collaborative Learning. In International Conference on Learning Representations, 2020.\n\nAlexandra Chouldechova and Aaron Roth. A Snapshot of the Frontiers of Fairness in Machine\n\nLearning. Commun. ACM, 63(5):82–89, 2020. ISSN 0001-0782.\n\nRachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the Compatibility of Privacy and Fairness. In Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, pp. 309–315, 2019. ISBN 9781450367110.\n\nCynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Found. ISSN 1551-305X. doi: 10.1561/\n\nTrends Theor. Comput. Sci., 9(3–4):211–407, August 2014. 0400000042.\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating Noise to Sensitivity in Private Data Analysis. In Theory of Cryptography, pp. 265–284. Springer Berlin Heidelberg, 2006. ISBN 978-3-540-32732-5.\n\nMichael D. Ekstrand, Rezvan Joshaghani, and Hoda Mehrpouyan. Privacy for All: Ensuring Fair and Equitable Privacy Protections. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81, pp. 35–47, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTom Farrand, Fatemehsadat Mireshghallah, Sahib Singh, and Andrew Trask. Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy. In Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice, pp. 15–19, 2020.\n\nCraig Gentry. Fully Homomorphic Encryption Using Ideal Lattices. In Proceedings of the Forty-\n\nFirst Annual ACM Symposium on Theory of Computing, pp. 169–178, 2009.\n\nM.F. Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. Communications in Statistics - Simulation and Computation, 19(2):433–450, 1990. doi: 10.1080/03610919008812866.\n\nMatthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi Malvajerdi, and Jonathan Ullman. Differentially Private Fair Learning. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3000–3008, 2019.\n\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? Advances in Neural Information Processing Systems, 33: 22205–22216, 2020.\n\nMimansa Jaiswal and Emily Mower Provost. Privacy Enhanced Multimodal Neural Representations for Emotion Recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05): 7985–7993, 2020. doi: 10.1609/aaai.v34i05.6307.\n\nShivam Kalra, Junfeng Wen, Jesse C. Cresswell, Maksims Volkovs, and Hamid R. Tizhoosh. arXiv preprint\n\nProxyFL: Decentralized Federated Learning through Proxy Model Sharing. arXiv:2111.11343, 2021.\n\nTai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, and Eirini Ntoutsi. A survey on datasets for fairness-aware machine learning. WIREs Data Mining and Knowledge Discovery, pp. e1452, 2022.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\n\nIn Proceedings of International Conference on Computer Vision, December 2015.\n\nH. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017.\n\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A Survey\n\non Bias and Fairness in Machine Learning. ACM Comput. Surv., 54(6), 2021.\n\nIlya Mironov. R ́enyi differential privacy. 2017 IEEE 30th Computer Security Foundations Sympo-\n\nsium (CSF), Aug 2017. doi: 10.1109/csf.2017.11.\n\nIlya Mironov, Kunal Talwar, and Li Zhang. R ́enyi Differential Privacy of the Sampled Gaussian\n\nMechanism. arXiv preprint arXiv:1908.10530, 2019.\n\nHussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In International Conference on Machine Learning, pp. 7066–7075. PMLR, 2020.\n\nMilad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papemoti, and Nicholas Carlin. Adversary instantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposium on Security and Privacy, pp. 866–882. IEEE, 2021.\n\nVenkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X. Yu, Sashank J. Reddi, and Sanjiv Ku-\n\nmar. AdaCliP: Adaptive Clipping for Private SGD. arXiv preprint arXiv:1908.07643, 2019.\n\nDavid Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, Ashwin Machanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 189–199, 2020.\n\nCuong Tran, My Dinh, and Ferdinando Fioretto. Differentially Private Empirical Risk Minimization under the Fairness Lens. In Advances in Neural Information Processing Systems, volume 34, pp. 27555–27565, 2021a.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nCuong Tran, Ferdinando Fioretto, and Pascal Van Hentenryck. Differentially Private and Fair Deep Learning: A Lagrangian Dual Approach. Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):9932–9939, May 2021b.\n\nPaul van der Laan. The 2001 Census in the Netherlands Integration of Registers and Surveys. In\n\nInsee-Eurostat Seminar on Censuses after 2001, 2000.\n\nDepeng Xu, Shuhan Yuan, and Xintao Wu. Achieving differential privacy and fairness in logistic regression. In Companion Proceedings of The 2019 World Wide Web Conference, pp. 594–599, 2019. doi: 10.1145/3308560.3317584.\n\nDepeng Xu, Wei Du, and Xintao Wu. Removing Disparate Impact on Model Accuracy in Differentially Private Stochastic Gradient Descent. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1924–1932, 2021.\n\nAndrew Chi-Chih Yao. How to generate and exchange secrets.\n\nIn 27th Annual Symposium on\n\nFoundations of Computer Science, pp. 162–167, 1986.\n\nA THEORETICAL RESULTS\n\nA.1 PROOFS OF MAIN RESULTS\n\nIn this section we provide complete proofs for our theoretical contributions.\n\nProposition 2. Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The excessive risk due to clipping experienced by group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as\n\n(cid:68)\n\nRclip\n\na ≈ ηt (cid:68)\n\n+ ηt\n\ngDa , E\n\ngDa , E (cid:104) ∥ ̄gB ∥\n\n(cid:104)(cid:16)\n\n1 − ∥ ̄gB ∥ ∥gB ∥\n\n(cid:17)\n\n(cid:105)(cid:69)\n\n+ η2\n\nt 2\n\nE\n\n(cid:104)(cid:16) ∥ ̄gB ∥2\n\n∥gB ∥2 − 1\n\n(cid:17)\n\n(cid:105)\n\nBH a gT\n\nl gB\n\n∥gB ∥ (gB−MBgB)\n\n+ η2\n\nt 2\n\nE\n\n(cid:104) ∥ ̄gB ∥2 ∥gB ∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB)−gT\n\nBH a\n\nl gB\n\ngB (cid:105)(cid:69)\n\n(Rmag a ) (cid:1)(cid:105)\n\n,\n\n(Rdir a )\n\nwhere gDa , ̄gDa denote the average non-clipped and clipped gradients over group a at iteration t, H a l refers to the Hessian over group a, and MB is an orthogonal matrix such that ̄gB and MBgB are colinear. The expectations are taken over batches of data.\n\nWe remark that assuming a twice-differentiable loss is a mild requirement in machine learning where most loss functions and models are designed to be smooth enough for backpropagation.\n\nProof.\n\nThe proof is based on a Taylor expansion of the excessive risk, as in Tran et al. (2021a).\n\n(cid:13) (cid:13) Let MB be an orthogonal matrix such that ̄gB = MB (cid:13) and gB and ∥ ̄gB ∥ ∥gB ∥ gB are colinear, and so the former characterizes direction error, and the latter error in magnitude. The excessive risk due to error in magnitude for group a at iteration t is then given by\n\n. In this way, ∥ ̄gB∥ =\n\n∥gB ∥ gB\n\n∥gB ∥ gB\n\n(cid:16) ∥ ̄gB ∥\n\n∥ ̄gB ∥\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:17)\n\n(cid:20)\n\n(cid:18)\n\nL\n\nθt − ηt\n\nE\n\n∥ ̄gB∥ ∥gB∥\n\n(cid:19)\n\n(cid:21)\n\ngB; Da\n\n− L (θt − ηtgB; Da)\n\n,\n\nin loss of using the update vector ∥ ̄gB ∥\n\n∥gB ∥ gB rather than gB, where the expectathe cost tion is over randomness of batch sampling. We perform second-order Taylor expansion of E\n\nand take the expectation to get that\n\n∥ ̄gB ∥\n\n(cid:17)(cid:105)\n\nL\n\n(cid:16)\n\n(cid:104)\n\nθt − ηt\n\n∥gB ∥ gB; Da\n\n(cid:20)\n\n(cid:18)\n\nL\n\nθt − ηt\n\nE\n\n∥ ̄gB∥ ∥gB∥\n\n(cid:19)(cid:21)\n\ngB; Da\n\n≈ L (θt; Da) − ηt\n\n(cid:28)\n\ngDa , E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\ngB\n\n+\n\nη2 t\n2\n\nE\n\n12\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n.\n\nPublished as a conference paper at ICLR 2023\n\nHence,\n\nRclip\n\na = ηt⟨gDa , gD − ̄gD⟩ +\n\n= ηt⟨gDa , gD − ̄gD⟩ +\n\n(cid:28)\n\n(cid:28)\n\n− ηt\n\n+ ηt\n\ngDa , E\n\ngDa, E\n\nη2 t\n2 η2 t\n2 (cid:20) ∥ ̄gB∥ ∥gB∥ (cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:0)E[ ̄gT\n\nBH a\n\nl ̄gB] − E[gT\n\nBH a\n\nl gB](cid:1)\n\n(cid:0)E[ ̄gT\n\nl ̄gB] − E[gT BH a (cid:21)(cid:29)\n\nl gB](cid:1)\n\nBH a (cid:20) ∥ ̄gB∥2 ∥gB∥2 gT (cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\n(cid:21)\n\n(cid:21)\n\nBH a\n\nl gB\n\nBH a\n\nl gB\n\nη2 t\n2 η2 t\n2\n\nE\n\nE\n\ngB\n\ngB\n\n(cid:21)(cid:29)\n\n+\n\n−\n\n(cid:28)\n\n= ηt\n\ngDa , gD − E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:28)\n\ngDa , E\n\n+ ηt\n\n(cid:28)\n\n= ηt\n\ngDa , gD − E\n\ngB\n\n(cid:20) ∥ ̄gB∥ ∥gB∥ (cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\nη2 t\n2\n\n+\n\n(cid:29)\n\ngB\n\n(cid:21)\n\n− ̄gD\n\n+\n\n(cid:21)(cid:29)\n\nη2 t\n2\n\n+\n\n(cid:29)\n\ngB\n\n(cid:21)\n\n(cid:18)\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT E (cid:2) ̄gT\n\nBH a\n\n(cid:18)\n\nη2 t\n2 (cid:20)(cid:18) ∥ ̄gB∥2\n\nE\n\nη2 t\n2\n\n∥gB∥2 − 1 E (cid:2) ̄gT\n\nBH a\n\nl ̄gB\n\n(cid:18)\n\ngB\n\n− ̄gD\n\n+\n\n(cid:28)\n\ngDa , E\n\n+ ηt\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\nBH a\n\nl gB\n\n(cid:21)\n\n− E (cid:2)gT\n\nBH a\n\nl gB\n\n(cid:3) − E\n\nl ̄gB\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(cid:19)\n\n(cid:3)\n\n(cid:21)(cid:19)\n\n(cid:19)\n\n(cid:21)\n\nBH a gT\n\nl gB\n\n(cid:3) − E\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(Rmag a )\n\n(cid:21)(cid:19)\n\n.\n\n(Rdir a )\n\nWe can also further simplify Rdir a linear transformation\n\na by using that ̄gD = E[ ̄gB], ̄gB = MB\n\n(cid:16) ∥ ̄gB ∥\n\n∥gB ∥ gB\n\n(cid:17)\n\nand that MB is\n\n(cid:28)\n\nRdir\n\na = ηt\n\ngDa , E\n\n(cid:18) ∥ ̄gB∥ ∥gB∥\n\ngB − MB\n\n(cid:20) ∥ ̄gB∥ ∥gB∥ (cid:20) ∥ ̄gB∥2 ∥gB∥2 (MBgB)T H a\n\nl (MBgB)\n\n(cid:19)(cid:21)(cid:29)\n\ngB\n\n(cid:18)\n\nE\n\nη2 t\n2\n\n(cid:21)\n\n− E\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(cid:21)(cid:19)\n\n+\n\n(cid:28)\n\n= ηt\n\ngDa , E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\n(gB − MBgB)\n\n+\n\nη2 t\n2\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(5)\n\n(cid:21)\n\n(cid:1)\n\n.\n\nProposition 3. Assume the loss l is twice continuously differentiable and convex with respect to the model parameters. As well, assume that ηt ≤ (maxk∈[K] λk)−1 where λk is the maximum eigenvalue of the Hessian H k\n\nl . For groups a, b ∈ [K], Rdir\n\na > Rdir\n\nif\n\nb\n\nE (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) >\n\n∥gDb ∥ ∥gDa ∥\n\nE (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥ ̄gB∥2] ∥gDa ∥\n\n,\n\n(6)\n\nwhere θk\n\nB = ∠(gDk , gB) and ̄θk\n\nB = ∠(gDk , ̄gB) for a group k ∈ [K]. Furthermore, the bound is tight.\n\nAgain, requiring a twice continuously differentiable loss is a mild requirement. However, when neural networks are used most loss functions are non-convex. Empirically we see in Fig. 5 that the lower bound can still apply in practice. The requirement on the learning rate is under the control of the practitioner, and we have verified that in practice it can be satisfied.\n\nProof.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nThis proof follows some steps presented in Lemma 2 of Tran et al. (2021a). We seek a simplified condition for when the following is positive,\n\nRdir\n\na − Rdir\n\nb = ηt\n\n(cid:28)\n\ngDa, E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\n(cid:28)\n\n(gB − MBgB)\n\n− ηt\n\ngDb , E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\n(gB − MBgB)\n\n+\n\n−\n\nη2 t\n2 η2 t\n2\n\nE\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 (cid:20) ∥ ̄gB∥2 ∥gB∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n(7)\n\n(cid:0)(MBgB)T H b\n\nl (MBgB) − gT\n\nBH b\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n.\n\nLooking at one of the inner product terms, we use that ⟨x, y⟩ = ∥x∥∥y∥ cos(x, y) and linearity of expectation to obtain (cid:20) ∥ ̄gB∥ ∥gB∥\n\n(⟨gDa , gB⟩ − ⟨gDa , MBgB⟩)\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(gB − MBgB)\n\ngDa, E\n\n= E\n\n(cid:21)(cid:29)\n\n(cid:28)\n\n(cid:21)\n\n= ∥gDa ∥E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:0)∥gB∥ cos(gDa , gB)\n\n− ∥MBgB∥ cos(gDa , MBgB)(cid:1)\n\n(8)\n\n(cid:21)\n\nwhere θa the definition of MB such that ̄gB and MBgB are aligned and ∥gB∥ = ∥MBgB∥.\n\nB = ∠(gDa , MBgB) = ∠(gDa , ̄gB). The last equality follows from\n\nB = ∠(gDa , gB) and ̄θa\n\n= ∥gDa ∥E (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) ,\n\na\n\non bound the BH a l (MBgB) − gT l gB l is positive semi-definite such that xT H a\n\ndifference\n\n(cid:1)(cid:105)\n\nin\n\n.\n\nconjugates\n\nof\n\nthe Hessian,\n\nNote that since we assume the loss l is\n\nl x ≥ 0 for all vectors x. It follows\n\nalso\n\nget\n\nWe E\n\ncan (cid:104) ∥ ̄gB ∥2 ∥gB ∥2\n\n(cid:0)(MBgB)T H a convex, the Hessian H a that E[xT H a (cid:20) ∥ ̄gB∥2 ∥gB∥2\n\nE\n\nl x] ≥ 0 and so using linearity of expectation,\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n≤ E\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 (MBgB)T H a l is symmetric and hence xT H a\n\nl (MBgB)\n\n(cid:21)\n\n.\n\nl x ≤ l . We then again use that ∥MBgB∥ = ∥gB∥ and\n\n(9)\n\nSince l is twice continuously differentiable we have that H a λa∥x∥2 where λa is the maximum eigenvalue of H a linearity of expectation to obtain\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n≤ λaE (cid:2)∥ ̄gB∥2(cid:3) .\n\n(10)\n\nSimilar analysis gives that E\n\n(cid:104) ∥ ̄gB ∥2 ∥gB ∥2\n\nCombining the above, it follows that\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:1)(cid:105)\n\n≥ −λaE[∥ ̄gB∥2].\n\nRdir\n\na − Rdir\n\nb ≥ ηt\n\n(cid:0)∥gDa ∥E (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) − ∥gDb ∥E (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3)(cid:1)\n\n−\n\nη2 t\n2\n\n(λa + λb)E[∥ ̄gB∥2],\n\nand since we assume ηt ≤\n\n1 maxk∈[K] λk\n\n,\n\nRdir\n\na − Rdir\n\nb ≥ ηt\n\n(cid:0)∥gDa ∥E (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) − ∥gDb ∥E (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) − E[∥ ̄gB∥2](cid:1) .\n\nIt follows that Rdir\n\na > Rdir\n\nb when the following is satisfied:\n\nE (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) >\n\n∥gDb ∥ ∥gDa ∥\n\nE (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥ ̄gB∥2] ∥gDa ∥\n\n.\n\n14\n\n(11)\n\n(12)\n\n(13)\n\nPublished as a conference paper at ICLR 2023\n\nFinally, to see that the bound is tight we simply note that the inequalities that were introduced can all be saturated simultaneously. In Eq. 9 we require that gT l gB = 0, and in Eq. 10 we l (MBgB) = λa∥MBgB∥2 for each batch. These independent conditions can require (MBgB)T H a plausibly be met for some H a l , gB, and MB. The only other inequality introduced is the assumption ηt ≤ for the sake of achieving saturation.\n\n, which we can strengthen to ηt =\n\nBH a\n\n1 maxk∈[K] λk\n\n1 maxk∈[K] λk\n\nA.2 ALTERNATE DECOMPOSITIONS OF THE CLIPPING ERROR\n\nIn Sec. A.2 we proposed a decomposition of the clipped batch gradient into parts representing\n\nmagnitude and direction error, ̄gB=MB to demonstrate that direction error causes the most severe problems for the final performance of models, and analysed the contributions of the two effects to the excessive risk in Prop. 2.\n\n. We presented a simple experiment in Table 1\n\n∥gB ∥ gB\n\n(cid:16) ∥ ̄gB ∥\n\n(cid:17)\n\nHowever, the decomposition we used is not unique, and furthermore it is not possible to completely isolate the two effects in the excessive risk analysis. For example, if we think of magnitude error as the difference in loss between using update vector gB and ∥ ̄gB ∥ ∥gB ∥ gB (γ in Fig. 6), then it follows that the remaining error is due to gradient misalignment, in other words, the difference in loss between using update vector ∥ ̄gB ∥ ∥gB ∥ gB and ̄gB (λ in Fig. 6). In this example, the error due to gradient misalignment includes both error in direction and error in magnitude, while magnitude error is “pure”,\n\nFigure 6: Decomposition of steps between gB and ̄gB.\n\nRmag\n\na = E[L(θt − ηt a = E[L(θt − ηt ̄gB; Da) − L(θt − ηt\n\nRdir\n\n∥ ̄gB ∥\n\n∥gB ∥ gB; Da) − L(θt − ηtgB; Da)],\n\n∥ ̄gB ∥\n\n∥gB ∥ gB; Da)].\n\n(14)\n\n(15)\n\nA different way of decomposing the clipping error is considering the direction error as the difference in loss between using update vector gB and MBgB (α in Fig. 6). In this case, direction error is pure, i.e. does not include difference in magnitudes. It follows that the remaining error is magnitude error, so is the difference in loss between using update vector MBgB and ̄gB (β in Fig. 6). Thus, the magnitude error in this case quantifies the difference in loss of scaling the already misaligned ̄gB,\n\nRdir a\nRmag a\n\n∗\n\n= E[L(θt − ηtMBgB; Da) − L(θt − ηtgB; Da)], ∗ = E[L(θt − ηt ̄gB; Da) − L(θt − ηtMBgB; Da)].\n\n(16)\n\n(17)\n\nIn our analysis we used the first decomposition where magnitude error can be completely corrected by an adjustment of the learning rate, and direction error, what we hypothesized to be the largest cause of disparate impact, is the remaining part of the clipping error. For completeness, by using the second decomposition we can derive alternative versions of Props. 2 and 3:\n\nProposition 2*. Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The excessive risk due to clipping experienced by group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as\n\nRclip\n\na ≈ ηt\n\n(cid:68)\n\ngDa , E\n\n(cid:104)(cid:16) ∥gB ∥\n\n∥ ̄gB ∥ −1\n\n(cid:17)\n\n(cid:105)(cid:69)\n\n ̄gB\n\n+ η2\n\nt 2\n\nE\n\n(cid:104)(cid:16)\n\n1 − ∥gB ∥2\n\n∥ ̄gB ∥2\n\n(cid:17)\n\nBH a ̄gT\n\nl ̄gB\n\n(cid:105)\n\n,\n\n+ E [ηt ⟨gDa, gD − MBgB]⟩ + η2\n\nt 2\n\nE (cid:2)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(Rmag\n\na\n\n∗)\n\n(Rdir a\n\n∗\n\n)\n\n(cid:3) ,\n\nwhere gDa , ̄gDa denote the average non-clipped and clipped gradients over group a at iteration t, H a l refers to the Hessian over group a, and MB is an orthogonal matrix such that ̄gB and MBgB are colinear. The expectations are taken over batches of data.\n\nProposition 3*. Assume the loss l is twice continuously differentiable and convex with respect to the model parameters. As well, assume that ηt ≤ (maxk∈[K] λk)−1 where λk is the maximum\n\n15\n\nPublished as a conference paper at ICLR 2023\n\neigenvalue of the Hessian H k\n\nl . For groups a, b ∈ [K], Rdir\n\na > Rdir\n\nb\n\nif\n\nE (cid:2)∥gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) >\n\n∥gDb ∥ ∥gDa ∥\n\nE (cid:2)∥gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥gB∥] ∥gDa ∥\n\n(18)\n\nwhere θk\n\nB = ∠(gDk , gB) and ̄θk\n\nB = ∠(gDk , ̄gB) for a group k ∈ [K].\n\nWe omit the proofs since they are directly analogous to those in App. A.1.\n\nB EXPERIMENTAL DETAILS\n\nB.1 DATASET PREPROCESSING\n\nMNIST We use the artificially unbalanced MNIST training dataset where class 8 is sampled with probability 9% such that class 8 only constitutes about 1% of the dataset on average. This gives about 6000 data samples for each class, other than class 8 with about 500. The protected group values are the class labels. As in Xu et al. (2021), we compare models on how they treat the under-represented class 8 versus the well-represented class 2. The test set remains balanced, with approximately 1000 samples for each class. Data is scaled to be in the domain [0,1].\n\nAdult The original Adult dataset4 consists of 48,842 samples, reduced to 45,222 by removing all samples with missing values. The “final weight” feature is removed and the “race” attribute is discretized by {white, non-white}, giving 5 numerical, 3 binary and 6 categorical features. The numerical features are normalized and the categorical features are one-hot encoded. As is typical in the fairness literature, choices for the protected attribute are “sex”, “race” (binary) and possibly the discretized “age”. We use “sex” by default. The classification label is “income” (whether or not income exceeds $50,000). Prior to sampling, the Adult dataset is unbalanced with respect to sex with 30,527 males and 14,695 females. We sample a balanced dataset as in Xu et al. (2021) with 14,000 females and 14,000 males on average.\n\nDutch The Dutch dataset van der Laan (2000)5 is preprocessed by dropping underage samples (14 and under) and removing the “weight” feature. As well, all “unemployed” samples are removed, as well as those with missing or middle-level “occupation”, for a total of 60,420 samples. Specifically, “occupation” values 3,6,7,8 are considered middle-level. “Occupation” is then made binary by considering values 4,5,9 as low-level professions (0) and 1,2 as high-level professions (1). The binary classification task is to predict “occupation”, given the rest of the features. We consider “sex” as the protected group attribute. The processed dataset is balanced with respect to “sex” with 30,147 male and 30,273 female samples.\n\nWe use an 80/20 train/test split for both tabular datasets.\n\nCelebA The CelebA dataset (Liu et al., 2015)6, consists of 64x64 pixel RGB images of celebrity faces, along with binary attributes describing each image. Many of these attributes are subjective, but we chose to use the most objective ones for training and group labels. We used the binary attribute “Male” for the classification target, which is roughly balanced at 84,434 males in 202,599 total images. The attribute “Eyeglasses” was our protected group label; although wearing eyeglasses in public typically does not construe sensitive information, we used this attribute because it was objectively defined, and formed a minority group which was empirically more difficult for models to classify accurately. Of the male images, 10,478 have eyeglasses, while only 2,715 female images have them. The training/validation/test split is provided with the dataset and is roughly in a 80/10/10 ratio.\n\n4The Adult dataset is available at archive.ics.uci.edu/ml/datasets/Adult. 5The Dutch dataset is also available through the work of Le Quy et al. (2022) at\n\nraw.githubusercontent.com/tailequy/fairness dataset/main/Dutch census/dutch census 2001.arff.\n\n6We accessed this dataset via kaggle.com/datasets/jessicali9530/celeba-dataset.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.2 EXPERIMENT SETTINGS\n\nWe set σ = 1, C0 = 0.5 for Adult, σ = 1, C0 = 0.1 for Dutch, while for MNIST and CelebA, we set σ = 0.8 and C0 = 1. For DPSGD-F, the gradient noise is unchanged σ2 = σ, and σ1 = 10σ2. For FairLens, we use regularization weights as in Tran et al. (2021a), λ1 = λ2 = 1. For non-global methods, the learning rate is ηt = 0.01 for all iterations t and all datasets except Dutch which has ηt = 0.8. For DPSGD-Global we have ηt = 1, Z = 50 for Adult, ηt = 2, Z = 1 for Dutch, ηt = 0.2, Z = 100 for MNIST, and ηt = 0.1, Z = 100 for CelebA. For DPSGD-Global-Adapt we have σ2 = 10, Z = 50, ηZ = 0.1 for all datasets (the only exception is for CelebA Z = 100), ηt = 0.2, τ = 1 for Adult, η = 1, τ = 1 for Dutch, and η = 0.1, τ = 0.7 for MNIST and CelebA. All methods for all datasets use training and test batches of size 256.\n\nExperiments were conducted on single TITAN V GPU machines. Approximately four GPU-days were used to train all methods over five seeds for the four datasets.\n\nB.3\n\nIMPLEMENTATION DETAILS\n\na\n\nand Rnoise\n\nThe excessive risk terms for different groups (Rclip a in Prop. 2) all involve the Hessian of the loss function with respect to the model parameters. Calculating the Hessian as a matrix is computationally expensive, but more crucially requires memory that scales quadratically in the number of parameters. In the previous work studying Rclip , Tran et al. (2021a) use the PyHessian library to compute the Hessian as a matrix, and then used it to compute the products and traces needed for Rclip . Because this approach incurs a high memory burden, the models trained were limited to small MLPs with a single hidden layer of 20 hidden units.7\n\nin Prop. 1 and Rmag\n\nand Rnoise\n\nand Rnoise\n\nand Rdir\n\na\n\na\n\na\n\na\n\na\n\na\n\nIn our implementation, provided as supplemental material, we avoid computing the Hessian as a matrix altogether which allows us to scale our experiments to common image datasets. For the four datasets, our models have parameter counts of N = 91650 for Adult, N = 120 for Dutch, N = 80522 for MNIST, and N = 120722 for CelebA, which would produce Hessian matrices with up to 14.5 billion entries. Instead, we compute the terms involving Hessians like H a l gB through Hessian-vector products (HVPs) using the functorch8 library with PyTorch 1.11. Using HVPs requires memory comparable to that used when computing gradients for SGD.\n\nl ) = (cid:80)N\n\nFor the trace of the Hessian matrix, also called the Laplacian, one possible approach that does not require realizing the entire matrix in memory is to compute HVPs with unit vectors to isolate each diagonal element: Tr(H a l Ii where Ii is the ith column of the identity matrix. While exact, this approach requires N HVPs for each group a ∈ K, of which there are at least two. Since this method is much too expensive for even the simple MLPs and CNNs we used, we instead employed Hutchinson’s trace estimator (Hutchinson, 1990) to estimate Tr(H a l z]. This estimator is unbiased when z is drawn from a Rademacher distribution which we used, and only requires n HVPs per group, where n can be chosen as large as required for convergence of the estimate. In practice we used n = 100.\n\nl ) = Ez[zT H a\n\ni=1 I T\n\ni H a\n\nAdditionally, whereas Tran et al. (2021a) replace dataset gradients gD and gDa with batch gradients when computing Rclip in Prop. 1, we use the exact gD and gDa . This eliminates an easily preventable source of noise in our results.\n\nand Rnoise\n\na\n\na\n\nTo further reduce computation time, we only evaluate excessive risk terms (Hessians) every 50, 100, 200, or 200 iterations for the Adult, Dutch, MNIST, and CelebA datasets respectively.\n\nB.4 DIRECTION ERROR IS MORE SEVERE THAN MAGNITUDE ERROR\n\nAs noted earlier, Prop. 2 only evaluates excessive risk for a single iteration, not necessarily capturing how each of Rdir contribute to convergence and disparate impact over the course of training. In order to evaluate the full impact of magnitude error and error due to gradient misalignment, we consider the difference in final loss and accuracy between models which have zero magnitude\n\na and Rmag\n\na\n\n7See implementation available at openreview.net/forum?id=7EFdodSWee4. 8See documentation at pytorch.org/functorch/stable/.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nerror and zero direction error in Table 1. In these experiments, we consider zero magnitude error to be when ∥ ̄gB∥ = ∥gB∥ for all batches, and zero direction error to be when gB and ̄gB are aligned for all batches. Note that these definitions correspond to comparing update vectors gB and ∥gB ∥ ∥ ̄gB ∥ ̄gB for the zero magnitude error experiment, and comparing update vectors gB and ∥ ̄gB ∥ ∥gB ∥ gB for the zero direction error experiment. These do not correspond to the definitions of Rdir in Prop. 2, but capture the intuitive definitions of direction and magnitude error. As described in App. A.2, while Rclip a = Rmag a , direction error and magnitude error cannot be purely separated with any definition of Rmag\n\na and Rmag\n\na + Rdir\n\na\n\na , Rdir a .\n\nB.5 BASELINE METHODS\n\nWe compared our approach DPSGD-Global-Adapt with its predecessor DPSGD-Global, which was designed to improve convergence, not fairness, as well as two approaches specifically designed to improve fairness.\n\nDPSGD-Global (Bu et al., 2021) is presented in Alg. 2, and involves scaling almost all per-sample gradients by a global factor rather than only scaling large gradients with ∥gi∥ > C0 by a normdependent factor. We say “almost all”, because scaling alone does not provide a strict upper bound on the sensitivity, as required for an application of the Gaussian mechanism, see Fig. 2b. The method additionally clips gradients to zero if their norm is above a strict upper bound Z, which we found to be unnecessarily aggressive. Otherwise, the global scaling factor is C0/Z, which ensures that the sensitivity, namely C0, is finite. The advantage of DPSGD-Global is that it can better preserve the direction of ̄gB, especially when no gradients are clipped to zero. Hence, Bu et al. (2021) advocate for setting Z larger than ∥gi∥ for any sample in the batch. The drawback of a large Z is that all gradients are scaled down by a larger factor, so the convergence will be slowed unless the learning rate is increased to compensate. Setting Z is itself a challenge because we cannot inspect the batch to determine maxi ∥gi∥ without accounting for that expense in our privacy budget. In Sec. 5 we described how DPSGD-Global-Adapt resolves these concerns, first by clipping less aggressively, to C0 instead of 0, while maintaining the same sensitivity, and second by adaptively setting Z each round according to a private estimate of how many gradients in a batch exceeded τ · Z (using the tolerance threshold τ ).\n\nXu et al. (2021) designed DPSGD-F as a method for removing disparate impact caused by DPSGD by adaptively setting the clipping threshold for different protected groups. The method was based on the observation that negatively impacted groups tended to have large gradient norms which were affected more by clipping. Hence, the clipping threshold is raised for groups with larger gradient norms, based on a private estimate of how many gradients per-group have ∥gi∥ > C0. Given large enough batch sizes, the private estimate can be done with much more noise as compared to the gradient update, so it does not meaningfully increase the privacy budget.\n\nOne drawback of this approach is that it requires group label information for every datapoint in the training set. In practice, especially in highly regulated industries, such information may not be permissible to use or even collect. Collecting additional private information from data subjects on protected attributes can itself be a negative process and creates unnecessary privacy risks. One major advantage of DPSGD-Global-Adapt is that it reduces unfairness without ever using group label information.\n\nWhile each group is clipped using its own threshold, noise is added to the batched gradient based on the sensitivity, determined by the largest group threshold. While all groups receive the same theoretical privacy guarantee in terms of (ε, δ), groups that are clipped to smaller thresholds may enjoy stronger empirical privacy guarantees, as determined for example by adversarial attacks (Jagielski et al., 2020; Nasr et al., 2021). Hence, it appears likely that DPSGD-F can produce unfairness in the amount of privacy afforded to different groups.\n\nDPSGD-F is shown in Alg. 3. Note that we present the algorithm as implemented in the author’s codebase, not as written in their paper. In our experiments we use the version shown in Alg. 3.\n\nOur final baseline, referred to as “FairLens” was developed in (Tran et al., 2021a) to reduce excessive risk from clipping, Rclip . Regularization terms are added to the loss function in DPSGD that specifically target these sources of excessive risk. The source of Rnoise was identified\n\na , and adding noise, Rnoise\n\na\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 3 DPSGD-F\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, noise multipliers σ1, σ2,\n\nlearning rates ηt\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, ai, yi) in B do\n\n▷ Compute per-sample gradients\n\n▷ Count samples per-group above/below clipping bound\n\n▷ Privatize unit sensitivity count vectors\n\nk∈[K]\n\n▷ Postprocessing\n\ngi ← ∇θl(fθt(xi), yi)\n\n(cid:9)(cid:12) (cid:12) (cid:9)(cid:12) (cid:12)\n\ni ∥ > C0 i ∥ ≤ C0\n\nfor k in [K] do mk ← (cid:12) (cid:8)i : ∥gk (cid:12) ok ← (cid:12) (cid:8)i : ∥gk (cid:12) k∈[K] ← (cid:8)mk, ok(cid:9) (cid:8) ̃mk, ̃ok(cid:9) k∈[K] ← (cid:8)max(⌊ ̃mk⌋, 0), max(⌊ ̃ok⌋, 0)(cid:9) (cid:8) ̃mk, ̃ok(cid:9) ̃m = (cid:80) k∈[K] ̃mk for k in [K] do\n\nk∈[K] + N (0, σ2\n\nI)\n\n1\n\n ̃bk = ̃mk + ̃ok Ck = C0 ·\n\n(cid:16)\n\n1 + ̃mk/ ̃bk\n\n ̃m/|B|\n\n(cid:17)\n\nfor (xi, ai, yi) in B do ̄gi ← gi · min\n\n(cid:16)\n\n(cid:17)\n\n(cid:0)(cid:80)\n\n ̃gB ← 1 |B| θt+1 ← θt − ηt ̃gB\n\n1, Ck ∥gi∥ i∈B ̄gi + N (0, σ2\n\n2C 2\n\n0\n\nI)(cid:1)\n\nwhere k = ai\n\n▷ Clip according to per-group clipping bounds\n\nto involve the per-group Laplacian of the loss l with respect to model parameters - a second order derivative whose computation scales poorly with model size. To avoid this difficulty, the authors used a stand-in for the Laplacian based on the distance of a point to the decision boundary.\n\nOur implementation is directly based off of code made available by the authors on OpenReview at openreview.net/forum?id=7EFdodSWee4. The version implemented in their code is shown in Alg. 4, and assumes there are only two mutually exclusive protected groups, denoted a and b. Hence, it is not applicable to the MNIST dataset. We also attempted to use this code for our CelebA experiments but found that the implementation did not scale to the simple CNNs we used. Therefore, we omitted FairLens from the CelebA experiments.\n\nB.6 VERIFYING ASSUMPTIONS ON LOWER BOUND\n\nb\n\na −Rdir\n\nIn Sec. 6.3 and Fig. 5 we compared the usefulness of our lower bound on Rdir from Prop. 3, to a previous lower bound in the literature. For our lower bound to be valid, the assumptions of Prop. 3 should be satisfied. The first assumption, that the loss is twice continuously differentiable with respect to the model parameters, holds since the model architecture is an MLP with tanh activations. However, the loss is not in general convex. The third assumption, that the inverse of the learning rate upper bounds the largest eigenvalue of any group’s Hessian, is checked empirically for each iteration in Fig. 7.\n\nFigure 7: The maximum eigenvalue of any group’s Hessian remains below ηt\n\n−1.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 FairLens\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, noise multiplier σ, learning\n\nrates ηt, regularization weights γ1, γ2\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, ai, yi) in B do\n\ngi ← ∇θl(fθt(xi), yi) 1, C0 ̄gi ← gi · min ∥gi∥\n\n(cid:16)\n\n(cid:17)\n\n(cid:80)\n\n|B|\n\n(cid:80)\n\ngB ← 1 i∈B gi ̄gB ← 1 i∈B ̄gi |B| for k in {a, b} do (cid:80) gBk ← 1 fk ← 1\n\n|Bk|\n\n(cid:80)\n\n|Bk|\n\ni∈B,ai=k gi i∈B,ai=k fθt(xi)\n\nR1 = |⟨gBa − gBb , ̄gB − gB⟩| R2 = 1 L = l(fθt(xi), yi) + γ1R1 + γ2R2 for (xi, ai, yi) in B do\n\n2 (fa · (1 − fa) + fb · (1 − fb))\n\n(cid:16)\n\ng′ i ← ∇θL(fθt(xi), yi) i ← g′ ̄g′ B ← 1 ̃g′ θt+1 ← θt − ηt ̃g′\n\ni · min (cid:0)(cid:80)\n\ni∈B ̄g′\n\n1, C0 ∥g′ i∥ i + N (0, σ2C 2\n\n|B|\n\n(cid:17)\n\n0\n\nB\n\nI)(cid:1)\n\n▷ Compute per-sample gradients of original loss\n\n▷ Define regularized loss\n\n▷ Compute per-sample gradients of regularized loss\n\n▷ Clip to ensure finite sensitivity\n\nB.7 ADDITIONAL RESULTS\n\nIn this section we complete the set of experimental results shown in Sec. 6 over all datasets and methods. All results are averaged over five random seeds with one standard error shown.\n\nTable 4: Performance and Fairness metrics for Adult dataset\n\nMETHOD\n\nACC M ACC F\n\nπM\n\nπF\n\nπM,F LOSS M LOSS F\n\nRM\n\nRF\n\nRM,F\n\nNON PRIVATE 80.5±0.4 92.2±0.1 -\n3.6±0.1 6.9±0.3 0.78±0.01 0.40±0.01 0.39±0.00 0.21±0.01 0.17±0.01 DPSGD 3.7±0.1 7.9±0.2 0.57±0.00 0.42±0.00 0.18±0.00 0.23±0.00 0.05±0.00 FAIRLENS 2.7±0.1 0.2±0.3 0.49±0.00 0.31±0.01 0.09±0.00 0.12±0.00 0.02±0.01 DPSGD-F 2.2±0.1 0.2±0.2 0.43±0.00 0.25±0.00 0.04±0.00 0.05±0.00 0.02±0.00 DPSGD-G. DPSGD-G.-A. 80.7±0.4 92.3±0.1 −0.1±0.1 −0.1±0.1 0.0±0.1 0.39±0.00 0.18±0.00 0.00±0.00 0.00±0.00 0.00±0.00\n\n- 69.9±0.4 88.5±0.1 10.6±0.3 68.8±0.4 88.5±0.1 11.7±0.2 2.5±0.3 78.0±0.6 89.4±0.1 2.0±0.2 78.5±0.5 89.9±0.1\n\n- 0.40±0.00 0.19±0.00\n\n-\n\n-\n\n-\n\nTable 5: Performance and Fairness metrics for Dutch dataset\n\nMETHOD\n\nACC M ACC F\n\nπM\n\nπF\n\nπM,F\n\nLOSS M\n\nLOSS F\n\nRM\n\nRF\n\nRM,F\n\nNON PRIVATE 79.9±0.2 86.9±0.0 -\n- 76.0±0.2 86.4±0.1 3.8±0.3 0.4±0.0 3.4±0.4 0.520±0.001 0.450±0.001 0.021±0.001 0.003±0.001 0.018±0.002 DPSGD 78.6±0.3 86.9±0.1 1.3±0.2 −0.1±0.0 1.4±0.2 0.552±0.001 0.526±0.000 0.053±0.001 0.079±0.000 0.026±0.001 FAIRLENS 0.3±0.0 0.7±0.1 0.503±0.001 0.447±0.001 0.005±0.001 0.000±0.000 0.005±0.001 78.9±0.2 86.6±0.1 0.9±0.1 DPSGD-F 0.4±0.0 0.4±0.2 0.510±0.001 0.460±0.001 0.012±0.001 0.013±0.001 0.002±0.001 79.0±0.2 86.5±0.1 0.8±0.1 DPSGD-G. 0.2±0.0 0.2±0.2 0.504±0.001 0.452±0.001 0.006±0.001 0.005±0.001 0.001±0.001 DPSGD-G.-A. 79.4±0.1 86.7±0.1 0.4±0.2\n\n- 0.499±0.000 0.447±0.000\n\n-\n\n-\n\n-\n\nFirst we look at the final performance and fairness metrics on the test set for Adult in Table 4 and Dutch in Table 5 (cf. MNIST in Table 2 and CelebA in Table 3). We see that FairLens is inconsistent in reducing the privacy cost gap and excessive risk gap compared to DPSGD. DPSGDF improves both fairness metrics while achieving better performance. DPSGD-Global improves over or is comparable to DPSGD-F in all metrics, and does so without requiring access to protected group membership information. Our method DPSGD-Global-Adapt further improves both performance and fairness by clipping less aggressively and adaptively setting the upper clipping threshold Z.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Adult dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\nFigure 9: MNIST dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\nFigure 10: CelebA dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\nTo go along with the training curves shown for Dutch in Fig. 3, we present the same for Adult in Fig. 8, MNIST in Fig. 9, and CelebA in Fig. 10. The trends are consistent across datasets - whereas DPSGD produces large values and a large gap for the gradient norms and losses between protected groups, our method DPSGD-Global-Adapt reduces the values and gap at all stages of training.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: Dutch dataset. Top: Excessive risk due to gradient misalignment per group. Bottom: Excessive risk due to magnitude error per group.\n\nFigure 12: MNIST dataset. Top: Excessive risk due to gradient misalignment per group. Bottom: Excessive risk due to magnitude error per group.\n\nFigure 13: CelebA dataset. Top: Excessive risk due to gradient misalignment per group. Bottom: Excessive risk due to magnitude error per group.\n\nWe also present the values of terms Rdir over training for Dutch in Fig. 11, for MNIST in Fig. 12, and CelebA in Fig. 13 as was done for Adult in Fig. 4. Both Global methods dramatically reduce Rdir a . Comparing to the final training results where global methods also show the best performance, this provides further evidence for our hypothesis that gradient misalignment is the most significant cause of disparate impact in DPSGD.\n\na compared to DPSGD at the cost of larger Rmag\n\na and Rmag\n\na\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 14: Excessive risk due to noise error per group for the Adult dataset\n\nFigure 15: Excessive risk due to noise error per group for the Dutch dataset\n\nFigure 16: Excessive risk due to noise error per group for the MNIST dataset\n\nFigure 17: Excessive risk due to noise error per group for the CelebA dataset\n\nWe note that by tuning the learning rate in DPSGD-Global and DPSGD-Global-Adapt, there is a 0 σ2, trade-off between magnitude error and noise error. Referring to Prop. 1, Rnoise we see that the excessive risk due to noise is affected by the learning rate ηt, the noise multiplier σ, clipping bound C0 and the trace of the Hessian for group a. In choosing a larger learning rate for the global methods to offset the magnitude error, we increase the noise error quadratically. Refer to the values of Rnoise over training for Adult in Fig. 14, Dutch in Fig. 15, MNIST in Fig. 16, and CelebA in Fig. 17. While the excessive risk due to noise is significantly larger for the global methods, these methods outperform all other private methods at the end of training, see Tables 2, 3, 4, 5. Gaussian noise adds zero bias and the errors it introduces tend to cancel out over the course of training. These observations further validate that direction error is the core cause of disparate impact, and minimizing gradient misalignment should be prioritized over other sources of unfairness.\n\na = η2\n\n2 Tr(H a\n\nl )C 2\n\nk\n\nt\n\n23",
  "translations": [
    "# Summary Of The Paper\n\nThis paper conducts a fine-grained analysis of DP-SGD to show that gradient misalignment is  a principal cause of the disparate impact that occurs when DP-SGD is applied to a dataset. This analysis compares two key components of the DP-SGD: clipping and noise addition. Ultimately, this paper shows that clipping further impacts and exacerbates gradient misalignment, which greatly increases the disparate impact of DP-SGD. This insight is nicely illustrated in a simple experiment that keeps the noise level in DP-SGD fixed, but changes the level of gradient alignment to show that when the gradient is more misaligned, it leads to more disparate results. The fix proposed by this paper is to  scale down all per-sample gradients in a batch by the same amount. The improved results are demonstrated experimentally on the MNIST and adult datasets.\n\n# Strength And Weaknesses\n\n### Strengths\n\n- This paper refines the cause of disparate impact in SGD, which is an important problem. \n\n- The experiment and analysis presented in Section 4 clearly show the main take home of this paper.\n\n- The proposed modification to DP-SGD is quite simple and straightforward. The proposed optimizer can be easily plugged-in to other settings.\n\n### Weaknesses\n- One of the benefits of the proposed DP-SGD-Adapt global is that it does not need group identifiers. However, it is unclear whether the methods that use group identifiers lead to models with better utility. While the comparison is not 1:1 here, it would still be good to know how these results compare to when one actually takes into account group identifiers.\n\n- The datasets considered the paper are small and somewhat toyish even though these are the datasets that were used in previous papers. It is unclear how this proposal scales to larger models and bigger setups. However, I think future work can likely address these issues.\n\n- The theorems and analysis presented applies or explicitly assume convexity or in some cases that the loss is twice differentiable. It is nice that the analysis generalizes to model classes that do not satisfy these assumptions but this issue could be better highlighted.\n\n- This work highlights gradient misalignment as a key cause of disparate impact, but it is hard to judge how much that is more important than say group difficulty as measured by the trace of the hessian of the loss per group. It could be that more difficult to learn groups have hessian-trace values that are substantially different than other groups. \n\n-  (Minor) The term 'excessive risk' is used quite a bit in a number of places in the paper. I believe it should be 'excess risk'.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing in this paper is quite clear and easy to follow. I have a minor suggestion above, but overall, each section is written and presented clearly. This work builds on insights from previous analyses of DP-SGD; however, it brings new information to light. Overall, the results show that gradient alignment is a key cause of disparate impact.\n\n# Summary Of The Review\n\nThis paper provides new insights on an important problem. The paper also demonstrates empirically improvements based on a modified DP-SGD algorithm that reduces disparate impact.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nN/A",
    "# Summary Of The Paper\nThe paper \"DISPARATE IMPACT IN DIFFERENTIAL PRIVACY FROM GRADIENT MISALIGNMENT\" by Esipova et al. addresses the intersection of differential privacy (DP) and fairness in machine learning. The authors identify gradient misalignment due to inequitable gradient clipping in differentially private stochastic gradient descent (DPSGD) as a significant contributor to disparate impact on underrepresented groups. They propose a novel approach, DPSGD-Global, which aligns gradients more effectively and reduces unfairness without requiring protected group labels. Experimental results demonstrate that the proposed methods significantly outperform baseline techniques in terms of both accuracy and fairness metrics.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive analysis of the causes of disparate impact in DPSGD and its innovative solutions that tackle these issues. The identification of gradient misalignment as a key factor is well-founded and supported by theoretical propositions. The proposed DPSGD-Global and its adaptive variant show promise in addressing the fairness-utility trade-off inherent in DP methods. However, a potential weakness is the reliance on specific datasets for experimental validation, which may limit the generalizability of the findings. Additionally, while the paper discusses the privacy-utility trade-off, more detailed exploration of this aspect could enhance the understanding of practical implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The definitions and theoretical underpinnings are presented in a way that is accessible to the reader. The experimental setup is adequately described, allowing for reproducibility of results, although more details on implementation could further improve this aspect. The novelty of aligning gradients in the context of DPSGD is significant, and the empirical results effectively validate the authors' hypotheses.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the understanding of fairness in differential privacy, highlighting the impact of gradient misalignment and offering effective solutions through innovative methods. The findings have important implications for developing fair and private machine learning models, although some limitations in dataset generalizability and detailed trade-off discussions exist.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" addresses the intersection of privacy and fairness in machine learning, specifically focusing on the effects of Differentially Private Stochastic Gradient Descent (DPSGD) on disparate impact across demographic groups. The authors identify gradient misalignment as a critical factor contributing to fairness disparities in DPSGD, resulting from inequitable gradient clipping practices. They propose two novel variants of DPSGD: DPSGD-Global, which scales gradients globally to maintain directionality, and DPSGD-Global-Adapt, which dynamically adjusts clipping thresholds. Experimental results across multiple datasets, including MNIST and CelebA, demonstrate that these methods significantly reduce disparate impact while improving model performance.\n\n# Strength And Weaknesses\nThe principal strength of the paper lies in its novel identification of gradient misalignment as a source of unfairness in DPSGD, providing valuable insights into the relationship between privacy mechanisms and fairness. The empirical validation across diverse datasets strengthens the findings, demonstrating consistent improvements in accuracy and reduced disparities without compromising privacy. However, the paper does have weaknesses, including the inherent trade-off between privacy and utility, which persists even with the proposed solutions. Furthermore, the performance improvements depend on specific dataset characteristics, and the complexity of the adaptive methods may pose challenges for practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The quality of the writing is high, making complex concepts accessible. The novelty of identifying gradient misalignment as a core issue in DPSGD is significant, contributing to the ongoing discourse on fairness in machine learning. Reproducibility is facilitated by detailed descriptions of the experimental setup, including dataset characteristics and evaluation metrics, although the complexity of the adaptive methods may require additional clarification for practical application.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful contribution to the field by elucidating the role of gradient misalignment in exacerbating fairness issues in DPSGD. The proposed methods show promise in mitigating these disparities while maintaining model performance, although challenges related to privacy-utility trade-offs and implementation complexities remain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" investigates the interplay between privacy and fairness in machine learning, particularly within sensitive industries such as healthcare and banking. The authors highlight that Differentially Private Stochastic Gradient Descent (DPSGD) may inadvertently amplify model unfairness due to gradient misalignment caused by the clipping of gradients and the addition of noise. They propose a novel method called DPSGD-Global to mitigate this issue by scaling gradients globally, which aligns the direction of gradients across different groups, thereby reducing disparate impact while maintaining utility. Their experimental results demonstrate statistically significant improvements in both fairness and accuracy when applying DPSGD-Global compared to traditional methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough investigation of the relationship between differential privacy and fairness, an area that has received limited attention in the literature. The introduction of DPSGD-Global is a significant contribution that addresses a critical issue of gradient misalignment. The experiments are well-designed, using diverse datasets and providing compelling evidence of the proposed method's effectiveness. However, the paper does have weaknesses; the limitations regarding the privacy-utility trade-off are only briefly acknowledged, and the need for independent validation of fairness in the global scaling approach could be more explicitly discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its arguments, methodologies, and findings. The theoretical foundations are solidly supported by propositions that are rigorously derived. The experimental results are clearly presented with appropriate metrics and comparisons to baseline methods. While the novelty of the approach is evident, the reproducibility of the results could be enhanced with more detailed descriptions of the experimental setups, hyperparameters, and baseline methods utilized.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field by addressing the often-overlooked intersection of differential privacy and fairness. The proposed DPSGD-Global method provides a promising solution to mitigate disparate impact in machine learning models. However, the paper could benefit from a deeper discussion of the implications of the proposed method on the privacy-utility balance and the need for external validation of fairness.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" presents a novel exploration of how inequitable gradient clipping leads to disparate impacts in differentially private stochastic gradient descent (DPSGD). The authors identify gradient misalignment as a critical issue and propose a solution termed DPSGD-Global, which aims to align gradients and mitigate disparate impact without needing access to protected group labels. The findings are supported by extensive experiments across multiple datasets, demonstrating improvements in fairness while maintaining utility. The paper also discusses the privacy-fairness trade-off and provides theoretical foundations for understanding the sources of risk in DPSGD.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear identification of gradient misalignment as a pivotal factor in disparate impact, coupled with a promising new method (DPSGD-Global) that addresses this issue effectively. The extensive experimental validation enhances the credibility of the proposed solution. However, the paper has notable weaknesses, including the limited exploration of how different model architectures may impact gradient misalignment and the challenges posed by hyperparameter tuning for the clipping parameter Z. Additionally, while the theoretical foundations are solid, the assumptions made may limit applicability in practical, complex scenarios. The discussions on privacy-fairness trade-offs and nuanced fairness issues could benefit from deeper analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its arguments clearly, although some sections contain dense technical details that could hinder broader understanding. The novelty of the paper is significant, addressing an underexplored area in the intersection of differential privacy and fairness. However, the reproducibility of the findings could be enhanced by providing more detailed guidelines for implementing the proposed method and by expanding the experimental evaluation to include a wider variety of datasets and applications.\n\n# Summary Of The Review\nThis paper makes a meaningful contribution to the field by addressing the critical issue of disparate impact in DPSGD through the lens of gradient misalignment. While the proposed DPSGD-Global method shows promise, the paper could benefit from addressing its limitations related to hyperparameter sensitivity, theoretical assumptions, and the need for broader empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" by Esipova et al. investigates the intersection of differential privacy (DP) and fairness in machine learning, specifically addressing how traditional methods may inadvertently exacerbate unfairness against protected groups. The authors identify gradient misalignment as a significant factor contributing to disparate impact during model training with differentially private stochastic gradient descent (DPSGD). They introduce a novel approach, DPSGD-Adapt, which dynamically adjusts gradient clipping based on group characteristics to mitigate this issue. Theoretical foundations are provided to quantify the risks associated with different groups, and empirical experiments demonstrate that DPSGD-Adapt reduces disparate impact while maintaining model performance across various datasets.\n\n# Strengths and Weaknesses\nThe paper's strengths lie in its insightful analysis of gradient misalignment, which adds a vital understanding of fairness in DP contexts. The introduction of DPSGD-Adapt represents a methodological advancement by employing adaptive clipping mechanisms, which have been empirically validated to enhance not only fairness but also model performance. However, the complexity introduced by adaptive thresholding could pose implementation challenges for practitioners. Additionally, the evaluation metrics primarily focus on accuracy and loss, and the absence of comprehensive fairness assessments limits the depth of the analysis regarding societal implications of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, providing a solid theoretical background alongside empirical results. The methodology is novel, particularly the adaptive approach to gradient clipping in DPSGD, which sets it apart from existing techniques. However, while the theoretical and empirical validation is robust, the reproducibility may be impacted by the complexity of the adaptive mechanisms, which may require careful tuning and implementation.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the intersection of differential privacy and fairness in machine learning. By addressing the critical issue of gradient misalignment and proposing a novel adaptive methodology, the authors provide a meaningful pathway to reducing disparate impact while ensuring privacy. Future work could further explore practical implications and broader assessments of fairness metrics to enhance the understanding of societal impacts.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"A Novel Approach to Adversarial Training through Gradient Alignment\" presents a new methodology aimed at enhancing the robustness of deep learning models against adversarial attacks by addressing the problem of gradient misalignment. The authors introduce a technique called **Adversarial Gradient Alignment (AGA)**, which modifies traditional adversarial training by ensuring that gradients from both clean and adversarial examples are aligned during the training process. The theoretical foundation of the approach is established, showing that aligned gradients can lead to improved model robustness. Extensive empirical validation on benchmark datasets, including MNIST and CIFAR-10, demonstrates that AGA outperforms existing adversarial training methods in both accuracy and resistance to adversarial perturbations.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel insight into the significance of gradient alignment in adversarial training, providing a fresh perspective that could enhance defenses against adversarial attacks. The theoretical foundation is robust, supporting the claims regarding the benefits of gradient alignment. Additionally, the empirical results are compelling, demonstrating the superiority of the AGA method over traditional techniques. However, weaknesses include the potential complexity of implementing gradient alignment in practical applications, particularly in large-scale settings. Furthermore, the limited variety of datasets and attack scenarios used in experiments may restrict the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its methodology and findings. The quality of the theoretical analysis and empirical validation is high, providing a solid basis for the claims made. The novelty of the approach is significant, as it shifts the focus from traditional metrics of adversarial robustness to the alignment of gradients. However, reproducibility may be affected by the added complexity of the method, which could pose challenges for practitioners looking to implement AGA in diverse settings.\n\n# Summary Of The Review\nOverall, this paper provides a meaningful contribution to the field of adversarial training by introducing the concept of gradient alignment, supported by strong theoretical and empirical evidence. Despite some challenges related to implementation complexity and limited experimental scope, the proposed method shows promise in enhancing model robustness against adversarial attacks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" explores the intersection of differential privacy (DP) and fairness in machine learning. The authors claim to identify gradient misalignment as the primary cause of disparate impact in differentially private stochastic gradient descent (DPSGD) and propose a novel variant of DPSGD that aims to eliminate this issue. Their empirical findings suggest that this new method not only reduces disparate impact but also enhances model utility across various datasets, including MNIST, Adult, and CelebA, while allegedly not requiring access to protected group labels.\n\n# Strength And Weaknesses\nThe strengths of the paper include its ambitious approach to addressing fairness in the context of differential privacy, as well as the introduction of a new method that aligns gradients to mitigate unfairness. The empirical validation across multiple datasets provides a foundation for the proposed claims. However, the paper also has significant weaknesses. The assertion that gradient misalignment is the sole cause of disparate impact is an overstatement, neglecting other contributing factors. Additionally, the claims of comprehensive improvement over existing methods and the establishment of a new standard for fairness are exaggerated, as the results are often marginal and context-dependent.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible to readers. However, the clarity is sometimes undermined by the inflated claims regarding the contributions and effectiveness of the proposed method. While the methodology is novel in its approach to aligning gradients, the reproducibility of results may be called into question due to the emphasis on context-dependent improvements and the lack of thorough comparison with all existing methods.\n\n# Summary Of The Review\nOverall, the paper presents interesting contributions to the conversation around fairness in machine learning and differential privacy. However, the significance of these contributions is overstated, and the claims made regarding the effectiveness and generalizability of the proposed methods lack sufficient support.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" investigates the relationship between differential privacy (DP) and fairness in machine learning. It highlights that the use of differentially private stochastic gradient descent (DPSGD) can inadvertently intensify disparate impacts on underrepresented groups due to gradient misalignment stemming from inequitable gradient clipping. The authors propose a novel approach called DPSGD-Global-Adapt, which demonstrates improved fairness and accuracy across various datasets, including intentionally unbalanced MNIST and census datasets such as Adult and Dutch. Key findings indicate a significant reduction in disparate impacts while maintaining privacy guarantees, thus emphasizing the importance of aligning gradients in privacy-preserving models.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough investigation of the interplay between differential privacy and fairness, a critical issue in machine learning. The methodology is robust, utilizing multiple datasets with careful selection of protected group attributes, which enhances the validity of the findings. The introduction of DPSGD-Global-Adapt is a notable contribution that addresses the identified issues effectively. However, the paper could benefit from a deeper exploration of the theoretical implications of gradient misalignment and its broader applicability beyond the tested datasets. Additionally, while the experimental results are promising, further validation across diverse contexts would strengthen the conclusions drawn.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its objectives and findings, making it accessible to readers. The quality of the research is high, with comprehensive experimental designs and results that are easy to follow. The novelty of the proposed DPSGD-Global-Adapt method is noteworthy, and it is presented in a manner that allows for reproducibility, assuming access to the datasets and implementations used. However, the paper could improve its clarity regarding the theoretical foundations of its claims about gradient misalignment.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the intersection of differential privacy and fairness in machine learning by identifying gradient misalignment as a source of disparate impacts and proposing a novel method to mitigate this issue. The results are promising, and the methodology is sound, though further exploration of the theoretical aspects and broader applicability would enhance the work.\n\n# Correctness\nRating: 4/5  \nThe results appear to be accurate based on the presented data, but further validation is necessary for a comprehensive assessment.\n\n# Technical Novelty And Significance\nRating: 4/5  \nThe introduction of DPSGD-Global-Adapt is a significant advancement in addressing fairness within differential privacy, showcasing a novel approach to an existing problem.\n\n# Empirical Novelty And Significance\nRating: 4/5  \nThe empirical findings demonstrate a notable improvement in fairness metrics across various datasets, contributing valuable insights to the field, although additional datasets could enhance generalizability.",
    "# Summary Of The Paper\nThe paper addresses the challenge of fairness in differentially private stochastic gradient descent (DPSGD) by proposing that gradient misalignment, specifically due to inequitable gradient clipping, is a primary contributor to unfairness in these algorithms. The authors suggest a global scaling method to mitigate disparate impacts and argue that this approach can enhance fairness without compromising privacy. The methodology includes experiments on datasets such as MNIST, Adult, Dutch, and CelebA, demonstrating the proposed techniques’ effectiveness. However, the findings raise questions about the generalizability of the results across varied datasets and model configurations.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its identification of a significant issue within differential privacy related to fairness and its proposal of a novel global scaling method as a potential solution. However, the paper contains several weaknesses, including assumptions that may not universally apply, such as the reliance on specific loss functions and the implications of clipping strategies. Furthermore, the limited scope of the experiments calls into question the generalizability of the findings. The paper also overlooks broader definitions of fairness, which may limit its applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and articulates its contributions clearly. However, the novelty of the proposed solutions is somewhat undermined by the reliance on assumptions that may not hold true in various contexts. The reproducibility of the findings could be improved by providing more comprehensive details on the experimental setup and considering a wider variety of datasets. Overall, while the paper presents a relevant topic, the limitations in methodology and assumptions affect the clarity of its contributions.\n\n# Summary Of The Review\nWhile the paper highlights critical aspects of fairness in DPSGD and proposes interesting solutions, it suffers from several foundational assumptions that need further validation and scrutiny. The limited experimental scope and a narrow definition of fairness could hinder broader applicability and impact. Overall, the paper raises important questions but requires more rigorous examination of its claims.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the intersection of differential privacy and fairness in machine learning, focusing on the effects of differentially private stochastic gradient descent (DPSGD) on protected groups. The authors identify gradient misalignment, exacerbated by inequitable gradient clipping, as a primary cause of disparate impact during model training. They propose two novel methods, DPSGD-Global and DPSGD-Global-Adapt, which aim to mitigate unfairness by aligning gradients across groups. Empirical results demonstrate that these methods significantly reduce disparities in outcomes while maintaining model accuracy.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its clear identification of the problem of gradient misalignment and its innovative approach to addressing fairness in the context of differential privacy. The proposed methods are well-motivated and supported by a solid experimental framework that showcases improvements in fairness metrics. However, the paper could benefit from a more thorough discussion of potential limitations and trade-offs associated with the proposed methods, particularly regarding the privacy-utility balance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and generally clear, with a logical flow that guides the reader through the background, methodology, and results. The quality of writing is high, making complex concepts accessible. The novelty is notable, as it addresses a critical gap at the intersection of privacy and fairness, though the empirical contributions could be enhanced by including more diverse datasets and scenarios to bolster reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in understanding and mitigating fairness issues in machine learning under differential privacy. The proposed methods effectively address gradient misalignment, providing a valuable contribution to the field. However, further exploration of limitations and robustness across various settings would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper addresses the intersection of fairness, privacy, and model performance in machine learning, proposing a novel framework that seeks to enhance outcomes in practical applications. The authors introduce a method that incorporates fairness constraints into model training while preserving privacy and performance metrics. Their empirical findings indicate that the proposed approach significantly outperforms existing methods in various benchmark datasets, demonstrating improved fairness without compromising accuracy.\n\n# Strengths And Weaknesses\n**Strengths**:\n- **Relevance**: The topic is timely, addressing growing concerns over ethical considerations in AI, particularly regarding fairness and privacy.\n- **Innovation**: The framework offers new insights into integrating fairness into the model training process, which is essential for responsible AI development.\n- **Empirical Validation**: The extensive experiments conducted illustrate the effectiveness of the proposed method across multiple datasets, lending credibility to the findings.\n\n**Weaknesses**:\n- **Clarity**: Some methodological details are inadequately explained, which may hinder reproducibility. For instance, the exact implementation steps for the fairness constraints could be elaborated.\n- **Scope of Experiments**: While the experiments are robust, testing on a wider array of datasets or real-world applications would enhance the generalizability of the findings.\n- **Discussion of Limitations**: The paper lacks a thorough discussion on potential limitations and biases that may still exist within the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper has a solid overall structure, but certain sections suffer from a lack of clarity, particularly in the methodology. The novelty of the proposed approach is significant, as it combines fairness and privacy in a unique way. However, the reproducibility of the results could be improved by providing more detailed implementation guidelines and clearer descriptions of the experimental setup.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the fields of machine learning and AI ethics by proposing a framework that effectively incorporates fairness and privacy considerations. With revisions to enhance clarity and expand the evaluation scope, the work has the potential to make a substantial impact in addressing ethical challenges in AI.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" addresses the critical intersection of data privacy and fairness in machine learning, particularly in sensitive domains such as healthcare and finance. The authors identify gradient misalignment due to inequitable gradient clipping as a primary cause of disparate impact in the context of differentially private stochastic gradient descent (DPSGD). They propose a novel method called global scaling to align gradients, which effectively reduces unfairness in model outcomes without requiring protected group labels. The findings suggest that while this method enhances both fairness and utility, it does not eliminate the inherent privacy-utility trade-off or guarantee non-discriminatory outcomes.\n\n# Strength And Weaknesses\nThe paper makes a significant contribution by explicitly linking gradient misalignment to fairness issues in DPSGD, a topic that has not been extensively explored in existing literature. The proposed global scaling method is a practical innovation that addresses these concerns without necessitating additional data, which is a clear strength. However, the paper does not provide extensive empirical validation of the proposed method across diverse datasets or scenarios, which could limit the generalizability of the findings. Additionally, while the authors acknowledge the trade-offs involved, a more detailed discussion on the implications of these trade-offs would enhance the paper's overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its arguments, methodology, and findings. The writing quality is high, with a logical flow that guides the reader through the problem statement, contributions, and results. The novelty of the proposed global scaling method is commendable, as it introduces a new approach to addressing a well-known issue in the field. However, the reproducibility of the results could be improved by providing more details on experimental setups and datasets used in the study.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the discussion of fairness in differentially private machine learning, particularly through the lens of gradient misalignment. The proposed global scaling method offers a novel solution that enhances fairness without compromising privacy. Despite some limitations in empirical validation, the work is notable for its clarity and practical implications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper \"Disparate Impact in Differential Privacy from Gradient Misalignment\" by Maria S. Esipova et al. investigates the intersection of data privacy and fairness within machine learning, particularly focusing on the shortcomings of Differentially Private Stochastic Gradient Descent (DPSGD). The authors argue that gradient misalignment, stemming from inequitable gradient clipping, significantly contributes to unfairness in model outcomes. To address this issue, they propose a novel method called DPSGD-Global, which aims to align gradients and mitigate disparate impacts while maintaining model utility. The experimental results demonstrate that this new method effectively reduces disparities across various datasets, including MNIST, Adult, and CelebA.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its identification of gradient misalignment as a crucial factor contributing to inequities in differential privacy applications. The theoretical propositions presented provide a solid foundation for understanding the effects of clipping errors on model performance. Furthermore, the empirical evaluation showcases the proposed DPSGD-Global method's effectiveness in enhancing fairness without significantly compromising utility. However, a notable weakness is the acknowledgment that the privacy-utility trade-off remains, suggesting that the proposed solutions may not fully resolve the inherent challenges of balancing privacy and fairness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem of disparate impacts in the context of differential privacy. The methodology is adequately described, allowing for reproducibility of the proposed experiments. The novelty of the approach is significant, as it provides a new perspective on addressing fairness in privacy-preserving machine learning. However, the paper could benefit from a more detailed discussion on the limitations of the proposed method and potential avenues for future work.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the fields of privacy and fairness in machine learning by identifying a critical issue related to gradient misalignment in DPSGD and proposing an effective solution to mitigate it. While the findings are promising, the paper recognizes the ongoing challenges associated with the privacy-utility trade-off, indicating that further exploration is necessary for comprehensive solutions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing the challenges of privacy and fairness in machine learning, particularly in regulated industries. The authors propose a new algorithm, DPSGD-Global, which integrates differential privacy mechanisms with fairness constraints to mitigate disparate impact in decision-making processes. Through comprehensive experiments and statistical analyses, the findings demonstrate that the proposed method effectively balances privacy and fairness, outperforming existing approaches in various scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive coverage of relevant literature and the introduction of a novel methodology that is both innovative and practical. The clear presentation of results, supported by appropriate statistical tests, enhances the credibility of the findings. However, one weakness is the limited discussion on potential biases that may arise from the assumptions made during the methodology design. Additionally, while the future work section is insightful, it could benefit from more specific recommendations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with varying expertise in the field. The methodology is detailed enough to allow for reproducibility, which is a significant asset. The novelty of the proposed DPSGD-Global algorithm adds substantial value to the existing body of work on privacy and fairness in machine learning, marking a meaningful contribution to the field.\n\n# Summary Of The Review\nOverall, the paper provides a significant contribution to the areas of privacy and fairness in machine learning through a well-defined and innovative approach. The clarity of the writing and the robustness of the results support its acceptance, although addressing certain limitations could further strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" investigates the unintended amplification of disparate impacts in machine learning models utilizing Differentially Private Stochastic Gradient Descent (DPSGD). The authors attribute these inequities to gradient misalignment caused by inequitable gradient clipping methodologies. They propose a novel algorithmic approach, specifically DPSGD-Global and its refined version DPSGD-Global-Adapt, which aims to align gradients during the DPSGD process, thereby mitigating disparate impacts. Empirical results on datasets like MNIST, Adult, and CelebA reveal that the proposed methods significantly improve fairness metrics over baseline models while addressing the privacy-utility tradeoff.\n\n# Strength And Weaknesses\nThe paper's main strengths lie in its clear identification of the problem of gradient misalignment in the context of DPSGD and its innovative algorithmic solutions. The proposed methods are well-justified and demonstrate substantial improvements in fairness metrics, making a significant contribution to the intersection of differential privacy and fairness in machine learning. However, a potential weakness is the reliance on specific datasets, which may limit the generalizability of the findings. Additionally, while the paper discusses the privacy-utility tradeoff, further exploration of the implications of this tradeoff in practical scenarios could strengthen the analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and findings. The theoretical underpinnings are solid, and the empirical results are presented in a straightforward manner. The novelty of the proposed methods is clear, as they address an underexplored aspect of DPSGD related to gradient clipping. Reproducibility is supported through detailed descriptions of the experimental settings and methodologies used, although providing access to code or supplementary materials could enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper provides a meaningful contribution to the field by addressing the critical issue of disparate impact in differentially private models through innovative gradient alignment techniques. The findings are compelling and underscore the importance of considering fairness alongside privacy in machine learning. The paper is clear and reproducible, though it could benefit from broader applicability discussions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the issue of disparate impact in differential privacy, particularly focusing on the Differentially Private Stochastic Gradient Descent (DPSGD) method. The authors claim to identify \"gradient misalignment\" as a key contributor to unfair outcomes in DPSGD and propose two variations, DPSGD-Global and DPSGD-Global-Adapt, as potential solutions. However, the paper's exploration of these methods is limited, relying on a narrow range of datasets and lacking a comprehensive discussion on the broader implications of their findings in real-world applications.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its attempt to tackle a significant issue within the realm of differential privacy and fairness, specifically highlighting the concept of gradient misalignment. However, the weaknesses are pronounced; the empirical evidence supporting the claims is limited, and the proposed adaptations appear to provide minimal theoretical advancements. Furthermore, the analysis of fairness metrics is overly simplistic and fails to account for the complexities inherent in fairness in machine learning. The paper does not adequately engage with existing literature, diminishing its contribution to the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is somewhat compromised by its superficial discussions, particularly regarding related work, which lack depth and critical engagement. The quality of the methodology raises questions about reproducibility due to the reliance on limited datasets and the assumption of twice-differentiable loss functions, which may not reflect real-world scenarios. While the attempts to innovate within the space of differential privacy are noted, the novelty of the proposed solutions is questionable, as they do not significantly advance the state of the art.\n\n# Summary Of The Review\nOverall, this paper addresses an important problem in the intersection of differential privacy and fairness but does so with limited empirical support and theoretical depth. The proposed methods do not convincingly challenge existing techniques or offer substantial improvements, and the lack of engagement with broader implications undermines its contributions.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents groundbreaking research at the intersection of differential privacy (DP) and fairness in machine learning, specifically focusing on the Differentially Private Stochastic Gradient Descent (DPSGD) method. The authors identify gradient misalignment as a key factor contributing to unfairness in DPSGD and propose innovative solutions, namely DPSGD-Global and its adaptation, DPSGD-Global-Adapt. These methods significantly enhance model fairness without relying on protected group labels, achieving a dual benefit of reducing disparate impact while improving overall model utility. Empirical evaluations demonstrate statistically significant improvements in accuracy, loss, privacy cost, and excessive risk metrics across various datasets.\n\n# Strength And Weaknesses\nThe paper's strength lies in its identification of gradient misalignment as a critical issue in differential privacy that exacerbates disparate impact, providing a fresh perspective in fairness-aware machine learning. The proposed methodologies, DPSGD-Global and DPSGD-Global-Adapt, are innovative and practical, addressing a pressing need in the field without compromising privacy. However, while the empirical validation shows promising results, further exploration of potential limitations and edge cases of the proposed methods would strengthen the findings. Additionally, a broader range of datasets and scenarios could further validate the robustness of the solutions presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The methodology is described in sufficient detail to allow for reproduction, although practical implementation considerations could be further elaborated. The novelty of addressing gradient misalignment within the context of differential privacy and fairness is significant, marking a notable advancement in the field. Overall, the quality of writing and presentation is high, contributing positively to the paper's clarity.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the fields of differential privacy and fairness in machine learning by identifying gradient misalignment as a critical issue and proposing innovative solutions. The empirical results demonstrate the effectiveness of these methods, although further validation across diverse datasets would enhance the findings. Overall, this research offers valuable insights and methodologies that could have a transformative impact on ethical AI deployment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the interaction between differential privacy (DP) and fairness in machine learning, specifically focusing on how disparate impact arises during differentially private stochastic gradient descent (DPSGD) due to gradient misalignment. The authors propose a theoretical framework, termed DPSGD-Global, which adapts the gradient clipping mechanism to mitigate the identified misalignment. Key findings indicate that direction errors, rather than magnitude errors, are the primary source of disparate impact during training, providing new insights into balancing privacy and fairness without requiring sensitive group labels.\n\n# Strength And Weaknesses\nThe paper makes several significant contributions, including a novel theoretical analysis of gradient misalignment and its implications for disparate impact in DPSGD. The mathematical analysis, particularly the excessive risk decomposition and the proposed propositions, offers a rigorous foundation for understanding the sources of unfairness. However, the paper could benefit from empirical validation of the proposed methods, as the current focus is heavily theoretical, leaving potential practical applications untested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections that guide the reader through complex theoretical concepts. The quality of the writing is high, and the mathematical notations are appropriately used. The novelty is pronounced, as the paper addresses a less-explored area in the intersection of DP and fairness. However, reproducibility may be a concern due to the lack of empirical results to support the theoretical claims, which could hinder practical applications of the proposed methods.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical framework to understand and mitigate disparate impact in DPSGD, emphasizing the significance of gradient misalignment. While the theoretical contributions are robust and innovative, the lack of empirical validation raises questions about the practical applicability of the proposed solutions.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 3",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing gradient misalignment in differentially private stochastic gradient descent (DPSGD) through the proposed algorithm DPSGD-Global-Adapt. By introducing global scaling of gradients and adaptive clipping bounds, the authors aim to improve fairness in machine learning models while maintaining differential privacy. Experimental evaluations on various datasets, including MNIST and CelebA, demonstrate that the proposed method enhances performance metrics such as accuracy and loss, particularly in the context of fairness.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to tackle gradient misalignment, which is a critical issue in ensuring fairness in machine learning. The authors successfully demonstrate the efficacy of DPSGD-Global-Adapt through extensive experiments and provide a well-structured methodology. However, a notable weakness is the paper's heavy emphasis on technical implementation and empirical results, which may overshadow broader implications for fairness and ethical considerations. Additionally, while the results indicate improvements, the discussion of the implications of these findings is somewhat superficial.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it easy to follow the methodology and results. The quality of the experiments is high, with clear reporting of performance metrics and robust averaging across multiple runs. The novelty of the proposed algorithm is significant, as it directly addresses a known challenge in the field. The authors provide sufficient details and code availability for reproducibility, which is commendable.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of differentially private machine learning by addressing gradient misalignment in DPSGD. While the proposed method shows promise in improving fairness and maintaining privacy, the discussion around ethical implications could be expanded to offer a more comprehensive understanding of its societal impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a new method termed DPSGD-Global-Adapt, aimed at addressing disparate impact in differential privacy stochastic gradient descent (DPSGD). The authors argue that gradient misalignment is a significant contributor to the disparities observed in DPSGD, and they claim that their approach effectively reduces this impact without requiring protected group labels. The findings suggest improvements in accuracy and loss metrics compared to previous methods, but the paper lacks comprehensive comparisons and context against established baselines.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle an important issue in fairness within machine learning. However, it falls short in several areas. The authors overstate the novelty of their contributions, as related concepts have been previously explored without addressing fairness concerns. The reliance on selective citations undermines the robustness of their arguments, and the lack of thorough experimental comparisons raises concerns about the validity of their claims. Moreover, the paper generalizes findings from limited datasets, which may not adequately reflect real-world complexities.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by the selective presentation of prior works and the lack of detailed comparisons with established methods. While the proposed method is presented as novel, it is not sufficiently differentiated from existing approaches such as those by Xu et al. (2021) and Bu et al. (2021). Reproducibility is a concern due to insufficient details regarding experimental setups and performance metrics, making it difficult for other researchers to validate the findings.\n\n# Summary Of The Review\nOverall, the paper attempts to address a critical issue in fairness and differential privacy but does so with insufficient novelty and rigorous validation. The authors' claims are not adequately supported by their experimental results, and the selective treatment of prior literature weakens the paper's contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"DISPARATE IMPACT IN DIFFERENTIAL PRIVACY FROM GRADIENT MISALIGNMENT\" investigates the unintended biases that can arise in differentially private machine learning models, specifically those utilizing Differentially Private Stochastic Gradient Descent (DPSGD). The authors propose that gradient misalignment due to inequitable gradient clipping contributes to disparate impact on certain demographic groups. Their methodology includes theoretical analysis of the disparity caused by DPSGD, followed by empirical experiments to validate their claims. The findings indicate that the proposed modifications to DPSGD can mitigate these disparities and improve its utility.\n\n# Strength And Weaknesses\nThe paper provides a significant contribution by highlighting the relationship between gradient clipping and disparate impact, an area that has not been extensively covered in existing literature. The theoretical framework is sound, and the empirical results support the claims made. However, the paper could benefit from clearer explanations in certain areas, such as the definition of \"greedy nature\" in loss minimization and the specifics of disparate treatment. Additionally, some formatting inconsistencies and minor grammatical errors detract from the overall presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat diminished by inconsistent terminology and formatting issues, such as the definition of abbreviations and the clarity of equations. While the overall quality of the analysis is solid, the presentation could be improved for better readability. The novelty of the work lies in its focus on gradient misalignment within DPSGD, which is a significant area of research. The reproducibility of results is supported by the empirical experiments; however, clearer references to figures and tables and a more consistent format would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field by addressing the relationship between gradient clipping and disparate impact in differentially private models. Despite some clarity and formatting issues, the theoretical insights and empirical results are compelling and provide a basis for further exploration in this area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates gradient misalignment in Differentially Private Stochastic Gradient Descent (DPSGD) and proposes a novel method named DPSGD-Global-Adapt to mitigate this issue. The authors empirically validate their approach, demonstrating improvements in fairness metrics without requiring protected group labels. However, the examination of broader implications and robustness testing is limited, leading to questions about the generalizability and applicability of the proposed solution.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical validation of DPSGD-Global-Adapt, which addresses gradient misalignment and its impact on fairness. However, the paper has several weaknesses, including a lack of exploration of related privacy-enhancing technologies, insufficient robustness testing, and limited dataset diversity. Furthermore, the theoretical underpinnings of the proposed methods are inadequately addressed, and the potential risks of omitting protected group labels are not thoroughly discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its methodology and findings clearly, although it could benefit from a more comprehensive theoretical treatment. The novelty of addressing gradient misalignment in the context of DPSGD is noteworthy, yet the lack of robustness and sensitivity analysis raises concerns about reproducibility. The empirical results are promising, but the narrow focus on specific datasets limits the generalizability of the findings.\n\n# Summary Of The Review\nOverall, the paper provides an interesting perspective on gradient misalignment in DPSGD and introduces a novel method to address fairness. However, it falls short in exploring broader implications, robustness, and a critical self-assessment of its limitations, which undermines the overall contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper explores the intersection of differential privacy (DP) and fairness in machine learning, specifically how differential privacy can lead to disparate impacts on underrepresented groups when employing differentially private stochastic gradient descent (DPSGD). The main contributions include identifying gradient misalignment as a critical factor contributing to bias and proposing methods to align gradients to mitigate these disparities. The authors conducted experiments across various datasets (MNIST, Adult, Dutch, and CelebA) to validate their claims, demonstrating statistically significant improvements in both accuracy and fairness metrics by employing gradient alignment techniques.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its thorough examination of the relationship between differential privacy and fairness, particularly the clear identification of gradient misalignment as a major issue. The experimental design is robust, utilizing diverse datasets and comprehensive statistical analysis to support the findings. However, a potential weakness is the focus on specific metrics for fairness, such as accuracy parity and excessive risk, which may not capture all dimensions of fairness. Additionally, while the paper suggests future work could extend the exploration of fairness metrics, it does not provide concrete plans for addressing these dimensions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, presenting complex ideas in a clear and accessible manner. The methodology is rigorous, employing appropriate statistical tests (one-sided Wilcoxon signed rank tests) to ascertain significance, which enhances the quality of the findings. The reproducibility of the experiments is supported by the detailed reporting of metrics and methodologies, although additional information on hyperparameter settings and implementation details could further enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to understanding the relationship between differential privacy and fairness in machine learning, effectively identifying gradient misalignment as a critical factor and proposing viable solutions. The rigorous experimental methodology and statistical analysis lend credibility to the findings, though the scope of fairness metrics could be expanded in future work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the intersection of differential privacy and fairness in machine learning, particularly focusing on the issues of gradient misalignment and disparate impact across different demographic groups. The authors propose a novel method aimed at mitigating the impact of these issues while retaining privacy guarantees. Key findings suggest that their approach can improve fairness metrics on standard datasets, such as MNIST, Adult, and CelebA, although the method's applicability to diverse datasets remains untested.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to bridge the gap between differential privacy and fairness, addressing a pertinent issue in contemporary machine learning. However, several weaknesses undermine the overall contribution. The paper does not sufficiently explore the broader implications of deploying differentially private models in real-world contexts or the critical privacy-utility trade-off. Moreover, it lacks a rigorous evaluation of the computational overhead and the sensitivity of the proposed methods to hyperparameter tuning. The reliance on specific datasets raises concerns about generalizability, and there is no discussion of the ethical implications of using protected attributes in training.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly, making it accessible to readers with a background in machine learning. However, the novelty of the proposed methods is somewhat limited due to the lack of comprehensive evaluation and discussions on potential biases in data collection. Reproducibility could be enhanced by providing more details on the experimental setup and a wider range of datasets used for validation.\n\n# Summary Of The Review\nWhile the paper presents a relevant approach to addressing fairness and privacy in machine learning, it suffers from significant limitations in terms of generalizability, computational considerations, and the exploration of ethical implications. The findings are interesting, but the lack of robust evaluation and broader discussions diminishes their impact.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"Disparate Impact in Differential Privacy from Gradient Misalignment\" investigates the intersection of differential privacy and fairness in machine learning models. The authors claim to demonstrate that the application of differential privacy, specifically through the Differentially Private Stochastic Gradient Descent (DPSGD) framework, can exacerbate fairness issues in models, leading to disparate impacts on certain demographic groups. They identify gradient misalignment as a key factor contributing to this problem and propose a new technique for aligning gradients to mitigate these issues. Empirical results indicate that their method improves model utility while reducing disparate impact, although these findings are presented in a context that feels somewhat familiar.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its empirical investigation of the relationship between differential privacy and fairness, which is a relevant and important topic in the current machine learning landscape. The authors successfully demonstrate that gradient misalignment can lead to increased unfairness when implementing differential privacy, which adds value to ongoing discussions about ethical AI. However, the paper's weaknesses include a lack of novel insights, as the issues of privacy and fairness have been well-documented in the literature. The methodology appears conventional, and the proposed solutions do not seem to break new ground, leading to an overall sense that the contributions are incremental rather than groundbreaking.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that facilitates understanding. However, the novelty of the findings is questionable, as the issues addressed are not particularly innovative. The methodology is standard for this line of research, which raises concerns about the reproducibility of their results. While the authors provide sufficient detail for others to replicate their experiments, the underlying ideas do not present a significant advancement in the field, limiting the perceived impact of their work.\n\n# Summary Of The Review\nOverall, this paper contributes to the discussion on differential privacy and fairness but does so in a manner that feels derivative rather than pioneering. While it successfully highlights important issues and provides empirical results, the lack of novel insights and the conventional nature of the methodology detract from its overall significance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the issue of gradient misalignment as a significant factor contributing to disparate impacts in differentially private stochastic gradient descent (DPSGD). The authors propose two new methods, DPSGD-Global and DPSGD-Global-Adapt, aimed at reducing unfairness in machine learning models while maintaining privacy. They conduct experiments using the MNIST and CelebA datasets, demonstrating that their approaches yield improved fairness metrics without requiring protected group labels, which is vital for real-world applications.\n\n# Strength And Weaknesses\nThe paper makes meaningful contributions by identifying gradient misalignment as a source of unfairness in DPSGD, which aligns with ongoing research in fairness-aware machine learning. The proposed methods show promise, but there is room for further exploration, especially regarding adaptive learning rate strategies. The authors' emphasis on the privacy-utility trade-off is commendable; however, extending the experimental evaluation to include more complex real-world datasets could strengthen the findings. While the results show improved fairness metrics, the analysis could benefit from incorporating intersectional fairness metrics that consider multiple protected attributes.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its methodology, making it accessible to the reader. The quality of the experiments is generally high, although the inclusion of additional datasets could enhance reproducibility and validation of the proposed methods. The novelty of addressing gradient alignment in the context of DPSGD is noteworthy, yet the authors should consider exploring adaptive techniques further. The reproducibility of results may also benefit from standardized fairness benchmarks.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in addressing fairness in differentially private stochastic gradient descent through the identification of gradient misalignment. The proposed methods are promising, but further exploration into adaptive learning rates and intersectional fairness metrics is recommended to enhance the robustness and applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces two novel methods, DPSGD-Global and DPSGD-Global-Adapt, aimed at improving both model performance and fairness in machine learning under differential privacy constraints. The methodology involves aligning gradients to enhance accuracy while reducing disparities across different demographic groups. Empirical results demonstrate significant improvements in accuracy and loss across multiple datasets, including MNIST, CelebA, Adult, and Dutch, with DPSGD-Global-Adapt achieving notable enhancements in class-specific performance and fairness metrics.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its comprehensive benchmark performance across various datasets, which clearly illustrates the effectiveness of the proposed methods in improving both accuracy and fairness. The statistical significance of the results adds robustness to the claims made. However, a potential weakness is the lack of extensive discussion on the computational overhead associated with the proposed methods, which could be critical for real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly conveys its contributions and findings. The methodology is described in sufficient detail, allowing for reproducibility of the results. The novelty of combining differential privacy with fairness considerations in a systematic way is evident, though further exploration of the implications of implementation in practical settings could enhance the clarity of its impact.\n\n# Summary Of The Review\nOverall, the paper presents significant advancements in machine learning by addressing critical issues related to accuracy and fairness under differential privacy. The empirical results are compelling, showcasing the potential of the proposed methods to enhance model utility while mitigating disparate impacts across demographic groups.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing the interplay between privacy and fairness in machine learning algorithms. The authors propose a new algorithm that incorporates differential privacy to protect sensitive data while ensuring fairness across different demographic groups. Through extensive experiments on benchmark datasets, the methodology demonstrates improved performance in balancing privacy guarantees with equitable outcomes. The findings indicate that the proposed approach significantly enhances fairness metrics without compromising the privacy of individual data entries.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of differential privacy with fairness considerations, which is a timely and important issue in the field of machine learning. The methodology is clearly articulated and supported by robust empirical evidence, showcasing its effectiveness across diverse scenarios. However, the paper's weaknesses include a somewhat dense abstract that could overwhelm readers, as well as some technical jargon that may alienate those unfamiliar with the specific terms used. Additionally, while the experimental results are compelling, the paper could benefit from a more thorough discussion of the limitations of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from clarity issues, particularly in the abstract and some complex sentence structures, which detract from its overall accessibility. Consistent formatting and clearer graphical elements are needed to enhance comprehension. Nevertheless, the novelty of the proposed approach is significant, particularly in its application of differential privacy to fairness, marking a meaningful contribution to the field. The methodology appears reproducible, although additional details regarding implementation and parameter tuning would facilitate replication efforts.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution by effectively merging privacy and fairness in machine learning, albeit with clarity and presentation issues that could hinder reader engagement. Addressing these shortcomings would enhance the paper's impact and accessibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3487157956602895,
    -1.6305224104781761,
    -1.592465085064885,
    -1.614014553092474,
    -1.7519835798467371,
    -1.717371955054835,
    -1.3740097954255752,
    -1.7345531796473717,
    -1.5451018680473272,
    -1.8808390697034798,
    -1.5443411919543026,
    -1.5378181516059926,
    -1.5798607776294145,
    -1.540756741071451,
    -1.623657942459327,
    -1.5477603826229056,
    -1.8164416337708496,
    -1.7149690732232727,
    -1.6367520715538044,
    -1.5746638508637467,
    -1.909617988284562,
    -1.730723893536353,
    -1.6725650794348021,
    -1.6790068374263953,
    -1.6870385839189954,
    -1.535107176746925,
    -1.7845598241248912,
    -1.8025424211588068,
    -1.6676798801227815
  ],
  "logp_cond": [
    [
      0.0,
      -2.167203418501409,
      -2.1813935942433607,
      -2.171343917356472,
      -2.1623563756856417,
      -2.1862620333065244,
      -2.2496920150076405,
      -2.18622387002927,
      -2.160806630468233,
      -2.1927635532976737,
      -2.1816291370154275,
      -2.253473287728187,
      -2.1761446909353444,
      -2.1768348133687625,
      -2.2289684369527056,
      -2.162095521639623,
      -2.1669809515220715,
      -2.1397792650409087,
      -2.1637636592717526,
      -2.185256108702632,
      -2.164593557897212,
      -2.1836201545656273,
      -2.1686064329323727,
      -2.179511738831133,
      -2.212033949135082,
      -2.1964315246781263,
      -2.1634594337503916,
      -2.197687312357025,
      -2.2554656329211937
    ],
    [
      -1.2798379618918307,
      0.0,
      -1.102528077246595,
      -1.1477419683259205,
      -1.1191475323445421,
      -1.1605803280968179,
      -1.3875326222152486,
      -1.2067393570921292,
      -1.1591021760572842,
      -1.2085856792974414,
      -1.1425318172713006,
      -1.3475326324473458,
      -1.0883437830915452,
      -1.0818140422005624,
      -1.2628534445586164,
      -1.0866711514195881,
      -1.1998280433783228,
      -1.1161392434724529,
      -1.1875329053878778,
      -1.234064652811735,
      -1.1840122640608663,
      -1.1518267710740868,
      -1.2347814213363586,
      -1.1667457937517285,
      -1.2602951311327784,
      -1.2463112033352177,
      -1.1500580160695975,
      -1.2080230673801933,
      -1.3167416489387955
    ],
    [
      -1.28291611073552,
      -1.117067760005536,
      0.0,
      -1.1601445656896352,
      -1.169454737296094,
      -1.1996787251141265,
      -1.3455754008935885,
      -1.2109349478418046,
      -1.1926676621172239,
      -1.2132981003151628,
      -1.1696192669423953,
      -1.3002111582968126,
      -1.158329465167539,
      -1.1000862227346204,
      -1.2775915985766246,
      -1.1459675500030522,
      -1.2117305464891055,
      -1.1750207312640495,
      -1.2451366257889713,
      -1.2089586489278579,
      -1.193575146462742,
      -1.1744953016195505,
      -1.217754677811377,
      -1.198345101530528,
      -1.2840081184866712,
      -1.2229006872507413,
      -1.1978302563704342,
      -1.1877686647227053,
      -1.3064766725785606
    ],
    [
      -1.3352694107953922,
      -1.251941424970024,
      -1.2324998688900803,
      0.0,
      -1.286543708565164,
      -1.3244233794492057,
      -1.4034382252726143,
      -1.288163483175882,
      -1.2718520898714485,
      -1.3569758930326932,
      -1.3033196190845577,
      -1.4067039490162188,
      -1.2515795743111777,
      -1.246848286434761,
      -1.3333564490005647,
      -1.2821308474501742,
      -1.324489968102228,
      -1.2983731747678722,
      -1.2824258267147495,
      -1.3240074311697114,
      -1.340358686144412,
      -1.3229564228510036,
      -1.326164828301906,
      -1.3199999283653647,
      -1.351105462926357,
      -1.2993617867804064,
      -1.33159056219463,
      -1.3446342767476211,
      -1.3767400482429097
    ],
    [
      -1.4297720693661253,
      -1.270714939952612,
      -1.2834249582803947,
      -1.3461104236841723,
      0.0,
      -1.3956266446468697,
      -1.4967145596773443,
      -1.3459312842898647,
      -1.3801376664724858,
      -1.3664271722836623,
      -1.3192928546416172,
      -1.4819934590911708,
      -1.310236590551437,
      -1.298791871239253,
      -1.4330016932422416,
      -1.3185319771051862,
      -1.405443684585295,
      -1.353134938096096,
      -1.3904914346297368,
      -1.4476250296881747,
      -1.3815157248168666,
      -1.3640155164681969,
      -1.3556440464381567,
      -1.4058564912075446,
      -1.4312038668943483,
      -1.4194909966911065,
      -1.3486531978274283,
      -1.431137573331208,
      -1.4821181594675943
    ],
    [
      -1.4699245463392898,
      -1.3285296992006157,
      -1.3045556231239712,
      -1.3508289728075396,
      -1.3756716548309262,
      0.0,
      -1.4222661266683354,
      -1.3489095444572552,
      -1.3726851679546805,
      -1.4764963195362841,
      -1.3836120661456082,
      -1.451388647543498,
      -1.3754711863434101,
      -1.349996579323741,
      -1.4624315237312624,
      -1.3663326623550716,
      -1.4144274391695377,
      -1.355289158488439,
      -1.3777236487105005,
      -1.4001138011710108,
      -1.4401830009871717,
      -1.4048306244915127,
      -1.4510602201419556,
      -1.4193667437956121,
      -1.448975773749173,
      -1.3645298154936212,
      -1.4354179674272018,
      -1.4319306322877745,
      -1.4656566598972038
    ],
    [
      -1.1565256313645773,
      -1.0713408139913823,
      -1.097146450047218,
      -1.105452260576484,
      -1.0822942395197772,
      -1.0414654586259822,
      0.0,
      -1.0471162053823497,
      -1.1041816438081324,
      -1.1142971247849853,
      -1.1270529413443051,
      -1.1050911772786285,
      -1.0807482910104256,
      -1.1096480390920205,
      -1.130008584852369,
      -1.0979857648186104,
      -1.062247729310773,
      -1.0986074002712993,
      -1.1200668743307152,
      -1.1229252878538276,
      -1.0940841585295726,
      -1.0847992008039375,
      -1.1006088375679897,
      -1.124650396223772,
      -1.0923002258148888,
      -1.067783393759931,
      -1.0920805368319844,
      -1.1377898165278932,
      -1.0692968749345857
    ],
    [
      -1.4229486845672084,
      -1.2793549389068026,
      -1.282905569919014,
      -1.3682351346857542,
      -1.3373423882646025,
      -1.3338174587269631,
      -1.4500107774426272,
      0.0,
      -1.3443926300992937,
      -1.3965668951768564,
      -1.3717182182404126,
      -1.4693711437178225,
      -1.2862481919796835,
      -1.285380984782393,
      -1.4351545604777816,
      -1.3432500500549773,
      -1.3599090303762376,
      -1.3369564060841403,
      -1.419337959412248,
      -1.4322792515973077,
      -1.394785422664118,
      -1.391406087042624,
      -1.3777231741132163,
      -1.36138376941696,
      -1.3855579869210628,
      -1.2947824201776674,
      -1.3720386843244297,
      -1.4048765212465566,
      -1.4418644706525252
    ],
    [
      -1.3008791273738405,
      -1.1784488697694244,
      -1.1846619577598496,
      -1.2310040169792693,
      -1.2218455539567676,
      -1.2600981747162172,
      -1.3416951512245163,
      -1.222376549252604,
      0.0,
      -1.2292199369179369,
      -1.214032778315419,
      -1.328233189048397,
      -1.1983102772129186,
      -1.2098996974179186,
      -1.3065500904970953,
      -1.1723858049075737,
      -1.2243213820312357,
      -1.242490715114488,
      -1.2745867503459505,
      -1.2608944512713727,
      -1.2240489756203463,
      -1.2652503775374324,
      -1.2378789613141956,
      -1.236180162553969,
      -1.2690407882491739,
      -1.2383119833885943,
      -1.2220975318146856,
      -1.2639315229474533,
      -1.3108090783390007
    ],
    [
      -1.5645542677106687,
      -1.4711477661623742,
      -1.4514939206430022,
      -1.5609856853494781,
      -1.4964413110329167,
      -1.584310483686033,
      -1.6399368352000434,
      -1.5435370987480654,
      -1.5175516844568935,
      0.0,
      -1.489603533842967,
      -1.646961465338552,
      -1.505105668906907,
      -1.4751365823397107,
      -1.58222091895648,
      -1.5146071346577887,
      -1.5017461562792505,
      -1.4689693041069836,
      -1.5463051910289776,
      -1.546433450946642,
      -1.539997051521596,
      -1.5000424977162305,
      -1.5632603027051941,
      -1.4793414062579084,
      -1.574654971735714,
      -1.5705240069452377,
      -1.5005327431990163,
      -1.5096022762079127,
      -1.6273260852294278
    ],
    [
      -1.174551203818503,
      -0.9823011631101375,
      -0.967932723265632,
      -1.0659625900492826,
      -1.03178341284974,
      -1.1097428134709801,
      -1.2500258140998042,
      -1.161996709342907,
      -1.0832211186423693,
      -1.1033971185502893,
      0.0,
      -1.2751591553294306,
      -1.062098238881891,
      -1.0294913234726535,
      -1.1567405795183543,
      -1.0074729775883613,
      -1.1106779416043928,
      -1.0177848006065808,
      -1.0962603631557644,
      -1.133257412776715,
      -1.0933216209016985,
      -1.0807565739541134,
      -1.0584871200561454,
      -1.1120996067310382,
      -1.146715540654613,
      -1.179527914735864,
      -1.0416561884415918,
      -1.0813573785979054,
      -1.2147663635691717
    ],
    [
      -1.3348868283672686,
      -1.3198317830327426,
      -1.3096452759285089,
      -1.3049510967730433,
      -1.2992583269570082,
      -1.2635522583774836,
      -1.288537027791803,
      -1.2753772725695125,
      -1.310042848583841,
      -1.3292000294588078,
      -1.2892076448427774,
      0.0,
      -1.2933457439776794,
      -1.2943356452933745,
      -1.2898221912142072,
      -1.3336822341762622,
      -1.301301606095694,
      -1.286811201590337,
      -1.329297243445022,
      -1.328348286620318,
      -1.29891347053102,
      -1.267549113926517,
      -1.3229795901734904,
      -1.3188810383887046,
      -1.2600545894404598,
      -1.285328826496019,
      -1.3113058038659218,
      -1.3180916291679072,
      -1.248310316331519
    ],
    [
      -1.2989195251874686,
      -1.1264855441001036,
      -1.1710804382299473,
      -1.1399372294522112,
      -1.1962840738943261,
      -1.2337204980289418,
      -1.3273524359080404,
      -1.1774866287500119,
      -1.1958021331087765,
      -1.2262405676109431,
      -1.2033990157430972,
      -1.3016444507345561,
      0.0,
      -1.1643475142870425,
      -1.2761594600843433,
      -1.2255744952710548,
      -1.2369250962636633,
      -1.2304514876720023,
      -1.2550725188006264,
      -1.2655438068837435,
      -1.2401223213678696,
      -1.2404531469666118,
      -1.2470117114253696,
      -1.2647595803949396,
      -1.2348031116002907,
      -1.186458869626079,
      -1.2374468824227,
      -1.2951262977514184,
      -1.2732662995977175
    ],
    [
      -1.2446173258442124,
      -1.0097853750910666,
      -1.0066610225065777,
      -1.107474663915336,
      -1.0741978303448692,
      -1.1700229183905064,
      -1.3052654154399157,
      -1.118184921564455,
      -1.1328203898077556,
      -1.1165726446792892,
      -1.0792140694577768,
      -1.278639076818315,
      -1.054438651983178,
      0.0,
      -1.224881468390636,
      -1.0226020379911063,
      -1.182131814340065,
      -1.1400462551872756,
      -1.1663265806842562,
      -1.1735436391718057,
      -1.2115697255189455,
      -1.1437405804490415,
      -1.1905119472801082,
      -1.1591526167063022,
      -1.2019190052415405,
      -1.1680789068313397,
      -1.159680210256645,
      -1.167678715803883,
      -1.271854274714867
    ],
    [
      -1.3257276388889,
      -1.2350962443618811,
      -1.238205171754339,
      -1.292480317155546,
      -1.221806469996502,
      -1.3507058551508664,
      -1.342434846042782,
      -1.320035705021101,
      -1.2857748964354319,
      -1.250654203475939,
      -1.2757815327711228,
      -1.2988805645989259,
      -1.2520341821793866,
      -1.2313665660188802,
      0.0,
      -1.2436689884032233,
      -1.2790821343063374,
      -1.2320641699786337,
      -1.2627596507731067,
      -1.2271128386503218,
      -1.2496260904406378,
      -1.2608357821313223,
      -1.2690589262937249,
      -1.279712534776844,
      -1.3328970130619833,
      -1.315158088481279,
      -1.245567312301189,
      -1.2192137607947457,
      -1.212088417558007
    ],
    [
      -1.2534221106578733,
      -1.0765428823214078,
      -1.077903121641318,
      -1.1353543549715792,
      -1.1313483857039786,
      -1.2257267015587132,
      -1.3108965454973935,
      -1.1905181155570432,
      -1.1208978356019248,
      -1.1893319819072312,
      -1.1171348404203274,
      -1.311141944967969,
      -1.1576304419869448,
      -1.0739683159535969,
      -1.2350496315059598,
      0.0,
      -1.187042044476115,
      -1.1446659424070569,
      -1.2148563908553944,
      -1.1692619169621812,
      -1.1722257513447978,
      -1.1310619444436194,
      -1.1667015121321895,
      -1.1900263850262272,
      -1.2023934013890145,
      -1.2094876351090567,
      -1.1447246502752455,
      -1.1702794387200386,
      -1.2493902064493723
    ],
    [
      -1.5038550329126186,
      -1.3829350996050378,
      -1.3633392672955913,
      -1.41143721482026,
      -1.408921217768873,
      -1.404133251140146,
      -1.5290627702103392,
      -1.3915653647381938,
      -1.4003376230941267,
      -1.4389594789483628,
      -1.388881264488608,
      -1.5467436639042276,
      -1.3880042383741245,
      -1.406053279218499,
      -1.4874262285816895,
      -1.3667492527608043,
      0.0,
      -1.3464124044551695,
      -1.4181732167666585,
      -1.4641195034216226,
      -1.3800230234860564,
      -1.4773443624855682,
      -1.4106852260581892,
      -1.415047284422989,
      -1.4467022837053063,
      -1.4232309368895038,
      -1.3557991708975432,
      -1.4371396363949707,
      -1.507088988227691
    ],
    [
      -1.3201156132317544,
      -1.2263189849908553,
      -1.222319238795604,
      -1.2884520355808955,
      -1.271658338839507,
      -1.3043151927624006,
      -1.4644400171672063,
      -1.2974981965493753,
      -1.318669527932801,
      -1.354524421729252,
      -1.2623191183035796,
      -1.4727741793193982,
      -1.2803742966741474,
      -1.3066221227316093,
      -1.4119269300694504,
      -1.271190355407336,
      -1.2841343237421126,
      0.0,
      -1.2902349149958638,
      -1.375340737853868,
      -1.2637564545075894,
      -1.3487945394932974,
      -1.311190895436245,
      -1.285265011869901,
      -1.4089634344285349,
      -1.315069573232655,
      -1.2537717090281364,
      -1.3165431445525149,
      -1.4540731048975057
    ],
    [
      -1.2772800762074419,
      -1.2363769614613314,
      -1.2054251886049683,
      -1.1963691247386075,
      -1.2019025598400122,
      -1.2564037679708109,
      -1.3799157246945792,
      -1.22885251286023,
      -1.2524722728193265,
      -1.2885301486331866,
      -1.2319943418142039,
      -1.3994116346733807,
      -1.2217536389278654,
      -1.2519837556439113,
      -1.3231210400764764,
      -1.2339612284597996,
      -1.2246203691062467,
      -1.2184315958881131,
      0.0,
      -1.2833766700233569,
      -1.2410837447702536,
      -1.2604451762909237,
      -1.2318351668917928,
      -1.2611916176517646,
      -1.3215281483248356,
      -1.2794575832290649,
      -1.2324625209657414,
      -1.3206839537688044,
      -1.3798547371534402
    ],
    [
      -1.2666563715310046,
      -1.1758715842500158,
      -1.1533696774521254,
      -1.2075865594761463,
      -1.186832199795441,
      -1.2259910241360292,
      -1.2991675435423398,
      -1.215332382612376,
      -1.2089175413843725,
      -1.1839865938351593,
      -1.1830739325889268,
      -1.2969659265889275,
      -1.2162187295598392,
      -1.1614297281099253,
      -1.209473127701205,
      -1.1333019281077286,
      -1.1608127628779714,
      -1.1291433528049242,
      -1.2398356134422506,
      0.0,
      -1.1104454389078022,
      -1.2003046048175985,
      -1.1031967964243723,
      -1.179771590267345,
      -1.2234896449612875,
      -1.2301609798086968,
      -1.107583388472339,
      -1.1553963024660712,
      -1.2645441623346163
    ],
    [
      -1.4506999427615315,
      -1.376939322749608,
      -1.401621902883902,
      -1.4869915551155906,
      -1.4104647179169816,
      -1.5210558089413555,
      -1.6228117488622533,
      -1.4444828802261749,
      -1.4818613146006079,
      -1.4587283258060704,
      -1.445325570652884,
      -1.6496612811736957,
      -1.4765677226288918,
      -1.451051750699666,
      -1.5348147512151709,
      -1.4232517491726442,
      -1.4431769121501157,
      -1.3496241437447054,
      -1.4355850676468391,
      -1.4407419623681703,
      0.0,
      -1.476661858417362,
      -1.4193912430180637,
      -1.4698415896946362,
      -1.5625829843852121,
      -1.527566671079946,
      -1.37767882775006,
      -1.4770587758451368,
      -1.6162462539513864
    ],
    [
      -1.4012562128193198,
      -1.2698590267399008,
      -1.3063354065562387,
      -1.3237370694079267,
      -1.3380791508350323,
      -1.3480553354775204,
      -1.4885794386248021,
      -1.3398080332574316,
      -1.3390649837060262,
      -1.3670746881120823,
      -1.3495074110238698,
      -1.4540942440987499,
      -1.2888029100401799,
      -1.2921896916446498,
      -1.413639323975712,
      -1.3091834582803734,
      -1.395378137236318,
      -1.382729450373835,
      -1.3816830992225853,
      -1.4348631398628027,
      -1.3934131571724906,
      0.0,
      -1.3986920208764126,
      -1.3759848386188314,
      -1.401518785426052,
      -1.3144352614907457,
      -1.4032426319057678,
      -1.4167060716392295,
      -1.4294532281235695
    ],
    [
      -1.2562537858432696,
      -1.11243188421877,
      -1.113722873265028,
      -1.1899424463321986,
      -1.144095984580544,
      -1.2519185057933513,
      -1.3307838804531014,
      -1.178582504716217,
      -1.1893717577694003,
      -1.2197768588062419,
      -1.119133396624597,
      -1.396027002327487,
      -1.1735008545593006,
      -1.1915096700333991,
      -1.2996425452616447,
      -1.1216750975526983,
      -1.1316504952022024,
      -1.0717762276103062,
      -1.2057942281271927,
      -1.1628543010432733,
      -1.1381106290793888,
      -1.2265704779908138,
      0.0,
      -1.227163871146492,
      -1.2083547054741908,
      -1.266559467601646,
      -1.072501964313913,
      -1.2212180026263422,
      -1.3599864337899121
    ],
    [
      -1.327305029231377,
      -1.2504683931036635,
      -1.2639466679886155,
      -1.2807483649962934,
      -1.2806742943738965,
      -1.304964264129707,
      -1.4177202670075784,
      -1.2708367694920024,
      -1.2463610839959072,
      -1.2564826993005478,
      -1.3025018591291937,
      -1.4208554233112236,
      -1.2933744021417943,
      -1.2906917913531588,
      -1.3511543139012752,
      -1.2896339990919596,
      -1.2689090059503045,
      -1.2461170454067798,
      -1.298774693415459,
      -1.3028025246357924,
      -1.3030386965636418,
      -1.2996733642072387,
      -1.3089560891615484,
      0.0,
      -1.3146140892865281,
      -1.2475210038054254,
      -1.2985805867469757,
      -1.2782104097023697,
      -1.378537751437849
    ],
    [
      -1.4011682215322128,
      -1.305165340636582,
      -1.2923856950275117,
      -1.3516464872362146,
      -1.3351831780999959,
      -1.3483958836179628,
      -1.3897183885554072,
      -1.272255140534357,
      -1.35967694358639,
      -1.3271853731629242,
      -1.2944704737456922,
      -1.3885435275255773,
      -1.3095357006701425,
      -1.3166170398566686,
      -1.4119352339951843,
      -1.3009834485504999,
      -1.2904073704190349,
      -1.287809685937478,
      -1.3871991384992273,
      -1.3547032910010088,
      -1.3715825501204961,
      -1.39393968172744,
      -1.2713102868877104,
      -1.33617887061734,
      0.0,
      -1.3348439803512298,
      -1.3058596009318884,
      -1.3194678032866747,
      -1.3393540795993188
    ],
    [
      -1.314090396284638,
      -1.1901890130219306,
      -1.1628403277977073,
      -1.1808219057896576,
      -1.1986535655213282,
      -1.2000766428043295,
      -1.2636405382478502,
      -1.1509625982865848,
      -1.1969923180326407,
      -1.2676325836402833,
      -1.2445035195949856,
      -1.272135792639007,
      -1.1511613418163698,
      -1.1944288746613294,
      -1.261803602352148,
      -1.1868238720945261,
      -1.192979347086946,
      -1.2010921535608148,
      -1.2463126969416838,
      -1.2526139852459695,
      -1.2534167707797677,
      -1.1894496850949952,
      -1.2357967513533465,
      -1.2101358080845808,
      -1.214088069733364,
      0.0,
      -1.2554519507638298,
      -1.261280621416591,
      -1.2178272426450756
    ],
    [
      -1.4046763736830716,
      -1.2929657325108497,
      -1.3061845115204904,
      -1.401516227695423,
      -1.3474576277053076,
      -1.3883663061668898,
      -1.5195900255736747,
      -1.369117759075286,
      -1.378212781027127,
      -1.389613618389363,
      -1.3280527561729523,
      -1.5557947623418311,
      -1.397219528137608,
      -1.3446781279246571,
      -1.495376485547535,
      -1.3380570167003023,
      -1.3148254383211802,
      -1.2786122306481171,
      -1.4072635864797172,
      -1.3532052216532697,
      -1.3313772426593564,
      -1.4119814738780176,
      -1.292122145910564,
      -1.3971307778138156,
      -1.4173700338047317,
      -1.444412443834514,
      0.0,
      -1.388034866333463,
      -1.5628826511212244
    ],
    [
      -1.4017345227346119,
      -1.3211002890321792,
      -1.3034628864491218,
      -1.3677753480837167,
      -1.3786665578098594,
      -1.4143121097952391,
      -1.4627825376676447,
      -1.3402675588252666,
      -1.3345286511256356,
      -1.358166787665412,
      -1.318967561048528,
      -1.490443725803893,
      -1.3829345142189617,
      -1.294144700995752,
      -1.4292643759245534,
      -1.2981913271111232,
      -1.3577063876100544,
      -1.2619823678310564,
      -1.3860873858390532,
      -1.326358369493998,
      -1.2918693787183864,
      -1.3975810354132767,
      -1.3395688355946156,
      -1.3084797387972893,
      -1.393409607460659,
      -1.3918947411746243,
      -1.3345893724929414,
      0.0,
      -1.4665809732962296
    ],
    [
      -1.4248574998104666,
      -1.3853997291715225,
      -1.3700231232532718,
      -1.3803846210350923,
      -1.3746723175908047,
      -1.3814756242797104,
      -1.3473844401336548,
      -1.317854330564195,
      -1.3611915240046628,
      -1.3665895810836932,
      -1.3890630427195585,
      -1.3419588905748783,
      -1.327931218391013,
      -1.3846851245675946,
      -1.3393218017611925,
      -1.3727856718140823,
      -1.3249232853278459,
      -1.3655603378271477,
      -1.371704378170403,
      -1.3706291171870493,
      -1.376544566210072,
      -1.3732279931190883,
      -1.3815061654721903,
      -1.3440053808399344,
      -1.3145991100656964,
      -1.3233458823070967,
      -1.3829988623764102,
      -1.3937796040165942,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.1815123771588807,
      0.16732220141692888,
      0.17737187830381762,
      0.1863594199746479,
      0.16245376235376519,
      0.09902378065264905,
      0.16249192563101955,
      0.18790916519205636,
      0.15595224236261584,
      0.16708665864486205,
      0.09524250793210243,
      0.1725711047249452,
      0.17188098229152704,
      0.1197473587075839,
      0.1866202740206666,
      0.18173484413821805,
      0.2089365306193809,
      0.18495213638853691,
      0.16345968695765745,
      0.18412223776307757,
      0.16509564109466224,
      0.1801093627279169,
      0.16920405682915662,
      0.13668184652520754,
      0.15228427098216324,
      0.18525636190989792,
      0.15102848330326468,
      0.09325016273909581
    ],
    [
      0.3506844485863454,
      0.0,
      0.5279943332315811,
      0.4827804421522557,
      0.511374878133634,
      0.46994208238135826,
      0.2429897882629275,
      0.42378305338604694,
      0.471420234420892,
      0.4219367311807347,
      0.48799059320687554,
      0.28298977803083036,
      0.5421786273866309,
      0.5487083682776137,
      0.3676689659195598,
      0.543851259058588,
      0.4306943670998533,
      0.5143831670057233,
      0.44298950509029833,
      0.3964577576664412,
      0.4465101464173098,
      0.4786956394040893,
      0.39574098914181755,
      0.46377661672644765,
      0.3702272793453978,
      0.3842112071429584,
      0.4804643944085787,
      0.42249934309798287,
      0.3137807615393806
    ],
    [
      0.3095489743293649,
      0.47539732505934884,
      0.0,
      0.43232051937524973,
      0.423010347768791,
      0.3927863599507584,
      0.24688968417129642,
      0.3815301372230804,
      0.3997974229476611,
      0.3791669847497221,
      0.42284581812248967,
      0.2922539267680724,
      0.43413561989734584,
      0.4923788623302645,
      0.3148734864882603,
      0.44649753506183276,
      0.38073453857577944,
      0.41744435380083544,
      0.3473284592759136,
      0.38350643613702706,
      0.39888993860214295,
      0.4179697834453344,
      0.3747104072535079,
      0.39411998353435695,
      0.30845696657821375,
      0.36956439781414363,
      0.39463482869445077,
      0.4046964203421797,
      0.28598841248632434
    ],
    [
      0.2787451422970819,
      0.36207312812245007,
      0.38151468420239376,
      0.0,
      0.32747084452731,
      0.28959117364326836,
      0.2105763278198598,
      0.3258510699165922,
      0.3421624632210256,
      0.25703866005978093,
      0.31069493400791637,
      0.20731060407625534,
      0.3624349787812964,
      0.3671662666577131,
      0.2806581040919094,
      0.33188370564229985,
      0.28952458499024614,
      0.31564137832460193,
      0.33158872637772463,
      0.2900071219227627,
      0.2736558669480622,
      0.29105813024147054,
      0.28784972479056803,
      0.2940146247271094,
      0.2629090901661171,
      0.31465276631206773,
      0.2824239908978441,
      0.26938027634485295,
      0.23727450484956436
    ],
    [
      0.3222115104806118,
      0.48126863989412505,
      0.4685586215663424,
      0.4058731561625648,
      0.0,
      0.35635693519986744,
      0.2552690201693928,
      0.4060522955568724,
      0.3718459133742513,
      0.38555640756307485,
      0.43269072520512,
      0.2699901207555664,
      0.4417469892953001,
      0.45319170860748414,
      0.31898188660449556,
      0.4334516027415509,
      0.34653989526144224,
      0.39884864175064116,
      0.36149214521700035,
      0.3043585501585624,
      0.37046785502987056,
      0.3879680633785403,
      0.3963395334085804,
      0.3461270886391925,
      0.32077971295238883,
      0.3324925831556307,
      0.4033303820193088,
      0.3208460065155292,
      0.26986542037914285
    ],
    [
      0.24744740871554516,
      0.3888422558542193,
      0.41281633193086376,
      0.3665429822472954,
      0.34170030022390874,
      0.0,
      0.29510582838649957,
      0.3684624105975798,
      0.34468678710015443,
      0.24087563551855085,
      0.3337598889092268,
      0.2659833075113369,
      0.34190076871142483,
      0.367375375731094,
      0.2549404313235726,
      0.3510392926997634,
      0.30294451588529725,
      0.36208279656639597,
      0.3396483063443345,
      0.31725815388382417,
      0.2771889540676633,
      0.31254133056332223,
      0.2663117349128794,
      0.29800521125922286,
      0.2683961813056619,
      0.3528421395612138,
      0.28195398762763313,
      0.28544132276706047,
      0.25171529515763114
    ],
    [
      0.2174841640609979,
      0.30266898143419296,
      0.27686334537835733,
      0.2685575348490912,
      0.291715555905798,
      0.33254433679959305,
      0.0,
      0.3268935900432255,
      0.2698281516174428,
      0.2597126706405899,
      0.2469568540812701,
      0.26891861814694673,
      0.29326150441514964,
      0.26436175633355474,
      0.24400121057320612,
      0.2760240306069648,
      0.31176206611480217,
      0.2754023951542759,
      0.25394292109486005,
      0.2510845075717476,
      0.2799256368960026,
      0.2892105946216377,
      0.27340095785758556,
      0.24935939920180328,
      0.2817095696106864,
      0.3062264016656442,
      0.28192925859359086,
      0.23621997889768198,
      0.3047129204909895
    ],
    [
      0.3116044950801633,
      0.455198240740569,
      0.4516476097283577,
      0.36631804496161746,
      0.39721079138276916,
      0.40073572092040854,
      0.28454240220474447,
      0.0,
      0.390160549548078,
      0.33798628447051526,
      0.36283496140695903,
      0.26518203592954914,
      0.4483049876676881,
      0.4491721948649787,
      0.2993986191695901,
      0.3913031295923943,
      0.3746441492711341,
      0.3975967735632313,
      0.3152152202351237,
      0.3022739280500639,
      0.33976775698325357,
      0.34314709260474774,
      0.35683000553415534,
      0.3731694102304117,
      0.34899519272630886,
      0.43977075946970423,
      0.362514495322942,
      0.32967665840081506,
      0.29268870899484645
    ],
    [
      0.2442227406734867,
      0.36665299827790276,
      0.36043991028747757,
      0.3140978510680579,
      0.32325631409055955,
      0.28500369333111,
      0.20340671682281086,
      0.3227253187947232,
      0.0,
      0.31588193112939034,
      0.3310690897319082,
      0.21686867899893025,
      0.3467915908344086,
      0.3352021706294086,
      0.23855177755023194,
      0.3727160631397535,
      0.32078048601609144,
      0.30261115293283924,
      0.2705151177013767,
      0.2842074167759545,
      0.3210528924269809,
      0.2798514905098948,
      0.3072229067331316,
      0.3089217054933582,
      0.2760610797981533,
      0.3067898846587329,
      0.3230043362326416,
      0.28117034509987393,
      0.23429278970832645
    ],
    [
      0.31628480199281106,
      0.4096913035411056,
      0.4293451490604776,
      0.3198533843540017,
      0.38439775867056314,
      0.2965285860174467,
      0.2409022345034364,
      0.33730197095541437,
      0.3632873852465863,
      0.0,
      0.39123553586051285,
      0.23387760436492777,
      0.3757334007965729,
      0.4057024873637691,
      0.2986181507469998,
      0.36623193504569107,
      0.37909291342422935,
      0.41186976559649624,
      0.33453387867450224,
      0.33440561875683783,
      0.34084201818188387,
      0.38079657198724925,
      0.3175787669982857,
      0.40149766344557136,
      0.3061840979677659,
      0.31031506275824206,
      0.3803063265044635,
      0.37123679349556715,
      0.253512984474052
    ],
    [
      0.36978998813579955,
      0.5620400288441652,
      0.5764084686886707,
      0.47837860190502,
      0.5125577791045626,
      0.4345983784833225,
      0.29431537785449846,
      0.38234448261139553,
      0.4611200733119334,
      0.4409440734040133,
      0.0,
      0.26918203662487206,
      0.4822429530724117,
      0.5148498684816492,
      0.38760061243594834,
      0.5368682143659413,
      0.4336632503499098,
      0.5265563913477218,
      0.4480808287985383,
      0.41108377917758765,
      0.45101957105260415,
      0.46358461800018924,
      0.4858540718981572,
      0.4322415852232644,
      0.39762565129968963,
      0.3648132772184387,
      0.5026850035127108,
      0.46298381335639727,
      0.32957482838513097
    ],
    [
      0.20293132323872398,
      0.21798636857324993,
      0.22817287567748368,
      0.23286705483294923,
      0.2385598246489844,
      0.2742658932285089,
      0.24928112381418965,
      0.26244087903648006,
      0.22777530302215165,
      0.20861812214718478,
      0.24861050676321517,
      0.0,
      0.24447240762831313,
      0.24348250631261803,
      0.2479959603917854,
      0.20413591742973036,
      0.23651654551029866,
      0.25100695001565554,
      0.20852090816097046,
      0.20946986498567455,
      0.23890468107497265,
      0.27026903767947563,
      0.21483856143250213,
      0.21893711321728793,
      0.2777635621655328,
      0.2524893251099736,
      0.22651234774007079,
      0.21972652243808533,
      0.28950783527447355
    ],
    [
      0.2809412524419459,
      0.45337523352931086,
      0.4087803393994671,
      0.43992354817720325,
      0.38357670373508834,
      0.34614027960047267,
      0.25250834172137404,
      0.4023741488794026,
      0.384058644520638,
      0.3536202100184713,
      0.3764617618863173,
      0.2782163268948583,
      0.0,
      0.41551326334237193,
      0.3037013175450711,
      0.3542862823583597,
      0.34293568136575114,
      0.34940928995741216,
      0.32478825882878803,
      0.314316970745671,
      0.33973845626154486,
      0.33940763066280266,
      0.33284906620404486,
      0.31510119723447483,
      0.3450576660291238,
      0.3934019080033355,
      0.3424138952067144,
      0.28473447987799605,
      0.3065944780316969
    ],
    [
      0.29613941522723874,
      0.5309713659803845,
      0.5340957185648734,
      0.433282077156115,
      0.46655891072658195,
      0.3707338226809447,
      0.2354913256315354,
      0.42257181950699607,
      0.40793635126369554,
      0.4241840963921619,
      0.46154267161367435,
      0.2621176642531362,
      0.486318089088273,
      0.0,
      0.31587527268081517,
      0.5181547030803448,
      0.35862492673138613,
      0.40071048588417546,
      0.3744301603871949,
      0.3672131018996454,
      0.3291870155525056,
      0.39701616062240963,
      0.3502447937913429,
      0.3816041243651489,
      0.3388377358299106,
      0.37267783424011136,
      0.3810765308148061,
      0.37307802526756806,
      0.2689024663565842
    ],
    [
      0.2979303035704268,
      0.3885616980974458,
      0.38545277070498796,
      0.33117762530378103,
      0.4018514724628248,
      0.27295208730846054,
      0.28122309641654497,
      0.30362223743822603,
      0.3378830460238951,
      0.373003738983388,
      0.3478764096882041,
      0.32477737786040106,
      0.3716237602799404,
      0.39229137644044676,
      0.0,
      0.37998895405610367,
      0.3445758081529895,
      0.39159377248069327,
      0.36089829168622023,
      0.3965451038090051,
      0.37403185201868916,
      0.36282216032800463,
      0.3545990161656021,
      0.34394540768248283,
      0.29076092939734366,
      0.3084998539780479,
      0.37809063015813793,
      0.4044441816645812,
      0.41156952490132004
    ],
    [
      0.29433827196503226,
      0.4712175003014978,
      0.46985726098158764,
      0.4124060276513264,
      0.41641199691892705,
      0.3220336810641924,
      0.23686383712551207,
      0.3572422670658624,
      0.4268625470209808,
      0.3584284007156744,
      0.4306255422025782,
      0.2366184376549365,
      0.39012994063596085,
      0.47379206666930873,
      0.3127107511169458,
      0.0,
      0.36071833814679066,
      0.4030944402158487,
      0.3329039917675112,
      0.3784984656607244,
      0.3755346312781078,
      0.4166984381792862,
      0.3810588704907161,
      0.35773399759667845,
      0.3453669812338911,
      0.33827274751384895,
      0.4030357323476601,
      0.377480943902867,
      0.29837017617353334
    ],
    [
      0.312586600858231,
      0.43350653416581175,
      0.45310236647525826,
      0.4050044189505895,
      0.4075204160019765,
      0.41230838263070346,
      0.28737886356051034,
      0.42487626903265574,
      0.41610401067672287,
      0.3774821548224867,
      0.4275603692822416,
      0.269697969866622,
      0.4284373953967251,
      0.41038835455235056,
      0.32901540518916006,
      0.4496923810100453,
      0.0,
      0.47002922931568003,
      0.398268417004191,
      0.352322130349227,
      0.4364186102847931,
      0.33909727128528133,
      0.4057564077126603,
      0.40139434934786045,
      0.36973935006554326,
      0.3932106968813458,
      0.46064246287330635,
      0.3793019973758789,
      0.30935264554315856
    ],
    [
      0.39485345999151833,
      0.4886500882324174,
      0.49264983442766885,
      0.42651703764237725,
      0.44331073438376567,
      0.41065388046087214,
      0.25052905605606646,
      0.4174708766738975,
      0.39629954529047184,
      0.3604446514940207,
      0.4526499549196932,
      0.2421948939038745,
      0.4345947765491254,
      0.4083469504916635,
      0.3030421431538224,
      0.4437787178159367,
      0.4308347494811602,
      0.0,
      0.4247341582274089,
      0.33962833536940473,
      0.4512126187156833,
      0.3661745337299753,
      0.40377817778702774,
      0.4297040613533718,
      0.30600563879473786,
      0.3998994999906178,
      0.46119736419513635,
      0.3984259286707579,
      0.260895968325767
    ],
    [
      0.35947199534636254,
      0.40037511009247306,
      0.43132688294883614,
      0.44038294681519696,
      0.43484951171379227,
      0.38034830358299354,
      0.2568363468592252,
      0.4078995586935745,
      0.38427979873447793,
      0.3482219229206178,
      0.40475772973960056,
      0.23734043688042372,
      0.41499843262593905,
      0.38476831590989313,
      0.31363103147732807,
      0.4027908430940048,
      0.4121317024475577,
      0.4183204756656913,
      0.0,
      0.35337540153044755,
      0.3956683267835508,
      0.37630689526288075,
      0.4049169046620116,
      0.37556045390203985,
      0.31522392322896886,
      0.35729448832473953,
      0.40428955058806304,
      0.316068117785,
      0.2568973344003642
    ],
    [
      0.3080074793327421,
      0.3987922666137309,
      0.42129417341162134,
      0.36707729138760037,
      0.38783165106830575,
      0.3486728267277175,
      0.27549630732140695,
      0.35933146825137063,
      0.36574630947937425,
      0.39067725702858747,
      0.3915899182748199,
      0.2776979242748192,
      0.3584451213039075,
      0.41323412275382143,
      0.36519072316254175,
      0.44136192275601815,
      0.4138510879857753,
      0.44552049805882254,
      0.33482823742149614,
      0.0,
      0.4642184119559445,
      0.37435924604614823,
      0.4714670544393744,
      0.3948922605964018,
      0.3511742059024592,
      0.34450287105504995,
      0.46708046239140777,
      0.41926754839767555,
      0.3101196885291304
    ],
    [
      0.4589180455230306,
      0.532678665534954,
      0.50799608540066,
      0.4226264331689715,
      0.4991532703675805,
      0.3885621793432066,
      0.28680623942230876,
      0.4651351080583872,
      0.4277566736839542,
      0.4508896624784917,
      0.46429241763167806,
      0.2599567071108664,
      0.4330502656556703,
      0.458566237584896,
      0.3748032370693912,
      0.4863662391119179,
      0.4664410761344464,
      0.5599938445398567,
      0.47403292063772295,
      0.46887602591639177,
      0.0,
      0.4329561298672,
      0.49022674526649834,
      0.4397763985899259,
      0.34703500389934994,
      0.38205131720461605,
      0.5319391605345021,
      0.4325592124394253,
      0.29337173433317565
    ],
    [
      0.32946768071703314,
      0.4608648667964521,
      0.4243884869801142,
      0.40698682412842624,
      0.39264474270132066,
      0.3826685580588325,
      0.2421444549115508,
      0.39091586027892133,
      0.3916589098303267,
      0.3636492054242706,
      0.3812164825124831,
      0.27662964943760304,
      0.44192098349617304,
      0.4385342018917031,
      0.317084569560641,
      0.4215404352559795,
      0.3353457563000348,
      0.34799444316251793,
      0.34904079431376767,
      0.29586075367355025,
      0.33731073636386233,
      0.0,
      0.3320318726599403,
      0.35473905491752156,
      0.3292051081103009,
      0.4162886320456072,
      0.3274812616305851,
      0.31401782189712346,
      0.30127066541278347
    ],
    [
      0.4163112935915325,
      0.5601331952160322,
      0.5588422061697742,
      0.48262263310260356,
      0.5284690948542581,
      0.42064657364145086,
      0.3417811989817008,
      0.4939825747185851,
      0.48319332166540185,
      0.4527882206285603,
      0.553431682810205,
      0.27653807710731515,
      0.4990642248755015,
      0.481055409401403,
      0.37292253417315746,
      0.5508899818821038,
      0.5409145842325997,
      0.6007888518244959,
      0.4667708513076094,
      0.5097107783915289,
      0.5344544503554134,
      0.4459946014439884,
      0.0,
      0.4454012082883101,
      0.4642103739606114,
      0.4060056118331561,
      0.6000631151208891,
      0.45134707680846,
      0.31257864564489
    ],
    [
      0.3517018081950183,
      0.4285384443227318,
      0.4150601694377798,
      0.39825847243010193,
      0.3983325430524989,
      0.37404257329668833,
      0.261286570418817,
      0.4081700679343929,
      0.43264575343048817,
      0.42252413812584755,
      0.37650497829720164,
      0.25815141411517173,
      0.385632435284601,
      0.3883150460732365,
      0.3278525235251202,
      0.3893728383344357,
      0.4100978314760908,
      0.4328897920196155,
      0.3802321440109364,
      0.37620431279060296,
      0.37596814086275354,
      0.37933347321915667,
      0.37005074826484696,
      0.0,
      0.3643927481398672,
      0.4314858336209699,
      0.3804262506794196,
      0.4007964277240257,
      0.3004690859885464
    ],
    [
      0.28587036238678265,
      0.38187324328241345,
      0.39465288889148376,
      0.3353920966827808,
      0.35185540581899954,
      0.3386427003010326,
      0.2973201953635882,
      0.41478344338463846,
      0.3273616403326054,
      0.35985321075607124,
      0.3925681101733032,
      0.2984950563934181,
      0.37750288324885295,
      0.37042154406232686,
      0.27510334992381114,
      0.38605513536849556,
      0.39663121349996056,
      0.3992288979815175,
      0.29983944541976815,
      0.3323352929179866,
      0.3154560337984993,
      0.2930989021915553,
      0.415728297031285,
      0.35085971330165533,
      0.0,
      0.35219460356776566,
      0.38117898298710706,
      0.36757078063232074,
      0.34768450431967657
    ],
    [
      0.22101678046228712,
      0.3449181637249945,
      0.3722668489492178,
      0.3542852709572675,
      0.3364536112255969,
      0.33503053394259563,
      0.27146663849907493,
      0.38414457846034034,
      0.33811485871428437,
      0.2674745931066418,
      0.2906036571519395,
      0.2629713841079182,
      0.38394583493055534,
      0.3406783020855957,
      0.27330357439477715,
      0.348283304652399,
      0.342127829659979,
      0.3340150231861103,
      0.2887944798052413,
      0.28249319150095564,
      0.28169040596715744,
      0.34565749165192994,
      0.29931042539357855,
      0.32497136866234433,
      0.32101910701356107,
      0.0,
      0.27965522598309533,
      0.27382655533033406,
      0.3172799341018495
    ],
    [
      0.3798834504418196,
      0.49159409161404155,
      0.4783753126044008,
      0.3830435964294683,
      0.4371021964195836,
      0.3961935179580014,
      0.2649697985512165,
      0.4154420650496051,
      0.40634704309776426,
      0.3949462057355282,
      0.45650706795193896,
      0.2287650617830601,
      0.38734029598728315,
      0.4398816962002341,
      0.2891833385773561,
      0.4465028074245889,
      0.4697343858037111,
      0.5059475934767741,
      0.37729623764517406,
      0.4313546024716215,
      0.4531825814655348,
      0.3725783502468736,
      0.49243767821432716,
      0.38742904631107566,
      0.3671897903201595,
      0.34014738029037717,
      0.0,
      0.39652495779142827,
      0.22167717300366685
    ],
    [
      0.40080789842419495,
      0.48144213212662756,
      0.499079534709685,
      0.4347670730750901,
      0.42387586334894745,
      0.38823031136356767,
      0.33975988349116215,
      0.4622748623335402,
      0.46801377003317124,
      0.4443756334933948,
      0.4835748601102787,
      0.31209869535491386,
      0.41960790693984507,
      0.5083977201630547,
      0.37327804523425345,
      0.5043510940476836,
      0.44483603354875245,
      0.5405600533277504,
      0.4164550353197536,
      0.4761840516648088,
      0.5106730424404204,
      0.4049613857455301,
      0.4629735855641912,
      0.4940626823615175,
      0.4091328136981478,
      0.4106476799841825,
      0.4679530486658654,
      0.0,
      0.3359614478625772
    ],
    [
      0.24282238031231484,
      0.2822801509512589,
      0.29765675686950965,
      0.28729525908768916,
      0.2930075625319768,
      0.2862042558430711,
      0.3202954399891267,
      0.34982554955858647,
      0.30648835611811864,
      0.30109029903908824,
      0.27861683740322296,
      0.3257209895479032,
      0.33974866173176843,
      0.2829947555551868,
      0.3283580783615889,
      0.29489420830869917,
      0.3427565947949356,
      0.30211954229563376,
      0.29597550195237843,
      0.2970507629357322,
      0.2911353139127095,
      0.2944518870036932,
      0.28617371465059116,
      0.3236744992828471,
      0.35308077005708505,
      0.34433399781568474,
      0.2846810177463712,
      0.2739002761061873,
      0.0
    ]
  ],
  "row_avgs": [
    0.16248790219093948,
    0.43631159848936246,
    0.38290992609941954,
    0.29911260264143374,
    0.3700893361086589,
    0.31563603340582763,
    0.27623853259491743,
    0.3638532221091115,
    0.29976315890883987,
    0.3461130053851951,
    0.44332170024801937,
    0.23735926148394793,
    0.34872237973070386,
    0.3878420237710557,
    0.35402116025207847,
    0.3706537958427781,
    0.39143555216110776,
    0.39423134414743716,
    0.3710118836434305,
    0.3807760119974311,
    0.43702917987532425,
    0.3608179575881937,
    0.4732468704296978,
    0.3792405916096773,
    0.3514127833578465,
    0.3148499633436294,
    0.39684204724523614,
    0.4399405765868895,
    0.3038083364201057
  ],
  "col_avgs": [
    0.31435798129923015,
    0.4190394427886709,
    0.42235575600342357,
    0.37878607436994677,
    0.3907292679201269,
    0.35245969250407905,
    0.2615878528208223,
    0.3778549960382647,
    0.37609800103280727,
    0.35169011944248135,
    0.3823627149139192,
    0.26056383163166547,
    0.39421914075862885,
    0.4007375682521132,
    0.30528901639817585,
    0.40210474314151046,
    0.3714710627285437,
    0.4030213046453713,
    0.34778918139622655,
    0.34355507513112304,
    0.36386529226646414,
    0.35789651967919045,
    0.36108537074954017,
    0.3596508672257443,
    0.3276150848615066,
    0.35276310829279883,
    0.38057930019239994,
    0.34350893913324715,
    0.2860414320502742
  ],
  "combined_avgs": [
    0.23842294174508483,
    0.4276755206390167,
    0.40263284105142155,
    0.33894933850569026,
    0.38040930201439294,
    0.33404786295495337,
    0.26891319270786984,
    0.3708541090736881,
    0.33793057997082354,
    0.34890156241383824,
    0.4128422075809693,
    0.2489615465578067,
    0.37147076024466635,
    0.3942897960115844,
    0.32965508832512713,
    0.3863792694921443,
    0.38145330744482575,
    0.3986263243964042,
    0.3594005325198285,
    0.3621655435642771,
    0.40044723607089416,
    0.35935723863369207,
    0.417166120589619,
    0.3694457294177108,
    0.33951393410967656,
    0.3338065358182141,
    0.38871067371881807,
    0.39172475786006833,
    0.2949248842351899
  ],
  "gppm": [
    583.0024610140791,
    570.5702554780562,
    569.8640517453185,
    587.2802515039579,
    581.1047084419887,
    597.5335946610664,
    643.6660981948645,
    588.6685997803609,
    585.2795924359989,
    598.2006799309045,
    591.3070613553646,
    641.8044019768118,
    581.5061539590454,
    580.2339482530126,
    626.005124729653,
    579.1089954594901,
    590.6862953253757,
    577.2328446908388,
    604.4591622101962,
    608.3369316577786,
    594.2636935945222,
    597.2166019067532,
    599.9813990069813,
    598.0073024226938,
    612.7981681371358,
    600.7072767876613,
    588.2863315289615,
    607.1612920658408,
    632.2466624049241
  ],
  "gppm_normalized": [
    1.3465955763871043,
    1.2858566217656782,
    1.2840437118220138,
    1.320737406441861,
    1.305267901055745,
    1.3497999244147134,
    1.457955031683276,
    1.3244004735795503,
    1.315265668540935,
    1.3486673828538471,
    1.331573930404404,
    1.4525047612682898,
    1.3098056390908692,
    1.3042582784646128,
    1.4111362691238452,
    1.305318789480576,
    1.3288062095923596,
    1.292349834802222,
    1.3591419071429558,
    1.374149987126471,
    1.3296895214542321,
    1.3448793395599101,
    1.3461872705219033,
    1.3471367578431523,
    1.3789098562611801,
    1.3534636035098242,
    1.3235598218770903,
    1.3645274679849997,
    1.4267150225660448
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388
  ],
  "response_lengths": [
    3671,
    2585,
    2730,
    2738,
    2654,
    2752,
    2699,
    2363,
    3302,
    2457,
    2188,
    2564,
    2636,
    2618,
    2032,
    2629,
    2360,
    2681,
    2273,
    2234,
    2208,
    2405,
    1989,
    2522,
    2293,
    2661,
    2352,
    2023,
    2365
  ]
}