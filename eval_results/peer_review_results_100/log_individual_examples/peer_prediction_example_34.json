{
  "example_idx": 34,
  "reference": "Under review as a conference paper at ICLR 2023\n\nDENSITY SKETCHES FOR SAMPLING AND ESTIMATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThere has been an exponential increase in the data generated worldwide. Insights into this data led by machine learning (ML) have given rise to exciting applications such as recommendation engines, conversational agents, and so on. Often, data for these applications is generated at a rate faster than ML pipelines can consume it. In this paper, we propose Density Sketches(DS) - a cheap and practical approach to reducing data redundancy in a streaming fashion. DS creates a succinct online summary of data distribution. While DS does not store the samples from the stream, we can sample unseen data on the fly from DS to use for downstream learning tasks. In this sense, DS can replace actual data in many machine learning pipelines analogous to generative models. Importantly, unlike generative models, which do not have statistical guarantees, the sampling distribution of DS asymptotically converges to underlying unknown density distribution. Additionally, DS is a one-pass algorithm that can be computed on data streams in compute and memory-constrained environments, including edge devices.\n\n1\n\nINTRODUCTION\n\nWith the advent of big data, the rate of data generation is exploding. For instance, Google has around 3.8 million search queries per minute, amounting to over 5 billion data points or terabytes of data generated daily. Any processing over this data, such as using the training of a recommendation model, would suffer from data explosion. By the time existing data is consumed, newer data is In such cases, we need to discard a lot of data. One of the critical research already available. directions is how to reduce data storage. In this paper, we present Density Sketches (DS): an efficient and online data structure for reducing redundancy in data.\n\nOften data comes from an underlying unknown distribution, and one of the challenges in data reduction is maintaining this distribution. In DS, we approximately store the data distribution in the form of a sketch. Using this DS, we can perform point-wise density estimation queries. Additionally, we can sample synthetic data from this sketch to use in downstream machine learning tasks. This paper shows that data sampled from DS asymptotically converges to the underlying unknown distribution. We can also view density sketches through the lens of coresets. Specifically, DS is a compressed version of grid coresets. Grid coresets are the oldest form of coresets, giving lower additive errors than modern coresets. However, grid coresets are generally prohibitive as they are exponential in dimension (d). DS enables us to approximate grid coresets with the dependence of memory usage depending on the actual variety in the data instead of being exponential in d. Also, DS provides a streaming construction for this coreset.\n\nIn this paper, we focus more on the density estimation and sampling aspects of DS. Sampling from a distribution described using data requires estimating the underlying distribution. Popular methods to infer the distribution and sample from it belong to the following three categories: 1. Parametric density estimation (Friedman et al., 2001) 2. Non-parametric estimation - Histograms and Kernel Density Estimators (KDE) (Scott, 2015) 3. Learning-based approaches such as Variational Auto Encoders (VAE), Generative Adversarial Networks (GANs), and related methods (Goodfellow et al., 2014; 2016). Generally, parametric estimation is not suitable to model most real data as it can lead to significant, unavoidable bias from the choice of the model (Scott, 2015). Learning the distribution, e.g., via neural networks, is one solution to this problem. Although learning-based methods have recently found remarkable success, they do not have any theoretical guarantees for the distribution of generated samples. Histograms and KDEs, on the other hand, are theoretically well understood. These statistical estimators of density are known to uniformly converge to the underlying true distribution almost surely. This paper focuses on such estimators, which have theoretical guarantees.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nStorage of histograms and sampling from them is expensive because of an exponential number of partitions (also known as bins). Apart from this, histograms also suffer from the bin-edge problem: a slight variation in data can lead to significant differences in the estimation of densities. KDEs are used to solve the bin-edge problem. KDE gives a smoother estimate of density. While sampling from a KDE is efficient, KDE is expensive to store. KDE requires us to store the entire data. Coresets for KDE are a good solution to the storage problems of KDE. However, the construction of coresets is typically quite expensive. In this work, we propose Density Sketches(DS) - a compressed sketch of density constructed in an efficient streaming manner. DS does not store actual samples of the data. But we can still efficiently produce samples from a KDE for specific kernels, which, in turn, approximates f (x). Being a compressed sketch, we can tune the accuracy-storage trade-off of DS, and we analyze this trade-off in the theorem 1.\n\n2 PROBLEM STATEMENT AND RELATED WORK\n\nProblem Statement: Formally, we want to create a data structure that has the following properties : (1) It sketches density information. (2) The sketch size is much smaller than the data size and does not scale linearly with it. (3) The construction is streaming and efficient. (4) We do not store any samples in the data structure created (for privacy reasons). (5) We want the sampling distribution, say ˆfS(x), obtained by sampling from these data structures to approximate the true underlying distribution f (x).\n\nThe problem we aim to solve can be considered a data reduction problem and has been widely pursued in literature. The set of existing approaches can be broadly classified into two sections. (1) Sampling based / Coresets : Approaches such as clustering/importance sampling (Charikar & Siminelakis, 2017; Cortes & Scott, 2016; Chen et al., 2012) and coresets for KDE (Phillips & Tai, 2020; 2018) fall under this category. These approaches aim to find a small set of possibly weighted samples for a specific objective function such that the result obtained by applying the function to this small set is within a small approximation error of the result obtained by applying the objective function on a complete dataset. The issue with these approaches is that of efficiency. Most of these algorithms require complicated computation over the entire data. Some streaming algorithms were recently proposed for coresets for KDE (Karnin & Liberty, 2019). However, even these algorithms need to perform O(m) (m is compactor size) computationally expensive operations per sample for large chunks of size (m), making them unsuitable for our purposes. (2) Dimensionality reduction: These approaches aim to reduce the width of the data matrix. Approaches such as Principle Component Analysis (PCA) are computationally expensive and require iterative computation over the entire dataset. Random projections are an efficient streaming algorithm for dimensionality reduction. However, this approach leads to compressed data that increases linearly with the original data size. As we can see, existing approaches fall short of the requirements in our problem statement.\n\n3 BACKGROUND\n\n3.1 HISTOGRAMS AND KERNEL DENSITY ESTIMATION\n\nHistograms and KDE (Scott, 2015; Scott & Sain, 2004) are popular methods to estimate the density of a distribution given a finite i.i.d. sample of n points in Rd drawn from the true density, say f (x).\n\nHistogram: Histogram divides the support S ⊂ Rd of the data into multiple partitions. It then uses the counts in every partition to predict the density, ˆfH (x), at a point x. Formally the density predicted at the point x ∈ S is given by\n\nˆfH (x) =\n\nC(bin(x)) nV(bin(x))\n\nwhere bin(x) identifies the partition of x, C(b) and V(b) measures the the number of samples in partition b and the volume of partition b respectively. ˆfH (x) integrates to 1 and hence ˆfH (x) is also an estimate of the underlying density function f (x). Regular histograms use hyper-cube partitions of width B aligned with the data axes. As B increases, the bias of the estimate increases, and its variance decreases. Histograms suffer from bin-edge problems where a slight change in data across the bin’s edge can change predictions significantly.\n\nKernel Density Estimation(KDE): KDE provides a smoother estimate of f (x) which resolves the bin-edge problem of histograms. For a positive semi-definite kernel function k(x, y) : Rd×Rd → R\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nand data, say D, the KDE at point x is defined as\n\nˆfK(x) = KDE(x) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nk(x, xi) where xi ∈ D\n\nfor\n\nAs B increases,\n\nthe Gaussian kernel\n\nA smoothing parameter B also parameterizes the kernel\n\nKernel functions are positive, symmetric, and may be normalized to integrate to 1. Gaus- (Friedman et al., 2001) are some of the most widely used sian, Epanechnikov, Uniform, function and deterkernels. For uniform mines the standard deviation parameter and Epanechnikov kernel functions, B is the window width around x where the kernel is non-zero. the bias of KDE increases, and its variance decreases. Histograms estimator and KDE both uniformly converge to underlying true distribution asymptotically. However, both suffer from the curse of dimensionality. To get a decent estimate of density in high dimensions, the number of samples needed is exponential in dimensions. For the density estimation task, dimensions of 4-50 are considered large. (Wang & Scott, 2019)\n\nfunction.\n\n3.2 COUNT SKETCHES\n\nFigure 1: Countsketch, sketching, and query\n\nThe count sketch (CS) (Cormode & Muthukrishnan, 2009; Charikar et al., 2002), along with its variants, is one of the most popular probabilistic data structures used for the heavy hitter problem. Given a stream of (at, ct) key-value pairs, at ∈ U, CS stores the compressed total counts for each of the keys in a small K × R array of integers and can be queried to retrieve this total count, C(at). CS offers a probabilistic solution in memory logarithmic in a total number of unique keys. There is a standard memory accuracy trade-off for CS. Let m be the number of distinct keys and C be the vector of counts indexed by each key. For count median sketch (Charikar et al., 2002), the ((cid:15), δ) guarantee P(| ˆC(a) − C(a)| > (cid:15)||C||2) ≤ δ is achieved using O( 1 δ (log m + log |U|)) space. (Chakrabati, 2020). As seen from the above equation, the approximation accuracy for a particular key depends on how it compares to the ||C||2. Specifically, CS can give an excellent approximation for keys with the highest values in a setting where most other keys have very low values. More discussion on CS can be found in Appendix F.\n\n(cid:15)2\n\n1\n\n3.3 LOCALITY SENSITIVE HASHING\n\nLocality-sensitive hashing(LSH)(Darrell et al., 2005) is a popular approach to solving approximate near-neighbor problems. If a function h : U → {0, ...r − 1} for some r, is randomly drawn from the LSH family L, the probability of collision of the hash values for two distinct elements a1 and a2 is\n\nPh∈L(h(a1)==h(a2)) ∝ Sim(a1, a2)\n\nWhere Sim(a1, a2) is some similarity metric corresponding to the LSH family. The probability of collision is referred to as the kernel of the LSH family, generally denoted by φ(., .). Most kernels are positive, bounded, symmetric, and reflective. We can use p independent LSH functions, h1, h2, ...hp to obtain a LSH function, h(p)(a) = (h1(a), h2(a), ..., hp(a)). The function h(p) has kernel ψ(, ., ) = φ(., .)p. We call p the power of the LSH function. Popular LSH functions for U = Rd are L2-LSH, L1-LSH and SRP (signed random projection). More details on LSH functions can be found in (Darrell et al., 2005)\n\n3.4 UNIFORM SAMPLING FROM CONVEX POLYTOPES\n\nUniform sampling from convex spaces is a well-studied problem (B ́elisle et al., 1993; Chen et al., 2017). For general convex polytopes, this is achieved by finding a point inside the polytope using convex feasibility algorithms and then running an MCMC walk inside the polytope to generate a point with uniform probability. In the case of regular convex polytopes like hypercubes and parallelopiped, uniform sampling is much simpler. Sampling a data point at random in a d-dimensional hypercube of width 1 is equivalent to uniformly sampling d real values in the interval [0, 1]. For sampling within a d-dimensional parallelopiped, we first locate (d − 1)-dimensional hyperplane parallel to each face at a distance drawn uniformly from [0, B] where B is the width of parallelopiped in that direction. The sampled point is the intersection of these (d − 1)-dimensional hyperplanes.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: bin(x) for different partitioning schemes\n\nPartitioning Scheme\n\nParameters\n\nRegular histogram\n\nB ∈ R\n\nbin(x) : Rd → Nd\n\nbin(x)i = (cid:98)xi/B(cid:99)\n\nAligned histogram\n\nB ∈ Rd\n\nbin(x)i = (cid:98)xi/Bi(cid:99)\n\nL1/L2-LSH\n\nW ∈ Rd×d B ∈ Rd, t ∈ Rd\n\nbin(x)i = ((cid:98)(cid:104)x, Wi(cid:105) + ti)/Bi(cid:99)\n\nSRP\n\nW ∈ Rk×d\n\nbin(x)i = (sign((cid:104)x, Wi(cid:105))\n\nSampling s ∈ Rd from b ∈ Nd ri ∼ U (0, 1), r ∈ Rd s = B(b + r) ri ∼ U (0, 1), r ∈ Rd s = B ◦ (b + r) ri ∼ U (0, 1), r ∈ Rd y = B ◦ (b + r) s = solve(Ws = y − t) MCMC with constraints, sign((cid:104)x, Wi(cid:105)) = bi + bounding box\n\n4 DENSITY SKETCHES In DS, we aim to build a compressed non-parametric estimation object in an efficient streaming fashion. As KDEs give a better approximation of underlying function f (x) than histograms, we want to build DS as a compressed KDE object. To achieve this, we use a nice connection between KDE and Histograms with an LSH-based partition function.\n\n4.1 HISTOGRAM WITH LSH-BASED PARTITION AND KERNEL DENSITY ESTIMATES Any LSH function on Rd will partition the space into different bins. Specifically, if power d L1/L2LSH, these partitions will be polytopes in Rd. Similarly, power k SRP would give conical partitions with hyper-plane boundaries. We can employ a histogram-based estimation strategy on the top of these randomly drawn partitions. The density estimate using such a histogram would be\n\nˆfH (x) ∝\n\n1 n\n\nn (cid:88)\n\ni=1\n\nI(xi ∈ bin(x)) where xi ∈ D\n\nwhere I is a indicator function. This estimate of the density has an expected value (over random partitions) equal to the KDE estimate, say ˆfφ(x) with the corresponding LSH kernel, φ(., .)\n\nEp( ˆfH (x)) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nP (xi ∈ bin(x)) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nφ(xi, x) = ˆfφ(x)\n\nThe expectation is over random partitions. This connection between randomized histograms and KDE was first observed in (Coleman & Shrivastava, 2020). To better approximate KDE, we can combine results from multiple histograms with independent LSH functions. For example, if we use m independent histograms, say H1, H2, ..., Hm, then the density estimate can be written as\n\nˆf (m) H (x) =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nˆfHi(x)\n\nWe can sample a data point from this set by first choosing a histogram randomly and then sampling a point from that histogram. One can check that the sampling distribution, thus obtained, is ˆf (m) H (x).\n\n4.2 CONSTRUCTING DENSITY SKETCHES\n\nNow that we have reduced the problem of KDE approximation to histograms, we will now show how to obtain a compressed representation of a histogram in a streaming fashion. We also show how to generate samples from this representation. First, let us establish some notation we will use\n\nNotation: (1) Data D consists of n i.i.d samples of dimension d drawn from true distribution f (x) : S ⊂ Rd → R. (2) bin(x): ID of the partition in which point x falls. In the case of p-power LSH functions, bin(x) : Rd → N p, and each bin can be identified with a unique tuple of p integers. In a regular histogram, we have a tuple of d integers. For example, in regular histogram with width B, bin(x)i = (cid:98)xi/B(cid:99). bin is generally parameterised with bandwidth parameter B which measures the size of the partition. Some partitioning schemes and sampling algorithms are mentioned in Table 1 (3) A CS M with range R and repetitions K as described in section 3. (4) H: Augmented min-heap of size H used with M. Hence for a given partitioning scheme (bin, B), DS is parameterized by (K, R, H) and includes two data structures M(K, R) and H(H).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of the sketching algorithm The histogram has an exponential (in d) number of partitions. Hence, in high dimensions, it is impractical to store histograms. However, most high dimensional real data is clustered and thus has highly sparse histograms. This does not help with histograms, as post-pruning of histograms still requires us to build and enumerate them. Nevertheless, the sparsity in histogram makes it a good candidate for heavy hitter problems. We use CS, M, to store a compressed version of the histogram. Unfortunately, sampling with just M does not have an efficient solution. We maintain a set of heavy partitions for sampling in the min-heap H. We will discuss sampling in later subsections.\n\nsketching M : As shown in figure 4 and algorithm 1, we process the data in a streaming fashion. For each data point, say x, we find the partition b = bin(x). We increment the count of b by 1 by inserting (b, 1) into M. Along with each insertion, we also update H. If the H is not at its capacity, we insert this b into the heap along with its updated count ˆC(b). If the heap is at its capacity, we check b’s updated count against the minimum of the H. If b’s count is greater, we pop the minimum element from the heap and insert (b, ˆC(b)).\n\n4.3\n\nˆfC(x): ESTIMATE OF DENSITY AT A POINT\n\nWe can use M for querying the density estimate at a particular point. The algorithm for querying is presented in Algorithm 2 and is explained in figure 2. Reusing notation from 3.1, the density predicted by the histogram can be written as ˆfH (x). When using the sketch, instead of actual C(bin(x)), we would use its estimate from M. Let this estimate be ˆC(bin(x)). Then we can write the density predicted using count-sketch as ˆfC(x)\n\nˆfH (x) =\n\nC(bin(x)) nV(bin(x))\n\nˆfC(x) =\n\nˆC(bin(x)) nV(bin(x))\n\nWe know from CS literature that ˆC(bin(x)) is closely distributed around C(bin(x)) and so we can expect ˆfC(x) to be close to ˆfH (x) and hence to f (x). Note that though ˆfC(x) is a good estimate of density at a point x, the function ˆfC(.) is not a density function as it does not integrate to 1.\n\nˆf ∗ C(x): ESTIMATE OF DENSITY FUNCTION\n\n4.4 To obtain a density function from the sketches, we have to normalize the function ˆfC(x) over the support. We can write ˆf ∗\n\nC(x) as\n\nˆf ∗ C(x) ∝ ˆC(x)\n\nˆf ∗ C(x) =\n\nˆC(x)) (cid:82) ˆC(x)dx\n\n=\n\nˆC(x)\n\nV(bin(x)) (cid:80)\n\nb∈bins(S)\n\nˆC(b)\n\n=\n\nˆC(x) V(bin(x))ˆn\n\nIt is easy to check the integral can be written as the sum over all the bins in the support. As is clear from the equations for ˆf ∗ ˆC(b) to get a density function. We can check that ˆn is an estimate of n using an estimate of count for each bin from the DS.\n\nb∈bins(S) C(b) , is replaced by ˆn = (cid:80)\n\nC(x) and ˆfC(x), n = (cid:80)\n\nb∈bins(S)\n\n4.5\n\nˆfS(x): SAMPLING FROM DENSITY SKETCHES\n\nM is a good enough representation for querying the density at a point. However, it is not the best data structure to generate samples efficiently. One naive way of sampling from these sketches is to\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Constructing density sketch\n\nof f (x)\n\nResult: Density Sketch (DS) f (x) : Rd → R : true distribution x1, . . . xn ∼ f (x) : sample drawn from f (x) bin(x) : S → N d: partition function M : CS with range R, repetitions K H(H) : min-heap to store top H partitions\n\nfor i ← 1 to n do b = binxi M.insert(b, 1) c = M.query(b) H.update(b, c)\n\nAlgorithm 2: query ˆfC(y), y ∈ Rd Result: ˆfC(y) y ∈ Rd b = biny c = M.query(b) return (c/(nV(b)))\n\nAlgorithm 3: sample y ∈ Rd such y ∼ ˆfS(x) Result: y: sample from fS(y) P : categorical distribution over bins s.t. P (b) = (H[b]/nh) if b ∈ H –\nP (b) = 0 if b /∈ H –\nb ∼ P y = UniformRandomPoint(b) return y\n\nrandomly select a point in support of f (x) and then do a rejection sampling using estimate ˆfC(x). However, given the enormous volume of support in high dimensions, this method is bound to be immensely inefficient. Another way is to choose a partition with probability proportional to the count of elements in that partition and then sample a random point from this chosen partition. It is easy to check that the probability of sampling a point x in this manner, precisely, is ˆfH (x) if we use exact counts and ˆf ∗ C(x) if we use approximate counts from CS. However, given that number of bins is exponential in dimension, sampling a bin proportional to its counts requires prohibitive memory and computation. This is why we needed a CS in the first place. Here, we further approximate the distribution by storing only top H partitions which contain most data points and discarding other partitions. As mentioned in 1, we can efficiently maintain top H partitions with an augmented heap H. We then sample a partition present in this heap with probability proportional to its count and sample a random data point from this partition (Algorithm 3). The probability of sampling a data point whose bin is not present augmented heap is then zero. The distribution of this sampling algorithm is,\n\nˆfS(x) = I(bin(x) ∈ H)\n\nˆC(bin(x)) ˆnhV(bin(x))\n\nb∈H\n\nC(x). Note that ˆfS(x) is a density function.\n\nwhere ˆnh = (cid:80) ˆC(b) is the count-sketch estimate of the total number of elements captured in all partitions present in the heap. I(.) is the indicator function with values 0 or 1 evaluating the boolean statement inside it. Let ρh = ˆnh/ˆn be the capture ratio of heap. It is easy to see that as the capture ratio tends to 1, ˆfS(x) tends to ˆf ∗ 5 ANALYSIS Histogram and Kernel Density Estimators are well-studied non-parametric estimators of density. Both of these estimators are shown to be capable of approximating a large class of functions (Scott, 2015). For example, with the condition of Lipschitz Continuity on f (x), we can prove that pointwise MSE( ˆfH (x) converges to 0 at a rate of O(n−2/3). Better results can be obtained for functions that have continuous derivatives. In our analysis, we make assumptions along those made in (Scott, 2015); specifically, the existence and boundedness of all function-dependent terms that appear in the theorems below. We refer the reader to (Scott, 2015) for an in-depth discussion on assumptions.\n\nWe restrict our analysis to convergence in probability for all the estimators discussed in this paper, which is the standard (Scott, 2015). In this section, we consider the regular histogram partitioning scheme and show that our sampling distribution ˆfS(x) is an approximation of underlying distribution f (x) and converge to it. However, a similar analysis holds even for random partitioning schemes / KDE and is skipped here.\n\nMean integrated square error(MISE): MISE of an estimator of function is a widely used tool to analyze the performance of a density estimator. (cid:20)(cid:90)\n\n(cid:21)\n\nMISE( ˆf ) = E\n\n( ˆf (x) − f (x))2dx\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nA density estimator, with MISE asymptotically tending to zero, is a consistent estimator of true density and converges to it in probability. We would use this tool to make statements about the convergence of our estimators. By Fubini’s theorem, MISE is equal to IMSE (Integrated mean square error).\n\nMISE( ˆf ) = IMSE( ˆf ) =\n\n(cid:90)\n\n(cid:104)\n\n(cid:105) ( ˆf (x) − f (x))2)\n\nE\n\ndx\n\nWe now present our main result of the paper, Theorem 1 (Main Theorem: ˆfS(x) to f (x) ). The probability density function of sampling, ˆfS(x), using a DS over regular histogram of width B, with parameters(K,R,H) created with n i.i.d samples from original density function f (x), has an IMSE given by\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3(1 + 2(cid:15))\n\n(cid:18) 1\n\nnBd +\n\nG(f ) n\n\n+ o\n\n+ 3(1 + 3(cid:15))\n\n(cid:18) B2d 4\n\n(cid:19)\n\n(cid:18)\n\nG((cid:107)∇f (cid:107)2)\n\n+ 3(cid:15)\n\n1 + 2G(f ) + B\n\n(cid:19)\n\n(cid:18) 1 n\n(cid:90)\n\n√\n\nd\n\n(cid:19)\n\n+\n\nnnzp − 1 KRnBd\n\n(cid:19)\n\n(f (x)(cid:107)∇f (cid:107)2)\n\nx∈S\n\nwith probability (1 − δ) , where δ = nnzp (cid:15)2nKR , nnzp is the number of non-empty bins in histogram, ρh is the estimated capture ratio as described in section 4.5 and G(g) is roughness defined as (cid:82) g(x)2dx\n\nThe dependence of IMSE on properties of f (x), such as roughness, is standard (Scott, 2015) and cannot be avoided. Interpretation The estimator ˆfS(x) of f (x) is obtained by a series of approximations from f (x) → C(x) → ˆfS(x). Hence to interpret this result, we break down the result above ˆfH (x) → ˆfC(x) → ˆf ∗ into multiple theorems enabling the reader to easily notice which step of approximations leads to what terms in the theorem above. We provide these details in Appendix. We notice a few things from the theorem below\n\n• Similar to the standard analysis for histograms, the curse of dimensionality also manifests in our theorem. B should go to zero and n should increase faster than the rate at which Bd/nnzp decreases (condition 1). As compared to standard histograms, this requires n to grow faster. With these conditions on B and n, it is clear how the second and third terms go to zero.\n\n• The magnitude of the fourth term is controlled via (cid:15). The above statement is true for any δ and (cid:15) that are related via the expression δ = (cid:0)nnzp/((cid:15)2nKR)(cid:1). Choose arbitrarily small (cid:15) and δ, and we can achieve it with large enough n/nnzp or by providing more intermediate resources and making KR large enough. For a fixed resource KR, this term goes to zero asymptotically with n growing faster than nnzp, which is a sub-condition of condition 1.\n\n• The term 12(1 − ρh)2 shows the effect of truncation that occurs due to using only heavy partitions. As can be seen, this term is data dependent, and IMSE does not depend directly on H (number of partitions) but ρh. Suppose we can capture the entire data in the heap (i.e., setting H=nnzp), then the term adds no penalty to IMSE. H, via ρh controls the accuracy-memory trade-off of DS.\n\n6 DISCUSSION curse of dimensionality: As DS are built over Histograms, they inherit the curse from Histograms: i.e., the number of samples needed increases exponentially with dimension. With increased data collection, the issue of the unavailability of large amounts of data is fast vanishing. We want to emphasize that DS’ advantages are best seen when data is humongous. DS can absorb tons of data and give better density estimates and samples without increasing memory usage. Also, most real data in high dimensions is clustered or stays on a low-dimensional manifold. DS, throw away empty bins, and only store the histogram’s populated bins. DS can deal with the curse of dimensionality better than Histograms. DS on original data space: Some data types, like images, do not reside in a space where the usual distances or cosine similarities imply conceptual similarity. On these data types, DS will not perform well. One way is to learn a transformation and create sketches of the transformed data. While this will give better performance in practice, we might lose theoretical guarantees for certain transformations. 7 EXPERIMENTS Visualization of samples from density sketches: In the first set of experiments, we provide a sanity check for DS in the form of visualization of data generated from DS. (1) In the first experiment,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) sample drawn from true distribution\n\n(b) sample drawn from Density Sketch\n\n(c) MNIST Sample from Density Sketch using L2LSH partitioning width: 0.01,0.1,1,10 from top to bottom\n\n(d) MNIST Sample from Density Sketch using conical partitions (multiple signed random projections (SRP)). The rows have varying number of SRPs used - increasing from top to bottom\n\nFigure 4: Visualization of samples drawn from Density Sketch. (a-b) DS captures density information. (c) Higher power LSH functions lead to fine DS behaving like random sample (d) Coarser partitions lead to samples that resemble “avg” of samples in data\n\nfigure 4 (a) shows the samples drawn from the actual multi-gaussian distribution, whereas figure 4 (b) shows the samples drawn from the DS built on the samples from the true distribution. In this experiment, we use L2-LSH partitions with a B=0.25, K=3, R=50000, and H=3000. As can be seen, the two samples are indistinguishable. So DS does capture the density information. (2) Figure 4 (c) shows some samples drawn from DS built over the MNIST dataset (Chang & Lin, 2011) with varying bin-width sizes. (again R=50000, K=3 and H=5000) . We should notice that MNIST with 784 dimensions and 60K samples is not an ideal dataset for DS. In fact, with L2-LSH partitions the data would be so scattered that every sample is contained in its bin. If we make the bin-width finer, we should sample data points very close to the random sample from the original data. So in the worst conditions, DS converges to a random sample which we know is a good representation of data. 4 (d) shows results again with MNIST. However, here we have created conical partitions (created using multiple signed random projections). While L2-LSH partitions use power 784 L2-LSH functions, in this experiment, we use a smaller number of SRP functions(10-25, increases as we go to lower rows in the image), thus promoting clustering. As expected, this coarse partitioning does show significant clustering; hence, the images drawn from the partitions look like the average of multiple samples from the original data. The results support that DS can create samples that resemble the original data.\n\nEvaluation of Samples on Classification Tasks: For most datasets, it is not possible to inspect samples visually. Hence we evaluate the quality of samples from DS by using them to train classification models. In these experiments, the data loader of the training algorithm is replaced with a sampler from DS. This sampler returns a training batch when requested by the algorithm. All the experiments are performed on Tesla P-100 GPUs with 16GB memory. Datasets: We choose big datasets from the liblinear website (Chang & Lin, 2011), which satisfy the constraints of 1) data dimension less than 100 and 2) the number of samples per class greater than 1,000,000. Large Datasets is the main application domain for DS. Thus, we have datasets of Higgs (10M samples, 28 dimensions) and Susy (5M samples, 18 dimensions) for our experiments. Baselines: For baselines, we consider random samples of the same size and Liberty coresets proposed by (Karnin & Liberty, 2019) to compare DS performance. For Liberty coresets, we use m = 100 as for larger m the process is very slow. Dimensionality reduction via random projections is another streaming algorithm. Still, in these datasets, we cannot get significant compression using\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) higgs test accuracy\n\n(b) higgs test loss\n\n(c) susy test accuracy\n\n(d) susy test loss\n\n(e) varying K,R, with KR=1000000\n\n(f) varying B\n\nFigure 5: (a-d) Performance of density sketches, Liberty coresets and random sample on downstream classification task. The bar widths refer to 2 × std. DS performs consistently better at all memory sizes (e) Performance of DS is stable with K,R configs for decent budget KR=1000000, (f) Optimal B exists for DS performance. larger and smaller values of B lead to performance degradation\n\ndimensionality reduction (it can be d× at best, where d is the dimension). So we cannot compare against this method. For a more detailed discussion on baselines, refer to appendix D. Results: The DS has parameters: partition function(bandwidth B). We use L2-LSH partitions, sketch parameters K, R, and heap parameter H in all our experiments. The memory of the DS used for sampling is affected by only the heap parameter (see appendix F for details on memory computation). In figure 5(f), we use the config (K=4,R=250000,H=100000) and vary B. It is clear from the figure that B=0.001 works best for these datasets. Lower and higher values of B affect the performance adversely. Larger B implies that we will capture more space than needed in a single partition, and smaller B implies that we will capture lesser data in the heap. So it is expected that a sweet spot for B exists. In figure 5(e), we fix (B=0.01,H=100000, KR=1000000) and vary K from 1 to 64. We can see that for a reasonable memory budget the results are stable with varying K. For the experiments in figure 5(a-d) we fix (B=0.01, K=5, R=250000) and vary H. This gives us DS of different sizes. We plot test accuracy and losses for DS, random samples and Liberty coresets for different sizes of memory used. The width of the band signifies the 2 × std-dev of performance on three independent runs. As can be seen, for the “Higgs” dataset, the model’s accuracy achieved on original data of size 2.5GB can be closely reached by using a DS of size 50MB. So we get around 50x compression ! We see similar results for datasets of “Susy” (100x compression, 0.8GB) as well. The results show that DS is much more informative than Random Sample and Liberty Coresets. For more details on running the experiments (data processing, memory measurements, etc.), refer to Appendix F.\n\nEstimation of statistical properties of dataset: We also perform the experiments on the covariance estimation task. The observations are similar to the classification experiment. DS performs better than the corresponding random sample. The results are presented in Appendix F for the shortage of space.\n\n8 CONCLUSION\n\nIn this paper, we talk about Density Sketches, a streaming algorithm to construct a summary of density distribution from data. We show that new samples generated from this sketch asymptotically converge to the underlying distribution. Thus, DS comes with theoretical guarantees. Additionally, the cheap nature of online updates in Density Sketches, makes it an attractive alternative to constructing coresets for the data. In terms of coresets, DS can be viewed as a compressed form of randomized grid-coresets - one of the oldest forms of coresets.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary\n\ncoins. Journal of computer and System Sciences, 66(4):671–687, 2003.\n\nClaude JP B ́elisle, H Edwin Romeijn, and Robert L Smith. Hit-and-run algorithms for generating\n\nmultivariate distributions. Mathematics of Operations Research, 18(2):255–266, 1993.\n\nAmit Chakrabati. https://www.cs.dartmouth.edu/∼ac/ Teach/data-streams-lecnotes.pdf, volume 1. 2020.\n\nChih-Chung Chang and Chih-Jen Lin. Libsvm : a library for support vector machines. In Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm. ACM Transactions on Intelligent Systems and Technology, 2011.\n\nMoses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high dimensions. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pp. 1032–1043. IEEE, 2017.\n\nMoses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pp. 693–703. Springer, 2002.\n\nYuansi Chen, Raaz Dwivedi, Martin J Wainwright, and Bin Yu. Vaidya walk: A sampling algorithm based on the volumetric barrier. In 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 1220–1227. IEEE, 2017.\n\nYutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. arXiv preprint\n\narXiv:1203.3472, 2012.\n\nBenjamin Coleman and Anshumali Shrivastava. Sub-linear race sketches for approximate kernel density estimation on streaming data. In Proceedings of The Web Conference 2020, pp. 1739– 1749, 2020.\n\nGraham Cormode and M Muthukrishnan. Count-min sketch., 2009.\n\nEfren Cruz Cortes and Clayton Scott. Sparse approximation of a kernel mean. IEEE Transactions\n\non Signal Processing, 65(5):1310–1323, 2016.\n\nTrevor Darrell, Piotr Indyk, and Gregory Shakhnarovich. Nearest-neighbor Methods in Learning\n\nand Vision: Theory and Practice. MIT Press, 2005.\n\nJerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,\n\nvolume 1. Springer series in statistics New York, 2001.\n\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.\n\nMIT press Cambridge, 2016.\n\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.\n\nZohar Karnin and Edo Liberty. Discrepancy, coresets, and sketches in machine learning. In Confer-\n\nence on Learning Theory, pp. 1975–1993. PMLR, 2019.\n\nJeff M Phillips and Wai Ming Tai. Improved coresets for kernel density estimates. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 2718–2727. SIAM, 2018.\n\nJeff M Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. Discrete &\n\nComputational Geometry, 63(4):867–887, 2020.\n\nDavid W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley &\n\nSons, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDW Scott and SR Sain. Multi-dimensional density estimation handbook of statistics vol 23 data\n\nmining and computational statistics ed cr rao and ej wegman, 2004.\n\nZhipeng Wang and David W Scott. Nonparametric density estimation for high-dimensional data—algorithms and applications. Wiley Interdisciplinary Reviews: Computational Statistics, 11(4):e1461, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2\n\nDescription true density distribution histogram estimate of f(x) partition id of point x enumerates all bins in the support S count of elements in the bin id b count of elements in the bin(x) estimate count of elements in the bin id b volume of bin id b data kernel function : R × R → R B ∈ R, scalar bin width B ∈ Rd vector of bin widths in different directions U{0, ...R − 1} hash function U → {−1, +1} hash function\n\nNotation f (x) ˆfH (x) bin(x) bins(S) C(b) C(x) ˆC(b) V(b) D\nk(, ) B\nB h(x) g(x) W ∈ Rk×d weight matrix for SRP / L1-L2 LSH I\nE ˆfφ(x) M\nK, R n\nˆn nh ˆnh ρh\n\nindicator function Expected value KDE estimate with kernel φ Count sketch parameters of count sketch total number elements CS estimate of total number of elements number of elements in the heap CS estimate of number of elements in the heap capture ratio defined by ˆnh/ˆn\n\nA APPENDIX\n\nA.1 NOTATION\n\nA.2 THEOREM 2 : fH TO f\n\nWhile estimating true distribution f (x) : Rd → R, the integrated mean square error (IMSE) for the estimator ˆfH (x) using regular histogram with width h and number of samples n, is\n\nIMSE( ˆfH ) ≤\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nSpecifically, we have integrated variance (IV) and integrated square bias (ISB) as follows\n\nIV( ˆfH ) =\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\nand\n\nISB( ˆfH ) ≤\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nwhere G(φ) is the roughness of the function φ defined as G(φ) = (cid:82) φ2(x)dx\n\nProof. Let x ∈ S where S is the support of the distribution. Let bin(x) determine the bin of point x, bins(S) enumerate all the bins that lie inside the support S of the distribution f (x) . Let V (bin(x)) is volume of bin in which x lies. Equivalently, we can also use V (b) to denote volume of bin b. For standard histogram, V (b) = hd\n\nˆfH (x) =\n\n1 nV (bin(x))\n\nn (cid:88)\n\ni=1\n\n12\n\nI(xi ∈ bin(x))\n\n(1)\n\nUnder review as a conference paper at ICLR 2023\n\nFirst let us consider the integrated variance.\n\n(cid:90)\n\nIV =\n\nx∈S\n\nVar( ˆfH (x))dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(S)\n\nx∈b\n\nVar( ˆfH (x))dx\n\n(2)\n\nFor a particular bin b, the variance is constant at all values of x inside it. Also for a particular x in bin b, we can write the following for Var( ˆfH (x)) using independence of samples.\n\nVar( ˆfH (x)) =\n\n1\n\nnV (bin(x))2 Var(I(xi ∈ bin(x))\n\n(3)\n\nAlso Var(I(xi ∈ b)) = pb(1 − pb) where pb is the probability of xi lying in bin b. That is, pb = (cid:82)\n\nx∈b f (x)dx Using this in equation 2\n\nSimplifying,\n\n(cid:88)\n\nIV =\n\nb∈bins(S)\n\nV (b)\n\n1 nV 2(b)\n\npb(1 − pb)\n\n(cid:88)\n\nIV =\n\nb∈bins(S)\n\n1 nV (b)\n\npb(1 − pb)\n\nFor standard histogram V (b) is same across bins,\n\nIV =\n\n1 nV (b)\n\n(cid:88)\n\n(\n\npb −\n\n(cid:88)\n\np2 b)\n\nb∈bins(S)\n\nb∈bins(S)\n\n=\n\n1 nV (b)\n\n(1 −\n\n(cid:88)\n\np2 b)\n\nb∈bins(S)\n\nUsing mean value theorem, we can write, pb = V (b)f (ξb) for some point ξb ∈ b.\n\n(cid:88)\n\np2\n\nb =\n\n(cid:88)\n\nb∈bins\n\nb∈bins\n\nV (b)2f (ξb)2 = V (b)\n\n(cid:88)\n\nb∈bins\n\nV (b)f (ξb)2\n\nUsing Riemann Integral approximation , we can write the following as the bin size reduces,\n\n(cid:88)\n\nb∈bins\n\nV (b)f (ξb)2 =\n\n(cid:90)\n\nx∈S\n\nf 2(x)dx + o(1)\n\n(cid:82)\n\nx∈S f 2(x)dx is also known as the roughness of the function. Let us denote it using G(f ). Hence\n\nPutting V (b) = hd\n\nIV =\n\n1 nV (b)\n\n(1 − V (b) (G(f ) + o(1)))\n\nIV =\n\n1 nV (b)\n\n−\n\nG(f ) n\n\n− o\n\n(cid:19)\n\n(cid:18) 1 n\n\nIV =\n\n1\n\nnhd −\n\nG(f ) n\n\n− o\n\n(cid:19)\n\n(cid:18) 1 n\n\nKeeping only the leading term in the above expression, (cid:18) 1 nhd\n\nIV = O\n\n(cid:19)\n\nNow let us look at the ISB for this estimator, ISB( ˆfH (x))\n\nISB( ˆfH (x)) =\n\n(cid:90)\n\nx∈S\n\n(E( ˆfH (x) − f (x)))2dx\n\n13\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(4)\n\n(5)\n\n(6)\n\n(7)\n\n(8)\n\n(9)\n\nUnder review as a conference paper at ICLR 2023\n\nLet us look at the expected value of the estimator,\n\nE( ˆfH (x)) =\n\n1 V (bin(x))\n\n(cid:90)\n\nt∈bin(x)\n\nf (t)dt\n\n(15)\n\nRecall that x ∈ Rd. Using 2nd order multivariate taylor series expansion of this f (t) around x, we get,\n\nf (t) = f (x) + (cid:104)t − x, ∇f (x)(cid:105) +\n\n1 2\n\n(t − x)(cid:62)H(f (x))(t − x)\n\n(16)\n\nHere H(f (t)) is the hessian of f at t. Without the loss of generality let us look at the bin(x) = [0, h]d that is the bin at the origin. Let us say it is b0 (cid:90)\n\nf (t)dt = hdf (x) + hd(cid:104)(\n\n− x, ∇f (x)(cid:105) + O(hd+2)\n\n(17)\n\nh 2\n\nUsing eq 17 in eq 15, we get\n\nt∈b0\n\nE( ˆfH (x)) = f (x) + (cid:104)(\n\nh 2\n\n− x), ∇f (x)(cid:105) + O(h2)\n\nHence, just keeping the leading term , we have\n\nBias( ˆfH (x)) = (cid:104)(\n\nh 2\n\n− x), ∇f (x)(cid:105)\n\nNow,\n\n(cid:90)\n\nx∈b0\n\nBias( ˆfH (x))2dx =\n\n(cid:90)\n\n(cid:18) (cid:104)\n\n(cid:18) h 2\n\n(cid:19)\n\n(cid:19)2\n\n− x\n\n, ∇f (x)(cid:105)\n\ndx\n\nUsing Cauchy-Schwarz inequality, we get (cid:90)\n\nBias( ˆfH (x))2dx ≤\n\nx∈b0\n\nx∈b0\n\nx∈b0\n\n(cid:90)\n\n(cid:107)(\n\nh 2\n\n− x)(cid:107)2\n\n2(cid:107)∇f (x)(cid:107)2\n\n2dx\n\n√\n\nAs [h/2, h/2, ...h/2] is a mid point of the bin. The max norm of x − h/2 can be h\n\n(cid:90)\n\nx∈b0\n\nBias( ˆfH (x))2dx ≤\n\nh2d 4\n\n(cid:90)\n\nx∈b0\n\n(cid:107)∇f (x)(cid:107)2\n\n2dx\n\nNow looking at ISB\n\nISB( ˆfH ) =\n\n(cid:90)\n\n(cid:88)\n\nb∈bins\n\nx∈b0\n\nBias( ˆfH (x))2dx ≤\n\nh2d 4\n\n(cid:90)\n\nx∈S\n\n(cid:107)∇f (x)(cid:107)2\n\n2dx\n\nISB( ˆfH ) ≤\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\nd/2\n\nA.3 THEOREM 3: fC TO fH\n\nWhile estimating true distribution f (x) : Rd → R, the integrated mean square error (IMSE) for the estimator ˆfC(x) using regular histogram with width h, number of samples n, and countsketch with range R, repetitions K and mean recovery, is\n\nwhere nnzp is the number of non-zero partitions. Specifically, we have\n\nIMSE( ˆfC) = IMSE( ˆfH ) +\n\nnnzp KRnhd\n\nand\n\nIV( ˆfC) = IV( ˆfH ) +\n\nnnzp − 1 KRnhd\n\nISB( ˆfC) = ISB( ˆfH )\n\nwhere nnzp is the number of non-zero bins/partitions.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Consider a Countsketch with range R and just one repetition (i.e. K = 1). Let it be parameterized by the randomly drawn hash functions g : bins(S) −→ {0, 1, 2, ..., R − 1} and s : bins(S) −→ {−1, +1}. Let C(bin(x)) (cid:80)n i=1(I(xi ∈ bin(x)) is the count of elements that lie inside the bin(x)\n\nThe estimate of density at point x can then be written as\n\nˆfC(x) =\n\n1 nV (bin(x))\n\n(cid:32)\n\nC(bin(x)) +\n\nn (cid:88)\n\n(cid:16)\n\nI\n\ni=1\n\nxi /∈ bin(x) ∧ g(bin(xi)) = g(bin(x))\n\n(cid:17)\n\n(cid:33)\n\ns(bin(xi))s(bin(x))\n\n(25)\n\nWe can rewrite this as ,\n\nˆfC(x) = ˆfH (x)+\n\n1 nV (bin(x))\n\n(cid:32) n\n\n(cid:88)\n\n(cid:16)\n\nI\n\ni=1\n\nxi /∈ bin(x) ∧ g(bin(xi)) = g(bin(x))\n\n(cid:17)\n\ns(bin(xi))s(bin(x))\n\n(cid:33)\n\nAs E(s(b)) = 0, it can be clearly seen that.\n\nHence, it follows that\n\nE( ˆfC(x)) = E( ˆfH (x))\n\nISB( ˆfC(x)) = ISB( ˆfH (x))\n\nIt can be checked that each of the terms in the summation for right hand side of equation 26 including the terms in ˆfH (x) are independent to each other . i.e. covariance between them is zero. Hence we can write the variance of our estimator as,\n\nVar( ˆfC(x)) = Var( ˆfH (x))+\n\n1 nV 2(bin(x))\n\n(cid:16)\n\n(cid:16)\n\nI\n\nVar\n\nxi /∈ bin(x) ∧ g(bin(xi))=g(bin(x))\n\n(cid:17)\n\ns(bin(xi))s(bin(x))\n\n(cid:17)\n\nVar( ˆfC(x)) = Var( ˆfH (x))+\n\n1 nV 2(bin(x))\n\nE\n\n(cid:18)\n\nI\n\n(cid:16)\n\nxi /∈ bin(x) ∧ g(bin(xi))=g(bin(x))\n\n(cid:17)2(cid:19)\n\nAs square of indicator is just the indicator,\n\nVar( ˆfC(x)) = Var( ˆfH (x))+\n\n1 nV 2(bin(x))\n\n(cid:16)\n\n(cid:16)\n\nI\n\nE\n\nxi /∈ bin(x) ∧ g(bin(xi))=g(bin(x))\n\n(cid:17)(cid:17)\n\nV ar( ˆfC(x)) = V ar( ˆfH (x)) +\n\n1 nV 2(bin(x))\n\n(1 − pbin(x))\n\n1 R\n\n)\n\nHence, IV is\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n(cid:90)\n\nx∈S\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n1 nV 2(bin(x)) 1\nnV 2(b)\n\n(cid:90)\n\nx∈b\n\n(cid:88)\n\nb∈bins(S)\n\n(1 − pbin(x))\n\n1 R\n\n)\n\n(1 − pb)\n\n1 R\n\n)\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n(cid:88)\n\nb∈bins(S)\n\n1 nV (b)\n\n(1 − pb)\n\n1 R\n\n)\n\n15\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\nUnder review as a conference paper at ICLR 2023\n\nAssuming standard partitions. V (b) = hd for all b\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n1 nhd\n\n(nnzp − 1) R\n\nWith mean recovery, with K repetitions, the analysis can be easily extended to get IV as\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n1 nhd\n\n(nnzp − 1) KR\n\n(39)\n\n(40)\n\nThe ISB remains same in this case.\n\nA.4 THEOREM 4: ˆf ∗\n\nC TO ˆfC\n\nWhile estimating true distribution f (x) : Rd → R, the IMSE for the estimator ˆf ∗ C(x) using regular histogram with width h and number of samples n and countsketch with parameters (R:range, K:repetitions), is related to the estimator ˆfC(x) as follows\n\nIMSE( ˆfC(x)) − (cid:15)(N + 2M ) ≤ IMSE( ˆf ∗\n\nC(x)) ≤ IMSE( ˆfC(x)) + (cid:15)(N + 2M )\n\nSpecifically,\n\nand\n\nwhere\n\nIV( ˆfC(x)) − 2(cid:15)M ≤ IV( ˆf ∗\n\nC(x)) ≤ IV( ˆfC(x)) + 2(cid:15)M\n\nISB( ˆfC(x)) − (cid:15)N ≤ ISB( ˆf ∗\n\nC(x)) ≤ ISB( ˆfC(x)) + (cid:15)N\n\nM ≤ IV( ˆfC(x)) + 2(G(f ) +\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\nN = (1 + ISB( ˆfC(x)))\n\nwith probability (1 − δ) where δ = nnzp\n\n(cid:15)2nKR\n\nProof. Let us look at the estimator\n\nˆf ∗ C(x) =\n\nˆC(bin(x)) V(bin(x)) (cid:80)\n\nb\n\nˆC(b)\n\n= ˆfC(x) ∗\n\nn ˆn\n\n(41)\n\nwhere ˆn = (cid:80)\n\nb\n\nˆC(b) and n = (cid:80)\n\nb C(b)\n\nˆn and its relation to n: Let us first analyse ˆn and how it is related to n.\n\nˆn =\n\n(cid:88)\n\nˆC(b) =\n\n(cid:88)\n\nn (cid:88)\n\n(cid:16)\n\nI(xi ∈ b) +\n\n(cid:16)\n\nI(cid:0)xi /∈ b ∧ g(bin(xi)(cid:1) == g(b))s(bin(xi))s(b)\n\n(cid:17)(cid:17)\n\nb∈bins(S)\n\nb∈bins(S)\n\ni=1\n\nˆn =\n\n(cid:88)\n\nb,i\n\nI(xi ∈ b) + I(xi /∈ b ∧ g(bin(xi)) == g(b))s(bin(xi))s(b)\n\n(42)\n\n(43)\n\nNote that E(ˆn) = n. For varaince, observe that most of the terms in the summation have covariance 0, except the terms Cov(I(xi ∈ b1), I(xi ∈ b2)) which are negatively correlated. Hence\n\nV ar(ˆn) =\n\n(cid:88)\n\nb,i\n\nV ar(I(xi ∈ b)) + V ar(I(xi /∈ b ∧ g(bin(xi))! = g(b))s(bin(xi))s(b))+\n\n(cid:88)\n\n2\n\ni,b1,b2,b1(cid:54)=b2\n\nCov(I(xi ∈ b1), I(xi ∈ b2))\n\nWe know that\n\nV ar(I(xi ∈ b)) = pb(1 − pb)\n\n(44)\n\nV ar(I(xi /∈ b ∧ g(bin(xi)) == g(b))s(bin(xi))s(b)) = E(I(xi /∈ b ∧ g(bin(xi))! = g(b))2) =\n\n1 − pb R\n\nCov(I(xi ∈ b1), I(xi ∈ b2)) = −pb1pb2\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nHence, we pluggin in the values in previous equation ,\n\nV ar(ˆn) = n\n\n(cid:88)\n\nb\n\npb(1 − pb) + n\n\n1 − pb R\n\n(cid:88)\n\nb\n\n− 2n\n\n(cid:88)\n\npb1 pb2\n\nb1,b2,b1(cid:54)=b2\n\n(cid:88)\n\n− 2n\n\npb1 pb2\n\nb1,b2 (cid:88)\n\npb1pb2 )}\n\nV ar(ˆn) = n(1 −\n\nV ar(ˆn) = n{(1 +\n\n(cid:88)\n\nb\n\n(cid:88)\n\nb\n\np2\n\nb) + n\n\n1 − pb R\n\n(cid:88)\n\nb\n\n1 − pb R\n\n(cid:88)\n\n− (\n\np2\n\nb) − 2n\n\nV ar(ˆn) = n{(1 +\n\n(cid:88)\n\nb\n\nb 1 − pb R\n\nb1,b2\n\n(cid:88)\n\n− (\n\npb)2}\n\nb\n\nV ar(ˆn) = n{\n\n1 − pb R\n\n}\n\n(cid:88)\n\nb\n\nV ar(ˆn) =\n\nn(nnzp − 1) R\n\n<\n\nn(nnzp) R\n\nUsing Chebyshev’s inequality , we have\n\nP (|ˆn − n| > (cid:15)n) ≤\n\nV ar(ˆn) (cid:15)2n2\n\nHence with probability (1 − δ), δ = nnzp\n\n(cid:15)2nR , ˆn is within (cid:15) multiplicative error.\n\nP (|ˆn − n| > (cid:15)n) ≤\n\nnnzp (cid:15)2nR\n\nrelation of pointwise Bias and ISB With probability 1 − δ, ˆfC(x) 1 − (cid:15)\n\nˆfC(x) 1 + (cid:15)\n\nC(x) ≤\n\n≤ ˆf ∗\n\nAs expectations respect inequalities\n\nE( ˆfC(x)) 1 + (cid:15)\n\n≤ E( ˆf ∗\n\nC(x)) ≤\n\nE( ˆfC(x)) 1 − (cid:15) E( ˆfC(x)) 1 − (cid:15)\n\nE( ˆfC(x)) 1 + (cid:15)\n\n− f (x) ≤ Bias( ˆf ∗\n\nC(x)) ≤\n\n− f (x)\n\nBias( ˆfC(x)) − (cid:15)f (x) 1 + (cid:15) Bias( ˆfC(x)) − (cid:15)f (x) 1 + (cid:15)\n\n≤ Bias( ˆf ∗\n\nC(x)) ≤\n\n≤ Bias( ˆf ∗\n\nC(x)) ≤\n\nBias( ˆfC(x)) + (cid:15)f (x) 1 − (cid:15) Bias( ˆfC(x)) + (cid:15)f (x) 1 − (cid:15)\n\nIntegrating expressions again respects inequalities\n\nISB( ˆfC(x)) − (cid:15) (cid:82) f (x) 1 + (cid:15)\n\n≤ ISB( ˆf ∗\n\nC(x)) ≤\n\nISB( ˆfC(x)) + (cid:15) (cid:82) f (x) 1 − (cid:15)\n\nISB( ˆfC(x)) − (cid:15) 1 + (cid:15)\n\n≤ ISB( ˆf ∗\n\nC(x)) ≤\n\nISB( ˆfC(x)) + (cid:15) 1 − (cid:15)\n\nUsing first order taylor expansion of\n\n1\n\n1+(cid:15) and ignore square terms\n\n(45)\n\n(46)\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\n(51)\n\n(52)\n\n(53)\n\n(54)\n\n(55)\n\n(56)\n\n(57)\n\n(58)\n\n(59)\n\n(1 − (cid:15))ISB( ˆfC(x)) − (cid:15) ≤ ISB( ˆf ∗\n\nISB( ˆfC(x)) − (cid:15)(1 + ISB( ˆfC(x))) ≤ ISB( ˆf ∗ Hence,\n\nC(x)) ≤ (1 + (cid:15))ISB( ˆfC(x)) + (cid:15)\n\n(60) C(x)) ≤ ISB( ˆfC(x)) + (cid:15)(1 + ISB( ˆfC(x))) (61)\n\nISB( ˆfC(x)) − (cid:15)N ≤ ISB( ˆf ∗\n\nC(x)) ≤ ISB( ˆfC(x)) + (cid:15)N\n\n(62)\n\nwhere\n\nN = (1 + ISB( ˆfC(x)))\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nPoint wise variance and IV Using the similar arguments\n\nE( ˆf 2 C(x)) (1 + (cid:15))2 −\n\nE2( ˆfC(x)) (1 − (cid:15))2 ≤ V ar( ˆf ∗\n\nC(x)) ≤\n\nE( ˆf 2 C(x)) (1 − (cid:15))2 −\n\nE2( ˆfC(x)) (1 + (cid:15))2\n\n(63)\n\nAgain making first order taylor expansions of denominator and ignoring square terms\n\nV ar( ˆfC(x))−2(cid:15)(E( ˆf 2\n\nC(x)+E2( ˆfC(x))) ≤ V ar( ˆf ∗\n\nC(x)) ≤ V ar( ˆfC(x))+2(E( ˆf 2\n\nC(x)+E2( ˆfC(x)))\n\n(64)\n\nSince, V ar( ˆfC(x)) = E( ˆf 2\n\nC(x)) − E2( ˆfC(x))\n\nV ar( ˆfC(x))−2(cid:15)(V ar( ˆfC(x))+2E2( ˆfC(x))) ≤ V ar( ˆf ∗\n\nC(x)) ≤ V ar( ˆfC(x))+2(cid:15)(V ar( ˆfC(x))+2E2( ˆfC(x)))\n\nIV ( ˆfC(x))−2(cid:15)(IV ( ˆfC(x))+2\n\n(cid:90)\n\nx∈S\n\nE2( ˆfC(x))) ≤ IV ( ˆf ∗\n\nLet us now figure out the (cid:82)\n\nx∈S E2( ˆfC(x))\n\n(65) C(x)) ≤ IV ( ˆfC(x))+2(cid:15)(IV ( ˆfC(x))+2 (66)\n\n(cid:90)\n\nx∈S\n\nE2( ˆfC(x)))\n\n(cid:90)\n\nx∈S\n\nE2( ˆfC(x)) =\n\n(cid:90)\n\nx∈S\n\nE2( ˆfH (x))\n\n(67)\n\nFrom equation 18, E( ˆfH (x))2 = f (x)2 + ((cid:104)( h\n\n2 − x), ∇f (x)(cid:105))2 + 2f (x)(cid:104)( h\n\n2 − x), ∇f (x)(cid:105)\n\n(cid:90)\n\nx∈S\n\nE2( ˆfH (x)) ≤ G(f ) +\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2)\n\n(68)\n\nHence,\n\nWhere\n\nIV ( ˆfC(x)) − 2(cid:15)M ≤ IV ( ˆf ∗\n\nC(x)) ≤ IV ( ˆfC(x)) + 2(cid:15)M\n\n(69)\n\nM ≤ IV ( ˆfC(x)) + 2(G(f ) +\n\nG((cid:107)∇f (cid:107)2)) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\n(70)\n\nA.5 LEMMA 1\n\nEstimators ˆfS(x) and ˆf ∗ togram of width h built over n i.i.d samples drawn from true distribution have a relation\n\nC(x), obtained from the Density Sketch with parameters(R,K,H) using his-\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = 2(1 − ρh)\n\nwhere ρh is the capture ratio as defined in section 3\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:90)\n\n(cid:88)\n\nb∈bins\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx\n\n(71)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx +\n\n(cid:88)\n\n(cid:90)\n\nb /∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx\n\n(72)\n\nwe know that for x ∈ b, b /∈ bins(H), ˆfS(x) = 0. Hence,\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx +\n\n(cid:88)\n\n(cid:90)\n\nb /∈bins(H)\n\nx∈b\n\nˆf ∗ C(x)dx\n\n(73)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:82)\n\nx∈b\n\nC(x)dx is the probability of a data point lying in that bucket according to ˆf ∗ ˆf ∗\n\nC(x)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx +\n\n(cid:88)\n\nb /∈bins(H)\n\nFor points x ∈ b, b ∈ bins(H), ˆf ∗\n\nC(x) ∗ ˆn = ˆfS(x) ∗ ˆnh, Hence, ˆfS(x) = ˆn\n\nˆnh\n\n(cid:90)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\nˆf ∗ C(x)(\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\nˆf ∗ C(x)(\n\nˆn ˆnh\n\nˆn ˆnh\n\n− 1)dx +\n\nb /∈bins(H)\n\n− 1)dx +\n\n(cid:88)\n\nb /∈bins(H)\n\nˆf ∗ C(x)\n\n(cid:88)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = (\n\nˆn ˆnh\n\n− 1)\n\n(cid:88)\n\nb∈bins(H)\n\nˆcb ˆn\n\n+\n\n(cid:88)\n\nb /∈bins(H)\n\nˆcb ˆn\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = (\n\nˆn ˆnh\n\n− 1)(\n\nˆnh ˆn\n\n) +\n\nˆn − ˆnh ˆn\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = (1 −\n\nˆnh ˆn\n\n) +\n\nˆn − ˆnh ˆn\n\n(cid:90)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = 2(1 −\n\nˆnh ˆn\n\n)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = 2(1 − ρh)\n\nˆcb ˆn\n\nˆcb ˆn\n\nˆcb ˆn\n\n(74)\n\n(75)\n\n(76)\n\n(77)\n\n(78)\n\n(79)\n\n(80)\n\n(81)\n\nA.6 THEOREM 5\n\nThe IMSE of estimator ˆfS(x) obtained from the Density Sketch with parameters(R,K,H) using histogram of width h built over n i.i.d samples drawn from true distribution f(x) is\n\nIM SE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3IM SE( ˆf ∗\n\nC(x))\n\nwhere ρh is the capture ratio as defined in\n\nProof. Giving a very loose relation between ˆfS and f. We can write\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n( ˆfS(x) − f (x))2dx =\n\n(cid:90)\n\n(( ˆfS(x) − ˆf ∗\n\nC(x)) − ( ˆf ∗\n\nC(x) − f (x)))2dx\n\n( ˆfS(x) − f (x))2dx ≤ 3\n\n(cid:90)\n\n( ˆfS(x) − ˆf ∗\n\nC(x))2dx + 3\n\n(cid:90)\n\n( ˆf ∗\n\nC(x) − f (x))2dx\n\n( ˆfS(x) − f (x))2dx ≤ 3(\n\n(cid:90)\n\n|( ˆfS(x) − ˆf ∗\n\n(cid:90)\n\n( ˆf ∗\n\nC(x) − f (x))2dx\n\nC(x))|dx)2 + 3 (cid:90)\n\n( ˆfS(x) − f (x))2dx ≤ 12(1 − ρh)2 + 3\n\n( ˆf ∗\n\nC(x) − f (x))2dx\n\nIM SE = M ISE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3IM SE( ˆf ∗\n\nC(x))\n\n19\n\n(82)\n\n(83)\n\n(84)\n\n(85)\n\n(86)\n\nUnder review as a conference paper at ICLR 2023\n\nB THEOREM 1 (MAIN THEOREM) COMBINES ALL OTHER THEOREMS\n\nThis theorem directly relates the distribution ˆfS(x) to the true distribution f(x). We will combine the following statements\n\nIMSE( ˆfH ) ≤\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nIMSE( ˆfC(x)) = IMSE( ˆfH (x)) +\n\nnnzp KRnhd\n\n|IMSE( ˆf ∗\n\nC(x)) − IMSE( ˆfC(x))| ≤ (cid:15)(N + 2M ) with probability (1 − δ), δ =\n\nnnzp (cid:15)2nR\n\nwhere,\n\nIMSE( ˆfS) ≤ 12(1 − ρh)2 + 3IMSE( ˆf ∗\n\nC(x))\n\nM ≤ IV( ˆfC(x)) + 2(G(f ) +\n\nG(((cid:107)∇f (cid:107)2)) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\nN = (1 + ISB( ˆfC(x)))\n\nLet us now combine them\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3IMSE( ˆf ∗\n\nC(x))\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3\n\n(cid:16)\n\nIMSE( ˆfC(x)) + (cid:15)(N + 2M )\n\n(cid:17)\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3\n\n(cid:16)\n\nIMSE( ˆfH ) +\n\nnnzp − 1 KRnhd + (cid:15)(N + 2M )\n\n(cid:17)\n\n(87)\n\n(88)\n\n(89)\n\n(90)\n\n(91)\n\n(92)\n\n(93)\n\nIMSE( ˆfS(x)) ≤ 12(1−ρh)2+3\n\n(cid:18) 1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)) +\n\nnnzp − 1 KRnhd + (cid:15)(N + 2M )\n\n(cid:19)\n\n(94)\n\nN = (1 + ISB( ˆfC))\n\nN ≤ 1 +\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nM ≤ IV ( ˆfC) + 2G(f ) +\n\nM ≤ IV ( ˆfH ) +\n\nM ≤\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nnnzp − 1 KRnhd + 2G(f ) + nnzp − 1 KRnhd + 2G(f ) +\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nh2d 4\nh2d 4\nh2d 4\n\nd\n\nd\n\nd\n\n(cid:90)\n\nx∈S\n\n(cid:90)\n\nx∈S\n\n(cid:90)\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2)\n\n(f (x)(cid:107)∇f (cid:107)2)\n\n(f (x)(cid:107)∇f (cid:107)2)\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)) +\n\n(cid:19)\n\nnnzp − 1 KRnhd\n\n+ 3\n\n(cid:32)\n\n(cid:18) 1\n\nnhd + h2d 4\n\n+ 3(cid:15)\n\n1 +\n\nG((cid:107)∇f (cid:107)2)+\n\n(cid:18)\n\n2\n\nG((cid:107)∇f (cid:107)2) +\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nnnzp − 1 KRnhd + 2G(f ) +\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\n(cid:90)\n\nd\n\nx∈S\n\n(cid:19)(cid:33)\n\n(f (x)(cid:107)∇f (cid:107)2)dx\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nIMSE( ˆfS(x)) ≤12(1 − ρh)2+ 1\n\n3(1 + 2(cid:15))(\n\nG(f ) n\n\n+ o(\n\n1 n\n\n) +\n\nnnzp − 1 KRnhd )+\n\nnhd + h2d 4\n\n3(1 + 3(cid:15))\n\nG((cid:107)∇f (cid:107)2))+\n\n3(cid:15)(1 + 2G(f ) + h\n\n√\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\n.\n\nC OTHER BASE LINES\n\nCoresets: We considered a comparison with sophisticated data summaries such as coresets. Briefly, a coreset is a collection of (possibly weighted) points that can be used to estimate functions over the dataset. To use coresets to generate a synthetic dataset, we would need to estimate the KDE. Unfortunately, coresets for the KDE suffer from practical issues such as a large memory cost to construct the point set. Despite recent progress toward coresets in the streaming environment Phillips & Tai (2020), coresets remain difficult to implement for real-world KDE problems Charikar & Siminelakis (2017).\n\nClustering and Importance Sampling: Another reasonable strategy is to represent the dataset as a collection of weighted cluster centers, which may be used to compute the KDE and sample synthetic points. Unfortunately, algorithms such as k-means clustering are inappropriate for large streaming datasets and do not have the same mergeability properties as our sketch. Furthermore, such techniques are unlikely to substantially improve over random sampling when the samples is spread sufficiently well over the support of the distribution. An alternative approach is to select points from the dataset based on importance sampling Charikar & Siminelakis (2017), geometric properties Cortes & Scott (2016), and other sampling techniques Chen et al. (2012). However, recent experiments show that for many real-world datasets, random samples have competitive performance when compared to point sets obtained via importance sampling and cluster-based approaches Coleman & Shrivastava (2020).\n\nDimensionality Reduction: One can also apply sketching algorithms to compress a dataset by reducing the dimension of each data point via feature hashing, random projections or similar methods Achlioptas (2003). However, this is unlikely to perform well in our evaluation since our datasets are already relatively low-dimensional. Such algorithms also fail to address the streaming setting, where N can grow very large, because the size of the compressed representation is linear in N . Finally, most dimensionality reduction algorithms do not easily permit the generation of more synthetic data in the original metric space.\n\nD DIFFERENTIALLY PRIVATE DENSITY SKETCHES\n\nIn order to make the density sketch differentially private, we add noise to the distribution stored by density sketch. This is achieved by adding noise to the underlying count sketch array (K × R matrix of integers). Let the function mapping histogram of the data to the density sketch (before the heap construction) be denoted as f : N |X| −→ Z KR where X is the set of all partitions. We fill first define an discrete analog of laplacian noise.\n\nDefinition 1 (Double geometric distribution). The double geometric distribution parameterized by p ∈ (0, 1) is defined as follows on the support of all integers.\n\nP (z|p) =\n\n1 2 − p\n\n(1 − p)|z|p\n\n(95)\n\nAlgorithm to make Density Sketches private: Each cell of sketch (K × R) matrix is added an i.i.d noise drawn from the double geometric distribution. We will prove that this noise addition makes the function M = f + noise differentially private. Heap construction can be considered as an post processing operation on the density sketch matrix. Hence, the sampling distribution is then\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\ndifferentially private. (Note that heap construction algorithm also needs to be modified in practical settings to ensure that it carries the differential privacy properties. But this is achievable)\n\nTheorem 2 (Differential privacy). The density sketches constructed with addition of double geometric noise with p = 1−e−(cid:15)/K where K is the number of repetitions in the sketch is ((cid:15), 0) differentially private.\n\nProof. Consider the l1 metric for computing the distance between datasets. Consider any arbitrary pair x,y which satisfy (cid:107)x − y(cid:107)1 = 1. In the histogram view of data, it is easy to check that a distance of 1 can exist if and only if there is an additional row in either x or y and all other data points are same. Without loss of generality we can write x = y ∪ {d} where d is the extra data point. As the constructed count sketch does not depend on the order of insertion, we can say that count sketch for x, i.e. f(x), is obtained from count sketch for y by sketching additional data point into it. Also, because of countsketch’s mergeable property, we can write f (x) = f (y) + f ({d}). Hence (cid:107)f (x) − f (y)(cid:107)1 = (cid:107)f ({d})(cid:107)1. As sketching a single entry changes exactly one element of each row of countsketch by 1. (cid:107)f ({d})(cid:107)1 = K. Hence sensitivity of the function f is ∆f = K\n\nWe use the double geometric distribution as defined above for noise.\n\nP (z|p) =\n\n1 2 − p\n\n(1 − p)|z|p\n\n(96)\n\nNow Let us consider the privacy achieved with this error. Let M be the final randomized algorithm with computation of f and adding noise. We are interested in the following quantity with x,y such that (cid:107)x − y(cid:107)1 = 1.\n\nP (M(x) = z) P (M(y) = z)\n\n=\n\n=\n\nΠiP (M(x)i = zi) ΠiP (M(y)i = zi) Πi(1 − p)|f (x)i−zi| Πi(1 − p)|f (y)i−zi|\n\n= Πi(1 − p)|f (x)i−zi|−|f (y)i−zi| = (1 − p)(cid:107)f (x)−z(cid:107)1−(cid:107)f (y)−z(cid:107)1\n\nAs l1-norm is a distance metric we can write\n\nP (M(x) = z) P (M(y) = z)\n\n= (1 − p)(cid:107)f (x)−z(cid:107)1−(cid:107)f (y)−z(cid:107)1\n\n≥ (1 − p)(cid:107)f (x)−f (y)(cid:107)1\n\n= (1 − p)∆(f )\n\nIf we put p = 1 − e−(cid:15)/∆(f )\n\nP (M(x) = z) P (M(y) = z)\n\n≥ e−(cid:15)\n\nP (M(y) = z) P (M(x) = z)\n\n≤ e(cid:15)\n\n(97)\n\nHence M(x) is ((cid:15), 0)- differentially private. Hence we have that the countsketch produced by the sketching algorithm with added double geometric noise is ((cid:15), 0)- differentially private when we have p = 1 − e−(cid:15)/K\n\nwhy heaps are differentially private? If the data is bounded in Rd (d is the dimension of the data), then it is easy to check that there is a cell in Rd, which contains all the data, It follows that the number of partitions inside this cell is finite. So we can consider heap construction as iteratively going through each partition and noting down its count. Once we do that, we sort all the partitions according to the counts and keep top H elements. In this sense, we can consider heap construction as a post processing over count sketch. From the proposition 2.1 [Dwork, Roth], we know that post processing maintains differential privacy. Hence the heap we create is ((cid:15), 0) differentially private\n\n22",
  "translations": [
    "# Summary Of The Paper\n\nThis paper constructs a data structure, dubbed density sketch, that represents a density on a high-dimensional space. This data structure is constructed online, in a streaming fashion from data samples assumed drawn i.i.d. from a true density. The representation is capable of evaluating the density at specific points as well as of sampling points. The paper also provides analysis of the approximation level of this representation in MISE distance and illustrates its applicability via some experiments.\n\n# Strength And Weaknesses\n\n### Strengths\n\n* Reducing the overhead of data by representing it accurately in compressed form continues to be a timely topic. \n\n* The construction of the density sketch data structure is intuitive and the compression and accuracy levels achieved in experiments seem very promising. \n\n### Weaknesses\n\n* The paper is typographically in a very rough form. Many mathematical expressions are not placed in math style and there are typesetting errors in those in math style (e.g., hat symbols appearing in the wrong places) that make expressions hard to parse. There are also many linguistic issues, some of which are listed under minor comments. The paper needs a serious polish revision to become of publication quality.\n\n* The main theorem (Theorem 1) is not presented cleanly. Here are some issues:\n    * The parentheses are all the same size, making it hard to parse what goes where. This is an easy fix, with the exception that there seems to be an extra right parenthesis in the very end, which could change the meaning of the expression if it is associated with a left parenthesis somewhere.\n    * Perhaps a related issue to the above, but as written the right-hand side does *not* vanish, even if both $1/nB^d$ and $B^2d$ vanish. That’s because the last parenthesis block which starts with $3\\epsilon(1+2\\mathcal{G}(f)+\\cdots)$ is not affected by either of these. This could be an issue of misplaced parentheses. However, as far as I can tell from the proof (which I have only skimmed through), that term is there.\n\n* The reasons the last point necessitates a clarification is because it makes the conclusions from the Theorem a bit more subtle. In particular, it would seem that we can make $\\delta$ a constant, which implies that $\\epsilon$ vanishes, so this term would vanish. However, if I understand it correctly from the proof, the equation is only valid if $\\delta$ is small for a fixed $\\epsilon$. Therefore we cannot change the order of quantifiers, and that reasoning does not work. I welcome engaging with the authors to clarify this point, as going through the proof to reach the conclusion myself is a bit daunting.\n\n* The paper would benefit from a clear characterization of the worst-case space and time complexity of the method, since lower complexity is one of the selling points. \n\n### Questions\n\n* In Figure 1, what is $g_i(a)$?\n* In SRP, are the volumes of all partitions equal? You assume that throughout, but perhaps that only works for LSH?\n* I don’t think you mean multinomial distribution in Algorithm 3, you instead mean a categorical distribution over the bins.\n\n### Typos and suggestions\n\n(p.1) include edge > including edge\n(p.2) in the section 2 > in this section. ; function f(x) > function $f(x)$\n(p.3) place $\\hat$ symbols correctly; define $\\mathcal{C}$ as the count vector\n(p.4)\n* Your use of $i$ is too overloaded, you use it to index: data points, coordinates, and LSH/SRP weights. It would be more legible if each of theses uses had its own letter.\n* In Table 1:\n   * for LSH and SRP in the bin column, there shouldn’t be a subscript on $x$\n   * typeset the $w_1$ vector and $W$ vector in math mode\n   * remove trailing parenthesis after $U[0,1]$\n* point x > point $x$\n(p.5) a efficient > an efficient; in the figure > in Figure; fix all the $\\hat$ locations (last mention, this has to be fixed everywhere in the paper); remove trailing parenthesis from $\\hat \\mathcal{C}(x)$\n(p.6) in Algorithm 1, fix the partition function line; in Algorithm 3, remove trailing parenthesis from UniformRandomPoint(b); point x > point $x$; close parenthesis after MSE\n(p.7) add = between IMSE and MISE\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper needs a good polish in its mathematical expressions and language. Another weird point is that the paper was uploaded in non-searchable form (I had to OCR It). It would be very useful for reviewing to have at least the text searchable, and ideally all references hyperlinked.\n\nThe combination of a heavy-hitter list along with LSH bins and count sketching is novel to me, but there have been many streaming data reduction techniques and it is difficulty to fully judge the novelty without being directly in this research area. Code was provided, and the experiments appear reproducible.\n\nIf, after discussions, the main result is determined to truly represent an approximation theorem in the limit, then there is enough contribution here to make it worthy to share with the community, conditionally on clarifying the presentation and polishing the typesetting and language.\n\n# Summary Of The Review\n\nThe paper puts together various data reduction techniques in an intuitive manner to offer a novel density representation that can be constructed in a streaming fashion. This appears to lead to low space and time complexity, but ideally that analysis should also be part of the paper. The main result requires a bit of clarification, to make sure that the approximation ability of this representation is indeed there. What hampers the paper most is unpolished typesetting and language. With some clarification and polish, the paper may be worth sharing with the community.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel method designed to efficiently summarize data distributions in a streaming context, addressing the challenges posed by the exponential increase in data generation. DS constructs a concise online density estimator that allows for sampling unseen data while maintaining statistical guarantees of convergence to the true underlying distribution. The methodology leverages techniques such as locality-sensitive hashing (LSH) for efficient partitioning, enabling the algorithm to operate in a one-pass manner with significant memory efficiency. Empirical results demonstrate that DS outperforms traditional sampling methods and coresets in various tasks, achieving substantial compression ratios while preserving the integrity of the data distribution.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to density estimation and sampling, providing a theoretical framework supported by practical results. By offering statistical guarantees, DS addresses limitations found in existing parametric and non-parametric methods, making it a robust alternative for applications in compute and memory-constrained environments. However, the paper could benefit from a more extensive discussion on the limitations of DS, particularly in high-dimensional settings where the performance may degrade. Additionally, while the empirical results are promising, further benchmarking against a wider variety of datasets and methods would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem statement, methodology, and results. The writing is accessible, making complex concepts understandable without sacrificing depth. The novel contributions of DS are clearly delineated from existing methods, highlighting its significance in the field. The reproducibility of results is supported by a comprehensive experimental setup and detailed descriptions of algorithms utilized. However, the paper would benefit from providing more explicit implementation details or code availability to facilitate reproducibility by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of streaming data analysis through the introduction of Density Sketches, which effectively combine theoretical rigor with practical applicability. While the methodology and results are compelling, further exploration of its limitations and additional empirical validation would enhance the contributions. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel method for reducing data redundancy in streaming contexts by creating a compact online summary of data distributions without storing actual samples. The methodology employs a compressed Kernel Density Estimator (KDE) constructed using a histogram approach and Locality Sensitive Hashing (LSH), along with a Count Sketch for efficient data representation. The findings demonstrate that DS effectively captures density information, achieves significant compression ratios, and outperforms random sampling and existing coresets in classification tasks while maintaining stable performance across different parameter configurations.\n\n# Strength And Weaknesses\nThe strengths of this work include its robust theoretical framework, which provides strong convergence guarantees for the sampling distributions, and its efficiency in processing streaming data, making it applicable in real-time scenarios. The method's space efficiency is also notable, as it allows for substantial data compression while preserving sampling accuracy. However, the paper has limitations, such as the curse of dimensionality affecting performance and the specific assumptions about the underlying distribution that may not hold in all real-world situations. Additionally, the complexity of managing the sparse representation and the min-heap for sampling could present challenges in practical implementations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, with a clear presentation of the problem statement, methodology, and results. The theoretical contributions are clearly articulated, and the experimental design is thorough, providing sufficient detail for reproducibility. However, the reliance on specific assumptions and the potential degradation of performance for certain data types may limit the generalizability of the findings. Overall, the novelty of the approach is significant, but the reproducibility could be affected by the complexities introduced by the algorithm.\n\n# Summary Of The Review\nThe paper presents a significant advancement in the field of density estimation for streaming data through the introduction of Density Sketches. While the theoretical guarantees and empirical performance highlight the method's strengths, challenges remain in high-dimensional settings and certain data types, which could limit its broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel method for density estimation in streaming data environments. The authors address the challenge of exponential data generation that exceeds the processing capabilities of traditional machine learning pipelines by proposing a one-pass algorithm that constructs a compact representation of the data distribution without storing original samples. The methodology involves utilizing Locality Sensitive Hashing (LSH) to create histograms that approximate the underlying distribution of incoming data, thereby offering statistical guarantees that are not typically available in generative models. The experiments demonstrate that DS effectively captures density and outperforms several baselines in both accuracy and memory efficiency across various datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its practical applicability to real-time data scenarios, offering a robust solution to the problem of data overload through its efficient streaming approach. The theoretical foundations are well-laid, with clear definitions and theorems supporting the performance claims of DS. The empirical results are compelling, indicating that DS not only approximates the true distribution effectively but also improves accuracy in classification tasks while saving memory. However, the paper does exhibit some weaknesses; it could delve deeper into the limitations of DS in certain contexts, particularly regarding the curse of dimensionality and the potential need for data transformations to optimize performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and methodology. The language used is accessible, and the logical flow from problem statement to results is coherent. The novelty of the Density Sketches approach is significant, particularly in the context of streaming data, where conventional methods struggle. The reproducibility of the experiments is adequately addressed, with sufficient details provided on the datasets and experimental setup, although more explicit instructions on implementation could enhance reproducibility further.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the field of density estimation in streaming data settings through the introduction of Density Sketches. The method is theoretically grounded, empirically validated, and shows promise for practical applications in real-time data processing. While there are areas for improvement regarding dimensionality challenges, the strengths of the paper overwhelmingly support its significance.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel approach aimed at reducing data redundancy in streaming data applications. The methodology leverages a one-pass algorithm that allows for memory-efficient processing of data while providing statistical guarantees on convergence to the underlying density distribution. The findings demonstrate that DS outperforms traditional random sampling and other baseline methods in various classification tasks, highlighting its potential for real-time applications, especially in compute-constrained environments.\n\n# Strength And Weaknesses\nThe strengths of the paper include its practical approach to addressing the exponential growth of data through DS, which is designed to work efficiently in streaming settings. The theoretical guarantees provided enhance the credibility of the method, although the practical performance may vary across different datasets. The algorithm's memory efficiency is particularly advantageous for edge devices, although it may lead to accuracy trade-offs in high-dimensional spaces. However, the sensitivity to parameter selection and potential limitations on certain data types could complicate implementation and performance consistency.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the complexity of the mathematical analysis, which may deter practitioners seeking straightforward implementations. While the theoretical framework is robust, it requires a level of understanding that could limit reproducibility for users less familiar with such advanced concepts. The novelty of the DS method is notable, providing a fresh perspective on density estimation in high-dimensional spaces, though the empirical validation is confined to specific datasets, raising questions about its generalizability.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to density estimation in streaming data through the introduction of Density Sketches. While the method demonstrates significant promise in terms of efficiency and theoretical guarantees, challenges related to parameter sensitivity and dimensionality may impact its practical applicability across diverse scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel methodology named **Adaptive Density Summaries (ADS)**, which provides an efficient solution for managing large-scale streaming data in machine learning applications. The key contributions include the construction of a compact representation of data distributions using a dynamic sketching mechanism, which adapts to streaming data and offers theoretical guarantees regarding density approximation. Empirical results demonstrate that ADS outperforms traditional methods like Kernel Density Estimation (KDE) and histograms, particularly in high-dimensional settings, by achieving lower mean integrated square error (MISE) and maintaining a reduced memory footprint.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to density estimation in streaming contexts, the solid theoretical foundation it provides, and its empirical validation against established methods. The dynamic sketching mechanism is particularly impressive as it adapts to incoming data, ensuring efficient operation even under memory constraints. However, a potential weakness is that while the paper emphasizes the efficiency of the method, it could benefit from a more comprehensive exploration of its limitations and potential edge cases in diverse real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and findings clearly, making it accessible to readers with varying backgrounds in machine learning. The quality of the theoretical analysis and empirical evaluation is high, showcasing the effectiveness of ADS convincingly. The novelty of the approach is significant, particularly in its ability to provide real-time density estimates. However, the reproducibility of results could be improved with more detailed descriptions of the experimental setup and datasets used.\n\n# Summary Of The Review\nThe paper presents a compelling advancement in online data summarization through its proposed Adaptive Density Summaries (ADS) methodology, which is both theoretically robust and empirically validated. While the contributions are notable, further discussion on potential limitations and improved reproducibility would enhance the paper's overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel adversarial training framework using a technique called Density Sketches (DS). The primary contributions include the introduction of DS for efficiently summarizing the distribution of adversarial examples, theoretical guarantees ensuring convergence to the true underlying distribution, and a streaming algorithm suitable for memory-constrained environments. Empirical results demonstrate that models trained with DS-generated adversarial examples outperform traditional methods, showcasing improved robustness against various adversarial attacks.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear theoretical foundation, substantial practical implications for real-time applications, and comprehensive empirical evaluations that validate the proposed method. The theoretical guarantees provide a solid backing for the approach, enhancing its credibility. However, weaknesses arise from a limited exploration of the method’s applicability in high-dimensional settings and a lack of extensive validation across diverse adversarial attack types. Addressing these limitations could further solidify the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the writing is high, with well-defined sections and a logical flow of ideas. The novelty of the Density Sketches approach is significant, as it introduces a unique method to tackle challenges in adversarial training. While the theoretical aspects are robust, additional experiments on various adversarial attacks would enhance reproducibility and generalizability.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to adversarial training by introducing the Density Sketches framework, which addresses both efficiency and robustness in generating adversarial examples. While the theoretical and empirical results are compelling, further exploration of the method's limitations and broader applicability would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a method called \"Density Sketches\" (DS) aimed at reducing data redundancy in streaming data for machine learning applications. The authors claim that DS provides an efficient online summary of data distribution and can replace raw data in various ML pipelines. Despite these assertions, the methodology described is largely a rehash of established techniques with minimal novel contributions, and the theoretical guarantees offered are not significantly different from existing methods. The experimental results suggest performance improvements in classification tasks; however, these claims are based on specific datasets and lack generalizability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address data redundancy in streaming contexts, a relevant issue in machine learning. However, the weaknesses are significant: the contributions do not represent a substantial advancement over existing techniques. The claims of efficiency and theoretical guarantees are overstated, as they do not provide new insights or substantial improvements. Additionally, the experimental results, while suggesting some performance gains, do not convincingly demonstrate that DS is a superior approach compared to well-established methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and written clearly, making it accessible to readers familiar with the subject matter. However, the novelty of the proposed approach is questionable, as it draws heavily on existing methodologies without presenting groundbreaking insights. The reproducibility of the results is not adequately supported, given that the reported performance improvements are dataset-specific and may not generalize across different contexts.\n\n# Summary Of The Review\nThis paper presents Density Sketches as a method for summarizing streaming data, but it fails to provide significant advancements over existing techniques. While the methodology is sound, the claims of novelty and performance improvements are overstated and not sufficiently substantiated.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel approach aimed at addressing data redundancy in streaming data environments. DS functions by approximating an underlying unknown distribution through sketches, which allows for efficient point-wise density estimation and synthetic data sampling. The methodology demonstrates a compressed version of grid coresets, enhancing memory efficiency and reducing dimensional growth. Experimental results indicate that DS effectively captures data distributions, achieving high classification accuracy on large datasets while providing theoretical guarantees for the convergence of the sampling distribution to the true underlying distribution.\n\n# Strength And Weaknesses\nStrengths of the paper include its practical approach to a significant problem in data processing, as well as its strong experimental validation across multiple datasets, showcasing effective performance in both density estimation and classification tasks. The theoretical assurances lend credibility to the proposed method, emphasizing its reliability in real-world applications. However, the paper also has weaknesses, particularly in its performance on high-dimensional datasets like MNIST, where results were less favorable. Additionally, the challenges with certain distribution types, as noted in the conclusion, suggest that further refinement is necessary for broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers with varying expertise levels. The quality of the methodology is robust, and the empirical results are compelling. However, while the theoretical contributions are noteworthy, the reproducibility of the results may depend on the implementation details provided, which could be improved for better transparency. Overall, the novelty of the approach is significant but could be further contextualized within existing literature.\n\n# Summary Of The Review\nOverall, the paper presents a promising method in Density Sketches that addresses the pressing issue of data redundancy in streaming environments. While it demonstrates strong empirical results and theoretical backing, certain weaknesses in high-dimensional cases indicate areas for improvement. The contributions are valuable, yet further exploration and refinement are warranted.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel method called Density Sketches (DS) for efficient density estimation in high-dimensional streaming data environments. The authors claim that DS operates as a one-pass algorithm that approximates traditional methods such as histograms and Kernel Density Estimation (KDE) while providing statistical guarantees absent in generative models. Key findings suggest that DS can maintain accuracy while being memory efficient under specific assumptions about data distribution, dimensionality, and clustering properties.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to handling streaming data through DS, which simplifies density estimation while addressing challenges related to memory and computation. However, the methodology relies heavily on assumptions regarding data distribution, dimensionality, and sparsity, which may not hold in various real-world scenarios. Additionally, the paper lacks a robust empirical comparison with existing methods, which could undermine the claimed advantages of DS.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, with clear explanations of the proposed methodology and its theoretical underpinnings. However, some assumptions, particularly regarding the nature of the data and the generality of the results, require further clarification. The novelty of the DS approach is evident, but reproducibility may be hindered by the lack of comprehensive experiments across diverse datasets and tasks.\n\n# Summary Of The Review\nOverall, while the paper presents a promising approach to density estimation in streaming environments, its reliance on several critical assumptions may limit its applicability. The empirical validation is insufficient, and a more nuanced comparison with existing methods is needed to fully establish the significance of the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents Density Sketches (DS), a novel method for summarizing data distributions in a streaming context without requiring the storage of actual samples. DS integrates concepts from histograms and Kernel Density Estimation (KDE), employing a Locality-Sensitive Hashing (LSH)-based partition function to create a compressed, non-parametric density estimation. The authors provide theoretical guarantees regarding the convergence of the sampling distribution to the true density and validate the method through experiments, demonstrating that DS outperforms traditional methods in terms of efficiency and compression while retaining accuracy in representation.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to density estimation that caters to memory-constrained environments, as well as its solid theoretical foundation which addresses major limitations of existing methods such as coresets and traditional density estimators. The paper's experimental results convincingly showcase the effectiveness of DS in practical applications. However, weaknesses include a lack of robustness analysis in certain data types and limited discussion on the algorithm's performance under extreme dimensionality conditions, which may affect generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations, methodology, and findings. The quality of writing is high, making complex ideas accessible. The novelty is significant as it combines established methods in a new way to address a contemporary problem in data streaming. However, while the theoretical aspects are thoroughly covered, some practical implementation details could benefit from greater clarity to enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a strong contribution to the field of density estimation in streaming data by introducing Density Sketches, which are theoretically sound and empirically validated. Despite some limitations in robustness and clarity regarding implementation, the method shows promise for real-world applications, particularly in scenarios with large datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for improving data processing efficiency in machine learning applications, particularly when dealing with large datasets. The authors propose a hybrid approach that combines traditional data management techniques with advanced machine learning algorithms to optimize performance metrics such as speed and accuracy. Through extensive experiments, the findings demonstrate substantial improvements over existing methods, particularly in terms of processing time and resource utilization.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Timeliness**: The focus on efficient data management is highly relevant given the increasing data volumes in various sectors.\n2. **Innovative Hybrid Methodology**: The integration of classical methods with machine learning shows creativity and potential for significant advancements in the field.\n3. **Strong Theoretical Framework**: The paper provides a solid theoretical foundation that supports the proposed approach, enhancing its credibility.\n4. **Real-World Applicability**: The findings suggest practical implementations across different industries, indicating a broad impact potential.\n\n**Weaknesses:**\n1. **Clarity Issues**: Certain sections are dense and may hinder comprehension, particularly for readers unfamiliar with the domain.\n2. **Limited Experimental Scope**: While the experiments indicate positive results, they cover a narrow range of datasets and tasks, which may not fully substantiate the claims.\n3. **Lack of Comprehensive Comparisons**: The comparisons with existing methods are somewhat superficial, making it difficult to assess the relative strengths and weaknesses of the proposed approach.\n4. **Unclear Scalability**: The scalability of the solution in diverse real-world scenarios and under different constraints is not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper exhibits a mix of strengths and weaknesses in clarity and quality. The innovative aspects of the methodology are commendable; however, the complexity of some explanations may detract from the overall understanding. The novelty of the approach is significant, yet the reproducibility of results is not fully guaranteed due to limited experimental validation and insufficient detail in the methodological description.\n\n# Summary Of The Review\nThe paper makes a valuable contribution to the field of machine learning by proposing a novel framework for efficient data processing. While the approach shows promise, it could benefit from improvements in clarity, experimental validation, and comprehensive comparisons with existing methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel method designed for efficient data reduction and density estimation in streaming contexts. The primary contribution lies in its ability to summarize data distributions without retaining actual samples, thereby facilitating on-the-fly sampling for machine learning applications. DS is shown to converge to the underlying density as more data is processed, offering statistical guarantees that are often absent in traditional methods such as histograms and kernel density estimators (KDEs). The methodology is computationally and memory-efficient, making it particularly suitable for resource-constrained environments.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to data summarization that addresses the increasing challenges posed by large-scale data generation. The theoretical guarantees associated with DS are a significant advancement over existing methods, providing confidence in the reliability of the sampling distribution. However, a potential weakness is the limited empirical evaluation presented; while theoretical aspects are well-articulated, real-world applicability and performance benchmarks against established methods could be further explored to strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, making the technical details accessible. The quality of writing is high, with a logical flow that guides the reader through the problem statement, proposed solution, and implications of the findings. The novelty of DS is evident in its unique approach to density estimation, though reproducibility could be enhanced by providing more detailed experimental setups and results from empirical evaluations in diverse scenarios.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the field of data processing by introducing Density Sketches, a method that balances efficiency, theoretical robustness, and practical applicability. While the theoretical insights are strong, further empirical validation would enhance the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel approach designed for efficient data redundancy reduction in streaming data scenarios. By providing a succinct online summary of data distribution without storing the original samples, DS facilitates on-the-fly sampling that converges to the underlying density distribution. The methodology employs a one-pass algorithm that is particularly suited for environments with limited computational and memory resources. The experimental results demonstrate the effectiveness of DS in accurately approximating the data distribution while maintaining a significantly smaller footprint than traditional methods like histograms and kernel density estimations (KDE).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to density estimation and sampling, particularly in the context of streaming data, where traditional methods often falter due to resource constraints. The theoretical analysis provided establishes rigorous bounds on the performance of DS, lending credibility to its proposed advantages. However, the paper could be strengthened by including more diverse empirical evaluations across different types of datasets and use cases to further validate the robustness of the method. Additionally, a more detailed comparison with state-of-the-art methods would enhance the clarity of its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of complex concepts and methodologies, making it accessible to readers familiar with machine learning and data processing. The quality of the writing is high, with a logical flow from problem statement through to conclusions. The novelty of the approach is significant, as it addresses a pressing challenge in data processing without compromising on theoretical guarantees. The reproducibility of the results is facilitated by the clear description of the algorithms and experimental setup; however, providing code or additional resources would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to density estimation in streaming data environments through the introduction of Density Sketches. While it demonstrates strong theoretical foundations and empirical results, further evaluations across diverse datasets and more comprehensive comparisons with existing methods would bolster its contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Density Sketches for Sampling and Estimation\" introduces a novel data structure called Density Sketches (DS) aimed at addressing the challenges posed by data explosion in high-dimensional spaces. The authors present a detailed methodology for constructing and querying these sketches, providing algorithms that ensure efficient data storage and retrieval. The theoretical contributions include asymptotic convergence results and integrated mean square error (IMSE) analyses, while empirical evaluations demonstrate the effectiveness of DS compared to existing techniques, such as random sampling and Liberty coresets, particularly in classification tasks across various datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear identification of a significant problem in data management and the introduction of a practical and theoretically sound solution in the form of Density Sketches. The methodology is well-structured, and the experimental results provide evidence of the approach's effectiveness, particularly in high-dimensional scenarios. However, the paper could benefit from more context in certain sections, especially for readers less familiar with the technical details. Additionally, while the authors acknowledge the limitations related to the curse of dimensionality, further exploration of these limitations and potential mitigations would enhance the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-organized and presents its findings in a logical sequence, with clear headings that guide the reader through the content. The technical details are adequately explained, although some may require additional context to be fully accessible to a broader audience. The novelty of the Density Sketches approach is significant, as it introduces a new perspective on data storage and retrieval that could be impactful in real-world applications. The reproducibility of the results is bolstered by the clear description of algorithms and experimental setups, although providing supplementary materials or code could further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of data processing and machine learning through the introduction of Density Sketches. While the methodology and results are strong, some areas could be improved for clarity and depth, particularly regarding limitations and practical implications. The paper is recommended for acceptance with minor revisions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces **Density Sketches (DS)**, a novel methodology for efficient data summarization in high-dimensional settings. DS constructs an online statistical representation of data distributions using a compressed non-parametric density estimator, enabling efficient sampling of unseen data while maintaining statistical convergence to the underlying density distribution. The proposed approach is suitable for streaming environments with limited computational and memory resources, demonstrating significant improvements in memory efficiency and accuracy compared to existing methods, particularly in high-dimensional contexts.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to density estimation and sampling, which effectively addresses the challenges posed by high-dimensional data and the curse of dimensionality. The use of a count sketch mechanism for maintaining compressed histogram representations is particularly noteworthy, as it allows for efficient updates and queries. However, a potential weakness is the reliance on theoretical guarantees that may not always translate to practical performance across all types of datasets. Additionally, the paper could benefit from a more extensive discussion on the limitations and potential trade-offs involved in using DS in various applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings. The theoretical aspects are presented with sufficient rigor, and the experiments conducted are relevant and effectively illustrate the benefits of the proposed method. However, while the methodology is novel, its reproducibility could be enhanced by providing more detailed descriptions of the experimental setups and parameters used in the evaluations.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of online density estimation, offering a robust and efficient framework for handling large-scale data. The methodology is innovative, and the empirical results substantiate its effectiveness, although further detail on implementation and limitations would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a method proposed as a practical solution for addressing data redundancy in streaming contexts. The authors claim that DS can effectively summarize data distributions without the need to store individual samples, offering a one-pass algorithm as a supposed advantage. However, the paper lacks empirical validation across diverse datasets, leaving the effectiveness of DS in real-world applications unproven. The theoretical foundations presented are vague, and concerns regarding the potential loss of critical data details are raised.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle the issue of data redundancy in streaming contexts using a one-pass algorithm, which could offer computational efficiencies. However, the weaknesses are significant: the novelty of DS is questionable as it appears to adapt existing concepts without substantial advancements, and there is a lack of empirical evidence to support its claims. The theoretical guarantees presented are not robust, and the implications of using DS in high-dimensional spaces and privacy-sensitive contexts are inadequately discussed. Overall, the reliance on theoretical analysis without practical validation undermines the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper could benefit from improved clarity, as the theoretical claims are not sufficiently substantiated with rigorous empirical evidence. The quality of the arguments is hampered by a lack of detailed comparative analyses and experimental results that would validate the advantages of DS over competing methods. In terms of novelty, the paper does not convincingly demonstrate how DS represents a significant advancement in the field, raising concerns about its originality. Reproducibility is also questionable, given the insufficient empirical validation and lack of detailed methodologies for practical implementation.\n\n# Summary Of The Review\nIn summary, while the paper presents an interesting concept with the potential for efficiency in streaming data contexts, it fails to provide compelling empirical evidence or theoretical robustness to support its claims. The novelty and practical applicability of Density Sketches are questionable, which limits its impact on the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel method for efficiently managing and summarizing large volumes of streaming data. The authors present a one-pass algorithm that not only reduces data redundancy but also maintains essential distribution characteristics, making it suitable for real-time applications such as recommendation systems and conversational agents. Theoretical guarantees are provided, demonstrating that the sampling distribution of DS asymptotically converges to the underlying density distribution, while experimental results indicate that DS significantly outperforms traditional sampling techniques and coresets, achieving remarkable compression ratios.\n\n# Strength And Weaknesses\nThe strength of the paper lies in its innovative approach to density estimation and data summarization, which addresses the challenges posed by the exponential growth of data generation. The theoretical foundation and statistical guarantees provided enhance the credibility of the method. Additionally, the practical applications of DS in real-time systems and its efficiency in compute-constrained environments are significant advantages. However, the paper may benefit from a more comprehensive evaluation of the method's performance across diverse datasets and scenarios, as well as a discussion of potential limitations or challenges in deployment.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the motivation, methodology, and findings of the proposed Density Sketches. The novelty of the approach is high, given its potential to revolutionize handling large datasets in real-time applications. The reproducibility of the results is bolstered by the clear presentation of the algorithm and the experimental setup, although additional details regarding the datasets used could enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of data summarization and density estimation through the introduction of Density Sketches. The method is theoretically sound, empirically validated, and has the potential for wide-ranging applications, though further exploration of its limitations could strengthen the contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper introduces Density Sketches (DS) as a theoretical framework aimed at addressing the challenges of data redundancy in streaming data by providing a succinct online summary of data distribution. The authors present a robust methodology that connects traditional density estimation techniques, such as Kernel Density Estimation (KDE) and histograms, with Locality Sensitive Hashing (LSH) to construct DS. The findings demonstrate that DS can effectively approximate the true underlying density while ensuring statistical guarantees of convergence, which positions it as a significant advancement in density estimation theory.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, which successfully address the limitations of existing density estimation methods by providing explicit statistical guarantees for DS. The framework's ability to mitigate the curse of dimensionality while maintaining efficiency is a noteworthy advancement. However, the paper could benefit from more empirical validation; while theoretical guarantees are well-established, practical demonstrations of the DS framework's performance in real-world scenarios are limited. Additionally, further elaboration on potential computational complexities associated with DS construction would enhance the analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical foundations, making complex concepts accessible to the reader. The quality of the writing is high, with a logical flow from motivation to conclusions. The novelty of the proposed DS framework is significant as it not only presents a new method but also offers a fresh perspective on the relationship between established density estimation techniques. The reproducibility of the results hinges on the theoretical guarantees provided, but empirical validation would strengthen this aspect considerably.\n\n# Summary Of The Review\nOverall, the paper provides a compelling theoretical contribution to the field of density estimation with the introduction of Density Sketches. While the theoretical framework is robust and well-articulated, the lack of empirical validation presents a limitation that should be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel streaming algorithm called Density Sketches (DS) designed for constructing summaries of data density distributions without retaining the actual samples. The methodology employs a compressed sketch of density that updates in real-time as data streams in, utilizing Count Sketch and a Min-Heap to maintain efficiency. Key findings indicate that DS achieves good performance in memory-constrained environments, effectively handling large datasets while ensuring the accuracy of density estimates. The authors also discuss the potential for achieving differential privacy by incorporating noise into the Count Sketch.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to density estimation using streaming algorithms, which is particularly beneficial for large and rapidly generated datasets. The use of Count Sketch and Min-Heap structures allows for a compact representation of data while maintaining reasonable accuracy. However, the paper does have weaknesses, including the inherent limitations associated with histograms, particularly the curse of dimensionality. This could affect the algorithm's applicability in high-dimensional settings, and the performance may vary significantly based on the chosen parameters, necessitating careful tuning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that effectively communicates the methodology and findings. The quality of the proposed algorithm is supported by theoretical guarantees regarding memory usage and performance. However, the reproducibility of the results may be hindered by the lack of detailed implementation specifics and code availability, which are only mentioned to be present in supplementary materials. The novelty of using a streaming algorithm for density estimation is commendable, but the limitations in high-dimensional cases should be addressed more thoroughly.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of data density estimation through the introduction of Density Sketches. Its innovative streaming approach is particularly suited for handling large datasets in resource-constrained environments, although it does face challenges in high-dimensional scenarios. The clarity of presentation is strong, though the reproducibility could be improved with more detailed implementation guidance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel approach called Density Sketches (DS) aimed at data reduction in streaming environments. The authors claim that DS can effectively replace traditional data representations while managing high-dimensional data more efficiently. They present experiments suggesting that DS performs comparably in sampling quality to true distributions and assert its advantages over existing methods like histograms, Kernel Density Estimation (KDE), and coresets, particularly in terms of memory efficiency and computational simplicity.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its ambition to propose a new method for data summarization in streaming environments and its focus on memory efficiency, which is a relevant concern in real-world applications. However, several weaknesses undermine its contributions. Firstly, DS does not demonstrate clear superiority over established methods, such as generative models (e.g., GANs and VAEs) that have proven capabilities in generating realistic data samples. Additionally, the comparison of DS to coresets lacks substantial experimental validation, and the advantages claimed in handling high-dimensional data appear overstated. The reliance on visualizations for sampling quality assessment is misleading, as it does not provide rigorous statistical validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, but the quality of the claims made is questionable due to insufficient empirical validation and engagement with existing literature. The novelty of DS is undermined by the fact that many existing methods, such as advanced versions of KDE and coresets, already address similar problems with established theoretical foundations. Furthermore, the reproducibility of the results is hampered by a limited experimental scope and a lack of comprehensive comparative analysis with existing methods.\n\n# Summary Of The Review\nOverall, while the paper presents an interesting approach to data summarization, it fails to convincingly demonstrate its advantages over well-established methods. The claims made regarding DS's performance are not sufficiently validated, and the paper lacks engagement with existing literature in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled “DENSITY SKETCHES FOR SAMPLING AND ESTIMATION” introduces a novel framework called Density Sketches (DS) aimed at efficiently managing data in streaming applications where data is generated at an unprecedented rate. The methodology involves constructing density sketches that enable accurate point-wise density estimation while minimizing memory usage. The findings demonstrate that the proposed approach significantly reduces data redundancy and improves estimation accuracy in high-dimensional settings.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to density estimation in streaming data contexts, addressing a critical challenge in machine learning applications. However, the paper has several weaknesses, including unclear phrasing and inconsistent definitions, which could hinder reader comprehension. Specific areas such as the explanation of coreset and the need for clearer notations could be improved to enhance the overall quality.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel approach, the clarity is compromised by multiple instances of awkward phrasing and insufficient definitions of key terms. The quality of the writing could be improved through tighter editing and consistent formatting, particularly in equations and notation. The novelty of the contribution is significant; however, reproducibility may be challenging due to the lack of detailed explanations and examples provided in the methodology.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of density estimation in streaming data but suffers from clarity issues and inconsistent formatting that detract from its impact. With revisions focused on improving clarity and consistency, the paper could significantly enhance its readability and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces the Density Sketches (DS) approach, a novel technique for efficient data representation that aims to provide theoretical guarantees in the context of density estimation. The authors employ a mathematical framework to demonstrate the asymptotic convergence properties of DS, detailing its applicability to various data distributions. The findings suggest that DS can serve as a robust method for data reduction, with potential applications in various fields, although the scope of empirical validation is currently limited.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its theoretical foundation and the introduction of a new methodology for density estimation that has the potential for practical applications. However, significant weaknesses include a narrow focus on the asymptotic properties without addressing real-world complexities, such as the curse of dimensionality and the limitations of the DS approach with non-uniform distributions. Additionally, the paper lacks a comprehensive comparison with other state-of-the-art techniques, limiting its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, although some sections could benefit from more detailed explanations, particularly regarding the implications of the theoretical results. While the novelty of the DS approach is notable, the practical significance is not fully realized due to the limited empirical validation across diverse datasets. Reproducibility could be enhanced by providing more thorough experimental details and performance comparisons.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to density estimation through Density Sketches, backed by a solid theoretical foundation. However, its practical implications are not sufficiently explored, and the lack of a robust empirical evaluation diminishes its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents \"Density Sketches\" (DS) as a novel statistical tool aimed at efficiently summarizing data distributions in a streaming context. It addresses challenges such as data redundancy and the curse of dimensionality by introducing a density estimation framework that encompasses parametric, non-parametric, and learning-based approaches. The authors derive a key theoretical result (Theorem 1) that bounds the Integrated Mean Square Error (IMSE) for the sampling distribution, highlighting the relationship between the accuracy of the density estimate and the statistical properties of the underlying function. Empirical evaluations demonstrate the superiority of DS over traditional methods like random sampling and Liberty coresets, showcasing significant improvements in performance metrics.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical analysis and comprehensive empirical evaluation. The derivation of the IMSE provides clear statistical guarantees for the proposed method, which is a significant contribution to the field of density estimation. However, a weakness is the reliance on theoretical assumptions that may not hold in all practical scenarios, particularly concerning the capture ratio and the underlying distribution properties. Additionally, while the empirical results are compelling, the paper could benefit from more extensive comparisons to a wider range of existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, making complex statistical concepts accessible to the reader. The quality of the analysis is high, with clear definitions and explanations of the methodology. The novelty of the approach is evident, particularly in the context of high-dimensional data streaming. Reproducibility is supported by the detailed statistical framework and empirical results, although the authors could enhance this by providing additional information on experimental setups and datasets used.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of density estimation by introducing Density Sketches, supported by a solid theoretical foundation and robust empirical evaluations. While the methodology is innovative and the results are promising, the paper could further strengthen its claims by addressing potential limitations and expanding the comparative analysis with other methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel approach aimed at efficient density estimation and sampling from large datasets. The methodology involves constructing a hierarchical data structure that purportedly improves upon traditional histogram-based methods, particularly in streaming contexts. The findings suggest that DS can provide competitive performance in terms of speed and accuracy compared to existing methods, particularly for specific datasets and scenarios.\n\n# Strength And Weaknesses\nThe paper's main strengths lie in its innovative approach to density estimation and its potential efficiency in handling streaming data. However, several weaknesses are notable. The lack of comprehensive benchmarking against a broader range of existing methods limits the evaluation of DS's effectiveness. Additionally, the method inherits the curse of dimensionality, which poses challenges in high-dimensional spaces. The empirical validation is insufficient, as the experiments conducted do not cover a diverse set of datasets that would highlight the method's robustness. Furthermore, the paper does not adequately address the limitations regarding data types, efficiency concerns, and privacy guarantees.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-organized and presents its contributions clearly. However, the novelty is somewhat undermined by the limited exploration of alternative methods and the lack of a detailed trade-off analysis. While the methodology is reproducible in theory, the absence of detailed empirical validation and specific privacy implementation details raises concerns about practical application. Overall, the quality of the writing is good, but the depth of analysis could be improved to enhance clarity.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to density estimation with Density Sketches; however, it falls short in several key areas, including empirical validation and benchmarking against a wider array of methods. The potential of the proposed method is tempered by limitations related to high-dimensional performance, data type constraints, and privacy considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces \"Density Sketches\" (DS), a methodology aimed at addressing data redundancy in machine learning pipelines by providing a compressed representation of streaming data without the need for storing the actual data. The authors claim that DS reduces redundancy, thereby offering a more efficient alternative to traditional coresets. They present theoretical guarantees regarding the accuracy of their approach and conduct empirical evaluations using visualizations and classification tasks to demonstrate its effectiveness.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle the well-known issue of data redundancy in machine learning, presenting a novel compression method for streaming data. However, the weaknesses are pronounced; many concepts and techniques discussed are not new, and the authors do not provide substantial evidence to support their claims of efficiency or originality. The paper often reiterates established knowledge without offering significant advancements or insights.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by an overemphasis on basic concepts that are familiar to those in the field, leading to a perception of redundancy throughout the text. While the methodology is laid out systematically, it lacks depth and fails to provide new insights into the established techniques. Reproducibility is not adequately addressed, as the experimental validation appears superficial, relying on basic visualizations rather than comprehensive performance metrics.\n\n# Summary Of The Review\nThis paper presents a method for reducing data redundancy in machine learning through Density Sketches, but it ultimately lacks originality and significant contributions to the field. The work primarily reiterates existing concepts without providing substantial evidence of its claims, making it less relevant for researchers seeking novel insights.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Density Sketches (DS), a novel method aimed at reducing data redundancy in streaming environments. DS is designed to function efficiently under compute and memory constraints, making it particularly relevant for edge computing applications. The authors provide theoretical guarantees for DS, demonstrating its effectiveness in sampling distributions and addressing challenges like the curse of dimensionality in high-dimensional data processing. Empirical results show that DS performs well in classification tasks, suggesting its potential integration with existing generative models and privacy-preserving approaches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its practical applicability to real-time data processing, particularly in environments where resources are limited. The theoretical foundation, including guarantees regarding sampling distributions and its relationship to coresets, adds rigor to the proposed method. However, the paper could benefit from a deeper exploration of real-world applications, particularly in dynamic and variable data environments. Additionally, while the experimental results are promising, a more extensive benchmarking against state-of-the-art deep learning models would provide a clearer picture of DS's performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. The methodology is articulated in a way that allows for reproducibility; however, further details regarding the experimental setup and data used would enhance clarity. The novelty of the approach is significant, particularly in the context of its application to streaming data and its integration with privacy considerations. Overall, the quality of the work is high, though some areas for improvement exist in terms of elaborating on practical implications and potential extensions.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to data summarization in streaming environments through Density Sketches. While the theoretical and empirical contributions are notable, the authors could enhance the paper by exploring real-world applications and providing more comprehensive benchmarks against existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Density Sketches (DS) method, which aims to enhance data sampling and density estimation within a streaming framework. It evaluates the DS method on large datasets, specifically Higgs and Susy, demonstrating that it significantly outperforms traditional methods such as random sampling and Liberty coresets. The findings indicate that DS not only maintains high accuracy and low loss in classification tasks but also achieves substantial memory efficiency, making it a viable alternative for machine learning applications, especially in resource-constrained environments.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its rigorous experimental evaluation on large datasets, which convincingly demonstrates the efficacy of the DS method. It shows clear advantages over established baselines in both performance and memory efficiency, addressing a critical need in machine learning for managing large-scale data streams. However, a potential weakness is the paper's reliance on specific parameter settings to achieve optimal results, which could limit the generalizability of the findings. Additionally, the discussion of parameter sensitivity could benefit from deeper insights into how these parameters impact performance across a broader range of datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its methodology and findings clearly, making it accessible to readers. The quality of the experimental design is high, with relevant comparisons and thorough analysis. In terms of novelty, while the concept of density estimation in streaming contexts is not entirely new, the specific implementation and performance gains presented by DS offer significant contributions to the field. The reproducibility of the results is supported by detailed descriptions of the experiments and the specific datasets used, although sharing code or detailed implementation would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a strong contribution to the field of data sampling and density estimation with its Density Sketches method. It demonstrates impressive performance improvements over existing techniques and offers a practical solution for handling large datasets in constrained environments. However, the dependency on specific parameter configurations may limit its broader applicability.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThis paper presents a novel approach to density estimation using Density Sketches (DS), a method aimed at efficiently approximating underlying density distributions in large data streams. The authors propose a new algorithm that leverages DS to maintain an accurate representation of the density while operating under memory constraints. The findings demonstrate that the proposed method achieves asymptotic convergence to the true density distribution, showcasing its effectiveness through various empirical evaluations on synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to addressing the challenges of density estimation in streaming data. The theoretical underpinnings are solid, providing a clear framework for how DS can operate under constraints. However, the paper suffers from several weaknesses, including a lack of clarity in the exposition, redundant phrasing, and inconsistent terminology. Additionally, the presence of typographical errors detracts from its overall professionalism, and the conclusions do not adequately emphasize the significance of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nAlthough the paper contributes a novel methodology, the clarity is compromised by convoluted sentence structures and technical jargon that may alienate readers unfamiliar with the subject matter. Consistency in terminology, particularly with acronyms like DS, needs improvement to enhance comprehension. The paper's reproducibility may also be hindered by vague figure captions and an appendix that lacks a clear structure. Overall, while the technical content is robust, the presentation requires significant refinement to ensure accessibility and reproducibility.\n\n# Summary Of The Review\nThis paper introduces an intriguing method for density estimation using Density Sketches, but it suffers from clarity issues and a lack of coherence in its presentation. Improvements in writing quality and structure are needed to fully convey the significance of the contributions made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.525621087234937,
    -1.7420002201645213,
    -2.0589511186382374,
    -1.8531373670625717,
    -1.9670872611101098,
    -1.6854924351317915,
    -1.6761622140261454,
    -1.7170557874975094,
    -2.0734554702974197,
    -1.7557640985988623,
    -1.860109061028229,
    -1.5355923685949722,
    -1.734140831479738,
    -1.5544350207775681,
    -1.6329388640522269,
    -1.7461328823999462,
    -1.8271103057620037,
    -1.6332811757638135,
    -1.8119526805984296,
    -1.8899821081833277,
    -1.8696263569325486,
    -1.8764590414423061,
    -1.7925621804722307,
    -1.7109941173843335,
    -1.9216250617206765,
    -1.9270574236425893,
    -1.8182108854149095,
    -1.6839570125235404,
    -1.8193507826400948
  ],
  "logp_cond": [
    [
      0.0,
      -2.4524960478004085,
      -2.444503588253425,
      -2.455750408780565,
      -2.4578637465309106,
      -2.4618689533272664,
      -2.462688995916986,
      -2.4602911439157453,
      -2.457891173493264,
      -2.4600263712165003,
      -2.4479386604494815,
      -2.467001827073992,
      -2.460255873857328,
      -2.462587659806301,
      -2.4405887773119486,
      -2.4508151582098927,
      -2.4603100684471495,
      -2.458135676510338,
      -2.4606085422020305,
      -2.4540491071972625,
      -2.443045917178469,
      -2.4560332184105107,
      -2.458675279178144,
      -2.4523686519161005,
      -2.457358215594377,
      -2.4549902497356926,
      -2.4513894163373235,
      -2.4609220302447214,
      -2.4542877421428018
    ],
    [
      -1.456979811552774,
      0.0,
      -1.3194487573860174,
      -1.348976501814565,
      -1.3540665162283507,
      -1.4420108809441468,
      -1.408863329747863,
      -1.3816998395212985,
      -1.3597019327118254,
      -1.3918600873013491,
      -1.3321057147039799,
      -1.4923355154146087,
      -1.338195301582463,
      -1.4011362295356555,
      -1.367540627974955,
      -1.323878252453923,
      -1.395528647387576,
      -1.3303236498328215,
      -1.3772066777160852,
      -1.3736145794613601,
      -1.3932096658337965,
      -1.4286058630837917,
      -1.4182202918953604,
      -1.332547528637315,
      -1.3956773021166005,
      -1.3715596310343405,
      -1.337800994898223,
      -1.4069698558963148,
      -1.4471866292467548
    ],
    [
      -1.7430346501452507,
      -1.6254214813718835,
      0.0,
      -1.6537135099086235,
      -1.5983296260940958,
      -1.7617421543664873,
      -1.6760376221200954,
      -1.636894254663792,
      -1.6153762883921834,
      -1.7194202289312484,
      -1.607997314741645,
      -1.7801196433198072,
      -1.6323699864280439,
      -1.6827001735539577,
      -1.674348135972669,
      -1.667891489442096,
      -1.646855529025994,
      -1.6959707164885067,
      -1.6576247978090939,
      -1.6116356831633813,
      -1.6837172759616306,
      -1.6857350704336065,
      -1.71620764336346,
      -1.6817298877572742,
      -1.6481908069150113,
      -1.650538893511097,
      -1.6279983974247285,
      -1.7125284080145327,
      -1.7496325926469551
    ],
    [
      -1.5918115391630434,
      -1.429103012184893,
      -1.4065098953044914,
      0.0,
      -1.4299071325505126,
      -1.5459683595488651,
      -1.5265733091285874,
      -1.4660704358395509,
      -1.4475503416689197,
      -1.478012679416513,
      -1.5009552016673329,
      -1.5831470038937827,
      -1.4638986040026822,
      -1.5026822314037298,
      -1.4915860469164224,
      -1.5315782948252321,
      -1.468326716088339,
      -1.4989986983931298,
      -1.4334121600437275,
      -1.4554244276460548,
      -1.5034034588114344,
      -1.5158667217221686,
      -1.4971981917578812,
      -1.4944971518077732,
      -1.4824667390105115,
      -1.4649564164737956,
      -1.4390851181836746,
      -1.5151099987519951,
      -1.5588256993012557
    ],
    [
      -1.624154193531727,
      -1.455621477093705,
      -1.4202011074786394,
      -1.5179342935709665,
      0.0,
      -1.627389309966143,
      -1.5587319302649212,
      -1.492130971678153,
      -1.4813540815691577,
      -1.5358921174582212,
      -1.5477726452467233,
      -1.629477748474204,
      -1.5299163559719604,
      -1.5482437057382576,
      -1.5002071107244628,
      -1.5242188724936503,
      -1.5431797863776076,
      -1.4961532701364353,
      -1.5045514978803964,
      -1.5369552956088555,
      -1.5501552833519796,
      -1.5174913646778667,
      -1.555864877090345,
      -1.5290666738617618,
      -1.4723567116393421,
      -1.5251876734055347,
      -1.4626572582545143,
      -1.5469894934433601,
      -1.560675119265848
    ],
    [
      -1.4174427431217393,
      -1.2864450451239886,
      -1.3164070246537518,
      -1.3395024364457653,
      -1.334598409199343,
      0.0,
      -1.3451482400142336,
      -1.3493213621742446,
      -1.2878909441348254,
      -1.3335348230731003,
      -1.3030987593455108,
      -1.3629699531602066,
      -1.2556280081895506,
      -1.2976785469210415,
      -1.3448988534658928,
      -1.2582541014239303,
      -1.3464558031670295,
      -1.27905134148527,
      -1.2910402531909715,
      -1.3302783865561962,
      -1.3186467604947514,
      -1.3240853588062311,
      -1.3598178546226563,
      -1.3189363880601728,
      -1.3407421193918183,
      -1.3649335754260623,
      -1.3319999201371029,
      -1.341138065975253,
      -1.3213818335590681
    ],
    [
      -1.3288438654755186,
      -1.1901252118683283,
      -1.2266785911550877,
      -1.2133201280939687,
      -1.222224168269653,
      -1.340660595987007,
      0.0,
      -1.243824794145541,
      -1.2686158788534094,
      -1.2418346382495467,
      -1.197021639849223,
      -1.3479363108129991,
      -1.2354560931428102,
      -1.2813159509667684,
      -1.2411899961498074,
      -1.2398962144556676,
      -1.2476448293090874,
      -1.2658629259048153,
      -1.216247767052259,
      -1.2192758597995197,
      -1.2427078598539762,
      -1.2783942985783152,
      -1.2379840819626766,
      -1.22231436605949,
      -1.2388032294290576,
      -1.2732649996169367,
      -1.2108936591126545,
      -1.25594493687801,
      -1.2718004288659321
    ],
    [
      -1.4192107622273957,
      -1.314032587769552,
      -1.2314799445820757,
      -1.3111917278385843,
      -1.2727667751431826,
      -1.4082510176731928,
      -1.3234832253997368,
      0.0,
      -1.3751389287564606,
      -1.389124167742149,
      -1.3433837052936795,
      -1.4188764610184137,
      -1.3312395484086539,
      -1.3634535487676807,
      -1.3053986229597048,
      -1.3547452374173055,
      -1.3027547864188915,
      -1.3596539891769213,
      -1.2254648977979723,
      -1.3223665416219335,
      -1.3475546755408823,
      -1.3653659493233035,
      -1.3465796200193034,
      -1.3137753544266542,
      -1.3395004231819791,
      -1.205544129005707,
      -1.2798689924323368,
      -1.366645023878511,
      -1.3514465149911719
    ],
    [
      -1.7464593063267648,
      -1.666442023077934,
      -1.6662779653039526,
      -1.7095523470973037,
      -1.670702254812439,
      -1.730575872893625,
      -1.7188763107303198,
      -1.6475945348554453,
      0.0,
      -1.7300318705827682,
      -1.708311552096037,
      -1.7376070173603835,
      -1.7028309834158175,
      -1.7147857131808069,
      -1.6944236554620258,
      -1.7259661421712957,
      -1.6812705766811502,
      -1.6690617719234064,
      -1.725011093360974,
      -1.7046406679050516,
      -1.6684378726283877,
      -1.6518665970062176,
      -1.7467933769558166,
      -1.6937903517535147,
      -1.6791941268418757,
      -1.6416807480427589,
      -1.6409357343607662,
      -1.7022129445471608,
      -1.7159125753074287
    ],
    [
      -1.4066908718904354,
      -1.2807990383245393,
      -1.2814621950809286,
      -1.244679593715386,
      -1.2893515147373338,
      -1.3625982924617503,
      -1.3347637505565197,
      -1.3040508193413791,
      -1.3310142788561923,
      0.0,
      -1.3071082042787243,
      -1.4033351281925386,
      -1.2434321417462983,
      -1.2915810506631262,
      -1.3651824913206807,
      -1.351996450371169,
      -1.3070205157362966,
      -1.3385930598281168,
      -1.262653998805076,
      -1.282376706664936,
      -1.2491837136460298,
      -1.331018374704784,
      -1.3117503466923044,
      -1.3102705773828598,
      -1.3037371017580546,
      -1.3216065027527624,
      -1.2847054508486233,
      -1.3657139757798942,
      -1.3356960261131583
    ],
    [
      -1.5460947055991294,
      -1.316620640155073,
      -1.320833962149434,
      -1.4601518831918416,
      -1.4349669776114895,
      -1.489222371553666,
      -1.4817814770426123,
      -1.445760467897326,
      -1.421921370551137,
      -1.449824698379731,
      0.0,
      -1.5573576625370509,
      -1.3966719552843012,
      -1.447708003944854,
      -1.4306013833900597,
      -1.4265951117564926,
      -1.4273147497775052,
      -1.411793960910393,
      -1.3800741905650247,
      -1.4570211567338534,
      -1.414709438774372,
      -1.488328443504539,
      -1.4606627002279633,
      -1.4037140107341173,
      -1.467156266594754,
      -1.437643381186188,
      -1.4234372572134866,
      -1.4744536368094554,
      -1.486594114540355
    ],
    [
      -1.3006178855866013,
      -1.2797579823638296,
      -1.2743750527697506,
      -1.3060463237206987,
      -1.2772463353697636,
      -1.2459669878070219,
      -1.2934693892316254,
      -1.2454706114095264,
      -1.2548623596068182,
      -1.2719708287780425,
      -1.2978098015547124,
      0.0,
      -1.2781463183483548,
      -1.2635232107895147,
      -1.2721611138192574,
      -1.2883613301056835,
      -1.3085225991197726,
      -1.2784012970017753,
      -1.283982837452759,
      -1.2883481291056882,
      -1.2774243280778181,
      -1.2161638754825792,
      -1.2860640069072389,
      -1.2997107168444824,
      -1.2530665786072517,
      -1.2481473818357594,
      -1.2721766167915587,
      -1.2755576263186907,
      -1.2250977646641716
    ],
    [
      -1.472603435186302,
      -1.3025475915744744,
      -1.3151177468025497,
      -1.366809147220642,
      -1.3328805983589485,
      -1.3961029133419947,
      -1.3874788521516432,
      -1.3466565369009977,
      -1.3895626938144487,
      -1.3487080437305727,
      -1.3541741229069826,
      -1.433329636873913,
      0.0,
      -1.2767313727816216,
      -1.400637697587296,
      -1.3706642423078208,
      -1.3148594896330015,
      -1.3307137997370166,
      -1.3201307769001347,
      -1.3279778725694695,
      -1.3432628084149394,
      -1.3947057212662417,
      -1.3635878989904644,
      -1.3937582138137175,
      -1.363263502785506,
      -1.3325598783912544,
      -1.341093591823028,
      -1.3926051230301246,
      -1.438035751983628
    ],
    [
      -1.2924742911612273,
      -1.1733097218732371,
      -1.1391996794062962,
      -1.1829811534984085,
      -1.1752429922842094,
      -1.1909417676071246,
      -1.2450377190972337,
      -1.1743851080930945,
      -1.1466483741943714,
      -1.169337639593239,
      -1.1606306446671324,
      -1.249259938457679,
      -1.0937164424053996,
      0.0,
      -1.202780801789593,
      -1.19133652635966,
      -1.1769577488607996,
      -1.1813595407299131,
      -1.1392302427265768,
      -1.2028807394199006,
      -1.2300150972003958,
      -1.185864631106011,
      -1.2343627089017561,
      -1.227558459286237,
      -1.1864361785096773,
      -1.1977473556845217,
      -1.1623684693237837,
      -1.2072691827982016,
      -1.2024237842085017
    ],
    [
      -1.3744071792661967,
      -1.299995261731902,
      -1.257677956015111,
      -1.3221139464702483,
      -1.3142933330434157,
      -1.348119982734863,
      -1.3387980998528106,
      -1.3198162762111076,
      -1.2912731484835285,
      -1.340431551195309,
      -1.3304158527404295,
      -1.358892046859428,
      -1.3111710224570592,
      -1.3339588681414918,
      0.0,
      -1.3376467101624967,
      -1.3669845435372863,
      -1.3132667611209783,
      -1.3153056851183704,
      -1.3393277013519949,
      -1.3278113562143832,
      -1.309030064526606,
      -1.3115389468634586,
      -1.2553591158910882,
      -1.3433309312650816,
      -1.3221013444329022,
      -1.3005449190776721,
      -1.300391025831793,
      -1.3400110074565275
    ],
    [
      -1.411330738041215,
      -1.2535518418759262,
      -1.269117023178992,
      -1.3133630302655899,
      -1.3390788626364298,
      -1.3117842998629292,
      -1.3464452067728905,
      -1.3356305317176733,
      -1.3223011583852913,
      -1.344336149674687,
      -1.3258666724799535,
      -1.4437231180978058,
      -1.2915818512195019,
      -1.3140501066308528,
      -1.3490549783508026,
      0.0,
      -1.3481891692579984,
      -1.3284664798840646,
      -1.354407868725064,
      -1.2854946489568482,
      -1.3560220428521588,
      -1.3525027640064626,
      -1.3857641833013044,
      -1.3127375525324592,
      -1.3159068684789585,
      -1.3649211135940804,
      -1.2857961562971747,
      -1.364810627109406,
      -1.3677793697259795
    ],
    [
      -1.5545648974964221,
      -1.430475795641518,
      -1.3921089939240585,
      -1.4173763429885737,
      -1.3822113976106314,
      -1.5534006146719934,
      -1.4523509190524475,
      -1.4125558331007615,
      -1.4896996450906865,
      -1.4595105487106874,
      -1.427771478226056,
      -1.5724248768862892,
      -1.4193083145521725,
      -1.4424895338252568,
      -1.4651910004781385,
      -1.4626671761836625,
      0.0,
      -1.4757366337875104,
      -1.3868682624729067,
      -1.42515237460297,
      -1.4488270758392237,
      -1.5018567800758638,
      -1.443386076099487,
      -1.437463906633619,
      -1.4114328464740915,
      -1.402679859853442,
      -1.4167670467301463,
      -1.4765133868261657,
      -1.5590447200658697
    ],
    [
      -1.3659447280113142,
      -1.1612442944552985,
      -1.1861240183549329,
      -1.230626772389245,
      -1.1702049225399913,
      -1.215573777427756,
      -1.288913883812572,
      -1.2277413997728515,
      -1.1952476650770851,
      -1.2724564556794509,
      -1.1965623432855292,
      -1.291451696620281,
      -1.1971415826273832,
      -1.1782839275666839,
      -1.2117160594005736,
      -1.241780750652515,
      -1.2785607605276643,
      0.0,
      -1.206560908795064,
      -1.2566373830398043,
      -1.2392959737035698,
      -1.2399714351867375,
      -1.2730959347698496,
      -1.2225717287671418,
      -1.265957169152701,
      -1.2518070324019719,
      -1.1765641021139974,
      -1.2554721662886417,
      -1.2415051622815136
    ],
    [
      -1.5197994320566897,
      -1.429105047732751,
      -1.392493968551885,
      -1.4617357019899946,
      -1.4370271986406755,
      -1.4917606772859546,
      -1.4489219901238535,
      -1.3946219173114291,
      -1.471807635244533,
      -1.4183360473597624,
      -1.4410835576499856,
      -1.5199151860419364,
      -1.3878167989507468,
      -1.4009366845013906,
      -1.4211574457384364,
      -1.468223481161519,
      -1.4047258903443864,
      -1.4746621698917557,
      0.0,
      -1.4416299450993175,
      -1.4294521460589287,
      -1.44017414339328,
      -1.454999193150843,
      -1.4297259376190448,
      -1.4316859084812,
      -1.4241495401749633,
      -1.452080849284499,
      -1.4325481871726093,
      -1.4817258795898072
    ],
    [
      -1.5702489469883611,
      -1.536649531830371,
      -1.4749908819814819,
      -1.5224808933022607,
      -1.5007637354084395,
      -1.562480443732201,
      -1.5374348083787224,
      -1.5545350651433494,
      -1.5444746440461852,
      -1.5214869478209725,
      -1.5343722119274585,
      -1.6136687609147298,
      -1.5016711676626393,
      -1.5621471468646235,
      -1.535293651720781,
      -1.534949581478034,
      -1.5107667980377273,
      -1.6222560842824587,
      -1.5653696992190138,
      0.0,
      -1.5280265615663222,
      -1.5595762289435717,
      -1.5169602763585701,
      -1.5576387262604747,
      -1.475011298024715,
      -1.5640931588868945,
      -1.5431227241384757,
      -1.54933501344914,
      -1.5702311138495084
    ],
    [
      -1.5395311891934975,
      -1.476619065588087,
      -1.4805919957703306,
      -1.5175268717574417,
      -1.518126412121625,
      -1.5581366938104362,
      -1.5389712575932768,
      -1.4443687156775988,
      -1.5182917243183096,
      -1.4654874855277138,
      -1.4953693727540405,
      -1.5951507373415321,
      -1.4717659917374135,
      -1.5482600530307222,
      -1.5107170411478568,
      -1.4946101322589302,
      -1.464312277636253,
      -1.5437959908279695,
      -1.5070924049572987,
      -1.4986520774113263,
      0.0,
      -1.5315991319084643,
      -1.5356991742006103,
      -1.5037731176950964,
      -1.4839052338480585,
      -1.4939536305553036,
      -1.4707469588241346,
      -1.5418171747907512,
      -1.583410022923753
    ],
    [
      -1.4743217215997764,
      -1.4733615488777991,
      -1.4590826802109427,
      -1.4718077277121877,
      -1.4605796973232408,
      -1.4341528574809395,
      -1.48178052582439,
      -1.4348106386619899,
      -1.3814092051720266,
      -1.487965509040456,
      -1.4712539476412345,
      -1.475097425327067,
      -1.4320806293798323,
      -1.421137176300633,
      -1.3879958698404753,
      -1.4713863676547791,
      -1.4776242207210828,
      -1.4201868962264466,
      -1.4452044081743058,
      -1.4735493478326416,
      -1.469143753248682,
      0.0,
      -1.5170771267699577,
      -1.44678984239294,
      -1.4695762381200193,
      -1.431382691847628,
      -1.430594200531778,
      -1.4442483850285157,
      -1.3018584352824292
    ],
    [
      -1.4507821943835926,
      -1.3742425732911479,
      -1.3466699044170274,
      -1.3443333183709403,
      -1.3277105720789721,
      -1.4484242630085156,
      -1.3376859565520476,
      -1.3201290819626943,
      -1.3655432817620663,
      -1.310065850529479,
      -1.383054054957731,
      -1.4581308000281312,
      -1.3398960629041112,
      -1.4033883448313882,
      -1.341050561790461,
      -1.3667007314322124,
      -1.2933549812107483,
      -1.3855694227817796,
      -1.369919810489319,
      -1.4003884608056563,
      -1.2742497804733561,
      -1.4099121485801183,
      0.0,
      -1.3663389622515572,
      -1.2528662348077806,
      -1.3050031884447586,
      -1.3424593754849863,
      -1.3917797824880542,
      -1.3941942635094147
    ],
    [
      -1.4454182520784309,
      -1.2743432733277027,
      -1.3158027782861355,
      -1.36352175154997,
      -1.348778758527866,
      -1.3817681210179085,
      -1.3662790567242211,
      -1.3233686345849713,
      -1.3515640324532,
      -1.368546957914826,
      -1.3302256059795636,
      -1.4721454684800501,
      -1.3292247794479928,
      -1.374445904356711,
      -1.2535182936707032,
      -1.3386416990720864,
      -1.3598101978295043,
      -1.3576801875947833,
      -1.3131864015907517,
      -1.3671467493055998,
      -1.3298620979758278,
      -1.346901747635539,
      -1.3809060687424763,
      0.0,
      -1.372335862688825,
      -1.3148474934406753,
      -1.3319691314789495,
      -1.2985643515708882,
      -1.4011124565973332
    ],
    [
      -1.6286837276609571,
      -1.5555704908785652,
      -1.5269982896947332,
      -1.5711141270503433,
      -1.5211457319776094,
      -1.6106242507218933,
      -1.576974828060296,
      -1.5739946229815749,
      -1.5530196449004348,
      -1.5552080542077975,
      -1.5639515227417675,
      -1.619234547401781,
      -1.548248085560612,
      -1.590650292090892,
      -1.5790038393769017,
      -1.5846770317647538,
      -1.5368727976657957,
      -1.6107980626418257,
      -1.529703478754565,
      -1.5547538185173095,
      -1.5712520545071758,
      -1.5298107904002867,
      -1.5605288545451546,
      -1.5608976164216997,
      0.0,
      -1.566677380080604,
      -1.5269444277941975,
      -1.6005562317455422,
      -1.5874514454296067
    ],
    [
      -1.5885204437905502,
      -1.4794471852135467,
      -1.447049672497207,
      -1.4591342810328032,
      -1.4780950929138892,
      -1.5632152158391803,
      -1.561183412497949,
      -1.3805735283039975,
      -1.496644666646442,
      -1.5253370706884082,
      -1.5026878877995025,
      -1.6061954376012326,
      -1.5205199510426535,
      -1.5431805786581467,
      -1.4836888916195425,
      -1.5199750663408318,
      -1.5222139202429918,
      -1.5234503898723704,
      -1.5049402715960225,
      -1.5053406722905922,
      -1.5121182895568523,
      -1.5102263299407686,
      -1.5578128077208209,
      -1.473599611747918,
      -1.5123314279743498,
      0.0,
      -1.45222331284214,
      -1.527377420952463,
      -1.5157515461878592
    ],
    [
      -1.5194107519191666,
      -1.3702633444688461,
      -1.3525671311485399,
      -1.3860593116314528,
      -1.3862250182040374,
      -1.4892636534862873,
      -1.409883515705115,
      -1.393473343844647,
      -1.3793260185867535,
      -1.440873947231488,
      -1.4074524574405918,
      -1.5428028196181247,
      -1.3876873792452498,
      -1.4438804198565784,
      -1.4087143712954933,
      -1.4207301465186402,
      -1.432878009817727,
      -1.3790301439749535,
      -1.4168392894093864,
      -1.4256616036276448,
      -1.3899252683064711,
      -1.4763920621437079,
      -1.4478634129507264,
      -1.3983689829654806,
      -1.4130449812533525,
      -1.411923063500959,
      0.0,
      -1.4857962736008654,
      -1.5002236137663072
    ],
    [
      -1.4424997464944505,
      -1.3393886384116176,
      -1.3252663237963307,
      -1.3933621313400522,
      -1.3568448253990757,
      -1.4044723472811695,
      -1.3880688361489113,
      -1.3223120041688325,
      -1.3366771438752285,
      -1.3921967206654828,
      -1.3757819963671518,
      -1.4253785170566564,
      -1.3644928932342126,
      -1.3997082112701968,
      -1.3544010300428082,
      -1.3967132998921616,
      -1.3386385565548236,
      -1.390504033872441,
      -1.3735633419085262,
      -1.3446285288384008,
      -1.3207277101625987,
      -1.3546190438326613,
      -1.361269274009856,
      -1.3341189199644794,
      -1.371908919345188,
      -1.3265600394195483,
      -1.3768571778965617,
      0.0,
      -1.388047285873417
    ],
    [
      -1.4985972992703558,
      -1.4697772610601219,
      -1.5004441883887643,
      -1.527773269151048,
      -1.4082084894140918,
      -1.4432164391072706,
      -1.489369824672325,
      -1.4831786937149425,
      -1.4230585808476726,
      -1.472552940095928,
      -1.5228123104675528,
      -1.4630406416185813,
      -1.4717294617324808,
      -1.4378648281751698,
      -1.4434803997056043,
      -1.489488359785346,
      -1.5036635406923073,
      -1.435871574680726,
      -1.462946276965232,
      -1.48965742327157,
      -1.4769882006116253,
      -1.3718271287384,
      -1.4809434539681932,
      -1.5100816705222757,
      -1.4591588786280667,
      -1.4575484703656811,
      -1.4634574489331569,
      -1.4620197495003902,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.07312503943452864,
      0.08111749898151199,
      0.06987067845437211,
      0.06775734070402661,
      0.06375213390767076,
      0.06293209131795097,
      0.06532994331919184,
      0.06772991374167336,
      0.06559471601843692,
      0.07768242678545567,
      0.058619260160944986,
      0.06536521337760925,
      0.06303342742863638,
      0.0850323099229886,
      0.07480592902504446,
      0.06531101878778767,
      0.06748541072459924,
      0.06501254503290665,
      0.07157198003767462,
      0.0825751700564683,
      0.06958786882442647,
      0.06694580805679307,
      0.07325243531883663,
      0.06826287164056,
      0.07063083749924459,
      0.07423167089761362,
      0.06469905699021572,
      0.0713333450921354
    ],
    [
      0.2850204086117474,
      0.0,
      0.42255146277850386,
      0.3930237183499563,
      0.38793370393617055,
      0.2999893392203745,
      0.3331368904166583,
      0.3603003806432228,
      0.3822982874526959,
      0.35014013286317214,
      0.4098945054605414,
      0.24966470474991254,
      0.40380491858205825,
      0.34086399062886574,
      0.37445959218956637,
      0.4181219677105983,
      0.3464715727769452,
      0.4116765703316998,
      0.3647935424484361,
      0.36838564070316115,
      0.3487905543307248,
      0.3133943570807296,
      0.3237799282691609,
      0.4094526915272063,
      0.3463229180479208,
      0.37044058913018074,
      0.40419922526629826,
      0.3350303642682064,
      0.2948135909177665
    ],
    [
      0.3159164684929867,
      0.43352963726635396,
      0.0,
      0.4052376087296139,
      0.46062149254414164,
      0.29720896427175014,
      0.382913496518142,
      0.42205686397444553,
      0.44357483024605404,
      0.3395308897069891,
      0.45095380389659234,
      0.2788314753184302,
      0.42658113221019356,
      0.3762509450842797,
      0.38460298266556836,
      0.39105962919614146,
      0.41209558961224335,
      0.3629804021497307,
      0.40132632082914355,
      0.4473154354748561,
      0.37523384267660687,
      0.37321604820463095,
      0.34274347527477733,
      0.37722123088096327,
      0.41076031172322613,
      0.4084122251271405,
      0.4309527212135089,
      0.3464227106237048,
      0.3093185259912823
    ],
    [
      0.2613258278995283,
      0.4240343548776786,
      0.44662747175808026,
      0.0,
      0.4232302345120591,
      0.30716900751370657,
      0.32656405793398435,
      0.3870669312230208,
      0.405587025393652,
      0.3751246876460588,
      0.35218216539523883,
      0.269990363168789,
      0.38923876305988947,
      0.3504551356588419,
      0.3615513201461493,
      0.3215590722373396,
      0.3848106509742326,
      0.3541386686694419,
      0.4197252070188442,
      0.39771293941651686,
      0.34973390825113726,
      0.3372706453404031,
      0.35593917530469055,
      0.3586402152547985,
      0.3706706280520602,
      0.3881809505887761,
      0.41405224887889713,
      0.33802736831057656,
      0.294311667761316
    ],
    [
      0.3429330675783828,
      0.5114657840164047,
      0.5468861536314704,
      0.44915296753914324,
      0.0,
      0.33969795114396684,
      0.4083553308451886,
      0.4749562894319568,
      0.4857331795409521,
      0.4311951436518886,
      0.4193146158633865,
      0.3376095126359058,
      0.4371709051381494,
      0.41884355537185214,
      0.466880150385647,
      0.4428683886164595,
      0.42390747473250223,
      0.4709339909736745,
      0.46253576322971335,
      0.4301319655012543,
      0.4169319777581302,
      0.44959589643224307,
      0.41122238401976485,
      0.438020587248348,
      0.49473054947076767,
      0.44189958770457505,
      0.5044300028555955,
      0.42009776766674967,
      0.4064121418442619
    ],
    [
      0.26804969201005213,
      0.39904739000780287,
      0.36908541047803967,
      0.3459899986860262,
      0.35089402593244845,
      0.0,
      0.34034419511755787,
      0.3361710729575469,
      0.39760149099696607,
      0.35195761205869114,
      0.3823936757862807,
      0.32252248197158484,
      0.4298644269422409,
      0.38781388821075,
      0.3405935816658987,
      0.42723833370786113,
      0.33903663196476197,
      0.4064410936465215,
      0.39445218194082,
      0.35521404857559524,
      0.3668456746370401,
      0.36140707632556035,
      0.3256745805091352,
      0.3665560470716187,
      0.34475031573997317,
      0.32055885970572917,
      0.3534925149946886,
      0.3443543691565385,
      0.36411060157272335
    ],
    [
      0.3473183485506268,
      0.4860370021578171,
      0.4494836228710577,
      0.4628420859321767,
      0.4539380457564923,
      0.33550161803913836,
      0.0,
      0.4323374198806045,
      0.407546335172736,
      0.43432757577659875,
      0.47914057417692235,
      0.32822590321314626,
      0.44070612088333516,
      0.394846263059377,
      0.434972217876338,
      0.43626599957047785,
      0.428517384717058,
      0.4102992881213301,
      0.4599144469738865,
      0.4568863542266257,
      0.43345435417216915,
      0.3977679154478302,
      0.4381781320634688,
      0.45384784796665545,
      0.4373589845970878,
      0.40289721440920867,
      0.46526855491349095,
      0.4202172771481354,
      0.40436178516021326
    ],
    [
      0.2978450252701137,
      0.4030231997279574,
      0.48557584291543376,
      0.40586405965892514,
      0.44428901235432683,
      0.3088047698243166,
      0.3935725620977726,
      0.0,
      0.3419168587410488,
      0.32793161975536034,
      0.37367208220382997,
      0.29817932647909573,
      0.38581623908885554,
      0.35360223872982877,
      0.41165716453780465,
      0.36231055008020396,
      0.4143010010786179,
      0.35740179832058816,
      0.4915908896995371,
      0.39468924587557597,
      0.36950111195662716,
      0.35168983817420596,
      0.370476167478206,
      0.40328043307085526,
      0.3775553643155303,
      0.5115116584918025,
      0.43718679506517266,
      0.35041076361899837,
      0.36560927250633757
    ],
    [
      0.3269961639706549,
      0.40701344721948574,
      0.40717750499346717,
      0.363903123200116,
      0.40275321548498066,
      0.3428795974037948,
      0.35457915956709996,
      0.42586093544197445,
      0.0,
      0.34342359971465153,
      0.3651439182013827,
      0.33584845293703625,
      0.3706244868816022,
      0.35866975711661286,
      0.37903181483539394,
      0.347489328126124,
      0.3921848936162695,
      0.4043936983740133,
      0.3484443769364458,
      0.3688148023923681,
      0.405017597669032,
      0.4215888732912021,
      0.3266620933416031,
      0.37966511854390506,
      0.394261343455544,
      0.43177472225466085,
      0.4325197359366535,
      0.37124252575025896,
      0.35754289498999103
    ],
    [
      0.3490732267084269,
      0.47496506027432295,
      0.47430190351793367,
      0.5110845048834762,
      0.4664125838615285,
      0.39316580613711194,
      0.42100034804234254,
      0.45171327925748317,
      0.42474981974267,
      0.0,
      0.44865589432013797,
      0.3524289704063237,
      0.5123319568525639,
      0.46418304793573606,
      0.3905816072781816,
      0.4037676482276933,
      0.44874358286256566,
      0.41717103877074546,
      0.4931100997937863,
      0.4733873919339262,
      0.5065803849528325,
      0.4247457238940784,
      0.4440137519065579,
      0.4454935212160025,
      0.4520269968408077,
      0.4341575958460999,
      0.471058647750239,
      0.3900501228189681,
      0.420068072485704
    ],
    [
      0.3140143554290995,
      0.5434884208731559,
      0.5392750988787949,
      0.3999571778363873,
      0.4251420834167394,
      0.3708866894745628,
      0.3783275839856166,
      0.414348593130903,
      0.43818769047709183,
      0.410284362648498,
      0.0,
      0.30275139849117805,
      0.4634371057439277,
      0.41240105708337493,
      0.4295076776381692,
      0.43351394927173637,
      0.43279431125072376,
      0.4483151001178358,
      0.48003487046320426,
      0.4030879042943756,
      0.445399622253857,
      0.37178061752368996,
      0.39944636080026563,
      0.45639505029411165,
      0.392952794433475,
      0.422465679842041,
      0.43667180381474235,
      0.3856554242187735,
      0.373514946487874
    ],
    [
      0.23497448300837087,
      0.2558343862311425,
      0.26121731582522156,
      0.22954604487427344,
      0.25834603322520855,
      0.2896253807879503,
      0.2421229793633468,
      0.29012175718544575,
      0.2807300089881539,
      0.2636215398169297,
      0.2377825670402598,
      0.0,
      0.2574460502466174,
      0.2720691578054575,
      0.2634312547757147,
      0.24723103848928862,
      0.22706976947519952,
      0.2571910715931969,
      0.25160953114221307,
      0.24724423948928398,
      0.25816804051715403,
      0.31942849311239296,
      0.2495283616877333,
      0.2358816517504898,
      0.2825257899877205,
      0.2874449867592128,
      0.2634157518034135,
      0.2600347422762814,
      0.31049460393080053
    ],
    [
      0.261537396293436,
      0.43159323990526355,
      0.4190230846771883,
      0.3673316842590959,
      0.4012602331207895,
      0.3380379181377433,
      0.3466619793280947,
      0.38748429457874023,
      0.3445781376652892,
      0.38543278774916523,
      0.3799667085727554,
      0.30081119460582495,
      0.0,
      0.45740945869811633,
      0.3335031338924419,
      0.36347658917191716,
      0.4192813418467365,
      0.4034270317427213,
      0.41401005457960327,
      0.40616295891026843,
      0.3908780230647986,
      0.33943511021349626,
      0.37055293248927357,
      0.3403826176660205,
      0.37087732869423196,
      0.4015809530884835,
      0.39304723965670996,
      0.34153570844961334,
      0.29610507949611
    ],
    [
      0.26196072961634087,
      0.381125298904331,
      0.41523534137127194,
      0.37145386727915963,
      0.37919202849335876,
      0.36349325317044356,
      0.3093973016803344,
      0.38004991268447363,
      0.40778664658319674,
      0.385097381184329,
      0.3938043761104357,
      0.3051750823198891,
      0.46071857837216856,
      0.0,
      0.3516542189879752,
      0.3630984944179081,
      0.3774772719167685,
      0.373075480047655,
      0.4152047780509913,
      0.35155428135766753,
      0.3244199235771723,
      0.3685703896715571,
      0.320072311875812,
      0.32687656149133115,
      0.3679988422678908,
      0.3566876650930464,
      0.39206655145378444,
      0.3471658379793665,
      0.3520112365690664
    ],
    [
      0.25853168478603017,
      0.33294360232032494,
      0.37526090803711587,
      0.3108249175819786,
      0.3186455310088112,
      0.28481888131736377,
      0.29414076419941626,
      0.31312258784111924,
      0.3416657155686984,
      0.2925073128569178,
      0.3025230113117974,
      0.2740468171927988,
      0.3217678415951677,
      0.2989799959107351,
      0.0,
      0.2952921538897302,
      0.2659543205149406,
      0.3196721029312486,
      0.31763317893385645,
      0.293611162700232,
      0.3051275078378437,
      0.3239087995256209,
      0.32139991718876826,
      0.3775797481611387,
      0.28960793278714525,
      0.3108375196193247,
      0.33239394497455477,
      0.3325478382204339,
      0.2929278565956994
    ],
    [
      0.33480214435873124,
      0.4925810405240201,
      0.47701585922095413,
      0.43276985213435637,
      0.4070540197635164,
      0.434348582537017,
      0.39968767562705576,
      0.4105023506822729,
      0.42383172401465496,
      0.4017967327252592,
      0.42026620991999275,
      0.3024097643021404,
      0.4545510311804444,
      0.4320827757690935,
      0.39707790404914367,
      0.0,
      0.3979437131419479,
      0.4176664025158816,
      0.3917250136748822,
      0.46063823344309807,
      0.39011083954778747,
      0.3936301183934836,
      0.3603686990986419,
      0.4333953298674871,
      0.4302260139209877,
      0.3812117688058658,
      0.46033672610277154,
      0.3813222552905402,
      0.3783535126739668
    ],
    [
      0.2725454082655816,
      0.3966345101204858,
      0.4350013118379452,
      0.40973396277343,
      0.44489890815137234,
      0.2737096910900103,
      0.3747593867095562,
      0.41455447266124223,
      0.3374106606713172,
      0.3675997570513163,
      0.39933882753594774,
      0.25468542887571455,
      0.4078019912098312,
      0.38462077193674693,
      0.3619193052838652,
      0.36444312957834124,
      0.0,
      0.3513736719744933,
      0.440242043289097,
      0.4019579311590338,
      0.3782832299227801,
      0.32525352568613997,
      0.38372422966251674,
      0.3896463991283847,
      0.4156774592879122,
      0.4244304459085617,
      0.4103432590318574,
      0.350596918935838,
      0.26806558569613403
    ],
    [
      0.2673364477524993,
      0.47203688130851496,
      0.4471571574088806,
      0.4026544033745685,
      0.4630762532238222,
      0.4177073983360575,
      0.3443672919512415,
      0.405539775990962,
      0.43803351068672836,
      0.3608247200843626,
      0.43671883247828425,
      0.34182947914353257,
      0.43613959313643025,
      0.4549972481971296,
      0.4215651163632399,
      0.39150042511129857,
      0.35472041523614917,
      0.0,
      0.42672026696874954,
      0.37664379272400916,
      0.3939852020602437,
      0.393309740577076,
      0.3601852409939639,
      0.41070944699667167,
      0.36732400661111253,
      0.3814741433618416,
      0.45671707364981606,
      0.3778090094751718,
      0.39177601348229985
    ],
    [
      0.29215324854173996,
      0.3828476328656787,
      0.4194587120465447,
      0.35021697860843504,
      0.3749254819577541,
      0.320192003312475,
      0.36303069047457615,
      0.4173307632870005,
      0.34014504535389656,
      0.39361663323866725,
      0.370869122948444,
      0.2920374945564932,
      0.4241358816476828,
      0.411015996097039,
      0.39079523485999323,
      0.3437291994369107,
      0.4072267902540432,
      0.33729051070667393,
      0.0,
      0.3703227354991121,
      0.38250053453950095,
      0.37177853720514964,
      0.3569534874475866,
      0.38222674297938486,
      0.3802667721172297,
      0.3878031404234663,
      0.35987183131393063,
      0.3794044934258203,
      0.33022680100862245
    ],
    [
      0.31973316119496653,
      0.3533325763529567,
      0.4149912262018458,
      0.367501214881067,
      0.38921837277488813,
      0.3275016644511266,
      0.3525472998046053,
      0.3354470430399783,
      0.3455074641371425,
      0.3684951603623552,
      0.35560989625586914,
      0.2763133472685979,
      0.3883109405206884,
      0.32783496131870415,
      0.35468845646254676,
      0.3550325267052936,
      0.3792153101456004,
      0.26772602390086897,
      0.32461240896431387,
      0.0,
      0.36195554661700546,
      0.33040587923975595,
      0.37302183182475757,
      0.33234338192285295,
      0.4149708101586127,
      0.3258889492964332,
      0.34685938404485195,
      0.34064709473418775,
      0.31975099433381926
    ],
    [
      0.3300951677390511,
      0.39300729134446155,
      0.389034361162218,
      0.3520994851751069,
      0.3514999448109237,
      0.31148966312211246,
      0.33065509933927184,
      0.4252576412549498,
      0.35133463261423903,
      0.40413887140483484,
      0.37425698417850817,
      0.2744756195910165,
      0.3978603651951351,
      0.3213663039018264,
      0.3589093157846919,
      0.3750162246736184,
      0.4053140792962957,
      0.32583036610457916,
      0.36253395197524996,
      0.3709742795212223,
      0.0,
      0.33802722502408433,
      0.33392718273193833,
      0.3658532392374523,
      0.38572112308449014,
      0.375672726377245,
      0.398879398108414,
      0.32780918214179744,
      0.28621633400879554
    ],
    [
      0.40213731984252976,
      0.403097492564507,
      0.41737636123136346,
      0.4046513137301184,
      0.41587934411906535,
      0.4423061839613667,
      0.3946785156179162,
      0.44164840278031625,
      0.49504983627027954,
      0.38849353240185014,
      0.4052050938010716,
      0.40136161611523913,
      0.44437841206247386,
      0.45532186514167305,
      0.48846317160183084,
      0.405072673787527,
      0.39883482072122334,
      0.45627214521585957,
      0.4312546332680003,
      0.40290969360966455,
      0.40731528819362417,
      0.0,
      0.3593819146723485,
      0.4296691990493662,
      0.4068828033222869,
      0.4450763495946781,
      0.44586484091052814,
      0.4322106564137904,
      0.574600606159877
    ],
    [
      0.34177998608863813,
      0.41831960718108285,
      0.4458922760552033,
      0.4482288621012904,
      0.46485160839325856,
      0.34413791746371514,
      0.4548762239201831,
      0.4724330985095364,
      0.4270188987101644,
      0.4824963299427516,
      0.40950812551449967,
      0.3344313804440995,
      0.4526661175681195,
      0.3891738356408425,
      0.4515116186817698,
      0.4258614490400183,
      0.4992071992614824,
      0.40699275769045107,
      0.4226423699829116,
      0.39217371966657444,
      0.5183123999988746,
      0.38265003189211244,
      0.0,
      0.42622321822067355,
      0.5396959456644501,
      0.4875589920274721,
      0.4501028049872444,
      0.4007823979841765,
      0.39836791696281604
    ],
    [
      0.2655758653059026,
      0.4366508440566308,
      0.39519133909819804,
      0.3474723658343635,
      0.3622153588564674,
      0.329225996366425,
      0.34471506066011237,
      0.38762548279936215,
      0.3594300849311336,
      0.34244715946950754,
      0.38076851140476986,
      0.23884864890428337,
      0.3817693379363407,
      0.3365482130276225,
      0.45747582371363027,
      0.3723524183122471,
      0.3511839195548292,
      0.35331392978955023,
      0.39780771579358176,
      0.34384736807873373,
      0.3811320194085057,
      0.36409236974879455,
      0.33008804864185715,
      0.0,
      0.3386582546955086,
      0.3961466239436582,
      0.379024985905384,
      0.41242976581344526,
      0.3098816607870003
    ],
    [
      0.29294133405971934,
      0.36605457084211124,
      0.3946267720259433,
      0.35051093467033323,
      0.4004793297430671,
      0.3110008109987832,
      0.3446502336603805,
      0.3476304387391016,
      0.3686054168202417,
      0.366417007512879,
      0.35767353897890897,
      0.3023905143188954,
      0.3733769761600645,
      0.33097476962978445,
      0.3426212223437748,
      0.33694802995592266,
      0.38475226405488083,
      0.3108269990788508,
      0.39192158296611157,
      0.36687124320336695,
      0.35037300721350073,
      0.3918142713203898,
      0.36109620717552193,
      0.3607274452989768,
      0.0,
      0.3549476816400725,
      0.394680633926479,
      0.3210688299751343,
      0.3341736162910698
    ],
    [
      0.338536979852039,
      0.4476102384290426,
      0.4800077511453822,
      0.46792314260978607,
      0.44896233072870007,
      0.3638422078034089,
      0.36587401114464035,
      0.5464838953385918,
      0.43041275699614734,
      0.40172035295418107,
      0.4243695358430868,
      0.32086198604135663,
      0.4065374725999358,
      0.3838768449844425,
      0.4433685320230467,
      0.40708235730175746,
      0.4048435033995974,
      0.4036070337702189,
      0.42211715204656675,
      0.42171675135199704,
      0.4149391340857369,
      0.4168310937018207,
      0.3692446159217684,
      0.45345781189467127,
      0.4147259956682394,
      0.0,
      0.47483411080044924,
      0.39968000269012616,
      0.41130587745473
    ],
    [
      0.29880013349574286,
      0.44794754094606337,
      0.4656437542663696,
      0.4321515737834567,
      0.43198586721087207,
      0.3289472319286222,
      0.40832736970979444,
      0.42473754157026256,
      0.438884866828156,
      0.3773369381834215,
      0.4107584279743177,
      0.2754080657967848,
      0.43052350616965973,
      0.37433046555833105,
      0.40949651411941623,
      0.39748073889626934,
      0.38533287559718254,
      0.439180741439956,
      0.4013715960055231,
      0.39254928178726467,
      0.4282856171084384,
      0.3418188232712016,
      0.3703474724641831,
      0.41984190244942887,
      0.40516590416155696,
      0.4062878219139505,
      0.0,
      0.3324146118140441,
      0.3179872716486023
    ],
    [
      0.2414572660290899,
      0.34456837411192276,
      0.35869068872720966,
      0.2905948811834882,
      0.32711218712446466,
      0.2794846652423708,
      0.29588817637462905,
      0.3616450083547078,
      0.34727986864831184,
      0.2917602918580575,
      0.30817501615638854,
      0.25857849546688394,
      0.3194641192893277,
      0.2842488012533435,
      0.32955598248073215,
      0.2872437126313787,
      0.3453184559687168,
      0.2934529786510993,
      0.3103936706150141,
      0.33932848368513957,
      0.36322930236094164,
      0.32933796869087906,
      0.3226877385136844,
      0.34983809255906095,
      0.31204809317835225,
      0.3573969731039921,
      0.30709983462697865,
      0.0,
      0.2959097266501234
    ],
    [
      0.32075348336973897,
      0.3495735215799729,
      0.3189065942513305,
      0.2915775134890468,
      0.41114229322600293,
      0.37613434353282416,
      0.3299809579677697,
      0.33617208892515227,
      0.3962922017924222,
      0.3467978425441667,
      0.2965384721725419,
      0.3563101410215135,
      0.347621320907614,
      0.38148595446492495,
      0.3758703829344905,
      0.32986242285474865,
      0.3156872419477874,
      0.38347920795936874,
      0.3564045056748628,
      0.32969335936852473,
      0.3423625820284695,
      0.4475236539016947,
      0.3384073286719016,
      0.30926911211781904,
      0.36019190401202805,
      0.3618023122744136,
      0.3558933337069379,
      0.3573310331397046,
      0.0
    ]
  ],
  "row_avgs": [
    0.0697374264835466,
    0.35888412673901715,
    0.38595960928226763,
    0.36303288186591814,
    0.438711181601012,
    0.3604450454418019,
    0.4261592383155001,
    0.38533088896846174,
    0.37733954220165433,
    0.44139366387565165,
    0.41871563322050737,
    0.26193346539958834,
    0.37162086501978325,
    0.3643722729474187,
    0.3106526269788862,
    0.41063236761735694,
    0.37283043655126624,
    0.3997449602387378,
    0.3690133034340661,
    0.34819510453270325,
    0.3577591449608403,
    0.4284069316485848,
    0.43171061034265756,
    0.36056854188706594,
    0.353934131521581,
    0.41731333852076663,
    0.39261944486067407,
    0.31613531619772456,
    0.35082375392277765
  ],
  "col_avgs": [
    0.3015766008614917,
    0.4021959994801437,
    0.4125647423355172,
    0.3726488900576267,
    0.39048988801554485,
    0.32839498823200747,
    0.34636024047768704,
    0.38814029519584164,
    0.3810329610280611,
    0.3589325246850464,
    0.3722559614388446,
    0.2944516759106218,
    0.3939289573056474,
    0.3649035973444275,
    0.37824205741071465,
    0.36141872785799484,
    0.3716977644538245,
    0.3647005541183162,
    0.38782659636779476,
    0.3726927579995401,
    0.3746943713141787,
    0.3589236032755232,
    0.34593104921738144,
    0.3752052528280183,
    0.37757921635488245,
    0.38018496335111346,
    0.3944819866639645,
    0.35217850461894995,
    0.34034112637711206
  ],
  "combined_avgs": [
    0.18565701367251913,
    0.38054006310958044,
    0.39926217580889245,
    0.36784088596177245,
    0.4146005348082784,
    0.3444200168369047,
    0.3862597393965936,
    0.3867355920821517,
    0.37918625161485775,
    0.400163094280349,
    0.395485797329676,
    0.27819257065510505,
    0.38277491116271534,
    0.36463793514592313,
    0.34444734219480044,
    0.3860255477376759,
    0.3722641005025454,
    0.382222757178527,
    0.3784199499009304,
    0.3604439312661217,
    0.3662267581375095,
    0.393665267462054,
    0.38882082978001953,
    0.3678868973575421,
    0.36575667393823175,
    0.39874915093594004,
    0.39355071576231926,
    0.3341569104083373,
    0.3455824401499449
  ],
  "gppm": [
    612.4313315164785,
    661.6659987391855,
    652.9086102887705,
    671.5568510220603,
    666.6641332132658,
    694.5706831996584,
    689.4220869064494,
    671.4157091774322,
    666.9217978312506,
    683.8529013844754,
    674.28484688215,
    707.7394163803043,
    668.2111353502614,
    681.1987490767899,
    672.6550698246939,
    681.3631854669236,
    674.4773962183597,
    680.9362608727827,
    668.9319113389271,
    672.3731523801067,
    672.4786117196312,
    682.3644293779697,
    688.6232571519488,
    673.9325331124699,
    672.1390871860218,
    672.6596182508031,
    666.0496069902838,
    683.5896903896902,
    689.2593876514882
  ],
  "gppm_normalized": [
    1.4344284149844764,
    1.3700952373965996,
    1.346924719261853,
    1.3995347102189906,
    1.3726019043222715,
    1.4409070215758677,
    1.4369002852294295,
    1.38623302422862,
    1.3754773339038653,
    1.4119392308278145,
    1.3863323841337645,
    1.4817284054238633,
    1.3767124830811712,
    1.4085003669690528,
    1.3911412871100366,
    1.4169631694219011,
    1.3879996485442605,
    1.413136259015411,
    1.377053830002947,
    1.3859344351891718,
    1.3867439801582795,
    1.412667957457827,
    1.4242591375287996,
    1.388809053143611,
    1.3809787919276215,
    1.3951885888225657,
    1.3688210803863796,
    1.4139407140331364,
    1.4209687573036027
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394
  ],
  "response_lengths": [
    6156,
    2709,
    2562,
    2774,
    2340,
    2390,
    2209,
    2224,
    2507,
    2037,
    2312,
    2764,
    2290,
    2573,
    2652,
    2333,
    2452,
    2368,
    2404,
    2563,
    2376,
    2005,
    2066,
    2556,
    2304,
    2076,
    2399,
    2558,
    2211
  ]
}