{
  "example_idx": 40,
  "reference": "Published as a conference paper at ICLR 2023\n\nFEDERATED LEARNING FROM SMALL DATASETS\n\nMichael Kamp Institute for AI in medicine (IKIM) University Hospital Essen, Essen Germany, and Ruhr-University Bochum, Bochum Germany, and Monash University, Melbourne, Australia michael.kamp@uk-essen.de\n\nJonas Fischer Harvard T.H. Chan School of Public Health Department of Biostatistics Boston, MA, United States jfischer@hsph.harvard.edu\n\nJilles Vreeken CISPA Helmholtz Center for Information Security Saarbr ̈ucken, Germany vreeken@cispa.de\n\nABSTRACT\n\nFederated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning.\n\n1\n\nINTRODUCTION\n\nHow can we learn high quality models when data is inherently distributed across sites and cannot be shared or pooled? In federated learning, the solution is to iteratively train models locally at each site and share these models with the server to be aggregated to a global model. As only models are shared, data usually remains undisclosed. This process, however, requires sufficient data to be available at each site in order for the locally trained models to achieve a minimum quality—even a single bad model can render aggregation arbitrarily bad (Shamir and Srebro, 2014). In many relevant applications this requirement is not met: In healthcare settings we often have as little as a few dozens of samples (Granlund et al., 2020; Su et al., 2021; Painter et al., 2020). Also in domains where deep learning is generally regarded as highly successful, such as natural language processing and object detection, applications often suffer from a lack of data (Liu et al., 2020; Kang et al., 2019).\n\nTo tackle this problem, we propose a new building block called daisy-chaining for federated learning in which models are trained on one local dataset after another, much like a daisy chain. In a nutshell, at each client a model is trained locally, sent to the server, and then—instead of aggregating local models—sent to a random other client as is (see Fig. 1). This way, each local model is exposed to a daisy chain of clients and their local datasets. This allows us to learn from small, distributed datasets simply by consecutively training the model with the data available at each site. Daisy-chaining alone, however, violates privacy, since a client can infer from a model upon the data of the client it received it from (Shokri et al., 2017). Moreover, performing daisy-chaining naively would lead to overfitting which can cause learning to diverge (Haddadpour and Mahdavi, 2019). In this paper, we propose to combine daisy-chaining of local datasets with aggregation of models, both orchestrated by the server, and term this method federated daisy-chaining (FEDDC).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Federated learning settings. A standard federated learning setting with training of local models at clients (middle) with aggregation phases where models are communicated to the server, aggregated, and sent back to each client (left). We propose to add daisy chaining (right), where local models are sent to the server and then redistributed to a random permutation of clients as is.\n\nWe show that our simple, yet effective approach maintains privacy of local datasets, while it provably converges and guarantees improvement of model quality in convex problems with a suitable aggregation method. Formally, we show convergence for FEDDC on non-convex problems. We then show for convex problems that FEDDC succeeds on small datasets where standard federated learning fails. For that, we analyze FEDDC combined with aggregation via the Radon point from a PAC-learning perspective. We substantiate this theoretical analysis for convex problems by showing that FEDDC in practice matches the accuracy of a model trained on the full data of the SUSY binary classification dataset with only 2 samples per client, outperforming standard federated learning by a wide margin. For non-convex settings, we provide an extensive empirical evaluation, showing that FEDDC outperforms naive daisy-chaining, vanilla federated learning FEDAVG (McMahan et al., 2017), FEDPROX (Li et al., 2020a), FEDADAGRAD, FEDADAM, and FEDYOGI (Reddi et al., 2020) on low-sample CIFAR10 (Krizhevsky, 2009), including non-iid settings, and, more importantly, on two real-world medical imaging datasets. Not only does FEDDC provide a wide margin of improvement over existing federated methods, but it comes close to the performance of a gold-standard (centralized) neural network of the same architecture trained on the pooled data. To achieve that, it requires a small communication overhead compared to standard federated learning for the additional daisy-chaining rounds. As often found in healthcare, we consider a cross-SILO scenario where such small communication overhead is negligible. Moreover we show that with equal communication, standard federated averaging still underperforms in our considered settings.\n\nIn summary, our contributions are (i) FEDDC, a novel approach to federated learning from small datasets via a combination of model permutations across clients and aggregation, (ii) a formal proof of convergence for FEDDC, (iii) a theoretical guarantee that FEDDC improves models in terms of (cid:15), δ-guarantees which standard federated learning can not, (iv) a discussion of the privacy aspects and mitigations suitable for FEDDC, including an empirical evaluation of differentially private FEDDC, and (v) an extensive set of experiments showing that FEDDC substantially improves model quality on small datasets compared to standard federated learning approaches.\n\n2 RELATED WORK\n\nLearning from small datasets is a well studied problem in machine learning. In the literature, we find both general solutions, such as using simpler models and transfer learning (Torrey and Shavlik, 2010), and more specialized ones, such as data augmentation (Ibrahim et al., 2021) and few-shot learning (Vinyals et al., 2016; Prabhu et al., 2019). In our scenario overall data is abundant, but the problem is that data is distributed into small local datasets at each site, which we are not allowed to pool. Hao et al. (2021) propose local data augmentation for federated learning, but their method requires a sufficient quality of the local model for augmentation which is the opposite of the scenario we are considering. Huang et al. (2021) provide generalization bounds for federated averaging via the NTK-framework, but requires one-layer infinite-width NNs and infinitesimal learning rates.\n\nFederated learning and its variants have been shown to learn from incomplete local data sources, e.g., non-iid label distributions (Li et al., 2020a; Wang et al., 2019) and differing feature distributions (Li et al., 2020b; Reisizadeh et al., 2020a), but fail in case of large gradient diversity (Haddadpour and Mahdavi, 2019) and strongly dissimilar label distribution (Marfoq et al., 2021). For small\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ndatasets, local empirical distributions may vary greatly from the global distribution: the difference of empirical to true distribution decreases exponentially with the sample size (e.g., according to the Dvoretzky–Kiefer–Wolfowitz inequality), but for small samples the difference can be substantial, in particular if the distribution differs from a Normal distribution (Kwak and Kim, 2017). Shamir and Srebro (2014) have shown the adverse effect of bad local models on averaging, proving that even due to a single bad model averaging can be arbitrarily bad.\n\nA different approach to dealing with biased local data is by learning personalized models at each client. Such personalized FL (Li et al., 2021) can reduce sample complexity, e.g., by using shared representations (Collins et al., 2021) for client-specific models, e.g., in the medical domain (Yang et al., 2021), or by training sample-efficient personalized Bayesian methods (Achituve et al., 2021). It is not applicable, however, to settings where you are not allowed to learn the biases or batch effects of local clients, e.g., in many medical applications where this would expose sensitive client information. Kiss and Horvath (2021) propose a decentralized and communication-efficient variant of federated learning that migrates models over a decentralized network, storing incoming models locally at each client until sufficiently many models are collected on each client for an averaging step, similar to Gossip federated learing (Jelasity et al., 2005). The variant without averaging is similar to simple daisy-chaining which we compare to in Section 7. FEDDC is compatible with any aggregation operator, including the Radon machine (Kamp et al., 2017), the geometric median (Pillutla et al., 2022), or neuron-clustering (Yurochkin et al., 2019), and can be straightforwardly combined with approaches to improve communication-efficiency, such as dynamic averaging (Kamp et al., 2018), and model quantization (Reisizadeh et al., 2020b). We combine FEDDC with averaging, the Radon machine, and FedProx (Li et al., 2020a) in Sec. 7.\n\n3 PRELIMINARIES\n\ni\n\n(x,y)\n\nWe assume iterative learning algorithms (cf. Chp. 2.1.4 Kamp, 2019) A : X × Y × H → H that update a model h ∈ H using a dataset D ⊂ X × Y from an input space X and output space Y, i.e., ht+1 = A(D, ht). Given a set of m ∈ N clients with local datasets D1, . . . , Dm ⊂ X × Y drawn iid from a data distribution D and a loss function (cid:96) : Y × Y → R, the goal is to find a single model h∗ ∈ H that minimizes the risk ε(h) = E [(cid:96)(h(x), y)]. In centralized learning, datasets are pooled as D = (cid:83) [m] Di and A is applied to D until convergence. Note that applying A on D can be the application to any random subset, e.g., as in mini-batch training, and convergence is measured in terms of low training loss, small gradient, or small deviation from previous iterate. In standard federated learning (McMahan et al., 2017), A is applied in parallel for b ∈ N rounds on each client locally to produce local models h1, . . . , hm. These models are then centralized and aggregated using an aggregation operator agg : Hm → H, i.e., h = agg(h1, . . . , hm). The aggregated model h is then redistributed to local clients which perform another b rounds of training using h as a starting point. This is iterated until convergence of h. When aggregating by averaging, this method is known as federated averaging (FEDAVG). Next, we describe FEDDC.\n\n∼D\n\n∈\n\n4 FEDERATED DAISY-CHAINING\n\nWe propose federated daisy chaining as an extension to federated learning in a setup with m clients and one designated sever.1 We provide the pseudocode of our approach as Algorithm 1.\n\nThe client: Each client trains its local model in each round on local data (line 4), and sends its model to the server every b rounds for aggregation, where b is the aggregation period, and every d rounds for daisy chaining, where d is the daisy-chaining period (line 6). This re-distribution of models results in each individual model conceptually following a daisy chain of clients, training on each local dataset. Such a daisy chain is interrupted by each aggregation round.\n\nThe server: Upon receiving models, in a daisy-chaining round (line 9) the server draws a random permutation π of clients (line 10) and re-distributes the model of client i to client π(i) (line 11), while in an aggregation round (line 12), the server instead aggregates all local models and re-distributes the aggregate to all clients (line 13-14).\n\n1This star-topology can be extended to hierarchical networks in a straightforward manner. Federated learning\n\ncan also be performed in a decentralized network via gossip algorithms (Jelasity et al., 2005).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: Federated Daisy-Chaining FEDDC Input: daisy-chaining period d, aggregation period b, learning algorithm A, aggregation operator\n\nagg, m clients with local datasets D1, . . . , Dm, total number of rounds T\n\nOutput: final model aggregate hT 0, . . . , hm\n\n1 initialize local models h1 2 Locally at client i at time t do\n\n0\n\n3\n\n4\n\n5\n\nsample S from Di hi 1) if t mod d = d − 1 or t mod b = b − 1 then\n\nt ← A(S, hi\n\n−\n\nt\n\n6\n\nt to server\n\nsend hi receive new hi 8 At server at time t do\n\n7\n\nt from server\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\nif t mod d = d − 1 then\n\ndraw permutation π of [1,m] at random for all i ∈ [m] send model hi\n\nt to client π(i)\n\nelse if t mod b = b − 1 then\n\nht ← agg(h1 send ht to all clients\n\nt , . . . , hm t )\n\n// receives either aggregate ht or some hj\n\nt\n\n// daisy chaining\n\n// aggregation\n\nCommunication complexity: Note that we consider cross-SILO settings, such as healthcare, were communication is not a bottleneck and, hence, restrict ourselves to a brief discussion in the interest of space. Communication between clients and server happens in O( T b ) many rounds, where T is the overall number of rounds. Since FEDDC communicates every dth and bth round, the amount of communication rounds is similar to FEDAVG with averaging period bFedAvg = min{d, b}. That is, FEDDC increases communication over FEDAVG by a constant factor depending on the setting of b and d. The amount of communication per communication round is linear in the number of clients and model size, similar to federated averaging. We investigate the performance of FEDAVG provided with the same communication capacity as FEDDC in our experiments and in App. A.3.6.\n\nd + T\n\n5 THEORETICAL GUARANTEES\n\nIn this section, we formally show that FEDDC converges for averaging. We, further, provide theoretical bounds on the model quality in convex settings, showing that FEDDC has favorable generalization error in low sample settings compared to standard federated learning. More formally, we first show that under standard assumptions on the empirical risk, it follows from a result of Yu et al. (2019) that FEDDC converges when using averaging as aggregation and SGD for learning—a standard setting in, e.g., federated learning of neural networks. We provide all proofs in the appendix. Corollary 1. Let the empirical risks E i Di (cid:96)(hi(x), y) at each client i ∈ [m] be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, then FEDDC with mT ), where T is the number of local updates. averaging and SGD has a convergence rate of O(1/\n\nemp(h) = (cid:80) √\n\n(x,y)\n\n∈\n\nSince model quality in terms of generalization error does not necessarily depend on convergence of training (Haddadpour and Mahdavi, 2019; Kamp et al., 2018), we additionally analyze model quality in terms of probabilistic worst-case guarantees on the generalization error (Shalev-Shwartz and Ben-David, 2014). The average of local models can yield as bad a generalization error as the worst local model, hence, using averaging as aggregation scheme in standard federated learning can yield arbitrarily bad results (cf. Shamir and Srebro, 2014). As the probability of bad local models starkly increases with smaller sample sizes, this trivial bound often carries over to our considered practical settings. The Radon machine (Kamp et al., 2017) is a federated learning approach that overcomes these issues for a wide range of learning algorithms and allows us to analyze (non-trivial) quality bounds of aggregated models under the assumption of convexity. Next, we show that FEDDC can improve model quality for small local datasets where standard federated learning fails to do so.\n\nA Radon point (Radon, 1921) of a set of points S from a space X is—similar to the geometric median—a point in the convex hull of S with a high centrality (i.e., a Tukey depth (Tukey, 1975;\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a) FEDDC with Radon point with d = 1, b = 50.\n\n(b) Federated learning with Radon point with b = 1.\n\n(c) Federated learning with Radon point with b = 50.\n\nFigure 2: Results on SUSY. We visualize results in terms of train (green) and test error (orange) for (a) FEDDC (d = 1, b = 50) and standard federated learning using Radon points for aggregation with (b) b = 1, i.e., the same amount of communication as FEDDC, and (c) b = 50, i.e., the same aggregation period as FEDDC. The network has 441 clients with 2 data points per client. The performance of a central model trained on all data is indicated by the dashed line.\n\nGilad-Bachrach et al., 2004) of at least 2). For a Radon point to exist, S ⊂ X has to have a minimum size r ∈ N called the Radon number of X . For X ⊆ Rd the radon number is d + 2. Here, the set of points S are the local models, or more precisely their parameter vectors. We make the following standard assumption (Von Luxburg and Sch ̈olkopf, 2011) on the local learning algorithm A. Assumption 2 (((cid:15), δ)-guarantees). The learning algorithm A applied on a dataset drawn iid from D of size n ≥ n0 ∈ N produces a model h ∈ H s.t. with probability δ ∈ (0, 1] it holds for (cid:15) > 0 that P (ε(h) > (cid:15)) < δ. The sample size n0 is monotonically decreasing in δ and (cid:15) (note that typically n0 is a polynomial in (cid:15)−\n\n1 and log(δ−\n\n1)).\n\nHere ε(h) is the risk defined in Sec. 3. Now let r ∈ N be the Radon number of H, A be a learning algorithm as in assumption 2, and risk ε be convex. Assume m ≥ rh many clients with h ∈ N. For (cid:15) > 0, δ ∈ (0, 1] assume local datasets D1, . . . , Dm of size larger than n0((cid:15), δ) drawn iid from D, and h1, . . . , hm be local models trained on them using A. Let rh be the iterated Radon point (Clarkson et al., 1996) with h iterations computed on the local models (for details, see App. A.2). Then it follows from Theorem 3 in Kamp et al. (2017) that for all i ∈ [m] it holds that\n\nP (ε(rh) > (cid:15)) ≤ (r P (ε(hi) > (cid:15)))2h\n\n(1)\n\nwhere the probability is over the random draws of local datasets. That is, the probability that the aggregate rh is bad is doubly-exponentially smaller than the probability that a local model is bad. Note that in PAC-learning, the error bound and the probability of the bound to hold are typically linked, so that improving one can be translated to improving the other (Von Luxburg and Sch ̈olkopf, 2011). Eq. 1 implies that the iterated Radon point only improves the guarantee on the confidence compared to that for local models if δ < r− < 1 only holds for rδ < 1. Consequently, local models need to achieve a minimum quality for the federated learning system to improve model quality. Corollary 3. Let H be a model space with Radon number r ∈ N, ε a convex risk, and A a learning algorithm with sample size n0((cid:15), δ). Given (cid:15) > 0 and any h ∈ N, if local datasets D1, . . . , Dm with m ≥ rh are smaller than n0((cid:15), r− 1), then federated learning using the Radon point does not improve model quality in terms of ((cid:15), δ)-guarantees.\n\n1, i.e. P (ε(rh) > (cid:15)) ≤ (r P (ε(hi) > (cid:15)))2h\n\n< (rδ)2h\n\nIn other words, when using aggregation by Radon points alone, an improvement in terms of ((cid:15), δ)- 1, the guarantees is strongly dependent on large enough local datasets. Furthermore, given δ > r− guarantee can become arbitrarily bad by increasing the number of aggregation rounds.\n\nFederated Daisy-Chaining as given in Alg. 1 permutes local models at random, which is in theory equivalent to permuting local datasets. Since the permutation is drawn at random, the amount of permutation rounds T necessary for each model to observe a minimum number of distinct datasets k with probability 1 − ρ can be given with high probability via a variation of the coupon collector problem as T ≥ d m k), where Hm is the m-th harmonic number—see Lm. 5 in\n\n(Hm − Hm\n\n1 m\n\nρ\n\n−\n\n5\n\n01002003004005000.40.50.60.70.80.91centralized(test)roundsaccuracytraintest01002003004005000.40.50.60.70.80.91rounds01002003004005000.40.50.60.70.80.91roundsPublished as a conference paper at ICLR 2023\n\n1\n\n−\n\nm (Hm − Hm √\n\nApp. A.5 for details. It follows that when we perform daisy-chaining with m clients and local datasets of size n for at least dmρ− k) rounds, then each local model will with probability at least 1 − ρ be trained on at least kn distinct samples. For an (cid:15), δ-guarantee, we thus need to set b large enough so that kn ≥ n0((cid:15), δ. This way, the failure probability is the product of not all clients observing k distinct datasets and the model having a risk larger than (cid:15), √\nwhich is Proposition 4. Let H be a model space with Radon number r ∈ N, ε a convex risk , and A a learning 1) and any h ∈ N, and local datasets algorithm with sample size n0((cid:15), δ). Given (cid:15) > 0, δ ∈ (0, r− D1, . . . , Dm of size n ∈ N with m ≥ rh, then Alg. 1 using the Radon point with aggr. period\n\nδ) with probability at least 1 −\n\nδ = δ.\n\n√\n\n√\n\nδ\n\n(cid:16)\n\nb ≥ d\n\nm δ 1\n\n2m\n\nHm − Hm\n\n−\n\n(cid:17)\n\n(cid:100)n−1n0((cid:15),√δ)(cid:101)\n\n(2)\n\nimproves model quality in terms of ((cid:15), δ)-guarantees.\n\nThis result implies that if enough daisy-chaining rounds are performed in-between aggregation rounds, federated learning via the iterated Radon point improves model quality in terms of ((cid:15), δ)-guarantees: the resulting model has generalization error smaller than (cid:15) with probability at least 1 − δ. Note that the aggregation period cannot be arbitrarily increased without harming convergence. To illustrate the interplay between these variables, we provide a numerical analysis of Prop. 4 in App. A.5.1.\n\nThis theoretical result is also evident in practice, as we show in Fig. 2. There, we compare FEDDC with standard federated learning and equip both with the iterated Radon point on the SUSY binary classification dataset (Baldi et al., 2014). We train a linear model on 441 clients with only 2 samples per client. After 500 rounds FEDDC daisy-chaining every round (d = 1) and aggregating every fifty rounds (b = 50) reached the test accuracy of a gold-standard model that has been trained on the centralized dataset (ACC=0.77). Standard federated learning with the same communication complexity using b = 1 is outperformed by a large margin (ACC=0.68). We additionally provide results of standard federated learning with b = 50 (ACC=0.64), which shows that while the aggregated models perform reasonable, the standard approach heavily overfits on local datasets if not pulled to a global average in every round. More details on this experiment can be found in App. A.3.2. In Sec. 7 we show that the empirical results for averaging as aggregation operator are similar to those for the Radon machine. First, we discuss the privacy-aspects of FEDDC.\n\n6 DATA PRIVACY\n\nA major advantage of federated over centralized learning is that local data remains undisclosed to anyone but the local client, only model parameters are exchanged. This provides a natural benefit to data privacy, which is the main concern in applications such as healthcare. However, an attacker can make inferences about local data from model parameters (Ma et al., 2020) and model updates or gradients (Zhu and Han, 2020). In the daisy-chaining rounds of FEDDC clients receive a model that was directly trained on the local data of another client, instead of a model aggregate, potentially facilitating membership inference attacks (Shokri et al., 2017)—reconstruction attacks (Zhu and Han, 2020) remain difficult because model updates cannot be inferred since the server randomly permutes the order of clients in daisy-chaining rounds.\n\nFigure 3: Differential privacy results. Comparison of FEDDC (top solid line) to FEDDC with clipped parameter updates and Gaussian noise (dashed lines) on CIFAR10 with 250 clients.\n\nShould a malicious client obtain model updates through additional attacks, a common defense is applying appropriate clipping and noise before sending models. This guarantees (cid:15), δ-differential privacy for local data (Wei et al., 2020) at the cost of a slight-to-moderate loss in model quality. This technique is also proven to defend against backdoor and poisoning attacks (Sun et al., 2019). Moreover, FEDDC is compatible with standard defenses against such attacks, such as noisy or robust aggregation (Liu et al., 2022)—FEDDC with the Radon machine is an example of robust aggregation. We illustrate the effectiveness of FEDDC\n\n6\n\n05·10410·10415·10420·10400.20.40.60.8roundsaccuracyFEDDCDP-FEDDC(S=2,σ=0.01)DP-FEDDC(S=2,σ=0.02)DP-FEDDC(S=4,σ=0.05)Published as a conference paper at ICLR 2023\n\n(a) FEDDC with d = 1, b = 200.\n\n(b) FEDAVG with b = 1.\n\n(c) FEDAVG with b = 200.\n\nFigure 4: Synthetic data results. Comparison of FEDDC (a), FEDAVG with same communication (b) and same averaging period (c) for training fully connected NNs on synthetic data. We report mean and confidence accuracy per client in color and accuracy of central learning as dashed black line.\n\nwith differential privacy in the following experiment. We train a small ResNet on 250 clients using FEDDC with d = 2 and b = 10, postponing the details on the experimental setup to App. A.1.1 and A.1.2. Differential privacy is achieved by clipping local model updates and adding Gaussian noise as proposed by Geyer et al. (2017). The results as shown in Figure 3 indicate that the standard trade-off between model quality and privacy holds for FEDDC as well. Moreover, for mild privacy settings the model quality does not decrease. That is, FEDDC is able to robustly predict even under differential privacy. We provide an extended discussion on the privacy aspects of FEDDC in App. A.7.\n\n7 EXPERIMENTS ON DEEP LEARNING\n\nOur approach FEDDC, both provably and empirically, improves model quality when using Radon points as aggregation which, however, require convex problems. For non-convex problems, in particular deep learning, averaging is the state-of-the-art aggregation operator. We, hence, evaluate FEDDC with averaging against the state of the art in federated learning on synthetic and real world data using neural networks. As baselines, we consider federated averaging (FEDAVG) (McMahan et al., 2017) with optimal communication, FEDAVG with equal communication as FEDDC, and simple daisy-chaining without aggregation. We further consider the 4 state-of-the-art methods FEDPROX (Li et al., 2020a), FEDADAGRAD, FEDYOGI, and FEDADAM (Reddi et al., 2020). As datasets we consider a synthetic classification dataset, image classification in CIFAR10 (Krizhevsky, 2009), and two real medical datasets: MRI scans for brain tumors,2 and chest X-rays for pneumonia3. We provide additional results on MNIST in App. A.3.8. Details on the experimental setup are in App. A.1.1,A.1.2, code is publicly available at https://github.com/kampmichael/FedDC.\n\nSynthetic Data: We first investigate the potential of FEDDC on a synthetic binary classification dataset generated by the sklearn (Pedregosa et al., 2011) make_classification function with 100 features. On this dataset, we train a simple fully connected neural network with 3 hidden layers on m = 50 clients with n = 10 samples per client. We compare FEDDC with daisy-chaining period d = 1 and aggregation period b = 200 to FEDAVG with the same amount of communication b = 1 and the same averaging period b = 200. The results presented in Fig. 4 show that FEDDC achieves a test accuracy of 0.89. This is comparable to centralized training on all data which achieves a test accuracy of 0.88. It substantially outperforms both FEDAVG setups, which result in an accuracy of 0.80 and 0.76. Investigating the training of local models between aggreation periods reveals that the main issue of FEDAVG is overfitting of local clients, where FEDAVG train accuracy reaches 1.0 quickly after each averaging step. With these promising results on vanilla neural networks, we next turn to real-world image classification problems typically solved with CNNs.\n\nCIFAR10: As a first challenge for image classification, we consider the well-known CIFAR10 image benchmark. We first investigate the effect of the aggregation period b on FEDDC and FEDAVG, separately optimizing for an optimal period for both methods. We use a setting of 250 clients with\n\n2kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection 3kaggle.com/praveengovi/coronahack-chest-xraydataset\n\n7\n\n02004006008001,00000.20.40.60.81centralized(test)roundsaccuracytraintest02004006008001,00000.20.40.60.81rounds02004006008001,00000.20.40.60.81roundsPublished as a conference paper at ICLR 2023\n\na small version of ResNet, and 64 local samples each, which simulates our small sample setting, drawn at random without replacement (details in App. A.1.2). We report the results in Figure 5 and set the period for FEDDC to b = 10, and consider federated averaging with periods of both b = 1 (equivalent communication to FEDDC with d = 1, b = 10) and b = 10 (less communication than FEDDC by a factor of 10) for all subsequent experiments.\n\nNext, we consider a subset of 9600 samples spread across 150 clients (i.e. 64 samples per client), which corresponds to our small sample setting. Now, each client is equipped with a larger, untrained ResNet18.4 Note that the combined amount of examples is only one fifth of the original training data, hence we cannot expect typical CIFAR10 performance. To obtain a gold standard for comparison, we run centralized learning CENTRAL, separately optimizing its hyperparameters, yielding an accuracy of around 0.65. All results are reported in Table 1, where we report FEDAVG with b = 1 and b = 10, as these were the best performing settings and b = 1 corresponds to equal amounts of communication as FEDDC. We use a daisy chaining period of d = 1 for FEDDC throughout all experiments for consistency, and provide results for larger daisy chaining periods in App. A.3.5, which, depending on the data distribution, might be favorable. We observe that FEDDC achieves substantially higher accuracy over the baseline set by federated averaging. In App. A.3.7 we show that this holds also for client subsampling. Upon further inspection, we see that FEDAVG drastically overfits, achieving training accuracies of 0.97 (App. A.3.1), a similar trend as on the synthetic data before. Daisy-chaining alone, apart from privacy issues, also performs worse than FEDDC. Intriguingly, also the state of the art shows similar trends. FEDPROX, run with optimal b = 10 and μ = 0.1, only achieves an accuracy of 0.51 and FEDADAGRAD, FEDYOGI, and FEDADAM show even worse performance of around 0.22, 0.31, and 0.34, respectively. While applied successfully on large-scale data, these methods seem to have shortcomings when it comes to small sample regimes.\n\nFigure 5: Averaging periods on CIFAR10. For 150 clients with small ResNets and 64 samples per client, we visualize the test accuracy (higher is better) of FEDDC and FEDAVG for different aggregation periods b.\n\nTo model different data distributions across clients that could occur in for example our healthcare setting, we ran further experiments on simulated non-iid data, gradually increasing the locally available data, as well as on non-privacy preserving decentralized learning. We investigate the effect of non-iid data on FEDDC by studying the “pathological non-IID partition of the data” (McMahan et al., 2017). Here, each client only sees examples from 2 out of the 10 classes of CIFAR10. We again use a subset of the dataset. The results in Tab. 2 show that FEDDC outperforms FEDAVG by a wide margin. It also outperforms FEDPROX, a method specialized on heterogeneous datasets in our considered small sample setting. For a similar training setup as before, we show results for gradually increasing local datasets in App. A.3.4. Most notably, FEDDC outperforms FEDAVG even with 150 samples locally. Only when the full CIFAR10 dataset is distributed across the clients, FEDAVG is on par with FEDDC (see App. Fig. 7). We also compare with distributed training through gradient sharing (App. A.3.3), which discards any privacy concerns, implemented by mini-batch SGD with parameter settings corresponding to our federated setup as well as a separately optimized version. The results show that such an approach is outperformed by both FEDAVG as well as FEDDC, which is in line with previous findings and emphasize the importance of model aggregation.\n\nAs a final experiment on CIFAR10, we consider daisy-chaining with different combinations of aggregation methods, and hence its ability to serve as a building block that can be combined with other federated learning approaches. In particular, we consider the same setting as before and combine FEDPROX with daisy chaining. The results, reported in Tab. 2, show that this combination is not only successful, but also outperforms all others in terms of accuracy.\n\nMedical image data: Finally, we consider two real medical image datasets representing actual health related machine learning tasks, which are naturally of small sample size. For the brain MRI scans, we simulate 25 clients (e.g., hospitals) with 8 samples each. Each client is equipped with a CNN\n\n4Due to hardware restrictions we are limited to training 150 ResNets, hence 9600 samples across 150 clients.\n\n8\n\n1102050100200500∞00.20.40.60.8averagingperiodbaccuracyFEDDCFEDAVGPublished as a conference paper at ICLR 2023\n\nCIFAR10\n\nMRI\n\nPneumonia\n\nCIFAR10\n\nFEDDC (ours) DC (baseline) FEDAVG (b=1) FEDAVG (b=10) FEDPROX FEDADAGRAD FEDYOGI FEDADAM\n\nCENTRAL\n\n62.9 58.4 55.8 48.7 51.1 21.8 31.4 34.0\n\n0.78\n\n0.85\n\n0.87\n\n0.02 78.4 57.7 74.1 75.6 76.5 45.7 71.3 73.8\n\n0.80\n\n0.01\n\n4.37\n\n0.23\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n1.57\n\n1.68\n\n1.18\n\n0.61 83.2 79.8 80.1 79.4 80.0 62.5 77.6 73.5\n\n0.50\n\n1.62\n\n1.25\n\n1.98\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n65.1\n\n1.44\n\n±\n\n82.1\n\n1.00\n\n±\n\n84.1\n\n0.84\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n0.99\n\n1.53\n\n1.11\n\n0.36\n\n0.01\n\n0.64\n\n0.36\n\n3.31\n\n62.9 FEDDC FEDDC +FEDPROX 63.2\n\n±\n\n0.02\n\n0.38\n\n± Non-IID\n\nFEDDC FEDAVG (b=1) FEDAVG (b=10) FEDPROX FEDADAGRAD FEDADAM FEDYOGI\n\n34.2 30.2 24.9 32.8 11.7 13.0 12.5\n\n0.61\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n2.11\n\n1.95\n\n0.00\n\n0.00\n\n0.00\n\n0.04\n\nTable 1: Results on image data, reported is the average test accuracy of the final model over three runs (± denotes maximum deviation from the average).\n\nTable 2: Combination of FEDDC with FEDAVG and FEDPROX and non-iid results on CIFAR10.\n\n(see App. A.1.1). The results for brain tumor prediction evaluated on a test set of 53 of these scans are reported in Table 1. Overall, FEDDC performs best among the federated learning approaches and is close to the centralized model. Whereas FEDPROX performed comparably poorly on CIFAR10, it now outperforms FEDAVG. Similar to before, we observe a considerable margin between all competing methods and FEDDC. To investigate the effect of skewed distributions of sample sizes across clients, such as smaller hospitals having less data than larger ones, we provide additional experiments in App. A.3.5. The key insight is that also in these settings, FEDDC outperforms FEDAVG considerably, and is close to its performance on the unskewed datasets.\n\nFor the pneumonia dataset, we simulate 150 clients training ResNet18 (see App. A.1.1) with 8 samples per client, the hold out test set are 624 images. The results, reported in Table 1, show similar trends as for the other datasets, with FEDDC outperforming all baselines and the state of the art, and being within the performance of the centrally trained model. Moreover it highlights that FEDDC enables us to train a ResNet18 to high accuracy with as little as 8 samples per client.\n\n8 DISCUSSION AND CONCLUSION\n\nWe propose to combine daisy-chaining and aggregation to effectively learn high quality models in a federated setting where only little data is available locally. We formally prove convergence of our approach FEDDC, and for convex settings provide PAC-like generalization guarantees when aggregating by iterated Radon points. Empirical results on the SUSY benchmark underline these theoretical guarantees, with FEDDC matching the performance of centralized learning. Extensive empirical evaluation shows that the proposed combination of daisy-chaining and aggregation enables federated learning from small datasets in practice.When using averaging, we improve upon the state of the art for federated deep learning by a large margin for the considered small sample settings. Last but not least, we show that daisy-chaining is not restricted to FEDDC, but can be straight-forwardly included in FEDAVG, Radon machines, and FEDPROX as a building block, too.\n\nFEDDC permits differential privacy mechanisms that introduce noise on model parameters, offering protection against membership inference, poisoning and backdoor attacks. Through the random permutations in daisy-chaining rounds, FEDDC is also robust against reconstruction attacks. Through the daisy-chaining rounds, we see a linear increase in communication. As we are primarily interested in healthcare applications, where communication is not a bottleneck, such an increase in communication is negligible. Importantly, FEDDC outperforms FEDAVG in practice also when both use the same amount of communication. Improving the communication efficiency considering settings where bandwidth is limited, e.g., model training on mobile devices, would make for engaging future work.\n\nWe conclude that daisy-chaining lends itself as a simple, yet effective building block to improve federated learning, complementing existing work to extend to settings where little data is available per client. FEDDC, thus, might offer a solution to the open problem of federated learning in healthcare, where very few, undisclosable samples are available at each site.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors thank Sebastian U. Stich for his detailed comments on an earlier draft. Michael Kamp received support from the Cancer Research Center Cologne Essen (CCCE). Jonas Fischer is supported by a grant from the US National Cancer Institute (R35CA220523).\n\nREFERENCES\n\nIdan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik, and Ethan Fetaya. Personalized federated learning with gaussian processes. In Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021. 3\n\nGiuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. International Journal of Security and Networks, 10(3):137–150, 2015. 22\n\nPierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy\n\nphysics with deep learning. Nature communications, 5(1):1–9, 2014. 6, 15\n\nArjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, pages 634–643. PMLR, 2019. 22\n\nKenneth L Clarkson, David Eppstein, Gary L Miller, Carl Sturtivant, and Shang-Hua Teng. Approximating center points with iterative radon points. International Journal of Computational Geometry & Applications, 6(03):357–377, 1996. 5, 15\n\nLiam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 2089–2099. PMLR, 18–24 Jul 2021. 3\n\nRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client\n\nlevel perspective. arXiv preprint arXiv:1712.07557, 2017. 7\n\nRan Gilad-Bachrach, Amir Navot, and Naftali Tishby. Bayes and tukey meet at the center point. In International Conference on Computational Learning Theory, pages 549–563. Springer, 2004. 5\n\nKristin L Granlund, Sui-Seng Tee, Hebert A Vargas, Serge K Lyashchenko, Ed Reznik, Samson Fine, Vincent Laudone, James A Eastham, Karim A Touijer, Victor E Reuter, et al. Hyperpolarized mri of human prostate cancer reveals increased lactate with tumor grade driven by monocarboxylate transporter 1. Cell metabolism, 31(1):105–114, 2020. 1\n\nFarzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated\n\nlearning. arXiv preprint arXiv:1910.14425, 2019. 1, 2, 4\n\nWeituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen, and Lawrence Carin Duke. Towards fair federated learning with zero-shot data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3310–3319, 2021. 2\n\nBaihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning analysis. In International Conference on Machine Learning, pages 4423–4434. PMLR, 2021. 2\n\nMarwa Ibrahim, Mohammad Wedyan, Ryan Alturki, Muazzam A Khan, and Adel Al-Jumaily. Augmentation in healthcare: Augmented biosignal using deep learning and tensor representation. Journal of Healthcare Engineering, 2021, 2021. 2\n\nM ́ark Jelasity, Alberto Montresor, and Ozalp Babaoglu. Gossip-based aggregation in large dynamic\n\nnetworks. ACM Transactions on Computer Systems (TOCS), 23(3):219–252, 2005. 3\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMichael Kamp. Black-Box Parallelization for Machine Learning. PhD thesis, Rheinische Friedrich-\n\nWilhelms-Universit ̈at Bonn, Universit ̈ats-und Landesbibliothek Bonn, 2019. 3\n\nMichael Kamp, Mario Boley, Olana Missura, and Thomas G ̈artner. Effective parallelisation for machine learning. In Advances in Neural Information Processing Systems, volume 30, pages 6480–6491. Curran Associates, Inc., 2017. 3, 4, 5, 14, 15\n\nMichael Kamp, Linara Adilova, Joachim Sicking, Fabian H ̈uger, Peter Schlicht, Tim Wirtz, and Stefan Wrobel. Efficient decentralized deep learning by dynamic model averaging. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 393–409. Springer, 2018. 3, 4\n\nBingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8420–8429, 2019. 1\n\nP ́eter Kiss and Tomas Horvath. Migrating models: A decentralized view on federated learning. In Proceedings of the Workshop on Parallel, Distributed, and Federated Learning. Springer, 2021. 3\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University\n\nof Toronto, Toronto, 2009. 2, 7\n\nSang Gyu Kwak and Jong Hae Kim. Central limit theorem: the cornerstone of modern statistics.\n\nKorean journal of anesthesiology, 70(2):144, 2017. 3\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 18\n\nQinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10713–10722, 2021. 3\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Conference on Machine Learning and Systems, 2020a, 2020a. 2, 3, 7\n\nXiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning In International Conference on Learning\n\non non-iid features via local batch normalization. Representations, 2020b. 2\n\nPei Liu, Xuemin Wang, Chao Xiang, and Weiye Meng. A survey of text data augmentation. In 2020 International Conference on Computer Communication and Network Security (CCNS), pages 191–195. IEEE, 2020. 1\n\nPengrui Liu, Xiangrui Xu, and Wei Wang. Threats, attacks and defenses to federated learning: issues,\n\ntaxonomy and perspectives. Cybersecurity, 5(1):4, 2022. 6, 22\n\nChuan Ma, Jun Li, Ming Ding, Howard H Yang, Feng Shu, Tony QS Quek, and H Vincent Poor. On safeguarding privacy and security in the framework of federated learning. IEEE network, 34(4): 242–248, 2020. 6, 22\n\nOthmane Marfoq, Giovanni Neglia, Aur ́elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2021. 2\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273–1282, 2017. 2, 3, 7, 8\n\nPeter Neal. The generalised coupon collector problem. Journal of Applied Probability, 45(3):\n\n621–629, 2008. 20\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nCorrie A Painter, Esha Jain, Brett N Tomson, Michael Dunphy, Rachel E Stoddard, Beena S Thomas, Alyssa L Damon, Shahrayz Shah, Dewey Kim, Jorge G ́omez Tejeda Za ̃nudo, et al. The angiosarcoma project: enabling genomic and clinical discoveries in a rare cancer through patient-partnered research. Nature medicine, 26(2):181–187, 2020. 1\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 7, 14\n\nKrishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\n\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022. 3\n\nViraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chaplain, David Sontag, and Xavier Amatriain. Few-shot learning for dermatological disease diagnosis. In Machine Learning for Healthcare Conference, pages 532–552. PMLR, 2019. 2\n\nJohann Radon. Mengen konvexer K ̈orper, die einen gemeinsamen Punkt enthalten. Mathematische\n\nAnnalen, 83(1):113–115, 1921. 4, 15\n\nSashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2020. 2, 7, 14\n\nAmirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. In Advances in Neural Information Processing Systems, volume 33, pages 21554–21565. Curran Associates, Inc., 2020a. 2\n\nAmirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization. In International Conference on Artificial Intelligence and Statistics, pages 2021–2031. PMLR, 2020b. 3\n\nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to\n\nalgorithms. Cambridge university press, 2014. 4\n\nOhad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850– 857. IEEE, 2014. 1, 3, 4, 16\n\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3–18. IEEE, 2017. 1, 6, 22\n\nXiaoping Su, Xiaofan Lu, Sehrish Khan Bazai, Eva Comp ́erat, Roger Mouawad, Hui Yao, Morgan Rouprˆet, Jean-Philippe Spano, David Khayat, Irwin Davidson, et al. Comprehensive integrative profiling of upper tract urothelial carcinomas. Genome biology, 22(1):1–25, 2021. 1\n\nZiteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really\n\nbackdoor federated learning? arXiv preprint arXiv:1911.07963, 2019. 6, 22\n\nLisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, pages 242–264. IGI global, 2010. 2\n\nJohn W Tukey. Mathematics and picturing data. In Proceedings of the International Congress of\n\nMathematics, volume 2, pages 523–531, 1975. 4\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, volume 29, pages 3630–3638. Curran Associates, Inc., 2016. 2\n\nUlrike Von Luxburg and Bernhard Sch ̈olkopf. Statistical learning theory: Models, concepts, and\n\nresults. In Handbook of the History of Logic, volume 10, pages 651–706. Elsevier, 2011. 5\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2019. 2\n\nKang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security, 15:3454–3469, 2020. 6, 22\n\nQian Yang, Jianyi Zhang, Weituo Hao, Gregory P. Spell, and Lawrence Carin. Flop: Federated learning on medical datasets using partial networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 3845–3853. Association for Computing Machinery, 2021. 3\n\nHao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5693–5700, 2019. 4, 19\n\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pages 7252–7261. PMLR, 2019. 3\n\nChengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. Batchcrypt: Efficient homomorphic encryption for cross-silo federated learning. In USENIX Annual Technical Conference, pages 493–506, 2020. 22\n\nLigeng Zhu and Song Han. Deep leakage from gradients. In Federated learning, pages 17–31.\n\nSpringer, 2020. 6\n\n13",
  "translations": [
    "# Summary Of The Paper\n\nIn this work, a practical and efficient FL learning framework is proposed for improving aggregation performance over multiple clients. The core step of this model is to use the interleaving model aggregation and permutation steps so that the proposed federated daisy-chaining method can work well for data sparse problems. Multiple experimental results show that FedDC outperforms the state-of-the-art methods significantly, e.g., FedAvg.\n\n# Strength And Weaknesses\n\nStrength: this work considers a practical issue of data sparse in the FL systems. It further provides theoretical analysis results on convergence, generalization performance evaluation, requirements on communications, and massive numerical results on classification problems.\n\nWeaknesses: even the work is new and encouraging, there are still some major concerns as follows:\n\n1) A convergence result is given in Corollary 1, which basically uses Yu et al. (2019)' result. How does the permutation affect the convergence is not analyzed. Does Yu et al. (2019)'s analysis covers the client permutation? Note that there is a significant difference between the selection with and without replacement. There is a gap between the FedDC and the analysis.\n\n2)  In the convergence result, $b$ needs to be less than a threshold, but eq.(2) $b$ is required to be large than a threshold. Is there an overlap between these?\n\n3) This work assumes that the risk considered is convex while their convergence result is borrowed from the nonconvex. A consistent argument is encouraged.\n\n4)  The authors claimed that FedDC can perform better in the data sparse case, however, the theory seems uncorrelated with any parameters regarding the data sample size (Lemma 4). In Prop. 5, if m is small and n is large, will $b$ be a negative number? A sufficient discussion of the theoretical results should be provided.\n\n5) $m$ is assumed to be large enough so that the local model can improve the performance. Will this method also work for $m$ is small in the sense that the same as FedAvg or will diverge? also, how decide $r$ in practice?\n\n6) The theoretical results seem completely independent on which federated learning algorithm adopted, proxFed, or adam based algorithm. In other word, the convergence rate of the algorithm will not affect the $b$ and $T$, right?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: this paper proposes a daisy-chaining based FL learning framework, but the algorithm is not presented clearly. Oracle agg means performing average, but fig.1 (rirght) seems allowing partial average (note that $\\bar{h}$ still needed here). The proofs are not derived in a step-by-step way. For example, most of the convergence analysis used other papers' result directly.\n\nQuality: the idea of daisy chaining is interesting and it is expected that FedDC would work well and outperform others.  The theoretical analysis is not rigorous, where there is a gap between the high level design of the daisy chaining and implemented algorithm.\n\nNovelty: However, actually this strategy is very similar as permutation based SGD training by shuffling the data while here FedDC reshuffles the clients.\n\nReproducibility: the code is publicly accessible but not runnable.\n\n# Summary Of The Review\n\nIn summary, this paper proposes an efficient training framework, FedDC, for dealing with the data sparse problem, but the theoretical justification of the convergence and generalization contains a gap, which weakens the contributions of this work. \n\n====================== after Zoom Meeting ===================\n\nAfter the discussion with the AC and other reviewers, I suppose that there are some merits of this work and increase the score, even 1) the setting of the convergence part is not consistent with the PAC analysis and there is no condition of step size to ensure the convergence; 2) tuning the multiple hyper-parameters to satisfy the theoretical conditions is a very difficult task, 3) and there is still a gap between the theory and practical implementation for the PAC part.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable",
    "# Summary Of The Paper\nThe paper presents a novel approach to Federated Learning (FL) termed Federated Daisy-Chaining (FEDDC), which addresses the challenges of training models on small datasets that are common in sensitive domains like healthcare. The methodology combines model aggregation with a daisy-chaining technique, allowing models trained on different clients' datasets to learn collectively while maintaining data privacy. The authors provide formal proofs of convergence and theoretical guarantees for model improvement, alongside extensive empirical evaluations demonstrating that FEDDC significantly outperforms existing federated methods on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to improving model quality in federated settings with small local datasets, which is a critical issue in many applications. The theoretical contributions, including formal proofs and generalization error analyses, offer solid foundations for the proposed method. Additionally, the empirical results across varied datasets showcase the practicality and effectiveness of FEDDC. However, a potential weakness is the complexity of the proposed method, which may introduce challenges in implementation and understanding, particularly concerning the interplay between daisy-chaining and aggregation. The paper could also benefit from further exploration of the communication efficiency, especially in bandwidth-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents the problem, methodology, and results, making it accessible to a broad audience. The quality of the writing is high, with clear definitions and explanations of the methods used. The novelty of the approach is significant, particularly in the context of federated learning with small datasets, as it introduces a unique combination of techniques that have not been explored together before. Reproducibility is supported by detailed methodological descriptions and algorithmic steps, although more implementation details or code availability would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a compelling contribution to the field of federated learning by introducing the FEDDC method, which effectively addresses the challenges posed by small datasets. With a strong theoretical foundation and promising empirical results, the work is both significant and relevant, particularly for sensitive applications in healthcare.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Federated Daisy-Chaining (FEDDC), a novel approach to Federated Learning (FL) that addresses the challenges associated with training local models on small datasets. FEDDC employs a daisy-chaining technique, where local models are trained on sequential datasets from multiple clients, interspersed with aggregation steps to improve model performance. The authors provide theoretical guarantees for the convergence of FEDDC on convex problems and demonstrate its effectiveness through extensive experiments on synthetic data, CIFAR10, and two medical imaging datasets. The results indicate that FEDDC consistently outperforms traditional federated learning methods, achieving higher accuracy even with limited local data.\n\n# Strength And Weaknesses\nStrengths of the paper include its effective methodology for learning from small datasets, strong theoretical foundations with proven convergence, and versatility in compatibility with various aggregation techniques. The empirical results are compelling, showing significant performance improvements over existing federated learning methods, especially in sensitive applications such as healthcare. However, weaknesses include potential communication overhead due to increased rounds required for training, dependency on the initial quality of local models, and lingering privacy concerns associated with membership inference attacks during the daisy-chaining process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the proposed method along with its theoretical and empirical analyses. The quality of writing is high, with comprehensive explanations of the methodology and results. The novelty of the approach lies in the integration of daisy-chaining into federated learning, providing a fresh perspective on model training with small datasets. Reproducibility is supported by detailed descriptions of the experimental setup, including datasets and comparative methods, allowing other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in federated learning through the introduction of FEDDC, showcasing its ability to effectively train models on small, distributed datasets while maintaining privacy. The empirical results and theoretical guarantees support its applicability in sensitive domains, though attention must be paid to communication overhead and privacy risks.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper \"Federated Learning From Small Datasets\" presents a novel approach to enhance federated learning (FL) in scenarios where local datasets are limited, such as in healthcare. The authors introduce Federated Daisy-Chaining (FEDDC), a method that interleaves model aggregation and training across different datasets to improve model performance while ensuring data privacy. Key contributions include formal proofs of convergence for FEDDC, theoretical guarantees for model quality improvements in convex problems, and extensive empirical evaluations that demonstrate the superiority of FEDDC over standard FL approaches in terms of accuracy and privacy preservation.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to tackling the challenges of FL with small datasets, a significant concern in real-world applications, particularly in healthcare. The development of FEDDC and the accompanying theoretical proofs provide a solid foundation for its effectiveness. Moreover, the empirical results across various datasets, including real-world medical datasets, offer compelling evidence for the method's practical utility. However, a potential weakness is the reliance on specific assumptions underlying the convergence proofs, which may limit the applicability of FEDDC in more generalized settings. Additionally, further exploration is needed regarding the communication efficiency of the proposed method in bandwidth-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, theoretical foundations, and empirical results. The quality of the writing is high, with technical concepts explained sufficiently to facilitate understanding among readers with a background in machine learning and FL. The novelty of the proposed FEDDC approach stands out, particularly in its ability to address the challenges posed by small datasets. Reproducibility is supported by detailed descriptions of the experimental setup and the datasets used, although providing access to code and data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of federated learning by addressing a critical challenge associated with small datasets. The introduction of FEDDC, along with theoretical guarantees and robust empirical results, indicates strong promise for practical applications, particularly in healthcare. However, considerations regarding generalizability and communication efficiency warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel method called federated daisy-chaining (FEDDC) aimed at enhancing model training in small data environments by employing model permutations across clients and aggregation techniques. The authors provide theoretical guarantees for the convergence of FEDDC, demonstrating its effectiveness for convex problems along with PAC-like generalization guarantees. Extensive empirical validation on various datasets, particularly small medical imaging datasets, supports the practical applicability of the approach, which also incorporates privacy mechanisms and maintains low communication overhead.\n\n# Strength And Weaknesses\nThe paper's main contributions lie in its innovative approach to federated learning, particularly in the context of small datasets, and its provision of theoretical and empirical evidence supporting the method's performance. However, the theoretical analysis is primarily limited to convex problems, leaving a gap regarding its application in non-convex scenarios commonly encountered in deep learning. Additionally, while the empirical results are promising, they may not fully capture the performance of FEDDC in diverse real-world applications or under heterogeneous data distributions. The discussion of privacy considerations is valuable, although it raises concerns about potential security vulnerabilities that warrant further exploration. Lastly, the integration of FEDDC into existing frameworks may introduce complexity that requires careful implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and findings, making it accessible to readers. The quality of writing is high, with rigorous theoretical proofs and a comprehensive empirical evaluation. While the novelty of the FEDDC approach is significant, further validation across a broader array of datasets is needed to enhance reproducibility and ensure generalization of results.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in the field of federated learning, particularly for small datasets, backed by sound theoretical analysis and strong empirical results. However, it also identifies critical areas for further research, particularly regarding the robustness of the method in non-convex settings and the implications of privacy concerns.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel methodology for federated learning known as Federated Model Permutation (FMP), which addresses the challenges of training models on small local datasets in collaborative environments. The FMP approach utilizes a permutation-based redistribution of local models across clients, enhancing training efficiency while preserving data privacy. The authors provide a formal proof of convergence for FMP and demonstrate its efficacy through extensive empirical evaluations, showing that FMP outperforms traditional federated learning methods like FEDAVG, particularly in medical imaging applications.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative FMP approach, which offers a significant advancement over traditional federated learning methods that struggle with small datasets. The theoretical foundation is robust, providing formal convergence proofs and analysis of generalization error in low-sample scenarios. The empirical results are compelling, demonstrating both the performance benefits and practical applicability of FMP. However, a notable weakness is the potential communication overhead resulting from the increased number of model permutations, which, despite being deemed negligible in healthcare applications, warrants further exploration in other contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodologies, contributions, and results. The quality of writing is high, with a logical flow that makes the concepts accessible. The novelty of the FMP approach is significant, representing a departure from conventional federated learning techniques. Reproducibility appears to be supported by detailed empirical evaluation and thorough documentation of experiments, although providing access to code or datasets would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of federated learning by introducing the innovative FMP methodology, which effectively addresses the challenges associated with small local datasets while preserving data privacy. The theoretical and empirical findings support its significance, although some aspects related to communication overhead could be further examined.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel adversarial training technique tailored for scenarios with limited and distributed datasets, focusing on enhancing model robustness against adversarial attacks. The main contributions include the introduction of a \"daisy-chaining\" method for sequentially training adversarial models on local datasets, a model redistribution strategy to share knowledge without compromising data privacy, and a formal proof demonstrating the convergence and effectiveness of the proposed approach. Empirical results indicate that this method significantly outperforms conventional adversarial training techniques, particularly in low-data environments.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative integration of model redistribution with adversarial training, addressing a crucial gap in the literature regarding small datasets. The theoretical foundation lends credibility to the claims made, and the extensive empirical evaluations demonstrate clear improvements in robustness against adversarial attacks across various datasets. However, the paper lacks a detailed analysis of the communication overhead associated with the daisy-chaining method and could benefit from a deeper investigation into the effects of different adversarial perturbations on model performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to the reader. The methodology is described in sufficient detail to allow for reproducibility, although additional details on the experimental setups would enhance clarity. The novelty is apparent in the proposed techniques and their application to small datasets, which is an important area of research in adversarial learning.\n\n# Summary Of The Review\nThis paper offers a significant contribution to adversarial training by proposing a method that effectively addresses the challenges posed by small, distributed datasets. With a strong theoretical foundation and robust empirical validation, the work represents a meaningful advancement in the field, although it could further address certain practical implications related to communication overhead and adversarial perturbations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel method called federated daisy-chaining (FEDDC) aimed at enhancing federated learning, particularly when working with small datasets. The authors argue that traditional approaches falter under such constraints, and they position FEDDC as a solution that not only improves model aggregation but also significantly preserves data privacy. The methodology involves a sequential training technique across clients, which purportedly leads to superior model performance even in challenging conditions. Experimental results indicate that FEDDC outperforms existing federated learning methods and achieves accuracies comparable to centralized models.\n\n# Strength And Weaknesses\nStrengths of the paper include its ambitious approach to addressing the limitations of traditional federated learning, especially in scenarios with minimal data. The theoretical contributions, particularly the proof of convergence for convex problems, are noteworthy and provide a solid foundation for the proposed method. Additionally, the privacy claims, including reduced risks of membership inference attacks, are compelling. However, weaknesses include a lack of thorough empirical validation across diverse datasets, which could strengthen the claims made. Furthermore, while the theoretical guarantees are appealing, the practicality of implementing FEDDC in real-world scenarios remains somewhat underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, articulating the motivations behind the research and the proposed methodology clearly. The novelty of the approach is substantial, introducing a new technique that may redefine federated learning practices. However, some sections could benefit from additional detail, particularly regarding the implementation of FEDDC and its comparative performance metrics against a broader range of existing methods. The reproducibility of the results may be challenged due to the specificity of the datasets used, which were not extensively discussed.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in federated learning with the introduction of FEDDC, addressing critical issues associated with small datasets. While the theoretical contributions and privacy considerations are strong, the empirical validation could be enhanced to bolster the claims made. The potential implications for machine learning in sensitive fields are promising, but further exploration is necessary to fully assess the practicality of the approach.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to federated learning called \"federated daisy-chaining\" (FEDDC), which addresses the challenges of training models on small local datasets, particularly in sensitive domains like healthcare. The methodology involves interleaving model aggregation with permutation steps to improve learning from limited samples while preserving privacy. The findings demonstrate that FEDDC significantly enhances model performance compared to standard federated learning techniques, achieving high accuracy levels on multiple datasets, including SUSY, CIFAR10, and medical imaging datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to federated learning, particularly its ability to improve model accuracy with minimal local data while ensuring privacy. The theoretical contributions, including convergence guarantees and performance benchmarks, are well-articulated and provide a solid foundation for the proposed method. However, the paper's weaknesses include a limited exploration of potential limitations of the FEDDC method and the need for further empirical validation across diverse datasets and tasks beyond those presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the proposed methodology and thorough experimental results. The quality of the writing is high, making complex concepts accessible. The novelty of the FEDDC approach is significant in the context of federated learning, particularly for small datasets. However, while the results are promising, the reproducibility could be improved by providing additional details on the experimental setup and parameters used in the training process.\n\n# Summary Of The Review\nOverall, the paper offers a compelling contribution to the field of federated learning by introducing an effective method for enhancing model performance with small datasets. The innovative approach and strong empirical results suggest that FEDDC could have a substantial impact, particularly in sensitive applications like healthcare.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel federated learning framework termed FEDDC (Federated Daisy-Chaining), which aims to enhance model training efficiency by daisy-chaining local datasets from multiple clients. The authors assert that this approach can improve model quality while preserving data privacy. They provide theoretical insights and empirical results demonstrating the effectiveness of FEDDC on specific datasets, such as SUSY and CIFAR10, as well as medical imaging datasets. However, the methodology relies on several assumptions regarding data quality, communication overhead, and the nature of local datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper is its innovative approach to leveraging multiple local datasets through daisy-chaining, which could potentially enhance model performance in federated learning scenarios. The theoretical analysis, while grounded in established principles, does raise concerns about its applicability to non-convex problems. A significant weakness is the paper's reliance on the assumption that local datasets are of sufficient quality and representative, which overlooks the potential risks of poor-quality data affecting model training. Additionally, the communication overhead and generalization of results across different domains are not thoroughly explored, potentially limiting the framework's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. However, some assumptions made, particularly regarding data distribution and communication costs, could benefit from more rigorous justification. The novelty of the proposed FEDDC framework is commendable, but the reproducibility of results may be hindered by the limited range of datasets used for empirical validation. The lack of comprehensive performance metrics beyond accuracy is another critical aspect that could be improved for better clarity.\n\n# Summary Of The Review\nThe paper presents a promising approach to federated learning with its FEDDC framework, which aims to improve model training through daisy-chaining. However, the reliance on several assumptions regarding data quality, communication overhead, and the generalizability of results raises concerns about the robustness and applicability of the proposed method in diverse settings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel federated learning method called FEDDC, which interleaves model aggregation and permutation steps to enhance learning from small datasets while preserving privacy. The authors address the challenge of poor local models adversely impacting overall model quality in federated learning, particularly in data-sparse domains such as healthcare. The methodology involves a daisy-chaining approach to sequentially train models on local datasets, preserving privacy and improving performance. The theoretical guarantees of the algorithm are established, demonstrating convergence in convex settings. Extensive empirical evaluations indicate that FEDDC significantly outperforms traditional federated learning methods, offering improved accuracy and robustness in scenarios with limited data.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its clear identification of a significant problem in federated learning—learning from small datasets—and its innovative solution through the FEDDC approach. The theoretical contributions, including formal proofs of convergence and analysis of generalization error, add rigor to the methodology. However, the paper could benefit from a more detailed discussion on the potential limitations of the FEDDC algorithm in non-convex settings and broader applicability beyond healthcare datasets. Additionally, while the experimental results are compelling, they could be strengthened by including comparisons with more baseline methods beyond the standard federated learning approaches discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the complex ideas accessible to readers. The methodology is detailed, and the theoretical underpinnings are solid, contributing to the overall quality of the work. The novelty of the approach is evident, particularly in its combination of daisy-chaining and model aggregation. Reproducibility is addressed to some extent through the detailed description of the algorithm and experimental setup; however, providing access to code or datasets would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of federated learning, particularly for applications involving small datasets. The innovative FEDDC approach, supported by theoretical guarantees and empirical results, addresses significant challenges in this domain. Nonetheless, the paper could strengthen its discussion of limitations and expand its comparisons with other methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing model training in distributed data environments while addressing privacy concerns. It introduces a new algorithm that integrates federated learning with differential privacy techniques, aiming to improve both model performance and data security. Through extensive theoretical analysis and empirical validation across various datasets, the authors demonstrate that their method outperforms existing baseline models in terms of accuracy and privacy guarantees.\n\n# Strength And Weaknesses\n**Strengths:**\n- **Innovative Approach**: The integration of federated learning with differential privacy is a notable advancement, making the method relevant in today's data-sensitive landscape.\n- **Theoretical Contributions**: The paper provides well-founded theoretical insights, including convergence proofs and privacy guarantees that bolster the proposed algorithm's reliability.\n- **Comprehensive Experiments**: The empirical validation is thorough, encompassing multiple datasets and scenarios, which strengthens the credibility of the findings.\n\n**Weaknesses:**\n- **Clarity Issues**: Some methodological details, particularly the algorithm's implementation and parameter selection, lack sufficient clarity, which could hinder reproducibility.\n- **Limited Benchmarks**: The comparative analysis, while present, could benefit from a broader range of baseline methods to better contextualize the performance improvements.\n- **Discussion of Limitations**: The paper does not adequately address potential limitations or drawbacks of the proposed approach, which could provide a more balanced evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper exhibits a high level of quality in terms of theoretical foundations and empirical evaluation. However, clarity could be improved, particularly in explaining complex concepts and implementation details. The novelty of the approach is significant, as it addresses pressing issues in the intersection of federated learning and privacy. Nonetheless, the lack of comprehensive detail may pose challenges to reproducibility.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method that enhances model training in distributed environments while addressing privacy concerns. While the contributions are noteworthy and theoretically sound, the clarity of presentation and scope of empirical comparisons require improvement to strengthen the overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to federated learning called Federated Daisy-Chaining (FEDDC), which addresses the challenges posed by small local datasets in collaborative model training. The methodology interleaves model aggregation and permutation steps to enhance the effectiveness of training under data-sparse conditions, while maintaining data privacy. The findings indicate that FEDDC not only improves the quality of the aggregated model but also achieves performance comparable to centralized learning, especially in scenarios typical of healthcare where data privacy and small datasets are prevalent.\n\n# Strength And Weaknesses\nThe main contribution of the paper is the introduction of the FEDDC method, which successfully combines daisy-chaining and aggregation techniques to improve federated learning outcomes with small datasets. This is particularly relevant for applications in healthcare, where data privacy is critical. One strength is the empirical validation that demonstrates FEDDC's superior performance compared to existing federated learning methods. However, a potential weakness lies in the scalability of the method to larger datasets or more complex model architectures, which is not fully addressed in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem of federated learning with small datasets and the proposed solution. The methodology is described in sufficient detail, allowing for reproducibility of experiments. The novelty of the approach is significant, particularly in its combination of daisy-chaining and model aggregation, which has not been widely explored in the literature. However, further empirical evaluations across diverse datasets would enhance the robustness of the findings.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of federated learning by presenting an innovative method tailored for small datasets, especially in privacy-sensitive domains like healthcare. The empirical results substantiate the theoretical claims, showcasing the method's effectiveness. Future work could explore the scalability and application of FEDDC in a broader range of contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Federated Learning from Small Datasets\" by Kamp et al. (2023) presents a novel approach called Federated Daisy-Chaining (FEDDC) aimed at improving federated learning performance in scenarios where local datasets are limited. The authors introduce a methodology that interleaves model aggregation with random permutation of local datasets to enhance model training while preserving data privacy. Through extensive empirical evaluations on both synthetic and real-world datasets, including CIFAR10 and medical imaging, the authors demonstrate that FEDDC consistently outperforms existing federated learning techniques, such as FEDAVG and FEDPROX, particularly in low-sample environments.\n\n# Strength And Weaknesses\nThe strengths of this paper include its innovative approach to addressing the challenges posed by small datasets in federated learning, as well as the thorough empirical validation that supports the proposed methodology. The formal convergence proof provided for FEDDC is a notable theoretical contribution that enhances the credibility of the approach. However, the paper could benefit from a deeper exploration of the communication efficiency of the proposed method, especially in bandwidth-constrained environments, which is a critical factor in practical applications of federated learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The writing quality is high, making complex concepts accessible without oversimplifying. The novelty of the FEDDC approach is significant, as it addresses a pressing challenge in federated learning—training on small datasets. The reproducibility of the results appears strong, given the detailed description of the methodology and the extensive empirical validation across various datasets.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning, particularly for applications involving small datasets. The proposed FEDDC method is both technically sound and empirically validated, although further work on communication efficiency could enhance its applicability in real-world scenarios.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper, titled \"Federated Learning from Small Datasets\" by Michael Kamp, Jonas Fischer, and Jilles Vreeken, introduces a novel federated learning approach called FEDDC, which effectively addresses the challenges of training models with small datasets. The methodology combines model aggregation with permutation techniques to enhance learning efficiency and accuracy. The authors provide theoretical guarantees related to convergence and model quality, as well as empirical results demonstrating significant improvements over existing methods when tested on synthetic datasets, CIFAR10, and medical imaging tasks.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its comprehensive approach to federated learning in scenarios where data is scarce, particularly in healthcare, where such conditions are common. The proposed FEDDC method is well-justified, with clear theoretical foundations and extensive experimental validation. However, a weakness is the relatively limited exploration of the method's limitations, which could provide a more balanced view of its applicability. Additionally, the paper could benefit from a more detailed description of experimental setups to enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to a broad audience. The quality of the work is high, with a thorough review of related literature and a sound methodological framework. The novelty of the approach is significant, as it combines existing techniques in a new way tailored for small datasets. However, while the authors provide some pseudocode and a clear description of the process, additional details on experimental setups would improve reproducibility.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the field of federated learning by proposing an innovative method for small datasets, supported by both theoretical and empirical evidence. The clarity of the writing and the relevance of the findings particularly stand out, although the paper could benefit from a deeper discussion of limitations and more detailed experimental descriptions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel federated learning methodology termed Federated Daisy-Chaining (FEDDC) designed specifically to enhance model training from small datasets, common in fields like healthcare. The approach interleaves local model training with randomized redistribution of models among clients, rather than simple aggregation, thereby addressing the challenges of poor local model performance. The authors provide theoretical guarantees on convergence and demonstrate through extensive empirical evaluations that FEDDC outperforms traditional federated learning techniques, particularly in accuracy and robustness against overfitting, while maintaining data privacy.\n\n# Strength And Weaknesses\nThe paper makes significant contributions to the field of federated learning by proposing a new methodology that effectively addresses the issues associated with training on small datasets. The theoretical foundations are rigorously established, with clear convergence proofs and a well-defined aggregation mechanism using the Radon point. However, the paper could benefit from further exploration of potential security vulnerabilities introduced by the permutation of models, as well as a more detailed investigation into the communication efficiency in bandwidth-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology, theoretical underpinnings, and empirical results. The quality of writing is high, with sufficient detail provided for reproducibility. The novelty of the daisy-chaining approach is evident, as it introduces a fresh perspective on model aggregation in federated learning contexts. Nonetheless, while the theoretical aspects are sound, the practical implications regarding communication efficiency and potential privacy risks could be elucidated further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in federated learning by addressing the challenges of small datasets through the innovative FEDDC methodology. Its theoretical and empirical contributions are robust, although further exploration of communication and privacy implications is warranted.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach called Federated Daisy-Chaining (FEDDC) for federated learning, which emphasizes the aggregation of local models through a permutation-based technique. The authors claim that this method can effectively enhance model performance, particularly in situations with small local datasets, while maintaining privacy and reducing communication overhead. The findings suggest potential improvements over traditional federated learning strategies like FEDAVG, although the empirical validation relies on a limited set of datasets.\n\n# Strength And Weaknesses\nThe proposed method's primary strength lies in its innovative combination of daisy-chaining and model aggregation, which aims to optimize federated learning. However, significant weaknesses undermine its contributions: the dependence on local model permutation may not guarantee performance in practice; reliance on theoretical guarantees without robust empirical validation casts doubt on its real-world applicability; and concerns about privacy remain, particularly regarding membership inference attacks. Additionally, the performance comparisons to existing methods may not be adequately justified, and the limitations of the theoretical models used for convergence proofs are underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by a lack of thorough exploration of the limitations of the proposed approach and insufficient discussion of alternative methods. While the novelty of the FEDDC concept is evident, the practical implications are less clear due to limited empirical validation. Reproducibility may be hindered as the experiments are based on a small number of datasets and the potential variability in outcomes is not thoroughly addressed.\n\n# Summary Of The Review\nOverall, while the paper presents an interesting approach to federated learning through FEDDC, its theoretical foundations and empirical results raise several concerns regarding practical applicability, privacy, and robustness. The contributions, though novel, are overshadowed by significant weaknesses and limitations.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Federated Daisy-Chaining (FEDDC), a novel approach to federated learning tailored for small datasets, particularly within sensitive domains such as healthcare. FEDDC combines model aggregation and permutation techniques to facilitate effective learning while maintaining data privacy. The findings indicate that FEDDC significantly outperforms traditional federated learning methods, achieving accuracy levels comparable to centralized models while utilizing substantially less data. The empirical results demonstrate the method's robustness and potential for real-world applications, particularly in critical healthcare scenarios.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative methodology and the impressive empirical results it presents. By addressing the challenges of learning from small, distributed datasets, FEDDC fills a crucial gap in the field of federated learning. Additionally, the method's ability to integrate with differential privacy mechanisms enhances its appeal for privacy-sensitive applications. However, the paper could benefit from a more detailed exploration of the limitations of the proposed approach, as well as a comparison with a broader range of existing methods beyond FEDAVG and FEDPROX.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its ideas clearly, making it accessible to a broad audience. The quality of the methodology is high, and the results are presented with sufficient detail to allow for reproducibility. The novelty of the approach is significant, as it introduces a new paradigm in federated learning specifically designed for scenarios with limited data. The potential for future applications is also well articulated, although more explicit discussion of implementation challenges could enhance the paper's practical relevance.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in federated learning through the introduction of FEDDC, with strong empirical support for its efficacy in small dataset scenarios, particularly in healthcare. The methodology's innovative aspects and its impact on preserving privacy make it a valuable contribution to the field, though further exploration of its limitations would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel theoretical framework for Federated Learning (FL) called Daisy-Chaining, aimed at addressing the challenges posed by small datasets in decentralized learning settings, particularly in sensitive domains such as healthcare. It outlines the issues of local model quality and the impact of small sample sizes on model convergence and generalization. The proposed methodology demonstrates how daisy-chaining local models across clients can enhance robustness and convergence guarantees, with a focus on probabilistic model quality and the effectiveness of the Radon point as an aggregation method. Empirical results indicate that the Federated Daisy-Chaining (FEDDC) method outperforms traditional FL methods in scenarios with limited data.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, which provide a robust foundation for improving model quality in federated learning contexts with small datasets. The introduction of daisy-chaining as a mechanism for enhancing model robustness is innovative and adds significant value to the existing literature. However, the paper could benefit from clearer empirical validation of the proposed methods across diverse datasets and practical scenarios beyond theoretical proofs. Additionally, while the privacy implications of daisy-chaining are acknowledged, the discussion is somewhat superficial and could be expanded to address potential risks comprehensively.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making the theoretical constructs accessible to readers with varying levels of expertise in FL. The quality of the theoretical analysis is high, providing solid convergence guarantees and a comprehensive evaluation within the PAC-learning framework. However, the reproducibility of the results could be enhanced by including more detailed descriptions of experimental setups and datasets used for empirical validation. While the theoretical contributions are novel, the empirical results should be more robustly supported to reinforce the proposed claims.\n\n# Summary Of The Review\nOverall, the paper makes a significant theoretical contribution to the field of Federated Learning by introducing the Daisy-Chaining framework, which addresses critical issues associated with small datasets. While the theoretical foundations are strong, the empirical validation is less compelling and warrants further investigation to strengthen confidence in the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Federated Daisy-Chaining (FEDDC), a novel method designed to enhance federated learning performance, particularly in scenarios involving small datasets. The methodology involves clients training local models and periodically sending them to a central server for aggregation and daisy chaining, with a focus on communication efficiency. The authors provide theoretical guarantees for convergence in convex settings and demonstrate that FEDDC significantly improves model accuracy over traditional federated learning methods, including FEDAVG and others, across various datasets such as CIFAR10 and medical imaging datasets. Additionally, the paper addresses privacy concerns related to model updates and membership inference attacks and makes the code publicly available for reproducibility.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of FEDDC, which effectively addresses the limitations of existing federated learning methods when dealing with small datasets. The theoretical guarantees for convergence bolster the credibility of the proposed method. However, the paper could benefit from a more detailed discussion on the experimental setup, including the specific datasets, training configurations, and parameter tuning, as these aspects are crucial for understanding the practical implications of FEDDC. Another potential weakness is the assumption of non-bottleneck communication, which may not hold in all real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and findings, making it accessible to the reader. The quality of the writing is high, and the use of pseudocode enhances understanding. The novelty of FEDDC is significant, as it introduces a new approach to federated learning, yet the empirical results could be better contextualized with more comprehensive experimental details. The authors provide access to their code, which supports reproducibility, though additional documentation would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a promising advancement in federated learning through the introduction of FEDDC, demonstrating its effectiveness with theoretical backing and experimental validation. While it has notable strengths, particularly in addressing small dataset challenges, the paper would benefit from clearer details on experimental setups and broader applicability considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Federated Daisy-Chaining (FEDDC), a novel approach to federated learning that claims to outperform traditional methods such as FEDAVG, particularly on smaller datasets. The authors present empirical results suggesting that FEDDC achieves performance comparable to centralized models while utilizing significantly less local data. The methodology emphasizes improved communication efficiency and privacy preservation, although the theoretical guarantees are asserted without thorough contextualization within existing literature.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its proposed methodology, FEDDC, which demonstrates promising empirical results, particularly in small data scenarios. However, the paper has several weaknesses, including potential biases in its comparisons to existing methods, such as FEDAVG and FEDPROX, without adequately addressing their strengths. The authors make claims regarding the privacy advantages of FEDDC but do not sufficiently compare these guarantees to established privacy-preserving techniques. Furthermore, the theoretical claims lack contextual grounding, which may misrepresent the contributions of previous work in federated learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the lack of thorough discussions regarding the limitations of competing methods, which could confuse readers about the validity of the comparisons made. While the methodology appears novel, the presentation does not fully address reproducibility, as the conditions under which empirical results were gathered are inadequately discussed. The paper would benefit from a more balanced view of existing methods to enhance its quality and clarity.\n\n# Summary Of The Review\nOverall, while the FEDDC approach presents interesting results and claims to improve upon traditional federated learning methods, the paper lacks a fair and comprehensive evaluation of existing methodologies. The comparisons drawn may be misleading, and the theoretical claims require better contextualization within the broader literature.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Federated Learning from Small Datasets\" presents a novel approach to federated learning that addresses the challenges of training models with limited data availability across multiple parties. The authors propose a method that incorporates a daisy-chaining mechanism alongside aggregation to enhance model performance and robustness. Through comprehensive experiments on benchmark datasets such as CIFAR10, the authors demonstrate that their approach significantly improves learning outcomes compared to traditional federated learning methods, particularly when data is scarce.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative combination of daisy-chaining and aggregation techniques, which provides a practical solution for federated learning scenarios with small datasets. The methodology is well-structured, and the empirical results are convincing, showcasing the efficacy of the proposed approach. However, certain aspects could be improved, such as the clarity of the experimental setup, which lacks detailed descriptions of variations and the number of experiments conducted. Additionally, the paper could benefit from a more concise abstract and clearer conclusion to summarize key findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the clarity of the paper is hindered by several formatting issues and typographical errors, which detract from the reader's understanding. While the novelty of the approach is commendable, the paper's reproducibility may be challenged due to insufficient details in the experimental setup. Consistent formatting of references, figures, and algorithms would enhance the overall quality and facilitate reproducibility.\n\n# Summary Of The Review\nThe paper presents a promising approach to federated learning from small datasets, combining daisy-chaining with aggregation techniques. While the contributions are significant, clarity issues and formatting inconsistencies detract from the overall presentation. Addressing these concerns could strengthen the paper's impact and accessibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to federated learning called FEDDC, specifically targeting small datasets, particularly in medical imaging contexts. It aims to enhance model accuracy while addressing privacy concerns associated with federated learning. The methodology involves a decentralized framework that promises improved convergence rates and model quality, although the evaluation is primarily limited to specific datasets. The findings suggest that FEDDC outperforms existing methods in the tested scenarios, but the generalizability of these results remains to be demonstrated across diverse domains.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its focus on small datasets, an area often overlooked in federated learning research. The proposed FEDDC framework addresses critical privacy issues, which is timely and relevant. However, the paper has notable weaknesses, including a limited scope of evaluation that does not extend to larger datasets or diverse real-world applications. Furthermore, while the authors touch upon communication overhead, they fail to provide a thorough analysis of optimizing communication efficiency. There is also a lack of exploration into adversarial threats beyond membership inference and scalability concerns with increased client numbers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that facilitates understanding of the proposed methodology and results. However, it could benefit from a deeper exploration of certain aspects, such as the theoretical guarantees provided, which are currently confined to convex problems. The novelty of the approach is commendable, but the lack of extensive empirical validation across varied contexts raises questions about its reproducibility and practical application.\n\n# Summary Of The Review\nOverall, while the paper introduces a novel approach to federated learning that addresses some key challenges, its contributions are somewhat limited by the narrow scope of evaluation and absence of comprehensive discussions on scalability and adversarial threats. The findings are promising, but further exploration and empirical validation are needed to fully substantiate the claims made.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel federated learning approach termed **Federated Daisy-Chaining (FEDDC)**, specifically designed for environments with small datasets distributed across multiple clients. The methodology employs rigorous statistical analyses to validate the efficiency of FEDDC compared to existing federated learning techniques. Key findings include a formal proof of convergence, probabilistic guarantees on generalization error, and empirical results showing significant improvements in model performance, particularly in non-IID settings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its rigorous statistical methodology, particularly the formal proofs of convergence and the analysis of generalization error bounds, which provide a strong theoretical foundation for the proposed approach. The use of Radon points to enhance aggregated model quality is a noteworthy contribution. However, the paper's reliance on specific sample size thresholds for effective performance may limit its applicability in scenarios where these conditions are not met. Additionally, while the empirical results are promising, the lack of extensive comparisons to a broader range of existing methods may less convincingly demonstrate FEDDC's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers with a background in federated learning and statistical analysis. The quality of writing is high, and the theoretical contributions are clearly presented. The novelty of the approach is significant, particularly in its application to small datasets and its theoretical underpinnings. Reproducibility is supported by detailed descriptions of the experimental setups and statistical methods used, although additional information on hyperparameter tuning and data preprocessing would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in federated learning for small datasets through the introduction of FEDDC. Its strong theoretical and empirical contributions demonstrate its potential effectiveness, although some limitations in scope and parameter considerations are noted.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Federated Daisy-Chaining (FEDDC), a novel approach aimed at enhancing privacy in federated learning by allowing clients to chain their model updates. The methodology focuses primarily on convex optimization problems, providing some theoretical guarantees. The authors demonstrate the feasibility of FEDDC through empirical results on specific datasets, including SUSY, CIFAR10, and medical imaging, highlighting improvements over standard federated learning methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to federated learning and its empirical validation on relevant datasets. However, significant weaknesses are evident, including a lack of comprehensive treatment of privacy concerns, particularly regarding membership inference attacks. The theoretical framework is predominantly limited to convex problems, which may not translate well to non-convex scenarios common in real-world applications. Furthermore, the analysis of communication complexity is context-specific, and the paper lacks a broader comparison with existing federated learning methods, particularly in non-IID settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. However, the discussion surrounding the scalability of the method and its performance in dynamic environments is insufficient. While the novelty of the approach is clear, the reproducibility of results could be improved by providing more extensive details on experimental setups and configurations. Additionally, the limited exploration of different daisy-chaining configurations suggests that there are gaps in understanding the optimal settings.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of federated learning through the introduction of FEDDC. However, it has notable limitations, particularly in its theoretical treatment of non-convex problems, privacy concerns, and comprehensive comparisons with existing methods. Future research is warranted to address these gaps and enhance the practical applicability of the proposed approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Federated Learning from Small Datasets\" discusses the challenges of training machine learning models in a federated setting, particularly when local datasets are small. The authors propose a method called Federated Daisy-Chaining (FEDDC), which they claim enhances model quality by redistributing model updates among participating clients. Their theoretical analysis purports to show convergence and improvement in model performance, although much of the discussion reiterates known concepts in the field. Experimental results suggest improvements over existing methods, but the benchmarks and datasets employed are standard in the literature.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its clear articulation of the issues surrounding federated learning with small datasets, which remains a relevant topic. However, the proposed daisy-chaining method lacks true novelty, resembling existing techniques without introducing significant innovation. The theoretical claims appear to lack depth and do not provide substantial new insights into the problem. Moreover, the experimental results, while showing improvements, rely on standard benchmarks that do not convincingly establish the method's superiority over existing approaches.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, allowing for a clear understanding of the authors' claims. However, the novelty of the proposed methods is questionable, as they often echo established ideas in the literature. The reproducibility of the results is not adequately addressed, as the authors do not provide sufficient details on the implementation or the datasets used, which could hinder validation by others in the community. \n\n# Summary Of The Review\nOverall, the paper presents a familiar topic with a method that lacks genuine innovation and depth. While it highlights important issues in federated learning, the proposed approach does not contribute significantly to the field, and many claims made are well-trodden ground. \n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces Federated Daisy-Chaining (FEDDC), an innovative method for federated learning that combines model permutations and aggregation techniques. It addresses the challenge of learning from small datasets while providing theoretical guarantees for convergence and generalization error in convex settings. The empirical results demonstrate that FEDDC outperforms various baseline federated learning methods, particularly in sensitive domains like healthcare, highlighting its potential for real-world applications.\n\n# Strength And Weaknesses\nThe main strengths of FEDDC lie in its novel approach to aggregation and its potential to improve model performance in federated learning contexts with limited data. The theoretical framework provided offers valuable insights into convergence properties, and the method shows strong empirical performance. However, the paper could benefit from a more comprehensive exploration of robust aggregation techniques, semi-supervised learning, and advanced privacy mechanisms. Additionally, the communication overhead associated with federated learning is acknowledged but not sufficiently addressed, suggesting a need for further investigation into efficiency improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and findings. The novelty of the approach is evident, particularly in the integration of daisy-chaining with existing federated learning methods. While the theoretical results are compelling, further work is needed to enhance reproducibility, particularly in non-convex settings and more complex model architectures.\n\n# Summary Of The Review\nOverall, FEDDC presents a significant advancement in federated learning by effectively addressing challenges associated with small datasets and local model performance. The paper's contributions are promising, but there are opportunities for further improvement in robustness, privacy guarantees, and communication efficiency.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel federated learning approach termed Federated Daisy-Chaining (FEDDC), which addresses the challenges of training models with limited data availability. The authors present a comprehensive evaluation of FEDDC, demonstrating its substantial performance improvements over existing federated learning methods, particularly in small dataset scenarios. Experimental results on datasets such as SUSY and CIFAR10 reveal that FEDDC achieves competitive accuracy levels comparable to centralized models, while also outperforming standard federated learning techniques like FEDAVG. Additionally, FEDDC shows promising results in real-world medical imaging applications, reinforcing its practical applicability.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to federated learning that effectively tackles data scarcity challenges, yielding significant improvements in model accuracy. The extensive empirical evaluation across various datasets highlights the robustness of FEDDC and its effectiveness in real-world scenarios where data is often limited. However, a potential weakness is the lack of detailed theoretical analysis or insights into the mechanisms that enable FEDDC's performance improvements, which could provide a deeper understanding of its advantages over existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with a logical progression through the methodology and results. The quality of the experiments is high, with thorough comparisons against multiple baselines, enhancing the credibility of the claims. In terms of novelty, the introduction of FEDDC represents a significant advancement in federated learning methodologies. However, the reproducibility of results may depend on the availability of the datasets and the implementation details, which should ideally be made accessible for verification purposes.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in federated learning through the introduction of FEDDC, demonstrating superior performance in scenarios with limited data availability. While the empirical results are strong and compelling, further theoretical insights could enhance the understanding of the approach's efficacy.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Federated Learning from Small Datasets\" presents a novel approach to enhance federated learning mechanisms when dealing with limited data availability. The authors propose a methodology that leverages techniques such as \"daisy chaining\" and \"Radon points\" to improve model performance in federated settings. Empirical evaluations demonstrate that the proposed method achieves superior results compared to existing techniques, showing significant improvements in model accuracy and convergence speed under small dataset conditions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to federated learning, particularly in addressing the challenges posed by small datasets. The methodology is well-conceived and grounded in theoretical principles, offering a fresh perspective on enhancing learning efficiency. However, the paper has notable weaknesses, including a lack of clarity in presentation and potential overuse of technical jargon that may alienate readers unfamiliar with the terms. Furthermore, the contribution section lacks a clear delineation, which could hinder readers' understanding of the key advancements made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing varies throughout the paper. While the technical details are robust, the overall structure and flow could be improved. The use of lengthy paragraphs and dense citations disrupts the reading experience and diminishes the paper's accessibility. Additionally, the novelty of the proposed approach is significant, yet the reproducibility is questionable due to insufficient detail in the empirical evaluation section, which lacks clear explanations of the setup and metrics used.\n\n# Summary Of The Review\nOverall, the paper presents a promising direction for federated learning in small dataset scenarios, highlighting important contributions to the field. However, issues in clarity, structure, and reproducibility need to be addressed for the paper to reach its full potential.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.7446142791938697,
    -1.591098711527919,
    -1.6390366084309762,
    -1.4160598969754838,
    -1.9042131908092987,
    -1.768734756604363,
    -1.7488218329214629,
    -1.756041845772736,
    -1.6569233266600494,
    -1.697312604838745,
    -1.6430499659055344,
    -1.5556597668207253,
    -1.6109950414506176,
    -1.483848519469414,
    -1.6328000026680902,
    -1.8480535734667518,
    -1.9688491678307916,
    -1.6106698162583872,
    -1.8770970836281804,
    -1.617165481050623,
    -1.9424885496772468,
    -1.6363593885112564,
    -1.894842260000573,
    -1.7448640034075664,
    -1.8338347366195689,
    -1.8182393521471523,
    -1.8155898060106503,
    -1.5441904261226367,
    -1.6814505774326922
  ],
  "logp_cond": [
    [
      0.0,
      -2.5944427452392818,
      -2.603674080631492,
      -2.5988273400456365,
      -2.5931306065717665,
      -2.6188097537727804,
      -2.6326281305035555,
      -2.614118039184818,
      -2.572291608962637,
      -2.6066821419073767,
      -2.5741761524765328,
      -2.633475858119715,
      -2.5778848696910353,
      -2.5777363542283473,
      -2.615726176971618,
      -2.59380525663108,
      -2.590806094317793,
      -2.603034787783826,
      -2.608517271005133,
      -2.6000276999497363,
      -2.6087137725245713,
      -2.6027920444230297,
      -2.6351979704056765,
      -2.5999518225736296,
      -2.6125563295720022,
      -2.6052102121280933,
      -2.597769314587138,
      -2.6056809299513826,
      -2.628301915951412
    ],
    [
      -1.3351493151121554,
      0.0,
      -1.1621521643419064,
      -1.1620485537687144,
      -1.1745089700677613,
      -1.2406581433853272,
      -1.242103676102573,
      -1.2011616583522768,
      -1.1221471395022564,
      -1.2207149591073474,
      -1.1423973903290479,
      -1.312101213904091,
      -1.0954837750170945,
      -1.1966011500585587,
      -1.1807059426832691,
      -1.1496085145203974,
      -1.2242274444137187,
      -1.1466276230865984,
      -1.1397362904540103,
      -1.1828542894014498,
      -1.2138882451109494,
      -1.1951713349776993,
      -1.2031281371632283,
      -1.144518327312433,
      -1.274777235168319,
      -1.2228395912513903,
      -1.1392081484727845,
      -1.1935802826343525,
      -1.2623835491999191
    ],
    [
      -1.3512092408689451,
      -1.1686072015913136,
      0.0,
      -1.199546774251192,
      -1.1992728917501385,
      -1.2861617647664647,
      -1.3131153156530064,
      -1.2245439969328167,
      -1.1669843771298205,
      -1.2435322504035804,
      -1.224794891621614,
      -1.3646374360853217,
      -1.1939321784173604,
      -1.2266501806602998,
      -1.2612772253824898,
      -1.1949794518581247,
      -1.2021140751934916,
      -1.1988018005483898,
      -1.2158820428230228,
      -1.2043259054929742,
      -1.2589294381127238,
      -1.2464368655242697,
      -1.2663225002800986,
      -1.2045800331509715,
      -1.2511424030261655,
      -1.296088170347474,
      -1.1684645829476237,
      -1.2516505378716534,
      -1.299922441008043
    ],
    [
      -1.1894716407827597,
      -1.0139751034399298,
      -1.0584233011078883,
      0.0,
      -1.0525671295665682,
      -1.1144716618214476,
      -1.1083472938470034,
      -1.0554928548654545,
      -1.0315987995316442,
      -1.064572454081848,
      -1.0105597540107762,
      -1.1612480166666508,
      -1.050255326899047,
      -0.9913067652213798,
      -1.101359204626673,
      -1.060896563956119,
      -1.0714936533129271,
      -1.0631330097670821,
      -1.0402963886210688,
      -1.0621727488994512,
      -1.0767713598936302,
      -1.0922692162067131,
      -1.0782943510304122,
      -1.0566446625375185,
      -1.0692456183487278,
      -1.0793895560249898,
      -1.0645214798912093,
      -1.0776882301626791,
      -1.1247503656075872
    ],
    [
      -1.5982108190111566,
      -1.5415533233129324,
      -1.5420427984265772,
      -1.5382873437905382,
      0.0,
      -1.5407545947023429,
      -1.5963845541724866,
      -1.4837220376585,
      -1.5280888138185407,
      -1.482860618213917,
      -1.5256409121081533,
      -1.5857539347086904,
      -1.5546887532221076,
      -1.5093361997686203,
      -1.523611354391234,
      -1.5092555407795303,
      -1.4841599920382111,
      -1.5580724438046092,
      -1.5001623534524207,
      -1.4907082980351087,
      -1.552357139859704,
      -1.5590063744797313,
      -1.4739448740082632,
      -1.5348934760348854,
      -1.525427647642775,
      -1.519048504797482,
      -1.4962615822480467,
      -1.57928110145778,
      -1.5472803962161503
    ],
    [
      -1.4839649998153541,
      -1.3554185040716153,
      -1.3922915852374784,
      -1.388698735122987,
      -1.3096572618879705,
      0.0,
      -1.4039024232401538,
      -1.3883142065021277,
      -1.3655736280512736,
      -1.3660820956006923,
      -1.3209986618596001,
      -1.432111432231376,
      -1.3880215718255626,
      -1.3159015726311587,
      -1.3302700361288036,
      -1.3171546313027447,
      -1.3898549933504372,
      -1.3736290761186245,
      -1.3721677795376703,
      -1.3836277985762158,
      -1.3994786373337145,
      -1.3903502968216146,
      -1.3916175594810967,
      -1.3770976291845378,
      -1.4600813120831813,
      -1.3663801952965067,
      -1.3524667199726341,
      -1.387497986488543,
      -1.362359961687744
    ],
    [
      -1.449997873601635,
      -1.3470742828705522,
      -1.3896818780865179,
      -1.3633548557686188,
      -1.3750231523557137,
      -1.3817414273347082,
      0.0,
      -1.3971290665137295,
      -1.3640472457034352,
      -1.3973246434631899,
      -1.3215077949699663,
      -1.4184381512427255,
      -1.3900887534938062,
      -1.301739915928939,
      -1.3978230072546385,
      -1.31448513828625,
      -1.3508349944899771,
      -1.3513519681516155,
      -1.343666727146971,
      -1.3484986596620139,
      -1.3674441080557231,
      -1.3520621535335855,
      -1.372024079058565,
      -1.339905230203676,
      -1.3718538364231423,
      -1.3949751456932102,
      -1.3671116188765162,
      -1.4100542312269642,
      -1.4015456812993008
    ],
    [
      -1.4875440755216924,
      -1.3981172715721641,
      -1.364820486275764,
      -1.3895903817393032,
      -1.3272708894370713,
      -1.4146294389771035,
      -1.4672775183344435,
      0.0,
      -1.3912687409338778,
      -1.3749399686983665,
      -1.394451172618122,
      -1.4523802295023829,
      -1.383953309935078,
      -1.4231157435884283,
      -1.377037242314996,
      -1.4198144250867606,
      -1.417340613369596,
      -1.3660422118859195,
      -1.3869702748354036,
      -1.3871997401921587,
      -1.439554529258117,
      -1.4107811242971424,
      -1.392473790020901,
      -1.4092823165865564,
      -1.444060136959593,
      -1.3912513817916112,
      -1.4122295760288048,
      -1.4126096242386943,
      -1.4198438460229046
    ],
    [
      -1.3551571533014009,
      -1.1454741596897453,
      -1.1743911605664452,
      -1.144311541795769,
      -1.1593061714041875,
      -1.2483166387449178,
      -1.292963387757507,
      -1.18546301158992,
      0.0,
      -1.2300186460861715,
      -1.1063229783274482,
      -1.3190209796259131,
      -1.1253541548988792,
      -1.122533961923982,
      -1.1379573083792025,
      -1.1765415108954722,
      -1.2232235815837702,
      -1.1469050470845386,
      -1.1797513604423215,
      -1.1703956043286565,
      -1.2391354760945794,
      -1.2377907440305491,
      -1.209695594989956,
      -1.2006272422504833,
      -1.2396798030251084,
      -1.2579845786578836,
      -1.1600267833728728,
      -1.1803348221485888,
      -1.2765767777472452
    ],
    [
      -1.4398294554881843,
      -1.3410188435177037,
      -1.3272913362914402,
      -1.351073650752421,
      -1.2505056554265048,
      -1.3588237299983008,
      -1.4318076883437139,
      -1.3136461490704356,
      -1.31081923279137,
      0.0,
      -1.3508188089848066,
      -1.3998791787822982,
      -1.3747264400704053,
      -1.350656312503128,
      -1.3580600249213783,
      -1.358777873898774,
      -1.3377575103243473,
      -1.3551792271020795,
      -1.3235352006800274,
      -1.3403240608482208,
      -1.3541708405194102,
      -1.3867966723285547,
      -1.3076121893253327,
      -1.368391876419962,
      -1.2977214328379174,
      -1.3415819789331913,
      -1.3298285363105522,
      -1.3232974460668918,
      -1.3841788894416938
    ],
    [
      -1.3268876693187495,
      -1.1890839022732953,
      -1.2710189379260366,
      -1.170258511301438,
      -1.2362808367607845,
      -1.2842703649475493,
      -1.294029646663473,
      -1.2703063911768002,
      -1.1378965980676115,
      -1.283679485546693,
      0.0,
      -1.385864561153384,
      -1.199945110058797,
      -1.2315941345859587,
      -1.258196026616139,
      -1.209881886381326,
      -1.261441721764156,
      -1.2087562640366152,
      -1.2532381001106985,
      -1.2776323901942355,
      -1.2732444560996579,
      -1.2653849633794507,
      -1.2682226439746438,
      -1.2536023898206434,
      -1.2777656541216282,
      -1.299964091134718,
      -1.2036535777038087,
      -1.3069866403707422,
      -1.299806437012118
    ],
    [
      -1.2969589695095238,
      -1.2782724433629786,
      -1.2656090645550246,
      -1.287395428795365,
      -1.201968112669791,
      -1.2351411804443762,
      -1.2473071848019606,
      -1.1897465886840535,
      -1.2397214435213473,
      -1.2189887114183666,
      -1.2552018987193458,
      0.0,
      -1.3038411485653756,
      -1.2464195574854184,
      -1.2084082642361087,
      -1.2482552306428578,
      -1.2465473113978707,
      -1.2637479550301018,
      -1.2131421925540966,
      -1.245285101342153,
      -1.266209304453774,
      -1.2251341918770071,
      -1.2347833902498078,
      -1.2435944136500405,
      -1.2600003999713252,
      -1.2093049902011788,
      -1.2856489613812394,
      -1.2768263042599126,
      -1.1987319564833472
    ],
    [
      -1.2953258475992597,
      -1.133134136278603,
      -1.232748390133585,
      -1.1478276759703825,
      -1.1595317751705154,
      -1.2760323376457778,
      -1.2760050371163298,
      -1.1801942546353879,
      -1.0904207820530305,
      -1.2587389149149344,
      -1.1002591530274553,
      -1.3223393213090509,
      0.0,
      -1.1417233546838914,
      -1.2039706177508227,
      -1.1555470613228953,
      -1.194313122694911,
      -1.096313432637815,
      -1.1804655356370157,
      -1.2187810022636447,
      -1.2233607891843552,
      -1.206354852109681,
      -1.2204187522883463,
      -1.202815420174988,
      -1.2628068604791427,
      -1.2290750917856421,
      -1.1418195159537678,
      -1.228569524010614,
      -1.2793048472042468
    ],
    [
      -1.1825106334484141,
      -1.0549930256853717,
      -1.1215937709244983,
      -1.0223775943495033,
      -1.0977652277890597,
      -1.0710253876681228,
      -1.1384183928427907,
      -1.0961918808140292,
      -1.0280533874053706,
      -1.081870876748203,
      -1.1070351825959894,
      -1.173664679273967,
      -1.0871609117424645,
      0.0,
      -1.0374348117606647,
      -1.060602217565309,
      -1.0836813700057313,
      -1.0525448865618372,
      -1.0887878734265286,
      -1.1098392239611206,
      -1.0964837926649411,
      -1.1203615682287682,
      -1.1187459084548217,
      -1.0985957104415032,
      -1.1240893975877335,
      -1.120911883075425,
      -1.114343331161808,
      -1.0578027893689381,
      -1.1447023916397865
    ],
    [
      -1.3310790548346492,
      -1.2652965273728067,
      -1.3124740136379287,
      -1.2854098754610128,
      -1.230145731302685,
      -1.273994790195079,
      -1.3557485472178494,
      -1.2814649027881777,
      -1.1917860637155258,
      -1.2691469917490341,
      -1.2504960321518765,
      -1.3135790762391113,
      -1.258320078386846,
      -1.2402426692638608,
      0.0,
      -1.2806425416062468,
      -1.2820830959394864,
      -1.222637168668797,
      -1.3110054836558236,
      -1.2432523082147,
      -1.2981386868360218,
      -1.2461008759480312,
      -1.2668327079593957,
      -1.2679515371213392,
      -1.303939788593541,
      -1.2123943542504256,
      -1.259437012903724,
      -1.2756337010365237,
      -1.2291550752464298
    ],
    [
      -1.4456769681367108,
      -1.341307972145081,
      -1.3617226860400153,
      -1.34297297191705,
      -1.3632181182675238,
      -1.417712486913708,
      -1.4592688615151688,
      -1.4362197327712327,
      -1.3482328812829307,
      -1.4372394722449988,
      -1.314719840842159,
      -1.4986970443830796,
      -1.3779251199818576,
      -1.3613283291910918,
      -1.422619243932137,
      0.0,
      -1.4134518726667822,
      -1.3877700319107598,
      -1.2863943958802282,
      -1.3948302756198514,
      -1.4465079454690604,
      -1.4320385528892075,
      -1.477324283784676,
      -1.3423179474232458,
      -1.4243087662230816,
      -1.4105829263447283,
      -1.3502209896222446,
      -1.471402781546626,
      -1.465356557670989
    ],
    [
      -1.6141708219832451,
      -1.5338905588087515,
      -1.471537279614317,
      -1.5224630276375144,
      -1.515986294654845,
      -1.6099592622511578,
      -1.6354790014592222,
      -1.569098197342606,
      -1.5146275755551215,
      -1.5470450854655837,
      -1.516269777972952,
      -1.6860646144128473,
      -1.5381616002001863,
      -1.4871507616985054,
      -1.5963211159521253,
      -1.5197070829849104,
      0.0,
      -1.4922546916550077,
      -1.553530467103828,
      -1.4493635014248893,
      -1.5465791057040086,
      -1.579887333093333,
      -1.548529574699766,
      -1.5184921115699674,
      -1.497107024507311,
      -1.5709169684298274,
      -1.474471584976495,
      -1.5383063174859153,
      -1.7000487880340198
    ],
    [
      -1.2974350233117118,
      -1.182228898781827,
      -1.2147288022101579,
      -1.19208139757517,
      -1.1928966292029846,
      -1.212335783371554,
      -1.253263040202678,
      -1.2112039952924643,
      -1.0876233677312743,
      -1.2421473799632945,
      -1.1476653566622572,
      -1.3057597601397977,
      -1.1513541314728801,
      -1.1546281723884657,
      -1.2187046966041069,
      -1.2235357387915708,
      -1.172771539206914,
      0.0,
      -1.1771680391711956,
      -1.1628734331508819,
      -1.119523171624754,
      -1.2277550120735823,
      -1.22653589303405,
      -1.1920771631568239,
      -1.231354273674409,
      -1.281941457108176,
      -1.183189960022073,
      -1.161394200903973,
      -1.2932279514971592
    ],
    [
      -1.5513802717823613,
      -1.4336780173796955,
      -1.443939387010484,
      -1.4639957314538883,
      -1.4526636533577133,
      -1.5377457539144441,
      -1.5338442404913855,
      -1.4641686462012993,
      -1.4485613803245287,
      -1.519452812562685,
      -1.4643792394998407,
      -1.6186300862937228,
      -1.4685459220783952,
      -1.459668970819593,
      -1.5152436412863013,
      -1.3853819498003896,
      -1.4520514414176526,
      -1.4633867766858064,
      0.0,
      -1.4373115809166124,
      -1.5035865030910953,
      -1.4960567064379107,
      -1.4952470678077585,
      -1.4341460426273505,
      -1.5159205919721606,
      -1.4844889504030478,
      -1.4487320825644932,
      -1.4878626498048368,
      -1.5213405443529229
    ],
    [
      -1.3115356429709923,
      -1.2097519048125214,
      -1.1902264694072113,
      -1.214715426176566,
      -1.2066975871837167,
      -1.3108074223198107,
      -1.2997694529095718,
      -1.2224026017053249,
      -1.1856583919356538,
      -1.2497454026054702,
      -1.240969362904463,
      -1.3556545792200878,
      -1.2547122712369156,
      -1.1824563291062031,
      -1.2406097271272374,
      -1.1913592427256305,
      -1.2301409075516003,
      -1.2401061362385,
      -1.2325719382254214,
      0.0,
      -1.238844948991702,
      -1.2459669565611093,
      -1.211959756018329,
      -1.2189640115171207,
      -1.2405071117884117,
      -1.2579137479838427,
      -1.2145457697947104,
      -1.2145938486447716,
      -1.2933807454696948
    ],
    [
      -1.6622815982944312,
      -1.53284266471904,
      -1.536514210367803,
      -1.5622642939592597,
      -1.5644611851588903,
      -1.6539821383006552,
      -1.6134860040105679,
      -1.5676132948577048,
      -1.5411678230541899,
      -1.5705693646272891,
      -1.5258154052847426,
      -1.6633612013777128,
      -1.5612522678671905,
      -1.5258416058592634,
      -1.6276075233379756,
      -1.5501578762034376,
      -1.5188989792960002,
      -1.4862027030863816,
      -1.540205738355392,
      -1.53540001984519,
      0.0,
      -1.5630217163769753,
      -1.5485586309464834,
      -1.5375333220599317,
      -1.5319794829690063,
      -1.6026699509967113,
      -1.5118662245554593,
      -1.5596301117247344,
      -1.639977398428959
    ],
    [
      -1.3454562929039349,
      -1.2951008816619034,
      -1.2880457096477926,
      -1.3192134437070953,
      -1.2767437763936331,
      -1.3371350546593066,
      -1.301375185300461,
      -1.3006188992208445,
      -1.2809287456270715,
      -1.2847854239598928,
      -1.2735306672523046,
      -1.2928051814933221,
      -1.2683066800698264,
      -1.2229942557222924,
      -1.288863512029535,
      -1.303453872067985,
      -1.2586675250468384,
      -1.267072842643377,
      -1.3038711683766484,
      -1.2757353573417143,
      -1.2703541112772925,
      0.0,
      -1.3187271412668566,
      -1.292466318359648,
      -1.284009505146307,
      -1.285078980296048,
      -1.2585742687037096,
      -1.2471396387326694,
      -1.2185590373270674
    ],
    [
      -1.6020656021735653,
      -1.5018832008272525,
      -1.457975052221054,
      -1.4926754901667854,
      -1.4367342083425751,
      -1.5441999897926229,
      -1.534411136117568,
      -1.4612371640672173,
      -1.483616874848445,
      -1.4523772281596254,
      -1.4653131457635473,
      -1.5722997815014466,
      -1.548193561947441,
      -1.5108020315489012,
      -1.494758263900933,
      -1.4745441639549655,
      -1.4679704270356126,
      -1.5174782161302742,
      -1.4920629638602072,
      -1.4068234442637835,
      -1.5165441506961646,
      -1.503309198972791,
      0.0,
      -1.5117216617797031,
      -1.4131123098558573,
      -1.5618698489517893,
      -1.4531241361429823,
      -1.5596936120130152,
      -1.5358134633898848
    ],
    [
      -1.441402173188402,
      -1.295394249925528,
      -1.3529075407856543,
      -1.3631816308953486,
      -1.3140008034323003,
      -1.3743544327153088,
      -1.391340398791525,
      -1.4004109569249792,
      -1.3480635060959831,
      -1.3767263291718244,
      -1.3677899650721863,
      -1.451962391009617,
      -1.4000145506439046,
      -1.3277193118584354,
      -1.382945761184168,
      -1.27890016129629,
      -1.3575388416176473,
      -1.3690542262876013,
      -1.3070796004856708,
      -1.3117172748300239,
      -1.361275475058439,
      -1.3878442588737345,
      -1.3928865763748284,
      0.0,
      -1.3574609081437277,
      -1.3678614187244975,
      -1.3430052359945996,
      -1.3802492827050032,
      -1.3399297039230285
    ],
    [
      -1.523885128843597,
      -1.443087147882786,
      -1.410220863000372,
      -1.4470138876921403,
      -1.327164004016551,
      -1.5486419510769505,
      -1.5318019687781799,
      -1.4317570905334056,
      -1.3745015351242493,
      -1.3404020419331644,
      -1.4475687527981056,
      -1.5539515058569864,
      -1.4279755825370932,
      -1.4205366607558227,
      -1.4903030177663492,
      -1.4092076592248397,
      -1.337931520194162,
      -1.4532841260588703,
      -1.4069592606722392,
      -1.3786401849971555,
      -1.419056874180431,
      -1.459707156796436,
      -1.372642209455756,
      -1.4310835949523404,
      0.0,
      -1.457420849337514,
      -1.3774778617579782,
      -1.4062447927753787,
      -1.516405898182003
    ],
    [
      -1.508863102423952,
      -1.4177824676148363,
      -1.457592309998187,
      -1.4274076286009842,
      -1.3956146389935011,
      -1.468976789415477,
      -1.5062261738464084,
      -1.461112834877387,
      -1.4673938009071266,
      -1.4116524301886229,
      -1.4321498837219904,
      -1.51422716575132,
      -1.4720932009618857,
      -1.446991423760265,
      -1.4164684254148634,
      -1.4119011931001288,
      -1.4570185270869127,
      -1.4909811757285185,
      -1.4212410447544108,
      -1.4516206766183852,
      -1.515702811067314,
      -1.4668146515605631,
      -1.504971330924712,
      -1.4753879548908952,
      -1.4782419305757701,
      0.0,
      -1.4508043471606433,
      -1.4866050867776275,
      -1.4277915529537264
    ],
    [
      -1.401065124790409,
      -1.2756904334994954,
      -1.2816928534960506,
      -1.3343735848983833,
      -1.2660516410550025,
      -1.383943044413814,
      -1.4100865557888678,
      -1.350144189616268,
      -1.2995129197883748,
      -1.3418650353853787,
      -1.2830867177951069,
      -1.4420591343952067,
      -1.3201588241838906,
      -1.3479701156604786,
      -1.408366032427881,
      -1.2763025546813225,
      -1.2620511910926853,
      -1.3044040588342878,
      -1.2696424904569805,
      -1.3060640245633262,
      -1.2956521200829847,
      -1.335264517362814,
      -1.3025048855399448,
      -1.3295857929540116,
      -1.3124764991573823,
      -1.3735601250013805,
      0.0,
      -1.3863280995987268,
      -1.4016822459686682
    ],
    [
      -1.2775373615735373,
      -1.1922867870016496,
      -1.2226529988640913,
      -1.2029698241602702,
      -1.1698212216609558,
      -1.239340794154602,
      -1.2833838346219255,
      -1.1869788292039052,
      -1.1466216870829424,
      -1.1799704148955896,
      -1.2152441993461334,
      -1.300872958584479,
      -1.1921627730474702,
      -1.194009566683004,
      -1.234359758360042,
      -1.1928905031009371,
      -1.2257728621442043,
      -1.1432515005189494,
      -1.1898193745058812,
      -1.2112498032233325,
      -1.1844574908157945,
      -1.1726508952618915,
      -1.2027663207483221,
      -1.1716443011991013,
      -1.21412197255518,
      -1.2271228499299591,
      -1.203459949491937,
      0.0,
      -1.255787283067189
    ],
    [
      -1.4388543650905807,
      -1.363137585333146,
      -1.4176019942346827,
      -1.3626349520044032,
      -1.3106773511571674,
      -1.3392935255063658,
      -1.38576148482382,
      -1.3319780882783536,
      -1.3831301042890007,
      -1.3364970568837844,
      -1.358389228515049,
      -1.3617242845861115,
      -1.3760440331373536,
      -1.3357513827865959,
      -1.295720811418594,
      -1.3326259957362605,
      -1.3919555667623258,
      -1.3629986879525047,
      -1.315104828425353,
      -1.3489352092291949,
      -1.3722768818079338,
      -1.279346073528673,
      -1.3575171644422692,
      -1.31525376868646,
      -1.3616242899502153,
      -1.3138502021633258,
      -1.3670273425455646,
      -1.368619915682064,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.15017153395458793,
      0.14094019856237772,
      0.14578693914823315,
      0.15148367262210316,
      0.12580452542108933,
      0.1119861486903142,
      0.1304962400090517,
      0.1723226702312326,
      0.137932137286493,
      0.17043812671733694,
      0.11113842107415461,
      0.16672940950283444,
      0.16687792496552234,
      0.1288881022222519,
      0.1508090225627896,
      0.15380818487607684,
      0.1415794914100439,
      0.1360970081887367,
      0.14458657924413343,
      0.13590050666929843,
      0.14182223477084,
      0.1094163087881932,
      0.14466245662024013,
      0.13205794962186745,
      0.13940406706577635,
      0.14684496460673158,
      0.13893334924248713,
      0.11631236324245764
    ],
    [
      0.25594939641576353,
      0.0,
      0.42894654718601255,
      0.4290501577592045,
      0.4165897414601576,
      0.3504405681425917,
      0.34899503542534593,
      0.3899370531756421,
      0.4689515720256625,
      0.37038375242057153,
      0.4487013211988711,
      0.27899749762382786,
      0.49561493651082444,
      0.3944975614693602,
      0.4103927688446498,
      0.44149019700752157,
      0.36687126711420026,
      0.44447108844132055,
      0.45136242107390867,
      0.40824442212646916,
      0.37721046641696954,
      0.3959273765502196,
      0.3879705743646906,
      0.44658038421548585,
      0.31632147635959984,
      0.36825912027652863,
      0.4518905630551344,
      0.3975184288935665,
      0.3287151623279998
    ],
    [
      0.2878273675620311,
      0.4704294068396626,
      0.0,
      0.43948983417978416,
      0.4397637166808377,
      0.3528748436645115,
      0.3259212927779698,
      0.4144926114981595,
      0.4720522313011557,
      0.3955043580273958,
      0.41424171680936217,
      0.2743991723456545,
      0.44510443001361577,
      0.41238642777067636,
      0.37775938304848644,
      0.4440571565728515,
      0.43692253323748464,
      0.4402348078825864,
      0.4231545656079534,
      0.43471070293800196,
      0.38010717031825236,
      0.3925997429067065,
      0.3727141081508776,
      0.4344565752800047,
      0.3878942054048107,
      0.34294843808350217,
      0.47057202548335253,
      0.38738607055932284,
      0.3391141674229332
    ],
    [
      0.22658825619272416,
      0.40208479353555404,
      0.35763659586759555,
      0.0,
      0.3634927674089157,
      0.3015882351540362,
      0.3077126031284805,
      0.3605670421100293,
      0.3844610974438396,
      0.35148744289363587,
      0.4055001429647076,
      0.25481188030883306,
      0.36580457007643674,
      0.424753131754104,
      0.31470069234881093,
      0.35516333301936487,
      0.3445662436625567,
      0.3529268872084017,
      0.3757635083544151,
      0.3538871480760326,
      0.3392885370818537,
      0.3237906807687707,
      0.3377655459450717,
      0.3594152344379653,
      0.34681427862675607,
      0.3366703409504941,
      0.3515384170842746,
      0.33837166681280473,
      0.2913095313678966
    ],
    [
      0.30600237179814216,
      0.36265986749636636,
      0.36217039238272153,
      0.3659258470187605,
      0.0,
      0.36345859610695586,
      0.30782863663681215,
      0.42049115315079866,
      0.37612437699075807,
      0.4213525725953817,
      0.3785722787011454,
      0.3184592561006083,
      0.3495244375871911,
      0.3948769910406784,
      0.3806018364180648,
      0.39495765002976846,
      0.4200531987710876,
      0.34614074700468955,
      0.4040508373568781,
      0.41350489277419,
      0.3518560509495947,
      0.3452068163295674,
      0.4302683168010355,
      0.3693197147744134,
      0.3787855431665237,
      0.38516468601181675,
      0.407951608561252,
      0.32493208935151885,
      0.3569327945931484
    ],
    [
      0.28476975678900884,
      0.4133162525327476,
      0.37644317136688454,
      0.38003602148137605,
      0.4590774947163925,
      0.0,
      0.36483233336420917,
      0.3804205501022353,
      0.4031611285530894,
      0.4026526610036707,
      0.44773609474476284,
      0.33662332437298703,
      0.38071318477880034,
      0.45283318397320427,
      0.4384647204755594,
      0.4515801253016183,
      0.3788797632539258,
      0.39510568048573846,
      0.39656697706669264,
      0.3851069580281472,
      0.3692561192706485,
      0.3783844597827484,
      0.3771171971232663,
      0.39163712741982515,
      0.3086534445211817,
      0.4023545613078563,
      0.41626803663172884,
      0.3812367701158199,
      0.406374794916619
    ],
    [
      0.29882395931982786,
      0.4017475500509107,
      0.359139954834945,
      0.38546697715284406,
      0.37379868056574916,
      0.3670804055867547,
      0.0,
      0.3516927664077334,
      0.3847745872180277,
      0.351497189458273,
      0.4273140379514966,
      0.33038368167873733,
      0.35873307942765664,
      0.447081916992524,
      0.3509988256668244,
      0.43433669463521296,
      0.39798683843148575,
      0.39746986476984736,
      0.40515510577449176,
      0.400323173259449,
      0.38137772486573973,
      0.3967596793878774,
      0.37679775386289793,
      0.40891660271778685,
      0.3769679964983206,
      0.35384668722825263,
      0.38171021404494665,
      0.3387676016944987,
      0.3472761516221621
    ],
    [
      0.2684977702510436,
      0.3579245742005719,
      0.391221359496972,
      0.36645146403343287,
      0.42877095633566475,
      0.34141240679563256,
      0.2887643274382925,
      0.0,
      0.3647731048388583,
      0.3811018770743695,
      0.36159067315461413,
      0.30366161627035315,
      0.372088535837658,
      0.33292610218430774,
      0.3790046034577401,
      0.33622742068597544,
      0.33870123240314,
      0.3899996338868166,
      0.36907157093733245,
      0.3688421055805773,
      0.3164873165146189,
      0.3452607214755936,
      0.36356805575183504,
      0.3467595291861796,
      0.31198170881314313,
      0.3647904639811248,
      0.3438122697439312,
      0.34343222153404174,
      0.33619799974983144
    ],
    [
      0.3017661733586485,
      0.5114491669703041,
      0.48253216609360416,
      0.5126117848642804,
      0.4976171552558619,
      0.40860668791513155,
      0.3639599389025423,
      0.4714603150701293,
      0.0,
      0.42690468057387787,
      0.5506003483326012,
      0.33790234703413624,
      0.5315691717611701,
      0.5343893647360674,
      0.5189660182808469,
      0.48038181576457717,
      0.4336997450762792,
      0.5100182795755108,
      0.47717196621772784,
      0.48652772233139285,
      0.41778785056546996,
      0.41913258262950026,
      0.44722773167009344,
      0.45629608440956604,
      0.417243523634941,
      0.3989387480021658,
      0.4968965432871766,
      0.4765885045114606,
      0.38034654891280417
    ],
    [
      0.2574831493505607,
      0.3562937613210413,
      0.37002126854730477,
      0.346238954086324,
      0.4468069494122402,
      0.3384888748404442,
      0.26550491649503116,
      0.3836664557683094,
      0.3864933720473751,
      0.0,
      0.34649379585393847,
      0.29743342605644685,
      0.3225861647683397,
      0.3466562923356171,
      0.3392525799173667,
      0.3385347309399711,
      0.3595550945143977,
      0.3421333777366655,
      0.37377740415871763,
      0.35698854399052427,
      0.34314176431933485,
      0.3105159325101903,
      0.3897004155134123,
      0.32892072841878295,
      0.3995911720008276,
      0.3557306259055537,
      0.3674840685281928,
      0.3740151587718532,
      0.31313371539705126
    ],
    [
      0.31616229658678496,
      0.4539660636322391,
      0.37203102797949783,
      0.47279145460409633,
      0.40676912914474994,
      0.3587796009579851,
      0.34902031924206134,
      0.3727435747287342,
      0.505153367837923,
      0.35937048035884134,
      0.0,
      0.25718540475215046,
      0.4431048558467374,
      0.41145583131957575,
      0.3848539392893955,
      0.4331680795242083,
      0.3816082441413784,
      0.43429370186891925,
      0.38981186579483595,
      0.36541757571129896,
      0.36980550980587656,
      0.3776650025260837,
      0.37482732193089063,
      0.389447576084891,
      0.3652843117839062,
      0.34308587477081653,
      0.4393963882017258,
      0.33606332553479223,
      0.34324352889341636
    ],
    [
      0.25870079731120144,
      0.27738732345774664,
      0.2900507022657006,
      0.2682643380253602,
      0.3536916541509343,
      0.3205185863763491,
      0.3083525820187647,
      0.3659131781366718,
      0.315938323299378,
      0.33667105540235864,
      0.3004578681013794,
      0.0,
      0.2518186182553497,
      0.3092402093353068,
      0.34725150258461657,
      0.30740453617786745,
      0.3091124554228546,
      0.2919118117906234,
      0.34251757426662865,
      0.3103746654785722,
      0.28945046236695116,
      0.33052557494371815,
      0.3208763765709175,
      0.31206535317068473,
      0.2956593668494001,
      0.3463547766195465,
      0.27001080543948586,
      0.27883346256081265,
      0.3569278103373781
    ],
    [
      0.31566919385135783,
      0.4778609051720146,
      0.3782466513170326,
      0.46316736548023507,
      0.45146326628010214,
      0.3349627038048397,
      0.3349900043342877,
      0.4308007868152297,
      0.5205742593975871,
      0.35225612653568317,
      0.5107358884231623,
      0.2886557201415667,
      0.0,
      0.46927168676672615,
      0.40702442369979486,
      0.45544798012772225,
      0.4166819187557065,
      0.5146816088128026,
      0.43052950581360183,
      0.39221403918697284,
      0.38763425226626236,
      0.4046401893409366,
      0.3905762891622713,
      0.40817962127562946,
      0.34818818097147486,
      0.38191994966497544,
      0.4691755254968497,
      0.3824255174400035,
      0.3316901942463708
    ],
    [
      0.30133788602099987,
      0.4288554937840423,
      0.3622547485449157,
      0.4614709251199107,
      0.38608329168035427,
      0.41282313180129115,
      0.3454301266266233,
      0.3876566386553848,
      0.45579513206404343,
      0.40197764272121095,
      0.37681333687342455,
      0.3101838401954471,
      0.39668760772694944,
      0.0,
      0.44641370770874933,
      0.42324630190410506,
      0.4001671494636827,
      0.4313036329075768,
      0.3950606460428854,
      0.3740092955082934,
      0.38736472680447287,
      0.3634869512406458,
      0.3651026110145923,
      0.3852528090279108,
      0.35975912188168047,
      0.362936636393989,
      0.3695051883076059,
      0.42604573010047586,
      0.3391461278296275
    ],
    [
      0.30172094783344106,
      0.3675034752952835,
      0.3203259890301615,
      0.3473901272070774,
      0.4026542713654053,
      0.3588052124730112,
      0.2770514554502408,
      0.3513350998799125,
      0.44101393895256447,
      0.3636530109190561,
      0.3823039705162137,
      0.3192209264289789,
      0.3744799242812442,
      0.3925573334042294,
      0.0,
      0.3521574610618434,
      0.3507169067286038,
      0.4101628339992933,
      0.32179451901226663,
      0.3895476944533902,
      0.3346613158320684,
      0.386699126720059,
      0.3659672947086945,
      0.364848465546751,
      0.32886021407454913,
      0.4204056484176646,
      0.3733629897643662,
      0.3571663016315665,
      0.40364492742166047
    ],
    [
      0.40237660533004105,
      0.5067456013216709,
      0.48633088742673647,
      0.5050806015497018,
      0.484835455199228,
      0.4303410865530437,
      0.38878471195158304,
      0.4118338406955191,
      0.4998206921838211,
      0.410814101221753,
      0.5333337326245928,
      0.3493565290836722,
      0.4701284534848942,
      0.48672524427566,
      0.4254343295346148,
      0.0,
      0.4346017007999696,
      0.46028354155599205,
      0.5616591775865236,
      0.4532232978469004,
      0.40154562799769145,
      0.41601502057754436,
      0.37072928968207575,
      0.505735626043506,
      0.42374480724367025,
      0.43747064712202355,
      0.49783258384450724,
      0.37665079192012585,
      0.38269701579576276
    ],
    [
      0.35467834584754643,
      0.4349586090220401,
      0.49731188821647465,
      0.44638614019327716,
      0.45286287317594653,
      0.35888990557963374,
      0.33337016637156935,
      0.39975097048818564,
      0.45422159227567005,
      0.42180408236520783,
      0.45257938985783963,
      0.2827845534179443,
      0.43068756763060523,
      0.4816984061322862,
      0.3725280518786662,
      0.44914208484588114,
      0.0,
      0.4765944761757839,
      0.41531870072696364,
      0.5194856664059022,
      0.422270062126783,
      0.3889618347374586,
      0.4203195931310255,
      0.4503570562608241,
      0.4717421433234805,
      0.39793219940096414,
      0.49437758285429645,
      0.43054285034487627,
      0.26880037979677174
    ],
    [
      0.31323479294667544,
      0.4284409174765602,
      0.39594101404822934,
      0.41858841868321717,
      0.4177731870554027,
      0.3983340328868332,
      0.35740677605570914,
      0.399465820965923,
      0.5230464485271129,
      0.36852243629509274,
      0.46300445959613,
      0.30491005611858957,
      0.4593156847855071,
      0.4560416438699215,
      0.3919651196542804,
      0.3871340774668164,
      0.4378982770514732,
      0.0,
      0.4335017770871916,
      0.44779638310750536,
      0.4911466446336332,
      0.38291480418480495,
      0.38413392322433726,
      0.41859265310156335,
      0.37931554258397826,
      0.32872835915021126,
      0.42747985623631424,
      0.4492756153544142,
      0.31744186476122804
    ],
    [
      0.32571681184581913,
      0.4434190662484849,
      0.43315769661769643,
      0.4131013521742921,
      0.4244334302704671,
      0.33935132971373627,
      0.34325284313679494,
      0.4129284374268811,
      0.42853570330365165,
      0.35764427106549546,
      0.4127178441283397,
      0.25846699733445755,
      0.4085511615497852,
      0.4174281128085875,
      0.3618534423418791,
      0.49171513382779075,
      0.42504564221052776,
      0.413710306942374,
      0.0,
      0.439785502711568,
      0.3735105805370851,
      0.38104037719026973,
      0.3818500158204219,
      0.44295104100082994,
      0.36117649165601984,
      0.39260813322513255,
      0.42836500106368725,
      0.38923443382334355,
      0.3557565392752575
    ],
    [
      0.30562983807963073,
      0.4074135762381017,
      0.42693901164341175,
      0.40245005487405705,
      0.4104678938669064,
      0.30635805873081234,
      0.3173960281410513,
      0.3947628793452982,
      0.43150708911496927,
      0.3674200784451529,
      0.37619611814616016,
      0.26151090183053527,
      0.36245320981370743,
      0.43470915194441995,
      0.3765557539233857,
      0.42580623832499254,
      0.38702457349902275,
      0.3770593448121231,
      0.38459354282520164,
      0.0,
      0.37832053205892113,
      0.37119852448951374,
      0.405205725032294,
      0.39820146953350233,
      0.3766583692622114,
      0.3592517330667804,
      0.40261971125591267,
      0.4025716324058515,
      0.32378473558092824
    ],
    [
      0.2802069513828156,
      0.40964588495820675,
      0.40597433930944393,
      0.3802242557179871,
      0.3780273645183565,
      0.2885064113765916,
      0.32900254566667897,
      0.374875254819542,
      0.40132072662305696,
      0.3719191850499577,
      0.41667314439250425,
      0.279127348299534,
      0.3812362818100563,
      0.4166469438179834,
      0.3148810263392712,
      0.39233067347380923,
      0.42358957038124667,
      0.45628584659086524,
      0.40228281132185484,
      0.4070885298320568,
      0.0,
      0.37946683330027153,
      0.39392991873076344,
      0.40495522761731517,
      0.4105090667082405,
      0.33981859868053554,
      0.43062232512178755,
      0.3828584379525124,
      0.30251115124828787
    ],
    [
      0.2909030956073215,
      0.341258506849353,
      0.34831367886346376,
      0.31714594480416114,
      0.3596156121176233,
      0.2992243338519498,
      0.3349842032107955,
      0.3357404892904119,
      0.3554306428841849,
      0.3515739645513636,
      0.3628287212589518,
      0.34355420701793427,
      0.36805270844143,
      0.41336513278896403,
      0.3474958764817213,
      0.3329055164432715,
      0.37769186346441797,
      0.36928654586787935,
      0.332488220134608,
      0.3606240311695421,
      0.3660052772339639,
      0.0,
      0.31763224724439976,
      0.3438930701516083,
      0.35234988336494943,
      0.35128040821520834,
      0.3777851198075468,
      0.389219749778587,
      0.417800351184189
    ],
    [
      0.2927766578270077,
      0.3929590591733205,
      0.43686720777951904,
      0.40216676983378763,
      0.45810805165799784,
      0.35064227020795014,
      0.3604311238830049,
      0.4336050959333557,
      0.4112253851521279,
      0.44246503184094754,
      0.42952911423702567,
      0.32254247849912643,
      0.34664869805313203,
      0.38404022845167174,
      0.40008399609964007,
      0.4202980960456075,
      0.42687183296496034,
      0.37736404387029876,
      0.4027792961403658,
      0.4880188157367895,
      0.37829810930440844,
      0.39153306102778207,
      0.0,
      0.38312059822086986,
      0.48172995014471565,
      0.3329724110487837,
      0.4417181238575907,
      0.33514864798755784,
      0.3590287966106882
    ],
    [
      0.3034618302191645,
      0.44946975348203844,
      0.3919564626219121,
      0.3816823725122178,
      0.4308631999752661,
      0.37050957069225765,
      0.3535236046160415,
      0.34445304648258723,
      0.3968004973115833,
      0.36813767423574206,
      0.3770740383353801,
      0.2929016123979493,
      0.34484945276366186,
      0.41714469154913103,
      0.3619182422233984,
      0.4659638421112764,
      0.38732516178991916,
      0.3758097771199651,
      0.4377844029218956,
      0.43314672857754255,
      0.3835885283491274,
      0.3570197445338319,
      0.35197742703273804,
      0.0,
      0.38740309526383876,
      0.3770025846830689,
      0.4018587674129668,
      0.3646147207025632,
      0.40493429948453796
    ],
    [
      0.3099496077759718,
      0.39074758873678284,
      0.4236138736191968,
      0.3868208489274285,
      0.5066707326030178,
      0.2851927855426184,
      0.302032767841389,
      0.40207764608616325,
      0.45933320149531953,
      0.4934326946864045,
      0.38626598382146327,
      0.2798832307625825,
      0.40585915408247564,
      0.4132980758637461,
      0.34353171885321965,
      0.42462707739472916,
      0.4959032164254069,
      0.3805506105606986,
      0.42687547594732966,
      0.4551945516224134,
      0.41477786243913783,
      0.37412757982313294,
      0.46119252716381287,
      0.4027511416672285,
      0.0,
      0.3764138872820548,
      0.4563568748615907,
      0.42758994384419013,
      0.31742883843756586
    ],
    [
      0.3093762497232002,
      0.400456884532316,
      0.36064704214896537,
      0.390831723546168,
      0.42262471315365113,
      0.3492625627316752,
      0.3120131783007438,
      0.3571265172697653,
      0.3508455512400257,
      0.4065869219585294,
      0.3860894684251619,
      0.3040121863958323,
      0.3461461511852666,
      0.37124792838688725,
      0.40177092673228887,
      0.4063381590470234,
      0.36122082506023956,
      0.32725817641863375,
      0.39699830739274145,
      0.36661867552876704,
      0.3025365410798382,
      0.35142470058658914,
      0.31326802122244035,
      0.342851397256257,
      0.33999742157138213,
      0.0,
      0.36743500498650894,
      0.3316342653695248,
      0.39044779919342587
    ],
    [
      0.41452468122024144,
      0.5398993725111549,
      0.5338969525145998,
      0.48121622111226703,
      0.5495381649556479,
      0.43164676159683624,
      0.4055032502217826,
      0.4654456163943823,
      0.5160768862222755,
      0.47372477062527163,
      0.5325030882155435,
      0.3735306716154436,
      0.4954309818267597,
      0.4676196903501717,
      0.4072237735827693,
      0.5392872513293279,
      0.5535386149179651,
      0.5111857471763626,
      0.5459473155536698,
      0.5095257814473242,
      0.5199376859276656,
      0.4803252886478364,
      0.5130849204707055,
      0.48600401305663876,
      0.503113306853268,
      0.44202968100926987,
      0.0,
      0.4292617064119235,
      0.41390756004198215
    ],
    [
      0.26665306454909943,
      0.3519036391209871,
      0.3215374272585454,
      0.3412206019623665,
      0.37436920446168087,
      0.3048496319680347,
      0.26080659150071117,
      0.3572115969187315,
      0.39756873903969425,
      0.36422001122704706,
      0.3289462267765033,
      0.24331746753815775,
      0.3520276530751665,
      0.35018085943963273,
      0.3098306677625946,
      0.35129992302169954,
      0.31841756397843235,
      0.40093892560368727,
      0.35437105161675553,
      0.33294062289930415,
      0.35973293530684214,
      0.3715395308607452,
      0.34142410537431456,
      0.37254612492353534,
      0.33006845356745673,
      0.31706757619267756,
      0.3407304766306998,
      0.0,
      0.2884031430554477
    ],
    [
      0.24259621234211148,
      0.3183129920995462,
      0.2638485831980095,
      0.318815625428289,
      0.3707732262755248,
      0.3421570519263264,
      0.2956890926088722,
      0.3494724891543386,
      0.2983204731436915,
      0.3449535205489078,
      0.32306134891764327,
      0.3197262928465807,
      0.30540654429533864,
      0.34569919464609633,
      0.38572976601409814,
      0.3488245816964317,
      0.2894950106703664,
      0.31845188948018754,
      0.3663457490073392,
      0.33251536820349736,
      0.3091736956247584,
      0.4021045039040192,
      0.323933412990423,
      0.36619680874623217,
      0.3198262874824769,
      0.3676003752693664,
      0.31442323488712765,
      0.31283066175062824,
      0.0
    ]
  ],
  "row_avgs": [
    0.14082966204704483,
    0.39536717349578926,
    0.4003256807988551,
    0.34387359302087006,
    0.37275619858928105,
    0.39141435333859803,
    0.37807948932518837,
    0.3504757722004876,
    0.45530689163349525,
    0.34845152476806474,
    0.3859466304590647,
    0.3095100632398772,
    0.40891656266361426,
    0.38772037278396043,
    0.36285754937177145,
    0.44693325001617235,
    0.42072704187799653,
    0.4064768066753092,
    0.39490384640180987,
    0.3776452062958877,
    0.37694702339434044,
    0.35208747864570716,
    0.3958204625567869,
    0.3826133974786287,
    0.40008926779168114,
    0.35953811787299467,
    0.48339034842175316,
    0.3358615648439482,
    0.3284387140413653
  ],
  "col_avgs": [
    0.2997637166906494,
    0.4016886292862013,
    0.3827963156693547,
    0.3917811829100049,
    0.41496520915593516,
    0.34253250615710445,
    0.3212338072870608,
    0.3803722559563966,
    0.41341581395423865,
    0.378427276120989,
    0.40365365282415205,
    0.2940957516979365,
    0.38326259389902123,
    0.40520175937046643,
    0.37233484983517806,
    0.40123697001228764,
    0.3859983796095289,
    0.39597188142698886,
    0.3983154037117683,
    0.39750890977773423,
    0.36722049488097397,
    0.3700031741338306,
    0.37102060815994614,
    0.3880326603630725,
    0.3647034754726669,
    0.3615352578223622,
    0.3977865811091891,
    0.3679696305857543,
    0.3404752961695511
  ],
  "combined_avgs": [
    0.2202966893688471,
    0.39852790139099525,
    0.3915609982341049,
    0.36782738796543746,
    0.3938607038726081,
    0.36697342974785124,
    0.3496566483061246,
    0.3654240140784421,
    0.43436135279386695,
    0.36343940044452683,
    0.39480014164160837,
    0.3018029074689068,
    0.39608957828131774,
    0.39646106607721343,
    0.3675961996034748,
    0.42408511001423,
    0.40336271074376273,
    0.40122434405114904,
    0.3966096250567891,
    0.387577058036811,
    0.37208375913765723,
    0.3610453263897689,
    0.3834205353583665,
    0.3853230289208506,
    0.382396371632174,
    0.36053668784767845,
    0.4405884647654711,
    0.35191559771485126,
    0.33445700510545817
  ],
  "gppm": [
    597.2636724381481,
    612.3871559568126,
    620.7108546917771,
    619.1179255613163,
    603.1958040178487,
    636.9384864308482,
    647.7262151904877,
    618.5909166355033,
    608.7413814435986,
    621.4917796716596,
    610.3197592950904,
    660.0178689230019,
    621.4406120404776,
    614.0561172250726,
    626.1481575951553,
    611.8825463526539,
    616.9981948351166,
    616.1308903866086,
    609.713910465038,
    614.2948371150065,
    625.3982641716814,
    628.6905729048004,
    623.705423399757,
    618.2462633874409,
    627.8613700591468,
    628.7868731277022,
    615.3526924612142,
    629.0614146329926,
    640.6443228555573
  ],
  "gppm_normalized": [
    1.406984743279678,
    1.5160644631678541,
    1.5402898059759518,
    1.5376039396306602,
    1.494896597019702,
    1.5807986560935878,
    1.6049276942447521,
    1.5418491657945297,
    1.509619119687107,
    1.5438142973948707,
    1.5117038137144623,
    1.651538729971856,
    1.5381696406639604,
    1.521261734856473,
    1.5583799422722742,
    1.5184057936485356,
    1.5316344303202158,
    1.5303471398567776,
    1.5137618972891052,
    1.524767805648102,
    1.5538235917787517,
    1.5607079536640256,
    1.547395434089336,
    1.5382189150778458,
    1.5590875398081667,
    1.5609364535932564,
    1.5250920973647428,
    1.5611381160990874,
    1.5903530170806306
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394
  ],
  "response_lengths": [
    4309,
    2680,
    2582,
    2715,
    2495,
    2434,
    2357,
    2674,
    2218,
    2471,
    2695,
    2594,
    2353,
    2335,
    2309,
    2326,
    2254,
    2437,
    2672,
    2622,
    2254,
    2211,
    2381,
    2377,
    2286,
    2195,
    2126,
    2453,
    2165
  ]
}