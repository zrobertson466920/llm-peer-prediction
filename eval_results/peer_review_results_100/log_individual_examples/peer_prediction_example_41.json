{
  "example_idx": 41,
  "reference": "Published as a conference paper at ICLR 2023\n\nQUANT: QUANTUM ANNEALING WITH LEARNT COUPLINGS\n\nMarcel Seelbach Benkner Universit ̈at Siegen\n\nMaximilian Krahn MPI for Informatics, SIC Aalto University\n\nEdith Tretschk MPI for Informatics, SIC\n\nZorah L ̈ahner & Michael Moeller Universit ̈at Siegen\n\nVladislav Golyanik MPI for Informatics, SIC\n\nABSTRACT\n\nModern quantum annealers can find high-quality solutions to combinatorial optimisation objectives given as quadratic unconstrained binary optimisation (QUBO) problems. Unfortunately, obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations. Moreover, such explicit formulations impose tangible constraints on solution encodings. In stark contrast to prior work, this paper proposes to learn QUBO forms from data through gradient backpropagation instead of deriving them. As a result, the solution encodings can be chosen flexibly and compactly. Furthermore, our methodology is general and virtually independent of the specifics of the target problem type. We demonstrate the advantages of learnt QUBOs on the diverse problem types of graph matching, 2D point cloud alignment and 3D rotation estimation. Our results are competitive with the previous quantum state of the art while requiring much fewer logical and physical qubits, enabling our method to scale to larger problems. The code and the new dataset are available at https://4dqv.mpi-inf.mpg.de/QuAnt/.\n\n1\n\nINTRODUCTION\n\nHybrid computer vision methods that can be executed partially on a quantum computer (QC) are an emerging research area (Boyda et al., 2017; Cavallaro et al., 2020; Seelbach Benkner et al., 2021; Yurtsever et al., 2022). Compared to classical methods, they promise to solve computationally demanding (e.g., combinatorial) sub-problems faster, with improved scaling, and without relaxations that often lead to approximate solutions. Although quantum primacy has not yet been demonstrated in remotely practical usages of quantum computing, all existing quantum computer vision (QCV) methods fundamentally assume that it will be achieved in the future. Thus, solving these suitable algorithmic parts on a QC has the potential to reshape the field. However, reformulating them for execution on a QC is often non-trivial.\n\nQCV continues building up momentum, fuelled by accessible experimental quantum annealers (QA) allowing to solve practical (N P-hard) optimisation problems. Existing QCV methods using QAs rely on analytically deriving QUBOs (both QUBO matrices and solution encodings) for a specific problem type, which is challenging, especially since solutions need to be encoded as binary vectors (Li & Ghosh, 2020; Seelbach Benkner et al., 2020; 2021; Birdal et al., 2021). This often leads to larger encodings than necessary, severely impacting scalability. Alternatively, QUBO derivations with neural networks are conceivable but have not yet been scrutinised in the QA literature.\n\nIn stark contrast to the state of the art, this paper proposes, for the first time, to learn QUBO forms from data for any problem type using backpropagation (see Fig. 1). Our framework captures, in the weights of a neural network, the entire subset of QUBOs belonging to a problem type; a single forward pass yields the QUBO form for a given problem instance. It is thus a meta-learning approach in the context of hybrid (quantum-classical) neural network training, in which the superordinate network instantiates the parameters of the QUBO form. We find that sampling instantiated QUBOs can be a reasonable alternative to non-quantum neural baselines that regress the solution directly.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: We propose QuAnt for QUBO learning, i.e., a quantum-classical meta-learning algorithm that avoids analytical QUBO derivations by learning to regress QUBOs to solve problems of a given type. We first represent a problem instance as a vector p and then feed it into an MLP that regresses the entries of the QUBO matrix A. We then initialise a quantum annealer with A and use quantum annealing to find a QUBO minimiser and extract it as the solution x∗ to the problem instance. We define losses involving x∗ that avoid backpropagation through the annealing and backpropagate gradients through the MLP to train it. We demonstrate the generalisability of QuAnt on graph matching, point set registration, and rotation estimation.\n\nIn particular, we show how a (combinatorial) quantum annealing solver can be integrated into a vanilla neural network as a custom layer and be used in the forward and backward passes, which may be useful in other contexts. To that end, we introduce a contrastive loss that circumvents the inherently discontinuous and non-differentiable nature of QUBO solvers. Our method is compatible with any QUBO solver at training and test time—we consider parallelised exhaustive search, simulated annealing, and quantum annealing. QUBO learning, i.e., determining a function returning QUBO forms given a problem instance of some problem type as input, is a non-trivial and challenging task. In summary, this paper makes several technical contributions to enable QUBO learning:\n\n1. QuAnt, i.e., a new meta-learning approach to obtain QUBO forms executable on modern QAs for computer vision problems. While prior methods rely on analytical derivations, we learn QUBOs from data (Sec. 3.1).\n\n2. A new training strategy for neural methods with backpropagation involving finding lowenergy solutions to instantaneous (optimised) QUBO forms, independent of the solver (Secs. 3.2 and 3.3).\n\n3. Application of the new framework to several problems with solutions encoded by permu-\n\ntations and discretised rigid transformations (Secs. 3.4 and 3.5).\n\nWe show that our methodology is a standardised way of obtaining QUBOs independent of the target problem type. This paper focuses on three problem types already tackled by QCV methods relying on analytical QUBO derivations: graph matching and point set alignment (with and without known prior point matches in the 3D and 2D cases, respectively). We emphasise that we do not claim to outperform existing specialised methods for these problem types or that QA is particularly wellsuited for them. Rather, we show that this wide variety of problems can be tackled successfully and competitively by our general quantum approach already now, before quantum primacy. Thus, in the future, computer vision methods may readily benefit from the (widely expected) speed-up of QC through an easy and flexible re-formulation of algorithmic parts as QUBOs, thanks to our proposed method. We run our experiments on D-Wave Advantage5.1 (Dattani et al., 2019), an experimental realisation of AQC with remote access. This paper assumes familiarity with the basics of quantum computing. For convenience, we summarise several relevant definitions in the Appendix.\n\n2 RELATED WORK\n\nThe two main paradigms for quantum computing are gate-based QC and adiabatic quantum computing (AQC). Our method uses quantum annealing, which is derived from AQC, and is not gate-based. The predominantly theoretical field of quantum machine learning (QML) investigates how quantum computations can be integrated into machine learning (Biamonte et al., 2016; Dunjko & Briegel, 2018; Sim et al., 2019; Havl ́ıˇcek et al., 2019; Du et al., 2020; Mariella & Simonetto, 2021; K ̈ubler et al., 2021). Many QML methods assume gate-based quantum computers and define a quantum\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nvariational layer, i.e., a sequence of parametrised unitary transformations meant for execution on quantum hardware (Mitarai et al., 2018). QML methods are often optimised using variants of the backpropagation algorithm (McClean et al., 2018; Verdon et al., 2019; Beer et al., 2020). Quantum variational models were recently applied to combinatorial optimisation (Khairy et al., 2020) and reinforcement learning (Dunjko et al., 2016; Lockwood & Si, 2020). Instead of learning to regress unitary transformation parameters for gate-based QC, we learn to regress QUBO forms for QA.\n\nIn contrast to gate-based machines, QAs can already solve real problems formulated as QUBOs (Neukart et al., 2017; Teplukhin et al., 2019; Stollenwerk et al., 2019; Orus et al., 2019; Mato et al., 2021; Speziali et al., 2021). Recently, QCV has rapidly transitioned from theoretical considerations (Venegas-Andraca & Bose, 2003; Chin et al., 2020; Neven et al., 2008b;a) to practical algorithms leveraging quantum-mechanical effects of quantum computers, ranging from image retrieval and processing (Venegas-Andraca & Bose, 2003; Yan et al., 2016), classification (Boyda et al., 2017; Nguyen & Kenyon, 2019; Cavallaro et al., 2020; Willsch et al., 2020; Dema et al., 2020) and tracking (Li & Ghosh, 2020; Zaech et al., 2022), to problems on graphs (Zick et al., 2015; Seelbach Benkner et al., 2020; Mariella & Simonetto, 2021), consensus maximisation (Doan et al., 2022), shape alignment Noormandipour & Wang (2021); Seelbach Benkner et al. (2021), segmentation (Arrigoni et al., 2022) and ensuring cycle-consistency (Birdal et al., 2021; Yurtsever et al., 2022).\n\nMany of these methods are evaluated on real quantum hardware, as both gate-based and QA machines can be accessed remotely (D-Wave Systems, 2022; Rigetti Computing, 2022).\n\nWe demonstrate the efficacy of our QuAnt approach on the applications of graph matching and point set alignment where we compare against recent quantum state-of-the-art methods Seelbach Benkner et al. (2020); Golyanik & Theobalt (2020), respectively.\n\nAnother line of work in different domains concerns learning the best adiabatic quantum algorithm. While some works (Pastorello et al., 2021; Pastorello & Blanzieri, 2019) develop an algorithm inspired by tabu search, our method uses a neural network to output a coupling matrix. Orthogonal to our work, others train neural networks to solve problem-specific QUBOs (Gabor et al., 2020). N ̈ußlein et al. (2022) optimize a blackbox function by finding a QUBO as surrogate model. QML on gate-based QC has been studied at length, but machine learning with QA remains largely underexplored, with only a few exceptions (e.g., linear regression (Date & Potok, 2021) and binary neural networks (Sasdelli & Chin, 2021)). In stark contrast to existing QCV methods with analytically derived QUBOs (Li & Ghosh, 2020; Seelbach Benkner et al., 2020; 2021; Birdal et al., 2021), our approach enables more flexible and compact solution encodings.\n\nQuAnt is also related to recent non-quantum approaches that aim to improve combinatorial optimisation by seamlessly integrating deep learning and combinatorial building blocks as custom layers and backpropagating through them (Ferber et al., 2020; Rol ́ınek et al., 2020; Vlastelica et al., 2020). In this respect, ours is the first work that uses a quantum QUBO solver in neural architectures.\n\n3 QUBO LEARNING APPROACH\n\nWe present a new meta-learning approach for regressing quadratic unconstrained binary optimisation problems (QUBOs) suitable for modern quantum annealers (QA); see Fig. 1. While existing works analytically derive QUBOs for different problems (Birdal et al., 2021; Seelbach Benkner et al., 2020; 2021; Zaech et al., 2022), we propose to instead learn a function that turns a problem instance into a QUBO to be solved by a QA. Specifically, we train a multi-layer perceptron (MLP) that takes a vectorised problem instance and regresses the QUBO weights such that the QUBO minimiser is the solution to the problem. Note that we only specify the bit encoding of the solution but let the network learn to derive QUBOs. Crucially, we show how training the MLP is possible despite quantum annealing (like any QUBO solver) being discontinuous and non-differentiable.\n\n3.1 QUBOS AND QUANTUM ANNEALING\n\nis\n\nQuantum annealing form to arg mins∈{−1,1}n s⊤Js + b⊤s, where s is a binary vector, J ∈ Rn×n is a matrix of couplings, and b ∈ Rn contains biases (McGeoch, 2014). We can rewrite this as a QUBO: arg minx∈{0,1}n x⊤Ax, by substituting x = 1 2 diag(b).\n\n2 (s + 1n) and A = 1\n\n2 (J1n + 1⊤\n\na metaheuristic\n\nsolve N P-hard\n\nn J) + 1\n\n4 J + 1\n\nproblems\n\nthe\n\nof\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIn quantum annealing, the binary n-dimensional vector x describes the measurement outcomes of n qubits. Annealing starts out with an equal superposition quantum state of the qubits that assigns an equal probability to all possible binary states {0, 1}n. During the anneal, the couplings and biases of A are gradually imposed on the qubits. The adiabatic theorem (Born & Fock, 1928) implies that doing so sufficiently slow forces the qubits into a quantum state that assigns nonvanishing probability only to binary states that minimise the QUBO (Farhi et al., 2001). We then only need to measure the qubits to determine their binary state, which is our solution x. For a more detailed description of quantum annealing, we refer to prior computer-vision works (Seelbach Benkner et al., 2020; 2021; Golyanik & Theobalt, 2020; Li & Ghosh, 2020) and Appendix A.\n\n3.2 NETWORK ARCHITECTURE AND LOSSES\n\nIn this section, we describe the network architecture that takes as input a problem instance and regresses a QUBO whose solution (e.g., obtained via quantum annealing) solves the problem instance. For a given problem type, we require a problem description that is amenable to QUBO learning: A parametrisation of problem instances as real-valued vectors p ∈ Rm, and a parametrisation of solutions as binary vectors x ∈ {0, 1}n. Since we use supervised training, we additionally need a training set D = {(pd, ˆxd)}D d=1 containing D problem instances pd with ground-truth solutions ˆxd. We use a multilayer perceptron (MLP) with L layers and H hidden dimensions, ReLU activations (except for the last layer, which uses sin (Sitzmann et al., 2020)), and concatenating skip connections from the input into odd-numbered layers (except for the first and last layers). The input to the network is a problem instance p, and the output is a QUBO matrix A: A = MLP(p).\n\nWe could now use a dataset of problem instances and corresponding A to supervise the MLP directly. However, this requires specifying how A is to be derived for a certain instance, which comes (1) A problem-type-specific algorithm for analytically deriving instancewith two downsides: specific A needs to be designed to generate enough training data {(pd, Ad)}D d=1, which is nontrivial, and (2) The binary parametrisation (x) of the solution space depends on the algorithm, which can lead to more variables than intrinsically needed by the problem (e.g., if x needs to represent one of k numbers, a one-hot parametrisation would have length n=k, while a binary-encoding parametrisation would have length n= log k). This is particularly problematic as contemporary quantum hardware only provides a limited number of qubits. We thus choose to supervise A not directly and, instead, supervise the solutions of the QUBO. This strategy tackles both issues as it lets the network learn an algorithm compatible with the (potentially shorter) solution parametrisation. Therefore, our method is easily applicable to new problem types, as we show in Secs. 3.4 and 3.5.\n\nThe regressed A defines a QUBO, which can be solved by any QUBO solver. But how can we, during training, backpropagate gradients from the solution binary vector x through the QUBO solver despite these solvers having zero gradients almost everywhere (Vlastelica et al., 2020)? We circumvent this issue by exploiting a contrastive loss (cf. LeCun et al. (2006, Eq. (10))) as follows: We know the energy of the ground-truth solution ˆx of the problem instance, namely ˆx⊤Aˆx. If the energy of the minimiser x∗ = x∗(A) of the current QUBO is lower, then A does not yet describe a QUBO that outputs the right solution. We, therefore, seek to push the energy of ˆx lower while pulling the energy of the minimiser x∗ up:\n\nLgap = ˆx⊤Aˆx − x∗⊤Ax∗, (1) which has a zero gradient if x∗ = ˆx, as desired. Lgap avoids backpropagation through x∗ and is even compatible with automatic differentiation. It yields the following update for an entry of A, which can then be further backpropagated into the MLP via the chain rule: ∂x∗(A) ∂Ai,j\n\n∂Lgap(A) ∂Ai,j\n\n= 2ˆxi ˆxj − 2x∗\n\nj − 2\n\nAx∗,\n\ni x∗\n\n(2)\n\nwhere the last term comes from the dependency of x∗ on A via the QUBO solver. While intuitively useful, we note that this term is zero “almost everywhere” (in the mathematical sense), and we hence ignore it as it provides almost no information. Note that this is a common approximation (e.g., auto-differentiation frameworks backpropagate through max(·) pooling in the same manner).\n\nUnfortunately, Lgap alone would not prevent degenerate A, which have multiple solutions, including undesirable ones. We, therefore, discourage such A that have more than one solution x∗:\n\nLunique = −|ˆx⊤Aˆx − x+⊤Ax+|,\n\n(3)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nwhere x+ is the x that minimises x⊤Ax over {0, 1}n \\ {x∗}, and |·| is the absolute value operator.\n\nIn addition to these data terms, we found it helpful to regularise the network by encouraging sparsity on all intermediate features of the MLP (Liu et al., 2015):\n\n(cid:88)\n\nLmlp =\n\n1 |f |\n\n∥f ∥1,\n\n(4)\n\nf ∈F where F is the set of all network layer outputs (except for the last layer). The total loss then reads: L = Lgap + λuniqueLunique + λmlpLmlp, (5)\n\nwhere we set λmlp = 10−4 and λunique = 10−3 regardless of problem type.\n\n3.3 QUBOS ON D-WAVE QUANTUM ANNEALERS\n\nWe can use D-Wave to solve any generated QUBO A, where Ai,j ∈ R describes the direction and coupling strength between logical qubits i and j. However, each physical qubit in the annealer is only connected to a small subset of other physical qubits, which makes the regressed A not directly compatible with the annealer. We tackle this issue by manually pre-determining a sparse connectivity pattern of the physical qubits and then masking out the other entries of A before solving, such that the MLP focuses on only these sparse entries. For example, when x ∈ Rn with n=8, we can use D-Wave’s Chimera architecture, which is made up of interconnected K4,4 unit cells (Dattani et al., 2019). (K4,4 is a complete bipartite graph with two sets of four qubits each.) Since n=8, we can fit one problem instance into one unit cell, which allows us to anneal many problem instances in parallel by putting them in different unit cells. This speeds up training and saves expensive time on the quantum annealer. For larger problems with n=32, we can use D-Wave’s Pegasus architecture (Boothby et al., 2020), which has more interconnections (qubit couplings) between its K4,4 unit cells than Chimera. We use four such unit cells per problem instance, following D-Wave’s pattern (Til). See Fig. 4b for an exemplary colour-coded qubit connectivity pattern of A.\n\nGiven our full method, we next show how it can be applied to three problem types, i.e., graph matching, point set registration, and rotation estimation; see Appendix for further details.\n\n3.4 GRAPH MATCHING\n\nThe goal of graph matching is to determine correspondences from k key points in two images or graphs; see Fig. 2 for an example. This can be formalised as a quadratic assignment problem with a permutation matrix representing the matching (Seelbach Benkner et al., 2020): arg minX∈Pk is the vectorised permutation matrix, and W ∈ Rk2×k2 permutation constraint cannot be directly realised on the quantum annealer.\n\nx⊤Wx, where Pk is the set of permutation matrices, x = vec(X) ∈ {0, 1}k2\n\ncontains pairwise weights. Unfortunately, the\n\nInstead, note that a permutation P : [k] → [k] is fully defined by the sequence P (1), P (2), . . . , P (k). Our method can use an efficient binary encoding for each entry of this sequence, using only k log k binary variables in total. Note that not all vectors x ∈ {0, 1}k log k are valid permutations. As an optional post-processing step, we can perform a projection to the nearest permutation with respect to the Hamming distance in our binary encoding. Unless stated otherwise, we do not apply this post-processing.\n\n(a) Source\n\n(b) Target\n\nIn addition to the solution parametrisation, we also need to design the problem description p. For real data, we use p = vec(W), where the diagonal contains cosine similarities between the feature vectors extracted with AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Deng et al., 2009) of all key point pairs, and the off-diagonal follows the geometric term from (Torresani et al., 2008, Eq. (6)). For evaluation, we also introduce the synthetic RandGraph dataset; it uses matrices of random distances D ∈ Rk×k with entries Di,j ∈ U(0, 1) to define Wk·i+P (i),k·j+P (j) = (cid:12) (cid:12) (cid:12). The MLP thus learns to compress the input matrix into a much smaller QUBO.\n\nFigure 2: Example matching on four key points from the Willow dataset (Cho et al., 2013). Corresponding points (same colours) are found based on feature similarity.\n\n(cid:12)Di,j − DP (i),P (j)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n3.5 POINT SET REGISTRATION AND ROTATION ESTIMATION\n\nIn 2D point set registration, we are given two point sets with potentially different numbers of points and no correspondences, and we seek to find a rotation angle that best aligns them. We follow Golyanik et al. (Golyanik & Theobalt, 2020) and use the vectorised form of their input matrix to represent a problem instance. We parametrise the solution space of x ∈ {0, 1}9 by splitting the output space [0, 1 3 π] into 29 equally sized bins and consecutively indexing them with a 9-bit integer. In 3D rotation estimation, we are given two 3D point clouds with known matches and seek to estimate the 3D rotation aligning them. We represent a problem instance by the vectorised covariance matrix of the two point clouds; 3D rotation is parametrised by Euler angles α, β, γ. We discretise each angle into 25 bins, such that x ∈ {0, 1}15.\n\n4 EXPERIMENTAL EVALUATION\n\nWe next experimentally evaluate QuAnt. Our goal is to show that it outperforms the previous quantum state of the art. For reference, we also report comparisons against specialised classical methods.\n\nData. We evaluate graph matching on the Willow object dataset (Cho et al., 2013), which contains labelled key points. We use k=4 randomly chosen key point pairs per image. We use 5640 images for training, and test on 846 images. Both of the sets are obtained via pygmtools (ThinkLab, 2021). We also evaluate on our synthetic dataset RandGraph (see Sec. 3.4), with both k=4 and k=5. We evaluate 2D point set registration on the 2D Shape Structure dataset (Carlier et al., 2016) providing 2D silhouette images of real-world objects. We treat the silhouette outlines as 2D points. We use 500 shapes from various classes for training, and test on 50 shapes. For each shape, we apply 1000 (for train) or 100 (for test) different rotations of up to 60◦ and pick random pairs to generate problem instances. For 3D rotation estimation, we evaluate on ModelNet10 (Wu et al., 2015), which contains CAD models of ten object categories. We proceed with point cloud representations of each shape. We use 300 shapes from various classes for training, and test on 30 shapes from various classes. For each shape, we apply 1000 different 3D rotations with angle ranges α, γ ∈ [− 1 9 π] and β ∈ [− 1 18 π] and pick random pairs to generate problem instances.\n\n18 π, 1\n\n9 π, 1\n\nComparisons. We compare QuAnt to two baselines and specialised methods, depending on the problem type. For all problem types, we demonstrate the power of using QUBOs compared to the Diag baseline that regresses a diagonal QUBO matrix A (which is trivially solvable). While this baseline ablates the QUBO itself, we also consider a more natural neural network baseline, i.e., Pure, that regresses the binary solution directly (there is no activation after the last layer) and uses an l1-loss between the output and ˆx instead of Lgap and Lunique. At test time, we threshold the network output of Pure at 0 to obtain binary vectors. For QuAnt, Diag and Pure variants, we experiment with all combinations of the numbers of layers L ∈ {3, 5} and hidden dimensions H ∈ {32, 78}.\n\nWe compare our graph matching results with the Direct baseline on Willow (Cho et al., 2013); we directly solve the quadratic assignment problem given by W with exhaustive search, which provides an upper bound for our method. We also compare against Quantum Graph Matching (QGM) (Seelbach Benkner et al., 2020), to which we pass our input matrices W. For 2D point set registration, we compare against the analytic quantum method (AQM) (Golyanik & Theobalt, 2020), which is an upper bound for our technique since we take its vectorised QUBO as input, and against the classical, specialised ICP algorithm (Lu & Milios, 1997). For 3D rotation estimation, we use Procrustes as a classical specialised method, which is thus an upper bound for our (general) method.\n\nMetrics. We measure accuracy of the graph matching solutions as the percentage of correctly recovered permutation matrices. For 2D point set registration and 3D rotation estimation, we quantify the difference between the known ground-truth rotations and the estimated rotations by their geodesic distances (angles) in the rotation groups SO(2) and SO(3), respectively.\n\nQUBO Solvers. For graph matching, we follow Sec. 3.3 to make our regressed QUBOs compatible with the QA. Due to a restricted QA compute budget, we train and test with simulated annealing unless stated otherwise. For the point cloud experiments, we regress dense A and use our exhaustive search implementation at train and test time unless stated otherwise. When evaluating on the QA, we rely on minor embeddings to make the regressed A compatible with the QA. Please refer to the Appendix for the details.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4.1 RESULTS\n\nGeneral Baselines. The quantitative results for graph matching, point set registration, and rotation estimation are reported in Tables 1, 2, and 3, respectively. Across network sizes and all three problem types, the results show that having a full, N P-hard QUBO (ours) instead of only a diagonal QUBO (Diag) is advantageous. We also find that the proposed method yields better results than Pure on both point set registration and rotation estimation, although Pure yields better results for graph matching.\n\nTable 1: Comparison to general baselines on graph matching. We report the accuracy (in %).\n\n(a) RandGraph for k=4.\n\nOurs Diag\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\n9 30 11 49\n\n8 18 11 43\n\nPure 91 96 89 96\n\n(b) Willow object dataset (Cho et al., 2013) for k=4. Trained for 300 epochs with L=5, H=78.\n\nOurs Diag\n\nPure Direct\n\n69\n\n53\n\n90\n\n97\n\nTable 2: Comparison to general baselines on point set registration. We report averages of the mean errors and their standard deviations over three runs.\n\nTable 3: Comparison to general baselines on rotation estimation. We report averages of the mean errors and their standard deviations over three runs.\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 8.4 ± 0.8 7.2 ± 1.1 8.6 ± 0.5 6.8 ± 0.3\n\nDiag 11.1 ± 1.3 8.3 ± 0.7 10.9 ± 1.2 7.7 ± 0.5\n\nPure 8.2 ± 1.2 9.3 ± 1.9 9.3 ± 1.9 11.3 ± 4.5\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 5.9 ± 3.0 4.1 ± 0.5 3.7 ± 0.8 3.4 ± 0.4\n\nDiag 5.4 ± 1.0 5.0 ± 0.3 5.0 ± 0.4 4.7 ± 0.2\n\nPure 7.9 ± 0.5 7.1 ± 0.1 16.2 ± 7.1 10.1 ± 1.8\n\nSpecialised Methods. For reference, we compare QuAnt to methods specialised to a certain problem type. Since our approach is general, they mostly provide an upper bound for our performance.\n\nWe evaluate QGM (Seelbach Benkner et al., 2020) on several RandGraph instances. We confirm their finding that strongly enforcing permutation constraints eventually retrieves the right permutation as the sample with the lowest energy. However, using the analytical bound for the penalty term leads to a success probability (i.e., the probability of getting the best solution across anneals) smaller than random guessing due to experimental errors in the couplings. Next, we find that the QUBOs of QuAnt are much smaller and better suited to be solved with a quantum annealer than QGM’s. For RandGraph with k=5, our method needs 15 physical qubits while their baseline and row-wise methods need 89 qubits on average and a chain length of four, and their Inserted method needs, on average, 39 qubits and a chain length of three on D-Wave Advantage. Thus, our success probability of 26% when evaluating on test data is orders of magnitude higher than Inserted’s 0.22% (best in QGM). This shows how QuAnt improves over the quantum state of the art even though we merely focus on the solution with the lowest energy across anneals, while they focus on the success probabilities. We refer to Appendix D for a detailed evaluation. Table 1b confirms that Direct is an upper bound to our approach.\n\nTable 8 shows quantitative results for 2D point set matching without noise. AQM slightly outperforms QuAnt, which is expected as we take AQM’s QUBO as input; hence, its performance is an upper bound for our method. Fig. 3 shows a qualitative example.\n\nAs expected, quantitative results in Table 7 (with no incorrect correspondences) show that classical, specialised Procrustes performs better than our general method on 3D rotation estimation. Note that our technique yields better results than Procrustes under test-time noise, as we discuss later in detail.\n\n7\n\nFigure 3: Test-time example inputs and outputs of QuAnt trained for 2D point set registration.\n\nRegress Couplings with MLPEncode Problem InstanceQUBOSolutionClassicalQuantumDecode QUBO SolutionPublished as a conference paper at ICLR 2023\n\n4.2 ABLATION\n\nWe ablate Lunique and Lmlp in Table 4. For graph matching, we use RandGraph with k=4. We find that removing either Lunique or Lmlp leads to mixed results on graph matching and worse results in almost all cases for points set registration and rotation estimation.\n\nTable 4: Loss ablations. We report accuracy for graph matching (in %) and mean/median error otherwise.\n\nGraph Matching\n\nw/o Lunique w/o LMLP Ours w/o Lunique 17.8 / 12.0 15.5 / 8.0 18.1 / 11.7 18.3 / 11.7\n\n9 30 11 49\n\n7 30 14 46\n\n9 29 6\n54\n\nPoint Set Registration w/o LMLP 18.6 / 13.4 21.8 / 17.0 19.0 / 7.7 17.8 / 11.7\n\nOurs 15.0 / 8.7 14.5 / 7.7 9.0 / 4.6 18.5 / 7.7\n\nRotation Estimation\n\nw/o Lunique w/o LMLP 3.4 / 3.0 4.6 / 5.0 2.5 / 2.0 4.1 / 4.0\n\n5.1 / 5.0 2.9 / 3.0 3.4 / 3.0 3.7 / 4.0\n\nOurs 3.4 / 3.0 4.2 / 4.0 2.3 / 2.0 3.3 / 3.0\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\n4.3 EVALUATION ON D-WAVE\n\nBy design, our QuAnt is agnostic to the type of QUBO solver used. After training with exhaustive search, we compare how the performance on the test set differs under exhaustive search, SA, or QA. The results in Fig. 4a show that the exact solutions of exhaustive search only slightly outperform the less computationally expensive QA and SA. Moreover SA yields results very similar to QA.\n\n0.2\n\ns t\nl\n\nu s\ne R\n\n0 0\n0 1\n\nn\n\ni\n\ns e\nc n\na t\ns n\nI\n\n2\n\n1 0\n\nES QA SA\n\nEpoch 0\n\n0 2 4 6 810 14\n\nEpoch 245\n\n0 2 4 6 810 14\n\n2\n\n1 0\n\nEpoch 450\n\n0 2 4 6 810 14\n\n2\n\n1 0\n\nf o\n%\n\n0.1\n\n0\n\n0 1 2 3 4 5 6 7 8 9 10 Error in degree\n\n(a) Errors on rotation estimation.\n\n(b) Evolution of result (top) and coupling matrix (bottom).\n\nFigure 4: (a) Histogram of errors for rotation estimation using exhaustive search (ES), quantum annealing (QA), and simulated annealing (SA) at test time. The maximum error on the x-axis amounts to 59 and no methods have higher errors than shown. (b) Evolution over different epochs of the Hamming distance between predicted solutions and the ground truth (top), and coupling matrix when training our approach for graph matching (bottom). (Top): The x-axis shows the Hamming distance. Blue indicates unprojected results, and red means after projection to a permutation. We only project after training.\n\nNext, we compare the test-time results of QA and SA (after training the method with the same technique, QA and SA, respectively). See Table 5 for the results, both with and without projecting the final binary solution to a valid permutation encoding during postprocessing. Training with QA delivers better results than training with SA. We attribute this to a better second-best solution x+ used by Lunique. While SA yields solutions x∗ that are comparable to QA, its second-best solutions are worse than QA’s. We refer to the appendix for details. Unfortunately, real-world compute resources for training with QA remain limited, as of this writing. We, therefore, fall back on SA for larger-scale experiments in this work. However, Table 5 suggests that our results could improve noticeably on QA.\n\nTable 5: QuAnt (L=5, H=78) on RandGraph (k=5) trained for 450 epochs (QA or SA).\n\nBefore projection After projection\n\nSA QA 18 10 36 24\n\n4.4 FURTHER ANALYSIS\n\nTraining. Fig. 4b visualises how the instantaneous solution and A matrix evolve for graph matching.\n\nVarying Problem Difficulty. We provide a more detailed analysis of the performance of our method on point set registration for varying difficulty levels. Table 6 shows that a larger input misalignment between the two point clouds worsens the results, as expected.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Interval analysis for point set registration with L=5, H=78. We evaluate on point cloud pairs with ground-truth angles uniformly sampled within the given intervals. We report the mean/median error in degrees.\n\nangle interval mean / median\n\n0 - 1 π 18 4.0 / 1.9\n\n18 - 2 π 1 π 18 5.1 / 2.6\n\n18 - 3 π 2 π 18 6.4 / 3.0\n\n18 - 4 π 3 π 18 7.9 / 3.8\n\n18 - 5 π 4 π 18 7.9 / 3.8\n\n18 - 6 π 5 π 18 8.4 / 4.0\n\nTable 7: Robustness to varying amounts of incorrect test-time correspondences in rotation estimation. We report mean/median error for L=3, H=32. The first column specifies the percentage of incorrect correspondences at test time.\n\nRobustness to Noise. We investigate the robustness of our method and other approaches against input noise at test time after training without noisy data. We look at rotation estimation, where we randomly pick a fixed percentage of points and randomly permute their correspondences (among themselves). Table 7 contains results. The quality of QuAnt’s results barely degrades with increasing noise levels, even for 20% of incorrect correspondences. QuAnt already outperforms the classical Procrustes for even 1% of incorrect correspondences, even though Procrustes also starts from the same covariance matrix. We observe that the advantage of our method grows with larger noise levels. QuAnt also consistently performs better than the general baselines, which are similarly robust to increasing noise levels.\n\nProcrustes 0.0 / 0.0 5.8 / 3.0 25.7 / 13.0 43.8 / 21.0 64.7 / 58.0 75.3 / 79.0\n\nOurs 3.9 / 4.0 3.4 / 3.0 3.4 / 3.0 3.2 / 3.0 3.5 / 3.0 3.7 / 3.0\n\nPure 8.1 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0\n\nDiag 5.6 / 6.0 5.7 / 6.0 6.0 / 6.0 6.2 / 6.0 6.2 / 6.0 5.8 / 6.0\n\n% 0\n1 5\n10 15 20\n\nWe next look at point set registration under input noise at test time and after training without noise. Here, we add uniform noise to one point cloud, where the range of the noise is a percentage of the maximum extent of the point cloud. Table 8 contains the results. ICP, an iterative approach, is robust to the noise, gives highly accurate results and, thus, outperforms the competing non-iterative approaches. Since QuAnt takes as input the vectorised QUBO that AQM solves, AQM constitutes an upper bound for the performance of our approach. However, QuAnt could, in principle, scale to 3D point set matching while AQM’s solution parametrisation severely inhibits scaling to larger problems. Finally, QuAnt performs better than the general baselines.\n\n5 DISCUSSION\n\nTable 8: Robustness to varying amounts of uniform noise in point set registration. We report mean/median error for L=5, H=78 and the number of logical/physical qubits. The first column states the range of the noise in % of the maximum extent of the point cloud. “†”: uncoupled qubits.\n\n% 0\n5 10 15 20 Qubits\n\nOurs 5.8 / 3.5 6.4 / 3.3 6.5 / 3.3 7.2 / 3.5 10.3 / 5.4 9/14\n\nDiag 7.3 / 4.7 7.0 / 5.2 8.4 / 5.2 9.5 / 5.9 11.6 / 6.6 (9/9)†\n\nPure 6.8 / 5.9 7.0 / 6.1 7.1 / 6.5 7.9 / 6.7 8.2 / 6.8 n/a\n\nAQM 4.3 / 2.6 4.5 / 2.9 5.6 / 3.8 5.6 / 3.8 5.9 / 3.3 21/∼55\n\nLimitations and Future Work. As all learning-based approaches, QuAnt can perform worse on problem instances that fall significantly outside the training distribution. While our general method does not outperform classical methods specialised on certain problem types, we achieve performance on par with hand-crafted QUBO designs used in state-of-the-art QCV methods. We achieve this while greatly reducing the effort required for new problem types. For our point cloud experiments, we rely on minor embeddings to transfer the regressed dense QUBOs to the QA. On existing hardware, large minor embeddings can worsen the resulting quality noticeably. However, we only need to embed a QUBO with nine logical qubits into 14 physical qubits. Although our focus is on a general design, our core idea of learning QUBOs can be specialised to any given problem type by designing a more specific network architecture and losses that capture priors for the problem type.\n\nConclusion. We showed that learning to regress QUBO forms for different problems instead of deriving them analytically can be a reasonable alternative to existing methods. We showed the generality of QuAnt on diverse problem types. Our experiments demonstrated that learning QUBO forms and solving them either on a quantum annealer or with simulated annealing, in most cases, leads to better results than directly regressing solutions. Moreover, QuAnt significantly outperformed the previous quantum state of the art in graph matching and rotation estimation in the setting with noise. We believe our work considerably broadens the available toolbox for development and analysis of quantum computer vision methods and opens up numerous avenues for future research.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nSource code for dwave.system.composites.tiling.\n\nhttps://docs.ocean.dwavesys.\n\ncom/projects/system/en/stable/_modules/dwave/system/composites/ tiling.html. Accessed: 2022-02-09.\n\nTameem Albash and Daniel A Lidar. Adiabatic quantum computation. Reviews of Modern Physics,\n\n90(1):015002, 2018.\n\nFederica Arrigoni, Willi Menapace, Marcel Seelbach Benkner, Elisa Ricci, and Vladislav Golyanik. Quantum motion segmentation. In European Conference on Computer Vision (ECCV), 2022.\n\nKerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, Daniel Scheiermann, and Ramona Wolf. Training deep quantum neural networks. Nature Communications, 11(808), 2020.\n\nJacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.\n\nQuantum machine learning. Nature, 549, 11 2016.\n\nTolga Birdal, Vladislav Golyanik, Christian Theobalt, and Leonidas Guibas. Quantum permutation\n\nsynchronization. In Computer Vision and Pattern Recognition (CVPR), 2021.\n\nKelly Boothby, Paul Bunyk, Jack Raymond, and Aidan Roy. Next-Generation Topology of D-Wave\n\nQuantum Processors. arXiv e-prints, 2020.\n\nMax Born and Vladimir Fock. Beweis des adiabatensatzes. Zeitschrift f ̈ur Physik, 51(3):165–180,\n\n1928.\n\nEdward Boyda, Saikat Basu, Sangram Ganguly, Andrew Michaelis, Supratik Mukhopadhyay, and Ramakrishna R. Nemani. Deploying a quantum annealing processor to detect tree cover in aerial imagery of california. PLoS ONE, 12, 2017.\n\nJun Cai, William G. Macready, and Aidan Roy. A practical heuristic for finding graph minors. arXiv\n\ne-prints, 2014.\n\nAxel Carlier, Kathryn Leonard, Stefanie Hahmann, Geraldine Morin, and Misha Collins. The 2d shape structure dataset: A user annotated open access database. Computers & Graphics, 58: 23–30, 2016.\n\nGabriele Cavallaro, Dennis Willsch, Madita Willsch, Kristel Michielsen, and Morris Riedel. Approaching remote sensing image classification with ensembles of support vector machines on the d-wave quantum annealer. In IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2020.\n\nTat-Jun Chin, David Suter, Shin-Fang Ch’ng, and James Quach. Quantum robust fitting. In Pro-\n\nceedings of the Asian Conference on Computer Vision (ACCV), November 2020.\n\nMinsu Cho, Karteek Alahari, and Jean Ponce. Learning graphs to match. In International Confer-\n\nence on Computer Vision, pp. 25–32, 2013.\n\nD-Wave Systems. Leap. https://docs.dwavesys.com/docs/latest/leap.html,\n\n2022. online; accessed on the 24.01.2022.\n\nD-Wave Systems, Inc. D-wave system documentation. https://docs.dwavesys.com/\n\ndocs/latest/c_gs_4.html. online; accessed on 2022-03-06.\n\nD-Wave\n\nSystems,\n\nsimulated test-projecttemplate-dimod.readthedocs.io/en/latest/reference/ sampler_composites/samplers.html, 2022a. online.\n\nannealing\n\nsampler.\n\ndimod\n\nInc.\n\nhttps://\n\nD-Wave Systems, Inc. dwave-neal. https://github.com/dwavesystems/dwave-neal,\n\n2022b. online.\n\nD-Wave Systems, Inc. D-wave ocean.\n\nhttps://docs.ocean.dwavesys.com/en/\n\nstable, 2022c. online.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPrasanna Date and Thomas Potok. Adiabatic quantum linear regression. Scientific Reports, 11,\n\n2021.\n\nNike Dattani, Szilard Szalay, and Nick Chancellor. Pegasus: The second connectivity graph for\n\nlarge-scale quantum annealing hardware. arXiv e-prints, 2019.\n\nB Dema, Junya ARAI, and Keitarou HORIKAWA. Support vector machine for multiclass classifi-\n\ncation using quantum annealers. 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, pp. 248–255. IEEE, 2009.\n\nAnh-Dzung Doan, Michele Sasdelli, David Suter, and Tat-Jun Chin. A hybrid quantum-classical\n\nalgorithm for robust fitting. In Computer Vision and Pattern Recognition (CVPR), 2022.\n\nYuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive power of parametrized\n\nquantum circuits. Phys. Rev. Research, 2:033125, 2020.\n\nVedran Dunjko and Hans J Briegel. Machine learning & artificial intelligence in the quantum do-\n\nmain: a review of recent progress. Reports on Progress in Physics, 81(7):074001, 2018.\n\nVedran Dunjko, Jacob M. Taylor, and Hans J. Briegel. Quantum-enhanced machine learning. Phys.\n\nRev. Lett., 117:130501, 2016.\n\nEdward Farhi, Jeffrey Goldstone, Sam Gutmann, Joshua Lapan, Andrew Lundgren, and Daniel Preda. A quantum adiabatic evolution algorithm applied to random instances of an np-complete problem. Science, 292(5516):472–475, 2001.\n\nAaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. Mipaal: Mixed integer program as\n\na layer. AAAI Conference on Artificial Intelligence, 34:1504–1511, 2020.\n\nThomas Gabor, Sebastian Feld, Hila Safi, Thomy Phan, and Claudia Linnhoff-Popien. Insights on In Proceedings of the IEEE/ACM 42nd International\n\ntraining neural networks for qubo tasks. Conference on Software Engineering Workshops, pp. 436–441, 2020.\n\nFrank Gaitan and Lane Clark. Graph isomorphism and adiabatic quantum computing. Phys. Rev. A, 89:022342, Feb 2014. doi: 10.1103/PhysRevA.89.022342. URL https://link.aps.org/ doi/10.1103/PhysRevA.89.022342.\n\nVladislav Golyanik and Christian Theobalt. A quantum computational approach to correspondence problems on point sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9182–9191, 2020.\n\nVojtˇech Havl ́ıˇcek, Antonio D. C ́orcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature, 567:209–212, 2019.\n\nSami Khairy, Ruslan Shaydulin, Lukasz Cincio, Yuri Alexeev, and Prasanna Balaprakash. Learning to optimize variational quantum circuits to solve combinatorial problems. In AAAI Conference on Artificial Intelligence, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 25, 2012.\n\nJonas M. K ̈ubler, Simon Buchholz, and Bernhard Sch ̈olkopf. The inductive bias of quantum kernels.\n\nIn Neural Information Processing Systems (NIPS), 2021.\n\nYann LeCun, Sumit Chopra, Raia Hadsell, Fu Jie Huang, and et al. A tutorial on energy-based\n\nlearning. In Predicting Structured Data. MIT Press, 2006.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJunde Li and Swaroop Ghosh. Quantum-soft qubo suppression for accurate object detection. In\n\nEuropean Conference on Computer Vision (ECCV), 2020.\n\nBaoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 806–814, 2015.\n\nOwen Lockwood and Mei Si. Reinforcement learning with quantum variational circuit. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), 16:245–251, 2020.\n\nFeng Lu and Evangelos Milios. Robot pose estimation in unknown environments by matching 2d\n\nrange scans. Journal of Intelligent and Robotic systems, 18(3):249–275, 1997.\n\nNicola Mariella and Andrea Simonetto. A Quantum Algorithm for the Sub-Graph Isomorphism\n\nProblem. arXiv e-prints, 2021.\n\nKevin Mato, Riccardo Mengoni, Daniele Ottaviani, and Gianluca Palermo. Quantum molecular\n\nunfolding. arXiv e-prints, 2021.\n\nJarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren plateaus in quantum neural network training landscapes. Nature Communications, 9(4812), 2018.\n\nCatherine C. McGeoch. Adiabatic quantum computation and quantum annealing: Theory and prac-\n\ntice. Synthesis Lectures on Quantum Computing, 5(2):1–93, 2014.\n\nK. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum circuit learning. Phys. Rev. A, 98:\n\n032309, 2018.\n\nFlorian Neukart, Gabriele Compostella, Christian Seidel, David von Dollen, Sheir Yarkoni, and Bob\n\nParney. Traffic flow optimization using a quantum annealer. Frontiers in ICT, 4, 2017.\n\nHartmut Neven, Vasil S. Denchev, Geordie Rose, and William G. Macready. Training a binary\n\nclassifier with the quantum adiabatic algorithm. arXiv e-prints, 2008a.\n\nHartmut Neven, Geordie Rose, and William G. Macready.\n\nImage recognition with an adiabatic quantum computer i. mapping to quadratic unconstrained binary optimization. arXiv e-prints, 2008b.\n\nNga T. T. Nguyen and Garrett T. Kenyon.\n\nImage classification using quantum inference on the\n\nd-wave 2x. arXiv e-prints, 2019.\n\nMohammadreza Noormandipour and Hanchen Wang. Matching point sets with quantum circuit\n\nlearning. arXiv e-prints, 2102.06697, 2021.\n\nJonas N ̈ußlein, Christoph Roch, Thomas Gabor, Claudia Linnhoff-Popien, and Sebastian arXiv preprint\n\nFeld. Black box optimization using qubo and the cross entropy method. arXiv:2206.12510, 2022.\n\nT. P. Orlando, J. E. Mooij, Lin Tian, Caspar H. van der Wal, L. S. Levitov, Seth Lloyd, and J. J.\n\nMazo. Superconducting persistent-current qubit. Phys. Rev. B, 60:15398–15413, Dec 1999.\n\nRoman Orus, Samuel Mugel, and Enrique Lizaso. Forecasting financial crashes with quantum com-\n\nputing. Physical Review A, 99, 06 2019.\n\nDavide Pastorello and Enrico Blanzieri. Quantum annealing learning search for solving qubo prob-\n\nlems. Quantum Information Processing, 18(10):1–17, 2019.\n\nDavide Pastorello, Enrico Blanzieri, and Valter Cavecchia. Learning adiabatic quantum algorithms\n\nover optimization problems. Quantum Machine Intelligence, 3(1):1–19, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019.\n\nRigetti Computing. Rigetti quantum cloud services. https://docs.rigetti.com/qcs/,\n\n2022. online; accessed on the 24.01.2022.\n\nMichal Rol ́ınek, Paul Swoboda, Dominik Zietlow, Anselm Paulus, V ́ıt Musil, and Georg Martius. Deep graph matching via blackbox differentiation of combinatorial solvers. In European Conference on Computer Vision (ECCV), pp. 407–424, 2020.\n\nMichele Sasdelli and Tat-Jun Chin. Quantum annealing formulation for binary neural networks.\n\narXiv preprint arXiv:2107.02751, 2021.\n\nMarcel Seelbach Benkner, Vladislav Golyanik, Christian Theobalt, and Michael Moeller. Adiabatic quantum graph matching with permutation matrix constraints. In International Conference on 3D Vision (3DV), 2020.\n\nMarcel Seelbach Benkner, Zorah L ̈ahner, Vladislav Golyanik, Christof Wunderlich, Christian Theobalt, and Michael Moeller. Q-match: Iterative shape matching via quantum annealing. In International Conference on Computer Vision (ICCV), 2021.\n\nSukin Sim, Peter D. Johnson, and Al ́an Aspuru-Guzik. Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms. Advanced Quantum Technologies, 2(12):1900070, 2019.\n\nVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Proc. NeurIPS, 2020.\n\nStefano Speziali, Federico Bianchi, Andrea Marini, Lorenzo Menculini, Massimiliano Proietti, Loris Francesco Termite, Alberto Garinei, Marcello Marconi, and Andrea Delogu. Solving sensor placement problems in real water distribution networks using adiabatic quantum computation. Quantum Computing and Engineering (QCE), 2021.\n\nTobias Stollenwerk, Elisabeth Lobe, and Martin Jung. Flight gate assignment with a quantum an-\n\nnealer. In Quantum Technology and Optimization Problems, 2019.\n\nAlexander Teplukhin, Brian K. Kendrick, and Dmitri Babikov. Calculation of molecular vibrational\n\nspectra on a quantum annealer. Journal of chemical theory and computation, 2019.\n\nThinkLab. pygmtools. Shanghai Jiao Tong University. Github, 2021. URL https://github.\n\ncom/Thinklab-SJTU/pygmtools.\n\nLorenzo Torresani, Vladimir Kolmogorov, and Carsten Rother. Feature correspondence via graph In European conference on computer vision, pp.\n\nmatching: Models and global optimization. 596–609. Springer, 2008.\n\nSalvador E. Venegas-Andraca and Sougato Bose. Storing, processing, and retrieving an image using quantum mechanics. In Quantum Information and Computation, volume 5105, pp. 137 – 147, 2003.\n\nGuillaume Verdon, Jason Pye, and Michael Broughton. A universal training algorithm for quantum\n\ndeep learning. In APS March Meeting Abstracts, 2019.\n\nMarin Vlastelica, Anselm Paulus, V ́ıt Musil, Georg Martius, and Michal Rol ́ınek. Differentiation In International Conference on Learning Representations\n\nof blackbox combinatorial solvers. (ICLR), 2020.\n\nD. Willsch, M. Willsch, H. De Raedt, and K. Michielsen. Support vector machines on the d-wave quantum annealer. Computer Physics Communications, 248:107006, 2020. ISSN 0010-4655. doi: https://doi.org/10.1016/j.cpc.2019.107006. URL https://www.sciencedirect.com/ science/article/pii/S001046551930342X.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912–1920, 2015.\n\nFei Yan, Abdullah Iliyasu, and Salvador Venegas-Andraca. A survey of quantum image representa-\n\ntions. Quantum Information Processing, 15, 1 2016.\n\nAlp Yurtsever, Tolga Birdal, and Vladislav Golyanik. Q-fw: A hybrid classical-quantum frank-wolfe for quadratic binary optimization. In European Conference on Computer Vision (ECCV), 2022.\n\nJan-Nico Zaech, Alexander Liniger, Martin Danelljan, Dengxin Dai, and Luc Van Gool. Adiabatic Quantum Computing for Multi Object Tracking. In Computer Vision and Pattern Recognition (CVPR), 2022.\n\nKenneth M. Zick, Omar Shehab, and Matthew French. Experimental quantum annealing: case study\n\ninvolving the graph isomorphism problem. Scientific reports, 5(1), 2015.\n\nAPPENDIX\n\nThis appendix provides more details on adiabatic quantum computing in Sec. A. We provide training and implementation settings in Sec. B. Further details on the problem description for graph matching and a failure case are in Sec. C. Sec. D contains a deeper comparison with QGM (Seelbach Benkner et al., 2020) and Sec. E compares SA and QA. In Sec. F, we provide further quantitative and qualitative results on rotation estimation. Finally, Sec. G contains more details and experiments on point set registration.\n\nA QUANTUM COMPUTING BACKGROUND\n\nA.1 QUANTUM ANNEALING IN DETAIL\n\nAs we have seen, quantum annealing is a metaheuristic to solve the N P-hard Ising problem:\n\narg min s∈{−1,1}n\n\ns⊤Js + b⊤s,\n\n(6)\n\nwhere s is a binary vector, J ∈ Rn×n is a matrix of couplings, and b ∈ Rn contains biases (McGeoch, 2014). Here, we give a brief overview of how this fits in the framework of quantum mechanics. D-Wave quantum annealers rely on magnetic fluxes in superconducting Niobium loops (Orlando et al., 1999). The direction of the current flowing through them can be modelled as a qubit, i.e., as a two-dimensional, complex, normalised vector |ψ⟩ ∈ C2 in the Dirac notation. In the so-called computational basis, the basis vectors correspond to the current flowing clockwise or anti-clockwise. After measuring the state, the system will collapse to either basis state. The absolute value of the complex-valued coefficients of the linear combination (probability amplitudes) is the probability of each outcome after measurement (e.g., clockwise or anti-clockwise current). The state C2 and is thus a 2n-dimensional space of n ∈ N qubits can be expressed with the tensor product\n\nn (cid:78)\n\ncomplex vector space. We need that many parameters because entangled states cannot be described separately. If the two states |ψ⟩ , |φ⟩ corresponding to different physical systems (e.g., two niobium loops or two atoms) can be described independent from each other, the whole system is described by |ψ⟩ ⊗ |φ⟩. Note that if every state could be decomposed this way, one would only need 2n parameters.\n\ni=1\n\nThe evolution of a quantum state |ψ⟩ over time can be described with the time-dependent Schr ̈odinger equation:\n\nH |ψ⟩ = iħ ∂\n\n∂t\n\n|ψ⟩ ,\n\n(7)\n\nwhere the Hamilton operator H is a Hermitian Matrix describing the possible energies of the system, i is the imaginary unit, ħ is a constant, and t denotes time. For adiabatic quantum computing, one needs a problem Hamiltonian HP , where the eigenvector corresponding to the lowest eigenvalue is\n\n14\n\nPublished as a conference paper at ICLR 2023\n\na solution to the particular Ising problem, and an initial Hamiltonian HI with an easy-to-prepare ground state. The Adiabatic Theorem (Born & Fock, 1928) states that if we start with the ground state of HI and take a sufficiently long time τ to gradually change from HI to HP , e.g., with:\n\nH(t) = (1 −\n\nt τ\n\n)HI +\n\nt τ\n\nHP ,\n\n(8)\n\nthen we end up in the ground state of HP . From the latter, we can deduce the solution of the particular Ising problem. Simulating this whole process classically can be difficult (or even intractable) because we are dealing with 2n × 2n matrices HI and HP , where n is the number of qubits. How difficult the classical simulation is, depends on the exact form of the Hamiltonians. (Particularly promising for speed-ups are, e.g., so-called non-stoquastic Hamiltonians (Albash & Lidar, 2018).)\n\nA.2 LOGICAL AND PHYSICAL QUBITS\n\nThe QUBO defines the couplings between two logical qubits i and j. Such a QUBO can contain couplings between any two qubits. However, in contemporary hardware realisations, each physical qubit is only connected to a few others (see Fig. 5a and Fig. 5b). In the main paper, we show how the QUBO matrix A can account for this sparse connectivity pattern by setting entries between logical qubits i and j to 0 if the physical qubits i and j have no connection. Still, D-Wave supports denser connectivity patterns than what is implied by the hardware: Multiple physical qubits can be chained together to represent a single logical qubit of x that has many connections. The physical qubits in the chain will then have strong couplings along the chain to encourage them to all end up in the same final state (either all 0 or all 1), representing the final state of the corresponding logical qubit. This is formalised as a minor embedding (of the connectivity graph of the logical qubits) into the connectivity graph of the physical qubits. Using the heuristic method of Cai and colleagues (Cai et al., 2014) is popular to determine the minor embeddings in practice.\n\n(a) Chimera\n\n(b) Pegasus\n\nFigure 5: Visualisation of qubit connectivities. (a) The connectivity pattern of the physical qubits in the Chimera architecture. The unit cells (green boxes) have fewer interconnections than on Pegasus. (b) The connectivity pattern of the physical qubits in the Pegasus architecture. Green, red, and yellow correspond to one problem instance each. Images due to D-Wave (D-Wave Systems, Inc.).\n\nB IMPLEMENTATION DETAILS\n\nOur code, which we will release, is implemented in Pytorch (Paszke et al., 2019). We use Adam (Kingma & Ba, 2014) with a learning rate of 10−3 for training. For graph matching on RandGraph with k=4, we use a batch size of 141 and train for 150 epochs, which takes about seven hours. For RandGraph k=5, we use 450 epochs, which takes about 23 hours. For Willow, we train for 300 epochs, which takes about 14 hours. The baselines are trained for the same number of epochs. While Diag takes a comparable amount of time, the Pure baseline takes about three minutes to train. For the experiments with k=5, we use 10−5 as the learning rate.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nFor the point cloud experiments, we use a batch size of 32 and train for 20 epochs, which takes about four hours. We train Pure and Diag with the same batch sizes and epochs. The training of the Diag baseline takes four hours as well, and Pure is trained within three hours. When solving a QUBO on a QA, we anneal 100 times and pick the lowest-energy solution. We access the QA via Leap 2 (D-Wave Systems, 2022) using the Ocean SDK (D-Wave Systems, Inc., 2022c). When solving with SA, we use 100 iterations from the default neal SA sampler.\n\nC GRAPH MATCHING\n\nC.1 PROBLEM DESCRIPTION\n\nHere, we describe the design of the problem description p for graph matching. We use p = vec(W), where the diagonal of W contains cosine similarities between the feature vectors extracted with AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Deng et al., 2009) of all pairs of key points. The off-diagonal follows the geometric term described in (Eq. (7)) from Torresani et al. (Torresani et al., 2008). In particular, we use the term Wgeom from Eq. (7) from Torresani et al. (Torresani et al., 2008) with minus signs in the beginning and in the exponential, and set η=0.98. The convex combination with WAlex, where the cosine similarities of the feature vectors are on the diagonal, is then:\n\nW = τWAlex + (1 − τ )Wgeom. We choose τ =0.81 and η=0.98 such that the QAP often coincides with the ground-truth correspondences (see Table 1b, “Direct” from the main paper).\n\n(9)\n\nC.2 FAILURE CASE\n\nWe show a failure case of our method when applied to graph matching in Fig. 6. It occurs due to large differences in the observed appearance.\n\n(a) Source\n\n(b) Matching\n\nFigure 6: Failure case for graph matching. We visualise the ground truth of an image pair. Here, our method does not find the correct matching: Only the beak and neck are matched correctly, while the geometric information for the other key points differs too strongly.\n\nD DETAILED COMPARISON WITH QGM (SEELBACH BENKNER ET AL.,\n\n2020)\n\nHere, we compare our method with QGM (Seelbach Benkner et al., 2020) in detail. Note that the focus of both works differs. Their work focuses more on the probability distribution of the retrieved solutions. Our work is more concerned with incorporating the quantum annealer into the training pipeline. When training the neural network, Lgap equation 1 uses the retrieved solution with the smallest energy across anneals, while they are also interested in the success probabilities, i.e., the probability to get the best solution across anneals.\n\nThe individual QUBOs occurring in our QuAnt framework are much easier to solve by the QA than QUBOs that would arise in QGM (Seelbach Benkner et al., 2020). To show this, we compute the average success probabilities of the various methods from QGM (Seelbach Benkner et al., 2020)\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Average success probabilities of different QGM variants (Seelbach Benkner et al., 2020) over 141 problem instances on RandGraph compared to QuAnt with k = 5, in %.\n\nInserted Baseline\n\nRow-wise Ours\n\n0.22\n\n0.07\n\n0.07\n\n26\n\nover 141 instances of RandGraph with k=5; see Table 9. We also apply QuAnt to these problem instances. We solve the resulting QUBO with QA and find the average probability to be 26% with a standard deviation of 18%, better than any method from the QGM paper (Seelbach Benkner et al., 2020).\n\nThis difference is not surprising since we construct our method such that we only use trivial embeddings and do not need to apply the minorminer heuristic (Cai et al., 2014). Because of that, for RandGraph with k = 5, our method needs only 15 physical qubits while their baseline and row-wise methods need 89 qubits, on average, and a chain length of 4; their Inserted method needs, on average, 39 qubits and a chain length of 3 on D-Wave Advantage. Note that a heuristic search for better penalty parameters, as in Q-Sync (Birdal et al., 2021), could give rise to better results for the methods from QGM (Seelbach Benkner et al., 2020) in Table 9. However, the corresponding embeddings would still be problematic. Directly using the binary encoding for permutations (Gaitan & Clark, 2014) requires additional qubits because the problem would a priori not be quadratic.\n\nE SOLUTION QUALITY OF SA AND QA\n\nIn the main paper, we show that training with QA yields better performance than training with SA. Here, we analyse the quality of the solutions found by both techniques further. Fig. 7 contains histograms that depict the output of the quantum annealer and the two different simulated annealing solvers from neal (D-Wave Systems, Inc., 2022b) and from dimod (D-Wave Systems, Inc., 2022a). In contrast to the solver from dimod, neal is highly optimised for performance, so we used it for our experiments.\n\nWe focus our analysis on the number of sweeps in SA, i.e., the number of steps in the ’cooling’ schedule. We observe that it strongly influences the quality of the second-best solution.\n\n) 0\n0 1\n×\n\n(\n\n10\n\n5\n\ns e\nc n\ne r\nr u\nc c\no\n\n10\n\n5\n\n10\n\n5\n\n10\n\n5\n\n10\n\n5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n(a) SA, 103 sweeps\n\n(b) SA, 10 sweeps\n\n(c) SA, 2 sweeps\n\n(d) Dimod\n\n(e) QA\n\nFigure 7: Energy histograms of (ideally optimal) 103 samples for SA (different number of sweeps and dimod) and QA on one instance of RandGraph with k=5 after 450 epochs training.\n\nTable 10 illustrates this by averaging the fraction of the second-best energies over 141 instances and analysing 1000 samples from different solvers. We see that the quantum annealer produces the second-best samples with the lowest energies.\n\nNote, however, that we do not claim that this is an intrinsic general advantage of QA over SA, but merely that in our setting, QA outperforms SA. Still, prior work (Willsch et al., 2020) also reaches the conclusion that quantum annealing has much potential for finding reasonable near-optimal solutions.\n\nThe dimod sampler also produces second-best solutions with low energies but is computationally expensive (D-Wave Systems, Inc., 2022a). This is, perhaps, because many non-optimal solutions are produced compared to the implementation from neal.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: Second-best energies of SA relative (in %) to the second-best energies of QA. We report the mean and std. deviation over 141 instances. The higher the better.\n\nSA (neal), 103 sweeps 94.2± 10.3\n\nSA (neal), 10 sweeps 86.0 ± 19\n\nSA (dimod) 99.8 ± 0.9\n\nF 3D ROTATION ESTIMATION\n\nF.1 THREE STAGES (EULER ANGLES)\n\nWe obtain improved results when regressing three Euler angles one after another compared to direct regression of three angles. Thus, we use one stage per angle, i.e., with one network per stage. The training setup is as follows. We feed the first network with problem instances where α, β, γ ̸= 0. The network then regresses α. We feed the second stage network with problem instances, where α = 0 and β, γ ̸= 0. Subsequently, the network regresses β. Lastly, we feed the third network with problem instances, where α, β = 0 and γ ̸= 0. Here it regresses γ. During test time, the first network determines α, which is then applied to the input. The updated input is re-encoded before the second stage, which outputs β. Finally, with α and β applied already, the third stage regresses γ. Fig. 8 shows how the three different networks regress the angles and how the solution progresses towards the final one.\n\n(a) Initial problem instance\n\n(b) Application of the α rotation\n\n(c) Application of the α, β rotations\n\n(d) Application of the α, β, γ rotations\n\nFigure 8: Visualisation of the different steps of our rotation-estimation network. We show (blue) the original 3D point cloud and (red) the rotated point cloud. The green points are points of the red point cloud with unknown correspondences. Here, 10% of the correspondences are unknown.\n\nF.2 VARIANCE ACROSS RUNS\n\nTo better judge the stability under different random seeds, we repeat the main experiment from the paper three times for our QuAnt method and each baseline. In Table 11, we report the mean and std. deviation of the median. Here, similar to the results from the main paper, we outperform the Diag and Pure baselines in all but one setting.\n\nF.3 TRAINING ON NOISY DATA\n\nTable 11: Comparison to general baselines on rotation estimation. We report the mean of the per-experiment median and std. deviation across experiments for three different random seeds.\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 6.0 ± 3.6 4.0 ± 1.0 3.7 ± 1.2 3.7 ± 0.6\n\nDiag 5.3 ± 1.1 5.0 ± 0.0 5.0 ± 0.0 5.0 ± 0.0\n\nPure 7.0 ± 1.0 7.0 ± 0.0 16.3 ± 7.2 9.0 ± 1.0\n\nTable 12 shows how our method performs on noise-free and noisy test data after training on noisy data. We observe that the noisy training data appears to negatively affect the training and its performance drops, while Diag improves and Pure remains unchanged.\n\nF.4 QUALITATIVE COMPARISON ON NOISY TEST DATA\n\nFig. 9 visualises differences between our solution after training on noise-free data and Procrustes alignment on a problem with noisy data (unknown correspondences).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 12: Robustness to varying amounts of incorrect test-time correspondences in rotation estimation. We report the mean/median error for L=3, H=32. The first column specifies the percentage of incorrect correspondences at test time.\n\n(a) Training without noise\n\n(b) Training with 10% incorrect correspondences\n\n% 0\n1 5\n10 15 20\n\nOurs 3.9 / 4.0 3.4 / 3.0 3.4 / 3.0 3.2 / 3.0 3.5 / 3.0 3.7 / 3.0\n\nProcrustes 0.0 / 0.0 5.8 / 3.0 25.7 / 13.0 43.8 / 21.0 64.7 / 58.0 75.3 / 79.0\n\nDiag 5.6 / 6.0 5.7 / 6.0 6.0 / 6.0 6.2 / 6.0 6.2 / 6.0 5.8 / 6.0\n\nPure 8.1 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0\n\n% 0\n1 5\n10 15 20\n\nOurs 4.4 / 4.0 4.3 / 4.0 5.0 / 5.0 5.2 / 5.0 5.2 / 5.0 5.6 / 6.0\n\nProcrustes 0.0 / 0.0 5.8 / 3.0 25.7 / 13.0 43.8 / 21.0 64.7 / 58.0 75.3 / 79.0\n\nDiag 4.6 / 5.0 5.1 / 5.0 5.1 / 5.0 5.2 / 5.0 5.0 / 5.0 4.9 / 5.0\n\nPure 8.1 / 8.0 8.3 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0\n\n(a) Initial problem setting\n\n(b) Our solution\n\n(c) Solution by Procrustes\n\nFigure 9: Comparison of initial input rotation, our method and Procrustes. The initial 3D point cloud is blue and the rotated one is red, where the unknown correspondences are displayed in green. Here, 10% of the correspondences are unknown.\n\nF.5 COMPARISON TO AQM (GOLYANIK & THEOBALT, 2020)\n\nQuAnt can estimate 3D rotations with known point matches. However, AQM (Golyanik & Theobalt, 2020) would require 81 densely connected logical qubits, which is not supported by the current quantum-hardware generations. Hence, we cannot compare against AQM for this problem setting and instead only compare to classical methods such as Procrustes.\n\nG 2D POINT SET REGISTRATION\n\nG.1 SETTING DETAILS\n\nWe encode the point set registration instances similar to Golyanik et al. (Golyanik & Theobalt, 2020). As the correspondences between the template and the reference in point set registration are not known, we use k-nearest neighbours to find possible correspondences. Our network is trained with three nearest neighbours per each template point.\n\nG.2 VARIANCE\n\nIn Table 13, we report the mean median. Here, we outperform the baselines in all cases. In nearly all setups, we outperform the baselines, and only for one case, we are on par with the Pure baseline. Still, even in that case, QuAnt performs more consistently, as evidenced by its lower std. deviation. These experiments show that we consistently outperform the baselines and the performance is not dependent on the random seed.\n\nTable 13: Comparison of QuAnt to general baselines on point set registration. We report the mean of the per-experiment median and std. deviation across experiments for three different random seeds.\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 4.8 ± 0.6 3.7 ± 0.3 4.6 ± 0.1 3.4 ± 0.1\n\nDiag 6.8 ± 0.2 5.1 ± 0.4 7.1 ± 1.0 4.9 ± 0.0\n\nPure 5.8 ± 0.9 4.8 ± 0.3 7.1 ± 1.6 7.9 ± 2.7\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: Robustness to varying amounts of uniform noise. We report the mean/median error for L=3, H=32. The first column specifies the range of the added uniform noise, in %, of the maximum extent of the point cloud.\n\n% 0\n5 10 15 20\n\nOurs 7.4 / 4.2 6.7 / 3.8 7.2 / 4.5 8.2 / 4.9 11.0 / 6.0\n\nDiag 11.3 / 7.0 11.7 / 7.3 12.2 / 6.8 12.6 / 6.8 13.9 / 8.2\n\nPure 7.8 / 6.6 8.0 / 6.8 8.6 / 7.0 9.7 / 8.0 14.3 / 10.4\n\nAQM 4.3 / 2.6 4.5 / 2.9 5.6 / 3.8 5.6 / 3.8 5.9 / 3.3\n\nG.3 NOISE\n\nIn addition to the experiments in the main paper that uses the largest architecture, we also test the noise resistance on the smallest network setup with L = 3, H = 32; see Table 14. Here, while QuAnt is better than the baselines, we do not outperform AQM. However, by construction, AQM is an upper bound for our method as the matrix introduced by Golyanik et al. (Golyanik & Theobalt, 2020) is the same as our input into the network, but it gets directly solved by the QA.\n\nG.4 QUALITATIVE ABLATION RESULTS\n\nIn addition to the quantitative loss ablation in the paper, we visualise the effect of the losses here. In Figure 10, the full loss results in a nearly ground-truth rotation. However, if we leave out Lgap or LMLP during training, a significant reduction in rotational accuracy is visible.\n\n(a) Initial problem instance\n\n(b) Ours with the full loss\n\n(c) Ours without LMLP\n\n(d) Ours without Lunique\n\nFigure 10: Qualitative loss ablations. We show the original point cloud (blue) and rotated point cloud (red). Removing either Lunique or Lmlp leads to significantly worse results.\n\nG.5 FAILURE CASE\n\nWe continue our analysis with failure cases. Results in the main paper show that an increasing input angle leads to a reduction in the accuracy of our regressed angle. This can be traced back to our problem-instance encoding as Golyanik et al. (Golyanik & Theobalt, 2020) mention that an increasing angle makes finding the correspondences more error-prone. Therefore, an imperfect input encoding makes it is also more likely for us to regress wrong angles.\n\nSimilar to most prior work on point set registration, nearly symmetric shapes can be difficult, as most points can be nearly perfectly aligned even with wrong rotations. Fig. 11 contains such a failure case. The initial angle of this problem instance, 50.8◦, is relatively large for our setup, and the shape (which looks like the silhouette of a fish) is nearly rotationally symmetric. In cases like this, our method has difficulties regressing the correct rotation after a single QUBO sampling.\n\n20\n\n21012210121.51.00.50.00.51.01.52.02.01.51.00.50.00.51.01.51.51.00.50.00.51.01.52.0210121.51.00.50.00.51.01.52.021012Published as a conference paper at ICLR 2023\n\n(a) Initial problem instance\n\n(b) Failing to regress correct rotation\n\nFigure 11: Example of a failure case in point set registration. We show the initial image (blue) as well as the rotated image (red).\n\n21\n\n1.51.00.50.00.51.01.51.51.00.50.00.51.01.51.51.00.50.00.51.01.51.51.00.50.00.51.01.5",
  "translations": [
    "# Summary Of The Paper\n\nThis paper tackles the problem of generating QUBO forms from an unspecified problem instance. It learns a low-energy formulation of quadratic unconstrained binary optimization problems with multilayer perceptron and gradient backpropagation, instead of problem-specific analytical derivations.  After learning a QUBO form, they solve it with existing quantum annealing techniques. Their model excels previous models on some benchmarks both in computational expenditure and performance.\n\nChallenge: \n\nDepth: A new training strategy for neural methods with backpropagation, independent of the solver.\nResults:\n-Task: graph matching, 2D point cloud alignment, and 3D rotation estimation.\n-Metric: Accuracy, and the number of qubits of the forms obtained by the proposed algorithms.\n\n# Strength And Weaknesses\n\nPros:\n- The idea of finding the forms of QUBO with gradient is interesting.\n- A wide variety of problems can be tackled successfully and competitively by the proposed general quantum approach.\n- The proposed algorithm finds formulation with fewer qubits than state-of-the-art.\n\nCons:\n- The selected baselines are not strong enough. It is not clear how the selected baselines, Diag and Pure measure the quality of the solution. The author should compare to state-of-the-art.\n- The comparisons with specialized methods are limited. \n- The synthetic data are relatively small, e.g., random matrix 4 by 4 and 5 by 5.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAbout Organization & Presentation:\n- The presentation is succinct and cogent. Abundant visual demonstrations are provided to aid in comprehension. The results are presented clearly in tables and graphs.\n- The paper is organized in a coherent and logical manner.\n\nAbout Novelty:\n- A new neural meta-learning approach to obtain QUBO forms executable on modern quantum annealing algorithms for computer-vision problems.\n-This paper is not very novel in its technicality, nor in its application of MLP. Nonetheless, it is commendable to take a general problem by learning its form and transforming it into the ones we have solutions to (rather than attempting to solve it directly).\n\n# Summary Of The Review\n\nAbout Challenge:\n- Obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations.\n- The MLP is trained with the results from AQM, resulting in its performance bounded by AQM. Given such deficiency, is it really fair to conclude that QUBO learning is capable to deal with a general problem?\n- The results are not impressive. Their MLP surprisingly fails to beat binary regression (“pure”) in some cases. Is it an isolated incident that QUBO learning beats some SOTA? \n\nAbout Contribution:\n-The paper contributes to the area by looking beyond the traditional approach of deriving and solving QUBO from a specific problem. It addresses a bigger picture concerning how to identify and classify a mission. By aiming at generality, it extends the application of quantum annealing as well.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents QuAnt, a novel framework for learning Quadratic Unconstrained Binary Optimization (QUBO) forms directly from data, thereby addressing the challenges associated with analytical derivation in quantum computing applications, particularly in computer vision. The methodology involves a multi-layer perceptron (MLP) that inputs vectorized problem instances and outputs QUBO matrices, which can then be solved using quantum annealers. Experimental results demonstrate QuAnt's effectiveness in tasks such as graph matching, 2D point cloud alignment, and 3D rotation estimation, showing competitive performance against existing quantum methods while utilizing fewer qubits, thus enhancing scalability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to learning QUBO forms, which could significantly streamline the integration of quantum methods into practical applications. The contrastive loss function is a noteworthy contribution, enabling backpropagation through QUBO solvers without requiring direct gradients. However, a potential weakness is the performance drop observed when the model is applied to problem instances outside the training distribution, which raises questions about its generalizability. Additionally, while the empirical results are promising, the gap between QuAnt's performance and the best classical methods indicates room for improvement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the motivation, methodology, and results. The novelty of the approach is significant, as it combines machine learning with quantum optimization, a relatively unexplored area. However, the reproducibility of results could be impacted by the specified network architecture and loss functions, which may require careful tuning. The authors have made the code and dataset publicly available, which is commendable and enhances reproducibility.\n\n# Summary Of The Review\nQuAnt represents a significant advancement in the field of quantum computing for combinatorial optimization, notably in computer vision. Its ability to learn QUBO forms from data marks a novel approach with promising applications, although further work is needed to improve its robustness and generalizability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents QuAnt, a novel framework for learning quadratic unconstrained binary optimization (QUBO) forms from data, aimed at enhancing the capability of quantum annealers in solving combinatorial optimization problems. The methodology employs a meta-learning approach using a multi-layer perceptron (MLP) to derive QUBOs for various applications, including graph matching, point set registration, and 3D rotation estimation. The findings indicate that QuAnt outperforms traditional methods, achieving significant improvements in efficiency and accuracy across diverse problem types while requiring fewer qubits.\n\n# Strength And Weaknesses\nThe strengths of the paper include its flexibility in learning QUBO forms without extensive analytical derivation, which is a significant advancement in the field. The efficiency gained by reducing the qubit requirements is particularly noteworthy given the current limitations of quantum hardware. Additionally, the generalizability of the framework across different tasks showcases its versatility. However, the paper also has weaknesses, such as potential challenges with out-of-distribution data and the dependency on simulated annealing for larger problems, which may limit its practical applicability. Furthermore, while competitive, QuAnt does not consistently outperform specialized classical methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology and its applications, making it accessible to readers. The experimental design is thorough, with appropriate metrics and comparative baselines that enhance the study's rigor. However, the reproducibility of the results may be affected by the reliance on specific quantum hardware and the complexity of the learned couplings. The novelty of the approach is substantial, given its departure from traditional analytical methods for QUBO formulation.\n\n# Summary Of The Review\nOverall, QuAnt represents a promising advancement in the formulation of QUBOs for quantum annealers through learned couplings, demonstrating competitive performance across various optimization tasks. While the framework exhibits notable strengths in flexibility and efficiency, challenges regarding out-of-distribution performance and dependency on specific hardware remain areas for further investigation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"QUANT: Quantum Annealing with Learnt Couplings\" presents a novel framework called QuAnt, which aims to derive Quadratic Unconstrained Binary Optimization (QUBO) forms directly from data using gradient backpropagation. This meta-learning approach enhances the scalability of hybrid quantum-classical methods in computer vision by allowing for the flexible learning of QUBOs rather than relying on cumbersome analytical derivations. The authors demonstrate the effectiveness of QuAnt on tasks such as graph matching, 2D point cloud alignment, and 3D rotation estimation, achieving competitive results with fewer qubits compared to existing quantum methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to learning QUBO forms, which addresses a significant limitation in current quantum optimization methods that depend on problem-specific formulations. The introduction of a multi-layer perceptron (MLP) architecture combined with a unique loss function enhances the model's ability to generalize across various tasks. However, the paper does have weaknesses, including potential limitations in performance on instances outside the training distribution and the dependency on effective minor embeddings for quantum annealers, which may not be universally applicable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, making it accessible to readers familiar with both quantum computing and machine learning. The quality of the experimental evaluation is commendable, with a clear comparison against baselines and thorough reporting of results. The novelty of using neural networks to learn QUBOs is significant, pushing the boundaries of existing research in quantum machine learning. However, reproducibility could be hindered by the reliance on specific datasets and the need for precise minor embeddings tailored to particular quantum hardware.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to the field of quantum computing by presenting a flexible and generalizable method for learning QUBO forms from data. While it shows promising results across several tasks, the limitations regarding performance on unseen instances and the requirement for specific implementations could impact broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"QUANT: Quantum Annealing with Learnt Couplings\" introduces a novel approach for learning Quadratic Unconstrained Binary Optimization (QUBO) forms from data using gradient backpropagation, diverging from traditional analytical methods. The authors propose a flexible and compact encoding for solutions, enabling better scalability and integration of quantum annealing solvers as layers within neural network architectures. Experimental results indicate that the proposed method demonstrates competitive performance with existing quantum techniques while requiring fewer qubits. However, its generalizability to a wider range of problem types and its performance against specialized classical methods remain to be fully validated.\n\n# Strength And Weaknesses\nThe paper showcases several strengths, including its innovative approach to QUBO learning and robust performance against noise, which adds significant value to its applicability in real-world scenarios. The flexibility in solution encoding is a notable advantage, yet it introduces complexity that could be challenging for practitioners less familiar with deep learning. While the comprehensive experimental evaluation strengthens the empirical foundation, the limited benchmarking against other quantum methods raises questions about the method's relative effectiveness. Furthermore, the dependence on quantum hardware, which is not yet widely accessible, limits practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its technical contributions, especially the introduction of a contrastive loss for facilitating backpropagation. The quality of the writing is high, with detailed explanations and a logical flow. However, the complexity of the proposed loss functions and network architecture may hinder reproducibility for some researchers. Although the availability of open-source code and datasets enhances reproducibility, the requirement for substantial computational resources could limit accessibility.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative method for learning QUBO forms that integrates quantum annealing into neural network frameworks. While it demonstrates competitive results and robustness, several limitations regarding generalizability, practical implementation, and complexity need to be addressed to strengthen its utility in broader applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents QuAnt, a novel framework designed to enhance the formulation and solution of quadratic unconstrained binary optimization (QUBO) problems within quantum annealing, particularly in computer vision applications. The key contributions include a learning-based approach for QUBO formulation that leverages meta-learning to derive QUBOs from data instead of relying on traditional analytical methods. The authors integrate quantum solvers into deep learning architectures, allowing for a hybrid quantum-classical learning strategy. Experimental results demonstrate that QuAnt performs competitively against existing quantum methods while requiring fewer qubits, thus improving scalability across various applications such as graph matching, 2D point cloud alignment, and 3D rotation estimation.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative learning-based approach to QUBO formulation, which broadens the applicability of quantum annealing, and the successful integration of quantum solvers into deep learning frameworks, which highlights its versatility. Experimental evaluations yield promising performance metrics, showcasing robustness and efficiency across different problem types. However, the paper lacks a comprehensive discussion of the limitations of the proposed methodology, particularly regarding the scalability of the learning process as problem complexity increases. Additionally, the comparison with state-of-the-art classical methods is somewhat limited, which could weaken the claims of superiority in performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its theoretical underpinnings clearly, demonstrating a solid grasp of both quantum computing and deep learning principles. The novelty of the proposed approach is significant, marking a departure from traditional analytical methodologies. However, while the experimental results are promising, the reproducibility could be enhanced by providing more detailed methodologies and datasets used in the experiments.\n\n# Summary Of The Review\nOverall, the paper makes a compelling case for the integration of learned QUBO formulations in quantum annealing, showcasing a significant advancement over traditional methods. The empirical results support the effectiveness of the proposed framework, though further exploration of limitations and comparisons with classical methods would strengthen its contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"QUANT: Quantum Annealing with Learnt Couplings\" presents a novel approach to adversarial training by introducing a method that learns coupling parameters from data, as opposed to relying on traditional analytical formulations. The methodology leverages gradient backpropagation within a neural network framework to optimize coupling configurations, resulting in enhanced flexibility and efficiency for training adversarial models. The findings indicate that this approach yields competitive performance, matching or surpassing state-of-the-art methods while utilizing fewer resources, and demonstrates robustness against noise and variations in input data.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach**: The method's ability to learn couplings through a neural network represents a significant shift from conventional adversarial training techniques, which often depend on fixed parameters.\n2. **Flexibility**: By adapting couplings based on data, the model exhibits improved adaptability across various adversarial tasks, making it broadly applicable.\n3. **Competitive Results**: The authors provide empirical evidence that their method achieves performance levels comparable to or better than existing approaches, with reduced computational requirements.\n4. **Robustness**: The model's resilience to input noise and variations suggests practical applicability in real-world scenarios.\n\n**Weaknesses:**\n1. **Complexity**: The training complexity associated with learning coupling parameters may hinder interpretability and computational efficiency.\n2. **Scalability Concerns**: The paper does not extensively address potential scalability issues when applied to larger datasets or more intricate adversarial settings.\n3. **Empirical Validation**: While results are promising, additional empirical validation across diverse adversarial tasks would bolster claims regarding generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of writing is high, with logical flow and thorough explanations of concepts. The novelty of the approach is significant, as it combines neural networks with adversarial training in a unique manner. However, the reproducibility might be challenged by the complexity of the training process and the potential need for extensive computational resources.\n\n# Summary Of The Review\nThis paper introduces an innovative method for adversarial training that learns coupling parameters via a neural network framework, demonstrating competitive and robust performance. While the approach shows promise, concerns regarding complexity, scalability, and the need for broader empirical validation must be addressed.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the QuAnt framework, which aims to learn Quadratic Unconstrained Binary Optimization (QUBO) forms from data using gradient backpropagation. It claims to provide a novel learning mechanism that enhances flexibility in solution encoding and applies effectively to various problems in computer vision, including graph matching, point cloud alignment, and 3D rotation estimation. The authors assert that the method demonstrates competitive performance against existing quantum state-of-the-art approaches while requiring fewer qubits, although the actual improvements may be marginal.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to learning QUBO forms without relying on analytical derivations, which could simplify the problem-solving process in quantum computing. The authors present a new training strategy for neural methods that ostensibly allows for effective QUBO solutions. However, the contributions may be overstated, as the improvements in performance are often modest and context-dependent. Additionally, the method's lack of robustness outside the training distribution raises concerns about its practical applicability. Overall, while the paper presents interesting ideas, the incremental nature of the contributions limits its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is generally adequate, with a coherent structure that presents the methodology and findings effectively. However, the novelty of the proposed techniques is somewhat diluted by the reliance on established machine learning practices. Additionally, while the empirical evaluation is presented, the reproducibility of results may be hindered by the modest improvements and the specific conditions under which they were obtained. The discussion surrounding the limitations of the method could be more comprehensive to enhance understanding of its applicability.\n\n# Summary Of The Review\nThe QuAnt framework proposed in this paper offers a novel approach to learning QUBO forms but presents limited empirical gains over existing methods. While the contributions are interesting and may have some relevance in niche applications, the overall impact appears to be more incremental than groundbreaking.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"QUANT: Quantum Annealing with Learnt Couplings\" proposes a novel methodology for learning Quadratic Unconstrained Binary Optimization (QUBO) forms through gradient backpropagation, enhancing flexibility and applicability across various computer vision tasks. The authors present a neural network framework that can effectively regress QUBO forms from problem instances, demonstrating its generalizability in applications such as graph matching, 2D point cloud alignment, and 3D rotation estimation. Experimental results indicate that the proposed approach, QuAnt, achieves competitive performance—outperforming some existing quantum methods while also exhibiting improved robustness to noise.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to learning QUBO forms, addressing the limitations of traditional analytical methods, and demonstrating significant improvements in accuracy and robustness across different tasks. The experimental evaluation is thorough, with a well-defined methodology and ablation studies that effectively highlight the importance of specific loss terms in the training process. However, a noted weakness is that while QuAnt shows promise, it still falls short compared to specialized methods in certain instances, suggesting that further optimization and exploration of its capabilities are needed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible for readers familiar with quantum computing and optimization methods. The quality of the experiments is satisfactory, supported by comprehensive data and clear comparisons. The novelty of the approach is significant, as it introduces a meta-learning perspective to QUBO formulation. However, the reproducibility of the results may depend on the availability of the datasets and implementation details, which should be explicitly provided to facilitate independent validation.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of quantum computing for computer vision applications through the learning of QUBO forms. While it demonstrates significant improvements over previous methods, the reliance on specialized techniques in certain areas indicates room for further refinement and exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"QUANT: Quantum Annealing with Learnt Couplings\" presents a novel approach to utilizing learnt Quadratic Unconstrained Binary Optimization (QUBO) forms for quantum annealing. The authors propose a meta-learning framework to derive QUBOs from data, positing that this approach can offer greater flexibility and efficiency compared to traditional analytical methods. Findings suggest that learnt QUBOs may perform better in specific contexts, though the paper does not adequately address the generalizability of these results across diverse problem types or the practical limitations of current quantum hardware.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to QUBO learning and the integration of meta-learning techniques, which could potentially lead to advancements in quantum optimization. However, several weaknesses are identified in the assumptions made regarding the future effectiveness of quantum computing, the generalizability of learnt QUBOs, and the robustness of the methodology under various conditions. The reliance on specific datasets for evaluation raises concerns about the broader applicability of the results, while the limitations of quantum annealers in handling learnt QUBOs are not sufficiently explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly. However, the assumptions underlying the proposed approach, particularly regarding the scalability and performance of learnt QUBOs on quantum hardware, are inadequately substantiated. While the novelty of the approach is commendable, reproducibility may be hindered by the lack of comprehensive empirical validation across various problem domains and the absence of rigorous comparisons with classical optimization methods.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to quantum annealing using learnt QUBOs, yet it is marred by significant assumptions that may limit its applicability and effectiveness. Further empirical validation and a more robust theoretical foundation are necessary to substantiate the claims made.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces QuAnt, a novel methodology for learning Quadratic Unconstrained Binary Optimization (QUBO) forms using gradient backpropagation, aimed at enhancing combinatorial optimization tasks in computer vision. The authors present QuAnt as a meta-learning framework that utilizes a neural network to regress QUBO weights from problem instances, addressing the limitations of traditional analytical approaches. Experimental evaluations demonstrate that QuAnt achieves competitive performance across various tasks, including graph matching, point cloud alignment, and rotation estimation, while requiring fewer qubits compared to classical and quantum state-of-the-art methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to learning QUBO formulations, which provides a flexible and scalable alternative to traditional methods. The use of a contrastive loss to tackle the non-differentiability of QUBO solvers is particularly noteworthy, as it allows for effective training of the model. Additionally, the comprehensive experimental evaluation across multiple datasets showcases the robustness and effectiveness of QuAnt. However, a notable weakness is the method's performance on out-of-distribution instances, which raises questions about its generalizability. The paper also acknowledges the limitations imposed by current quantum hardware capabilities, which may affect practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the methodology and findings, making it accessible to readers from both quantum computing and computer vision backgrounds. The quality of the writing is high, with a logical flow and clear explanations of complex concepts. The novelty of the approach is significant, as it presents a shift from analytical to learned QUBO formulations. However, reproducibility could be a concern due to the dependence on specific quantum hardware capabilities and the potential variability in results across different implementations.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to learning QUBO forms through QuAnt, demonstrating significant potential for advancing combinatorial optimization in computer vision. While it shows strong empirical results, the limitations regarding out-of-distribution performance and hardware dependency warrant further investigation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel machine learning methodology designed to enhance performance on complex datasets by employing an innovative architecture. The authors identify limitations in existing models, proposing a new framework that integrates advanced techniques to improve accuracy and computational efficiency. Experimental results demonstrate that the proposed method outperforms several state-of-the-art models across various benchmark datasets, showcasing its potential for broader applications in the field.\n\n# Strength And Weaknesses\n**Strengths:**\n1. The innovation presented in the methodology is a significant advancement over existing approaches, addressing key limitations and introducing effective new mechanisms.\n2. The proposed framework exhibits strong scalability, allowing it to be applied to larger datasets without a corresponding increase in computational cost.\n3. The generalizability of the method across different domains is a notable strength, indicating its versatility and potential for widespread adoption.\n\n**Weaknesses:**\n1. The methodology, while sound, could benefit from clearer explanations of the underlying principles to enhance accessibility for readers.\n2. The paper lacks a comprehensive analysis of comparative performance against a wider range of baseline methods, which would provide deeper insights into the proposed method's advantages and limitations.\n3. There is insufficient discussion of the method's limitations, particularly regarding scenarios where it may not perform optimally.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, providing a logical flow of ideas and results. However, certain sections could be improved for clarity, especially those detailing the methodology. The novelty of the approach stands out, as it offers a unique perspective on addressing common challenges in the field. Regarding reproducibility, the authors provide adequate details on the experimental setup, but including specific hyperparameters and configurations would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative approach to a significant problem in machine learning, with strong empirical results. While there are areas for improvement in clarity and comparative analysis, the contributions made are noteworthy and suggest valuable implications for future research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"QUANT: Quantum Annealing with Learnt Couplings\" presents a novel framework for formulating quadratic unconstrained binary optimization (QUBO) problems specifically tailored for computer vision tasks. The authors propose a methodology that leverages gradient backpropagation to learn QUBO forms from data, thereby enabling flexible and compact solution encodings. The findings demonstrate that the learned QUBOs outperform traditional analytical methods while requiring fewer qubits, making them more scalable for larger problems. The approach is generalized across various applications, including graph matching, 2D point cloud alignment, and 3D rotation estimation, indicating broad relevance in the field of quantum computer vision.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to QUBO formulation, which addresses the limitations of existing analytical methods that often hinder scalability in quantum computing applications. The use of neural networks to learn QUBO forms is a significant contribution, promoting flexibility in solution representation. Furthermore, the empirical results show competitiveness with established quantum methods, suggesting practical applicability. However, weaknesses include the potential need for a more comprehensive evaluation against a wider range of benchmarks to fully establish the superiority of the learned QUBOs. Additionally, the implementation details could be better articulated to enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions. The quality of writing is high, with an effective presentation of the methodology and results. The novelty of learning QUBO forms from data is substantial, as it introduces a departure from traditional methods. However, the reproducibility of the results may be hindered by insufficient details regarding the implementation and training of the neural networks used. A more thorough description of the dataset and the experimental setup would enhance the reproducibility of the findings.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of quantum computer vision by introducing a learning-based approach to QUBO formulation that enhances flexibility and scalability. While the methodology is promising and the results are competitive, there is room for improvement in terms of reproducibility and comprehensive benchmarking against a wider range of problems.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"QUANT: Quantum Annealing with Learnt Couplings\" presents a novel approach to learning Quadratic Unconstrained Binary Optimization (QUBO) forms from data using a meta-learning framework. The authors propose a multi-layer perceptron (MLP) that maps problem instances to QUBOs, integrating a new loss function to address the non-differentiability challenges posed by QUBO solvers. Through experimental evaluations, the proposed method demonstrates improved performance in applications such as graph matching, 2D point cloud alignment, and 3D rotation estimation, achieving competitive results compared to existing quantum methods while requiring fewer qubits.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to learning QUBO formulations, which could potentially streamline the integration of quantum computing with machine learning techniques. The use of a meta-learning framework and a custom loss function effectively addresses the complexities of traditional QUBO derivation. However, the paper has some weaknesses, including a limited exploration of the scalability of the approach in addressing larger or more complex datasets. Additionally, while the results are promising, further validation with a broader range of problem types would strengthen the claims made regarding robustness and flexibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly written, making it accessible to readers with a background in quantum computing and machine learning. The methodology is detailed, and the authors provide a robust description of their experimental setup, which aids in reproducibility. However, some technical details regarding the MLP architecture and the loss function could be elaborated for greater clarity. The novelty of integrating learned QUBOs into quantum annealing frameworks is noteworthy, although the full impact of this approach on the field remains to be evaluated in future studies.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the learning of QUBO formulations for quantum annealing, with promising empirical results across various applications. While the methodology is innovative and the findings are compelling, further exploration of scalability and a broader validation of the approach would enhance the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"QUANT: Quantum Annealing with Learnt Couplings\" introduces the QuAnt framework, which utilizes machine learning techniques to learn Quadratic Unconstrained Binary Optimization (QUBO) forms from data, specifically targeting applications in computer vision. The authors employ multi-layer perceptrons (MLPs) to facilitate the learning of QUBO structures, integrating quantum annealers as custom layers within neural networks. The experimental results demonstrate that the proposed approach yields competitive performance across multiple problem types, including graph matching, point set registration, and rotation estimation, while using fewer qubits compared to traditional analytic QUBO derivations.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to overcoming the limitations of traditional QUBO formulation methods in quantum computing. By leveraging learned couplings, the authors present a flexible framework that adapts to various problem instances, potentially increasing the applicability of quantum annealing in real-world scenarios. However, the paper acknowledges limitations regarding performance in out-of-distribution cases and suggests that specialized methods may outperform QuAnt in certain specific problems. This indicates a need for further refinement and targeted research to enhance the robustness of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to its intended audience. The terminology is consistent and appropriate throughout the text. The novel integration of learned QUBO forms into quantum annealing settings is a significant contribution to the field. The methodology appears reproducible, with detailed descriptions of the experimental setup and evaluation metrics, although further clarification on specific methodological details could enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the intersection of quantum computing and machine learning, showcasing the potential of learned QUBO forms in computer vision applications. While the contributions are significant and the results promising, some limitations and the need for additional refinement are acknowledged. The paper should be considered for acceptance with minor revisions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"QUANT: Quantum Annealing with Learnt Couplings\" presents a novel approach to formulating Quadratic Unconstrained Binary Optimization (QUBO) problems, traditionally derived analytically, by leveraging gradient backpropagation through neural networks. This method allows for learning QUBO representations directly from empirical data, enhancing flexibility in solution encoding and scalability across diverse problem domains. Empirical evaluations demonstrate that the proposed learnt QUBOs outperform traditional methods in applications such as graph matching, 2D point cloud alignment, and 3D rotation estimation, achieving competitive results with a reduced number of qubits.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to QUBO formulation, which addresses the limitations of traditional analytical derivations that hinder scalability and flexibility. The use of a multi-layer perceptron (MLP) to learn QUBOs from empirical data is a significant methodological advancement, suggesting broad applicability in various combinatorial optimization tasks. However, the paper could benefit from a more comprehensive discussion on the limitations of the proposed method, particularly regarding the robustness of the learnt QUBOs across different problem types and the potential challenges in training the neural network effectively.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The clarity of the writing and the logical flow of the sections facilitate comprehension of complex concepts. The novelty of the approach is compelling, as it represents a departure from conventional methods in QUBO formulation. In terms of reproducibility, while the paper provides sufficient details regarding the network architecture and training process, further documentation of experimental setups and datasets would enhance the ability for independent verification of results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of quantum annealing by introducing a learnable framework for QUBO formulations. Its innovative methodology shows promise for improving the efficiency and adaptability of quantum computing applications in combinatorial optimization. However, addressing potential limitations and providing more details on reproducibility would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel approach for learning Quadratic Unconstrained Binary Optimization (QUBO) forms using a neural network framework. The methodology includes a contrastive loss designed to enhance learning, and the authors claim that their approach is flexible and applicable to a variety of problem types. However, the findings indicate that while the method shows competitive performance in certain instances, it does not consistently outperform established specialized techniques, and the experimental validation is limited.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to applying neural networks for QUBO learning, which could potentially open new avenues for research in this area. However, the weaknesses are significant; the lack of rigorous validation and comparative analysis with existing methods raises doubts about the effectiveness of the proposed approach. Additionally, the reliance on backpropagation through a non-differentiable QUBO solver introduces methodological flaws, and the selective reporting of experimental results undermines the credibility of the findings. The choice of datasets appears arbitrary, limiting the generalizability of the results and highlighting the method's deficiencies in scalability and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper struggles with clarity, particularly in articulating how the QUBO learning framework adapts to new problem types, which could lead to confusion among readers. The overall quality of the presentation is hampered by vague performance metrics and a lack of comprehensive benchmarks. While the proposed contrastive loss is a novel contribution, its effectiveness in overcoming the inherent challenges of training neural networks in this context is not convincingly demonstrated. Reproducibility is also a concern, as the methodology's complexity and the authors' failure to provide sufficient evidence of robust solutions across varying input conditions complicate replication efforts.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to learning QUBO forms through a neural network but suffers from significant methodological flaws and a lack of rigorous validation. The findings, while potentially valuable, are limited by selective reporting and an insufficient examination of the proposed method's performance compared to established techniques.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"QUANT: Quantum Annealing with Learnt Couplings\" introduces a novel method for learning Quadratic Unconstrained Binary Optimization (QUBO) formulations from data using gradient backpropagation. This methodology is characterized by its flexibility and compactness in selecting solution encodings, making it broadly applicable across various problem domains, including computer vision. The authors demonstrate the efficacy of their approach through competitive performance on several challenging tasks, such as graph matching and 3D rotation estimation, while also showcasing significant resource efficiency by requiring fewer physical and logical qubits. Additionally, the integration of quantum annealing solvers into neural networks represents a pioneering hybrid approach, enhancing the learning efficiency through innovative training strategies and promoting accessibility through open-source contributions.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its revolutionary approach to learning QUBO forms, which is clearly demonstrated to be effective across multiple challenging problems. The flexibility and general applicability of the methodology allow for a wide range of future research opportunities. The integration of quantum and classical techniques is particularly noteworthy, as it opens new pathways for solving complex problems. However, potential weaknesses could include the reliance on specific types of neural network architectures and the need for further validation across an even broader set of applications to fully establish robustness and generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and results clearly, allowing for easy comprehension of its contributions. The quality of the experiments appears solid, with competitive performance metrics reported. The novelty is significant, as the paper combines quantum annealing techniques with neural networks in a fresh and impactful way. The open-source provision of code and datasets enhances reproducibility, making it easier for other researchers to build upon this work.\n\n# Summary Of The Review\nOverall, \"QUANT: Quantum Annealing with Learnt Couplings\" presents a compelling and innovative approach that significantly advances the field of quantum computing and its applications in computer vision. The paper's contributions are both impactful and broadly applicable, setting the stage for future research and development in this domain.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel approach to Quantum Annealing (QA) by proposing a method for learning Quadratic Unconstrained Binary Optimization (QUBO) forms through a neural network framework. The methodology leverages backpropagation to adaptively derive QUBOs from data, thus moving away from traditional analytical derivations. The findings suggest that this hybrid quantum-classical approach can improve computational efficiency in complex optimization problems, particularly in fields like computer vision, while establishing a theoretical foundation for understanding the interplay between problem structure and optimization landscapes.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical implications, offering a fresh perspective on QUBO learning and its potential applications across various domains. The proposed meta-learning approach could significantly enhance generalizability and problem adaptability. However, a notable weakness is the lack of extensive empirical validation, which leaves the practical effectiveness of the proposed method somewhat uncertain. Additionally, while the theoretical discussions are robust, direct comparisons to existing methodologies are limited, which may hinder the assessment of its competitive performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex theoretical concepts accessible. The quality of the theoretical analysis is high, showcasing a deep understanding of both quantum mechanics and optimization. In terms of novelty, the integration of deep learning with quantum optimization is a significant contribution; however, the reproducibility of the results could be better addressed, as empirical details and specific experimentation setups are not thoroughly presented.\n\n# Summary Of The Review\nThe paper presents a significant theoretical advancement in the field of quantum optimization by proposing a neural network-based method for learning QUBO forms. While the theoretical contributions are compelling and innovative, the lack of comprehensive empirical validation raises questions about practical applicability. Overall, the work has the potential to reshape the understanding of optimization in quantum computing, but further experiments are necessary to confirm its effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"QUANT: Quantum Annealing with Learnt Couplings\" introduces a novel method for learning Quadratic Unconstrained Binary Optimization (QUBO) forms from data using gradient backpropagation, providing an alternative to traditional analytical derivation. The methodology employs a multi-layer perceptron (MLP) that takes vectorized problem instances as input and outputs a QUBO matrix, utilizing specific loss functions designed to optimize the training process while avoiding backpropagation through the QUBO solver. The experimental results demonstrate that the proposed method outperforms several baseline approaches on datasets like RandGraph and Willow, although specific performance metrics are not extensively detailed.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to QUBO formulation, leveraging machine learning techniques for flexible and compact solution encodings suitable for various problem types. The compatibility of the proposed method with different QUBO solvers, including quantum annealers, is also a significant advantage. However, the paper lacks detailed performance metrics in the results section, which diminishes the clarity of its empirical contributions. The limitations regarding performance degradation on instances outside the training distribution and issues related to minor embeddings also warrant further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly communicates its objectives and methodology. The quality of the implementation, as indicated by the availability of code and documentation, enhances reproducibility. The novelty of the approach is notable, as it integrates learning-based methods into the realm of quantum computing optimization. However, the lack of comprehensive experimental results and clarity on the significance of findings could hinder the reader's understanding of the practical implications.\n\n# Summary Of The Review\nOverall, the paper presents a promising method for learning QUBO forms through a machine learning framework, showcasing strong potential for application in quantum annealing contexts. While the theoretical contributions and innovative aspects are commendable, the lack of detailed empirical results and clarity on limitations could impact the overall assessment of the method's effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents QuAnt, a novel method for learning Quadratic Unconstrained Binary Optimization (QUBO) formulations directly from data, claiming it as a first-of-its-kind approach. The authors argue that QuAnt achieves competitive performance against existing quantum techniques while utilizing fewer qubits. The methodology integrates a quantum annealer within a neural network framework, purportedly enhancing the robustness of the solution against noise. Empirical results are presented, suggesting that QuAnt outperforms certain baselines in various problem types, although the paper does not adequately benchmark against specialized methods or provide comprehensive comparisons.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to learning QUBOs and the potential applicability of QuAnt in quantum computer vision. However, the work suffers from several weaknesses, including a lack of thorough literature review, particularly regarding prior methods that also utilize neural networks for QUBO derivation. The performance claims against other methods appear biased, as many of the comparisons lack context about the optimization levels of the competing techniques. The generalizability of QuAnt is questioned due to insufficient demonstration of its superiority over specialized methods, and the robustness analysis against noise lacks detailed conditions, making it difficult to assess real-world applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the methodology is outlined, the lack of comprehensive comparisons and contextual discussions hampers understanding of the true contributions. The quality of the results presented is undermined by selective benchmarking and insufficient detail in experimental setups. The novelty of integrating a quantum annealer into the neural network framework is questionable, as similar approaches exist. Reproducibility is not well-supported due to vague descriptions of experimental conditions and datasets employed in the study.\n\n# Summary Of The Review\nQuAnt proposes an intriguing method for learning QUBO forms from data, but it suffers from significant shortcomings in literature acknowledgment and comparative analysis. The paper's contributions are inflated due to selective comparisons with existing methods, and the robustness of its claims is not thoroughly evaluated. Overall, while the ideas presented are interesting, the execution and contextualization need substantial improvement.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"QUANT: Quantum Annealing with Learnt Couplings\" presents an innovative approach to enhancing quantum annealing processes by leveraging learned couplings derived from data. The authors introduce a novel methodology where quadratic unconstrained binary optimization (QUBO) forms are generated through gradient backpropagation, facilitating effective problem-solving in quantum computing. The findings demonstrate significant improvements in the optimization results when utilizing learned couplings over traditional static couplings, as evidenced by empirical evaluations on various benchmark problems.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its contribution to the integration of machine learning techniques with quantum computing, which addresses the limitations of conventional quantum annealing methods. The empirical results show promising performance enhancements, establishing the significance of learned couplings in practical applications. However, the paper suffers from several clarity issues, particularly in the presentation of mathematical formulations and the consistency of terminology. Additionally, while the proposed method shows promise, the scalability and generalizability of the approach across diverse problem domains require further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by inconsistent terminology and notation, which may confuse readers. The authors need to ensure that all acronyms are defined upon first use and that mathematical symbols are consistently formatted. While the novelty of integrating learned couplings into quantum annealing is commendable, the paper would benefit from more thorough explanations of the methodology and clearer definitions of key variables. Regarding reproducibility, the authors commit to releasing their code implemented in PyTorch, which is a positive step towards enabling others to validate and build upon their work.\n\n# Summary Of The Review\nOverall, this paper presents an intriguing approach to improving quantum annealing via learned couplings, showcasing significant empirical results. However, clarity and consistency issues detract from the overall quality, necessitating revisions to enhance comprehension and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach for learning Quadratic Unconstrained Binary Optimization (QUBO) formulations tailored to specific computer vision problems, such as graph matching, point set registration, and rotation estimation. The authors propose a methodology that is agnostic to the type of QUBO solver used and demonstrate its effectiveness through empirical evaluations on a limited set of datasets. However, the applicability of their approach to other domains, including natural language processing and bioinformatics, is not explored. The findings suggest that their method has potential but may be limited in scope and generalizability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its demonstration of a QUBO learning method specifically for computer vision tasks, which is a relevant and timely area in the context of quantum computing. However, the weaknesses are significant: the methodology lacks exploration beyond the tested problems, limiting its versatility. The comparative analysis against classical methods is also insufficient, as it only includes a few algorithms. Furthermore, the paper does not investigate the performance of different QUBO solvers or the scalability of the approach with larger or more complex problems. The limited datasets used for experimentation raise questions about the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but it could benefit from a more comprehensive discussion of its implications and a broader context within quantum machine learning. The novelty is present, particularly in the application to specific problems, but the lack of exploration into transfer learning, unsupervised methods, or ethical considerations diminishes its impact. Reproducibility is not thoroughly addressed, as the paper does not provide enough details on the computational resources or training time required for their method compared to traditional approaches.\n\n# Summary Of The Review\nWhile the paper introduces an interesting method for learning QUBO formulations in computer vision, its contributions are somewhat limited due to the narrow scope of tested problems and insufficient comparative analysis with classical methods. Expanding the evaluation and addressing the broader implications of their findings could strengthen the paper's significance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents QuAnt, a novel approach for learning Quadratic Unconstrained Binary Optimization (QUBO) forms using gradient backpropagation, which enhances quantum annealing techniques for combinatorial optimization problems. It introduces a meta-learning framework that captures relationships between problem instances and their QUBO representations through a neural network. The methodology includes a unique training strategy utilizing a contrastive loss function \\( L_{gap} \\) and other components to effectively optimize QUBO representations. Experimental results demonstrate QuAnt's superiority over several classical and quantum baselines, showcasing its effectiveness across various problem types and highlighting its potential in quantum machine learning.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to learning QUBOs through a meta-learning framework and the introduction of a well-defined training strategy that accommodates the non-differentiability of traditional solvers. The empirical evaluation is robust, with comprehensive baseline comparisons and statistical analyses that validate the proposed method. However, a potential weakness is the lack of a thorough exploration of the limitations and specific scenarios where QuAnt may not perform as well, which could provide a more balanced view of its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings, making it accessible to readers familiar with quantum annealing and machine learning. The quality of the experimental evaluation, including the use of diverse datasets and rigorous statistical methods, enhances the reproducibility of results. The novelty of the approach is notable, particularly in the context of integrating meta-learning with quantum optimization, which positions it as a significant contribution to the field.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the intersection of quantum machine learning and combinatorial optimization through its innovative QuAnt framework. The thorough empirical evaluation and statistical robustness further reinforce its significance, though a more comprehensive discussion of limitations would improve the overall assessment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents QuAnt, a novel framework for learning Quadratic Unconstrained Binary Optimization (QUBO) formulations suitable for quantum annealers. The authors propose a methodology that involves regressing dense QUBOs and adapting them for quantum hardware. Key findings indicate that while QuAnt achieves competitive performance compared to existing methods, it does not surpass specialized classical algorithms in various scenarios. The paper also highlights the challenges faced in training on noisy data and the limitations imposed by current quantum hardware.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to leveraging machine learning for QUBO formulation, which could potentially streamline problem-solving in quantum computing. The comparative analysis with classical methods provides valuable insights into the framework's performance. However, notable weaknesses include the framework's lack of generalizability, as evidenced by its poor performance on problem instances deviating from the training distribution. Additionally, reliance on minor embeddings may degrade solution quality, and the potential to specialize the learning approach for specific problem types was not fully explored, limiting the overall effectiveness of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making the methodology and findings accessible. However, the clarity could be improved by providing more detailed explanations of the challenges encountered during implementation and the implications of training on noisy data. While the novelty of combining machine learning with quantum optimization is evident, the lack of robustness in various scenarios raises questions about the reproducibility of results. The authors suggest future work that could refine the approach, indicating an ongoing research trajectory that could enhance the framework's applicability.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of quantum optimization through the introduction of the QuAnt framework. However, its limitations in generalizability and performance under noise highlight the need for further refinement and specialization. The potential for future work is promising, suggesting avenues for enhancing the method's robustness and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"QUANT: Quantum Annealing with Learnt Couplings\" proposes a methodology for deriving Quadratic Unconstrained Binary Optimization (QUBO) formulations using neural networks and gradient backpropagation. The authors claim that their approach is general and independent of specific problem types, suggesting a broader application of their findings in quantum computer vision tasks. Despite their assertions, the methodology appears to build on existing concepts in the field, such as meta-learning and contrastive loss, without providing substantial new insights or innovations. The experimental section compares their results with state-of-the-art methods, though the evaluations primarily rely on standard accuracy metrics.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its attempt to bridge quantum computing and neural networks, which could be relevant for future research. However, the weaknesses are pronounced: the novelty of the approach is questionable, as it seems to repackage existing ideas rather than introduce groundbreaking concepts. Furthermore, the authors' claims of independence and generalizability are not well substantiated, and the evaluation lacks depth, relying heavily on familiar methods and metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in a clear and comprehensible manner, making it accessible to readers with a background in quantum computing and machine learning. However, the quality of the contributions is undermined by the lack of original thought. The novelty of the approach appears limited, as it draws heavily from established techniques without providing sufficient evidence of improvement or innovation. Reproducibility might be a concern, given the reliance on standard methods without substantial new algorithms or frameworks.\n\n# Summary Of The Review\nIn summary, while the paper presents an interesting intersection of quantum computing and machine learning, it ultimately lacks significant novelty and depth. The contributions seem to reiterate existing knowledge rather than advance the field, and the experimental validation does not convincingly demonstrate the effectiveness of the proposed methodology.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach, QuAnt, which learns Quadratic Unconstrained Binary Optimization (QUBO) forms from data using gradient backpropagation, integrating quantum annealers as custom layers within a neural network. The methodology demonstrates the ability to handle a variety of combinatorial optimization problems, particularly in the realm of computer vision, and introduces a unique contrastive loss function to enhance model performance. The findings indicate that QuAnt outperforms several classical methods, although specialized techniques still maintain superiority in specific cases, showcasing the method's robustness in noisy conditions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of quantum computing with neural networks, which opens up new avenues for optimization problems. The capability to manage multiple problem types and the introduction of a contrastive loss function are particularly noteworthy contributions. However, the analysis could benefit from a more comprehensive benchmarking against classical methods across a wider array of combinatorial optimization problems. Furthermore, while the focus on computer vision is commendable, expanding the methodology's applicability to other domains would enhance its significance. The interpretability of the learned QUBO forms remains an area for future exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers with varying levels of expertise in quantum computing and neural networks. The proposed methodology is novel and shows significant promise, though reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and datasets used. The exploration of feature importance and interpretability techniques like SHAP or LIME could further strengthen the work, especially for practitioners seeking to apply these methods in real-world scenarios.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of optimization by effectively combining quantum annealing with neural networks. While it demonstrates promising results and robustness, further exploration of its applications across diverse domains and comparisons with classical methods would enhance its impact and understanding.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents QuAnt, a novel quantum method aimed at addressing key tasks in quantum computer vision, including graph matching, 2D point cloud registration, and 3D rotation estimation. The methodology involves utilizing Quantum Unconstrained Binary Optimization (QUBO) for efficient computation while maintaining robustness against noise and incorrect correspondences. The findings demonstrate that QuAnt achieves competitive performance against state-of-the-art methods, including notable improvements in accuracy and error rates across various benchmarks, as well as requiring fewer physical qubits than existing methods, thus enhancing scalability for practical quantum applications.\n\n# Strength And Weaknesses\nStrengths of the paper include the clear demonstration of QuAnt's superiority over classical baselines in several tasks, particularly in graph matching and point set registration, where it shows significant accuracy improvements. The robustness to noise is another highlighted strength, indicating QuAnt's practical applicability in real-world scenarios with imperfect data. However, a potential weakness is the reliance on simulated annealing and quantum annealing for training, which may limit reproducibility and generalization in environments with different quantum resource availability. Additionally, while the paper discusses performance, it could further elaborate on computational efficiency and runtime comparisons.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections detailing the methodology, results, and discussions. The quality of the experimental design is commendable, with multiple benchmarks used to validate the proposed method. The novelty of QuAnt lies in its approach to integrating quantum techniques with classical computer vision tasks, showcasing both technical and empirical contributions. However, the reproducibility of results may be impacted by the dependency on specific quantum resources and the absence of detailed implementation instructions.\n\n# Summary Of The Review\nQuAnt presents a significant advancement in quantum computer vision by demonstrating competitive performance in key tasks while being robust against noise and requiring fewer physical qubits. Despite minor limitations in reproducibility, the empirical results strongly position QuAnt as a valuable tool in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"QUANT: QUANTUM ANNEALING WITH LEARNT COUPLINGS\" proposes a novel methodology for quantum annealing that incorporates learned couplings to enhance performance. The authors employ a combination of quantum computational techniques and machine learning principles to optimize the coupling strengths in QUBO formulations. The findings suggest that the proposed approach significantly outperforms traditional methods in terms of solution quality and convergence speed on benchmark problems.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative integration of machine learning with quantum annealing, which presents a fresh perspective on tackling optimization problems in this domain. Additionally, the use of figures and visual aids helps clarify complex concepts. However, the paper's weaknesses lie in its clarity and organization. The lengthy abstract, complex sentence structures, and the use of technical jargon without sufficient explanation may hinder accessibility for a broader audience. Furthermore, inconsistent formatting issues detract from the professionalism of the presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by long sentences, dense descriptions of technical concepts, and insufficient definitions of key terms like \"quantum primacy\" and \"QUBO.\" Although the methodology is novel, the presentation lacks coherence due to paragraph length and a lack of explicit references to figures in the text. The reproducibility of the results may be affected by the dense explanations of loss functions and the overall flow, which could deter readers from fully understanding the methodology and its implications.\n\n# Summary Of The Review\nOverall, while the paper presents an innovative approach to quantum annealing that merges machine learning with quantum methods, its clarity and organization need significant improvement. The insights gained from the proposed methodology are valuable, but the presentation could benefit from simplifying complex concepts and enhancing readability for a wider audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.877084720657495,
    -1.6710737810014737,
    -1.8072047483176406,
    -1.8048038296535938,
    -2.0402990452649363,
    -1.7390717813344023,
    -1.532563820764796,
    -2.018513040016109,
    -1.920053213019065,
    -1.7886280930409262,
    -1.5772078412654655,
    -1.596704432143025,
    -1.6740356421966653,
    -1.63962253826764,
    -1.8177427998933353,
    -1.585328147712728,
    -1.745901354198631,
    -1.828971655095444,
    -1.7235620139866972,
    -1.9057077362892612,
    -2.000290203834194,
    -1.7178452279953251,
    -1.7235851716641704,
    -1.8037586291578005,
    -1.8141031397999763,
    -1.7269949303376957,
    -1.78269550426709,
    -1.873874991284111,
    -1.7240329793631082
  ],
  "logp_cond": [
    [
      0.0,
      -2.613945808838977,
      -2.6285446194445514,
      -2.5894462007914885,
      -2.6637596853511796,
      -2.630084261239859,
      -2.698832402554657,
      -2.6000462601464487,
      -2.587628843737627,
      -2.7033229296298393,
      -2.6251287847896982,
      -2.7332235493174992,
      -2.60978411615438,
      -2.60464091857404,
      -2.629264168320062,
      -2.6167768241596097,
      -2.682256200841702,
      -2.6356183561261286,
      -2.67508344132565,
      -2.6575415031162444,
      -2.693803598050379,
      -2.689052498949176,
      -2.6727118709164195,
      -2.6575974000990548,
      -2.716014761822813,
      -2.6817135570432464,
      -2.6812160492583716,
      -2.6727010202003485,
      -2.7177253977355584
    ],
    [
      -1.2933010874150244,
      0.0,
      -1.177975302870343,
      -1.1024505498140869,
      -1.212369081498921,
      -1.1549999045943669,
      -1.326085699663972,
      -1.1171097926895641,
      -1.1410656247290056,
      -1.2944683563312103,
      -1.078278934435066,
      -1.3879745765169715,
      -1.2058824198311995,
      -1.1804690949585839,
      -1.2307210521052703,
      -1.2209482476297089,
      -1.3313340545696415,
      -1.231091087895767,
      -1.3109652564812317,
      -1.1915844929492498,
      -1.2967583648571834,
      -1.339788703501217,
      -1.2930223348395418,
      -1.270528699295976,
      -1.27817757650632,
      -1.3397798245960413,
      -1.2205420313190547,
      -1.2265053983833423,
      -1.3334473558307776
    ],
    [
      -1.4234398061060183,
      -1.2957856936991152,
      0.0,
      -1.2588816117125998,
      -1.358579632658243,
      -1.331980234726761,
      -1.4108850865072033,
      -1.3307223516008617,
      -1.281777972927163,
      -1.389294425079503,
      -1.271178154547861,
      -1.5276718523492459,
      -1.3356454317046187,
      -1.2946689607992263,
      -1.2350511145308847,
      -1.3688311969735,
      -1.432959144554091,
      -1.369320244633188,
      -1.4875146650366007,
      -1.3530699629199492,
      -1.4294921198719084,
      -1.460103765640258,
      -1.3990943727788212,
      -1.4294414118327037,
      -1.409905477929073,
      -1.4262164043015992,
      -1.3904015983024045,
      -1.3976463477726542,
      -1.4307827805750037
    ],
    [
      -1.3440630025032207,
      -1.2202036371088085,
      -1.3064098201225813,
      0.0,
      -1.2764063107002963,
      -1.3743844762929218,
      -1.40978096218779,
      -1.2587386481376823,
      -1.175599625401445,
      -1.3001178131692817,
      -1.238520679853681,
      -1.5701941576185818,
      -1.2360010726679174,
      -1.2519189426486272,
      -1.2931896526897055,
      -1.2394942319896838,
      -1.407162384732801,
      -1.2812018009362327,
      -1.4406882371832017,
      -1.211195547701524,
      -1.4283227006476835,
      -1.3808262697402154,
      -1.4868591584333823,
      -1.3409539056481783,
      -1.3817847808272363,
      -1.3673401926592108,
      -1.3898122751370026,
      -1.4338294038320045,
      -1.461168041641014
    ],
    [
      -1.6975023732941479,
      -1.550356729087139,
      -1.6226655340729057,
      -1.567982201986557,
      0.0,
      -1.6665201928535525,
      -1.5858205922836153,
      -1.5533638032219113,
      -1.519563680886865,
      -1.5540853857934978,
      -1.5780313558717691,
      -1.7803374886053374,
      -1.5334517334546909,
      -1.5556259388199807,
      -1.5605678671544378,
      -1.5737027749024313,
      -1.5825035098739224,
      -1.5350032831161398,
      -1.6796833625478231,
      -1.5037212973734173,
      -1.6182260532089034,
      -1.629587347181946,
      -1.7119491192974643,
      -1.636100117032026,
      -1.6459818911447415,
      -1.599926009661287,
      -1.6211231431664241,
      -1.7058652010752413,
      -1.6992743443241431
    ],
    [
      -1.3544931393188993,
      -1.2382147209554255,
      -1.2680158768708498,
      -1.2461381751292406,
      -1.3096843761291501,
      0.0,
      -1.3786942980962136,
      -1.3062806793819148,
      -1.2568089092028938,
      -1.3353632200142602,
      -1.254994344056319,
      -1.4226022857695244,
      -1.2274945101782628,
      -1.2441409132558914,
      -1.3074853290514845,
      -1.2514082528439974,
      -1.4215218926944002,
      -1.2920374165481683,
      -1.3427014303419538,
      -1.3352598936747793,
      -1.378250155751576,
      -1.3834315536476025,
      -1.3652625292384866,
      -1.3531277687203402,
      -1.392139686580565,
      -1.3676839218908745,
      -1.3507830502977973,
      -1.372167891524276,
      -1.3796126735636824
    ],
    [
      -1.2997263876239897,
      -1.231277360503606,
      -1.231110585060031,
      -1.167431556187467,
      -1.1618972048602054,
      -1.2593729536709257,
      0.0,
      -1.2372566848460846,
      -1.1609002461118092,
      -1.2193776268238472,
      -1.2279035932473734,
      -1.291467384516387,
      -1.1856150694099796,
      -1.2188128057027738,
      -1.2047441776174292,
      -1.1850730927946562,
      -1.2883541902425406,
      -1.1998962430150686,
      -1.2762335190214091,
      -1.1867641267594118,
      -1.3139158924789072,
      -1.2121636966303426,
      -1.29154271078718,
      -1.2658635575019084,
      -1.2700154175589686,
      -1.2042604254822797,
      -1.262925018304138,
      -1.2746685100717856,
      -1.220387673825032
    ],
    [
      -1.5075254576053954,
      -1.4506650001161725,
      -1.5356728913954978,
      -1.4169500807898385,
      -1.4915585339810786,
      -1.5575565444245019,
      -1.6784130710150176,
      0.0,
      -1.466785106715259,
      -1.560992931620155,
      -1.4691160465479176,
      -1.7189240620248694,
      -1.4793352571632468,
      -1.5017669322644616,
      -1.4901807717038262,
      -1.4818724599215751,
      -1.6128622154564334,
      -1.5049353384095363,
      -1.6348903341497714,
      -1.503262638136544,
      -1.5930362165150598,
      -1.5840641144036782,
      -1.6516235149661984,
      -1.5437552628408064,
      -1.533388393147756,
      -1.5913225991804156,
      -1.5931071735683888,
      -1.5809677187375615,
      -1.683198572565658
    ],
    [
      -1.4774229131942427,
      -1.448278947056938,
      -1.460752991211027,
      -1.371386704785857,
      -1.4468758452178474,
      -1.5284718047517807,
      -1.5505188184469385,
      -1.4066146666527193,
      0.0,
      -1.4501353545633673,
      -1.4333312330114698,
      -1.6834889298447104,
      -1.3903393066223262,
      -1.4023937038001602,
      -1.4087420990302304,
      -1.4283468926168619,
      -1.5405364495370462,
      -1.4749645157598248,
      -1.5734120833222731,
      -1.4249575400836447,
      -1.555547086093776,
      -1.4914649057244518,
      -1.5946394127343393,
      -1.5036694039052056,
      -1.549794821277314,
      -1.488373089053471,
      -1.5193114209419982,
      -1.5853842369195985,
      -1.5833369577834686
    ],
    [
      -1.4882530181358808,
      -1.406867306301037,
      -1.3744536824889737,
      -1.28617067604113,
      -1.334841375691533,
      -1.4364836457613326,
      -1.411799614365034,
      -1.3797116932230604,
      -1.3177312902818419,
      0.0,
      -1.420773634197,
      -1.5402483553600033,
      -1.3504108992187454,
      -1.296837357429646,
      -1.341365496703015,
      -1.3377972605366493,
      -1.4333549091543532,
      -1.379758979095108,
      -1.426010595338421,
      -1.3200989354848705,
      -1.4769059817446633,
      -1.404655378301266,
      -1.5035674348586243,
      -1.4084585366547449,
      -1.4093415567177987,
      -1.3692384636110313,
      -1.470890821781983,
      -1.4972935354913197,
      -1.489045920307981
    ],
    [
      -1.2096748480519555,
      -0.9893971488545822,
      -1.0241718785626464,
      -0.9826250147473841,
      -1.0689657320598676,
      -1.0821853220631996,
      -1.2115319632967325,
      -1.0181931375906565,
      -1.0055360856214495,
      -1.1874357646392462,
      0.0,
      -1.2967525973521263,
      -1.0952112096685351,
      -1.094372999083735,
      -1.1172233176995399,
      -1.112568603967636,
      -1.1730799480337364,
      -1.1296315048146777,
      -1.205964504118402,
      -1.13032659114459,
      -1.2123500437705415,
      -1.2338181868497182,
      -1.1641774014285111,
      -1.096293940266405,
      -1.1447356491857554,
      -1.1667365201444446,
      -1.1070788217558298,
      -1.1832085713405172,
      -1.2303429882052122
    ],
    [
      -1.3663282362152465,
      -1.297121183060619,
      -1.2988636231071966,
      -1.3341503949689792,
      -1.303128142297235,
      -1.2668157553627546,
      -1.2796630098276063,
      -1.3085031884262186,
      -1.3182634896390222,
      -1.3347533764809913,
      -1.2948068405532966,
      0.0,
      -1.3079484473884104,
      -1.3135316761303633,
      -1.3034543083760173,
      -1.3178089361910363,
      -1.3082489456520774,
      -1.3499539609299709,
      -1.3387010422486505,
      -1.322941397792006,
      -1.3051114968961854,
      -1.2960225763566036,
      -1.2747009639605882,
      -1.3412757260210835,
      -1.3420137113713142,
      -1.329164056713591,
      -1.2933192406923781,
      -1.310968665163629,
      -1.2993538766158343
    ],
    [
      -1.271166138137158,
      -1.2016728248326196,
      -1.266897771139503,
      -1.1383274961051766,
      -1.1653358746000884,
      -1.2210412757278326,
      -1.2581323789420407,
      -1.1642050079289035,
      -1.1118504659262816,
      -1.2453887682212519,
      -1.2765749915133802,
      -1.4252552030549592,
      0.0,
      -1.180847549962638,
      -1.2224018739177671,
      -1.118952155281059,
      -1.3115555206498852,
      -1.1558394736908186,
      -1.3593166123765366,
      -1.1760985026364834,
      -1.3148799860873652,
      -1.2365995882042826,
      -1.3198634965137948,
      -1.3148061267103655,
      -1.304853621210806,
      -1.2658012979555073,
      -1.2966129032132314,
      -1.3134269316079497,
      -1.3244643504255238
    ],
    [
      -1.2483568738949742,
      -1.1691652340067837,
      -1.1787185344465998,
      -1.076108990440263,
      -1.1725593339601657,
      -1.164593320746924,
      -1.2579918643395225,
      -1.180367367819473,
      -1.1328790507271325,
      -1.1630853832316967,
      -1.1896886085946443,
      -1.3660518593078523,
      -1.10488942568209,
      0.0,
      -1.0773543682318008,
      -1.0951014080565689,
      -1.2740144223967005,
      -1.1413566258547194,
      -1.2756249057013682,
      -1.116859900403692,
      -1.2500278693102267,
      -1.2057077947787942,
      -1.2997810640976495,
      -1.2327570104899328,
      -1.2913566987331622,
      -1.2026890712042608,
      -1.2890148221023858,
      -1.258224736731624,
      -1.2947550375080423
    ],
    [
      -1.424539560556944,
      -1.3291999966258365,
      -1.3048907486029409,
      -1.3013001254353596,
      -1.3404774734789728,
      -1.3701096664640964,
      -1.4444144356543038,
      -1.3067792623689416,
      -1.2921790378984015,
      -1.3672008471453023,
      -1.3675515966414091,
      -1.5585090185032,
      -1.3038843978386099,
      -1.275145445292238,
      0.0,
      -1.3309996293177917,
      -1.4661905163187925,
      -1.3315172786187859,
      -1.4499667062929318,
      -1.2885300523543775,
      -1.4199325039655728,
      -1.378044786412775,
      -1.4244463110835395,
      -1.4646532601295805,
      -1.4387122268086263,
      -1.3711499054666316,
      -1.3549978341358249,
      -1.4693128324412894,
      -1.4477239571779301
    ],
    [
      -1.2042508788814175,
      -1.1286598644898582,
      -1.1582255593919708,
      -0.9968930556802588,
      -1.1151094605028098,
      -1.1758693957952795,
      -1.1789465663436307,
      -1.113506097274293,
      -1.073552876188313,
      -1.1678173330062112,
      -1.139850709358751,
      -1.3301397099294647,
      -1.0119444748842892,
      -1.0798996476650555,
      -1.0981780723669905,
      0.0,
      -1.2678721254952239,
      -1.1075449798686219,
      -1.2466634851474445,
      -1.0795331206928191,
      -1.268798344436735,
      -1.2178112928817755,
      -1.2755513990479788,
      -1.2446537570434015,
      -1.2633077404768551,
      -1.1861604285577114,
      -1.2406133618870046,
      -1.2359485506811947,
      -1.2529295667874196
    ],
    [
      -1.4444909282495484,
      -1.4124433998736687,
      -1.3958192787634438,
      -1.4058167829992025,
      -1.3162215332801126,
      -1.4898661546973968,
      -1.4860924236207487,
      -1.3532360351018897,
      -1.3518466990855558,
      -1.3904599095714496,
      -1.3904566320861809,
      -1.5306163502375434,
      -1.427295734359214,
      -1.391322355350125,
      -1.4033910935306337,
      -1.4380651285344237,
      0.0,
      -1.40513075163883,
      -1.4033827488599546,
      -1.363433664007059,
      -1.3722396045289618,
      -1.443327241795494,
      -1.4211417957042103,
      -1.39216771054528,
      -1.361621000719342,
      -1.4107424355391156,
      -1.3891813475038797,
      -1.4702159574724198,
      -1.4859235670957627
    ],
    [
      -1.4595625470398905,
      -1.4078179822631303,
      -1.4691230486175595,
      -1.3384020944252293,
      -1.3678048509165417,
      -1.4212136903246708,
      -1.4784566648621162,
      -1.3529934735187716,
      -1.3326634062410982,
      -1.4420972819159827,
      -1.4375254325054252,
      -1.5845533884906666,
      -1.3273124831635754,
      -1.3745410583773598,
      -1.361773622162913,
      -1.3552988429016717,
      -1.4683842994174587,
      0.0,
      -1.5627639823521642,
      -1.3600379925782828,
      -1.4834100316688295,
      -1.4572879602577076,
      -1.4720045957948162,
      -1.5035234140638427,
      -1.525966500673169,
      -1.4057355668672442,
      -1.4711897748323493,
      -1.4753544042562892,
      -1.5140990188667618
    ],
    [
      -1.3610265854158927,
      -1.330756526954516,
      -1.3406894663530335,
      -1.2686944690413386,
      -1.3400552650413273,
      -1.3680079852671159,
      -1.3906609979032576,
      -1.2553167632387365,
      -1.2750886614849615,
      -1.3292703679061304,
      -1.2914416525272419,
      -1.4825106856862114,
      -1.309448032900417,
      -1.3586680044125523,
      -1.3614811559610085,
      -1.3247527067912879,
      -1.3069400558935216,
      -1.3354789172521098,
      0.0,
      -1.2955416423467725,
      -1.3269861960365164,
      -1.3984168589417156,
      -1.3801108520420948,
      -1.3101485488264373,
      -1.3432285672987867,
      -1.3480905333230013,
      -1.355356812517377,
      -1.3451876782055814,
      -1.4633316874350695
    ],
    [
      -1.574329685671801,
      -1.4069321518405844,
      -1.4744433643617885,
      -1.3171448013514675,
      -1.3807607263384414,
      -1.5714897181520755,
      -1.508958854538284,
      -1.4364040734446644,
      -1.4056366913188523,
      -1.478538889571783,
      -1.4526047459238676,
      -1.6583142652779779,
      -1.3907998962854173,
      -1.4241522766416828,
      -1.38553581716866,
      -1.4239836435421662,
      -1.5466963353462553,
      -1.3600107079653838,
      -1.5521990650227429,
      0.0,
      -1.5594309681618603,
      -1.4812896933794029,
      -1.5884415214593206,
      -1.507635691736913,
      -1.4879387285439367,
      -1.5011478393470121,
      -1.5277179102943261,
      -1.5799797640946305,
      -1.5769588611578063
    ],
    [
      -1.6519931411559237,
      -1.5639290261381695,
      -1.5932284531206065,
      -1.5393368944446342,
      -1.539010809441131,
      -1.6013644554256854,
      -1.6672899749095498,
      -1.543202931359166,
      -1.5442495599545598,
      -1.6111855521991518,
      -1.6265559975449346,
      -1.7315849356511608,
      -1.5782098844197257,
      -1.5576229997615592,
      -1.528219931902666,
      -1.5913703473326615,
      -1.6192847669714199,
      -1.5611919927753453,
      -1.5994792400857434,
      -1.6048740882678771,
      0.0,
      -1.6345888444404815,
      -1.6732915972746538,
      -1.5598203658808247,
      -1.5891611609287182,
      -1.556389074096378,
      -1.5739570421118687,
      -1.5953297968327564,
      -1.6753320900875037
    ],
    [
      -1.3721455288524171,
      -1.3304702221855085,
      -1.3532064866540559,
      -1.2653747568250282,
      -1.2459332599833841,
      -1.3824081656772735,
      -1.281744688108429,
      -1.2909361654065745,
      -1.2505730463806042,
      -1.265656444240465,
      -1.325126456288356,
      -1.420444161831952,
      -1.244669294780137,
      -1.2647311335923064,
      -1.2705709342881852,
      -1.2678797542998208,
      -1.3621606360858458,
      -1.246718153289795,
      -1.386675907937091,
      -1.2111442458418626,
      -1.3722387814072612,
      0.0,
      -1.3929217829773741,
      -1.3226251242667382,
      -1.3544825034982308,
      -1.2557097206598877,
      -1.3036596915786869,
      -1.3679471445282836,
      -1.3318683258147725
    ],
    [
      -1.4523484362304413,
      -1.3567777969446773,
      -1.3641516198853851,
      -1.4040454565357046,
      -1.4170642873127883,
      -1.3609810485277065,
      -1.4356282749630007,
      -1.3878559152444614,
      -1.4143553440301175,
      -1.4000914858074243,
      -1.3516980074110416,
      -1.4526196602718395,
      -1.397406916463099,
      -1.3927687801454072,
      -1.3865514135921866,
      -1.4102383122439874,
      -1.4077145284205885,
      -1.3841691467120856,
      -1.4589567698266226,
      -1.4251310430181017,
      -1.420372042786101,
      -1.4302441029048865,
      0.0,
      -1.388540679861959,
      -1.4117809646765511,
      -1.4203552701605553,
      -1.3606371625383074,
      -1.396877043881622,
      -1.4070860270374854
    ],
    [
      -1.4085101191048097,
      -1.3110621152900324,
      -1.345063177505803,
      -1.285211390443332,
      -1.320075265236303,
      -1.3757451542910588,
      -1.4190361012727761,
      -1.3011027330971738,
      -1.2883436670335457,
      -1.339468794494715,
      -1.2703805467135434,
      -1.5054851362994222,
      -1.375991290993647,
      -1.3062001428663623,
      -1.3842073396649242,
      -1.3290202675619347,
      -1.3753059580399014,
      -1.365764738621978,
      -1.410501547656947,
      -1.244543965339579,
      -1.3889551832419011,
      -1.3946799335201703,
      -1.455249248818847,
      0.0,
      -1.3435053953538427,
      -1.3781292095011357,
      -1.3393640312553192,
      -1.4169431395552972,
      -1.4433171563476133
    ],
    [
      -1.522308967827616,
      -1.4471347715624439,
      -1.3986391138527214,
      -1.4611927551917905,
      -1.440891853269556,
      -1.4711204545070253,
      -1.5045782117934277,
      -1.423123707459609,
      -1.412716344842942,
      -1.4261801270424062,
      -1.408383514757758,
      -1.5405600203214507,
      -1.4521931388654106,
      -1.440612254327592,
      -1.4313244704159962,
      -1.452722435208479,
      -1.425075451778437,
      -1.4738677941076372,
      -1.470481680355041,
      -1.3899623912665855,
      -1.4594012170643997,
      -1.4499402416199807,
      -1.4655538306473217,
      -1.4047303910285678,
      0.0,
      -1.496122634827757,
      -1.4289003159778364,
      -1.4639167585838802,
      -1.455399545812225
    ],
    [
      -1.3791969015244838,
      -1.3802771861706353,
      -1.389063185990192,
      -1.2364481165666683,
      -1.2699783302479202,
      -1.402301388687312,
      -1.341967830504218,
      -1.3023664508063555,
      -1.223143506567183,
      -1.2642703989103852,
      -1.3348146149812672,
      -1.4545291243554117,
      -1.2539832683792296,
      -1.2586944317543214,
      -1.2789817303444482,
      -1.2493963368436918,
      -1.3298924856040435,
      -1.2355669616792233,
      -1.3695674101579685,
      -1.2483152659715075,
      -1.3579513334796767,
      -1.2940078895228562,
      -1.382749261436369,
      -1.3141594603302487,
      -1.3666438103514946,
      0.0,
      -1.3781913813612607,
      -1.3891326161780388,
      -1.3393931246058877
    ],
    [
      -1.4374720839278246,
      -1.3310256567257754,
      -1.342096715652666,
      -1.395990564312086,
      -1.3047947845680887,
      -1.3768826900163826,
      -1.4007429334699548,
      -1.3337948029279012,
      -1.3322260043784075,
      -1.3762037271031906,
      -1.3003349923661496,
      -1.5129124097784816,
      -1.3904976132954232,
      -1.4077954883200945,
      -1.3572037744422851,
      -1.399784641525932,
      -1.3601884465935938,
      -1.368710355539589,
      -1.4115945958699825,
      -1.366808095084748,
      -1.40087706534614,
      -1.4112192744281564,
      -1.392447950976623,
      -1.3891882176307178,
      -1.380821024080061,
      -1.421124781029318,
      0.0,
      -1.4628591780791769,
      -1.426144198483356
    ],
    [
      -1.51854735402047,
      -1.3892247967632823,
      -1.380328592365989,
      -1.4076324465666028,
      -1.4645929612557174,
      -1.384137163235684,
      -1.5043242572371238,
      -1.382650585486691,
      -1.3633748825289935,
      -1.5045634550695948,
      -1.4239859926630654,
      -1.5487886105515696,
      -1.387469826555255,
      -1.4065910033889142,
      -1.4244381466139542,
      -1.4426324919867413,
      -1.5195651476671066,
      -1.4098250682312834,
      -1.5110886279953968,
      -1.4743799845103578,
      -1.4041452904086236,
      -1.5191396498624221,
      -1.4499776910416464,
      -1.4273597758116798,
      -1.463241794602211,
      -1.4725608507389623,
      -1.454377433624138,
      0.0,
      -1.5191823437565817
    ],
    [
      -1.461397910819783,
      -1.3776256003380098,
      -1.3521961652591035,
      -1.3724565833046247,
      -1.3519650944099737,
      -1.400947252087961,
      -1.3473541200293215,
      -1.4187993612435459,
      -1.3642676258068132,
      -1.3531495379749572,
      -1.3906456572238874,
      -1.437250869351689,
      -1.3507236778378593,
      -1.3526167812178784,
      -1.3754766486387595,
      -1.387105249906542,
      -1.418557555690014,
      -1.3610866980703376,
      -1.4751537139677124,
      -1.3383745297018557,
      -1.409112197264074,
      -1.3457066335410175,
      -1.4095416379880814,
      -1.387087564789737,
      -1.391706352183466,
      -1.3513074604364403,
      -1.387869362667019,
      -1.4422917555910602,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2631389118185181,
      0.24854010121294356,
      0.2876385198660065,
      0.2133250353063154,
      0.24700045941763582,
      0.17825231810283793,
      0.2770384605110463,
      0.28945587691986807,
      0.17376179102765565,
      0.25195593586779674,
      0.14386117133999576,
      0.26730060450311477,
      0.2724438020834552,
      0.24782055233743305,
      0.2603078964978853,
      0.19482851981579286,
      0.24146636453136638,
      0.20200127933184486,
      0.2195432175412506,
      0.18328112260711604,
      0.18803222170831901,
      0.2043728497410755,
      0.21948732055844022,
      0.16106995883468178,
      0.19537116361424856,
      0.19586867139912334,
      0.2043837004571465,
      0.15935932292193655
    ],
    [
      0.3777726935864494,
      0.0,
      0.4930984781311307,
      0.5686232311873869,
      0.45870469950255277,
      0.5160738764071069,
      0.3449880813375017,
      0.5539639883119096,
      0.5300081562724681,
      0.3766054246702635,
      0.5927948465664077,
      0.2830992044845022,
      0.46519136117027426,
      0.49060468604288987,
      0.44035272889620347,
      0.45012553337176486,
      0.33973972643183226,
      0.4399826931057067,
      0.360108524520242,
      0.4794892880522239,
      0.3743154161442903,
      0.3312850775002567,
      0.378051446161932,
      0.40054508170549785,
      0.39289620449515383,
      0.33129395640543247,
      0.45053174968241905,
      0.4445683826181315,
      0.3376264251706962
    ],
    [
      0.3837649422116223,
      0.5114190546185253,
      0.0,
      0.5483231366050407,
      0.44862511565939744,
      0.47522451359087947,
      0.3963196618104372,
      0.47648239671677883,
      0.5254267753904776,
      0.4179103232381376,
      0.5360265937697795,
      0.2795328959683947,
      0.47155931661302186,
      0.5125357875184142,
      0.5721536337867559,
      0.43837355134414047,
      0.37424560376354954,
      0.43788450368445253,
      0.31969008328103987,
      0.45413478539769137,
      0.37771262844573217,
      0.34710098267738254,
      0.40811037553881935,
      0.3777633364849369,
      0.3972992703885676,
      0.3809883440160413,
      0.41680315001523605,
      0.4095584005449864,
      0.3764219677426368
    ],
    [
      0.460740827150373,
      0.5846001925447852,
      0.4983940095310124,
      0.0,
      0.5283975189532975,
      0.43041935336067194,
      0.39502286746580384,
      0.5460651815159114,
      0.6292042042521488,
      0.5046860164843121,
      0.5662831497999128,
      0.23460967203501193,
      0.5688027569856764,
      0.5528848870049665,
      0.5116141769638882,
      0.56530959766391,
      0.3976414449207928,
      0.5236020287173611,
      0.36411559247039205,
      0.5936082819520698,
      0.37648112900591024,
      0.4239775599133784,
      0.3179446712202114,
      0.46384992400541547,
      0.42301904882635744,
      0.43746363699438295,
      0.4149915545165912,
      0.37097442582158924,
      0.34363578801257977
    ],
    [
      0.34279667197078845,
      0.4899423161777974,
      0.4176335111920306,
      0.4723168432783793,
      0.0,
      0.37377885241138387,
      0.45447845298132106,
      0.4869352420430251,
      0.5207353643780712,
      0.4862136594714386,
      0.4622676893931672,
      0.25996155665959897,
      0.5068473118102454,
      0.48467310644495565,
      0.4797311781104985,
      0.46659627036250506,
      0.4577955353910139,
      0.5052957621487966,
      0.3606156827171132,
      0.536577747891519,
      0.4220729920560329,
      0.41071169808299035,
      0.32834992596747203,
      0.40419892823291037,
      0.39431715412019486,
      0.44037303560364927,
      0.4191759020985122,
      0.334433844189695,
      0.3410247009407932
    ],
    [
      0.38457864201550307,
      0.5008570603789768,
      0.4710559044635525,
      0.4929336062051617,
      0.4293874052052522,
      0.0,
      0.36037748323818874,
      0.4327911019524875,
      0.48226287213150854,
      0.4037085613201421,
      0.4840774372780834,
      0.3164694955648779,
      0.5115772711561395,
      0.49493086807851094,
      0.4315864522829178,
      0.48766352849040495,
      0.3175498886400021,
      0.44703436478623404,
      0.39637035099244855,
      0.40381188765962306,
      0.36082162558282627,
      0.3556402276867998,
      0.37380925209591576,
      0.3859440126140621,
      0.34693209475383724,
      0.3713878594435278,
      0.388288731036605,
      0.3669038898101262,
      0.3594591077707199
    ],
    [
      0.2328374331408063,
      0.30128646026119,
      0.301453235704765,
      0.36513226457732895,
      0.3706666159045906,
      0.2731908670938703,
      0.0,
      0.2953071359187114,
      0.37166357465298683,
      0.3131861939409488,
      0.3046602275174226,
      0.24109643624840893,
      0.3469487513548164,
      0.3137510150620222,
      0.3278196431473668,
      0.3474907279701398,
      0.24420963052225542,
      0.33266757774972744,
      0.2563303017433869,
      0.34579969400538424,
      0.21864792828588886,
      0.3204001241344534,
      0.24102110997761605,
      0.26670026326288765,
      0.2625484032058274,
      0.3283033952825163,
      0.26963880246065797,
      0.2578953106930104,
      0.3121761469397639
    ],
    [
      0.5109875824107137,
      0.5678480398999366,
      0.4828401486206113,
      0.6015629592262706,
      0.5269545060350305,
      0.46095649559160723,
      0.3400999690010915,
      0.0,
      0.55172793330085,
      0.45752010839595414,
      0.5493969934681915,
      0.29958897799123974,
      0.5391777828528623,
      0.5167461077516475,
      0.5283322683122829,
      0.536640580094534,
      0.4056508245596757,
      0.5135777016065728,
      0.38362270586633773,
      0.5152504018795652,
      0.4254768235010493,
      0.43444892561243087,
      0.3668895250499107,
      0.47475777717530265,
      0.485124646868353,
      0.42719044083569346,
      0.42540586644772027,
      0.4375453212785476,
      0.3353144674504511
    ],
    [
      0.4426302998248224,
      0.4717742659621271,
      0.459300221808038,
      0.5486665082332081,
      0.47317736780121766,
      0.3915814082672844,
      0.36953439457212656,
      0.5134385463663458,
      0.0,
      0.4699178584556978,
      0.48672198000759526,
      0.23656428317435463,
      0.5297139063967389,
      0.5176595092189049,
      0.5113111139888347,
      0.4917063204022032,
      0.3795167634820189,
      0.4450886972592403,
      0.34664112969679195,
      0.49509567293542034,
      0.36450612692528916,
      0.4285883072946133,
      0.3254138002847258,
      0.4163838091138594,
      0.370258391741751,
      0.4316801239655941,
      0.40074179207706684,
      0.33466897609946655,
      0.33671625523559645
    ],
    [
      0.30037507490504534,
      0.3817607867398891,
      0.4141744105519525,
      0.5024574169997962,
      0.45378671734939324,
      0.3521444472795936,
      0.3768284786758922,
      0.40891639981786576,
      0.4708968027590843,
      0.0,
      0.3678544588439261,
      0.24837973768092292,
      0.43821719382218083,
      0.4917907356112803,
      0.4472625963379111,
      0.4508308325042769,
      0.35527318388657303,
      0.40886911394581826,
      0.3626174977025052,
      0.46852915755605573,
      0.3117221112962629,
      0.3839727147396601,
      0.28506065818230186,
      0.38016955638618133,
      0.3792865363231275,
      0.4193896294298949,
      0.31773727125894324,
      0.29133455754960647,
      0.2995821727329451
    ],
    [
      0.3675329932135101,
      0.5878106924108834,
      0.5530359627028192,
      0.5945828265180815,
      0.508242109205598,
      0.4950225192022659,
      0.365675877968733,
      0.559014703674809,
      0.5716717556440161,
      0.3897720766262194,
      0.0,
      0.2804552439133392,
      0.4819966315969304,
      0.48283484218173056,
      0.4599845235659257,
      0.46463923729782963,
      0.40412789323172915,
      0.44757633645078787,
      0.3712433371470636,
      0.44688125012087543,
      0.3648577974949241,
      0.3433896544157473,
      0.4130304398369544,
      0.4809139009990606,
      0.4324721920797101,
      0.4104713211210209,
      0.4701290195096357,
      0.3939992699249484,
      0.3468648530602534
    ],
    [
      0.23037619592777858,
      0.29958324908240597,
      0.29784080903582844,
      0.2625540371740458,
      0.2935762898457901,
      0.32988867678027045,
      0.31704142231541876,
      0.28820124371680644,
      0.27844094250400286,
      0.2619510556620337,
      0.3018975915897284,
      0.0,
      0.2887559847546146,
      0.2831727560126618,
      0.29325012376700776,
      0.2788954959519887,
      0.28845548649094765,
      0.24675047121305416,
      0.25800338989437455,
      0.2737630343510191,
      0.2915929352468396,
      0.3006818557864215,
      0.32200346818243686,
      0.2554287061219416,
      0.25469072077171084,
      0.2675403754294341,
      0.3033851914506469,
      0.28573576697939607,
      0.2973505555271907
    ],
    [
      0.4028695040595074,
      0.47236281736404573,
      0.40713787105716226,
      0.5357081460914888,
      0.508699767596577,
      0.45299436646883273,
      0.4159032632546247,
      0.5098306342677619,
      0.5621851762703838,
      0.42864687397541346,
      0.3974606506832852,
      0.2487804391417061,
      0.0,
      0.49318809223402726,
      0.45163376827889823,
      0.5550834869156063,
      0.3624801215467801,
      0.5181961685058467,
      0.3147190298201288,
      0.49793713956018193,
      0.3591556561093001,
      0.4374360539923827,
      0.3541721456828706,
      0.3592295154862999,
      0.3691820209858594,
      0.408234344241158,
      0.3774227389834339,
      0.3606087105887157,
      0.34957129177114155
    ],
    [
      0.39126566437266574,
      0.4704573042608562,
      0.46090400382104013,
      0.563513547827377,
      0.4670632043074743,
      0.47502921752071603,
      0.38163067392811745,
      0.4592551704481669,
      0.5067434875405075,
      0.4765371550359432,
      0.4499339296729956,
      0.2735706789597876,
      0.5347331125855499,
      0.0,
      0.5622681700358392,
      0.5445211302110711,
      0.36560811587093944,
      0.4982659124129205,
      0.3639976325662717,
      0.5227626378639478,
      0.38959466895741324,
      0.43391474348884573,
      0.33984147416999044,
      0.4068655277777071,
      0.34826583953447776,
      0.4369334670633791,
      0.3506077161652541,
      0.381397801536016,
      0.3448675007595976
    ],
    [
      0.39320323933639134,
      0.48854280326749877,
      0.5128520512903945,
      0.5164426744579758,
      0.4772653264143625,
      0.44763313342923894,
      0.3733283642390315,
      0.5109635375243937,
      0.5255637619949338,
      0.450541952748033,
      0.4501912032519262,
      0.2592337813901353,
      0.5138584020547254,
      0.5425973546010974,
      0.0,
      0.4867431705755436,
      0.3515522835745428,
      0.48622552127454943,
      0.36777609360040353,
      0.5292127475389579,
      0.3978102959277625,
      0.43969801348056037,
      0.39329648880979584,
      0.3530895397637548,
      0.379030573084709,
      0.4465928944267037,
      0.46274496575751045,
      0.3484299674520459,
      0.3700188427154052
    ],
    [
      0.38107726883131043,
      0.4566682832228697,
      0.4271025883207571,
      0.5884350920324691,
      0.4702186872099181,
      0.4094587519174484,
      0.4063815813690972,
      0.47182205043843495,
      0.5117752715244148,
      0.41751081470651674,
      0.4454774383539768,
      0.25518843778326317,
      0.5733836728284387,
      0.5054285000476724,
      0.4871500753457374,
      0.0,
      0.317456022217504,
      0.477783167844106,
      0.3386646625652834,
      0.5057950270199087,
      0.3165298032759929,
      0.3675168548309524,
      0.30977674866474914,
      0.34067439066932637,
      0.3220204072358728,
      0.3991677191550165,
      0.34471478582572335,
      0.3493795970315332,
      0.3323985809253083
    ],
    [
      0.3014104259490826,
      0.3334579543249623,
      0.3500820754351872,
      0.3400845711994285,
      0.4296798209185184,
      0.2560351995012342,
      0.25980893057788235,
      0.3926653190967413,
      0.39405465511307525,
      0.3554414446271814,
      0.35544472211245015,
      0.21528500396108763,
      0.31860561983941693,
      0.35457899884850597,
      0.3425102606679973,
      0.3078362256642073,
      0.0,
      0.34077060255980096,
      0.3425186053386764,
      0.382467690191572,
      0.37366174966966925,
      0.3025741124031369,
      0.32475955849442073,
      0.3537336436533509,
      0.3842803534792891,
      0.3351589186595154,
      0.35672000669475135,
      0.2756853967262112,
      0.25997778710286834
    ],
    [
      0.3694091080555535,
      0.42115367283231375,
      0.3598486064778845,
      0.49056956067021473,
      0.4611668041789023,
      0.4077579647707732,
      0.3505149902333278,
      0.4759781815766724,
      0.49630824885434577,
      0.3868743731794613,
      0.3914462225900188,
      0.2444182666047774,
      0.5016591719318686,
      0.45443059671808417,
      0.4671980329325309,
      0.47367281219377233,
      0.3605873556779853,
      0.0,
      0.26620767274327983,
      0.4689336625171612,
      0.34556162342661456,
      0.37168369483773644,
      0.3569670593006278,
      0.3254482410316013,
      0.30300515442227494,
      0.4232360882281998,
      0.35778188026309476,
      0.35361725083915485,
      0.3148726362286822
    ],
    [
      0.36253542857080445,
      0.3928054870321811,
      0.38287254763366363,
      0.4548675449453585,
      0.38350674894536985,
      0.3555540287195813,
      0.3329010160834396,
      0.46824525074796064,
      0.4484733525017357,
      0.39429164608056677,
      0.4321203614594553,
      0.2410513283004858,
      0.41411398108628017,
      0.36489400957414486,
      0.3620808580256887,
      0.3988093071954093,
      0.4166219580931756,
      0.3880830967345874,
      0.0,
      0.4280203716399247,
      0.3965758179501808,
      0.32514515504498154,
      0.34345116194460235,
      0.4134134651602599,
      0.38033344668791047,
      0.37547148066369584,
      0.36820520146932023,
      0.3783743357811158,
      0.2602303265516277
    ],
    [
      0.3313780506174602,
      0.49877558444867676,
      0.4312643719274727,
      0.5885629349377937,
      0.5249470099508198,
      0.33421801813718566,
      0.3967488817509772,
      0.4693036628445968,
      0.5000710449704089,
      0.4271688467174781,
      0.4531029903653936,
      0.24739347101128328,
      0.5149078400038438,
      0.4815554596475784,
      0.5201719191206011,
      0.48172409274709493,
      0.3590114009430059,
      0.5456970283238773,
      0.3535086712665183,
      0.0,
      0.3462767681274008,
      0.4244180429098583,
      0.31726621482994055,
      0.39807204455234824,
      0.41776900774532444,
      0.40455989694224903,
      0.377989825994935,
      0.3257279721946307,
      0.3287488751314549
    ],
    [
      0.34829706267827043,
      0.43636117769602456,
      0.40706175071358763,
      0.4609533093895599,
      0.4612793943930631,
      0.3989257484085087,
      0.3330002289246443,
      0.4570872724750281,
      0.4560406438796343,
      0.38910465163504226,
      0.37373420628925946,
      0.2687052681830333,
      0.42208031941446844,
      0.44266720407263493,
      0.4720702719315282,
      0.4089198565015326,
      0.3810054368627742,
      0.43909821105884883,
      0.4008109637484507,
      0.39541611556631695,
      0.0,
      0.3657013593937126,
      0.3269986065595403,
      0.44046983795336936,
      0.4111290429054759,
      0.44390112973781615,
      0.4263331617223254,
      0.40496040700143765,
      0.32495811374669037
    ],
    [
      0.345699699142908,
      0.38737500580981665,
      0.3646387413412693,
      0.45247047117029693,
      0.471911968011941,
      0.3354370623180516,
      0.43610053988689623,
      0.4269090625887506,
      0.46727218161472095,
      0.45218878375486016,
      0.3927187717069691,
      0.29740106616337303,
      0.47317593321518814,
      0.45311409440301875,
      0.4472742937071399,
      0.4499654736955043,
      0.3556845919094793,
      0.47112707470553006,
      0.3311693200582342,
      0.5067009821534625,
      0.34560644658806394,
      0.0,
      0.324923445017951,
      0.39522010372858696,
      0.3633627244970943,
      0.4621355073354374,
      0.41418553641663824,
      0.34989808346704154,
      0.3859769021805526
    ],
    [
      0.27123673543372906,
      0.3668073747194931,
      0.3594335517787852,
      0.3195397151284658,
      0.30652088435138203,
      0.36260412313646384,
      0.28795689670116964,
      0.33572925641970897,
      0.3092298276340528,
      0.3234936858567461,
      0.37188716425312873,
      0.2709655113923308,
      0.32617825520107147,
      0.3308163915187632,
      0.33703375807198377,
      0.3133468594201829,
      0.31587064324358183,
      0.3394160249520848,
      0.26462840183754777,
      0.2984541286460687,
      0.3032131288780693,
      0.2933410687592839,
      0.0,
      0.3350444918022113,
      0.31180420698761924,
      0.30322990150361506,
      0.362948009125863,
      0.3267081277825483,
      0.31649914462668494
    ],
    [
      0.3952485100529908,
      0.49269651386776814,
      0.45869545165199743,
      0.5185472387144685,
      0.48368336392149747,
      0.4280134748667417,
      0.38472252788502437,
      0.5026558960606267,
      0.5154149621242547,
      0.4642898346630855,
      0.5333780824442571,
      0.29827349285837834,
      0.4277673381641536,
      0.4975584862914382,
      0.41955128949287634,
      0.4747383615958658,
      0.4284526711178991,
      0.4379938905358225,
      0.3932570815008536,
      0.5592146638182216,
      0.41480344591589935,
      0.4090786956376302,
      0.3485093803389534,
      0.0,
      0.4602532338039578,
      0.4256294196566648,
      0.4643945979024813,
      0.38681548960250334,
      0.36044147281018724
    ],
    [
      0.29179417197236024,
      0.3669683682375324,
      0.41546402594725484,
      0.3529103846081858,
      0.37321128653042024,
      0.34298268529295095,
      0.3095249280065486,
      0.39097943234036725,
      0.4013867949570342,
      0.38792301275757013,
      0.40571962504221837,
      0.2735431194785256,
      0.3619100009345657,
      0.3734908854723842,
      0.3827786693839801,
      0.3613807045914972,
      0.3890276880215393,
      0.34023534569233904,
      0.3436214594449354,
      0.4241407485333908,
      0.35470192273557655,
      0.3641628981799956,
      0.3485493091526546,
      0.4093727487714085,
      0.0,
      0.3179805049722193,
      0.3852028238221399,
      0.35018638121609613,
      0.3587035939877512
    ],
    [
      0.34779802881321187,
      0.34671774416706036,
      0.3379317443475036,
      0.4905468137710274,
      0.4570166000897755,
      0.3246935416503838,
      0.3850270998334777,
      0.4246284795313402,
      0.5038514237705127,
      0.4627245314273105,
      0.3921803153564285,
      0.272465805982284,
      0.4730116619584661,
      0.4683004985833743,
      0.4480131999932475,
      0.4775985934940039,
      0.3971024447336522,
      0.4914279686584724,
      0.35742752017972723,
      0.47867966436618814,
      0.369043596858019,
      0.4329870408148395,
      0.3442456689013267,
      0.412835470007447,
      0.36035111998620106,
      0.0,
      0.34880354897643495,
      0.33786231415965684,
      0.387601805731808
    ],
    [
      0.34522342033926545,
      0.45166984754131456,
      0.44059878861442403,
      0.386704939955004,
      0.4779007196990013,
      0.40581281425070737,
      0.3819525707971352,
      0.4489007013391888,
      0.45046949988868246,
      0.4064917771638994,
      0.4823605119009404,
      0.2697830944886084,
      0.3921978909716668,
      0.3749000159469955,
      0.4254917298248049,
      0.38291086274115793,
      0.42250705767349617,
      0.41398514872750103,
      0.3711009083971075,
      0.4158874091823419,
      0.38181843892095,
      0.3714762298389336,
      0.390247553290467,
      0.39350728663637224,
      0.4018744801870291,
      0.36157072323777206,
      0.0,
      0.31983632618791313,
      0.35655130578373395
    ],
    [
      0.35532763726364114,
      0.4846501945208288,
      0.49354639891812213,
      0.46624254471750826,
      0.4092820300283937,
      0.489737828048427,
      0.36955073404698724,
      0.49122440579742,
      0.5105001087551175,
      0.3693115362145163,
      0.4498889986210457,
      0.3250863807325415,
      0.4864051647288561,
      0.4672839878951969,
      0.4494368446701569,
      0.4312424992973698,
      0.35430984361700446,
      0.46404992305282766,
      0.3627863632887143,
      0.39949500677375327,
      0.4697297008754875,
      0.35473534142168894,
      0.42389730024246464,
      0.4465152154724312,
      0.4106331966819001,
      0.4013141405451488,
      0.4194975576599731,
      0.0,
      0.3546926475275294
    ],
    [
      0.2626350685433252,
      0.3464073790250983,
      0.3718368141040047,
      0.35157639605848345,
      0.3720678849531345,
      0.3230857272751473,
      0.37667885933378664,
      0.3052336181195623,
      0.359765353556295,
      0.370883441388151,
      0.33338732213922073,
      0.2867821100114192,
      0.3733093015252489,
      0.37141619814522975,
      0.34855633072434866,
      0.33692772945656624,
      0.3054754236730941,
      0.3629462812927706,
      0.24887926539539573,
      0.3856584496612525,
      0.31492078209903407,
      0.3783263458220907,
      0.31449134137502677,
      0.33694541457337124,
      0.3323266271796421,
      0.37272551892666783,
      0.3361636166960891,
      0.281741223772048,
      0.0
    ]
  ],
  "row_avgs": [
    0.22110382678124468,
    0.4286586057833081,
    0.4311211118151026,
    0.46529783921745416,
    0.43070896200449643,
    0.4127932493798013,
    0.2986724739556697,
    0.4678798528958726,
    0.4281784939496762,
    0.38461500895960316,
    0.44600818789697866,
    0.2839574225559213,
    0.42538677839047934,
    0.4357267674533881,
    0.4383728208565137,
    0.4117484386140572,
    0.3335460590289365,
    0.3928681761898898,
    0.38082316837941105,
    0.4214407117200074,
    0.40346681260152073,
    0.40570156666388485,
    0.32014061675580846,
    0.4422885309748749,
    0.3634947685744087,
    0.4046740802193994,
    0.3972761447688005,
    0.42537048326482324,
    0.3378982080294823
  ],
  "col_avgs": [
    0.35467151358535326,
    0.4347820907940491,
    0.41495136344054273,
    0.4686595441266362,
    0.43715231722389236,
    0.38911625553982004,
    0.3600125390829115,
    0.4421273690058007,
    0.46930714475555685,
    0.40066633682944924,
    0.4290846221551779,
    0.2632694975537024,
    0.44726380498072943,
    0.44250888846469955,
    0.434801373703728,
    0.4330000263659989,
    0.35863526999687995,
    0.42875346362610733,
    0.3358011973969667,
    0.44397360229911953,
    0.3553747315324141,
    0.369300892871753,
    0.3401946778219554,
    0.38023498406072653,
    0.3662691449219254,
    0.3842605334798819,
    0.37951477412246876,
    0.345115401118047,
    0.33041580682452093
  ],
  "combined_avgs": [
    0.28788767018329897,
    0.4317203482886786,
    0.42303623762782266,
    0.4669786916720452,
    0.4339306396141944,
    0.40095475245981066,
    0.3293425065192906,
    0.45500361095083663,
    0.4487428193526165,
    0.3926406728945262,
    0.4375464050260783,
    0.27361346005481185,
    0.4363252916856044,
    0.43911782795904386,
    0.43658709728012085,
    0.42237423249002803,
    0.34609066451290826,
    0.41081081990799856,
    0.3583121828881889,
    0.43270715700956347,
    0.37942077206696745,
    0.3875012297678189,
    0.3301676472888819,
    0.4112617575178007,
    0.36488195674816704,
    0.39446730684964065,
    0.38839545944563464,
    0.3852429421914351,
    0.3341570074270016
  ],
  "gppm": [
    628.1294266620641,
    634.5025049815273,
    641.4849325862391,
    617.0848922691074,
    628.7083900310039,
    653.6844032039099,
    667.9124167486603,
    627.0434033755404,
    615.8974839345817,
    650.5850459577247,
    638.4801162600221,
    713.6671963029357,
    627.8424167979059,
    631.2854771803475,
    632.6142325256603,
    636.194671141424,
    668.0625831054402,
    634.5171142698665,
    679.7485860601524,
    627.88241935935,
    664.4091200644741,
    665.4467338747205,
    675.9618146953713,
    658.3956329336919,
    664.8752830953947,
    658.4296612277136,
    658.3868696950714,
    672.2097685548382,
    683.1798731405422
  ],
  "gppm_normalized": [
    1.3972942006393485,
    1.5073992961556772,
    1.5300426046362179,
    1.4654490217579863,
    1.4940795425118487,
    1.5471374070710995,
    1.5864425384010994,
    1.4789327382520334,
    1.4600373600081116,
    1.5398775231352808,
    1.507324878437572,
    1.683212798070127,
    1.4908343511559807,
    1.4938063078031956,
    1.498575294596418,
    1.5026133433748399,
    1.576770595856353,
    1.5047693480698352,
    1.6012679541169341,
    1.493314252363583,
    1.5646075690051375,
    1.5670692110098492,
    1.589977516330295,
    1.5516271715621492,
    1.5714385263742365,
    1.5514907140037215,
    1.554703528624469,
    1.587251760547405,
    1.6064414398968359
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406
  ],
  "response_lengths": [
    3448,
    2424,
    2489,
    2484,
    2567,
    2598,
    2907,
    2395,
    2448,
    2271,
    2584,
    2534,
    2651,
    2517,
    2525,
    2579,
    2556,
    2645,
    2481,
    2504,
    2661,
    2422,
    2531,
    2463,
    2513,
    2337,
    2526,
    2530,
    2231
  ]
}