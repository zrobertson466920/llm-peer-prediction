{
  "example_idx": 20,
  "reference": "Published as a conference paper at ICLR 2023\n\nFROM PLAY TO POLICY: CONDITIONAL BEHAVIOR GENERATION FROM UNCURATED ROBOT DATA\n\nZichen Jeff Cui∗\n\nYibin Wang\n\nNur Muhammad (Mahi) Shafiullah\n\nLerrel Pinto\n\nNew York University\n\nABSTRACT\n\nWhile large-scale sequence modeling from offline data has led to impressive performance gains in natural language and image generation, directly translating such ideas to robotics has been challenging. One critical reason for this is that uncurated robot demonstration data, i.e. play data, collected from non-expert human demonstrators are often noisy, diverse, and distributionally multi-modal. This makes extracting useful, task-centric behaviors from such data a difficult generative modeling problem. In this work, we present Conditional Behavior Transformers (C-BeT), a method that combines the multi-modal generation ability of Behavior Transformer with future-conditioned goal specification. On a suite of simulated benchmark tasks, we find that C-BeT improves upon prior state-of-the-art work in learning from play data by an average of 45.7%. Further, we demonstrate for the first time that useful task-centric behaviors can be learned on a real-world robot purely from play data without any task labels or reward information. Robot videos are best viewed on our project website: play-to-policy.github.io.\n\n1\n\nINTRODUCTION\n\nMachine Learning is undergoing a Cambrian explosion in large generative models for applications across vision (Ramesh et al., 2022) and language (Brown et al., 2020). A shared property across these models is that they are trained on large and uncurated data, often scraped from the internet. Interestingly, although these models are trained without explicit task-specific labels in a self-supervised manner, they demonstrate a preternatural ability to generalize by simply conditioning the model on desirable outputs (e.g. “prompts” in text or image generation). Yet, the success of conditional generation from uncurated data has remained elusive for decision making problems, particularly in robotic behavior generation.\n\nTo address this gap in behavior generation, several works (Lynch et al., 2019; Pertsch et al., 2020b) have studied the use of generative models on play data. Here, play data is a form of offline, uncurated data that comes from either humans or a set of expert policies interacting with the environment. However, once trained, many of these generative models require significant amounts of additional online training with task-specific rewards (Gupta et al., 2019; Singh et al., 2020). In order to obtain task-specific policies without online training, a new line of approaches employ offline RL to learn goal-conditioned policies (Levine et al., 2020; Ma et al., 2022). These methods often require rewards or reward functions to accompany the data, either specified during data collection or inferred through hand-crafted distance metrics, for compatibility with RL training. Unfortunately, for many real-world applications, data does not readily come with rewards. This prompts the question: how do we learn conditional models for behavior generation from reward-free, play data?\n\nTable 1: Comparison between existing algorithms to learn from large, uncurated datasets: GCBC (Lynch et al., 2019), GCSL (Ghosh et al., 2019), Offline GCRL (Ma et al., 2022), Decision Transformer Chen et al. (2021)\n\nGCBC GCSL Offline RL Decision Transformer C-BeT (ours)\n\nReward-free Offline Multi-modal\n\n✓ ✓\n✗\n\n✓ ✗\n✗\n\n✗ ✓\n✗\n\n✗ ✓\n✗\n\n✓ ✓\n✓\n\n∗Corresponding author, email: jeff.cui@nyu.edu\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Multiple conditioned roll-outs of visual robot policies learned on our toy kitchen with only 4.5 hours of human play interactions. Our model learns purely from image and proprioception without human labeling or data curation. During evaluation, the policy can be conditioned either on a goal observation or a demonstration. Note that the last three rows contain distractor objects in the environment that were never seen during training.\n\nTo answer this question, we turn towards transformer-based generative models that are commonplace in text generation. Here, given a prompt, models like GPT-3 (Brown et al., 2020) can generate text that coherently follow or satisfy the prompt. However, directly applying such models to behavior generation requires overcoming two significant challenges. First, unlike the discrete tokens used in text generation, behavior generation will need models that can output continuous actions while also modeling any multi-modality present in the underlying data. Second, unlike textual prompts that serve as conditioning for text generation, behavior generation may not have the condition and the operand be part of the same token set, and may instead require conditioning on future outcomes.\n\nIn this work, we present Conditional Behavior Transformers (C-BeT), a new model for learning conditional behaviors from offline data. To produce a distribution over continuous actions instead of discrete tokens, C-BeT augments standard text generation transformers with the action discretization introduced in Behavior Transformers (BeT) (Shafiullah et al., 2022). Conditioning in C-BeT is done by specifying desired future states as input similar to Play-Goal Conditioned Behavior Cloning (Play-GCBC) (Lynch et al., 2019). By combining these two ideas, C-BeT is able to leverage the multi-modal generation capabilities of transformer models with the future conditioning capabilities of conditional policy learning. Importantly, C-BeT does not require any online environment interactions during training, nor the specification of rewards or Q functions needed in offline RL.\n\n2\n\nInitializationGrasping ovenPartially openingRetractingAttempt to reopenMissed graspGoal: Open oven Result: FailureInitializationGrasping potMoving potWedging pot in sinkMoved potRetractingGoal: Move pot to sink Result: SuccessInitializationGrasping microwaveMissed graspRe-opening microwaveGrasping OvenOpening OvenInitializationGrasping r. knobTurning r. knobRetractingGrasping l. knobTurning l. knobGoal: Turn right knob & left knob Result: SuccessGoal: Open microwave & oven Result: SuccessGrasping microwaveOpening microwaveClosing microwaveGrasping PotTransporting PotPlacing PotGoal: Open microwave & move pot Result: FailureInitializationTurning left knobRetractingGrasping OvenOpening OvenRetractingGoal: Turn left knob & open oven Result: SuccessPublished as a conference paper at ICLR 2023\n\nWe experimentally evaluate C-BeT on three simulated benchmarks (visual self-driving in CARLA (Dosovitskiy et al., 2017), multi-modal block pushing (Florence et al., 2021), and simulated kitchen (Gupta et al., 2019)), and on a real Franka robot trained with play data collected by human volunteers. The main findings from these experiments can be summarized as:\n\n1. On future-conditioned tasks, C-BeT achieves significantly higher performance compared to\n\nprior work in learning from play.\n\n2. C-BeT demonstrates that competent visual policies for real-world tasks can be learned from\n\nfully offline multi-modal play data (rollouts visualized in Figure 1).\n\n2 BACKGROUND AND PRELIMINARIES\n\nPlay-like data: Learning from Demonstrations (Argall et al., 2009) is one of the earliest frameworks explored for behavior learning algorithms from offline data. Typically, the datasets used in these frameworks have a built in assumption that the demonstrations are collected from an expert repeatedly demonstrating a single task in exactly the same way. On the contrary, play datasets violate many of such assumptions, like those of expertise of the demonstrator, and the unimodality of the task and the demonstrations. Algorithms that learn from such datasets sometimes assume that the demonstrations collected are from a rational agent with possibly some latent intent in their behavior (Lynch et al., 2019). Note that, unlike standard offline-RL datasets (Fu et al., 2020), play-like behavior datasets neither contain fully random behaviors, nor have rewards associated with the demonstrations.\n\nBehavior Transformers (BeT): BeT (Shafiullah et al., 2022) is a multi-modal behavior cloning model designed particularly for tackling play-like behavior datasets. BeT uses a GPT-like transformer architecture to model the probability distribution of action given a sequence of states π(at | st−h:t) from a given dataset. However, unlike previous behavior learning algorithms, BeT does not assume a unimodal prior for the action distribution. Instead, it uses a k-means discretization to bin the actions from the demonstration set into k bins, and then uses the bins to decompose each action into a discrete and continuous component. This support for multi-modal action distributions make BeT particularly suited for multi-modal, play-like behavior datasets where unimodal behavior cloning algorithms fail. However, vanilla BeT only supports unconditonal behavior rollouts, which means that it is not possible to choose a targeted mode of behavior during BeT policy execution.\n\nConditional behavior learning: Generally, the problem of behavior learning for an agent is considered the task of learning a policy π : O → A mapping from the environment observations to the agent’s actions that elicit some desired behavior. Conditional behavior learning is concerned with learning a policy π : O × G → A conditioned additionally on a secondary variable g sampled from a distribution p(g). This condition variable could be specific environment states, latents (such as one-hot vectors), or even image observations. The success of a conditioned policy can be evaluated either through pre-specified reward functions, distance function between achieved outcome g′ and specified outcome g, or by discounted visitation probability dπ(·|g) = Eτ ∼π[(cid:80)∞ t=0 γtδ(φ(ot) = g)] if a mapping φ between states and achieved outcome is defined (Eysenbach et al., 2022).\n\n(cid:81)\n\nGoal Conditioned Behavior Cloning (GCBC): In GCBC (Lynch et al., 2019; Emmons et al., 2021), the agent is presented with a dataset of (observation, action, goal) tuples (o, a, g), or sequences of such tuples, and the objective of the agent is to learn a goal-conditioned behavior policy. The simplest way to achieve so is by training a policy π(· | o, g) that maximizes the probability of the seen P[a ∼ π(· | o, g)]. Assuming a unimodal Gaussian distribution for data π∗ = arg maxπ π(a | o, g) and a model parametrized by θ, this comes down to finding the parameter θ minimizing (o,a,g) ||a − π(o, g; θ)||2. To make GCBC compatible with play data the MSE loss, θ∗ = arg minθ that inherently does not have goal labels, goal relabeling from future states is often necessary. A common form of data augmentation in training such models, useful when G ⊂ O, is hindsight data relabeling (Andrychowicz et al., 2017), where the dataset {(o, a, g)} is augmented with {(ot, a, ot′) | t′ > t} by relabeling any reached state in a future timestep as a goal state and adding it to the dataset.\n\n(o,a,g)\n\n(cid:80)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 APPROACH\n\nGiven a dataset {(o, a)} ∈ O × A of sequences of (observation, action) pairs from a play dataset, our goal is to learn a behavior generation model that is capable of handling multiple tasks and multiple ways of accomplishing each task. At the same time, we wish to be able to extract desired behavior from the dataset in the form of a policy through our model, or, in terms of generative models, “controllably generate” our desired behavior (see Figure 2). Finally, in the process of learning this controllable, conditional generative model, we wish to minimize the amount of additional human annotation or curation required in preparing the dataset. The method we develop to address these needs is called Conditional Behavior Transformer.\n\nFigure 2: Conditional behavior learning from play demonstrations. Here, a policy conditioned on reaching 1⃝ or 2⃝ has only one possible course of action, but conditioned on reaching 3⃝ there are two reasonable paths.\n\n3.1 CONDITIONAL BEHAVIOR TRANSFORMERS (C-BET)\n\nConditional task formulation: First, we formulate the task of learning from a play dataset as learning a conditional behavior policy, i.e. given the current state, we need to model the distribution of actions that can lead to particular future states. For simplicity, our formulation can be expressed as π : O × O → D(A) where, given a current observation oc and a future observation og, our policy π models the distribution of the possible actions that can take the agent from oc to og. Mathematically, given a set of play trajectories T , we model the distribution π(a | oc, og) ≜ Pτ ∈T (a | oc = τt, og = τt′, t′ > t). Next, to make our policy more robust since we operate in the partially observable setting, we replace singular observations with a sequence of observations; namely replacing oc and og with ̄oc = o(1:N ) for some integer N . Thus, the final task formulation becomes learning a generative model π with: (cid:16)\n\nand ̄og = o(1:N )\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\ng\n\nc\n\nπ\n\na | o(1:N )\n\n, o(1:N )\n\ng\n\nc\n\n≜ Pτ ∈T\n\na | o(1:N )\n\nc\n\n= τt:t+N , o(1:N )\n\ng\n\n= τt′:t′+N , t′ > t\n\n(1)\n\nArchitecture selection: Note that the model for our task described in the previous paragraph is necessarily multi-modal, since depending on the sequences ̄oc and ̄og, there could be multiple plausible sequences of actions with non-zero probability mass. As a result, we choose Behavior Transformers (BeT) (Shafiullah et al., 2022) as our generative architecture base as it can learn action generation with multiple modes. We modify the input to the BeT to be a concatenation of our future conditional observation sequence and current observation sequence. We choose to concatenate the inputs instead of stacking them, as this allows us to independently choose sequence lengths for the current and future conditional observations. Since BeT is a sequence-to-sequence model, we only consider the actions associated with the current observations as our actions. We show the detailed architecture of our model in Figure 3.\n\nDataset preparation: To train a C-BeT model on our play dataset {(o, a)}, we will need to appropriately prepare the dataset. We first convert the dataset to hold sequences of observations associated with actions, {(ot:t+N , at:t+N )}. Then, during training time, we dynamically augment each pair with a sequence of future observations, functionally converting our dataset into {(ot:t+N , at:t+N , ot′:t′+N ′)} for some t′ > t, and treat the sequence ot′:t′+N ′ as ̄og.\n\nTraining objective: We employ the same objective as BeT in training C-BeT. For each of the current observation and future conditional pair, we compute the BeT loss (see appendix B for details) between the ground truth actions and the predicted actions. We compute the focal loss (Lin et al., 2017) on the predicted action bins, and the MT-loss (Girshick, 2015) on the predicted action offsets corresponding to the action bins as described in BeT.\n\nTest-time conditioning with C-BeT: During test time, we again concatenate our future conditional sequence with our current observations, and sample actions from our model according to the BeT framework. While in this work, we primarily condition C-BeT on future observations, we also study\n\n4\n\n123Published as a conference paper at ICLR 2023\n\nFigure 3: End-to-end training and evaluation of C-BeT. (A) Our dataset consists of play data in an environment, which may contain semi-optimal behavior, multi-modal demonstrations, and failures, and does not contain any annotations or task labels. (B) We train our C-BeT model by conditioning on current and future states using BeT (Section 2) (C) During evaluation, our algorithm can be conditioned by target observations or newly collected demonstrations to generate targeted behavior.\n\nother ways of training and conditioning it, such as binary latent vectors denoting the modes in a trajectory in our experiments, and compare its performance to observation-conditioned C-BeT (see Section 4.5).\n\n4 C-BET ON SIMULATED BENCHMARKS\n\nIn this section, we discuss our experiments in simulation that are designed to answer the following key questions: How well does C-BeT learn behaviors from play? How important is multi-modal action modeling? And finally, how does C-BeT compare to other forms of conditioning?\n\n4.1 BASELINES\n\nWe compare with the following state-of-the-art methods in learning from reward-free offline data:\n\n• Goal Conditioned BC (GCBC): GCBC (Lynch et al., 2019; Emmons et al., 2021) learns a policy\n\nby optimizing the probability of seen actions given current and the end state in a trajectory.\n\n• Weighted Goal Conditioned Supervised Learning (WGCSL) (Yang et al., 2022): GCSL (Ghosh et al., 2019) is an online algorithm with multiple rounds of collecting online data, relabeling, and training a policy on that data using GCBC. WGCSL (Yang et al., 2022) improves GCSL by learning an additional value function used to weight the GCSL loss. We compare against an single-round, offline variant of WGCSL in this work.\n\n• Learning Motor Primitives from Play (Play-LMP): Play-LMP (Lynch et al., 2019) is a behavior generation algorithm that focuses on learning short (∼ 30 timesteps) motor primitives from play data. Play-LMP does so by using a variational-autoencoder (VAE) to encode action sequences into motor program latents and decoding actions from them.\n\n• Relay Imitation Learning (RIL): Relay Imitation Learning (Gupta et al., 2019) is a hierarchical imitation learning with a high level controller that generates short term target state given long term goals, and a low level controller that generates action given short term target.\n\n• Conditional Implicit Behavioral Cloning (C-IBC): Implicit behavioral cloning (Florence et al., 2021) learns an energy based model (EBM) E(a | o) over demos and during test samples action a given an observation o. We compare against a conditional IBC by training an EBM E(a | o, g).\n\n• Generalization Through Imitation (GTI): GTI (Mandlekar et al., 2020) encodes the goal condition using a CVAE, and autoregressively rolls out action sequences given observation and goal-latent. We follow their architecture and forgo collecting new trajectories with an intermediate model since that does not fit an offline framework.\n\n• Offline Goal-Conditioned RL: While offline RL is generally incompatible with play data without rewards, recently some offline goal-conditioned RL algorithms achieved success by optimizing for\n\n5\n\n(A)Datasetoc:c+hog:g+h′ (B)TrainingBehavior TransformerCurrent obsFuture obsCAT(),D(̂ac:c+h)Estimated action distributionac:c+hGround truth actionsBeT loss (Focal + MT loss)(C)EvaluationTarget frameorTarget demonstrationoc−h:cog:g+h′ ObservationsConditionalCAT(),Behavior TransformerD(̂ac−h:c)Action distribution∼acSampled actionPublished as a conference paper at ICLR 2023\n\nTable 2: Results of future-conditioned algorithms on a set of simulated environments. The numbers reported for CARLA, BlockPush, and Kitchen are out of 1, 1, and 4 respectively, following Shafiullah et al. (2022). In CARLA, success counts as reaching the location corresponding to the observation; for BlockPush, it is pushing one or both blocks into the target squares; and for Kitchen, success corresponds to the number of conditioned tasks, out of four, completed successfully.\n\nGCBC WGCSL Play-LMP RIL C-IBC GTI GoFAR BeT\n\nC-BeT (unimodal)\n\nC-BeT (multimodal)\n\nCARLA BlockPush Kitchen\n\n0.04 0.06 0.74\n\n0.02 0.10 1.17\n\n0.0 0.02 0.04\n\n0.59 0.07 0.39\n\n0.65 0.01 0.13\n\n0.74 0.04 1.61\n\n0.72 0.04 1.24\n\n0.31 0.34 1.77\n\n0.62 0.35 2.74\n\n0.98 0.90 2.80\n\na proxy reward defined through state occupancy. Our baseline, GoFAR (Ma et al., 2022), is one such algorithm that learns a goal-conditioned value function and optimizes a policy to maximize it.\n\n• Behavior Transformers (BeT): We include unconditional BeT (Sec. 2) in our baseline to understand the improvements made by the C-BeT conditioning. In practice, it acts as a “random” baseline that performs the tasks without regard for the goal.\n\n• Unimodal C-BeT: We use our method without the multi-modal head introduced in BeT. This also corresponds to a variant of Decision Transformer conditioning on outcomes instead of rewards.\n\nNote that neither WGCSL nor GoFAR are directly compatible with image states and goals, since they require a proxy reward function r : S × G → R. Thus, we had to design a proxy reward function on the image representations, exp (−(1/4||g − s||)2) to apply them on image-based environments. For a fair comparison, we also upgrade baseline Play-LMP, C-IBC, and GTI architectures by giving them sequences of observations and retrofitting them with transformers whenever applicable.\n\n4.2 SIMULATED ENVIRONMENTS AND DATASETS\n\nWe run our algorithms and baselines on a collection of simulated environments as a benchmark to select the best algorithms to run on our real robotic setup. The simulated environments are selected to cover a variety of properties that are necessary for the real world environment, such as pixel-based observations, diverse modes in the play dataset, and complex action spaces (see Figure. 4).\n\n1. CARLA self-driving: CARLA (Dosovitskiy et al., 2017) is a simulated self-driving environment created using Unreal Engine. In this environment, the observations are RGB pixel values of dimension (224, 224, 3), and actions are two-dimensional (accelerate/brake and steer). We use an environment with a fork in the road (see Figure 2) following two possible routes to the same goal, collecting 200 demonstrations in total. We condition on one of the two possible routes to the goal, and at the goal where choosing either of the two modes is valid.\n\n2. Multi-modal block-pushing: We use the multi-modal block-pushing environment from Florence et al. (2021) for complicated multi-modal demonstrations. In this environment, an xArm robot pushes two blocks, red and green, into two square targets colored red and green. All positions are randomized with some noise at episode start. We use 1,000 demonstrations collected using a deterministic controller, and condition on just the future block positions on each baseline.\n\n3. Franka relay kitchen: Originally introduced in Gupta et al. (2019), Relay Kitchen is a robotic environment in a simulated kitchen with seven possible tasks. A Franka Panda robot is used to manipulate the kitchen, and the associated dataset comes with 566 demonstrations collected by humans with VR controllers performing four of the seven tasks in some sequence.\n\n4.3 HOW WELL DOES C-BET LEARN BEHAVIORS FROM PLAY?\n\nOn each of these environments, we train conditional behavior generation models and evaluate them on a set of conditions sampled from the dataset. The success is defined by the model performing the same tasks as conditioned by the future outcome. We see from Table. 2 that C-BeT performs significantly better compared to the baselines on all three tasks. BeT, as our unconditioned “random” baseline, shows the success rate of completing tasks unconditionally, and see that none of the baselines surpasses it consistently. Out of the MLP-based baselines, WGCSL performs best in the state-based tasks. However, GoFAR performs best on the CARLA vision based environment where the other two MLP-based baselines fail almost completely. We note that Play-LMP performs poorly because our tasks are long-horizon and quite far from its intended motor primitive regime, which may be challenging for Play-LMP’s short-horizon auto-encoding architecture.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4.4 HOW IMPORTANT IS MULTI-MODAL ACTION MODELING?\n\nWhile we use a multi-modal behavior model in this work, it is not immediately obvious that it may be necessary. Specifically, some previous outcome-conditioned policy learning works (Chen et al., 2021; Emmons et al., 2021) implicitly assume that policies are unimodal once conditioned on an outcome. In Table 2 the comparison between C-BeT and unimodal C-BeT shows that this assumption may not be true for all environments, and all else being equal, having an explicitly multi-modal model helps learning an outcome conditioned policy when there may be multiple ways to achieve an outcome.\n\n4.5 HOW DOES C-BET COMPARE TO OTHER FORMS OF CONDITIONING?\n\nWe consider the question of how much comparative advantage there is in getting human labels for our tasks. We do so by adding manual one-hot (CARLA, BlockPush) or binary (Kitchen) labels to our tasks, and training and evaluating C-BeT with those labels. As we see on Table 3, on the three simulated environments, C-BeT conditioned on only future observations performs comparably to conditioning with human labels.\n\nTable 3: Comparison between C-BeT with no supervised labels and labels acquired with human supervision.\n\nNo labels Labels\n\nCARLA BlockPush Kitchen\n\n0.98 0.90 2.80\n\n1.0 0.89 2.75\n\n5 C-BET ON REAL-WORLD ROBOTIC MANIPULATION\n\nWe now discuss our robot experiments, which are geared towards understanding the usefulness of C-BeT on real-world play data.\n\n5.1 ROBOTIC ENVIRONMENT AND DATASET\n\nRobot setup: Our environment consists of a Franka Emika Panda robot, similar to the simulated Franka Kitchen environment, set up with a children’s toy kitchen set (see Figure 1). The toy kitchen has an oven, a microwave, a pot, and two stove knobs that are relevant to our play dataset. The action space in this environment contains the seven joint angle deltas normalized within the [−1, 1] range, and a binary gripper control.\n\nPlay dataset: We collected 460 sequences totaling to 265 minutes (about 4.5 hours) of play data on the toy kitchen with volunteers using a Vive VR controller to move the Franka. While collecting the play data, we did not give the volunteers any explicit instructions about doing any particular tasks, or number of tasks, beyond specifying the interactable items, and stipulating that the pot only goes on the left stove or in the sink, to prevent dropping the pot and reaching an unresettable state. As the observations, we save the RGB observations from two cameras on the left and right of the setup, as well as the robot’s proprioceptive joint angles. Overall, the dataset contains 45 287 frames of play interactions and their associated actions.\n\nRepresentation learning To simplify the task of learning policies on image space, we decouple the task of image representation learning from policy learning following Pari et al. (2021). For each camera, we first fine-tune a pretrained ResNet-18 (He et al., 2016) encoder on the acquired frames with BYOL self-supervision (Grill et al., 2020). Then, during policy learning and evaluation, instead of the image from the cameras, we pass the two 512-dimensional BYOL embeddings as part of the observation. For the proprioceptive part of the observation, we repeat the (sin, cos) of seven joint states 74 times to get a 1036-dimensional proprioceptive representation, making our overall observation representation 2060-dimensional.\n\n5.2 CONDITIONAL BEHAVIOR GENERATION ON REAL ROBOT\n\nBehavior generation on single tasks: Our first experiment in the real robot is about extracting single-task policies from the play dataset. We define our tasks as manipulating the four types of interactable objects one at a time: opening the oven door, opening the microwave door, moving the pot from the stove to the sink, and rotating a knob 90 degrees to the right. We use appropriate conditioning frames from our observation dataset, and start the robot from the neutral state to complete the four tasks. The result of this experiment is presented in Table 4. We see that on single task conditionals,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Single-task success rate in a real world kitchen with conditional models. We present the success rate and number of trials on each task, with cumulative results presented on the last column.\n\nKnobs Oven Microwave\n\nPot\n\nCumulative\n\nGoFAR Unconditional BeT Unimodal C-BeT Multimodal C-BeT\n\n0/10 5/20 1/20 3/20\n\n0/5 6/10 8/10 9/10\n\n0/5 1/10 4/10 7/10\n\n0/5 0/10 0/10 5/10\n\n0/25 12/50 13/50 24/50\n\nTable 5: Task success rate in a real world kitchen with conditional models evaluated on a long-horizon goal. We present the success rate and number of trials on each task, with cumulative result presented on the last column.\n\nOven → Pot Microwave → Oven\n\nPot → Microwave Avg. Tasks/Run\n\nUnconditional BeT Unimodal C-BeT Multimodal C-BeT\n\n(6, 0)/10 (1, 1)/10 (5, 4)/10\n\n(1, 6)/10 (2, 0)/10 (8, 8)/10\n\n(0, 1)/10 (8, 0)/10 (4, 4)/10\n\n0.47 0.37 1.1\n\nC-BeT is able to complete all tasks except the knobs consistently, outperforming all our baselines, showing that C-BeT is able to extract single-task policies out of uncurated, real-world play data. We discuss failures of C-BeT on the knob tasks in Section 5.3. While our GoFAR baseline was able to move towards the task targets, it was unable to successfully grasp or interact with any of the target objects. We believe it may be the case because unlike the robot experiment in Ma et al. (2022), we do not have the underlying environment state, the tasks are much more complicated, and our dataset is an order of magnitude smaller (400 K vs 45 K).\n\nBehavior generation for longer horizons: Next, we ask how well our models work for longerhorizon conditioning with multiple tasks. We choose play sequences from the dataset with multiple tasks completed and use their associated states as the conditions for our models. In our roll-outs, we calculate how many tasks completed in the original sequence were also completed in the conditional roll-outs. We calculate this metric over 3 conditioning sequences, and report the results in Table 5. We see that even without any high level controller, C-BeT is able to stitch together multiple tasks from play demonstrations to complete long-horizon goals.\n\nGeneralization to prompt and environment perturbations: A major requirement from any robot system deployed in the real world is to generalize to novel scenarios. We evaluate the generalizability of our learned policies in two different ways. In the first set of experiments, we collect fresh demonstrations that were not in the training set, and we condition our policies on such trajectories. We find that across the different tasks, even with unseen conditionings, C-BeT retains 67% of the single-task performance, with 16/50 task successes in total. In the second set of experiments, we add environmental distractors in the setup (Figure 1, bottom three rows) and run the single- and multi-task conditions on the modified environments. We see once again that the performance drops to around 67% of original with two distractors on the scene, but if we keep adding (four or more) distractors, the robot is unable to complete any tasks.\n\n5.3 ANALYSIS OF FAILURE MODES\n\nWe see a few failure modes in our experiments that may provide additional insights into learning from real-world play data. We discuss the most salient ones in this section.\n\nFailure in knob operation in the real world: We see that in all of our real world experiments, the accuracy in operating the knob is consistently lower than all other tasks. This is due to the failure of the learned representations. Upon inspection of the dataset images’ nearest neighbors in the representation space, we see that the BYOL-trained representation cannot identify the knob state better than random chance: the returned nearest neighbor differs in knob status often. Since the representation cannot identify the knob status properly, conditioning on it naturally fails.\n\nImportance of a multi-modal policy architecture: One of our motivations behind incorporating the BeT architecture in our work is its ability to learn multi-modal action distributions. In our experiments, we show that for some single-task conditions such as opening the oven door, having no multi-modality is sufficient (Table 4), but for more complicated tasks and learning from a more interconnected form of play data, it is always the case that a multi-modal architecture prevents our policies from collapsing to sub-optimal solutions (Table 5).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n6 RELATED WORK\n\nOutcome-conditioned behavior learning: Behavior learning conditioned on particular outcomes, such as reward or goals, is a long studied problem (Kaelbling, 1993; Schaul et al., 2015; Veeriah et al., 2018; Zhao et al., 2019). Compared to standard behavior learning, learning conditioned behavior can generally be more demanding since the same model can be expected to learn a multitude of behaviors depending on the outcome, which can make learning long-term behavior harder (Levy et al., 2017; Nachum et al., 2018). As a result, a common line of work in outcome-conditioned learning is to use some form of relabeling of demonstrations or experience buffer as a form of data augmentation (Kaelbling, 1993; Andrychowicz et al., 2017; Ghosh et al., 2019; Goyal et al., 2022) similar to what we do in the paper. As opposed to goal or state conditioned learning, which we focus on in this paper, recently reward conditioned learning using a transformer (Chen et al., 2021) was introduced. However, later work found that it may not work as expected in all environments (Paster et al., 2022; Brandfonbrener et al., 2022) and large transformer models may not be necessary (Emmons et al., 2021) for reward conditioned learning. In this work, we find that using transformers is crucial, particularly when dealing with high dimensional visual observation and multi-modal actions.\n\nLearning from play data: Our work is most closely related to previous works such as Lynch et al. (2019); Gupta et al. (2019), which also focus on learning from play demonstrations that may not be strictly optimal and uniformly curated for a single task. Learning policies capable of multiple tasks from play data allows knowledge sharing, which is why it may be more efficient compared to learning from demonstrations directly (Zhang et al., 2018; Rahmatizadeh et al., 2018; Duan et al., 2017; Pari et al., 2021; Young et al., 2021). Gupta et al. (2022) attempts reset-free learning with play data, but requires human annotation and instrumentation in the environment for goal labels.\n\nGenerative modeling of behavior: Our method of learning a generative model for behavior learning follows a long line of work, including Inverse Reinforcement Learning or IRL (Russell, 1998; Ng et al., 2000; Ho & Ermon, 2016), where given expert demonstrations, a model tries to construct the reward function, which is then used to generate desirable behavior. Another class of algorithms learn a generative action decoder (Pertsch et al., 2020a; Singh et al., 2020) from interaction data to make downstream reinforcement learning faster and easier, nominally making multi-modal action distribution easier. Finally, a class of algorithms, most notably Liu et al. (2020); Florence et al. (2021); Kostrikov et al. (2021); Nachum & Yang (2021) do not directly learn a generative model, but instead learn energy based models that need to be sampled to generate behavior, although they do not primarily focus on goal-conditioning.\n\nTransformers for behavior learning: Our work follows earlier notable works in using transformers to learn a behavior model from an offline dataset, such as Chen et al. (2021); Janner et al. (2021); Shafiullah et al. (2022). Our work is most closely related to Shafiullah et al. (2022) as we build on their transformer architecture, while our unimodal baseline is a variant of Chen et al. (2021) that learns outcome conditioned instead of reward conditioned policy. Beyond these, Dasari & Gupta (2020); Mandi et al. (2021) summarizes historical visual context using transformers, and Clever et al. (2021) relies on the long-term extrapolation abilities of transformers as sequence models. The goal of C-BeT is orthogonal to these use cases, but can be combined with them for future applications.\n\n7 DISCUSSION AND LIMITATIONS\n\nIn this work, we have presented C-BeT, a new approach for conditional behavior generation that can learn from offline play data. Across a variety of benchmarks, both simulated and real, we find that C-BeT significantly improves upon prior state-of-the-art work. However, we have noticed two limitations in C-BeT, particularly for real-robot behavior learning. First, if the features provided to C-BeT do not appropriately capture relevant objects in the scene, the robot execution often fails to interact with that object in its environment. Second, some tasks, like opening the oven door, have simpler underlying data that is not multimodal, which renders only meager gains with C-BeT. A more detailed analysis of these limitations are presented in Section 5.3. We believe that future work in visual representation learning can address poor environment features, while the collection of even larger play datasets will provide more realistic offline data for large-scale behavior learning models.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nWe thank Sridhar Arunachalam, David Brandfonbrener, Irmak Guzey, Yixin Lin, Jyo Pari, Abitha Thankaraj, and Austin Wang for their valuable feedback and discussions. This work was supported by awards from Honda, Meta, Hyundai, Amazon, and ONR awards N00014-21-1-2758 and N0001422-1-2773.\n\nREFERENCES\n\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. 3, 9\n\nBrenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning\n\nfrom demonstration. Robotics and autonomous systems, 57(5):469–483, 2009. 3\n\nDavid Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? arXiv preprint arXiv: Arxiv-2206.01079, 2022. 9\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 1, 2\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021. 1, 7, 9\n\nHenry M Clever, Ankur Handa, Hammad Mazhar, Kevin Parker, Omer Shapira, Qian Wan, Yashraj Narang, Iretiayo Akinola, Maya Cakmak, and Dieter Fox. Assistive tele-op: Leveraging transformers to collect robotic task demonstrations. arXiv preprint arXiv:2112.05129, 2021. 9\n\nSudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. arXiv preprint\n\narXiv:2011.05970, 2020. 9\n\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An\n\nopen urban driving simulator. In Conference on robot learning, pp. 1–16. PMLR, 2017. 3, 6\n\nYan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. Advances in neural information processing systems, 30, 2017. 9\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021. 3, 5, 7, 9\n\nBenjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine. Contrastive learning as goal-conditioned reinforcement learning. arXiv preprint arXiv: Arxiv-2206.07568, 2022. 3\n\nPete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. arXiv preprint arXiv:2109.00137, 2021. 3, 5, 6, 9\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. 3\n\nDibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019. 1, 5, 9\n\nRoss Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision,\n\npp. 1440–1448, 2015. 4, 13\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAnirudh Goyal, Abram Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez, Mehdi Mirza, Peter C Humphreys, Ksenia Konyushova, et al. Retrieval-augmented reinforcement learning. In International Conference on Machine Learning, pp. 7740–7765. PMLR, 2022. 9\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020. 7\n\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019. 1, 3, 5, 6, 9\n\nAbhishek Gupta, Corey Lynch, Brandon Kinman, Garrett Peake, Sergey Levine, and Karol Hausman. Bootstrapped autonomous practicing via multi-task reinforcement learning. arXiv preprint arXiv:2203.15755, 2022. 9\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 7\n\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural\n\ninformation processing systems, volume 29, pp. 4565–4573, 2016. 9\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. In Advances in Neural Information Processing Systems, 2021. 9\n\nLeslie Pack Kaelbling. Learning to achieve goals. In IN PROC. OF IJCAI-93, pp. 1094–1098.\n\nMorgan Kaufmann, 1993. 9\n\nIlya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum. Offline reinforcement learning\n\nwith fisher divergence critic regularization. arXiv preprint arXiv:2103.08050, 2021. 9\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\n\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 1\n\nAndrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierarchies\n\nwith hindsight. arXiv preprint arXiv:1712.00948, 2017. 9\n\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017. 4, 13\n\nMinghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. arXiv\n\npreprint arXiv:2004.09395, 33, 2020. 9\n\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, S. Levine, and Pierre\n\nSermanet. Learning latent plans from play. Corl, 2019. 1, 2, 3, 5, 9\n\nYecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i’ll go: Offline goalconditioned reinforcement learning via f -advantage regression. arXiv preprint arXiv:2206.03023, 2022. 1, 6, 8\n\nZhao Mandi, Fangchen Liu, Kimin Lee, and Pieter Abbeel. Towards more generalizable one-shot\n\nvisual imitation learning. arXiv preprint arXiv:2110.13423, 2021. 9\n\nAjay Mandlekar, Danfei Xu, Roberto Martín-Martín, Silvio Savarese, and Li Fei-Fei. Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations. arXiv e-prints, art. arXiv:2003.06085, March 2020. 5\n\nOfir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive\n\nfourier features. arXiv preprint arXiv:2105.12272, 2021. 9\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nOfir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical\n\nreinforcement learning. Advances in neural information processing systems, 31, 2018. 9\n\nAndrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,\n\nvolume 1, pp. 663–670, 2000. 9\n\nJyothish Pari, Nur Muhammad, Sridhar Pandian Arunachalam, and Lerrel Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021. 7, 9\n\nKeiran Paster, Sheila McIlraith, and Jimmy Ba. You can’t count on luck: Why decision transformers\n\nfail in stochastic environments. arXiv preprint arXiv: Arxiv-2205.15967, 2022. 9\n\nKarl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned\n\nskill priors. arXiv preprint arXiv:2010.11944, 2020a. 9\n\nKarl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned\n\nskill priors. arXiv preprint arXiv:2010.11944, 2020b. 1\n\nRouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Bölöni, and Sergey Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 3758–3765. IEEE, 2018. 9\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1\n\nStuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual\n\nconference on Computational learning theory, pp. 101–103, 1998. 9\n\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1312–1320, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ schaul15.html. 9\n\nNur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning k modes with one stone. arXiv preprint arXiv:2206.11251, 2022. 2, 3, 4, 6, 9, 13, 14\n\nAvi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. arXiv preprint arXiv:2011.10024, 2020. 1, 9\n\nVivek Veeriah, Junhyuk Oh, and Satinder Singh. Many-goals reinforcement learning. arXiv preprint\n\narXiv:1806.09605, 2018. 9\n\nRui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. Iclr, 2022. 5\n\nSarah Young, Jyothish Pari, Pieter Abbeel, and Lerrel Pinto. Playful interactions for representation\n\nlearning. arXiv preprint arXiv: Arxiv-2107.09046, 2021. 9\n\nTianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 5628–5635. IEEE, 2018. 9\n\nRui Zhao, Xudong Sun, and Volker Tresp. Maximum entropy-regularized multi-goal reinforcement learning. In International Conference on Machine Learning, pp. 7553–7562. PMLR, 2019. 9\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA SIMULATED ENVIRONMENT VISUALIZATIONS\n\nFigure 4: Visualizations of simulated environments that we evaluate our methods on, from left to right: CARLA self-driving (top down view and agent POV), BlockPush, and Franka Kitchen.\n\nB BEHAVIOR TRANSFORMERS\n\nWe use Behavior Transformers from Shafiullah et al. (2022) as our backbone architecture, building our conditional algorithm on top of it. In this section, we describe the BeT architecture and the training objective to help the readers understand the details of our algorithm.\n\nB.1 BET ARCHITECTURE\n\nBeT uses a repurposed MinGPT architecture to model multi-modal behavior. It uses the MinGPT trunk as a sequence-to-sequence model that tries to predict a sequence of actions at:t+h given a sequence of states or observations ot:t+h. Beyond just prediction the actions, however, BeT tries to model the multi-modal action distribtuion given the observations. To create a multi-modal model over the continuous action distribution, BeT uses an action encoder-decoder architecture that can encode each action vector into a discrete latent and a smaller-norm continuous offset. BeT does so by using an offline action dataset to create a k-means model of the actions. Then, an action is encoded into its associated bin out of the k-bins (binning), and a small continuous offset from the associated bin.\n\nThe BeT model, given a sequence of observations ot:t+h, predicts a k-dimensional multinomial distribution over the k-bins, as well as a k × |A| dimensional matrix for offsets associated with each action bins. Sampling from the BeT action distribution is done via sampling a discrete bin first, taking its associated action offset, and then adding the bin center with the action offset.\n\nB.2 BET TRAINING OBJECTIVE\n\nGiven an observation o and its associated ground truth action a, we will now present the simplified version of how the BeT loss is calculated. Let us assume the BeT model prediction is π(o)d ∈ Rk, π(o)c ∈ Rk×|A| for the discrete and the continuous parts of the predictions. Let us also assume that ⌊a⌋ is the discrete bin out of the k bins that a belongs to, and ⟨a⟩ = a − BinCenter(⌊a⌋). Then, the BeT loss becomes\n\nLBeT = Lf ocal(π(o)d, ⌊a⌋) + λ · LM T (⟨a⟩, π(o)c)\n\nWhere Lf ocal is the Focal loss (Lin et al., 2017), a special case of the negative log likelihood loss defined as\n\nLf ocal(pt) = −(1 − pt)γ log(pt)\n\nand LM T is the multi-task loss (Girshick, 2015) defined as\n\n(cid:18)\n\nMT-Loss\n\na,\n\n(cid:16)\n\n(cid:17)k\n\n⟨ˆa(j) i ⟩\n\nj=1\n\nI[⌊a⌋ = j] · ∥⟨a⟩ − ⟨ˆa(j)⟩∥2\n\n2\n\n(cid:19)\n\n=\n\nk (cid:88)\n\nj=1\n\n13\n\nCARLA self drivingBlockPushFranka KitchenPublished as a conference paper at ICLR 2023\n\nC IMPLEMENTATION DETAILS\n\nC.1\n\nIMPLEMENTATION USED\n\nIn our work, we base our C-BeT implementation off of the official repo published at https: //github.com/notmahi/bet. For the GCBC, WGCSL, and GoFAR baselines, we use the official repo released by the GoFAR authors https://github.com/JasonMa2016/GoFAR/.\n\nC.2 HYPERPARAMETERS LIST:\n\nWe present the C-BeT hyperparameters in Table 6 below, which were mostly using the default hyperparameters in the original Shafiullah et al. (2022) paper:\n\nTable 6: Environment-dependent hyperparameters in BeT.\n\nHyperparameter Layers Attention heads Embedding width Dropout probability Context size Training epochs Batch size Number of bins k Future conditional frames\n\nThe shared hyperparameters are in Table 7.\n\nCARLA Block-push Kitchen 4\n4 72 0.1 5\n350 64 24 3\n\n3 4\n256 0.6 10 40 128 32 10\n\n6 6\n120 0.1 10 50 64 64 10\n\nTable 7: Shared hyperparameters for BeT training\n\nName Optimizer Learning rate Weight decay Betas Gradient clip norm\n\nValue Adam 1e-4 0.1 (0.9, 0.95) 1.0\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nD ROBOT ENVIRONMENT DEMONSTRATION TRAJECTORIES\n\nFigure 5: Sample demonstration trajectories for the real kitchen environment.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nE SIMULATED ENVIRONMENT ROLLOUT TRAJECTORIES\n\nFigure 6: Sample demonstration trajectories for the CARLA self driving environment, conditioning on going to the right path.\n\nFigure 7: Sample demonstration trajectories for the multi-modal block pushing environment, conditioning on pushing the green block to green square and red block to red square.\n\nFigure 8: Sample demonstration trajectories for the Franka Kitchen environment, conditioning on completing the microwave, bottom knob, slide cabinet, hinge cabinet tasks.\n\n16\n\nGoFARWGCSLGCBCC-BeTGoFARWGCSLGCBCC-BeTGoFARWGCSLGCBCC-BeT",
  "translations": [
    "# Summary Of The Paper\n\nThis work presents a straightforward, but impressive extension of prior work on “Behavior Transformers” for learning policies from offline, reward-free data, by introducing the ability to learn goal-conditioned policies from undirected play data. Crucially, the play data used to learn policies in this work in inherently multimodal (and in some cases suboptimal); yet, using the goal conditioned behavior transformer (C-BeT) approach, we’re able to overcome such problems — *without any online exploration*.\n\nThe proposed approach builds on a history of prior work in goal-conditioned behavior cloning from visual observations, and makes a simple tweak to the underlying Behavior Transformer (think a causal transformer that eats states, and outputs actions) architecture, by additionally conditioning on target goal observations (images) of where we’d like the environment to end up. This is a powerful objective, and across three complex simulated environments (CARLA self-driving, multimodal block pushing, and the Franka Relay Kitchen environment) — each with varying levels of multimodality/suboptimality, C-BeT shows itself to be extremely strong.\n\nHowever, the truly impressive part of this work is the real-robot evaluation; from just 4.5 hours of real user “play” on a Franka robot, C-BeT is able to learn 5 complex manipulation tasks in a “toy kitchen” to a meaningful level of competency — far better than other approaches from the offline RL literature like GO-FAR. In general, I found the evaluation to be thorough, ablations comprehensive, and comparisons to prior art meaningful!\n\n# Strength And Weaknesses\n\nI think this is a very strong paper, showing that existing Behavior Transformers can scale to multimodal, reward free play data. In general, I really believe in the results in this paper, and am excited for the possibilities of using such an approach for general offline policy learning on real (and simulated) robotic tasks.\n\nThe weaknesses of this work are generally just weaknesses around the feasibility of visual goal-conditioning in the wild. In the real world, it’s not clear whether (given a new task, or a combinatorial explosion of objects/states) we’ll be able to provide robots with meaningful “goal images” that capture what we want to happen; for example, across all the tasks in this work, only a handful of object positions change from start state to goal state; in heavily cluttered environments with dynamic objects, it feels like such an approach would really suffer (in sample efficiency and maybe in just general applicability). This is somewhat shown in the videos, where we add random objects (though that’s also just because the environment is so out of distribution).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI found this paper incredibly clear and easy to read; the quality and thoroughness of the experiments (and attention to baselines) is commendable, and I really buy these results. I like that the authors chose to evaluate on open-source simulated environments as well as a real-robot; it speaks a lot to the reproducibility of this work!\n\nMinor: Seems like Claim 2 in the introduction “C-BeT represents the first work to demonstrate that competent visual policies for real-world tasks can be learned from reward-free play data” isn’t quite right; Implicit BC learns from play-esque multimodal data without rewards, as does follow-up work like Implicit Kinematic Policies (but I could be wrong; might just be a wording thing around the distinction between “play” and “multimodal demos”).\n\n# Summary Of The Review\n\nI think this is a straightforward and impactful extension of prior work on behavior transformers. I think that being able to learn robust goal-conditioned policies from uncurated reward-free play data in a manner that is 1) simple and 2) completely offline is very meaningful, and a clear win for the field.\n\nI really like this work!\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper \"FROM PLAY TO POLICY: CONDITIONAL BEHAVIOR GENERATION FROM UNCURATED ROBOT DATA\" introduces Conditional Behavior Transformers (C-BeT), a framework aimed at generating conditional behaviors from uncurated play data collected from robotic interactions. The authors tackle the challenges associated with noisy and diverse play data, presenting C-BeT as a solution that combines multi-modal action modeling with future-conditioned goal specification. The experimental results demonstrate that C-BeT outperforms state-of-the-art methods by an average of 45.7% across various simulated tasks, and it successfully learns task-centric behaviors in real-world settings without requiring explicit task labels or reward signals.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to leveraging uncurated data for behavior generation, which addresses a significant gap in existing literature. The performance improvements over established baselines, particularly in real-world scenarios, highlight the practical applicability of the proposed method. However, the paper also has weaknesses, such as the limitations in handling tasks requiring precise state recognition and the challenges faced in complex environments. Additionally, while the model's architecture is compelling, it may benefit from further exploration of its limitations and potential avenues for improvement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings of the research. The quality of the experiments is robust, with a thorough evaluation against multiple baselines in both simulated and real-world environments. The novelty of C-BeT lies in its ability to generate behaviors from uncurated data, which is a significant advancement compared to traditional approaches. Reproducibility is supported through detailed descriptions of the methodology and experimental setup, although sharing of the code and datasets would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in conditional behavior generation from uncurated robot data, demonstrating both theoretical and practical contributions. The proposed C-BeT framework shows strong empirical results, although certain limitations in complex tasks need to be addressed. The work is well-articulated and offers valuable insights into the potential of using uncurated play data in robotics.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"FROM PLAY TO POLICY: CONDITIONAL BEHAVIOR GENERATION FROM UNCURATED ROBOT DATA\" addresses the challenge of generating task-centric behaviors from uncurated robot demonstration data collected from non-expert human demonstrators. The authors introduce Conditional Behavior Transformers (C-BeT), a novel architecture that enhances the learning process from play data without requiring task labels or reward information. Key contributions include demonstrating that C-BeT achieves an average improvement of 45.7% over prior state-of-the-art methods, showcasing robust performance in both simulated and real-world robotic tasks, and successfully managing long-horizon goals.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its significant improvement in behavior learning from uncurated play data and its real-world applicability, demonstrated through successful robot experiments. C-BeT's multi-modal capabilities enhance its adaptability to various tasks, making it versatile for complex behaviors. However, the paper also has notable limitations, such as challenges in feature representation, particularly with certain objects, and a less pronounced benefit of its architecture for simpler tasks. Additionally, the efficacy of the method is closely tied to the quality and diversity of the collected play data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, experimental design, and results, which allows for a good understanding of its contributions. The novelty of the C-BeT architecture is a significant aspect, as it combines existing techniques in a new way to tackle a challenging problem in robotics. However, while the experimental results are compelling, reproducibility may be hampered by the dependency on high-quality data, which may not be easily obtainable in practice.\n\n# Summary Of The Review\nOverall, this paper presents a robust and innovative approach to conditional behavior generation in robotics, marking a meaningful advancement in leveraging uncurated play data. While the results are impressive and demonstrate the potential for real-world applications, there are limitations regarding feature representation and data dependency that should be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents **Conditional Behavior Transformers (C-BeT)**, a novel framework designed to facilitate the learning of conditional behavior generation from uncurated robot play data. C-BeT improves upon existing models by incorporating a multi-modal generation capability and future-conditioned goal specification, which allows for effective task-centric behavior learning without the need for explicit task labels or reward information. The authors demonstrate a significant performance improvement of 45.7% on average across a range of simulated benchmark tasks, as well as successful application in real-world robotic manipulation scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to leveraging uncurated play data for behavior generation, addressing a significant gap in the field of robotics where labeled data is often scarce. The methodology, particularly the integration of multi-modal action modeling, enhances the performance of the model in complex tasks. However, the paper does exhibit some weaknesses, including a reliance on appropriate feature representation, which may limit the scalability of the approach. Additionally, the analysis of failure modes suggests that the model may struggle with certain types of distractions, indicating areas for future improvement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The use of diagrams and visualizations in the appendix aids in understanding the simulated environments and the behavior transformer architecture. The novelty of the approach is evident, particularly in how C-BeT tackles the challenges associated with uncurated play data. However, the reproducibility may be hindered by the reliance on specific hyperparameter settings and the need for careful feature representation tuning, which is not extensively detailed in the main text.\n\n# Summary Of The Review\nOverall, the paper provides a significant contribution to the field of robotics by introducing C-BeT, which effectively learns conditional behaviors from uncurated play data. While the findings are promising, the paper could benefit from further exploration of the limitations related to feature representation and generalization in complex environments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Conditional Behavior Transformers (C-BeT), a novel framework designed to learn task-centric behaviors from uncurated play data without the need for explicit task labels or rewards. The methodology leverages multi-modal generation capabilities to adaptively learn from diverse demonstrations, demonstrating effectiveness in both simulated and real-world robotic tasks. The findings indicate a significant average performance improvement of 45.7% over prior state-of-the-art methods, highlighting the framework’s potential in practical applications, although challenges in generalization and robustness remain.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its substantial performance improvements and its innovative approach to learning from play data without explicit supervision. The capability to handle multi-modal input is another advantage, allowing for versatile behavior generation. However, the paper presents several weaknesses: the generalizability of the results is questioned due to insufficient detail about the contexts in which performance gains are achieved. Additionally, the reliance on uncurated play data could introduce noise, and the model's complexity may complicate training and inference processes. Furthermore, the performance drop in more complex tasks suggests limitations in the model's robustness and adaptability to dynamic environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its contributions clearly, although some sections could benefit from more detailed explanations of the underlying assumptions and the conditions for performance improvements. The novelty of the conditioning framework and the decoupled representation learning approach is commendable, contributing to the paper's overall significance. However, the reproducibility of the results may be hindered by the lack of detailed hyperparameter tuning strategies and guidelines for data quality improvement, which are critical for practitioners aiming to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to learning behaviors from uncurated play data with significant performance gains over existing methods. While the methodology is innovative and the empirical results are strong, concerns regarding generalizability, robustness, and data quality indicate areas that require further attention for practical deployment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces Conditional Behavior Generation Networks (CBG-Net), a novel method for behavior generation in robotics that effectively utilizes uncurated play data. The authors present a dual-modality architecture that combines image and proprioceptive data to generate behavior policies, addressing limitations of traditional methods that often require curated datasets. They validate the effectiveness of CBG-Net through extensive experiments, achieving up to a 50% improvement in task success rates compared to state-of-the-art approaches, and demonstrate its applicability on real robots.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative dual-modality approach, which enhances the model's ability to interpret complex environmental interactions, and a self-supervised learning framework that reduces reliance on labeled data, making it more scalable. The empirical results are compelling, showing significant performance improvements over existing methods. However, the paper lacks a thorough discussion of potential failure modes in novel environments, which could limit the understanding of the model's robustness. Additionally, more detailed comparisons with baseline methods would strengthen the clarity of performance claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, although a deeper analysis of failure modes would enhance the work's clarity. The quality of the experiments appears high, and the novelty of the proposed CBG-Net is significant, particularly in its self-supervised learning approach. Reproducibility is somewhat supported by the detailed methodology, but further information on implementation specifics and dataset characteristics could bolster confidence in replicating the results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in robotic behavior generation, showcasing the effectiveness of CBG-Net in leveraging uncurated play data. The substantial performance improvements and real-world applicability highlight the importance of this research. However, addressing potential failure scenarios and providing clearer comparisons with baseline methods would enhance the contributions of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"FROM PLAY TO POLICY: CONDITIONAL BEHAVIOR GENERATION FROM UNCURATED ROBOT DATA\" introduces a novel method called Adversarial Behavior Transformers (A-BeT), aimed at enhancing adversarial training in reinforcement learning. The authors focus on the challenges associated with learning robust policies from uncurated, noisy data by conditioning training on adversarial inputs. The methodology involves adapting transformer architectures to generate actions resilient to adversarial perturbations, and experimental results demonstrate that A-BeT outperforms existing adversarial training methods across various simulated environments.\n\n# Strengths And Weaknesses\nThe paper presents several strengths, including its innovative approach to adversarial training that addresses a notable gap in existing literature regarding uncurated data. The comprehensive evaluation across multiple simulated environments bolsters the credibility of the findings and highlights the model's adaptability to adversarial scenarios. However, the reliance on simulated environments could limit the practical applicability of the results, as real-world validation is lacking. Additionally, while the paper discusses the advantages of A-BeT, it does not provide an extensive comparison with a broader range of baseline adversarial training methods. The potential computational overhead associated with the transformer architecture is also not addressed, raising questions about scalability in larger applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, clearly outlining the methodology and findings. The quality of the experiments is high, with extensive data supporting the claims. The novelty of using transformer architectures in adversarial training is significant, reflecting current trends in deep learning. However, the reproducibility could be enhanced by including more details about the experimental setups and parameters used.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in adversarial training by introducing A-BeT, which effectively learns from uncurated data and demonstrates robustness against adversarial attacks. While the contributions are impactful, the lack of real-world validation and a more comprehensive comparison with existing methods are notable limitations that could be addressed in future work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data\" by Zichen Jeff Cui et al. presents a novel framework, Conditional Behavior Transformers (C-BeT), aimed at generating robot behaviors from uncurated play data without human intervention. The authors claim that C-BeT provides significant performance improvements over existing state-of-the-art methods, achieving a 45.7% increase in efficacy. The framework is purported to enable robots to autonomously learn complex behaviors, generalize to novel scenarios, and perform various tasks with minimal training, thereby positioning C-BeT as a potential cornerstone for future research in offline reinforcement learning and generative modeling.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its ambitious claims and the introduction of a new methodology that could potentially redefine aspects of behavior generation in robotics. The experimental results show high success rates across multiple simulated environments, indicating robust performance. However, the paper tends to downplay its limitations and presents an overly optimistic view of C-BeT's applicability and potential, which raises questions about the thoroughness of the analysis. Additionally, the comparison with existing algorithms is somewhat dismissive, lacking a nuanced discussion of their strengths and limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, although it occasionally leans towards hyperbole in its claims. While the novelty of the C-BeT framework is evident, the authors could provide a more detailed explanation of the methodology, including how it achieves its results. The reproducibility of the findings may be compromised due to the lack of comprehensive details on the experimental setup and parameters used, which are crucial for other researchers aiming to replicate the study.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative approach to behavior generation in robotics that could have substantial implications for the field. However, it is marred by an overemphasis on its transformative potential while underreporting limitations and lacking sufficient methodological detail for reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents Conditional Behavior Transformers (C-BeT), a novel approach for learning task-centric behaviors from uncurated robot data (play data) without relying on explicit task labels or reward structures. The methodology employs a transformer-based architecture capable of multi-modal generation and future-conditioned goal specifications. Key findings indicate that C-BeT outperforms previous methods, achieving an average performance improvement of 35.2% on simulated benchmark tasks, and demonstrates effective task execution in real-world robotics, although with some limitations in robustness and feature representation.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to leveraging uncurated play data for learning robot behaviors without requiring additional supervision, addressing a significant challenge in robotic learning. The methodology is well-structured, allowing for multi-modal outputs and future conditioning, which are critical for real-world applications. However, weaknesses include the over-optimistic performance claims—actual improvements were lower than stated—and limitations in generalization and feature representation that hinder performance in specific tasks, such as knob manipulation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible for readers with a background in robotics and machine learning. The methodology is clearly explained, although some details regarding the implementation and training processes could benefit from further elaboration. The novelty of the approach is significant, as it proposes a solution to the challenges of using generative models in robotics. Reproducibility may be a concern due to the lack of detailed experimental setups and specific hyperparameters, which could impact the ability of other researchers to replicate the results.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for learning robot behaviors from uncurated data. While it demonstrates significant improvements over previous methods, it also reveals limitations in robustness and feature representation that need to be addressed in future work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to behavior generation in robotic systems using uncurated play data, termed C-BeT (Conditioning Behavior through Play Data). The authors argue that learning from this unstructured dataset enables the model to capture diverse demonstrations, leading to effective policy learning. The methodology involves leveraging future conditioning to guide behavior generation while employing multi-modal action modeling. The findings suggest that the C-BeT approach performs comparably to models trained with human labels, although the authors acknowledge potential limitations in generalization to novel tasks.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative use of uncurated play data, which may offer a rich source of diverse behaviors for robots. However, the reliance on unstructured data raises concerns about noise and variability potentially hindering learning outcomes. The assumption that future conditioning is the optimal behavior generation method lacks exploration of alternative techniques that could provide more relevant signals. Furthermore, the paper's advocacy for multi-modality is not sufficiently justified, as it does not address scenarios where simpler approaches may be adequate. The assumption that simulation results will generalize to real-world environments is another potential weakness, as real-world complexities may not be captured in the training data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a logical flow in presenting the methodology and findings. However, some assumptions, such as the sufficiency of learned representations and the evaluation metrics used, could benefit from more thorough explanation and justification. The novelty of the approach is notable, but the paper does not adequately explore alternative methodologies, limiting its impact. Reproducibility could be further enhanced by providing more detailed experimental setups and data sources.\n\n# Summary Of The Review\nWhile the paper introduces a promising approach to behavior generation using uncurated play data, it raises several critical concerns regarding the assumptions made about data quality, conditioning techniques, and generalization to real-world scenarios. A more robust evaluation of alternative methods and a deeper exploration of the limitations could strengthen the overall contribution.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Conditional Behavior Transformers (C-BeT), a novel approach for generating task-centric behaviors from uncurated robot play data, addressing the challenges of noisy and multi-modal data. The methodology involves leveraging a multi-modal transformer architecture to learn conditional behavior policies without the need for task labels or rewards. Experimental results indicate that C-BeT significantly outperforms existing methods in both simulated environments and real-world robotic manipulation tasks, demonstrating effective learning and task execution from diverse play data.\n\n# Strength And Weaknesses\nOne of the strengths of the paper is its innovative approach to behavior generation by utilizing a multi-modal transformer, which effectively captures the complexities of uncurated data. The empirical results validate the method's performance improvements over baseline models, particularly in handling multi-modal action distributions. However, the paper also acknowledges some weaknesses, such as limitations in feature representation and challenges faced in simpler tasks, which may affect the generalizability of the findings. The authors propose future work to enhance visual representation learning and expand the dataset size, indicating a thoughtful approach to addressing these weaknesses.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and findings. The authors provide a comprehensive background that contextualizes their work within the existing literature, enhancing understanding. The novelty of the approach is notable, as it combines generative modeling and behavior learning in a way that has not been extensively explored in robotics. Reproducibility is partially addressed through detailed descriptions of the training process and experimental setups, though greater transparency regarding dataset specifics could improve this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of robotics by introducing C-BeT, offering a robust methodology for learning from uncurated play data. While the results are promising, the paper acknowledges limitations that warrant further investigation. The clarity and organization of the paper enhance its impact, making it a valuable contribution to the literature.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel algorithm designed to enhance the performance of deep learning models in the context of few-shot learning. The authors propose a meta-learning framework that integrates a novel attention mechanism to better utilize limited training samples. Through a series of experiments on benchmark datasets, the paper demonstrates that the proposed method significantly outperforms existing approaches, achieving higher accuracy and faster convergence.\n\n# Strength And Weaknesses\n**Strengths:**\n- The proposed framework effectively combines attention mechanisms with meta-learning, resulting in improved performance in few-shot learning tasks.\n- The authors provide a thorough review of the literature, situating their work within the context of existing methodologies and highlighting its relevance.\n- The experimental setup is robust, featuring a variety of datasets and a clear comparison with baseline methods.\n\n**Weaknesses:**\n- While the results are promising, the paper does not fully address potential limitations regarding the generalization of the proposed method to unseen domains or tasks.\n- The evaluation metrics utilized could be more comprehensive to give a fuller picture of the model's performance, particularly in terms of robustness and efficiency.\n- The complexity introduced by the attention mechanism may pose practical challenges for implementation and scalability in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly structured, making it accessible to readers with varying levels of expertise. The descriptions of the methodology and experiments are detailed, although some sections could benefit from additional clarity, particularly regarding the theoretical foundations of the proposed framework. The novelty of the approach is significant, particularly in its integration of attention mechanisms within a meta-learning context. However, reproducibility may be hindered by insufficient details on the implementation and hyperparameter tuning.\n\n# Summary Of The Review\nThis paper presents a novel and significant advancement in few-shot learning through the integration of attention mechanisms in a meta-learning framework. While the contributions are noteworthy and the results promising, addressing the limitations regarding generalization and providing clearer implementation details would enhance the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents Conditional Behavior Transformers (C-BeT), a novel model designed to harness uncurated robot demonstration data for effective robot learning. By leveraging the multi-modal generation capabilities of Behavior Transformers and incorporating future-conditioned goal specifications, C-BeT addresses the challenge of extracting task-specific behaviors from noisy and diverse robot data. The findings indicate that C-BeT achieves an average improvement of 45.7% over previous state-of-the-art methods in learning from play data in simulated benchmark tasks, demonstrating its ability to learn task-centric behaviors without the need for task labels or reward information.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to utilizing reward-free play data for robot learning, significantly advancing the field by eliminating the reliance on labeled data or structured training paradigms. The empirical results are compelling, showing substantial improvement over existing methods. However, a potential weakness is the limited exploration of the model's applicability in varied real-world scenarios, which may affect its generalizability. Additionally, the paper could benefit from a more in-depth discussion on the limitations of the proposed approach and the challenges associated with scaling up the model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers who may not be experts in the specific technical details of robot learning. The methodology is presented in a logical manner, although some sections could be elaborated to enhance understanding. In terms of novelty, the introduction of C-BeT represents a significant contribution to the field, particularly in its application of conditional generation from uncurated data. The reproducibility of the results is supported by the clear methodology, although further details on experimental setups and hyperparameter choices would strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to the field of robotics by demonstrating the effectiveness of Conditional Behavior Transformers in learning from uncurated play data. While the empirical results are impressive, the paper would benefit from a deeper exploration of the model's limitations and generalizability in real-world applications.\n\n# Correctness\nRating: 4/5\n\n# Technical Novelty And Significance\nRating: 5/5\n\n# Empirical Novelty And Significance\nRating: 4/5",
    "# Summary Of The Paper\nThe paper \"From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data\" introduces Conditional Behavior Transformers (C-BeT), a novel approach that extracts task-centric behaviors from noisy, uncurated robot demonstration data, referred to as play data. By leveraging transformer architecture, C-BeT models behavior generation conditioned on future goals while eliminating the need for online training or explicit reward functions. Experimental results demonstrate that C-BeT significantly outperforms baseline methods by an average of 45.7% on simulated benchmark tasks and achieves high success rates in real-world tasks using a Franka Emika Panda robot, suggesting a promising direction for learning useful behaviors in robotics without task labels.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative approach to behavior generation from uncurated data, addressing a significant challenge in robotics. The methodology is robust, enabling the use of transformer architecture to condition behavior generation on future goals, which is a novel contribution in the context of robotics. However, the paper has limitations; specifically, performance can degrade when the learned representations do not adequately capture relevant environment features, and the benefits of the multi-modal approach may not be evident in simpler tasks. These weaknesses highlight the need for further investigation into representation learning and task complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The experimental design is thorough, providing sufficient detail to understand the approaches and results. The novelty of the C-BeT framework is significant, as it combines elements of behavior modeling and future goal conditioning in a way that has not been previously explored in depth within the robotics domain. Reproducibility is facilitated by comprehensive descriptions of experiments and results, although providing access to code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a compelling contribution to the field of robotics by introducing a novel method for behavior generation from uncurated data. While the proposed C-BeT shows promising results and addresses key challenges, some limitations remain regarding representation learning and task complexity that warrant further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data\" introduces a novel method known as Conditional Behavior Transformers (C-BeT) aimed at learning from uncurated robot demonstration data, referred to as play data. The authors claim that their approach significantly improves performance—by 45.7%—over previous state-of-the-art methods in specific tasks without relying on task labels or reward information. The methodology involves multi-modal generation combined with future-conditioned goal specification, with empirical validation conducted on simulated benchmarks such as CARLA and block-pushing tasks.\n\n# Strength And Weaknesses\nThe paper addresses a critical gap in the field by leveraging uncurated data for robotic decision-making, highlighting its innovative approach through C-BeT. The empirical results demonstrate significant performance enhancements, showcasing the potential of learning behaviors without explicit task constraints. However, the work acknowledges limitations in applying the method to certain tasks, particularly those involving knob operations, and emphasizes the dependence on the quality of input features. These weaknesses suggest that while the method is promising, it may require further refinement for broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with clear sections that effectively communicate the methodology and findings. Technical terminology is defined adequately, which aids in understanding the proposed concepts. The novelty of the approach is marked by its focus on uncurated data, a less-explored area in robotics, and the empirical validation strengthens its credibility. However, the reproducibility aspects could be enhanced by providing more detailed descriptions of the dataset preparation and training objectives.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of robotic behavior generation through its innovative use of uncurated data and the introduction of the C-BeT framework. While the results are promising, the limitations noted in certain tasks indicate areas that require further exploration and improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Conditional Behavior Transformers (C-BeT), a novel framework designed for generating task-centric behaviors from uncurated play data in robotics. It addresses the challenges posed by the noise and diversity of such data, integrating multimodal generative capabilities with future-conditioned goal specifications. The authors demonstrate that C-BeT achieves a mean performance improvement of 45.7% over existing state-of-the-art methods across various simulated environments and effectively generalizes to real-world robotic tasks without the need for explicit task labels or reward structures.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to leveraging uncurated play data for robotic behavior generation, as well as the significant empirical performance improvements demonstrated over previous methods. The methodology is well-structured, with a clear formulation of the task and a robust architecture that caters to the complexities of multimodal action generation. However, weaknesses include the identification of failure modes, particularly in tasks requiring precise manipulations, which suggests potential limitations in the representation learning component of the model. Additionally, while the paper outlines future work to enhance these aspects, it could benefit from a more in-depth discussion of the implications of these shortcomings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, with a logical flow that guides the reader through the problem definition, methodology, and experimental evaluations. The clarity of the descriptions of the Conditional Behavior Transformers and their implementation in various environments contributes to the overall quality of the work. The novelty of the proposed framework is significant, as it bridges a gap in existing literature on generative models applied to robotic behavior generation. However, reproducibility may be a concern due to the reliance on large-scale uncurated datasets and complex multimodal training procedures, which may not be easily replicable without access to similar resources.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of robotic behavior generation through the introduction of C-BeT. With its strong empirical results and innovative approach, it addresses a critical gap in the utilization of uncurated play data. However, the identified weaknesses related to specific task performance and the potential challenges in reproducibility warrant attention.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents C-BeT, a novel approach aimed at improving conditional behavior generation in robotic systems. The authors claim that C-BeT achieves a performance improvement of 45.7% over previous methods by utilizing uncurated play data as its primary training source. The methodology involves a complex architecture based on transformer-based generative models, designed to learn behaviors without the need for expert demonstrations. However, the evaluation is primarily conducted on simulated benchmarks, raising concerns about the model's applicability in real-world scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its ambitious goal to enhance behavior generation through innovative use of large, uncurated datasets. The reported performance improvement, while significant, is questionable due to the lack of clarity regarding baseline metrics and potential overfitting. The reliance on play data, which may include irrelevant or misleading behaviors, poses a risk to the robustness of the learned policies. Additionally, the complexity of the model could hinder generalization across different tasks, as evidenced by low success rates in specific applications like knob operation. The evaluation's reliance on simulated environments does not adequately reflect real-world challenges, and the discussion of failure modes is insufficiently detailed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper lacks clarity in several areas, particularly regarding the experimental setup and the justification for the architectural choices made. While the attempt to innovate is commendable, the complexity of the model raises questions about its practicality and reproducibility. Furthermore, the novelty of the approach is undermined by the reliance on existing methods without thorough comparative analysis. The lack of detailed insights into the failure modes and limitations of the model diminishes its overall quality.\n\n# Summary Of The Review\nIn summary, while the paper introduces C-BeT with promising goals in robotic behavior generation, significant concerns regarding the reliance on uncurated data, model complexity, and inadequate evaluation hinder its contributions. The approach lacks the necessary rigor and clarity to be considered a substantial advancement in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Conditional Behavior Transformers (C-BeT), a novel framework for robot behavior generation that leverages uncurated play data without the reliance on task-specific labels or rewards. The methodology focuses on extracting useful behaviors from diverse, noisy data, resulting in significant performance improvements—averaging 45.7% over previous state-of-the-art methods on simulated benchmark tasks. The findings indicate that C-BeT can effectively learn task-centric behaviors from real-world play data, exhibiting high generalization capabilities and minimal requirements for human annotation, thus showcasing its applicability across a variety of real-world tasks.\n\n# Strength And Weaknesses\nC-BeT's strengths lie in its innovative approach to behavior learning, its impressive performance gains, and its robustness across various environments. The ability to learn from uncurated data significantly reduces the burden of data labeling, enhancing practicality in real-world applications. However, the paper could benefit from a more detailed analysis of potential limitations, such as the types of tasks or environments where C-BeT may struggle, and a discussion on the scalability of the approach in more complex scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making the innovative aspects of C-BeT easy to understand. The quality of the experiments is high, with thorough benchmarking against state-of-the-art methods. The novelty of the approach is significant, particularly in its capability to learn from uncurated data. However, the reproducibility could be enhanced by providing more detailed experimental setups and the datasets used for testing, which would allow other researchers to replicate the findings more easily.\n\n# Summary Of The Review\nOverall, this paper presents a groundbreaking advancement in robotic behavior generation through the introduction of C-BeT, demonstrating impressive performance and practical applicability in real-world settings. The innovative methodology and significant findings position it as a potential catalyst for future developments in robotics.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data\" presents a novel approach to behavior generation in robotics using Conditional Behavior Transformers (C-BeT). The main contributions include the theoretical formulation of a conditional behavior learning framework, which models the joint distribution of actions based on observations and future outcomes, and the integration of multi-modal action representations to accommodate the diversity in robot behaviors. The findings suggest that C-BeT enhances the generation of policies capable of generalizing across various environments without relying on explicit reward structures, thus paving the way for more effective learning from uncurated human demonstration data.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative application of generative modeling frameworks to robotics, particularly through the introduction of C-BeT, which theoretically advances the state of conditional policy learning. The multi-modal action representation is a significant contribution that acknowledges the complexities of real-world robot behavior. However, the paper could benefit from more empirical validation, as the theoretical insights may not be fully substantiated by experiments. Additionally, while the theoretical exploration is robust, it may lack practical guidance for implementation in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical constructs clearly, making complex ideas accessible. The quality of the writing is high, with a logical flow that facilitates understanding. The novelty of the approach is substantial, particularly in its integration of generative modeling with robotics. However, reproducibility could be an issue, as the paper does not provide sufficient details on the implementation of C-BeT and its training processes, which could hinder other researchers from effectively reproducing the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical framework for conditional behavior generation in robotics, highlighting the role of multi-modal distributions and future state conditioning. While the contributions are significant and the clarity of presentation is commendable, the lack of empirical validation and detailed methodology may limit the practical impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data\" presents a novel framework known as Conditional Behavior Transformers (C-BeT). This framework integrates Behavior Transformers with future-conditioned goal specification to enhance the generation of robot behaviors from uncurated data. The authors demonstrate that C-BeT achieves a 45.7% performance improvement over previous state-of-the-art methods in simulated benchmark tasks. The methodology involves the use of sequences of observations and actions, dynamic data augmentation during training, and a detailed hyperparameter configuration to optimize performance across various environments.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to behavior generation, leveraging uncurated data effectively. The architecture is built on a solid foundation of existing Behavior Transformers, and the improvements in performance metrics are compelling. However, the paper lacks a broader discussion on the implications of the research, focusing primarily on technical aspects without sufficient exploration of its potential impact in real-world applications or ethical considerations in robotic behavior generation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and provides clear explanations of its methodology, including implementation details and hyperparameter settings. The inclusion of code availability enhances reproducibility, allowing other researchers to validate and build upon the work. The novel integration of future-conditioned goals represents a significant contribution to the field, although the novelty could be better contextualized within existing literature.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the area of conditional behavior generation for robots, with a strong empirical evaluation demonstrating its effectiveness. However, it could benefit from a more thorough discussion of its broader implications and applications beyond technical performance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper introduces C-BeT, a novel framework aimed at enhancing behavior generation in robotic tasks by leveraging transformer architectures and conditioning on future states. The authors assert that C-BeT outperforms prior state-of-the-art methods by an average of 45.7%, primarily through its use of uncurated play data and multi-modal action modeling. They present empirical results demonstrating C-BeT's effectiveness in simulated environments, particularly in long-horizon tasks, and advocate for its applicability to real-world scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper include its ambitious approach to behavior generation and the utilization of transformer architectures, which are currently a focus in the field. However, the paper has notable weaknesses, particularly regarding the novelty of its contributions. Many claims, such as improvements through uncurated data and multi-modal action distributions, have been previously explored by other researchers. Furthermore, the benchmarks used for performance comparisons lack clarity, and the baseline comparisons do not convincingly demonstrate C-BeT's advancements over existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly; however, it lacks sufficient depth in discussing how its contributions differ from prior work. The novelty of C-BeT appears overstated, as it does not convincingly differentiate its approach from existing methodologies. Reproducibility is not thoroughly addressed; details on implementation or datasets used could enhance the ability of others to replicate the study.\n\n# Summary Of The Review\nOverall, while C-BeT presents an interesting approach to behavior generation, the paper does not convincingly establish its contributions as groundbreaking in light of existing research. The claims of superior performance and novelty are undermined by a lack of clarity in comparisons and insufficient differentiation from prior works.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to behavior generation in reinforcement learning (RL) through the integration of multi-task and multi-modal capabilities. The authors propose a framework that enables agents to learn a versatile behavior generation model capable of addressing various tasks with diverse methodologies. The methodology is grounded in empirical evaluations that demonstrate the model's effectiveness in terms of both performance and adaptability across different scenarios, yielding promising results that highlight its potential application in real-world environments.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to combining multi-task and multi-modal learning within a single framework, which is relatively underexplored in existing literature. The empirical results provide robust evidence of the model's capabilities, showcasing its performance against established baselines. However, a notable weakness is the lack of comprehensive clarity in the exposition, particularly with the definitions of key parameters and concepts. Additionally, the paper could benefit from deeper discussions on the implications of its findings and potential limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the overall quality of the research is commendable, the clarity of the exposition could be improved. Certain sections are dense, and critical terms are not consistently defined, which may hinder reproducibility for readers not familiar with the subject. The novelty of the proposed approach is significant, as it introduces a fresh perspective on behavior generation. However, the paper would benefit from additional attention to detail regarding methodological descriptions and clarity in figures and tables.\n\n# Summary Of The Review\nOverall, the paper contributes a significant advancement to the field of reinforcement learning through its innovative framework for behavior generation. While the empirical results are strong, the clarity and reproducibility of the work require enhancement to ensure broader accessibility and understanding. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel methodology for conditional behavior generation from play data, termed C-BeT (Conditional Behavior Transfer), aiming to enhance the performance of robotic systems in executing diverse tasks. The authors focus on the challenges posed by uncurated play data and propose a framework that learns to generate behaviors conditionally based on the observed play data. Key findings indicate that the C-BeT approach demonstrates reasonable task completion rates in specific environments, though it notably lacks exploration of more robust data integration techniques or broader applicability across varied robotic platforms.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical contributions to the field of conditional behavior generation, showcasing how learning from play data can be structured to improve task execution. However, several weaknesses limit its impact. The authors do not explore integrating structured task labels or expert demonstrations, which could enhance model robustness. The study's reliance on limited simulated environments and a solitary real-world robot setup restricts the generalizability of results. Additionally, evaluation metrics are insufficient, as they do not encompass the efficiency of learned policies or interaction quality. The analysis of failure modes is superficial, lacking depth in understanding potential improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting its methodology and findings in a clear manner. However, the novelty of the contributions is somewhat limited due to the lack of exploration of hybrid approaches or comparisons with other frameworks in behavioral cloning and reinforcement learning. The reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setups and methodologies used, particularly in the context of feature representation learning and potential transfer learning strategies.\n\n# Summary Of The Review\nWhile the paper contributes to the field of conditional behavior generation, its impact is mitigated by a lack of exploration of broader data integration techniques and limited generalizability of results. The evaluation metrics and discussions on failure modes could be significantly improved to offer deeper insights and practical implications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents C-BeT, a novel approach for behavior generation from uncurated robot data, addressing the challenges of noisy and multi-modal datasets. The methodology focuses on conditioning actions based on current and future observations, framing the task as a conditional probability distribution. The findings demonstrate a significant performance improvement of **45.7%** over prior state-of-the-art methods across simulated benchmark tasks, with the framework evaluated in both simulated environments and real-world scenarios.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to multi-modal action modeling and the robust performance metrics reported across various environments, indicating a strong potential for generalization and robustness. However, the study lacks rigorous statistical testing to substantiate the significance of the performance improvements, which raises concerns about the validity of the claims made. Additionally, while the evaluation framework is comprehensive, a deeper exploration of failure modes and their statistical implications would enhance the overall analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers. The quality of the writing is high, though the lack of detailed statistical validation diminishes reproducibility. The novel approach to behavior generation and the comparative analysis with baseline methods contribute to its significance in the field, yet the absence of statistical rigor could hinder its acceptance among the community.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for behavior generation with notable performance improvements. However, the lack of comprehensive statistical analysis to support the findings limits the robustness of the claims made. Addressing these gaps could significantly enhance the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces C-BeT, a novel approach aimed at improving the interaction capabilities of robots in complex environments through enhanced feature representation learning. The methodology leverages a combination of deep learning techniques to process visual inputs and generate action policies based on observed data. The findings indicate that while C-BeT demonstrates promising performance in certain scenarios, its effectiveness is heavily influenced by the quality of input features and the complexity of tasks, leading to challenges in generalization and robustness.\n\n# Strength And Weaknesses\nC-BeT's primary strength lies in its capacity to utilize advanced feature representation learning, which allows for some level of generalization to novel scenarios. However, the paper highlights notable weaknesses, including a reliance on high-quality features, limited performance on simpler tasks, and a dependence on larger datasets for training. The identified failure modes, particularly in tasks requiring nuanced understanding, raise concerns about the model's practical applicability. Furthermore, the suggested improvements for future work, such as enhancing visual representation and exploring multi-modal policies, underscore the current limitations of C-BeT.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to readers. However, the reproducibility of results may be hindered by the limited dataset used for training, which restricts the model's applicability across diverse scenarios. The novelty of the approach is commendable, yet the practical significance is tempered by the challenges in generalization and robustness, particularly in real-world applications.\n\n# Summary Of The Review\nOverall, the paper presents an innovative approach to robotic interaction through C-BeT but reveals critical limitations regarding feature dependency and task generalization. While the findings are relevant, the model's current fragility in diverse environments suggests a need for further development to enhance its robustness and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data\" introduces a new method called Conditional Behavior Transformers (C-BeT) aimed at addressing the challenges posed by noisy, diverse, and distributionally multi-modal data in robotics. The authors claim to combine multi-modal generation with future conditioning to enhance the generation of task-centric behaviors. They present experimental results demonstrating a 45.7% improvement in performance on simulated benchmarks using the CARLA environment. Additionally, the authors assert that their approach is a novel contribution to the field by learning behaviors from play data without task labels, although this notion aligns closely with established work in unsupervised learning.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its attempt to leverage uncurated robot data for behavior generation, which is a pertinent challenge in robotics. The integration of multi-modality and future conditioning could potentially yield beneficial outcomes. However, the paper suffers from several weaknesses, including a lack of clear definitions for key terms such as \"task-centric behaviors,\" and an overemphasis on the novelty of combining existing concepts without providing substantial evidence of groundbreaking advancements. The reported improvements, while statistically significant, may not be as impactful given the low baseline performance and the commonality of the benchmark used (CARLA).\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by jargon and a lack of precise definitions, which may hinder understanding for readers not deeply familiar with the topic. The quality of the methodology is presented adequately, but the novelty appears limited, as the concepts discussed are not entirely new to the field of machine learning and robotics. Reproducibility may be a concern due to the vagueness in the experimental setup and the reliance on a well-trodden benchmark.\n\n# Summary Of The Review\nOverall, the paper presents a method that attempts to address notable challenges in the robotics domain but lacks significant novelty and depth. While it introduces a framework that may be useful, it largely repackages existing ideas rather than presenting truly innovative solutions. The experimental results, while noteworthy, do not convincingly demonstrate a transformative advancement in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Conditional Behavior Transformers (C-BeT), a novel approach aimed at improving learning from uncurated play data in robotic applications. The methodology involves combining future-conditioned goal specifications with multi-modal generation capabilities to handle noisy and diverse datasets effectively. The findings demonstrate that C-BeT significantly outperforms existing methods, emphasizing the importance of multi-modal action modeling and conditioning on future states for enhanced decision-making in complex environments.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to address the challenges posed by uncurated play data, which is a significant hurdle in robotic learning. The combination of future conditioning and multi-modal capabilities is particularly noteworthy, as it opens avenues for more robust decision-making. However, there are weaknesses, such as the limited comparative analysis with other state-of-the-art methods and the need for improved visual feature extraction. The discussion on limitations could benefit from deeper exploration of alternative architectures and learning mechanisms that could enhance C-BeT's performance in diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings, making it accessible to the reader. The quality of the experiments and the demonstration of the effectiveness of C-BeT are commendable. In terms of novelty, the paper presents significant advancements in generative modeling for robotics, although further exploration of related methods could enhance its impact. Reproducibility is aided by the clarity of the description of methodologies, though more detailed implementation details would strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of robotic learning by introducing C-BeT, which effectively leverages uncurated play data. While the methodology shows great promise, further exploration of alternative approaches and comparative analyses is recommended to enhance the robustness of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper introduces the Conditional Behavior Transformers (C-BeT) model, which significantly enhances performance in various benchmark tasks related to behavior learning from uncurated play data. The methodology emphasizes the conditional aspect of the model, resulting in notable improvements in simulated environments such as CARLA self-driving, block-pushing, and kitchen tasks, where C-BeT achieved success rates of 0.98, 0.90, and 2.80, respectively. Additionally, the model demonstrated robust performance in real-world tasks, completing an average of 1.1 tasks per run and achieving a cumulative success rate of 24 out of 50 tasks, outperforming other baselines. The findings indicate that C-BeT retains 67% of its performance when conditioned on unseen demonstrations, reflecting its adaptability.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its clear demonstration of C-BeT’s superiority over existing methods, including unconditional Behavior Transformers (BeT), in both simulated and real-world environments. The empirical results are compelling and showcase the model's effectiveness in long-horizon tasks, marking a significant advancement in the field. However, the paper also identifies specific failure modes, particularly in tasks like knob operations, which suggest underlying limitations in representation learning that could be addressed in future work. Furthermore, while the results are impressive, the paper could benefit from a deeper analysis of the failure cases to better understand the limitations of the proposed model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The quality of the experiments is high, with thorough comparisons against state-of-the-art methods. The novelty of introducing a conditional approach to behavior learning is significant, as it addresses previously unmet challenges in the field. Reproducibility is partially supported through detailed performance metrics, but additional information on experimental setups and hyperparameters would enhance the reproducibility of the results.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in behavior learning through the introduction of C-BeT, demonstrating significant improvements over existing models in various tasks. While the empirical results are strong, the identification of failure modes suggests areas for future research. The clarity and quality of the presentation contribute positively to the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to conditional behavior generation in play data, introducing a framework that leverages advanced machine learning techniques to enhance the realism and adaptability of generated behaviors. The methodology involves a combination of supervised learning and reinforcement learning, enabling the model to dynamically adjust its outputs based on evolving context. Key findings demonstrate that the proposed method outperforms traditional models in both qualitative assessments and quantitative benchmarks, suggesting improved applicability for real-world scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of machine learning techniques, which enhances the model's capability to generate complex behaviors. Additionally, the empirical results provide strong evidence of the method's effectiveness, reinforcing its significance in the field. However, weaknesses include the paper's dense writing style and the need for clearer definitions of specific terminology, which may hinder accessibility for a broader audience. Furthermore, the discussion of limitations and future work is somewhat abrupt and could benefit from a more structured approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents interesting and potentially impactful research, its clarity is hindered by complex sentence structures and heavy technical jargon that may not be easily digestible for all readers. Improvements in the organization of the results section and the addition of narrative explanations would enhance the overall quality. The novel contributions are significant, but the reproducibility of the results could be called into question due to the lack of detailed descriptions in the experimental setup and insufficient clarity in figures and tables.\n\n# Summary Of The Review\nOverall, the paper introduces a valuable contribution to conditional behavior generation with promising empirical results. However, issues related to clarity and readability must be addressed to broaden its impact and ensure that the findings are accessible to a wider audience.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.494945327541577,
    -1.7056576969963346,
    -1.8503009930552703,
    -1.8297920497034776,
    -1.8880651934158257,
    -1.7378734188963092,
    -1.788363596622327,
    -1.8529166194494762,
    -1.851493638845005,
    -2.0278250817352617,
    -1.8249563303079017,
    -1.4413011998157486,
    -1.6717561891632589,
    -1.799208603650019,
    -2.064595453507242,
    -1.7522348263033694,
    -1.9234056845912864,
    -1.7527562438673068,
    -1.8975265747347405,
    -1.8288887033548502,
    -1.8832521180760982,
    -1.6740904388977913,
    -2.080147376994079,
    -2.0078078069746392,
    -1.8157627836706312,
    -1.9869595224573096,
    -1.7758874649784406,
    -1.8014386826471882,
    -1.8135932122352116
  ],
  "logp_cond": [
    [
      0.0,
      -2.3358624726939463,
      -2.336015754052831,
      -2.3259552512349586,
      -2.3313472462662688,
      -2.3662164189897332,
      -2.3520240969554,
      -2.336564923235824,
      -2.339301929729077,
      -2.352995512284859,
      -2.330366984019479,
      -2.3847341706977514,
      -2.323064200546926,
      -2.33356257824583,
      -2.3097867008533934,
      -2.332629333695362,
      -2.338680807116256,
      -2.343839109174988,
      -2.3476159537660184,
      -2.342503046032465,
      -2.343743596369246,
      -2.3878613736064063,
      -2.353367803776612,
      -2.3446762772090466,
      -2.3624458360603513,
      -2.338674074219487,
      -2.3365939363901536,
      -2.308926081590933,
      -2.3856790361051803
    ],
    [
      -1.3549734759580472,
      0.0,
      -1.0909970654493475,
      -1.0891913312339168,
      -1.189697523836352,
      -1.3313882112414244,
      -1.2440054598402062,
      -1.171004966960983,
      -1.159375181691491,
      -1.2621643304976506,
      -1.1924718219998425,
      -1.4287535606998576,
      -1.1665137776293288,
      -1.1067199428004337,
      -1.110402633621952,
      -1.1095138350354552,
      -1.2265841758812304,
      -1.2289478405800298,
      -1.2119182444199457,
      -1.178232955544815,
      -1.1973834962901828,
      -1.384498012166948,
      -1.301068835548447,
      -1.2227506936220118,
      -1.3465468235531786,
      -1.108921070268958,
      -1.2069535133975908,
      -1.275842435727729,
      -1.3578585999179509
    ],
    [
      -1.4739289151358854,
      -1.22617245986898,
      0.0,
      -1.2866520421675394,
      -1.323976565428228,
      -1.4982065973453669,
      -1.3993475824390675,
      -1.2997385513066544,
      -1.3124559473660824,
      -1.434156696126586,
      -1.3136852800060475,
      -1.5518309205509677,
      -1.3337167373269996,
      -1.2135544544846704,
      -1.2489614980132422,
      -1.321881612828659,
      -1.3815712534849456,
      -1.3904665866215902,
      -1.346281003124156,
      -1.3635462384979942,
      -1.3737424774069829,
      -1.5410385034946636,
      -1.4572360274935614,
      -1.4173172560496738,
      -1.4443190386374536,
      -1.3018799645212604,
      -1.430700317583651,
      -1.423318652817544,
      -1.534961735598476
    ],
    [
      -1.4474316573604822,
      -1.2890621877340662,
      -1.3024672934311863,
      0.0,
      -1.3134486357522797,
      -1.4785976506348046,
      -1.4603139264581912,
      -1.3684299746705646,
      -1.2384206097145918,
      -1.3936337994927834,
      -1.3615025951454136,
      -1.5375052010520254,
      -1.2423350537758082,
      -1.337175273432511,
      -1.2664771374610253,
      -1.2639479972396508,
      -1.3865856966277974,
      -1.3712686262582796,
      -1.3926657565949627,
      -1.3370212226668234,
      -1.379514026999278,
      -1.5259162243991509,
      -1.4292779785200151,
      -1.3631120463633937,
      -1.456933734338003,
      -1.317136649855904,
      -1.3216414770041742,
      -1.4151781111204693,
      -1.496207710365939
    ],
    [
      -1.5513810439773525,
      -1.4341337472961966,
      -1.3560623687522608,
      -1.3561290958281556,
      0.0,
      -1.5162543223693974,
      -1.5396143482962557,
      -1.4607121485055932,
      -1.4221747538370695,
      -1.4659433552670538,
      -1.4653231144428431,
      -1.6127794761808005,
      -1.4319451749323302,
      -1.4774885620606522,
      -1.4008434596650081,
      -1.3755731202005763,
      -1.4289275713125018,
      -1.3868633587561345,
      -1.5189927721504772,
      -1.5292671236450102,
      -1.4734441425611637,
      -1.5717607638745617,
      -1.5406466186588894,
      -1.4768635526302447,
      -1.4755369354902186,
      -1.4658613362358606,
      -1.4932621974482172,
      -1.4761495534617348,
      -1.5553868957897026
    ],
    [
      -1.436562463469035,
      -1.284665945103398,
      -1.2946996141639855,
      -1.2719299793007142,
      -1.3144988211921635,
      0.0,
      -1.372588537447922,
      -1.313155101892173,
      -1.341715991425404,
      -1.317667758491198,
      -1.3073060556621021,
      -1.439618205721501,
      -1.3143481240889598,
      -1.3558020547304008,
      -1.2858808000023525,
      -1.2603539500186856,
      -1.3127144465979834,
      -1.2847752609480994,
      -1.310554609268321,
      -1.36809653151914,
      -1.3241132560668734,
      -1.4094542530391219,
      -1.3480219102719502,
      -1.3602970663007976,
      -1.3437842039027255,
      -1.328519475268204,
      -1.3358659913370705,
      -1.3128314768837106,
      -1.3611322179570553
    ],
    [
      -1.4314886694929958,
      -1.3338102344189602,
      -1.3366599285708896,
      -1.4031784033451107,
      -1.3867012619930181,
      -1.419259078848766,
      0.0,
      -1.3602396940340198,
      -1.3679226473561412,
      -1.3984234790279964,
      -1.3811042999390113,
      -1.5140477845669258,
      -1.3953610844080464,
      -1.3098423971255804,
      -1.3333226202517583,
      -1.4303765783615123,
      -1.3957819976458217,
      -1.3950790689433745,
      -1.3485037682737817,
      -1.3326201052612336,
      -1.4178488371608176,
      -1.4922248809491236,
      -1.3912092360966908,
      -1.3513191975178203,
      -1.4403371337111885,
      -1.3533607102880236,
      -1.3628016123125686,
      -1.3774011280196514,
      -1.5048907103519242
    ],
    [
      -1.5416187355597541,
      -1.3727604398814974,
      -1.3667478124348555,
      -1.387696999067852,
      -1.4024992976209347,
      -1.4975819241841475,
      -1.4493786161815145,
      0.0,
      -1.4211686309373672,
      -1.4381422954082281,
      -1.4218038732517606,
      -1.5526435028957108,
      -1.440228187602707,
      -1.3773398585440848,
      -1.3359503369567658,
      -1.3990913789535806,
      -1.4120616002453077,
      -1.3901701900639747,
      -1.3328751832036596,
      -1.3662289001743202,
      -1.4042429516821286,
      -1.5218931628257502,
      -1.4814334335448278,
      -1.3963090723082165,
      -1.4579280566491932,
      -1.36060068726487,
      -1.4622793992096546,
      -1.4405498796659066,
      -1.4964583879080116
    ],
    [
      -1.496987861075351,
      -1.3735091943505349,
      -1.3215233280442193,
      -1.2504886624206608,
      -1.3441077932056746,
      -1.5440838878037235,
      -1.4367849558517707,
      -1.4066575844043367,
      0.0,
      -1.4312250830616626,
      -1.331434212944116,
      -1.5884231557739672,
      -1.2512274672648238,
      -1.3164529177468296,
      -1.2527588347753151,
      -1.2130266658755287,
      -1.3701943802273462,
      -1.3614864266924072,
      -1.3749422349890186,
      -1.3508836350803184,
      -1.4123927803436993,
      -1.5320433253207495,
      -1.462048901635793,
      -1.368420572141732,
      -1.462626850434972,
      -1.3524092687933835,
      -1.312141276528354,
      -1.4166808091409124,
      -1.53471415055516
    ],
    [
      -1.7063301313708854,
      -1.5746460931067963,
      -1.6096198602226572,
      -1.588686624465995,
      -1.5959449132431986,
      -1.6716897790850629,
      -1.6988073790325116,
      -1.618489357266575,
      -1.6563249711182348,
      0.0,
      -1.6124953636549173,
      -1.7853551284944456,
      -1.642215603080739,
      -1.6507605898419504,
      -1.595332820628925,
      -1.5999939324592933,
      -1.5448235501590268,
      -1.6305279638635213,
      -1.6206803625303197,
      -1.6860376186318877,
      -1.6006518926474977,
      -1.755316363097988,
      -1.5878915643354237,
      -1.651698814990128,
      -1.6293892056828392,
      -1.607648383368874,
      -1.6308519146828089,
      -1.674422704105806,
      -1.703067618396968
    ],
    [
      -1.4576421489028186,
      -1.2445118230485732,
      -1.2443833803847186,
      -1.2304804816966688,
      -1.314426752792515,
      -1.4189939116822277,
      -1.3974068265334354,
      -1.30818067317251,
      -1.2639735223709014,
      -1.3396583837417035,
      0.0,
      -1.5036118473000883,
      -1.332146995148203,
      -1.292352673348089,
      -1.2508727066430454,
      -1.2186760470608504,
      -1.3301804900511163,
      -1.3234297101688606,
      -1.3206722552716086,
      -1.4194916005470342,
      -1.363566165645742,
      -1.4894656424363177,
      -1.3882650736620903,
      -1.365858777301447,
      -1.3550074891607204,
      -1.2797358416597993,
      -1.3086623628753764,
      -1.335703760117344,
      -1.4238885847369633
    ],
    [
      -1.1757409882608347,
      -1.1580328884765156,
      -1.1488598304720867,
      -1.112137664643146,
      -1.1114559421954777,
      -1.122245119604578,
      -1.1774127943522052,
      -1.145394188603829,
      -1.1383279811244353,
      -1.118170955244592,
      -1.1471680485068096,
      0.0,
      -1.180967751650082,
      -1.1794223921848974,
      -1.1433425350552942,
      -1.1382667176355854,
      -1.0996614343952578,
      -1.1576512241940928,
      -1.1377667343030498,
      -1.1963079170287292,
      -1.1323466794450572,
      -1.1472798241845723,
      -1.179826295953251,
      -1.1514364281327771,
      -1.111651813144564,
      -1.1439521602588794,
      -1.15780068924235,
      -1.1599736173781465,
      -1.1237976591681167
    ],
    [
      -1.3193840358615678,
      -1.1625906875729488,
      -1.1675132736267408,
      -1.12142332864008,
      -1.2017822285992292,
      -1.3088154552621518,
      -1.299127712468717,
      -1.2361025770716294,
      -1.1158130398092356,
      -1.2917461307187033,
      -1.2046534775346591,
      -1.3995865973283015,
      0.0,
      -1.1925464850826473,
      -1.1198727555166368,
      -1.1460500826922912,
      -1.2770502526663665,
      -1.1802548982202088,
      -1.2302763250218713,
      -1.2307549597947949,
      -1.258193421910821,
      -1.3564898852577494,
      -1.3117110400867273,
      -1.2116243928920427,
      -1.344850643329756,
      -1.1765154575170427,
      -1.17456037064318,
      -1.2929740341249611,
      -1.3411972767168443
    ],
    [
      -1.4511565171213536,
      -1.2200016813839853,
      -1.2357314238203145,
      -1.2524037314963594,
      -1.3347693112413763,
      -1.4517330982050998,
      -1.3463487954835065,
      -1.337303884808289,
      -1.229122996659781,
      -1.3768145949207375,
      -1.313532889365053,
      -1.5166287941974927,
      -1.273219675327881,
      0.0,
      -1.2134108102374013,
      -1.340022696101449,
      -1.3966367412085006,
      -1.3171964526812767,
      -1.3310286218484326,
      -1.3010966552715078,
      -1.3745336470074132,
      -1.4586155427276166,
      -1.3793088520916774,
      -1.3459512645275835,
      -1.4453963488262342,
      -1.2722287222518551,
      -1.3324900719786172,
      -1.3944773545154487,
      -1.4761433129586135
    ],
    [
      -1.6966975130898916,
      -1.4686139838298835,
      -1.4744973983501655,
      -1.4756135563555333,
      -1.551723811032508,
      -1.6582490094954139,
      -1.59868865064818,
      -1.5098670213187841,
      -1.4836303960894601,
      -1.6000828678930799,
      -1.5409524247501458,
      -1.7267810128442842,
      -1.4929683401106284,
      -1.4886957285920248,
      0.0,
      -1.4911370521226137,
      -1.5340129912870983,
      -1.5611338610812453,
      -1.522737597418637,
      -1.4939902712666566,
      -1.579008841091396,
      -1.7015451435063913,
      -1.6552348707332007,
      -1.5584622427383843,
      -1.625736466905465,
      -1.4513369512494754,
      -1.5740790669469717,
      -1.5382516110534106,
      -1.679923619033499
    ],
    [
      -1.4204740326740586,
      -1.2053515769522432,
      -1.2371010895541308,
      -1.1688637345398307,
      -1.267436467404343,
      -1.3827999421384978,
      -1.3923070219324143,
      -1.3081917630468807,
      -1.2172312160184178,
      -1.3410275057779446,
      -1.2078887370148372,
      -1.4667115244297715,
      -1.261834055684381,
      -1.3194911938501503,
      -1.2195469667415852,
      0.0,
      -1.2964162572480653,
      -1.3165057632385788,
      -1.3484104336664922,
      -1.3356418078163832,
      -1.3184956268982577,
      -1.447663762448028,
      -1.3708811518253003,
      -1.303330543126038,
      -1.39690552247185,
      -1.291354381091266,
      -1.262900754554978,
      -1.3048094585470726,
      -1.394003463849007
    ],
    [
      -1.5942118009933859,
      -1.4683039183769107,
      -1.4312790162366924,
      -1.3814210758825496,
      -1.4637391982073755,
      -1.5031015307122215,
      -1.5498366576800318,
      -1.4560471760522204,
      -1.4564581924247482,
      -1.463360101790243,
      -1.473071357656105,
      -1.6190902032133507,
      -1.502524533640003,
      -1.521988817726525,
      -1.3905169371037511,
      -1.4060763513023018,
      0.0,
      -1.4563837472983088,
      -1.514239157778218,
      -1.5222004471821842,
      -1.4659420650004,
      -1.5995885176244893,
      -1.4981813253168255,
      -1.5029242247389292,
      -1.4933410803253233,
      -1.4746354942702535,
      -1.5047327521775966,
      -1.4370030087836643,
      -1.5633480829332027
    ],
    [
      -1.3840532339314622,
      -1.1988490407359402,
      -1.1981029412253514,
      -1.2051828090297843,
      -1.2041104067221648,
      -1.3101840446899429,
      -1.3345896461740128,
      -1.2344601043435477,
      -1.2101940413287584,
      -1.3090876164149639,
      -1.2298172171050843,
      -1.4649486883439748,
      -1.2085428875004212,
      -1.2549216562249137,
      -1.205186541677593,
      -1.1816096188787972,
      -1.2391922759663783,
      0.0,
      -1.2787639124835335,
      -1.2900676369399335,
      -1.237480792815612,
      -1.4339893078698271,
      -1.3598953292400815,
      -1.2573213086969748,
      -1.3319562762306678,
      -1.2242547803422321,
      -1.2976534480408655,
      -1.2742308218128982,
      -1.3762862415307626
    ],
    [
      -1.5433306065280072,
      -1.4368882149328113,
      -1.4131856928674158,
      -1.4400048017659335,
      -1.4960669538592968,
      -1.5254736080811881,
      -1.447681459149105,
      -1.3761542425714355,
      -1.4580954324531255,
      -1.4726258098404101,
      -1.4653098687201032,
      -1.5863018078054754,
      -1.505581143651588,
      -1.4603236705754017,
      -1.404046310978691,
      -1.4616419283331834,
      -1.5058350969425123,
      -1.4998635467139225,
      0.0,
      -1.4473140808005986,
      -1.4235155751584738,
      -1.5983812325472604,
      -1.5443114308850843,
      -1.4526070637899804,
      -1.5206993499372814,
      -1.3872825919283573,
      -1.491688391769209,
      -1.5003297098762522,
      -1.5519329373255735
    ],
    [
      -1.4694105431422413,
      -1.2877512971425245,
      -1.2933544124757637,
      -1.2732711211989731,
      -1.3667741437504475,
      -1.454243093738495,
      -1.3244359198293891,
      -1.253861275715752,
      -1.2767573476610392,
      -1.4013220312306904,
      -1.412608928894331,
      -1.502815623262012,
      -1.2907590032020206,
      -1.2518170612881934,
      -1.2068348769398478,
      -1.329728325758864,
      -1.3361312386480517,
      -1.3290400490385703,
      -1.3108215932798328,
      0.0,
      -1.3186048629483507,
      -1.4449694784210465,
      -1.3746795981391688,
      -1.244100041681898,
      -1.4360967738577652,
      -1.2650643383857658,
      -1.3497221929193366,
      -1.4135016553291126,
      -1.4408607678008032
    ],
    [
      -1.5000352965258796,
      -1.3889578318534854,
      -1.392970217721131,
      -1.3849286005012909,
      -1.4269926054804078,
      -1.5027571393031323,
      -1.5202545658472695,
      -1.4275364931955101,
      -1.4663560077973032,
      -1.408302236517427,
      -1.4010448207298767,
      -1.614447230851783,
      -1.4483965204613256,
      -1.4486221988824741,
      -1.4375249937602805,
      -1.4072092749656313,
      -1.3800395677439292,
      -1.4288857611538541,
      -1.4582849821440524,
      -1.4728002093400212,
      0.0,
      -1.574982058838065,
      -1.5418165960877335,
      -1.466964540480711,
      -1.491022924385804,
      -1.4268486343089062,
      -1.4658324030591077,
      -1.4892553605333991,
      -1.5513121572232853
    ],
    [
      -1.3980305385974188,
      -1.3224026757265979,
      -1.3561363158784219,
      -1.358365776718152,
      -1.356945830576771,
      -1.3657468215834003,
      -1.3371624570957565,
      -1.350439326673738,
      -1.324703902525853,
      -1.3623824221631629,
      -1.3450816667250964,
      -1.374428500959406,
      -1.3578767699334724,
      -1.3207188246086015,
      -1.354668423239429,
      -1.332543616230161,
      -1.3577622026587257,
      -1.364993524355176,
      -1.3619859294652643,
      -1.2952106453353873,
      -1.34060235602243,
      0.0,
      -1.3252530567629381,
      -1.2974179560708,
      -1.3455536122144534,
      -1.335523048772507,
      -1.3220718575028783,
      -1.3534976217888635,
      -1.2679008460111407
    ],
    [
      -1.669049521014455,
      -1.642571291576476,
      -1.6018121309131805,
      -1.5966729656222263,
      -1.6161208366713717,
      -1.697271477448347,
      -1.673492909044916,
      -1.6460700616257886,
      -1.584348426637729,
      -1.6083109436559995,
      -1.6106752761114347,
      -1.8093059315622357,
      -1.6521300751108856,
      -1.6251458581641023,
      -1.6261988136864944,
      -1.6120722773851435,
      -1.6270331562918123,
      -1.657794802805789,
      -1.6810433448174493,
      -1.668101906592268,
      -1.6754490503016286,
      -1.7449340761204668,
      0.0,
      -1.6145014868297713,
      -1.682556449512485,
      -1.6581145671916246,
      -1.6129340137795374,
      -1.6203653799267692,
      -1.713600620838203
    ],
    [
      -1.6013690221711512,
      -1.4910852550996592,
      -1.4804986161671352,
      -1.4347044879904345,
      -1.4955939485981444,
      -1.619321923617179,
      -1.5196977835187022,
      -1.4588448518009243,
      -1.4587364094715034,
      -1.5166055864365526,
      -1.4809559859218027,
      -1.716446335303021,
      -1.5171136740004116,
      -1.4745114838122937,
      -1.490952070264444,
      -1.4667908356990027,
      -1.478370076587444,
      -1.4872197131277385,
      -1.4728247413368358,
      -1.436529554279573,
      -1.4694481060132936,
      -1.604385282976101,
      -1.483998813275601,
      0.0,
      -1.5634248827193262,
      -1.4886968809130967,
      -1.4892726252076607,
      -1.573715552930421,
      -1.6314985097854509
    ],
    [
      -1.4690976859463194,
      -1.4566845043202623,
      -1.4213705069617424,
      -1.4040975726679743,
      -1.4275125100690256,
      -1.4586836144405921,
      -1.484734013873844,
      -1.4251560166711204,
      -1.42064629736305,
      -1.4129733595080178,
      -1.3790303978118528,
      -1.5082858970351312,
      -1.469145071075013,
      -1.4712970544672805,
      -1.432959632383858,
      -1.4145322369442441,
      -1.351074464756949,
      -1.4185438016051097,
      -1.4376740211985661,
      -1.5085955203286083,
      -1.4361654834554822,
      -1.4858044969638757,
      -1.4435826155469849,
      -1.4271179264192388,
      0.0,
      -1.4610259196873423,
      -1.3934152590611653,
      -1.4083763276142252,
      -1.4594297252089796
    ],
    [
      -1.6776122483034857,
      -1.4622123737850434,
      -1.4748394045371849,
      -1.4934448521646009,
      -1.55100863537848,
      -1.6324953720370936,
      -1.5667946686065262,
      -1.4801597483702367,
      -1.5010018497887223,
      -1.6026436548579253,
      -1.5568370190490433,
      -1.697168614571028,
      -1.5088895197765881,
      -1.4922805261845653,
      -1.4178249010858537,
      -1.4962953183329053,
      -1.5882345346698674,
      -1.5487053779003725,
      -1.5235342416551967,
      -1.509390587070243,
      -1.558678131264396,
      -1.6900771402143768,
      -1.6467267774625907,
      -1.5328682778398406,
      -1.6393185730140698,
      0.0,
      -1.5618989060561597,
      -1.603983833634675,
      -1.6629389393053107
    ],
    [
      -1.4184502336651637,
      -1.2924300668967845,
      -1.3485643358596862,
      -1.2862456366356987,
      -1.3453666952155712,
      -1.4134760040383227,
      -1.335755680341468,
      -1.3638366488199445,
      -1.2449263960083485,
      -1.3215320914096116,
      -1.3144176954584883,
      -1.5214787290607745,
      -1.28368766510551,
      -1.3220916835565524,
      -1.3157474076328488,
      -1.290263746662856,
      -1.3582273635334496,
      -1.3800068900838762,
      -1.3557597614282286,
      -1.3210569381744008,
      -1.3240099322527707,
      -1.4446385678802542,
      -1.323746624873197,
      -1.296329434384474,
      -1.3432451044221208,
      -1.3095438570956173,
      0.0,
      -1.4063722746080678,
      -1.4425387462624142
    ],
    [
      -1.45009722465462,
      -1.421757751525912,
      -1.3824307816646202,
      -1.414143693834627,
      -1.407838469088461,
      -1.475175674321516,
      -1.4718607424659715,
      -1.4375466163360366,
      -1.406891275570874,
      -1.4488916713939766,
      -1.3918986358943748,
      -1.5654684667449692,
      -1.4504448673241663,
      -1.465474758533302,
      -1.3880368697772016,
      -1.388220402731657,
      -1.430792347069813,
      -1.4619311941405477,
      -1.4458081692207643,
      -1.496825456869392,
      -1.441526644597834,
      -1.542101629329421,
      -1.472182652332605,
      -1.4502880581657376,
      -1.4453122477086873,
      -1.4446082532378677,
      -1.437691792611218,
      0.0,
      -1.5330457539504572
    ],
    [
      -1.4908692715678853,
      -1.4477517165555225,
      -1.4202450301824634,
      -1.4003666701480055,
      -1.3820619844611322,
      -1.4467968584426625,
      -1.4791166601848893,
      -1.4113787625043013,
      -1.4073925611628468,
      -1.4161336990369444,
      -1.4386170617644358,
      -1.4849117899915178,
      -1.4377052359158962,
      -1.485410599584802,
      -1.4240279720680002,
      -1.414709027595489,
      -1.4084049432385708,
      -1.4450192399143234,
      -1.4714826110038297,
      -1.4430817296494558,
      -1.4351246013376866,
      -1.383036389381181,
      -1.4002215328266134,
      -1.4496960858799834,
      -1.4192617436991497,
      -1.4335671773681933,
      -1.4402437889029984,
      -1.4226373724530568,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.15908285484763063,
      0.15892957348874592,
      0.16899007630661833,
      0.16359808127530817,
      0.1287289085518437,
      0.14292123058617712,
      0.15838040430575306,
      0.1556433978124998,
      0.14194981525671801,
      0.164578343522098,
      0.11021115684382554,
      0.17188112699465075,
      0.16138274929574692,
      0.1851586266881835,
      0.1623159938462151,
      0.1562645204253208,
      0.15110621836658877,
      0.14732937377555855,
      0.15244228150911177,
      0.15120173117233104,
      0.10708395393517067,
      0.14157752376496502,
      0.15026905033253035,
      0.13249949148122564,
      0.1562712533220898,
      0.15835139115142338,
      0.18601924595064379,
      0.10926629143639666
    ],
    [
      0.35068422103828745,
      0.0,
      0.6146606315469871,
      0.6164663657624179,
      0.5159601731599825,
      0.37426948575491026,
      0.46165223715612846,
      0.5346527300353516,
      0.5462825153048436,
      0.443493366498684,
      0.5131858749964922,
      0.27690413629647703,
      0.5391439193670058,
      0.5989377541959009,
      0.5952550633743827,
      0.5961438619608794,
      0.47907352111510426,
      0.4767098564163048,
      0.49373945257638896,
      0.5274247414515196,
      0.5082742007061518,
      0.3211596848293867,
      0.40458886144788764,
      0.4829070033743228,
      0.35911087344315606,
      0.5967366267273766,
      0.4987041835987438,
      0.4298152612686057,
      0.34779909707838375
    ],
    [
      0.3763720779193849,
      0.6241285331862902,
      0.0,
      0.5636489508877309,
      0.5263244276270422,
      0.3520943957099034,
      0.45095341061620275,
      0.5505624417486159,
      0.5378450456891879,
      0.4161442969286844,
      0.5366157130492228,
      0.2984700725043026,
      0.5165842557282707,
      0.6367465385705999,
      0.6013394950420281,
      0.5284193802266113,
      0.4687297395703247,
      0.4598344064336801,
      0.5040199899311142,
      0.4867547545572761,
      0.47655851564828744,
      0.30926248956060665,
      0.3930649655617089,
      0.43298373700559645,
      0.4059819544178167,
      0.5484210285340099,
      0.4196006754716193,
      0.4269823402377262,
      0.31533925745679436
    ],
    [
      0.38236039234299546,
      0.5407298619694114,
      0.5273247562722914,
      0.0,
      0.516343413951198,
      0.351194399068673,
      0.3694781232452864,
      0.46136207503291304,
      0.5913714399888859,
      0.4361582502106942,
      0.46828945455806403,
      0.29228684865145227,
      0.5874569959276694,
      0.49261677627096656,
      0.5633149122424523,
      0.5658440524638269,
      0.44320635307568024,
      0.45852342344519803,
      0.4371262931085149,
      0.4927708270366542,
      0.4502780227041996,
      0.3038758253043268,
      0.4005140711834625,
      0.46668000334008397,
      0.37285831536547454,
      0.5126553998475736,
      0.5081505726993034,
      0.4146139385830083,
      0.3335843393375386
    ],
    [
      0.33668414943847313,
      0.4539314461196291,
      0.5320028246635649,
      0.5319360975876701,
      0.0,
      0.3718108710464283,
      0.34845084511957003,
      0.4273530449102325,
      0.4658904395787562,
      0.4221218381487719,
      0.42274207897298255,
      0.27528571723502515,
      0.4561200184834955,
      0.4105766313551735,
      0.48722173375081756,
      0.5124920732152494,
      0.4591376221033239,
      0.5012018346596911,
      0.36907242126534845,
      0.3587980697708155,
      0.41462105085466194,
      0.31630442954126403,
      0.34741857475693627,
      0.41120164078558097,
      0.4125282579256071,
      0.4222038571799651,
      0.3948029959676085,
      0.41191563995409086,
      0.3326782976261231
    ],
    [
      0.30131095542727415,
      0.45320747379291126,
      0.4431738047323237,
      0.46594343959559503,
      0.42337459770414565,
      0.0,
      0.3652848814483871,
      0.4247183170041362,
      0.3961574274709052,
      0.4202056604051112,
      0.4305673632342071,
      0.2982552131748082,
      0.42352529480734935,
      0.38207136416590837,
      0.4519926188939567,
      0.47751946887762364,
      0.4251589722983258,
      0.45309815794820985,
      0.4273188096279883,
      0.3697768873771692,
      0.41376016282943584,
      0.32841916585718733,
      0.389851508624359,
      0.3775763525955116,
      0.39408921499358374,
      0.4093539436281053,
      0.4020074275592387,
      0.4250419420125986,
      0.3767412009392539
    ],
    [
      0.3568749271293312,
      0.45455336220336684,
      0.45170366805143747,
      0.38518519327721634,
      0.4016623346293089,
      0.369104517773561,
      0.0,
      0.42812390258830724,
      0.4204409492661858,
      0.38994011759433067,
      0.40725929668331573,
      0.27431581205540123,
      0.3930025122142806,
      0.47852119949674665,
      0.45504097637056873,
      0.3579870182608147,
      0.39258159897650535,
      0.3932845276789525,
      0.4398598283485453,
      0.4557434913610934,
      0.37051475946150947,
      0.2961387156732034,
      0.39715436052563624,
      0.43704439910450676,
      0.3480264629111385,
      0.4350028863343034,
      0.42556198430975845,
      0.4109624686026756,
      0.2834728862704028
    ],
    [
      0.311297883889722,
      0.48015617956797874,
      0.4861688070146206,
      0.4652196203816241,
      0.45041732182854144,
      0.3553346952653287,
      0.40353800326796163,
      0.0,
      0.43174798851210894,
      0.41477432404124803,
      0.43111274619771556,
      0.3002731165537653,
      0.4126884318467692,
      0.47557676090539136,
      0.5169662824927104,
      0.45382524049589557,
      0.4408550192041685,
      0.4627464293855015,
      0.5200414362458166,
      0.48668771927515597,
      0.4486736677673475,
      0.331023456623726,
      0.37148318590464835,
      0.4566075471412596,
      0.39498856280028294,
      0.49231593218460623,
      0.3906372202398216,
      0.4123667397835695,
      0.3564582315414646
    ],
    [
      0.35450577776965386,
      0.4779844444944701,
      0.5299703108007856,
      0.6010049764243441,
      0.5073858456393303,
      0.30740975104128143,
      0.41470868299323427,
      0.4448360544406682,
      0.0,
      0.4202685557833423,
      0.520059425900889,
      0.26307048307103775,
      0.6002661715801811,
      0.5350407210981754,
      0.5987348040696898,
      0.6384669729694763,
      0.48129925861765877,
      0.4900072121525978,
      0.4765514038559864,
      0.5006100037646866,
      0.43910085850130565,
      0.31945031352425546,
      0.389444737209212,
      0.483073066703273,
      0.38886678841003297,
      0.49908437005162143,
      0.539352362316651,
      0.4348128297040925,
      0.3167794882898449
    ],
    [
      0.32149495036437625,
      0.4531789886284654,
      0.4182052215126044,
      0.4391384572692667,
      0.431880168492063,
      0.35613530265019877,
      0.3290177027027501,
      0.4093357244686866,
      0.3715001106170268,
      0.0,
      0.41532971808034436,
      0.2424699532408161,
      0.38560947865452255,
      0.37706449189331126,
      0.43249226110633665,
      0.4278311492759683,
      0.4830015315762348,
      0.3972971178717404,
      0.407144719204942,
      0.34178746310337393,
      0.4271731890877639,
      0.27250871863727366,
      0.439933517399838,
      0.3761262667451337,
      0.3984358760524225,
      0.4201766983663877,
      0.3969731670524528,
      0.3534023776294557,
      0.3247574633382937
    ],
    [
      0.3673141814050831,
      0.5804445072593285,
      0.5805729499231831,
      0.5944758486112329,
      0.5105295775153866,
      0.40596241862567406,
      0.4275495037744663,
      0.5167756571353916,
      0.5609828079370003,
      0.4852979465661982,
      0.0,
      0.3213444830078134,
      0.4928093351596987,
      0.5326036569598127,
      0.5740836236648563,
      0.6062802832470513,
      0.49477584025678545,
      0.5015266201390411,
      0.5042840750362931,
      0.40546472976086756,
      0.46139016466215965,
      0.335490687871584,
      0.43669125664581143,
      0.4590975530064547,
      0.4699488411471813,
      0.5452204886481025,
      0.5162939674325253,
      0.4892525701905577,
      0.4010677455709384
    ],
    [
      0.2655602115549138,
      0.2832683113392329,
      0.29244136934366183,
      0.32916353517260255,
      0.32984525762027084,
      0.3190560802111706,
      0.2638884054635433,
      0.29590701121191954,
      0.3029732186913132,
      0.3231302445711566,
      0.2941331513089389,
      0.0,
      0.2603334481656665,
      0.26187880763085114,
      0.2979586647604544,
      0.30303448218016316,
      0.3416397654204908,
      0.2836499756216557,
      0.3035344655126988,
      0.24499328278701937,
      0.3089545203706914,
      0.29402137563117625,
      0.2614749038624975,
      0.2898647716829714,
      0.3296493866711845,
      0.2973490395568692,
      0.28350051057339853,
      0.28132758243760203,
      0.31750354064763187
    ],
    [
      0.352372153301691,
      0.50916550159031,
      0.504242915536518,
      0.5503328605231788,
      0.4699739605640296,
      0.36294073390110704,
      0.3726284766945418,
      0.43565361209162945,
      0.5559431493540232,
      0.3800100584445556,
      0.46710271162859973,
      0.27216959183495737,
      0.0,
      0.4792097040806116,
      0.5518834336466221,
      0.5257061064709676,
      0.39470593649689234,
      0.49150129094305006,
      0.4414798641413875,
      0.441001229368464,
      0.4135627672524378,
      0.31526630390550947,
      0.3600451490765315,
      0.46013179627121614,
      0.32690554583350284,
      0.4952407316462162,
      0.4971958185200789,
      0.3787821550382977,
      0.33055891244641455
    ],
    [
      0.34805208652866537,
      0.5792069222660337,
      0.5634771798297045,
      0.5468048721536596,
      0.4644392924086427,
      0.34747550544491923,
      0.4528598081665125,
      0.46190471884173,
      0.570085606990238,
      0.42239400872928146,
      0.4856757142849659,
      0.28257980945252625,
      0.525988928322138,
      0.0,
      0.5857977934126177,
      0.45918590754857,
      0.40257186244151844,
      0.4820121509687423,
      0.4681799818015864,
      0.49811194837851125,
      0.4246749566426058,
      0.3405930609224024,
      0.4198997515583416,
      0.4532573391224355,
      0.35381225482378476,
      0.5269798813981639,
      0.46671853167140176,
      0.40473124913457026,
      0.32306529069140555
    ],
    [
      0.3678979404173506,
      0.5959814696773587,
      0.5900980551570767,
      0.5889818971517089,
      0.5128716424747342,
      0.4063464440118283,
      0.46590680285906205,
      0.554728432188458,
      0.580965057417782,
      0.4645125856141623,
      0.5236430287570963,
      0.33781444066295796,
      0.5716271133966138,
      0.5758997249152173,
      0.0,
      0.5734584013846284,
      0.5305824622201438,
      0.5034615924259969,
      0.5418578560886051,
      0.5706051822405855,
      0.4855866124158461,
      0.36305031000085086,
      0.4093605827740414,
      0.5061332107688579,
      0.43885898660177713,
      0.6132585022577668,
      0.4905163865602704,
      0.5263438424538316,
      0.3846718344737432
    ],
    [
      0.33176079362931077,
      0.5468832493511262,
      0.5151337367492386,
      0.5833710917635386,
      0.48479835889902634,
      0.3694348841648716,
      0.35992780437095506,
      0.4440430632564887,
      0.5350036102849516,
      0.41120732052542475,
      0.5443460892885321,
      0.28552330187359787,
      0.49040077061898835,
      0.432743632453219,
      0.5326878595617841,
      0.0,
      0.45581856905530405,
      0.43572906306479053,
      0.4038243926368772,
      0.41659301848698616,
      0.43373919940511163,
      0.3045710638553414,
      0.381353674478069,
      0.44890428317733133,
      0.3553293038315193,
      0.46088044521210336,
      0.4893340717483914,
      0.4474253677562967,
      0.35823136245436227
    ],
    [
      0.32919388359790047,
      0.4551017662143757,
      0.49212666835459395,
      0.5419846087087368,
      0.45966648638391083,
      0.42030415387906483,
      0.3735690269112546,
      0.4673585085390659,
      0.4669474921665382,
      0.46004558280104346,
      0.4503343269351814,
      0.3043154813779356,
      0.4208811509512833,
      0.4014168668647613,
      0.5328887474875352,
      0.5173293332889846,
      0.0,
      0.4670219372929776,
      0.4091665268130684,
      0.4012052374091022,
      0.45746361959088633,
      0.3238171669667971,
      0.4252243592744609,
      0.4204814598523572,
      0.43006460426596305,
      0.4487701903210328,
      0.41867293241368975,
      0.4864026758076221,
      0.36005760165808365
    ],
    [
      0.3687030099358446,
      0.5539072031313665,
      0.5546533026419553,
      0.5475734348375225,
      0.5486458371451419,
      0.4425721991773639,
      0.418166597693294,
      0.5182961395237591,
      0.5425622025385484,
      0.4436686274523429,
      0.5229390267622225,
      0.2878075555233319,
      0.5442133563668856,
      0.4978345876423931,
      0.5475697021897137,
      0.5711466249885095,
      0.5135639679009285,
      0.0,
      0.4739923313837733,
      0.46268860692737324,
      0.5152754510516948,
      0.31876693599747963,
      0.3928609146272253,
      0.495434935170332,
      0.42079996763663896,
      0.5285014635250747,
      0.4551027958264413,
      0.4785254220544086,
      0.37647000233654415
    ],
    [
      0.35419596820673327,
      0.46063835980192924,
      0.4843408818673247,
      0.45752177296880703,
      0.40145962087544373,
      0.37205296665355236,
      0.44984511558563556,
      0.521372332163305,
      0.43943114228161506,
      0.4249007648943304,
      0.43221670601463735,
      0.31122476692926515,
      0.3919454310831525,
      0.4372029041593388,
      0.4934802637560496,
      0.4358846464015571,
      0.39169147779222824,
      0.397663028020818,
      0.0,
      0.45021249393414187,
      0.4740109995762667,
      0.2991453421874801,
      0.35321514384965624,
      0.4449195109447601,
      0.3768272247974591,
      0.5102439828063832,
      0.40583818296553154,
      0.39719686485848826,
      0.34559363740916704
    ],
    [
      0.3594781602126089,
      0.5411374062123258,
      0.5355342908790865,
      0.5556175821558771,
      0.46211455960440273,
      0.3746456096163553,
      0.5044527835254611,
      0.5750274276390983,
      0.552131355693811,
      0.4275666721241598,
      0.4162797744605191,
      0.3260730800928382,
      0.5381297001528296,
      0.5770716420666568,
      0.6220538264150024,
      0.4991603775959863,
      0.4927574647067985,
      0.4998486543162799,
      0.5180671100750174,
      0.0,
      0.5102838404064995,
      0.3839192249338037,
      0.45420910521568136,
      0.5847886616729523,
      0.39279192949708497,
      0.5638243649690844,
      0.4791665104355136,
      0.41538704802573756,
      0.388027935554047
    ],
    [
      0.3832168215502185,
      0.49429428622261273,
      0.4902819003549672,
      0.4983235175748073,
      0.4562595125956903,
      0.3804949787729659,
      0.3629975522288287,
      0.45571562488058803,
      0.4168961102787949,
      0.47494988155867124,
      0.4822072973462215,
      0.26880488722431517,
      0.43485559761477255,
      0.43462991919362404,
      0.4457271243158176,
      0.47604284311046685,
      0.5032125503321689,
      0.45436635692224403,
      0.4249671359320457,
      0.41045190873607695,
      0.0,
      0.3082700592380332,
      0.3414355219883647,
      0.41628757759538715,
      0.3922291936902942,
      0.456403483767192,
      0.4174197150169905,
      0.393996757542699,
      0.33193996085281285
    ],
    [
      0.27605990030037253,
      0.35168776317119343,
      0.31795412301936943,
      0.31572466217963924,
      0.3171446083210203,
      0.30834361731439097,
      0.33692798180203476,
      0.3236511122240533,
      0.3493865363719384,
      0.31170801673462845,
      0.32900877217269486,
      0.29966193793838536,
      0.3162136689643189,
      0.35337161428918984,
      0.31942201565836226,
      0.34154682266763037,
      0.3163282362390656,
      0.30909691454261523,
      0.312104509432527,
      0.378879793562404,
      0.33348808287536125,
      0.0,
      0.34883738213485316,
      0.3766724828269914,
      0.32853682668333795,
      0.33856739012528436,
      0.35201858139491304,
      0.32059281710892784,
      0.4061895928866506
    ],
    [
      0.4110978559796239,
      0.43757608541760273,
      0.4783352460808983,
      0.48347441137185254,
      0.46402654032270707,
      0.38287589954573176,
      0.4066544679491628,
      0.4340773153682902,
      0.4957989503563498,
      0.47183643333807934,
      0.4694721008826441,
      0.2708414454318431,
      0.4280173018831932,
      0.4550015188299765,
      0.4539485633075844,
      0.46807509960893534,
      0.4531142207022665,
      0.42235257418828986,
      0.3991040321766295,
      0.41204547040181083,
      0.4046983266924502,
      0.335213300873612,
      0.0,
      0.46564589016430746,
      0.39759092748159386,
      0.42203280980245417,
      0.46721336321454143,
      0.4597819970673096,
      0.36654675615587573
    ],
    [
      0.40643878480348805,
      0.5167225518749801,
      0.527309190807504,
      0.5731033189842047,
      0.5122138583764948,
      0.38848588335746026,
      0.488110023455937,
      0.548962955173715,
      0.5490713975031358,
      0.4912022205380866,
      0.5268518210528366,
      0.2913614716716182,
      0.4906941329742276,
      0.5332963231623455,
      0.5168557367101951,
      0.5410169712756365,
      0.5294377303871953,
      0.5205880938469007,
      0.5349830656378034,
      0.5712782526950662,
      0.5383597009613457,
      0.4034225239985383,
      0.5238089936990382,
      0.0,
      0.444382924255313,
      0.5191109260615425,
      0.5185351817669785,
      0.43409225404421825,
      0.3763092971891884
    ],
    [
      0.34666509772431175,
      0.35907827935036885,
      0.39439227670888877,
      0.41166521100265685,
      0.3882502736016056,
      0.357079169230039,
      0.3310287697967871,
      0.3906067669995108,
      0.3951164863075811,
      0.4027894241626133,
      0.4367323858587784,
      0.3074768866355,
      0.34661771259561824,
      0.3444657292033506,
      0.3828031512867731,
      0.401230546726387,
      0.4646883189136821,
      0.3972189820655214,
      0.378088762472065,
      0.3071672633420228,
      0.3795973002151489,
      0.3299582867067554,
      0.3721801681236463,
      0.3886448572513923,
      0.0,
      0.35473686398328885,
      0.4223475246094659,
      0.407386456056406,
      0.35633305846165153
    ],
    [
      0.3093472741538239,
      0.5247471486722661,
      0.5121201179201247,
      0.4935146702927087,
      0.43595088707882956,
      0.35446415042021595,
      0.4201648538507834,
      0.5067997740870729,
      0.48595767266858725,
      0.3843158675993843,
      0.4301225034082663,
      0.2897909078862815,
      0.4780700026807214,
      0.49467899627274425,
      0.5691346213714559,
      0.49066420412440426,
      0.39872498778744214,
      0.438254144556937,
      0.46342528080211287,
      0.47756893538706646,
      0.42828139119291353,
      0.29688238224293273,
      0.3402327449947189,
      0.45409124461746897,
      0.34764094944323976,
      0.0,
      0.4250606164011499,
      0.38297568882263455,
      0.3240205831519989
    ],
    [
      0.357437231313277,
      0.4834573980816561,
      0.4273231291187545,
      0.48964182834274195,
      0.43052076976286946,
      0.3624114609401179,
      0.44013178463697256,
      0.41205081615849615,
      0.5309610689700921,
      0.45435537356882905,
      0.46146976951995233,
      0.2544087359176661,
      0.4921997998729306,
      0.45379578142188826,
      0.4601400573455918,
      0.4856237183155847,
      0.417660101444991,
      0.3958805748945644,
      0.420127703550212,
      0.4548305268040398,
      0.4518775327256699,
      0.3312488970981864,
      0.4521408401052436,
      0.4795580305939666,
      0.4326423605563199,
      0.46634360788282336,
      0.0,
      0.36951519037037284,
      0.33334871871602645
    ],
    [
      0.3513414579925682,
      0.3796809311212761,
      0.419007900982568,
      0.3872949888125612,
      0.3936002135587271,
      0.3262630083256721,
      0.32957794018121667,
      0.36389206631115156,
      0.3945474070763142,
      0.35254701125321164,
      0.4095400467528134,
      0.235970215902219,
      0.3509938153230219,
      0.3359639241138863,
      0.41340181286998656,
      0.4132182799155313,
      0.37064633557737525,
      0.3395074885066405,
      0.35563051342642393,
      0.30461322577779626,
      0.3599120380493541,
      0.25933705331776724,
      0.3292560303145833,
      0.3511506244814506,
      0.35612643493850094,
      0.3568304294093205,
      0.36374689003597016,
      0.0,
      0.26839292869673104
    ],
    [
      0.32272394066732635,
      0.36584149567968915,
      0.39334818205274824,
      0.4132265420872061,
      0.4315312277740795,
      0.36679635379254916,
      0.33447655205032234,
      0.40221444973091036,
      0.4062006510723648,
      0.3974595131982672,
      0.3749761504707758,
      0.3286814222436938,
      0.3758879763193155,
      0.3281826126504097,
      0.3895652401672114,
      0.39888418463972264,
      0.40518826899664084,
      0.3685739723208883,
      0.34211060123138193,
      0.37051148258575584,
      0.37846861089752504,
      0.43055682285403063,
      0.4133716794085982,
      0.3638971263552282,
      0.39433146853606194,
      0.38002603486701836,
      0.3733494233322132,
      0.39095583978215487,
      0.0
    ]
  ],
  "row_avgs": [
    0.15119409522304897,
    0.4822762750172166,
    0.47013510320788005,
    0.45503461204385004,
    0.4145180179291735,
    0.40533934382234316,
    0.3967522913268716,
    0.42692794822709823,
    0.4632909168277778,
    0.387478635179359,
    0.4849118328985885,
    0.29500126142863387,
    0.43341830252154795,
    0.452162014783417,
    0.5026792999060125,
    0.4378214064983406,
    0.4336361570045789,
    0.47650865007100396,
    0.41836684045660916,
    0.4839123606519828,
    0.4216670741584882,
    0.3317544915336484,
    0.42915888944984354,
    0.4934287709380355,
    0.37694092890685055,
    0.42703580721029594,
    0.4286108145724942,
    0.35257110760802285,
    0.38004777949157464
  ],
  "col_avgs": [
    0.3464443603069504,
    0.468420492187328,
    0.4726726076932688,
    0.4856903511495617,
    0.4432424588996406,
    0.35764599443739925,
    0.3866024774333025,
    0.4452986599306893,
    0.4659943299357635,
    0.4103197778050718,
    0.4416711211464646,
    0.28240689040120204,
    0.4441484802874847,
    0.4456351047556499,
    0.4848899647128122,
    0.47244050161011714,
    0.43237200834409156,
    0.4268593785355864,
    0.42561185486038255,
    0.41967924384971234,
    0.42356361691846284,
    0.31709848400316715,
    0.3818081967325006,
    0.4261939436672736,
    0.37842339030326777,
    0.4563050940148486,
    0.42772046372450295,
    0.40609302008137865,
    0.337328761166706
  ],
  "combined_avgs": [
    0.2488192277649997,
    0.4753483836022723,
    0.4714038554505744,
    0.4703624815967059,
    0.42888023841440703,
    0.38149266912987123,
    0.39167738438008703,
    0.43611330407889376,
    0.46464262338177065,
    0.3988992064922154,
    0.4632914770225266,
    0.288704075914918,
    0.43878339140451633,
    0.4488985597695334,
    0.49378463230941233,
    0.4551309540542289,
    0.4330040826743352,
    0.4516840143032952,
    0.42198934765849583,
    0.4517958022508476,
    0.4226153455384755,
    0.32442648776840777,
    0.40548354309117207,
    0.45981135730265454,
    0.37768215960505913,
    0.44167045061257226,
    0.4281656391484986,
    0.37933206384470075,
    0.3586882703291403
  ],
  "gppm": [
    644.4671378764934,
    639.4303350425253,
    636.440314738301,
    631.4970171491648,
    649.0607157185196,
    690.6907889693874,
    675.518540502608,
    649.0680880990194,
    640.9868231893404,
    662.0333875877633,
    651.4135793215931,
    727.6666552949018,
    650.7499255310124,
    648.2965612958661,
    628.5878014394706,
    637.0903696905372,
    654.2464801759845,
    659.5832136516157,
    657.2066642157637,
    663.6259792617979,
    660.7621239667909,
    711.7410617690493,
    674.4243891760636,
    659.2460673825683,
    681.4730942127119,
    640.6450680381604,
    659.1233950352058,
    663.4033721929148,
    700.942366722812
  ],
  "gppm_normalized": [
    1.4866934077268497,
    1.392906119794516,
    1.3834881722864707,
    1.3712028910927416,
    1.4115695996386506,
    1.5025300560480175,
    1.4717703524190422,
    1.4107687048206858,
    1.3901238555514852,
    1.4395008561503826,
    1.4153948253599664,
    1.5883610250513773,
    1.4183888998312553,
    1.4123504034171344,
    1.3636174416793572,
    1.3872226414679047,
    1.4211778897950762,
    1.4327851424865805,
    1.4288622972022331,
    1.4372462607621852,
    1.4314203371676755,
    1.5453555300536252,
    1.4661437838208529,
    1.4243415929582028,
    1.4789708971544948,
    1.3949750242005325,
    1.4310979422579801,
    1.4518128424129348,
    1.5201430109851783
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384
  ],
  "response_lengths": [
    4375,
    2620,
    2410,
    2466,
    2584,
    2400,
    2530,
    2420,
    2354,
    2542,
    2490,
    2559,
    2576,
    2615,
    2351,
    2698,
    2444,
    2312,
    2548,
    2220,
    2144,
    2247,
    2508,
    2083,
    2263,
    2565,
    2328,
    2692,
    2271
  ]
}