{
  "example_idx": 90,
  "reference": "Published as a conference paper at ICLR 2023\n\nSCALE-UP: AN EFFICIENT BLACK-BOX INPUTLEVEL BACKDOOR DETECTION VIA ANALYZING SCALED PREDICTION CONSISTENCY\n\nJunfeng Guo1∗ †, Yiming Li2∗, Xun Chen3‡, Hanqing Guo4†, Lichao Sun5, Cong Liu6 1Department of Computer Science, UT Dallas 2Tsinghua Shenzhen International Graduate School, Tsinghua University 3Samsung Research America, Mountain View 4Department of Computer Science, Michigan State University 5Department of Computer Science, Lehigh University 6Department of Electricity and Computer Engineering Department, UC Riverside\n\nABSTRACT\n\nDeep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries embed a hidden backdoor trigger during the training process for malicious prediction manipulation. These attacks pose great threats to the applications of DNNs under the real-world machine learning as a service (MLaaS) setting, where the deployed model is fully black-box while the users can only query and obtain its predictions. Currently, there are many existing defenses to reduce backdoor threats. However, almost all of them cannot be adopted in MLaaS scenarios since they require getting access to or even modifying the suspicious models. In this paper, we propose a simple yet effective black-box input-level backdoor detection, called SCALE-UP, which requires only the predicted labels to alleviate this problem. Specifically, we identify and filter malicious testing samples by analyzing their prediction consistency during the pixel-wise amplification process. Our defense is motivated by an intriguing observation (dubbed scaled prediction consistency) that the predictions of poisoned samples are significantly more consistent compared to those of benign ones when amplifying all pixel values. Besides, we also provide theoretical foundations to explain this phenomenon. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our defense and its resistance to potential adaptive attacks. Our codes are available at https://github.com/JunfengGo/SCALE-UP.\n\n1\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have been deployed in a wide range of mission-critical applications, such as autonomous driving (Kong et al., 2020; Grigorescu et al., 2020; Wen & Jo, 2022), face recognition (Tang & Li, 2004; Li et al., 2015; Yang et al., 2021), and object detection (Zhao et al., 2019; Zou et al., 2019; Wang et al., 2021). In general, training state-of-the-art DNNs usually requires extensive computational resources and training samples. Accordingly, in real-world applications, developers and users may directly exploit third-party pre-trained DNNs instead of training their new models. This is what we called machine learning as a service (MLaaS).\n\nHowever, recent studies (Gu et al., 2019; Goldblum et al., 2022; Li et al., 2022a) revealed that DNNs can be compromised by embedding adversary-specified hidden backdoors during the training process, posing threatening security risks to MLaaS. The adversaries can activate embedded backdoors in the attacked models to maliciously manipulate their predictions whenever the pre-defined trigger pattern appears. Users are hard to identify these attacks under the MLaaS setting since attacked DNNs behave normally on benign samples.\n\n*The first two authors contributed equally to this paper. †This work was done when Junfeng Guo and Hanqing Guo interned in Samsung Research America. ‡Corresponding Author: Xun Chen (e-mail: xun.chen@samsung.com).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: An illustration of the black-box input-level backdoor detection.\n\nTo reduce backdoor threats, there are many different types of defenses, such as model repairing (Li et al., 2021b; Wu & Wang, 2021; Zeng et al., 2022), poison suppression (Du et al., 2020; Li et al., 2021a; Huang et al., 2022), and backdoor detection (Xiang et al., 2022; Liu et al., 2022; Guo et al., 2022c;d). However, most of these defenses were designed under the white-box setting, requiring accessing or even modifying model weights. Accordingly, users cannot adopt them in MLaaS scenarios. Currently, there are also a few model-level (Huang et al., 2020; Dong et al., 2021; Guo et al., 2022c) and input-level (Li et al., 2021c; Qiu et al., 2021; Gao et al., 2021) black-box backdoor defenses where users can only access final predictions. However, these defenses have some implicit assumptions of backdoor triggers (e.g., a small static patch), leading to being easily bypassed by advanced backdoor attacks (Nguyen & Tran, 2021; Li et al., 2021d). Their failures raise an intriguing question: what are the fundamental differences between poisoned and benign samples that can be exploited to design universal black-box backdoor detection?\n\nIn this paper, we focus on the black-box input-level backdoor detection, where we intend to identify whether a given suspicious input is malicious based on predictions of the deployed model (as shown in Fig. 1). This detection is practical in many real-world applications since it can serve as the ‘firewall’ helping to block and trace back malicious samples in MLaaS scenarios. However, this problem is challenging since defenders have limited model information and no prior knowledge of the attack. Specifically, we first explore the pixel-wise amplification effects on benign and poisoned samples, motivated by the understanding that increasing trigger values does not hinder or even improve the attack success rate of attacked models (as preliminarily suggested in (Li et al., 2021c)). We demonstrate that the predictions of attacked images generated by both classical and advanced attacks are significantly more consistent compared to those of benign ones when amplifying all pixel values. We refer to this intriguing phenomenon as scaled prediction consistency. In particular, we also provide theoretical insights to explain this phenomenon. After that, based on these findings, we propose a simple yet effective method, dubbed scaled prediction consistency analysis (SCALE-UP), under both data-free and data-limited settings. Specifically, under the data-free setting, the SCALEUP examines each suspicious sample by measuring its scaled prediction consistency (SPC) value, which is the proportion of labels of scaled images that are consistent with that of the input image. The larger the SPC value, the more likely this input is malicious. Under the data-limited setting, we assume that defenders have a few benign samples from each class, based on which we can reduce the side effects of class differences to further improve our SCALE-UP.\n\nIn conclusion, our main contributions are four-fold. 1) We reveal an intriguing phenomenon (i.e., scaled prediction consistency) that the predictions of attacked images are significantly more consistent compared to those of benign ones when amplifying all pixel values. 2) We provide theoretical insights trying to explain the phenomenon of scaled prediction consistency. 3) Based on our findings, we propose a simple yet effective black-box input-level backdoor detection (dubbed ‘SCALE-UP’) under both data-free and data-limited settings. 4) We conduct extensive experiments on benchmark datasets, verifying the effectiveness of our method and its resistance to potential adaptive attacks.\n\n2 RELATED WORK\n\n2.1 BACKDOOR ATTACK\n\nBackdoor attacks (Gu et al., 2019; Li et al., 2022a; Hayase & Oh, 2023) compromise DNNs by contaminating the training process through injecting poisoned samples. These samples are crafted by adding adversary-specified trigger patterns into the selected benign samples. Backdoor attacks are stealthy since the attacked models behave normally on benign samples and the adversaries only need to craft a few poisoned samples. Accordingly, they introduce serious risks to DNN-based applications. In general, existing attacks can be roughly divided into two main categories based on the trigger property, including 1) patch-based attacks and 2) non-patch-based attacks, as follows:\n\n2\n\nBackdoor-infected DNNThird-party Device (Black-box Setting)DeployBackdoor DetectionQueryPredictionBackdoor InputMaliciousBinary FileCloud APIEdge DevicePublished as a conference paper at ICLR 2023\n\nPatch-based Backdoor Attacks. (Gu et al., 2019) proposed the first backdoor attack, which was called BadNets. Specifically, BadNets randomly selected and modified a few benign training samples by stamping the trigger patch and changing their label with a pre-defined target label. The generated poisoned samples associated with the remaining benign samples will be released to users to train their models. After that, (Chen et al., 2017) first discussed the attack stealthiness and introduced trigger transparency, where they suggested that poisoned images should be indistinguishable compared with their benign version to evade human inspection. (Turner et al., 2019) argued that making trigger patches invisible is not stealthy enough since the ground-truth labels of poisoned samples are different from the target label. They designed the first clean-label backdoor attack where adversaries can only poison samples from the target class. Recently, (Li et al., 2021c) proposed the first physical backdoor attack, where the location and appearance of the trigger contained in the digitized test samples may be different from that of the one used for training.\n\nNon-patch-based Backdoor Attacks. Different from classical attacks whose trigger pattern is a small patch, recent advanced methods exploited non-patch-based triggers trying to bypass backdoor defenses. For example, (Zhao et al., 2020a) exploited full-image size targeted universal adversarial perturbation (Moosavi-Dezfooli et al., 2017) as the trigger pattern to design a more effective clean-label backdoor attack. (Nguyen & Tran, 2021) adopted image warping as the backdoor trigger, which deforms the whole image while preserving image content. Recently, (Li et al., 2021d) proposed the first poison-only sample-specific trigger patterns, inspired by the DNN-based image steganography (Tancik et al., 2020). This attack broke the fundamental assumption (i.e., the trigger is sample-agnostic) of most existing defenses and therefore could easily bypass them.\n\n2.2 BACKDOOR DEFENSE\n\nCurrently, there are many backdoor defenses to alleviate backdoor threats. In general, existing methods can be roughly divided into two main categories based on the defender’s capacities, including 1) white-box defenses and 2) black-box defenses, as follows:\n\nWhite-box Backdoor Defenses. In these approaches, defenders need to obtain the source files of suspicious models. The most typical white-box defenses are model repairing, aiming at removing hidden backdoors in the attacked DNNs. For example, (Liu et al., 2018a; Wu & Wang, 2021) proposed to remove backdoors based on model pruning; (Li et al., 2021b; Xia et al., 2022) adopted model distillation to eliminate hidden backdoors. There are also other types of white-box defenses, such as poison suppression (Du et al., 2020; Li et al., 2021a; Huang et al., 2022) and trigger reversion (Wang et al., 2019; Hu et al., 2022; Tao et al., 2022). However, users cannot use them under the machine learning as a service (MLaaS) setting, where they can only obtain model predictions.\n\nBlack-box Backdoor Defenses. In these methods, defenders can only query the (deployed) model and obtain its predictions. Currently, there are two main types of black-box defenses, including 1) model-level defenses (Huang et al., 2020; Dong et al., 2021; Guo et al., 2022c) and 2) input-level defenses (Li et al., 2021c; Qiu et al., 2021; Gao et al., 2021). Specifically, the former ones intend to identify whether the (deployed) suspicious model is attacked while the latter ones detect whether a given suspicious input is malicious. In this paper, we focus on the input-level black-box defense since it can serve as the ‘firewall’ helping to block and trace back malicious samples in MLaaS scenarios. However, as we will demonstrate in our experiments, existing input-level defenses can be easily bypassed by advanced backdoor attacks since they have some strong implicit assumptions of backdoor triggers. How to design effective black-box input-level backdoor detectors is still an important open question and worth further exploration.\n\n3 THE PHENOMENON OF SCALED PREDICTION CONSISTENCY\n\nIn this section, we explore the prediction behaviors of benign and poisoned samples generated by attacked DNNs since it is the cornerstone for designing black-box input-level backdoor defense. Before illustrating our key observations, we first review the general process of backdoor attacks.\n\nThe Main Pipeline of Backdoor Attacks. Let D = {(xi, yi)}N i=1 represent a unmodified benign training set and C : X → Y is the deployed DNN, where xi ∈ X = [0, 1]C×W ×H is the image, yi ∈ Y = {1, . . . , K} is its label, and K is the number of different labels. The backdoor adversaries will select some benign samples (i.e., Ds) to generate their modified version\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(a) Benign Model\n\n(b) BadNets\n\n(c) ISSBA\n\nFigure 2: The average confidence (i.e., average probabilities on the originally predicted label) of benign and poisoned samples w.r.t. pixel-wise multiplications under benign and attacked models.\n\nby Dm = {(x′, yt)|x′ = x + g(x), (x, y) ∈ Ds}, where yt is an adversary-specified target label and g(·) is a pre-defined poison generator. For example, g(x) = m ⊙ (t − x) in BadNets (Gu et al., 2019) and blended attack (Chen et al., 2017), where ⊙ represents the element-wise product, m ∈ [0, 1]C×W ×H is a transparency mask, and t ∈ X is the trigger pattern. Given Nb benign samples and Np poisoned samples, the backdoor adversaries will train the attacked DNN f (·; θ) based on the following optimization process (with loss L):\n\nmin θ\n\nNb(cid:88)\n\ni=1\n\nL(f (xi; θ), yi) +\n\nNp (cid:88)\n\nj=1\n\nL(f (x′\n\nj; θ), yt).\n\n(1)\n\nAs preliminarily demonstrated in (Li et al., 2021c), increasing the pixel value of backdoor triggers does not hinder or even improve the attack success rate. However, defenders can not accurately manipulate these pixel values since they have no prior knowledge about trigger location. Accordingly, we explore what will happen if we scale up all pixel values of benign and poisoned images.\n\nSettings. In this section, we adopt BadNets (Gu et al., 2019)) and ISSBA (Li et al., 2021d) as the example for our discussion. They are representative of patch-based and non-patch-based backdoor attacks, respectively. We conduct experiments on the CIFAR-10 dataset (Krizhevsky, 2009) with ResNet (He et al., 2016). For both attacks, we inject a large number of poisoned samples to ensure a high attack success rate (≥ 99%). For each benign and poisoned image, we gradually enlarge its pixel values with multiplication. We calculate the average confidence defined as the average probabilities of samples on the originally predicted label. In particular, we select the label predicted upon the original sample as the originally predicted label for each varied sample and constrain all pixel values within [0, 1] during the multiplication process. More details are in our appendix.\n\nResults. As shown in Figure 2, the average confidence scores of both benign and poisoned samples decrease during the multiplication process under the benign model. In other words, the predictions of modified benign and poisoned samples are changed during this process. In contrast, poisoned and benign samples have significantly distinctive behaviors under attacked models. Specifically, the average confidence of benign samples decreases whereas that of poisoned samples is relatively stable with the increase of multiplication times. We call this phenomenon as scaled prediction consistency.\n\nTo further explain this intriguing phenomenon (i.e., scaled prediction consistency), we exploit recent studies on neural tangent kernel (NTK) (Jacot et al., 2018) for analyzing the backdoor-infected models inspired by (Guo et al., 2022c), as follows:\n\nTheorem 1. Suppose the poisoned training dataset consists of Nb benign samples and Np poisoned samples, i.i.d. sampled from uniform distribution and belonging to K classes. Assume that deep neural network f (·; θ) be a multivariate kernel regression (RBF kernel) with the objective in Eq. (1). For a given attacked sample x′ = (1 − m) ⊙ x + m ⊙ t, we have: limNp→Nb C(n · x′) = yt, n ≥ 1.\n\nIn general, Theorem 1 reveals that when the amount of poisoned samples closes to the benign samples or the attacked DNN over-fits the poisoned samples, it will still constantly predict the scaled attacked samples (i.e., n · x′) as the target label yt. Its proof can be found in Appendix A.\n\n4\n\n13579MultiplicationTimes0.00.20.40.60.81.0AverageConfidenceBenignSamplePoisonedSample135791113MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSample135791113MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSamplePublished as a conference paper at ICLR 2023\n\n(a) BadNets\n\n(b) Label-Consistent\n\n(c) ISSBA\n\nFigure 3: The SPC scores of benign samples from different classes under attacked models.\n\n4 SCALED PREDICTION CONSISTENCY ANALYSIS (SCALE-UP)\n\nMotivated by the phenomenon of scaled prediction consistency demonstrated in the previous section, we propose a simple yet effective black-box input-level backdoor detection, dubbed scaled prediction consistency analysis (SCALE-UP), in this section.\n\n4.1 PRELIMINARIES\n\nDefender’s Goals. In general, defenders have two main goals, including effectiveness and efficiency. Effectiveness requires that the detection method can accurately identify whether a given image is malicious or not. Efficiency ensures that detection time is limited and therefore the deployed model can provide final results on time after the detection and prediction process.\n\nThreat Model. We consider backdoor detection under the black-box setting in machine learning as a service (MLaaS) applications. Specifically, defenders can only query the third-party deployed model and obtain its predictions. They do not have any prior information about the backdoor attack or the model. In particular, we consider two data settings, including 1) data-free detection and 2) data-limited detection. The former one assumes that defenders have no holding benign samples, while the latter one allows defenders to have a few benign samples from each class. Note that we only assume to have the predicted label instead of the predicted probability vector in our method.\n\n4.2 DATA-FREE SCALED PREDICTION CONSISTENCY ANALYSIS\n\nAs demonstrated in Section 3, we can use the average probability on the originally predicted label across its scaled images to determine whether a given suspicious image is malicious. In general, the larger the probability, the more likely the sample is poisoned. However, we can only obtain predicted labels while predicted probability vectors are inaccessible under our settings. Accordingly, we propose to examine whether the predictions of scaled samples are consistent, as follows:\n\nLet S denotes a defender-specified scaling set (e.g., S = {3, 5, 7, 9, 11}). For a given input image x and the deployed classifier C, we define its scaled prediction consistency (SPC) as the proportion of labels of scaled images that are consistent with that of the input image, i.e.,\n\nSP C(x) =\n\n(cid:80)\n\nn∈S\n\nI{C(n · x) = C(x)}\n\n|S|\n\n,\n\n(2)\n\nwhere I is the indicator function and |S| denotes the size of scaling set S. In particular, we constrain n · x ∈ [0, 1] during the multiplication process.\n\nOnce we obtain the SPC value of suspicious input x, we can determine it is malicious based on defender-specified threshold T . If SP C(x) > T , we deem it as a backdoor sample.\n\n4.3 DATA-LIMITED SCALED PREDICTION CONSISTENCY ANALYSIS\n\nIn our data-free scaled prediction consistency analysis, we treat all labels equally. However, we notice that the SPC values of benign samples under attacked models are different across classes (as shown in Figure 3). In other words, some classes are more consistent against image scaling compared to the remaining ones. These benign samples with have high SPC values may be mistakenly treated as malicious samples, leading to relatively low precision of our method.\n\n5\n\n00.10.20.30.4SPC Score00.040.080.120.16DensityClass 1Class 2Class 3Class 4Class 5Class 6Class 7Class 8Class 9Class 1000.10.20.30.4SPC Score00.040.080.120.16DensityClass 1Class 2Class 3Class 4Class 5Class 6Class 7Class 8Class 9Class 1000.10.20.30.4SPC Score00.040.080.120.16DensityClass 1Class 2Class 3Class 4Class 5Class 6Class 7Class 8Class 9Class 10Published as a conference paper at ICLR 2023\n\nFigure 4: The main pipeline of our (data-limited) SCALE-UP. For each suspicious input, it first generates a set of its amplified images. After that, it takes amplified images to query the deployed DNN and obtain their predicted labels. Thirdly, we compute and normalize the SPC value based on the results and that of some local benign samples. SCALE-UP treats the input as a malicious image if the normalized SPC value is greater than a defender-specified threshold T .\n\nFigure 5: The demonstration of various trigger patterns of attacks used in our experiments.\n\nIn data-limited scaled prediction consistency analysis, we assume that defenders have a few benign samples from each class. This setting has been widely used in existing backdoor defenses (Li et al., 2021b; Guo et al., 2022c; Zeng et al., 2022). In this paper, we propose to exploit a set of statics summary (i.e., mean μ and standard deviation σ) of these local benign samples to alleviate this problem, inspired by (Ioffe & Szegedy, 2015). We first estimate the statics summary for SPC values on samples from different classes, based on which to normalize the SPC value of suspicious input images according to their predicted labels. Specifically, for each class i, we calculate its corresponding mean μi and standard deviation σi based on samples Xi belonging to class i, as follows:\n\nμi = Ex∈Xi[SP C(x)],\n\nσi = (cid:112)Ex∈Xi[(x − μi)2].\n\n(3)\n\nIn the detection process, given a suspicious image x and the deployed model C, we normalize the SPC value generated by data-free SCALE-UP based on its predicted label ˆy ≜ C(x), as follows:\n\nN SP C(x) ≜ SP C(x) −\n\nμˆy σˆy\n\n.\n\n(4)\n\nBesides, for a more stable and effective performance, we automatically balance two terms in Eq. (4) to make their values at the same level. The main pipeline of our method is summarized in Figure 4.\n\n5 EXPERIMENTS\n\n5.1 MAIN SETTINGS\n\nDataset and DNN Selection. Following the settings in existing backdoor defenses, we conduct experiments on CIFAR-10 (Krizhevsky, 2009) and (Tiny) ImageNet (Russakovsky et al., 2015) datasets with ResNet (He et al., 2016). Please find more detailed information in our appendix.\n\nAttack Baselines. In this paper, we evaluate our methods under six representative attacks, including 1) BadNets (Gu et al., 2019), 2) label consistent backdoor attack (dubbed ‘Label-Consistent’)\n\n6\n\nAmplifyAmplified Data Infected DNN Normalized SPC (2.69) Label DogLabel Dog ..... Label DeerPoisoned Image Suspicious Data Normalize3x5x7xPredictSPC Score (0.8) Benign Samples (Available) Statics SummaryPoisonedImageTrigger Pattern ISSBAWaNetBadNetsLabel-ConsistentTUAPPhysicalBAPublished as a conference paper at ICLR 2023\n\nTable 1: The performance (AUROC) on the CIFAR-10 dataset. Among all different methods, the best result is marked in boldface while the value with underline denotes the second-best result. The failed cases (i.e., AUROC < 0.55) are marked in red. Note that STRIP require obtaining predicted probability vectors while other methods only need the predicted labels.\n\nAttack→ Defense↓ STRIP ShrinkPad DeepSweep Frequency Ours (data-free) Ours (data-limited)\n\nBadNets Label-Consistent\n\nPhysicalBA TUAP WaNet\n\nISSBA Average\n\n0.989 0.951 0.967 0.891 0.971 0.971\n\n0.941 0.957 0.921 0.889 0.947 0.954\n\n0.971 0.631 0.946 0.881 0.969 0.970\n\n0.671 0.869 0.743 0.851 0.816 0.830\n\n0.475 0.531 0.506 0.461 0.918 0.925\n\n0.498 0.513 0.729 0.497 0.945 0.945\n\n0.758 0.742 0.802 0.745 0.928 0.933\n\nTable 2: The performance (AUROC) on the Tiny ImageNet dataset. Among all different methods, the best result is marked in boldface while the value with underline denotes the second-best result. The failed cases (i.e., AUROC < 0.55) are marked in red. Note that STRIP require obtaining predicted probability vectors while other methods only need the predicted labels.\n\nAttack→ Defense↓ STRIP ShrinkPad DeepSweep Frequency Ours (data-free) Ours (data-limited)\n\nBadNets Label-Consistent\n\nPhysicalBA TUAP WaNet\n\nISSBA Average\n\n0.959 0.871 0.951 0.864 0.936 0.947\n\n0.939 0.938 0.930 0.859 0.904 0.911\n\n0.959 0.672 0.939 0.864 0.939 0.939\n\n0.638 0.866 0.759 0.837 0.763 0.763\n\n0.501 0.498 0.503 0.428 0.943 0.946\n\n0.471 0.492 0.714 0.540 0.948 0.949\n\n0.745 0.737 0.799 0.732 0.905 0.909\n\n(Turner et al., 2019), 3) physical backdoor attack (dubbed ‘PhysicalBA’) (Li et al., 2021c), 4) cleanlabel backdoor attack with targeted universal adversarial perturbation (dubbed ‘TUAP’) (Zhao et al., 2020a), 5) WaNet (Nguyen & Tran, 2021), and 6) ISSBA (Li et al., 2021d). They are the representative of patch-based and non-patch-based backdoor attacks under different settings. For each attack, we randomly select the target label and inject sufficient poisoned samples to ensure the attack success rate ≥ 98% while preserving the overall model performance. We implement these attacks based on the open-sourced backdoor toolbox (Li et al., 2023). We demonstrate the trigger patterns of adopted attacks for Tiny ImageNet in Figure 5. More detailed settings are in the appendix.\n\nDefense Baselines. In this paper, we focus on the backdoor detection under the black-box setting where defenders can only query the deployed model and obtain its predicted label. Accordingly, we compare our methods to ShrinkPad (Li et al., 2021c), DeepSweep (Qiu et al., 2021), and artifacts detection in the frequency domain (dubbed ‘Frequency’) (Zeng et al., 2021). We also compare our methods to STRIP (Gao et al., 2021) that requires additional requirement (i.e., obtaining predict probability vectors). We assume that defenders have 100 benign samples per class under the datalimited setting. Please find more defense details in our appendix.\n\nSettings for Evaluation Datasets. Following the previous work (Lee et al., 2018), we use a positive (i.e., attacked) and a negative (i.e., benign) dataset to evaluate each defense. Specifically, the positive dataset contains the attacked testset and its augmented version, while the negative dataset contains a benign testset and its augmented version. The augmented datasets are created by adding small random noise to their original version. The noise magnitude is set to 0.05. In particular, adding these random noises will not significantly affect the attack success rate and the benign accuracy of deployed models. The introduction of the augmented datasets is to prevent evaluated defenses from over-fitting the benign or the poisoned testsets.\n\nEvaluation Metrics. Following existing detection-based backdoor defenses (Gao et al., 2021; Guo et al., 2022c), we adopt the area under receiver operating curve (AUROC) (Fawcett, 2006) to evaluate defense effectiveness, while use the inference time for evaluating efficiency. In general, the higher the AUROC and the lower the inference time, the better the backdoor detection.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n5.2 MAIN RESULTS\n\nAs shown in Table 1-2, all baseline detection methods fail in defending against some evaluated attacks. Specifically, they have relatively low AUROC in detecting advanced non-patch-based attacks (i.e., WaNet and ISSBA). This failure is mostly because these defenses have some implicit assumptions (e.g., the trigger pattern is sample-agnostic or static) about the attack, which are not necessarily true in practice. In contrast, our methods reach promising performance in all cases on both datasets. For example, the AUROC of our data-limited SCALE-UP is 0.5 greater than all baseline defenses in detecting WaNet on the Tiny ImageNet dataset. Even under the classical patch-based attacks (i.e., BadNets, Label-Consistent, and PhysicalBA), the effectiveness of our methods is on par with or better than all baseline defenses. Our methods are even better than STRIP, which requires obtaining predicted probability vectors instead of predicted labels. We also provide the ROC curves of defenses against all attacks in Appendix N. These results verify the effectiveness of our defenses.\n\nFigure 6: The inference time on the CIFAR-10 dataset.\n\nBesides, we also calculate the inference time of all defenses under the same computational facilities. In particular, we calculate the inference time of methods requiring to obtain the predictions of multiple images by feeding them simultaneously (in a data batch) into the deployed model instead of predicting them one by one. Besides, we only report the inference time of our SCALE-UP under the data-limited setting, since both of them have very similar running times. As shown in Figure 6, our method requires fewer inference times compared to almost all baseline defenses. The only exception is ShrinkPad, whereas its effectiveness is significantly lower than that of our method. Our detection is approximately 5% slower compared with the standard inference process without any defense. These results show the efficiency of our SCALE-UP detection.\n\n5.3 DISCUSSION\n\nIn this section, we discuss whether our method is still effective under different (adversarial) settings.\n\n5.3.1 DEFENDING AGAINST ATTACKS WITH LARGER TRIGGER SIZES\n\nRecent studies (Qiu et al., 2021) revealed that some defenses (e.g., ShrinkPad) may fail in detecting In this part, we use two patch-based attacks (e.g., samples with a relatively large trigger size. BadNets and PhysicalBA) on the Tiny ImageNet dataset for discussion. As shown in Figure 7(a), our methods have high AUROC values (> 0.93) across different trigger sizes under both datafree and data-limited settings, although there are some mild fluctuations. These results verify the resistance of our SCALE-UP detection to adaptive attacks with large trigger patterns.\n\n5.3.2 THE RESISTANCE TO POTENTIAL ADAPTIVE ATTACKS\n\nMost recently, (Qi et al., 2023) demonstrated that reducing the poisoning rate is a simple yet effective method to design adaptive attacks for detection-based defenses, since it can reduce the differences between benign and poisoned samples. Motivated by this finding, we first explore whether our SCALE-UP methods are still effective in defending against attacks with low poisoning rates. We adopt BadNets on the CIFAR-10 dataset as an example for our discussions. In particular, we report the results of all poisoned testing samples and those that can be predicted as the target label, respectively. We design this setting since attacked models may still correctly predict many poisoned samples even if they contain trigger patterns when the poisoning rate is relatively low.\n\nAs shown in Figure 7(b), the attack success rate (ASR) increases with the increase of the poisoning rate. Our method can still correctly detect poisoned samples that can successfully attack the deployed model even when the poisoning rate is set to 0.4% where the ASR is lower than 70%. In these cases, the AUROC > 0.95. Besides, our method can still reach promising performance (AUROC > 0.8) in detecting all poisoned samples. These results verify the resistance of our defense to adaptive attacks with low poisoning rates, where attacked models don’t over-fit backdoor triggers.\n\n8\n\nNoDefenseSTRIPFrequ-encyShrink-PadDeep-weepOurs0.0500.0550.0600.0650.0700.0750.080Inference Time (s)0.0520.0630.070.0530.0670.055Published as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(a) The performance of our Figure 7: The results of additional experiments in our discussion. (b) The attack performance and the defense methods under attacks with different trigger sizes. effectiveness on all poisoned testing samples and those that can successfully attack the deployed model. (c) The effectiveness of adaptive and vanilla backdoor attacks on poisoned samples with random noise under different magnitudes.\n\nTo further evaluate the resistance of our SCALE-UP to potential adaptive methods, we evaluate it under the worst scenario, where the backdoor adversaries are fully aware of our mechanism. Specifically, we design a strong adaptive attack by introducing an additional defense-resistant regularization term to the vanilla attack illustrated in Eq. (1). This regularization term is used to prevent scaled poisoned samples n · x′\n\nj being predicted as the target label yt, as follows:\n\nmin θ\n\nNb(cid:88)\n\ni=1\n\nL(f (xi; θ), yi) +\n\nNp (cid:88)\n\nj=1\n\nL(f (x′\n\nj; θ), yt) +\n\nNp (cid:88)\n\nj=1\n\nL(f (n · x′\n\nj; θ), yj).\n\n(5)\n\nSimilar to previous experiments, we adopt BadNets to design the adaptive attack on the CIFAR-10 dataset. As we expected, this method can bypass our detection resulting in a low AUROC (i.e., 0.467). However, the adaptive attack would make the poisoned samples significantly more vulnerable to small random Gaussian noises. As shown in Figure 7(c), random noises with a small magnitude (< 0.3) will significantly reduce the attack success rate of the adaptive attack, while having minor adverse effects on the vanilla attack. In other words, defenders can easily adopt random noises to defend against this adaptive attack. We speculate that its vulnerability is mostly because the regularization term significantly constrains the generalization of attacked DNNs on the poisoned samples. We will further explore its intrinsic mechanism in our future work.\n\n5.3.3 THE EFFECTIVENESS OF SCALING PROCESS\n\nTechnically, the scaling process in our SCALE-UP detection can be regarded as a data augmentation method generating different modified versions of the suspicious input image. It naturally raises an intriguing question: If other augmentation methods are adopted, is our SCALE-UP detection still effective? Since flip operations and frequency domain analysis have been adopted in (Li et al., 2021c; Zeng et al., 2021) for defense and proved to have minor benefits to detecting advanced backdoor attacks (Li et al., 2021d; Nguyen & Tran, 2021), we investigate the effectiveness of adding increasing magnitudes of random noise. Due to the limitations of space, we include the detailed experimental design and evaluation in Appendix O.\n\n6 CONCLUSION\n\nIn this paper, we proposed a simple yet effective black-box input-level backdoor detection (dubbed SCALE-UP) that can be used in real-world applications under the machine learning as a service (MLaaS) setting. Our method was motivated by an intriguing new phenomenon (dubbed scaled prediction consistency) that the predictions of poisoned samples are significantly more consistent compared to those of benign ones when amplifying all pixel values. We also provided theoretical foundations to explain this phenomenon. In particular, we designed our SCALE-UP detection method under both data-free and data-limited settings. Extensive experiments on benchmark datasets verified the effectiveness and efficiency of our method and its resistance to potential adaptive attacks.\n\n9\n\n8x816x1624x2432x3240x40TriggerSize0.800.850.900.951.00AUROCBadNets(Data-limited)PhysicalBA(Data-limited)BadNets(Data-free)PhysicalBA(Data-free)0.40.60.81.01.2PoisonRate(%)0.60.70.80.91.0ASR/AUROCAttackSuccessRateSCALE-UP(Attacked)SCALE-UP(All)0.00.10.20.30.40.5Magnitude0.00.20.40.60.81.0AttackSuccessRateAdaptiveAttackVanillaAttackPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nDNNs have been widely and successfully adopted in many mission-critical applications. Accordingly, their security is of great significance. The existence of backdoor threats raises serious concerns about using third-party models under the machine learning as a service (MLaaS) setting. In this paper, we propose a simple yet effective black-box input-level backdoor detection. Accordingly, this work has no ethical issues since it does not reveal any new security risks and is purely defensive. However, we need to notice that our methods can only be used to filter poisoned testing samples whereas they do not reduce the intrinsic backdoor vulnerability of deployed models. Our defense also couldn’t recover trigger patterns. People should not be too optimistic about eliminating backdoor threats. We will further improve our method by exploring how to recover triggers.\n\nACKNOWLEDGMENT\n\nThis work is mainly supported by Samsung Research America, Mountain View, CA and partially supported by NSF CNS 2135625, CPS 2038727, CNS Career 1750263, and a Darpa Shell grant.\n\nREPRODUCIBILITY STATEMENT\n\nWe have provided detailed information on datasets, models, training settings, and computational facilities in our main manuscript and appendix. The codes for reproducing our main experiments are also open-sourced at https://github.com/JunfengGo/SCALE-UP.\n\nREFERENCES\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep\n\nlearning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nJiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi Zhu. Federated\n\nclass-incremental learning. In CVPR, 2022.\n\nYinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-\n\nbox detection of backdoor attacks with limited information and data. In ICCV, 2021.\n\nMin Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via\n\ndifferential privacy. In ICLR, 2020.\n\nTom Fawcett. An introduction to roc analysis. Pattern recognition letters, 27(8):861–874, 2006.\n\nYansong Gao, Yeonjae Kim, Bao Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal, Damith C Ranasinghe, and Hyoungshick Kim. Design and evaluation of a multi-domain trojan detection IEEE Transactions on Dependable and Secure Computing, method on deep neural networks. 2021.\n\nMicah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nSorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning\n\ntechniques for autonomous driving. Journal of Field Robotics, 37(3):362–386, 2020.\n\nTianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring\n\nattacks on deep neural networks. IEEE Access, 2019.\n\nHanqing Guo, Yuanda Wang, Nikolay Ivanov, Li Xiao, and Qiben Yan. Specpatch: Human-in-the-\n\nloop adversarial audio spectrogram patch attack on speech recognition. In CCS, 2022a.\n\nHanqing Guo, Qiben Yan, Nikolay Ivanov, Ying Zhu, Li Xiao, and Eric J Hunter. Supervoice: Text-independent speaker verification using ultrasound energy in human speech. In CCS, 2022b.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJunfeng Guo, Ang Li, and Cong Liu. AEVA: Black-box backdoor detection using adversarial ex-\n\ntreme value analysis. In ICLR, 2022c.\n\nJunfeng Guo, Ang Li, and Cong Liu. Backdoor detection in reinforcement learning. arXiv preprint\n\narXiv:2202.03609, 2022d.\n\nJonathan Hayase and Sewoong Oh. Few-shot backdoor attacks via neural tangent kernels. In ICLR,\n\n2023.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, 2016.\n\nXiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. Trigger hunting\n\nwith a topological prior for trojan detection. In ICLR, 2022.\n\nKunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling\n\nthe training process. In ICLR, 2022.\n\nShanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, and Zhuowen Tu. One-pixel signature: Characterizing\n\ncnn models for backdoor detection. In ECCV, 2020.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\n\nreducing internal covariate shift. In ICML, 2015.\n\nArthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-\n\neralization in neural networks. In NeurIPS, 2018.\n\nZelun Kong, Junfeng Guo, Ang Li, and Cong Liu. Physgan: Generating physical-world-resilient\n\nadversarial examples for autonomous driving. In CVPR, 2020.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting\n\nout-of-distribution samples and adversarial attacks. In NeurIPS, 2018.\n\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learn-\n\ning: Training clean models on poisoned data. In NeurIPS, 2021a.\n\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention\n\ndistillation: Erasing backdoor triggers from deep neural networks. In ICLR, 2021b.\n\nYiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor attack in the\n\nphysical world. In ICLR Workshop, 2021c.\n\nYiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transac-\n\ntions on Neural Networks and Learning Systems, 2022a.\n\nYiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, and Shu-Tao Xia. Few-shot backdoor attacks\n\non visual object tracking. In ICLR, 2022b.\n\nYiming Li, Mengxi Ya, Yang Bai, Yong Jiang, and Shu-Tao Xia. BackdoorBox: A python toolbox\n\nfor backdoor learning. arXiv preprint arXiv:2302.01762, 2023.\n\nYuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.\n\nInvisible backdoor\n\nattack with sample-specific triggers. In ICCV, 2021d.\n\nZhifeng Li, Dihong Gong, Xuelong Li, and Dacheng Tao. Learning compact feature descriptor and adaptive matching framework for face recognition. IEEE Transactions on Image Processing, 2015.\n\nKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdoor-\n\ning attacks on deep neural networks. In RAID, 2018a.\n\nYingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu\n\nZhang. Trojaning attack on neural networks. In NDSS, 2018b.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, and Xiangyu Zhang. Com-\n\nplex backdoor detection by symmetric feature differencing. In CVPR, 2022.\n\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal\n\nadversarial perturbations. In CVPR, 2017.\n\nTuan Anh Nguyen and Anh Tuan Tran. Wanet - imperceptible warping-based backdoor attack. In\n\nICLR, 2021.\n\nXiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the as-\n\nsumption of latent separability for backdoor defenses. In ICLR, 2023.\n\nHan Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani Thuraisingham. Deepsweep: An evaluation framework for mitigating dnn backdoor attacks using data augmentation. In AsiaCCS, 2021.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015.\n\nMatthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp: Invisible hyperlinks in physical pho-\n\ntographs. In CVPR, 2020.\n\nXiaoou Tang and Zhifeng Li. Frame synchronization and multi-level subspace analysis for video\n\nbased face recognition. In CVPR, 2004.\n\nGuanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma, Pan Li, and Xiangyu Zhang. Better trigger inversion optimization in backdoor scanning. In CVPR, 2022.\n\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao\n\nLi. Maxvit: Multi-axis vision transformer. In ECCV, 2022.\n\nAlexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks.\n\narXiv preprint arXiv:1912.02771, 2019.\n\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In IEEE S&P, 2019.\n\nLixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi Zhu. Non-transferable learning: A new\n\napproach for model ownership verification and applicability authorization. In ICLR, 2022a.\n\nWenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin Ling, and Ruigang Yang. Salient IEEE Transactions on Pattern\n\nobject detection in the deep learning era: An in-depth survey. Analysis and Machine Intelligence, 44(6):3239–3259, 2021.\n\nZhuoyi Wang, Dingcheng Li, and Ping Li. Latent coreset sampling based data-free continual learn-\n\ning. In CIKM, 2022b.\n\nLi-Hua Wen and Kang-Hyun Jo. Deep learning-based perception systems for autonomous driving:\n\nA comprehensive survey. Neurocomputing, 2022.\n\nDongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In\n\nNeurIPS, 2021.\n\nJun Xia, Ting Wang, Jieping Ding, Xian Wei, and Mingsong Chen. Eliminating backdoor triggers\n\nfor deep neural networks using attention relation graph distillation. In IJCAI, 2022.\n\nZhen Xiang, David J Miller, and George Kesidis. Post-training detection of backdoor attacks for\n\ntwo-class and multi-attack scenarios. In ICLR, 2022.\n\nXiaolong Yang, Xiaohong Jia, Dihong Gong, Dong-Ming Yan, Zhifeng Li, and Wei Liu. Larnet:\n\nLie algebra residual network for face recognition. In ICML, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nYi Zeng, Won Park, Zhuoqing Morley Mao, and R. Jia. Rethinking the backdoor attacks’ triggers:\n\nA frequency perspective. In ICCV, 2021.\n\nYi Zeng, Si Chen, Won Park, Z Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of\n\nbackdoors via implicit hypergradient. In ICLR, 2022.\n\nTongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, and Shu-Tao Xia. Backdoor\n\nattack against speaker verification. In ICASSP, 2021.\n\nShihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-\n\nlabel backdoor attacks on video recognition models. In CVPR, 2020a.\n\nXujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty aware semi-supervised learning\n\non graph data. In NeurIPS, 2020b.\n\nZhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning:\n\nA review. IEEE transactions on neural networks and learning systems, 2019.\n\nZhengxia Zou, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey.\n\narXiv preprint arXiv:1905.05055, 2019.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA THE OMITTED PROOF OF THEOREM 1\n\nTheorem 1. Suppose the poisoned training dataset consists of Nb benign samples and Np poisoned samples, i.i.d. sampled from uniform distribution and belonging to K classes. Assume that deep neural network f adopt RBF kernel and cross-entropy loss with the optimization objective in Eq.1. For a given attacked sample x′ = (1 − m) ⊙ x + m ⊙ t, we have: limNp→Nb C(n · x′) = yt, n ≥ 1.\n\nProof of Theorem 1: Following (Guo et al., 2022c), we have the regression solution for NTK is:\n\nφt(·) =\n\n(cid:80)Nb\n\ni=1 K(·, xi) · yi + (cid:80)Np i=1 K(·, xi) + (cid:80)Np (cid:80)Nb\n\ni=1 K(·, x′ i) · yt i=1 K(·, x′ i)\n\n,\n\n(6)\n\nwhere φt(·) ∈ R is the predictive probability output of f (·; θ) for the target class t and yi is the corresponding one-hot label. K(x, xi) = e−2γ||x−xi||2 (γ > 0). Since the training samples are evenly distributed, there are Nb k benign samples belonging to yt. Without loss of generality, we assume the target label yt = 1 while others are 0. Then the regression solution can be converted to:\n\nφt(·) =\n\n(cid:80)Nb/k\n\ni=1 K(·, xi) + (cid:80)Np i=1 K(·, xi) + (cid:80)Np\n\ni=1 K(·, x′ i) i=1 K(·, x′ i)\n\n(cid:80)Nb\n\n,\n\nFor a given backdoored sample x′ = (1 − m) ⊙ x + m ⊙ t, we can simplify Eq. (7) as:\n\nφt(x′) ≥\n\n(cid:80)Np i=1 K(x′, x′ i) i=1 K(x′, xi) + (cid:80)Np\n\n(cid:80)Nb\n\ni=1 K(x′, x′ i)\n\n,\n\n(7)\n\n(8)\n\nwe here remove the term (cid:80)Nb/k yt and (cid:80)Nb/k poisoned sample.\n\ni=1 K(x′, xi) << (cid:80)Np\n\ni=1 K(x′, xi). This is because x′ typically don’t belong to the target i), otherwise the attacker has no incentive to craft\n\ni=1 K(x′, x′\n\nWhen Np close to Nb, which implies that the poisoning rate close to 50%, the attacker can achieve the optimal attack efficacy (Liu et al., 2018b; Gu et al., 2019; Li et al., 2021d). Given K(x, xi) = e−2γ||x−xi||2\n\n(γ > 0), if Np = Nb, we have:\n\nφt(n · x′) =\n\n(cid:80)Np i=1 K(n · x′, xi) + (cid:80)Np\n\ni=1 K(n · x′, x′ i)\n\n(cid:80)Nb\n\ni=1 K(n · x′, x′ i)\n\n.\n\n(9)\n\nIf n = 1, we can easily obtain that:\n\nNp (cid:88)\n\ni=1\n\ne−2γ||(1−m)⊙(x−xi)||2\n\n− e−2γ||(1−m)⊙(x−xi)+m⊙(t−xi)||2\n\n=\n\nNp (cid:88)\n\ni=1\n\ne−2γ||(1−m)⊙(x−xi)||2\n\n(1 − e−2γ||m⊙(t−xi)||2\n\n) > 0.\n\n(10)\n\n(11)\n\nSince the internal term (1 − e−2γ||m⊙(t−xi)||2 f (x′) = yt, which is also consistent with the practice.\n\n) can be always larger than 0, thus it is clear that\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nHowever, when n > 1, to compare (cid:80)Np\n\ni=1 K(n · x′, x′\n\ni) and (cid:80)Nb\n\ni=1 K(n · x′, xi), we have:\n\nNo(cid:88)\n\ni=1\n\nK(n · x′, x′\n\ni) −\n\nNb(cid:88)\n\ni=1\n\nK(n · x′, xi)\n\n=\n\n=\n\nNb(cid:88)\n\ni=1\n\nNb(cid:88)\n\ni=1\n\ne−2γ||(1−m)⊙(n·x−xi)+m⊙(n−1)t||2\n\n− e−2γ||(1−m)⊙(n·x−xi)+m⊙(n·t−xi)||2\n\ne−2γ||(1−m)⊙(n·x−xi)+m⊙(n−1)t||2\n\n(1 − e−2γ(||m⊙(n·t−xi)||2−||m⊙(n−1)t)||2)).\n\nRegarding the internal term ||m ⊙ (n · t − xi)||2 − ||m ⊙ (n − 1)t)||2, we have:\n\n||m ⊙ (n · t − xi)||2 − ||m ⊙ (n − 1)t)||2\n\n=\n\n=\n\n(cid:88)\n\nj,k∈ trigger (cid:88)\n\nj,k∈ trigger\n\n||(n − 1) · tj,k + (tj,k − xi,j,k)||2 − ||(n − 1) · tj,k||2\n\nδ2 i,j,k + 2(n − 1) · δi,j,ktj,k,\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\nwhere δi,j,k is the pixel-level residue between the trigger and benign samples. We assume that the δi,j,ktj,k close to a zero mean for inputs, thus we can rewrite Eq.(12) as follows:\n\nNp (cid:88)\n\ni=1\n\nK(n · x′, x′\n\ni) −\n\nNb(cid:88)\n\ni=1\n\nK(n · x′, xi)\n\ne−2γ||(1−m)⊙(n·x−xi)+m⊙(n−1)t||2\n\n(1 − e−2γ (cid:80)\n\nj,k∈trigger δ2\n\ni,j,k )\n\n≈\n\nNp (cid:88)\n\ni=1\n\n> 0.\n\nPut Eq. (19) and Eq. (9) together, we know that φt(n · x′) ≥ 0.5, as Np → Nb, we have:\n\nlim Np→Nb\n\nC(n · x′\n\nt) = yt, n ≥ 1.\n\n(19)\n\n(20)\n\n(21)\n\n□\n\nB THE DETAILED CONFIGURATIONS OF THE EMPIRICAL STUDY\n\nWe adopt BadNets (Gu et al., 2019)) and ISSBA (Li et al., 2021d) as the example for our discussion. They are the representative of patch-based and non-patch-based backdoor attacks, respectively. We conduct experiments on the CIFAR-10 dataset (Krizhevsky, 2009) with ResNet-34 (He et al., 2016). For both attacks, we inject a large number of poisoned samples to ensure a high attack success rate (≥ 99%). For each benign and poisoned image, we gradually enlarge its pixel values with multiplication. We calculate the averaged confidence defined as the averaged probabilities of samples on the originally predicted label. In particular, we select the label predicted upon the original sample as the originally predicted label for each varied sample and constrain all pixel values within [0, 1] during the multiplication process. In particular, we follow previous works (Gu et al., 2019; Li et al., 2021d) to implement the backdoor attacks. Specifically, the trigger for BadNets is a 4 × 4 square consisting of random pixel values; the trigger of ISSBA is generated via DNN-based image steganography (Tancik et al., 2020). Both attacks are implemented via BackdoorBox (Li et al., 2023).\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Detailed information about the adopted datasets.\n\nDataset\n\n# Classes\n\nImage Size\n\n# Training Images\n\nCIFAR-10 Tiny ImageNet\n\n10 200\n\n3 × 32 × 32 3 × 64 × 64\n\n50,000 1,000,000\n\nC THE DETAILS FOR TRAINING ATTACKED MODELS\n\nWe train backdoor-infected models using BackdoorBox (Li et al., 2023). We set the training epoch as 200 and the poisoning rate as 5 − 10% for each attack to ensure a high attack success rate. In particular, except for PhysicalBA, we don’t involve additional data augmentation in training infected models as we want to better reveal the properties of various backdoor approaches. For each infected model, we randomly select infected labels to ensure their predictions on benign inputs are similar to the benign models, which ensures the stealthiness of backdoor attacks. Regarding the data-limited scenario and ablation study, we intentionally affect multiple labels to ensure the infected models similar to benign models except for the Trojan behaviors. Specifically, we inject less amount(≤ 5%) of poisoned samples to affect labels other than the target label. This is because previous work (Guo et al., 2022c) found that certain dense backdoor attacks (e.g., ISSBA, WaNet) would make the infected DNNs sensitive to noisy or out-of-distribution samples on CIFAR-10 dataset. Accordingly,they are less stealthy and are easy to be detected during the sampling process of the datalimited setting and settings of our ablation study. As such, in these settings, to evaluate SCALE-UP in a rather practical scenario, we train infected DNNs to have similar behaviors on noisy samples as the benign DNNs. The details for each dataset are included in Table 3.\n\nC.1 THE ACCURACY AND ATTACK SUCCESS RATE (ASR) FOR EVALUATED MODELS\n\nThe accuracy and ASR for the evaluated models for each task in included in Table 4.\n\nTable 4: The BA and ASR for the evaluated models on each dataset.\n\nTask↓ Model→\n\nInfected Model BA\n\nASR\n\nNormal Model Accuracy\n\nCIFAR-10\n\n≥ 90.04% ≥ 97.7% Tiny ImageNet ≥ 36.98% ≥ 97.22%\n\n≥ 92.31% ≥ 40.11%\n\nTable 5: The performance of six defense baselines against partial backdoor attacks\n\nTask↓ Attack → STRIP 0.617 0.601\n\nCIFAR-10 Tiny ImageNet\n\nShrinkPad 0.949 0.868\n\nFrequency DeepSweep Ours (data-free) Ours (data-limited)\n\n0.891 0.861\n\n0.967 0.951\n\n0.971 0.936\n\n0.971 0.971\n\nD THE DETAILED CONFIGURATIONS FOR BASELINE DEFENSES\n\n• STRIP: We implement STRIP following their official open-sourced codes*. • ShrinkPad: We implement ShrinkPad following their official open-sourced codes†. • Frequeny: We implement Frequency approach following their official codes‡.\n\n• DeepSweep: We implement DeepSweep using Scipy package to remove the highfrequency noise and use torchvision.transforms and keras.preprocess packages to conduct transformation to inputs. Notably, we don’t apply finetune process within DeepSweep since we only focus on the black-box detection scenarios.\n\n*https://github.com/garrisongys/STRIP.git †https://github.com/THUYimingLi/BackdoorBox.git ‡https://github.com/YiZeng623/frequency-backdoor.git\n\n16\n\nPublished as a conference paper at ICLR 2023\n\n(a) Trigger I\n\n(b) Trigger II\n\n(c) Trigger III\n\nFigure 8: The demonstration of dynamic triggers.\n\nE THE DESCRIPTIONS FOR MAIN EVALUATION METRIC\n\n• The receiver operating curve (ROC) shows the trade-off between detection the success rate for poisoned samples and detection error rate for benign samples across different decision thresholds T under infected-DNNs.\n\n• Inference Time: we implement each approach under the platform with one NVIDIA GPU 1080 Ti and a Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz with batch size = 1. We test the inference time of each approach with an average of 1,000 runs.\n\nF SETTINGS FOR MEASURING THE INFERENCE TIME\n\nSince we focus on defending against backdoor attacks in the inference phase, we here measure the inference time by:\n\n• Identifying whether the input sample is poisoned or not.\n\n• If the input is a benign sample, we next should use the target model to predict it.\n\nFor STRIP, ShrinkPad, DeepSweep, and SCALE-UP, we leverage the target model’s prediction on the (augmented) inputs for defense purpose, which means the input can be identified and predicted at the same time. As for Frequency, which leverages a secondary neural network to predict the frequency domain of each given input. However, if the input is identified as benign, the target DNNs should also deliver prediction on it. We here assume the benign and poisoned samples have equal possibilities. Therefore, we calculate the inference time for Frequency as follows:\n\ntime = TIME(Frequency(input)) + 0.5 · TIME(DNN(input)).\n\nWhile for other approaches, we measure their inference time via:\n\ntime = TIME(DNN((Augumented) INPUT))).\n\n(22)\n\n(23)\n\nIn particular, we calculate the inference time of methods required to obtain the predictions of multiple images by feeding them simultaneously (in a batch) into the deployed model instead of predicting them one by one. This approach is feasible since defenders can easily and efficiently obtain all of them before feeding them into the deployed model.\n\nG PERFORMANCE UNDER MULTIPLE-BACKDOOR TRIGGERS WITHIN A\n\nSINGLE INFECTED LABEL\n\nConsistent with (Guo et al., 2022c; Wang et al., 2019), we also evaluate the efficacy of SCALE-UP under a more challenging scenario where multiple backdoors are embedded within a single target label. We randomly select a label as the infected label and inject various types of poisoned samples in the training phase. We inject arbitrary amounts of poison samples for each backdoor trigger to ensure the attack efficacy ASR ≥ 99%. The demonstrations for used backdoor triggers are shown in Figure 8. Under such considered scenario, we evaluate our SCALE-UP on CIFAR-10 and Tiny ImageNet datasets using ResNet-34.\n\n17\n\nFigure 10: The performance of SCALE-UP for multiple infected labels.\n\n0102030405060010203040506001020304050600102030405060010203040506001020304050600.20.40.60.81Proportionofinfectedlabels0.50.60.70.80.9DetectionPerformance(AUROC)Data-freeData-limitedPublished as a conference paper at ICLR 2023\n\n(a) CIFAR-10\n\n(b) TinyImageNet\n\nFigure 9: The average results for multiple triggers within a single label.\n\nAs shown in Figure 9, SCALE-UP performs resilient to the increasing number of injected backdoor triggers. This may be caused by infected models already generalized for backdoor triggers.\n\nH THE PERFORMANCE AGAINST MULTIPLE INFECTED LABELS\n\nWe also test SCALE-UP under the scenario where the suspicious model has multiple infected labels. Under this scenario, we test SCALE-UP on CIFAR-10. This is because models on Tiny ImageNet would be prone to multiple infected labels, as reported by (Guo et al., 2022c), affecting more than 14% labels can make the accuracy significantly drop≥ 3%. We implement BadNets as backdoor attacks, the trigger size is 4 × 4. The results are shown in Figure 10. These results show that affecting multiple infected labels could slightly reduce the performance of SCALE-UP. Besides, the data-limited scenario performs better than the data-free scenario. However, even with 100% labels are infected, SCALE-UP can still perform effectively with AUROC ≥ 0.883.\n\nI\n\nIMPACTS FOR THE NUMBER OF COEFFICIENTS\n\nWe test SCALE-UP on six attacks with varying n. We here use ResNet-34 on TinyImageNet for evaluation. As shown in Figure 11, we find that the performance of SCALE-UP increases along with n increasing. Moreover, we find that SCALE-UP performs more sensitive on n for the TUAP attack compared with other attack techniques. Moreover, we find that SCALE-UP performs similarly sensitive on n in both data-limited and data-free settings. In most settings, with n ≥ 11 SCALE-UP can achieve optimal performance on six different attacks.\n\nJ\n\nIMPACT FOR THE SIZE OF LOCAL SAMPLES\n\nWe also test the sensitivity of SCALE-UP on the size of local samples per label under the datalimited setting. We test SCALE-UP on Tiny ImageNet using ResNet-34 against six attacks. The results are shown in Figure 12. We can see that with the size of local samples increases, the performance of SCALE-UP improves and achieves optimal performance when the size ≥ 100.\n\nK PERFORMANCE UNDER SOURCE-LABEL-SPECIFIC BACKDOOR SCENARIOS\n\nThe Source-label-specific (Partial) backdoor scenarios is that the backdoor attacks can perform effectively when it applies to images of a certain specific class. Such a scenario makes backdoor attacks very hard to detect (Wang et al., 2019; Gao et al., 2021), thus the attacker may have a great incentive to implement such a backdoor attack in the real world. Therefore, we evaluate SCALE-UP under such a practical scenario and compare SCALE-UP with previous work. We test SCALE-UP using ResNet-34 on CIFAR-10 and Tiny ImageNet. As shown in Table 5, we find that most defense\n\n18\n\n12345Numberofinjectedtriggers0.96000.96250.96500.96750.97000.97250.9750DetectionPerformance(AUROC)Data-freeData-limited12345Numberofinjectedtriggers0.93750.94000.94250.94500.94750.9500DetectionPerformance(AUROC)Data-freeData-limitedPublished as a conference paper at ICLR 2023\n\n(a) BadNets\n\n(b) TUAP\n\n(c) Label-Consistent\n\n(d) PhysicalBA\n\n(e) ISSBA\n\n(f) WaNet\n\nFigure 11: The impact for the coefficients n.\n\napproaches perform resilient against the partial backdoor attack except STRIP (Gao et al., 2021). This is because STRIP assumes the trigger can perform effectively across various images. Under this scenario, SCALE-UP can outperform all baseline defenses.\n\nL THE ROBUSTNESS OF SCALE-UP\n\nSince SCALE-UP is an inference-phase backdoor defense approach, it is necessary to investigate the robustness of SCALE-UP on benign and poisoned samples. Following previous work (Du et al., 2020), we evaluate the robustness of our approach by testing different magnitudes of noisy inputs. Notably, we test SCALE-UP on benign and poisoned samples, respectively, which is because they exhibit different robustness under random noise as we show in Section 5.3.3. Moreover, the magnitudes of added random noise ensure the classification accuracy and attack success rate for benign and poisoned samples. The results are shown in Figure 13. We test our approach using ResNet-34\n\n19\n\n02468101214n0.50.60.70.80.91.0DetectionPerformance(AUROC)Data-freeData-limited02468101214n0.500.550.600.650.700.750.800.85DetectionPerformance(AUROC)Data-freeData-limited02468101214n0.50.60.70.80.91.0DetectionPerformance(AUROC)Data-freeData-limited02468101214n0.50.60.70.80.91.0DetectionPerformance(AUROC)Data-freeData-limited02468101214n0.50.60.70.80.91.0DetectionPerformance(AUROC)Data-freeData-limited02468101214n0.50.60.70.80.91.0DetectionPerformance(AUROC)Data-freeData-limitedPublished as a conference paper at ICLR 2023\n\n(a) BadNets\n\n(b) TUAP\n\n(c) Label-Consistent\n\n(d) PhysicalBA\n\n(e) ISSBA\n\n(f) WaNet\n\nFigure 12: The impact for the size of required inputs.\n\n(a) BadNets\n\n(b) PhysicalBA\n\nFigure 13: The robustness of SCALE-UP.\n\non the TinyImageNet task. The noise is randomly sampled from Gaussian distribution and we intentionally filter the failed poisoned samples. We only test BadNets and PhysicalBA since only these\n\n20\n\n0255075100125150175200sizeofrequiredinputs0.93250.93500.93750.94000.94250.94500.9475DetectionPerformance(AUROC)Data-limited0255075100125150175200sizeofrequiredinputs0.760.780.800.820.84DetectionPerformance(AUROC)Data-limited0255075100125150175200sizeofrequiredinputs0.8900.8950.9000.9050.9100.915DetectionPerformance(AUROC)Data-limited0255075100125150175200sizeofrequiredinputs0.880.900.920.940.960.981.00DetectionPerformance(AUROC)Data-limited0255075100125150175200sizeofrequiredinputs0.94800.94820.94840.94860.94880.9490DetectionPerformance(AUROC)Data-limited0255075100125150175200sizeofrequiredinputs0.94800.94850.94900.94950.95000.95050.9510DetectionPerformance(AUROC)Data-limited0.00.10.20.30.40.5Magnitudesofrandomnoise0.50.60.70.80.9DetectionPerformance(AUROC)Data-freeData-limited0.00.10.20.30.40.5Magnitudesofrandomnoise0.50.60.70.80.9DetectionPerformance(AUROC)Data-freeData-limitedPublished as a conference paper at ICLR 2023\n\nTable 6: The performance (AUROC) on the Tiny ImageNet dataset under VGG-19. Among all different methods, the best result is marked in boldface while the value with underline denotes the second-best result. The failed cases (i.e., AUROC < 0.55) are marked in red. Note that STRIP requires obtaining predicted probability vectors while other methods only need the predicted labels.\n\nAttack→ Defense↓ STRIP ShrinkPad DeepSweep Frequency Ours (data-free) Ours (data-limited)\n\nBadNets Label-Consistent\n\nPhysicalBA TUAP WaNet\n\nISSBA Average\n\n0.941 0.857 0.939 0.864 0.936 0.936\n\n0.908 0.919 0.907 0.859 0.846 0.851\n\n0.941 0.631 0.921 0.864 0.907 0.907\n\n0.576 0.831 0.744 0.827 0.858 0.888\n\n0.521 0.499 0.511 0.428 0.893 0.904\n\n0.489 0.490 0.711 0.540 0.767 0.836\n\n0.729 0.705 0.788 0.730 0.868 0.887\n\nTable 7: The performance (AUROC) of SCALE-UP variants with random noises on CIFAR-10 and Tiny-ImageNet datasets. The failed cases (i.e., AUROC < 0.55) are marked in red.\n\nDataset↓\n\nCIFAR-10\n\nTiny ImageNet\n\nAttack→ Setting↓ data-free data-limited data-free data-limited\n\nBadNets Label Consistent\n\nPhysicalBA TUAP WaNet\n\nISSBA Average\n\n0.939 0.939 0.951 0.951\n\n0.816 0.873 0.711 0.761\n\n0.976 0.981 0.899 0.899\n\n0.698 0.706 0.632 0.644\n\n0.497 0.432 0.531 0.534\n\n0.421 0.444 0.467 0.501\n\n0.724 0.729 0.706 0.706\n\ntwo attacks perform robustness against random noise, as illustrated in Section 5.3.3. We find that our approach is robust against noisy poisoned samples.\n\nM ADDITIONAL RESULTS UNDER VGG ARCHITECTURE\n\nIn our main manuscript, we evaluate our method under the ResNet architecture. In this section, we conduct additional experiments under VGG-19 (BN) on Tiny ImageNet, to verify that the phenomenon of scaled prediction consistency is valid across different model architectures.\n\nAs shown in Figure 14, the scaled prediction consistency still holds in all cases. Specifically, the average confidence of benign samples decreases significantly faster than that of poisoned ones with the increase in multiplication time. Besides, as shown in Table 6, our methods are still better than all baseline defenses. These results verify the effectiveness of our methods again.\n\nN THE ROC CURVES OF DEFENSES\n\nTo better compare our method with baseline defenses, we also visualize the ROC curves of defenses (as shown in Figure 15-16) under each attack on both CIFAR-10 and Tiny ImageNet in this section.\n\nO DETAILS FOR THE EFFECTIVENESS OF SCALING PROCESS\n\nSpecifically, we design the SCALE-UP variant by replacing the scaling process with adding the same varied magnitudes of random noise to the given inputs. As shown in Table 7, using random noises is far less effective compared to the standard SCALE-UP methods, especially in detecting advanced attacks (i.e., WaNet and ISSBA). We speculate that it is mostly because they adopted invisible fullimage size trigger patterns and therefore the trigger-related features are less robust. Although we currently fail to provide theoretical analysis for the aforementioned phenomena, at least they verify the effectiveness of our scaling process. We will further discuss it in our future work.\n\nP POTENTIAL LIMITATIONS AND FUTURE WORK\n\nOur work is the first black-box label-only input-level backdoor detection and early-stage defenses under the black-box setting. Accordingly, we have to admit that our work still has some limitations.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n(a) BadNets\n\n(b) TUAP\n\n(c) Label-Consistent\n\n(d) PhysicalBA\n\n(e) ISSBA\n\n(f) WaNet\n\nFigure 14: The average confidence (i.e., average probabilities on the originally predicted label) of benign and poisoned samples w.r.t. pixel-wise multiplications under benign and attacked models on the Tiny ImageNet dataset with VGG-19 (with batch normalization).\n\nFirstly, our defense requires that the attacked DNNs overfit their poisoned samples. This assumption or potential limitation is also revealed by our theoretical analysis in Section 3. In other words, if the attack success rate of a malicious model is relatively low, the detection performance of our SCALEUP defense may degrade sharply. Secondly, we found that our SCALE-UP detection may fail when defending against attacks in some cases of simple tasks (e.g., MNIST and GTSRB). We speculate that it is mostly because attacked DNNs also overfit to benign samples due to the lack of diversity and simplicity of the dataset, making them indistinguishable from some poisoned samples when analyzing the scaled prediction consistency. We will further explore the latent mechanisms of these limitations and alleviate them in our future work.\n\nBesides, regarding another future direction of our methods, we intend to generalize and adopt them to more settings and applications, such as continual learning (Wang et al., 2022b), non-transferable learning (Wang et al., 2022a), federate learning (Dong et al., 2022), audio signal processing (Zhai et al., 2021; Guo et al., 2022a;b), and visual object tracking (Li et al., 2022b). We will also evaluate our methods under other DNN structures (e.g., ViT (Tu et al., 2022) and GCN (Zhao et al., 2020b)).\n\n22\n\n1357911MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSample1357911MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSample1357911MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSample1357911MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSample1357911MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSample1357911MultiplicationTimes0.00.20.40.60.81.0AverageConfidencePoisonedSampleBenignSamplePublished as a conference paper at ICLR 2023\n\n(a) BadNets\n\n(b) TUAP\n\n(c) Label-Consistent\n\n(d) PhysicalBA\n\n(e) ISSBA\n\n(f) WaNet\n\nFigure 15: The ROC curves of defenses under each attack on CIFAR-10.\n\n23\n\n0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)Published as a conference paper at ICLR 2023\n\n(a) BadNets\n\n(b) TUAP\n\n(c) Label-Consistent\n\n(d) PhysicalBA\n\n(e) ISSBA\n\n(f) WaNet\n\nFigure 16: The ROC curves of defenses under each attack on Tiny ImageNet.\n\n24\n\n0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSTRIP (soft-label)ShrinkPadFrequencyDeepSweepSCALE-UP (data-free)SCALE-UP (data-limited)",
  "translations": [
    "# Summary Of The Paper\n\nA. Paper summary\n\n- This paper proposes an easy-to-understand blackbox trojan detection method. By leveraging the phenomenon called \"scaled prediction consistency\", the author suggests scaling up the input images and checking the confidence score. If the confidence drops, the input should be a benign sample; otherwise, it is poisoned. The evaluation result shows that the method is very effective. The paper also proposes a new adaptive attacking trojan to fully understand the limitation of the current detection method introduced in the paper.\n\n# Strength And Weaknesses\n\nB. Strength: \n- The like the idea and the writing which are very clear and easy to understand.\n- The method is simple and effective.\n- The analysis and discussions have a lot of insights.\n- The appendix covers a lot of missing details in the paper.\n\nC. Weaknesses:\n- I think the key limitation of the method is that SCALE-UP does not recover the trojan pattern. In other words, it cannot identify if the model is trojaned or not offline (e,g, https://www.ijcai.org/proceedings/2019/647). So even though your method achieves a very high AUROC score, it is not applicable in real-time applications such as a self-driving car; you cannot afford to lose ~2% of the real-time frames due to the false positive samples. \n\n- I don't understand why your method is only ~5% slower compared to the \"no defense\" method. You need to infer the sample images multiple times (up to 14 times as given in Figure 12).  Also, there is no computation reuse between different inferences (due to input changes). Can you explain this issue?\n\nMinor: AUROC is the only metric you applied throughout the paper (and appendix). Is it possible to provide the evaluation score of other metrics as well? Or maybe you can simply plot out some ROC curves.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nMentioned in the strength of the paper.\n\n# Summary Of The Review\n\nQuestions of the paper are given in the \"weakness\". I don't have too many questions because the appendix answered most of them.\n\nOverall, the paper is good but still has room for improvement.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nNo ethics concerns.",
    "# Summary Of The Paper\nThe paper proposes SCALE-UP, an efficient black-box method for detecting backdoor attacks in deep neural networks (DNNs) by analyzing scaled prediction consistency (SPC). It leverages the observation that poisoned samples maintain more consistent predictions under pixel amplification than benign samples. The authors conduct extensive experiments on CIFAR-10 and Tiny ImageNet datasets against various backdoor attacks, demonstrating SCALE-UP's superior performance, particularly in data-free and data-limited settings, with lower inference times compared to existing methods. The theoretical underpinnings of SPC are provided, alongside evaluations showcasing SCALE-UP's robustness against adaptive attacks and varying trigger sizes.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its novel approach to backdoor detection without requiring model access, making it applicable in black-box scenarios. The theoretical analysis of SPC adds depth to the contributions, and the extensive empirical evaluation across multiple datasets and attack types underscores the method's effectiveness. However, the paper has limitations, including potential performance degradation if DNNs do not overfit poisoned samples and challenges in simpler datasets where benign and poisoned samples are closely aligned. These weaknesses suggest that while SCALE-UP is a significant advancement, its applicability may be context-dependent.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex concepts accessible. The quality of the methodology and experiments is high, reflecting rigorous testing and analysis. The novelty of introducing the SPC phenomenon and developing a practical detection method is commendable. The reproducibility is supported by the availability of the code, which enhances the reliability of the findings and allows for further exploration by the research community.\n\n# Summary Of The Review\nOverall, SCALE-UP presents a significant advancement in backdoor detection methods for DNNs, particularly in black-box settings. Its novel theoretical insights and robust empirical results make it a valuable contribution to the field, although certain limitations regarding context-specific performance should be acknowledged.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"SCALE-UP: An Efficient Black-Box Input-Level Backdoor Detection via Analyzing Scaled Prediction Consistency\" addresses the vulnerability of deep neural networks (DNNs) to backdoor attacks, particularly in machine learning as a service (MLaaS) settings where users can only access model predictions. The proposed method, SCALE-UP, utilizes the concept of \"scaled prediction consistency\" (SPC) to detect backdoor attacks based solely on prediction labels, thereby enabling detection in black-box scenarios. Through extensive experiments on CIFAR-10 and Tiny ImageNet datasets, SCALE-UP demonstrates superior effectiveness and efficiency compared to existing defenses, particularly against advanced non-patch-based backdoor attacks.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach, which allows for effective black-box detection using only prediction labels, thereby broadening the applicability of backdoor detection methods. The empirical results are robust, showcasing high effectiveness across multiple attacks and datasets, and providing theoretical foundations for the detection mechanism. However, the method's reliance on certain assumptions, such as the tendency of attacked DNNs to overfit on poisoned samples, poses limitations, particularly in scenarios with low attack success rates. Additionally, the performance may decline on simpler datasets due to a lack of diversity, and the method's generalizability across various architectures and settings remains untested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and presents its methodology and findings in a coherent manner. The quality of the experiments is commendable, with a thorough evaluation against multiple baseline methods and robust metrics. The novelty of the approach is significant, offering a new perspective on backdoor detection in black-box settings. However, while the theoretical insights enhance understanding, the reproducibility could benefit from more detailed descriptions of experimental setups and parameters.\n\n# Summary Of The Review\nOverall, the SCALE-UP paper presents a promising advancement in the field of backdoor detection, highlighting strong empirical results and an innovative approach suitable for black-box scenarios. While it has notable strengths, the identified limitations suggest avenues for further research, particularly regarding the method's robustness and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SCALE-UP, a novel method for black-box detection of backdoor attacks in deep neural networks (DNNs) by leveraging the concept of Scaled Prediction Consistency (SPC). The authors demonstrate that poisoned samples exhibit consistent predictions across various pixel-wise amplifications, unlike benign samples, which show significant variance. The methodology involves two settings: a data-free setting that calculates SPC for suspicious inputs and a data-limited setting that incorporates a few benign samples for normalization. Experimental results on CIFAR-10 and Tiny ImageNet datasets show that SCALE-UP outperforms existing baseline defenses in terms of detection accuracy, with high AUROC values and competitive inference times, even under adaptive attack scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to backdoor detection, which is particularly relevant for black-box scenarios. The theoretical foundation provided, including Theorem 1, offers a solid basis for understanding the behavior of predictions under different conditions. The experimental results are comprehensive, demonstrating the method's effectiveness against a variety of backdoor attack mechanisms and showing robustness in inference times. However, one weakness is the limited exploration of the method's performance under simple datasets and low attack efficacy, which might restrict generalizability. Additionally, while the method shows promise, further empirical evaluations in diverse settings would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers with a basic understanding of backdoor attacks and DNNs. The quality of the writing is high, with clear definitions and explanations of technical terms. The novelty of the approach is significant, as it introduces a new way to analyze backdoor attacks through SPC. Reproducibility is supported by detailed experimental configurations provided in the appendix, although additional benchmarks across more varied datasets and architectures would enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of adversarial machine learning by introducing SCALE-UP, a robust method for detecting backdoor attacks in black-box settings. The combination of theoretical insights and strong empirical results positions this work as a valuable addition to ongoing research in this area.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces SCALE-UP, a novel black-box input-level backdoor detection method that utilizes scaled prediction consistency to identify malicious samples. The methodology allows for the detection of backdoor attacks without requiring access to the internal workings of the model, making it particularly suited for machine learning as a service (MLaaS) scenarios. The findings demonstrate that SCALE-UP is effective and efficient, performing competitively against existing defenses in extensive empirical evaluations across benchmark datasets.\n\n# Strength And Weaknesses\nSCALE-UP presents several strengths, including its innovative approach to backdoor detection, broad applicability in real-world MLaaS situations, and robust empirical validation. However, it has limitations such as reliance on the assumption of overfitting in attacked neural networks, which may lead to performance degradation if this assumption is not met. Additionally, the method's performance may vary with simpler datasets due to limited sample diversity, and its theoretical foundations may not generalize across all backdoor attacks. Furthermore, while it shows resilience against some adaptive attacks, there is a risk of being bypassed by more sophisticated adversaries. Lastly, its performance in data-limited scenarios is contingent on the availability and quality of benign samples.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The theoretical insights into scaled prediction consistency add depth to the understanding of the detection mechanism. The open-source availability of the code promotes reproducibility, although the paper could benefit from more extensive testing across diverse architectures to further validate its robustness. Overall, the clarity and quality of the writing are commendable.\n\n# Summary Of The Review\nSCALE-UP offers a promising new method for backdoor detection in machine learning models, particularly in black-box scenarios. While it demonstrates strong empirical performance and theoretical foundations, its reliance on certain assumptions and limited testing across architectures may hinder broader applicability. The paper is well-written and contributes valuable insights to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SCALE-UP, a novel black-box input-level backdoor detection method that leverages \"scaled prediction variance\" to differentiate between benign and poisoned inputs during a pixel amplification process. The authors argue that predictions for poisoned samples exhibit greater stability across scaled variations compared to benign samples. The methodology is designed to operate in both data-free and data-limited settings, employing a metric derived from prediction consistency across different input scales. Comprehensive experiments on CIFAR-10 and Tiny ImageNet showcase SCALE-UP's effectiveness in detecting various backdoor attacks, particularly non-patch-based ones.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to backdoor detection, moving away from traditional pixel-wise checks to a more holistic analysis of prediction variance, which presents a fresh perspective in this domain. The theoretical insights provided bolster the proposed method, offering a solid foundation for the observed phenomena. Furthermore, the adaptability of SCALE-UP to black-box scenarios without requiring knowledge of attack specifics is a notable advantage. However, a potential weakness is the assumption that poisoned samples will consistently exhibit stable predictions across all scaled inputs, which might not hold true against evolving attack strategies. This reliance could limit the robustness of the approach in dynamic real-world settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions and methodologies. The quality of the experimental design is commendable, with a range of datasets and attack scenarios evaluated. The novelty of the approach is significant, as it introduces a new metric for assessing prediction stability, which has implications for future research in backdoor detection. Reproducibility appears to be feasible, as the paper provides a detailed description of the methodologies and experimental setups, though additional code and data accessibility would enhance this aspect.\n\n# Summary Of The Review\nOverall, SCALE-UP is a significant contribution to the field of backdoor detection in machine learning. Its novel approach of using prediction variance under scaling offers a promising avenue for enhancing the robustness of detection mechanisms in black-box scenarios. While the methodology demonstrates strong performance, careful consideration of its assumptions is necessary for practical applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes SCALE-UP, a novel methodology for black-box input-level backdoor detection in deep neural networks (DNNs) by leveraging the concept of scaled prediction consistency (SPC). The authors highlight the phenomenon where predictions of adversarially perturbed samples maintain higher consistency than benign samples when subjected to pixel-wise amplification. The methodology includes a theoretical justification for SPC, practical applications in black-box scenarios, and extensive experimental validation across benchmark datasets such as CIFAR-10 and Tiny ImageNet. The findings indicate that SCALE-UP significantly outperforms existing detection methods in terms of efficiency and effectiveness.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to adversarial defense through the introduction of SPC, which adds a fresh perspective to the current landscape of adversarial training. The experimental results are robust and clearly demonstrate the superiority of SCALE-UP over baseline methods, enhancing the paper's credibility. Additionally, the theoretical analysis provides important insights into the method's underlying mechanics, a feature that is often overlooked in similar works. However, the reliance on the assumption that adversarially trained models show consistent prediction behavior raises concerns about the method's applicability across varying attack types. Furthermore, while the experiments are well-conducted, the generalizability of SCALE-UP across different architectures and datasets has not been fully explored, which may limit its broader impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear structure that effectively communicates the proposed methodology and findings. The quality of the experimental setup is high, contributing to the reproducibility of results. The novelty of the approach is significant, as it introduces a new concept (SPC) into adversarial training, which could inspire further research in this domain. However, the potential limitations regarding model behavior assumptions may hinder reproducibility in diverse real-world applications.\n\n# Summary Of The Review\nOverall, the paper presents a noteworthy advancement in adversarial training through the introduction of SCALE-UP and scaled prediction consistency. Despite some concerns related to generalizability and assumption reliance, the innovative approach and strong empirical results make this work a valuable contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a new approach to backdoor detection in deep neural networks (DNNs), named SCALE-UP, which claims to be a significant advancement in the field. The proposed method relies solely on predicted labels without requiring access to the model itself, allegedly allowing it to outperform existing defenses. The authors claim to have identified a property of DNNs, termed \"scaled prediction consistency,\" which suggests that poisoned samples yield consistent predictions under pixel amplification. They conduct experiments on benchmark datasets, reporting high AUROC scores, but the scope and rigor of these experiments are limited.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to backdoor detection, which could simplify the detection process by eliminating the need for model access. However, the claims regarding the universality and significance of the findings are overstated. The theoretical foundations of the method are not rigorously substantiated, and the experiments conducted do not adequately encompass a range of attack vectors. The paper also downplays its limitations, particularly the assumption that attacked DNNs overfit poisoned data, which restricts the applicability of SCALE-UP.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the oversimplification of its assumptions can lead to misunderstandings regarding the applicability of SCALE-UP. The novelty of the method is not as groundbreaking as suggested, given that the theoretical contributions are superficial and the experiments lack diversity. Reproducibility may be an issue due to insufficient detail in the experimental setup and the selective reporting of results.\n\n# Summary Of The Review\nWhile the paper introduces an interesting concept in backdoor detection, the claims about its contributions are largely exaggerated, and the findings do not represent a substantial advance in the field. The limitations and assumptions made by the authors significantly affect the perceived effectiveness and applicability of the proposed method.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents SCALE-UP, an innovative black-box input-level backdoor detection method that relies solely on predicted labels. The authors identify a phenomenon termed scaled prediction consistency (SPC), where poisoned samples exhibit more stable prediction behaviors compared to benign samples when pixel values are amplified. The paper provides both theoretical insights into this phenomenon and empirical evidence demonstrating the effectiveness of SCALE-UP across various datasets, including CIFAR-10 and Tiny ImageNet, with superior performance compared to existing baselines.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel approach that circumvents the limitations of traditional white-box defenses, making it particularly relevant for machine learning as a service (MLaaS) environments. The theoretical explanation for SPC is a significant contribution that enhances the understanding of backdoor detection mechanisms. The extensive experimental results presented bolster the claims of SCALE-UP's efficacy and efficiency. However, the paper could improve on the clarity of its experimental results and the potential implications for real-world applications. The altered results raise concerns about reproducibility and transparency, which could undermine trust in the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and communicates its main ideas effectively. The novelty of the proposed method and its theoretical foundations contribute significantly to the field. However, the alteration of results in various sections may lead to confusion regarding the actual performance and reliability of SCALE-UP, potentially impacting its reproducibility. The authors have made their code available, which is a positive step toward facilitating reproducibility, although clarity in reporting results is crucial.\n\n# Summary Of The Review\nOverall, SCALE-UP presents a promising method for black-box backdoor detection with strong empirical support and theoretical grounding. While it demonstrates significant potential and novelty, concerns about the clarity and accuracy of the experimental results warrant careful consideration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel method for detecting backdoor attacks in deep neural networks (DNNs) by leveraging the prediction consistency of poisoned samples when subjected to pixel value amplification. The methodology is based on the assumption that attacked DNNs will exhibit overfitting to their poisoned samples, while benign samples will not. The authors validate their approach through experiments on specific datasets and architectures, demonstrating promising results in detecting backdoored models. However, the paper also raises concerns regarding the generalizability and robustness of the method under various conditions and assumptions.\n\n# Strength And Weaknesses\nThe paper contributes to the field by addressing a critical issue in the security of DNNs with a focus on detection methods for backdoor attacks. Its innovative approach to using prediction consistency and scaling factors is a noteworthy addition to existing literature. However, several weaknesses undermine its robustness: the assumption that poisoned samples consistently exhibit higher prediction consistency may not hold across all attack types; the reliance on overfitting as a detection criterion may limit the method's applicability; and the choice of specific scaling factors lacks empirical justification. Furthermore, the method's sensitivity to class distribution and its potential ineffectiveness against advanced adaptive attacks are significant concerns.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and generally clear, although certain assumptions could be articulated more explicitly. The methodology is presented in a logical manner, but the lack of empirical validation across diverse datasets and architectures raises questions about reproducibility. While the proposed method introduces a novel perspective on backdoor detection, its reliance on specific assumptions may hinder its practical application in real-world scenarios.\n\n# Summary Of The Review\nOverall, the paper provides a valuable contribution to the detection of backdoor attacks in DNNs, yet it is constrained by several critical assumptions and limitations regarding its generalizability and robustness. While the methodology shows promise, it requires further validation and exploration to enhance its applicability across various contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces SCALE-UP, a novel black-box input-level backdoor detection method for deep neural networks (DNNs) that operates solely on predicted labels. The authors identify a phenomenon termed scaled prediction consistency, where poisoned samples exhibit more stable predictions under pixel-wise amplification compared to benign samples. SCALE-UP capitalizes on this observation to classify inputs, demonstrating effectiveness in both data-free and data-limited scenarios. Extensive experimental validation on benchmark datasets (CIFAR-10, Tiny ImageNet) shows that SCALE-UP outperforms existing defenses, particularly in terms of area under the ROC curve (AUROC) and inference time.\n\n# Strength And Weaknesses\nThe primary strength of the paper is its address of backdoor attack detection in MLaaS settings, where existing defenses are mostly white-box and impractical. The introduction of the scaled prediction consistency phenomenon provides a solid theoretical foundation for the proposed method. However, a potential weakness is the reliance on empirical results without exhaustive comparisons against all state-of-the-art defenses, which may limit the generalizability of the findings. Additionally, while the method performs well on benchmark datasets, real-world applicability should be further explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The theoretical insights into scaled prediction consistency are presented logically, enhancing the reader's understanding. The experiments are detailed, and the results are clearly reported, which supports reproducibility. However, the paper could benefit from more in-depth analysis of the limitations of SCALE-UP and potential edge cases that may affect its performance.\n\n# Summary Of The Review\nSCALE-UP presents a significant advancement in the detection of backdoor attacks for DNNs in MLaaS environments, leveraging a novel theoretical insight into scaled prediction consistency. While the methodology and results are compelling, further exploration of the method's limitations and comparisons with more extensive datasets could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel framework aimed at enhancing the robustness and security of machine learning models against adversarial attacks. The authors introduce a multi-faceted approach that combines theoretical insights from game theory with practical implementations in neural networks. The methodology includes a comprehensive set of experiments demonstrating the effectiveness of the proposed framework across various datasets and attack scenarios, showing significant improvements in model performance and resilience.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The research addresses a critical and timely issue in machine learning, as the security of models is increasingly under scrutiny in real-world applications.\n2. **Innovation:** The integration of game-theoretic principles into machine learning provides a fresh perspective on tackling adversarial vulnerabilities, making this work stand out.\n3. **Experimental Validation:** The extensive empirical analysis supports the theoretical claims made, showcasing the framework's effectiveness across multiple datasets and conditions.\n\n**Weaknesses:**\n1. **Clarity of Methodology:** Certain aspects of the proposed method could be articulated more clearly, particularly how the game-theoretic concepts are applied in practice.\n2. **Literature Review:** While the authors reference relevant works, a more thorough exploration of existing solutions and their limitations would contextualize their contributions better.\n3. **Limitations and Future Work:** The paper does not adequately address potential shortcomings of the proposed approach, nor does it outline clear directions for future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, although some technical sections could benefit from more detailed explanations and examples to aid comprehension. The quality of the work is high, with a solid theoretical foundation supporting the proposed methods. In terms of novelty, the combination of game theory and machine learning presents a unique contribution. However, the reproducibility of results could be improved by providing more extensive details regarding the experimental setup, including hyperparameters and specific data preprocessing techniques.\n\n# Summary Of The Review\nThis paper presents a compelling approach to enhancing model robustness against adversarial attacks by leveraging game-theoretic principles. While the contributions are significant and well-supported by experiments, improvements in clarity, literature context, and acknowledgment of limitations are necessary to strengthen the work before publication.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents SCALE-UP, an innovative black-box input-level detection method aimed at identifying backdoor attacks in deep neural networks (DNNs) that are particularly relevant in Machine Learning as a Service (MLaaS) contexts. The methodology hinges on analyzing prediction consistency through a novel concept termed \"scaled prediction consistency,\" which reveals that poisoned samples exhibit more consistent predictions compared to benign samples. The authors validate the effectiveness of SCALE-UP through extensive experiments, demonstrating its resilience against various backdoor attack strategies and emphasizing its applicability in real-world scenarios.\n\n# Strength And Weaknesses\nStrengths of the paper include its practical relevance in addressing backdoor attacks under black-box conditions, a significant concern for users of MLaaS. The introduction of scaled prediction consistency is a notable theoretical contribution that enhances understanding of prediction behavior related to backdoor attacks. The empirical validation across different attack scenarios bolsters the findings. However, a potential weakness lies in the need for further exploration of the method's performance under more diverse and adaptive attack strategies, which may not have been fully evaluated in the current study.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the problem of backdoor attacks along with the proposed solution. The quality of writing is high, making complex concepts accessible. The novelty of introducing scaled prediction consistency as a detection mechanism is significant. However, reproducibility aspects could be strengthened by providing additional details on experimental setups and datasets used, allowing for easier replication of results by other researchers in the field.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution by introducing a practical and innovative method for detecting backdoor attacks in black-box settings. The proposed SCALE-UP method and its theoretical foundation provide valuable insights into the behavior of DNNs under attack, although further investigation into diverse attack scenarios could enhance its robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SCALE-UP, a novel black-box input-level backdoor detection method designed to identify malicious inputs without requiring model access. The core of the methodology is the concept of scaled prediction consistency, which highlights that poisoned samples exhibit more consistent predictions under pixel-wise amplification compared to benign samples. Through extensive experiments on CIFAR-10 and Tiny ImageNet with ResNet architecture, the authors demonstrate that SCALE-UP outperforms existing backdoor detection methods, achieving high AUROC scores while maintaining efficiency in inference time.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to backdoor detection in a black-box setting, an area often neglected due to the challenges posed by the lack of model access. The proposed method not only achieves superior detection performance but also shows resilience against adaptive attacks and varying trigger sizes, indicating its robustness. However, the paper could benefit from a more thorough discussion on limitations and potential failure cases, particularly in real-world applications where the nature of attacks may be more diverse than those tested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, clearly articulating the problem, methodology, and results. The novelty of the approach is significant, particularly in its application to black-box scenarios. The authors provide sufficient detail regarding their experimental setup, and the availability of implementation details and datasets enhances reproducibility, which is crucial for validating claims in the field of machine learning security.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in detecting backdoor attacks in a practical, black-box setting. SCALE-UP's innovative methodology and strong empirical results position it as a valuable contribution to the field, though further exploration of its limitations in varied real-world contexts would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"SCALE-UP: An Efficient Black-Box Input-Level Backdoor Detection via Analyzing Scaled Prediction Consistency\" presents a novel method for detecting backdoor attacks in deep neural networks (DNNs) within machine learning as a service (MLaaS) environments. The proposed method, SCALE-UP, is designed to operate effectively in both data-free and data-limited scenarios. The authors provide a comprehensive evaluation on standard datasets (CIFAR-10 and Tiny ImageNet), demonstrating that SCALE-UP outperforms existing baseline methods in terms of detection accuracy and inference time. The paper emphasizes the significance of its contributions in addressing a pressing security issue in machine learning.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its clear articulation of the problem of backdoor attacks and the innovative approach employed in SCALE-UP. The methodology is well-detailed, providing a solid theoretical foundation alongside practical applications. The experimental results are robust and convincingly demonstrate the method's effectiveness compared to existing approaches. However, one weakness is that while the discussion addresses potential limitations, it could benefit from a more in-depth exploration of the implications of the findings and their applicability across different contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its ideas clearly, making it accessible to readers with varying levels of familiarity with the topic. The use of figures and tables enhances understanding of complex information. The novelty of the approach lies in its perspective on black-box backdoor detection, which is a significant contribution to the field of secure machine learning. The methodology appears reproducible, as the authors provide sufficient details regarding the experimental setup and evaluation metrics.\n\n# Summary Of The Review\nOverall, this paper offers a significant advancement in the realm of black-box backdoor detection in deep neural networks. SCALE-UP presents a novel approach that is well-supported by empirical results, addressing a critical security challenge in MLaaS environments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents SCALE-UP, a novel black-box input-level backdoor detection mechanism that operates solely on predicted labels without requiring access to the underlying model, making it suitable for Machine Learning as a Service (MLaaS) environments. The core innovation lies in the concept of scaled prediction consistency (SPC), which captures the disparity in prediction consistency between benign and poisoned inputs when subjected to pixel-wise amplification. The authors validate SCALE-UP through extensive experiments on CIFAR-10 and Tiny ImageNet datasets, demonstrating its superior efficacy in detecting various backdoor attacks, particularly non-patch-based ones, while also maintaining operational efficiency in terms of inference time.\n\n# Strength And Weaknesses\nStrengths of the paper include its practical applicability in black-box scenarios, where model access is restricted, and its introduction of a theoretically grounded method (SPC) for backdoor detection. The empirical results highlight its effectiveness against multiple attack types, showcasing a significant improvement over existing methods. However, weaknesses include a lack of extensive real-world applicability testing and limited discussion regarding potential adversarial strategies to circumvent the proposed method. Additionally, while the theoretical contributions are solid, the exploration of overfitting and the need for diverse training data could be deeper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings, making it accessible to the reader. The quality of the writing is high, and the theoretical formulations are adequately explained. The novelty of the approach is substantial, as it addresses a critical gap in backdoor detection for black-box settings. The reproducibility of the results appears feasible, given the detailed description of methodologies and experimental setups, although actual implementation details or code availability could further enhance reproducibility.\n\n# Summary Of The Review\nOverall, SCALE-UP presents a significant advancement in black-box backdoor detection, leveraging the concept of scaled prediction consistency to effectively differentiate between benign and poisoned inputs. While the methodology and results are compelling, the paper could benefit from broader applicability testing and a deeper exploration of potential countermeasures against adaptive attacks.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a defense mechanism called SCALE-UP, aimed at mitigating backdoor attacks on deep neural networks (DNNs). The authors propose that attacked DNNs exhibit a phenomenon they term \"scaled prediction consistency,\" which is leveraged to detect and neutralize poisoned samples. The methodology relies on analyzing predicted labels rather than predicted probability vectors, and the authors conduct a series of experiments to demonstrate the effectiveness of SCALE-UP on benchmark datasets.\n\n# Strength And Weaknesses\nWhile the proposed method introduces a novel perspective on detection through scaled prediction consistency, it is built on the assumption that attacked DNNs will overfit to their poisoned samples—an assumption that may not hold in many realistic scenarios. This reliance raises concerns over the method's applicability and effectiveness, especially against advanced backdoor attacks and complex datasets. The authors' experimental scope appears limited, and some results indicate poor performance (e.g., low AUROC values) against specific attacks. Additionally, the method’s efficiency is questionable due to its requirement for multiple queries, which could hinder its practicality in real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas clearly, but the theoretical underpinnings regarding scaled prediction consistency may lack robustness, leading to potential misinterpretations. The novelty of the approach is noteworthy, yet its practical significance is undermined by the limitations highlighted. The reproducibility of the results is questionable due to the limited experimental scope and the narrow focus on certain types of backdoor attacks.\n\n# Summary Of The Review\nOverall, while SCALE-UP offers a unique approach to backdoor attack detection, its reliance on specific assumptions limits its applicability and effectiveness in real-world scenarios. The method's performance against advanced attacks and in diverse datasets raises significant concerns regarding its reliability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents SCALE-UP, an innovative method for black-box input-level backdoor detection in machine learning systems. The methodology focuses on analyzing scaled prediction consistency, enabling the identification of malicious inputs without requiring complex modifications to existing models. The findings demonstrate SCALE-UP's robustness against advanced backdoor attacks, achieving high AUROC scores across various datasets and attack types, while remaining effective in both data-free and data-limited settings.\n\n# Strength And Weaknesses\nSCALE-UP's strengths lie in its simplicity and effectiveness, making it accessible for a wide range of users. The novel concept of scaled prediction consistency serves as a significant advancement in backdoor detection. The extensive empirical validation on benchmark datasets reinforces the method's reliability. However, a potential weakness may be the limited exploration of its performance across less common backdoor attack types, which could affect its generalizability. Additionally, while the method is efficient, further comparative analysis with existing state-of-the-art methods would strengthen the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it easy to follow. The quality of the experiments appears robust, with thorough evaluations on benchmark datasets. The novelty of the approach, particularly the introduction of scaled prediction consistency, is significant in the context of backdoor detection. The authors have also contributed to reproducibility by open-sourcing their code, which enhances the community's ability to build upon their work.\n\n# Summary Of The Review\nSCALE-UP is a remarkable contribution to the field of backdoor detection in machine learning, combining theoretical insights with practical applicability. Its simplicity, robustness, and efficiency position it as a valuable tool for enhancing the security of machine learning systems. The potential for future applications across various domains further underscores its significance.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to backdoor detection in deep neural networks (DNNs) termed SCALE-UP, which introduces the concept of \"scaled prediction consistency\" (SPC). The methodology centers around assessing the consistency of predicted labels across pixel-wise scaled images to distinguish between benign and poisoned samples. Theoretical foundations are built on Neural Tangent Kernel (NTK) analysis, with the findings suggesting that poisoned samples demonstrate distinct prediction behaviors compared to benign ones, particularly as the ratio of poisoned to benign samples approaches equality. The framework is adaptable for both data-free and data-limited scenarios, emphasizing theoretical contributions over empirical validation.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a theoretically grounded concept (SPC) that addresses a significant gap in existing backdoor detection methods, which often rely on white-box access. The proposed SCALE-UP framework is commendable for its ability to operate without requiring model parameter access, thus enhancing its applicability in real-world MLaaS settings. However, one notable weakness is the lack of empirical validation to support the theoretical claims, which may limit the practical applicability and robustness of the proposed method. Additionally, the theoretical limitations concerning model overfitting could restrict the generalizability of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex theoretical concepts accessible. The quality of the theoretical analysis is high, with a systematic exploration of the relationship between prediction consistency and backdoor detection. However, the novelty, while present in the theoretical contributions, lacks empirical backing that could enhance reproducibility and validate the claims made. The exploration of both data-free and data-limited settings is a valuable addition, although further experimental validation would strengthen confidence in the framework's efficacy.\n\n# Summary Of The Review\nOverall, the paper makes a significant theoretical contribution to the field of backdoor detection in DNNs by introducing the SCALE-UP framework and the concept of scaled prediction consistency. However, the lack of empirical validation and limitations regarding model performance under certain conditions are critical factors that need addressing to enhance the practical relevance of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"SCALE-UP: An Efficient Black-Box Input-Level Backdoor Detection via Analyzing Scaled Prediction Consistency\" presents a novel approach to detect backdoor attacks in deep neural networks (DNNs) using a method that relies solely on predicted labels. The methodology involves analyzing prediction consistency through a scaling process, where input images are amplified using a scaling set, and a metric called Scaled Prediction Consistency (SPC) is computed. The authors demonstrate the effectiveness of SCALE-UP through extensive experiments on datasets like CIFAR-10 and Tiny ImageNet, showing that it outperforms existing baselines, especially in data-limited scenarios, while maintaining competitive inference times.\n\n# Strength And Weaknesses\nStrengths of this paper include its innovative approach to backdoor detection without requiring access to benign samples in the data-free setting, making it applicable in more constrained environments. The use of prediction consistency as a detection criterion is well-founded and offers a clear methodology. However, the paper's reliance on the assumption that attacked DNNs overfit to poisoned samples might limit its generalizability across different scenarios. Additionally, the simplicity of the datasets used for experiments may not fully represent challenges in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed method and experimental results. The quality of the writing is high, with detailed descriptions of the methodology and evaluation processes. The novelty lies in the focus on scaling and prediction consistency for backdoor detection, a relatively unexplored area. Furthermore, the authors provide open-source code, enhancing the reproducibility of their findings and allowing other researchers to build upon their work.\n\n# Summary Of The Review\nOverall, SCALE-UP presents a significant contribution to the field of backdoor detection in neural networks, showcasing a novel methodology with strong empirical results. While the assumptions made may limit the approach's applicability in diverse contexts, the clarity and reproducibility of the research are commendable.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper introduces a novel method called SCALE-UP, designed for black-box detection of backdoor attacks in machine learning models. The authors claim that SCALE-UP achieves effective performance with enhanced efficiency compared to existing methods. The theoretical foundation of the method revolves around the concept of scaled prediction consistency, and experimental results are presented to demonstrate its effectiveness against certain attacks. However, the paper lacks comprehensive comparisons with state-of-the-art techniques and does not adequately address the limitations of the proposed approach.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address the critical issue of black-box detection for backdoor attacks, which is a relevant area of research. However, the contributions appear limited in depth and innovation when compared to previous works like STRIP and DeepSweep. While the authors assert that SCALE-UP is efficient, the lack of detailed performance metrics for traditional methods makes it challenging to validate this claim. Furthermore, the experimental findings indicate that SCALE-UP does not outperform established techniques against advanced attacks, raising concerns about its overall effectiveness. The novelty of the scaled prediction consistency phenomenon is also questionable, as similar behaviors have been documented in prior research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its methodology and results in a clear manner; however, the novelty claims regarding the approach and its theoretical foundations are overstated. The authors do not sufficiently engage with existing work, which undermines the perceived significance of their contributions. While the experimental setup is described, the lack of comparisons with contemporary state-of-the-art methods diminishes the reproducibility and contextual understanding of SCALE-UP’s performance.\n\n# Summary Of The Review\nIn summary, while the paper presents a method that addresses an important issue in machine learning security, it lacks depth in innovation and comparative analysis with existing techniques. The claims of efficiency and novelty are not convincingly supported, and the overall contribution to the field appears limited.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"SCALE-UP: AN EFFICIENT BLACK-BOX INPUT-LEVEL BACKDOOR DETECTION VIA ANALYZING SCALED PREDICTION CONSISTENCY\" presents a novel method for detecting backdoors in deep neural networks (DNNs) within a black-box context. The authors propose an approach that leverages the concept of scaled prediction consistency (SPC) to identify poisoned samples without needing access to the model's internals. Through extensive experiments, the authors demonstrate that their method achieves competitive performance against existing techniques, highlighting its effectiveness in both data-free and data-limited scenarios.\n\n# Strength And Weaknesses\nOne of the primary strengths of the paper is its focus on black-box backdoor detection, addressing a significant gap in the current literature. The methodology is well-structured and presents a clear innovation in the use of SPC. However, the paper suffers from several weaknesses, including inconsistent formatting, vague terminologies, and a lack of clarity in some sections. The theoretical foundations of the proposed approach could be better articulated, and the results could benefit from more comprehensive discussions regarding the implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel approach to backdoor detection, clarity is compromised by inconsistent terminology and formatting throughout the text. Definitions of key terms, such as \"machine learning as a service (MLaaS)\" and \"AUROC,\" need to be standardized and clearly introduced. The methodology is reproducible; however, the paper would benefit from more detailed descriptions of experiments and clearer figures. Overall, the quality of writing affects the readers' ability to fully grasp the novel contributions of the research.\n\n# Summary Of The Review\nThe paper introduces a promising method for backdoor detection in DNNs, leveraging scaled prediction consistency in a black-box setting. However, issues with clarity, consistency, and formatting detract from the overall presentation and impact of the work. Addressing these concerns would enhance the paper's contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to black-box input-level backdoor detection in deep neural networks (DNNs). The authors focus on a specific methodology that evaluates the integrity of inputs to identify potential backdoor triggers, showcasing its effectiveness against certain attack vectors using standard datasets such as CIFAR-10 and Tiny ImageNet. However, the work does not delve into the adaptability of the method to more complex or diverse attack scenarios, nor does it explore the implications of varying adversarial knowledge.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its targeted approach to backdoor detection and the demonstrable effectiveness of the proposed method in controlled experiments. However, the work has several weaknesses, including a limited exploration of multi-modal detection systems, insufficient discussion regarding the adaptability of the method to future attack strategies, and the lack of comprehensive analysis on computational overhead and ethical implications. The authors also fail to provide a detailed comparison with emerging trends in backdoor detection methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is satisfactory, with a well-structured presentation of methodology and results. However, the novelty may be somewhat limited due to a lack of engagement with broader theoretical frameworks and a narrow focus on performance metrics. The reproducibility of results could be improved by providing more extensive details on experimental setups and potential limitations of the proposed method.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of backdoor detection in deep learning but lacks depth in critical areas such as adaptability to different attack vectors, real-world applicability, and ethical considerations. To enhance its impact, the authors should broaden the scope of their research and include a more comprehensive analysis of the implications of their work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to detecting backdoor attacks in deep neural networks (DNNs) through a method called SCALE-UP, which leverages a concept termed \"scaled prediction consistency\" (SPC). The authors establish a theoretical foundation using neural tangent kernel (NTK) analysis, demonstrating that poisoned samples exhibit statistically consistent predictions compared to benign samples. Extensive experiments on benchmark datasets like CIFAR-10 and Tiny ImageNet validate SCALE-UP's effectiveness, with results showing significant improvements in detection capabilities and efficiency over baseline methods.\n\n# Strength And Weaknesses\nStrengths of the paper include a robust theoretical framework and an innovative methodology that effectively differentiates between benign and poisoned samples. The use of statistical significance to validate findings enhances the paper's credibility and reliability. However, the paper could potentially benefit from a more detailed exploration of the limitations of the SPC approach and its performance in highly adversarial settings. Additionally, while empirical results are promising, further validation on a broader range of datasets and attack types would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The theoretical insights are articulated effectively, and the experimental setup is comprehensive, allowing for reproducibility of results. The novelty of the SPC concept is noteworthy, though the paper could enhance its clarity by providing more explicit comparisons with existing methods and discussing potential implications of its findings in practical scenarios.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of machine learning security by introducing SCALE-UP, a statistically grounded method for detecting backdoor attacks. Its rigorous experimental validation and theoretical backing provide a strong foundation for future research in this area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SCALE-UP, a novel method designed for detecting backdoor attacks in deep neural networks (DNNs). The methodology is predicated on the assumption that DNNs overfit their poisoned samples, thereby enabling detection of malicious inputs. However, the findings indicate limitations in the method's effectiveness, particularly in datasets with simpler tasks, where the distinction between benign and poisoned samples becomes blurred. Furthermore, SCALE-UP does not recover backdoor trigger patterns, which poses an ongoing vulnerability, and shows reduced performance against adaptive attacks.\n\n# Strength And Weaknesses\nThe primary strength of SCALE-UP lies in its innovative approach to backdoor detection based on overfitting behavior, which offers a unique perspective on mitigating these security threats. However, significant weaknesses emerge, including its reliance on high attack success rates for effective detection and its limited generalization across varying dataset complexities. The method struggles in simpler datasets due to the indistinguishability of benign and poisoned samples, and it fails to recover backdoor triggers, leaving models at risk. Additionally, the lack of robustness against adaptive attacks further diminishes its practicality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the methodology and findings. However, the novelty is somewhat marred by the inherent limitations of the approach, particularly regarding generalization and robustness. The reproducibility of results may be questionable, given the specific dataset dependencies and the need for further validation across diverse architectures and attack scenarios. Future work is suggested to enhance detection capabilities and broaden applicability.\n\n# Summary Of The Review\nSCALE-UP introduces a compelling method for detecting backdoor attacks in DNNs based on the overfitting hypothesis. Despite its innovative approach, the method exhibits significant limitations in terms of generalization, robustness against adaptive attacks, and the inability to recover backdoor triggers, which necessitates further investigation and refinement.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a method for detecting backdoor attacks in deep neural networks (DNNs) by analyzing prediction consistency through pixel-wise amplification. While claiming to provide a \"simple yet effective\" solution, the authors primarily revisit well-established concepts in the field of backdoor detection, asserting that their approach addresses the shortcomings of existing defenses, particularly in black-box scenarios. The experiments conducted demonstrate their method's effectiveness on standard benchmark datasets; however, the results align closely with expected performance metrics rather than presenting significant breakthroughs.\n\n# Strength And Weaknesses\nStrengths of the paper include the comprehensive experimental evaluation on benchmark datasets and the clear presentation of results that show the proposed method outperforms certain existing defenses. However, the weaknesses significantly overshadow these strengths. The contributions of the paper are largely trivial, as they seem to restate known issues in backdoor detection without introducing substantial innovation. The theoretical insights provided do not add meaningful depth to the method, and the critique of existing defenses lacks novelty, as it reiterates commonly understood problems within the community.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers familiar with the topic. However, the novelty of the contributions is questionable, as the primary concept of analyzing prediction consistency through scaling feels overly simplistic and lacks originality. The reproducibility aspect is addressed, with links to code on GitHub, but this is a standard practice rather than a distinguishing feature of the work.\n\n# Summary Of The Review\nWhile the paper presents a method for backdoor detection that is well-executed and clearly articulated, it ultimately fails to offer significant technical or empirical advancements over existing work. The contributions appear trivial, and the work seems to be a rehashing of established concepts rather than a breakthrough in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the concept of **scaled prediction consistency (SPC)** as a novel approach for black-box input-level backdoor detection in deep neural networks (DNNs). The proposed method, SCALE-UP, leverages the hypothesis that poisoned samples exhibit different prediction stability compared to benign samples. Empirical results demonstrate that SCALE-UP effectively detects various backdoor attacks, particularly in data-free and data-limited settings. The authors also discuss theoretical foundations behind SPC, suggesting potential expansions into other domains such as adversarial robustness and continual learning.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to backdoor detection and the theoretical underpinnings of SPC, which provide a solid framework for understanding prediction behavior. The emphasis on data-free and data-limited scenarios is particularly relevant for practical applications. However, the paper has notable weaknesses, including a reliance on the assumption that poisoned samples exhibit distinct prediction stability, which may require further quantification. Additionally, the empirical evaluations could benefit from comparative analyses across diverse datasets and model architectures to establish generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with a well-structured presentation of the methodology and findings. However, certain assumptions made in the detection process could be better justified to enhance the overall quality. The novelty of introducing SPC as a detection mechanism is significant, but there is a need for more robust empirical validation to support the claims made. Reproducibility could be improved through detailed descriptions of experimental setups, including datasets used and model architectures evaluated.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach for backdoor detection using scaled prediction consistency, supported by theoretical foundations and empirical results. However, further exploration of the underlying assumptions and broader applicability of the method is necessary to strengthen its contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents SCALE-UP, a novel approach for detecting backdoor attacks in machine learning models. It evaluates the effectiveness of this method on benchmark datasets CIFAR-10 and Tiny ImageNet using the Area Under the Receiver Operating Curve (AUROC) as the performance metric. The findings indicate that SCALE-UP outperforms several baseline methods across various types of attacks, demonstrating resilience against both data-limited and data-free settings. Notably, it maintains a high AUROC even under adaptive attack conditions and exhibits lower inference times compared to existing defenses.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its empirical validation on widely accepted benchmark datasets and its effectiveness across multiple attacks, showcasing superior performance compared to baseline methods like STRIP and ShrinkPad. The method's efficiency is an additional advantage, as it achieves competitive AUROC scores while keeping inference times low. However, a notable weakness is the acknowledgment that the detection performance may degrade under certain conditions, such as low attack success rates, which could limit its applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and results, making it accessible to readers. The novelty lies in its approach to backdoor detection, which is both technically sound and empirically validated. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and parameters used in the evaluations.\n\n# Summary Of The Review\nOverall, SCALE-UP offers a promising advancement in backdoor detection with strong empirical results across standard datasets. Its efficiency and robustness against various attacks highlight its potential for practical application, despite some limitations regarding performance under specific conditions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to detecting black-box input-level backdoors in machine learning models, named SCALE-UP. The methodology involves analyzing the consistency of predictions across scaled inputs to identify anomalies indicative of backdoor attacks. The findings demonstrate that SCALE-UP effectively detects backdoor triggers with a high degree of accuracy while maintaining low false positive rates, thereby contributing to the security of machine learning systems.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to backdoor detection, which addresses a critical vulnerability in machine learning systems. The use of scaled prediction consistency as a heuristic for detection is both creative and effective, as evidenced by the empirical results. However, the paper has weaknesses, including a lengthy and complex abstract that may hinder initial comprehension, as well as a lack of clarity in the experimental design, which could affect reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel concept with significant implications for the field, the clarity suffers due to the use of technical jargon and convoluted sentence structures. The quality of writing could be improved with more straightforward language and better organization. Additionally, the reproducibility of the results is somewhat compromised by the unclear description of the experimental setup. Overall, the paper requires more attention to clarity and detail to ensure the methodologies can be easily followed by other researchers.\n\n# Summary Of The Review\nThe paper offers a valuable contribution to the detection of backdoor attacks in machine learning through its innovative methodology. However, it requires significant improvements in clarity, organization, and the presentation of its experimental design to enhance understandability and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.5113307026071188,
    -1.8028575188086784,
    -1.790074007852724,
    -1.7497783225857928,
    -1.8940055509475904,
    -1.8726565692576222,
    -1.7633026789810131,
    -1.9189841719680412,
    -1.8905735744009793,
    -1.707247008050966,
    -1.7566014445729548,
    -1.429149896895954,
    -1.6872208357785914,
    -1.7671927307414177,
    -1.7748971785547,
    -1.689811848874257,
    -1.99297233108857,
    -1.7740866566028741,
    -1.843065923484042,
    -1.7798069302851782,
    -1.8133989219275266,
    -1.6886538454356654,
    -1.7806712501866324,
    -1.7164391396641108,
    -1.707934070731737,
    -2.0211238865409173,
    -1.7220806912164823,
    -1.7484383292942058,
    -1.6475118707979324
  ],
  "logp_cond": [
    [
      0.0,
      -2.222645074526928,
      -2.257127233743057,
      -2.225940230800274,
      -2.240708947874625,
      -2.2655860214893755,
      -2.245714830049849,
      -2.2528630458263796,
      -2.263794087386736,
      -2.2843684694399293,
      -2.237352173284654,
      -2.3424916326033562,
      -2.2529450787027527,
      -2.2183133009713907,
      -2.238135959046407,
      -2.2680035722912697,
      -2.2363313258901707,
      -2.247176672877516,
      -2.2921712885431433,
      -2.249251927601068,
      -2.2472279019777215,
      -2.2560869456774544,
      -2.3085265351137436,
      -2.2555214600747786,
      -2.287450957781941,
      -2.279508278340176,
      -2.2666572300460057,
      -2.269319126716406,
      -2.275906034405657
    ],
    [
      -1.3936517561826163,
      0.0,
      -1.230706106757956,
      -1.2356586577880888,
      -1.2872826717348358,
      -1.255050028625291,
      -1.25798457424735,
      -1.27425618075818,
      -1.1707959233295975,
      -1.365765215477227,
      -1.2127143674340055,
      -1.4832179376126606,
      -1.3397998868465921,
      -1.2546750985349315,
      -1.3159424829926556,
      -1.3071365243200133,
      -1.3093889654884159,
      -1.3183522903241114,
      -1.268706124219956,
      -1.2196052077022448,
      -1.33805280185586,
      -1.323280604631246,
      -1.480458880415326,
      -1.348998726443508,
      -1.3794003861990698,
      -1.3667820089135514,
      -1.3182875287698472,
      -1.3263723215521688,
      -1.352135191904065
    ],
    [
      -1.4450177762104441,
      -1.3015188173407357,
      0.0,
      -1.354235640778257,
      -1.2478866326493738,
      -1.3192014456351555,
      -1.3410652261390703,
      -1.318288358099992,
      -1.31229764718697,
      -1.4338834436492662,
      -1.3382701469277043,
      -1.5593123421021384,
      -1.3993101308600984,
      -1.35436972715343,
      -1.3210127772837592,
      -1.2670012900955605,
      -1.3223064929208754,
      -1.3580825176781717,
      -1.397272084574491,
      -1.267467527262653,
      -1.408171187122819,
      -1.347395131397249,
      -1.473202828902111,
      -1.4202793811862384,
      -1.4034072885401248,
      -1.458413548120891,
      -1.3651313459350225,
      -1.3829918602750926,
      -1.4064290108396427
    ],
    [
      -1.3809350039384731,
      -1.230671524850334,
      -1.273412209868486,
      0.0,
      -1.3281357923059305,
      -1.2734887040242637,
      -1.2555664166606764,
      -1.3180513670360308,
      -1.3079943085502221,
      -1.3703897626157817,
      -1.224365025767862,
      -1.4874189881167041,
      -1.3216895355533411,
      -1.2145593823712433,
      -1.3071850086192476,
      -1.2988848708645813,
      -1.349209784340287,
      -1.3552474163372736,
      -1.3170645825204352,
      -1.327782160085224,
      -1.3353057717526289,
      -1.3474622264004463,
      -1.4423579375562878,
      -1.330305596329347,
      -1.3749492906489096,
      -1.387438625655975,
      -1.3030585737361196,
      -1.3467510129295903,
      -1.4002151617561416
    ],
    [
      -1.4975620409987658,
      -1.3496019998577389,
      -1.2835838840579228,
      -1.3734829378717968,
      0.0,
      -1.4348719771996146,
      -1.3625635578278315,
      -1.4495939192752936,
      -1.3765933078712045,
      -1.4958026645818867,
      -1.3357916624616877,
      -1.606640733283254,
      -1.4305593218237393,
      -1.4420491378216729,
      -1.395563415262012,
      -1.4238169456612615,
      -1.4374864615499356,
      -1.389328708299196,
      -1.4842467546717766,
      -1.3745835058604894,
      -1.4717478272688367,
      -1.4267424115431961,
      -1.5414223807229175,
      -1.487604779442631,
      -1.4687202492783313,
      -1.5033433863280325,
      -1.4160835659584965,
      -1.4850619643776488,
      -1.4603398134834178
    ],
    [
      -1.5304014044172771,
      -1.375095977979191,
      -1.3637570382256632,
      -1.3876808616709608,
      -1.4855647946562374,
      0.0,
      -1.369207782760442,
      -1.4771139923874927,
      -1.3707034668835785,
      -1.527957556266232,
      -1.3962189135480967,
      -1.5743922388419063,
      -1.4735346920210137,
      -1.4162770922620527,
      -1.4181606756448857,
      -1.3568030101727664,
      -1.485741947657747,
      -1.4614341025614088,
      -1.4572340743814374,
      -1.419562576106249,
      -1.5133376936999874,
      -1.4944332149225457,
      -1.537262694907398,
      -1.4840949104457037,
      -1.54930226517059,
      -1.5270514082749669,
      -1.454119034731394,
      -1.5079265085769655,
      -1.4682369014083094
    ],
    [
      -1.396025450003552,
      -1.311131778903046,
      -1.3037837674152601,
      -1.3147986713645825,
      -1.3413305973163974,
      -1.338846428386248,
      0.0,
      -1.361967681958554,
      -1.3010041479832788,
      -1.3952830884941974,
      -1.295988536548399,
      -1.4900918918933288,
      -1.3769008022606304,
      -1.3048985006872749,
      -1.375997929114612,
      -1.3241329196928566,
      -1.3498738265625319,
      -1.3495859701404813,
      -1.3970407572313586,
      -1.3609943745199358,
      -1.3937829273793798,
      -1.395336282819247,
      -1.4247031240822992,
      -1.3491047471276265,
      -1.4311003458928018,
      -1.3869772686956896,
      -1.3245648266857426,
      -1.4197393778704894,
      -1.4208591355478073
    ],
    [
      -1.5404128877938585,
      -1.4223813608770668,
      -1.4381251505142365,
      -1.4178160529039685,
      -1.4638795122124828,
      -1.5033509524939486,
      -1.4358736508990984,
      0.0,
      -1.4132527821725267,
      -1.4857610524056721,
      -1.3643263567365385,
      -1.673310330483924,
      -1.4960211541998305,
      -1.4342012042600962,
      -1.4778225650205437,
      -1.4457451195099902,
      -1.395832073025518,
      -1.4698139910528398,
      -1.4774172442671656,
      -1.4832956198150655,
      -1.443913907199107,
      -1.4970368684801132,
      -1.5846201436763727,
      -1.4621382650164894,
      -1.519434992680358,
      -1.48838478856565,
      -1.4664674562733542,
      -1.4915361158145106,
      -1.5631426976926248
    ],
    [
      -1.4876530244146806,
      -1.299429170147252,
      -1.298512005541533,
      -1.3893096078717462,
      -1.3674153276697747,
      -1.3304979784574118,
      -1.3171064247835587,
      -1.3515900435764183,
      0.0,
      -1.4905894738606302,
      -1.235528926516692,
      -1.5686065675906875,
      -1.3878264603359347,
      -1.32840348373708,
      -1.3565434397893117,
      -1.2832608293876933,
      -1.4461633071601898,
      -1.4090660927352112,
      -1.3853340142696056,
      -1.3075367410785605,
      -1.4521380497966825,
      -1.3842681534260257,
      -1.5378511357100988,
      -1.4434141257847697,
      -1.5192381100709609,
      -1.4715391344763253,
      -1.4259361226141312,
      -1.4909773863015763,
      -1.4290924943127525
    ],
    [
      -1.3868472877042877,
      -1.3221878354835908,
      -1.3311653836465964,
      -1.3159481922771026,
      -1.3127590813929968,
      -1.3651419850213127,
      -1.3309933996100556,
      -1.3101983568583808,
      -1.3673498059784923,
      0.0,
      -1.3466329703349118,
      -1.4529566270654755,
      -1.3523623895268775,
      -1.3269102945688056,
      -1.3967998503641317,
      -1.3506349855788529,
      -1.3122907477091803,
      -1.3937250585706926,
      -1.3840157865074967,
      -1.3192745674702882,
      -1.3775412670810891,
      -1.3948875963288048,
      -1.4212086178018217,
      -1.3912183891784067,
      -1.3522532433016303,
      -1.3571108936925869,
      -1.3485314750852626,
      -1.4052144006632696,
      -1.4175468039670511
    ],
    [
      -1.3608657840135474,
      -1.223385659475076,
      -1.1920491671113524,
      -1.1798922596481978,
      -1.200177977814935,
      -1.2255855392579693,
      -1.2195867230672024,
      -1.2078167405910747,
      -1.1163977599132737,
      -1.3388680512396918,
      0.0,
      -1.457900638031698,
      -1.2027968849190283,
      -1.170551931177653,
      -1.1732612075327506,
      -1.1286872124853586,
      -1.2937006560960487,
      -1.2373582960278358,
      -1.2620448819427346,
      -1.2317454673890884,
      -1.277292810494854,
      -1.1859513673939395,
      -1.38942016152074,
      -1.2582132533762562,
      -1.364464522472816,
      -1.3110216669806058,
      -1.2374999262585855,
      -1.3068750211032687,
      -1.322820489578304
    ],
    [
      -1.1802540772141439,
      -1.144586517393164,
      -1.1561703264961416,
      -1.15285497958147,
      -1.1667955108052006,
      -1.1418352559364993,
      -1.12939604602531,
      -1.1599595902133517,
      -1.12796533950037,
      -1.1412225556902886,
      -1.1653318053005433,
      0.0,
      -1.1402233316310644,
      -1.1582004962855763,
      -1.1564135121328036,
      -1.1676883649829504,
      -1.14349480019111,
      -1.1554273697081965,
      -1.1412308893504046,
      -1.1452831762289675,
      -1.1542608214832073,
      -1.1275403506436221,
      -1.12422485523187,
      -1.1391253756040625,
      -1.166635140675353,
      -1.1270543858856643,
      -1.149042348993259,
      -1.1537075655957383,
      -1.1222114611855898
    ],
    [
      -1.296822059572553,
      -1.2277841804442347,
      -1.1854926370142094,
      -1.1914715829480511,
      -1.1943675216001812,
      -1.2635828267137834,
      -1.2073407627681774,
      -1.2361443297234391,
      -1.2011372509174527,
      -1.2879239366010624,
      -1.1843146595488028,
      -1.3689722639315651,
      0.0,
      -1.1406103493613904,
      -1.1940954481209052,
      -1.2124057006229672,
      -1.2184863499067902,
      -1.2398116250379863,
      -1.2131138397325485,
      -1.2839042873466262,
      -1.2010411637151805,
      -1.192513390888578,
      -1.3357660035822565,
      -1.2120152360622847,
      -1.2424462056892667,
      -1.3099030080292584,
      -1.204518972716954,
      -1.3017453370369516,
      -1.2567028544220302
    ],
    [
      -1.3233864349451876,
      -1.1545016633317748,
      -1.2110043645281927,
      -1.1475339613594684,
      -1.2057435322150774,
      -1.2484080652248615,
      -1.2016950037104839,
      -1.2284642910132686,
      -1.1594669860722726,
      -1.295282304431913,
      -1.1172002352690324,
      -1.4506971418358452,
      -1.2194892707130989,
      0.0,
      -1.200203431193731,
      -1.1619703956886533,
      -1.295356324525559,
      -1.230481507555835,
      -1.307254707597419,
      -1.2050512394862551,
      -1.2814860507602837,
      -1.2646584345699556,
      -1.3492125882243227,
      -1.2728602190815297,
      -1.3262301869964799,
      -1.279058482302069,
      -1.277016454591694,
      -1.2473319633371873,
      -1.29892182214417
    ],
    [
      -1.3812173426450647,
      -1.2432280515174634,
      -1.1491736987101955,
      -1.2282990867763084,
      -1.2555363853929455,
      -1.249861349212136,
      -1.2346389920013523,
      -1.3322290776613614,
      -1.2261913663776907,
      -1.3791427024995562,
      -1.1553648089568143,
      -1.453447695020805,
      -1.249423789952216,
      -1.2498306074940082,
      0.0,
      -1.2169230837935865,
      -1.317341315853982,
      -1.2521586925268897,
      -1.2841955970224175,
      -1.1565761220546331,
      -1.3178417556578086,
      -1.1739223231351426,
      -1.3803215604010393,
      -1.2908435526581175,
      -1.3658389579421824,
      -1.3681420604559495,
      -1.266024787576048,
      -1.287236440955286,
      -1.278412505594071
    ],
    [
      -1.3409575940556895,
      -1.232261087166888,
      -1.1461947270259045,
      -1.1866740733791699,
      -1.2584039613413387,
      -1.1877953614408616,
      -1.1983588011078934,
      -1.2480336349079753,
      -1.201275107981497,
      -1.3299985681653168,
      -1.1589346477771982,
      -1.4251972228458365,
      -1.2443966058285643,
      -1.1508966236522828,
      -1.2023065924561518,
      0.0,
      -1.285069540437285,
      -1.2635750685346667,
      -1.2919575828826622,
      -1.2708196784305148,
      -1.2788529227364533,
      -1.2745242650547148,
      -1.3174719053088118,
      -1.2487945565621315,
      -1.3277808221811342,
      -1.3169108581843718,
      -1.275704838307093,
      -1.2896457746814454,
      -1.3125266308052508
    ],
    [
      -1.61738819606258,
      -1.6003969680584413,
      -1.5317541745663348,
      -1.5798316118422127,
      -1.552418037651604,
      -1.6566106112387942,
      -1.5702292499865942,
      -1.5031315241339118,
      -1.612360087748606,
      -1.5579975807526587,
      -1.5682773032298385,
      -1.7171446030759885,
      -1.5672075090707744,
      -1.604364663682351,
      -1.6206777349431836,
      -1.5959864668068788,
      0.0,
      -1.5589039625633168,
      -1.6490595097235838,
      -1.6152292580914367,
      -1.5446032450197187,
      -1.6488897514593839,
      -1.6834475926870949,
      -1.5821120109158298,
      -1.5623296850984887,
      -1.682361595072118,
      -1.6113713300704922,
      -1.6432800471407878,
      -1.7042015784972409
    ],
    [
      -1.377190044722021,
      -1.2600275913365881,
      -1.282194593971517,
      -1.2661397906720235,
      -1.2385164660258496,
      -1.2883781784853827,
      -1.2437724682689844,
      -1.299304105814774,
      -1.2664940578817319,
      -1.402338311969746,
      -1.2103711938772297,
      -1.4667786027476248,
      -1.270933292750422,
      -1.2821529415399082,
      -1.2630878574977575,
      -1.3063660024449093,
      -1.3185899142519453,
      0.0,
      -1.3398481871449381,
      -1.2862306298469923,
      -1.2998479606517885,
      -1.2788146807170642,
      -1.4219352796198441,
      -1.3278773399635533,
      -1.3801820671511074,
      -1.3731954174820993,
      -1.263350604033137,
      -1.3341523442628587,
      -1.3254530059970275
    ],
    [
      -1.5320896453153456,
      -1.3928708231721019,
      -1.4178411978201495,
      -1.4626460408893638,
      -1.4587441657959468,
      -1.430567640279652,
      -1.46642954404937,
      -1.4590598183130838,
      -1.3669847256372964,
      -1.505906339511872,
      -1.4247676109440546,
      -1.5938341094432242,
      -1.4396570149481618,
      -1.486539057942122,
      -1.4392217613869593,
      -1.4361287825087357,
      -1.4716102511829803,
      -1.4820689831763896,
      0.0,
      -1.4456924042356953,
      -1.519019154199293,
      -1.465109166925502,
      -1.5542843576302654,
      -1.454450282719636,
      -1.5355017564223763,
      -1.5398196823646382,
      -1.4820945391491283,
      -1.5256234648696798,
      -1.4864906045229531
    ],
    [
      -1.4634197248577083,
      -1.2215564076747403,
      -1.2489903248869108,
      -1.2846140629601663,
      -1.2876216416627928,
      -1.277988192695426,
      -1.349596574073333,
      -1.3468270687473056,
      -1.263502582087391,
      -1.3903246892937633,
      -1.2654728540197053,
      -1.5036518533341443,
      -1.3590904063641343,
      -1.3125002889923312,
      -1.206679463297018,
      -1.325376777019564,
      -1.3576286144473093,
      -1.3275949172287955,
      -1.3682977795561888,
      0.0,
      -1.4059932423644783,
      -1.3048090450828684,
      -1.4388370278416402,
      -1.4311363446822793,
      -1.4115198125082946,
      -1.4281513889840607,
      -1.34889222849848,
      -1.3708984555675743,
      -1.3270340133697929
    ],
    [
      -1.376479824321352,
      -1.4005557557854478,
      -1.3722392035394846,
      -1.3686676902689765,
      -1.3773793520246869,
      -1.4360563891125593,
      -1.3844650162018033,
      -1.3825287984667287,
      -1.3963320056436967,
      -1.4665526283995176,
      -1.3514052238068195,
      -1.5341336971271764,
      -1.393270273309745,
      -1.3711871224890073,
      -1.4172715620233935,
      -1.3878089967135925,
      -1.356994493499954,
      -1.3912739425899292,
      -1.469164690472898,
      -1.4670803177334568,
      0.0,
      -1.4308732408114175,
      -1.5200826951325908,
      -1.4129008432208399,
      -1.4245676441281123,
      -1.4575133408741148,
      -1.4094587923702118,
      -1.439227144638576,
      -1.4772536119132724
    ],
    [
      -1.2639304993324612,
      -1.1898447356587762,
      -1.1373812416853675,
      -1.2195483436616514,
      -1.173344536047688,
      -1.1987575106960724,
      -1.2081366450071194,
      -1.218161681534965,
      -1.1507685023684149,
      -1.3005368501812962,
      -1.1104429402206826,
      -1.3932421935802766,
      -1.2102383343271172,
      -1.1893179228700452,
      -1.087027027925517,
      -1.1902048754110455,
      -1.254419273803111,
      -1.16971166747042,
      -1.1944500920326717,
      -1.1433696348981115,
      -1.2401500511387298,
      0.0,
      -1.3062350380884507,
      -1.2530608820923126,
      -1.3025510397899402,
      -1.3022745678777676,
      -1.1529522435216764,
      -1.2942544676393932,
      -1.2332569828641222
    ],
    [
      -1.4466835300978664,
      -1.4135891692830753,
      -1.3665028304514875,
      -1.3750525782113148,
      -1.358908507071849,
      -1.382733484030521,
      -1.352873655301296,
      -1.4268563750932741,
      -1.3984676863353416,
      -1.3956869531337408,
      -1.3676265479849996,
      -1.4804889904610938,
      -1.3893880003187213,
      -1.3264438652434667,
      -1.328862739140165,
      -1.341865474656186,
      -1.4330346330206691,
      -1.3828306393883503,
      -1.43372456293525,
      -1.361446055757704,
      -1.3943160769243825,
      -1.3705316265289138,
      0.0,
      -1.4097459078284065,
      -1.4275601763819916,
      -1.4185096506403596,
      -1.3618123513656273,
      -1.3920990659535037,
      -1.4018790249759605
    ],
    [
      -1.301971712976661,
      -1.2055613002127599,
      -1.1779848673427609,
      -1.1903873031592258,
      -1.2233806516826176,
      -1.227440178840444,
      -1.1790704162553103,
      -1.2408603749451854,
      -1.2218308252852579,
      -1.2757726602771158,
      -1.1721087205377432,
      -1.4030356453142518,
      -1.1823998109772778,
      -1.1781234447771243,
      -1.2239869534176648,
      -1.2025697557035528,
      -1.2004648213431908,
      -1.229183733261952,
      -1.252203937246717,
      -1.2382331733807908,
      -1.2180295789292366,
      -1.269983112293647,
      -1.3586722472570816,
      0.0,
      -1.2607299775600207,
      -1.3208468850162491,
      -1.229599034330767,
      -1.246314215037659,
      -1.2813986509342803
    ],
    [
      -1.4184814957004064,
      -1.3850329717540262,
      -1.3240582631700188,
      -1.3811524169300875,
      -1.2993798035186428,
      -1.4375172124706443,
      -1.3672106214374975,
      -1.3490271633702657,
      -1.4015777984914952,
      -1.3320834251907414,
      -1.380111259300374,
      -1.4726282988332677,
      -1.3771587794761218,
      -1.3848301303345127,
      -1.3921717583959436,
      -1.3855556481987943,
      -1.2987897898920744,
      -1.3734436871702327,
      -1.4082792495937988,
      -1.4114935466792464,
      -1.332778999028205,
      -1.3811217865247551,
      -1.4113978135841583,
      -1.389362908585267,
      0.0,
      -1.3970283125309833,
      -1.3538179066578504,
      -1.3469012327981782,
      -1.3976569558845815
    ],
    [
      -1.6147956531443195,
      -1.5105654756826492,
      -1.540556728169363,
      -1.5300087979133679,
      -1.5521477499450411,
      -1.5467269211210273,
      -1.5204073208441444,
      -1.5811564854075204,
      -1.562151068457638,
      -1.592658205761878,
      -1.5330631943852144,
      -1.688747061971204,
      -1.5981645378960687,
      -1.5193258464593913,
      -1.599669197075668,
      -1.5392854014200497,
      -1.577236880106599,
      -1.5533178094438411,
      -1.6170655518602142,
      -1.550872852573771,
      -1.5795447740472854,
      -1.5714295660473967,
      -1.6891542078425037,
      -1.60746604908681,
      -1.6613981095133448,
      0.0,
      -1.6024201194460281,
      -1.6687786955000594,
      -1.6137298771606776
    ],
    [
      -1.31238978155387,
      -1.2226883137692062,
      -1.2194864822457951,
      -1.2168080406243034,
      -1.2224140750953185,
      -1.2214526474239231,
      -1.232848902311304,
      -1.303080571113272,
      -1.2385117381407142,
      -1.335100654195349,
      -1.1928627649867638,
      -1.4366228476208176,
      -1.247079564266648,
      -1.2695986928934613,
      -1.2464858646772228,
      -1.2463041948651576,
      -1.3001106388898296,
      -1.2321191998604513,
      -1.25586571447952,
      -1.2787050203367827,
      -1.2549546612486833,
      -1.2283458514158128,
      -1.3209253721920848,
      -1.2939526218560906,
      -1.3167486644119655,
      -1.365331979953302,
      0.0,
      -1.3169666240490394,
      -1.2733830064024674
    ],
    [
      -1.3689054665389375,
      -1.2296990837914614,
      -1.2042723354965643,
      -1.2053786713169654,
      -1.2877676545397962,
      -1.301583640938215,
      -1.2884555125605812,
      -1.3025131940537293,
      -1.2900136806607216,
      -1.3312081815863586,
      -1.2356138985331866,
      -1.4319169121540087,
      -1.3232317310581587,
      -1.269452081276413,
      -1.2324526257466737,
      -1.257986987196464,
      -1.3005398748796773,
      -1.2823451633555838,
      -1.3264170490467302,
      -1.3117780610811096,
      -1.311248136359627,
      -1.3125804789815065,
      -1.3342110934861262,
      -1.3170370133235485,
      -1.272465807692168,
      -1.3323224229582606,
      -1.2902031543340011,
      0.0,
      -1.3489319590915163
    ],
    [
      -1.2167899911242788,
      -1.1952624931147962,
      -1.1617444430722554,
      -1.2057329488880342,
      -1.1236109647809676,
      -1.1554242769962564,
      -1.1481695907053862,
      -1.2322283934906806,
      -1.1159840226160325,
      -1.2956790859181155,
      -1.172246897558258,
      -1.3183156845010806,
      -1.167173239863994,
      -1.1458220330119073,
      -1.1454681586013065,
      -1.1839136113058557,
      -1.2209302053426239,
      -1.1174931176469975,
      -1.1715693496599922,
      -1.124114569784639,
      -1.1899767342857248,
      -1.13525683775739,
      -1.263161677202,
      -1.196600783287582,
      -1.2464748985144818,
      -1.2653108300778444,
      -1.1663269336300166,
      -1.2447237721931705,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2886856280801906,
      0.25420346886406175,
      0.28539047180684474,
      0.2706217547324936,
      0.24574468111774328,
      0.2656158725572699,
      0.25846765678073913,
      0.24753661522038284,
      0.22696223316718944,
      0.2739785293224646,
      0.16883907000376253,
      0.25838562390436604,
      0.2930174016357281,
      0.27319474356071183,
      0.24332713031584907,
      0.2749993767169481,
      0.2641540297296028,
      0.21915941406397543,
      0.2620787750060507,
      0.2641028006293973,
      0.2552437569296644,
      0.20280416749337515,
      0.25580924253234016,
      0.2238797448251777,
      0.23182242426694266,
      0.24467347256111305,
      0.24201157589071265,
      0.2354246682014618
    ],
    [
      0.4092057626260621,
      0.0,
      0.5721514120507223,
      0.5671988610205896,
      0.5155748470738426,
      0.5478074901833874,
      0.5448729445613283,
      0.5286013380504984,
      0.6320615954790809,
      0.4370923033314513,
      0.5901431513746729,
      0.31963958119601776,
      0.46305763196208627,
      0.5481824202737469,
      0.4869150358160228,
      0.49572099448866513,
      0.4934685533202625,
      0.484505228484567,
      0.5341513945887224,
      0.5832523111064336,
      0.46480471695281844,
      0.4795769141774324,
      0.32239863839335237,
      0.45385879236517046,
      0.4234571326096086,
      0.436075509895127,
      0.48456999003883117,
      0.4764851972565096,
      0.45072232690461345
    ],
    [
      0.34505623164227983,
      0.48855519051198826,
      0.0,
      0.435838367074467,
      0.5421873752033501,
      0.47087256221756846,
      0.44900878171365366,
      0.47178564975273196,
      0.4777763606657539,
      0.35619056420345774,
      0.4518038609250197,
      0.23076166575058554,
      0.3907638769926256,
      0.4357042806992939,
      0.46906123056896476,
      0.5230727177571635,
      0.46776751493184854,
      0.4319914901745523,
      0.39280192327823293,
      0.522606480590071,
      0.3819028207299049,
      0.442678876455475,
      0.3168711789506129,
      0.36979462666648555,
      0.3866667193125992,
      0.3316604597318329,
      0.42494266191770147,
      0.4070821475776314,
      0.3836449970130813
    ],
    [
      0.3688433186473197,
      0.5191067977354589,
      0.4763661127173069,
      0.0,
      0.4216425302798623,
      0.47628961856152907,
      0.49421190592511643,
      0.43172695554976204,
      0.44178401403557066,
      0.3793885599700111,
      0.5254132968179308,
      0.26235933446908866,
      0.4280887870324517,
      0.5352189402145495,
      0.4425933139665452,
      0.45089345172121154,
      0.40056853824550576,
      0.39453090624851916,
      0.43271374006535757,
      0.4219961625005688,
      0.4144725508331639,
      0.4023160961853465,
      0.307420385029505,
      0.4194727262564457,
      0.3748290319368832,
      0.3623396969298178,
      0.44671974884967325,
      0.4030273096562025,
      0.3495631608296512
    ],
    [
      0.3964435099488246,
      0.5444035510898515,
      0.6104216668896676,
      0.5205226130757936,
      0.0,
      0.4591335737479758,
      0.5314419931197589,
      0.4444116316722968,
      0.5174122430763859,
      0.39820288636570367,
      0.5582138884859027,
      0.2873648176643364,
      0.46344622912385103,
      0.4519564131259175,
      0.4984421356855784,
      0.4701886052863289,
      0.45651908939765473,
      0.5046768426483943,
      0.4097587962758138,
      0.519422045087101,
      0.4222577236787537,
      0.46726313940439423,
      0.3525831702246729,
      0.40640077150495935,
      0.42528530166925904,
      0.39066216461955783,
      0.4779219849890939,
      0.40894358656994156,
      0.4336657374641726
    ],
    [
      0.3422551648403451,
      0.49756059127843133,
      0.5088995310319591,
      0.4849757075866614,
      0.3870917746013849,
      0.0,
      0.5034487864971802,
      0.3955425768701295,
      0.5019531023740438,
      0.3446990129913903,
      0.47643765570952556,
      0.298264330415716,
      0.3991218772366085,
      0.4563794769955696,
      0.45449589361273657,
      0.5158535590848559,
      0.38691462159987533,
      0.41122246669621343,
      0.4154224948761849,
      0.45309399315137333,
      0.35931887555763486,
      0.3782233543350766,
      0.3353938743502243,
      0.3885616588119185,
      0.32335430408703214,
      0.34560516098265537,
      0.4185375345262283,
      0.3647300606806567,
      0.4044196678493128
    ],
    [
      0.36727722897746107,
      0.452170900077967,
      0.459518911565753,
      0.4485040076164306,
      0.42197208166461575,
      0.4244562505947651,
      0.0,
      0.4013349970224591,
      0.4622985309977343,
      0.36801959048681576,
      0.46731414243261415,
      0.2732107870876843,
      0.38640187672038273,
      0.45840417829373825,
      0.38730474986640107,
      0.4391697592881565,
      0.41342885241848126,
      0.4137167088405318,
      0.36626192174965455,
      0.40230830446107735,
      0.3695197516016333,
      0.3679663961617661,
      0.33859955489871396,
      0.4141979318533866,
      0.33220233308821134,
      0.3763254102853235,
      0.4387378522952705,
      0.3435633011105237,
      0.3424435434332058
    ],
    [
      0.3785712841741826,
      0.49660281109097437,
      0.48085902145380466,
      0.5011681190640727,
      0.45510465975555836,
      0.4156332194740926,
      0.4831105210689428,
      0.0,
      0.5057313897955145,
      0.43322311956236903,
      0.5546578152315027,
      0.24567384148411708,
      0.42296301776821066,
      0.48478296770794493,
      0.4411616069474975,
      0.473239052458051,
      0.5231520989425231,
      0.44917018091520133,
      0.4415669277008756,
      0.43568855215297564,
      0.4750702647689342,
      0.42194730348792797,
      0.33436402829166845,
      0.4568459069515518,
      0.3995491792876831,
      0.4305993834023911,
      0.452516715694687,
      0.42744805615353054,
      0.35584147427541635
    ],
    [
      0.40292054998629867,
      0.5911444042537273,
      0.5920615688594464,
      0.5012639665292331,
      0.5231582467312046,
      0.5600755959435675,
      0.5734671496174206,
      0.538983530824561,
      0.0,
      0.39998410054034905,
      0.6550446478842873,
      0.3219670068102918,
      0.5027471140650446,
      0.5621700906638993,
      0.5340301346116676,
      0.607312745013286,
      0.44441026724078947,
      0.48150748166576807,
      0.5052395601313737,
      0.5830368333224187,
      0.4384355246042968,
      0.5063054209749536,
      0.3527224386908805,
      0.4471594486162096,
      0.3713354643300184,
      0.41903443992465395,
      0.4646374517868481,
      0.399596188099403,
      0.46148108008822675
    ],
    [
      0.3203997203466784,
      0.3850591725673753,
      0.37608162440436965,
      0.3912988157738635,
      0.3944879266579693,
      0.3421050230296534,
      0.3762536084409105,
      0.39704865119258526,
      0.33989720207247376,
      0.0,
      0.36061403771605427,
      0.2542903809854906,
      0.3548846185240886,
      0.38033671348216047,
      0.3104471576868344,
      0.3566120224721132,
      0.39495626034178577,
      0.31352194948027345,
      0.3232312215434694,
      0.3879724405806779,
      0.3297057409698769,
      0.3123594117221613,
      0.28603839024914435,
      0.3160286188725594,
      0.35499376474933575,
      0.3501361143583792,
      0.35871553296570347,
      0.30203260738769644,
      0.28970020408391495
    ],
    [
      0.39573566055940734,
      0.5332157850978787,
      0.5645522774616023,
      0.576709184924757,
      0.5564234667580197,
      0.5310159053149854,
      0.5370147215057524,
      0.5487847039818801,
      0.640203684659681,
      0.417733393333263,
      0.0,
      0.29870080654125686,
      0.5538045596539265,
      0.5860495133953016,
      0.5833402370402041,
      0.6279142320875961,
      0.4629007884769061,
      0.519243148545119,
      0.4945565626302202,
      0.5248559771838663,
      0.47930863407810076,
      0.5706500771790153,
      0.36718128305221476,
      0.4983881911966985,
      0.39213692210013873,
      0.4455797775923489,
      0.5191015183143692,
      0.44972642346968605,
      0.43378095499465075
    ],
    [
      0.24889581968181007,
      0.28456337950279,
      0.27297957039981235,
      0.27629491731448397,
      0.26235438609075334,
      0.2873146409594547,
      0.2997538508706439,
      0.26919030668260224,
      0.30118455739558403,
      0.2879273412056653,
      0.2638180915954107,
      0.0,
      0.2889265652648896,
      0.27094940061037764,
      0.2727363847631503,
      0.2614615319130036,
      0.2856550967048439,
      0.27372252718775747,
      0.28791900754554933,
      0.2838667206669865,
      0.27488907541274665,
      0.30160954625233183,
      0.30492504166408385,
      0.2900245212918915,
      0.26251475622060094,
      0.30209551101028964,
      0.2801075479026949,
      0.27544233130021567,
      0.30693843571036417
    ],
    [
      0.39039877620603836,
      0.45943665533435674,
      0.501728198764382,
      0.4957492528305403,
      0.49285331417841016,
      0.423638009064808,
      0.479880073010414,
      0.45107650605515226,
      0.4860835848611387,
      0.399296899177529,
      0.5029061762297886,
      0.31824857184702626,
      0.0,
      0.546610486417201,
      0.4931253876576862,
      0.47481513515562423,
      0.4687344858718012,
      0.44740921074060513,
      0.4741069960460429,
      0.4033165484319652,
      0.48617967206341084,
      0.4947074448900133,
      0.3514548321963349,
      0.4752055997163067,
      0.4447746300893247,
      0.377317827749333,
      0.4827018630616373,
      0.3854754987416398,
      0.43051798135656116
    ],
    [
      0.4438062957962301,
      0.6126910674096429,
      0.556188366213225,
      0.6196587693819493,
      0.5614491985263403,
      0.5187846655165562,
      0.5654977270309338,
      0.5387284397281491,
      0.6077257446691451,
      0.47191042630950464,
      0.6499924954723852,
      0.3164955889055725,
      0.5477034600283188,
      0.0,
      0.5669892995476866,
      0.6052223350527643,
      0.47183640621585865,
      0.5367112231855826,
      0.4599380231439987,
      0.5621414912551626,
      0.48570667998113404,
      0.5025342961714621,
      0.41798014251709503,
      0.494332511659888,
      0.4409625437449378,
      0.4881342484393487,
      0.4901762761497237,
      0.5198607674042304,
      0.4682709085972476
    ],
    [
      0.3936798359096354,
      0.5316691270372367,
      0.6257234798445046,
      0.5465980917783917,
      0.5193607931617545,
      0.5250358293425641,
      0.5402581865533478,
      0.44266810089333863,
      0.5487058121770094,
      0.3957544760551439,
      0.6195323695978858,
      0.321449483533895,
      0.525473388602484,
      0.5250665710606919,
      0.0,
      0.5579740947611136,
      0.45755586270071813,
      0.5227384860278104,
      0.4907015815322826,
      0.618321056500067,
      0.4570554228968915,
      0.6009748554195575,
      0.3945756181536608,
      0.48405362589658263,
      0.40905822061251773,
      0.40675511809875053,
      0.5088723909786521,
      0.487660737599414,
      0.49648467296062915
    ],
    [
      0.3488542548185676,
      0.4575507617073691,
      0.5436171218483525,
      0.5031377754950872,
      0.43140788753291837,
      0.5020164874333954,
      0.49145304776636367,
      0.4417782139662818,
      0.4885367408927601,
      0.35981328070894025,
      0.5308772010970588,
      0.26461462602842056,
      0.4454152430456928,
      0.5389152252219742,
      0.48750525641810527,
      0.0,
      0.4047423084369721,
      0.42623678033959034,
      0.3978542659915949,
      0.41899217044374226,
      0.4109589261378037,
      0.4152875838195422,
      0.3723399435654453,
      0.4410172923121256,
      0.3620310266931228,
      0.37290099068988525,
      0.41410701056716404,
      0.4001660741928117,
      0.3772852180690063
    ],
    [
      0.3755841350259901,
      0.39257536303012874,
      0.4612181565222353,
      0.4131407192463574,
      0.44055429343696617,
      0.3363617198497759,
      0.4227430811019759,
      0.48984080695465826,
      0.38061224333996413,
      0.4349747503359114,
      0.4246950278587316,
      0.2758277280125816,
      0.42576482201779564,
      0.3886076674062191,
      0.3722945961453865,
      0.39698586428169125,
      0.0,
      0.4340683685252533,
      0.34391282136498624,
      0.3777430729971334,
      0.4483690860688514,
      0.3440825796291862,
      0.3095247384014752,
      0.4108603201727403,
      0.4306426459900814,
      0.31061073601645206,
      0.3816010010180779,
      0.3496922839477823,
      0.2887707525913292
    ],
    [
      0.39689661188085323,
      0.514059065266286,
      0.4918920626313572,
      0.5079468659308506,
      0.5355701905770245,
      0.4857084781174914,
      0.5303141883338898,
      0.47478255078810006,
      0.5075925987211423,
      0.3717483446331282,
      0.5637154627256444,
      0.3073080538552493,
      0.5031533638524521,
      0.4919337150629659,
      0.5109987991051166,
      0.4677206541579648,
      0.45549674235092885,
      0.0,
      0.434238469457936,
      0.4878560267558818,
      0.4742386959510856,
      0.49527197588580996,
      0.35215137698303,
      0.44620931663932084,
      0.39390458945176676,
      0.40089123912077484,
      0.5107360525697371,
      0.43993431234001545,
      0.4486336506058466
    ],
    [
      0.31097627816869644,
      0.4501951003119402,
      0.4252247256638926,
      0.3804198825946783,
      0.3843217576880953,
      0.41249828320438997,
      0.3766363794346721,
      0.38400610517095823,
      0.47608119784674563,
      0.33715958397217016,
      0.4182983125399875,
      0.24923181404081785,
      0.4034089085358803,
      0.35652686554192003,
      0.40384416209708274,
      0.4069371409753064,
      0.3714556723010618,
      0.3609969403076525,
      0.0,
      0.39737351924834674,
      0.32404676928474907,
      0.37795675655854,
      0.2887815658537767,
      0.38861564076440613,
      0.3075641670616658,
      0.3032462411194039,
      0.3609713843349138,
      0.31744245861436227,
      0.35657531896108896
    ],
    [
      0.3163872054274699,
      0.558250522610438,
      0.5308166053982675,
      0.4951928673250119,
      0.4921852886223854,
      0.5018187375897523,
      0.4302103562118451,
      0.4329798615378726,
      0.5163043481977871,
      0.38948224099141493,
      0.5143340762654729,
      0.2761550769510339,
      0.42071652392104397,
      0.46730664129284705,
      0.5731274669881603,
      0.4544301532656143,
      0.4221783158378689,
      0.4522120130563827,
      0.4115091507289894,
      0.0,
      0.3738136879206999,
      0.4749978852023098,
      0.340969902443538,
      0.3486705856028989,
      0.3682871177768836,
      0.35165554130111754,
      0.43091470178669833,
      0.40890847471760394,
      0.45277291691538535
    ],
    [
      0.43691909760617453,
      0.41284316614207883,
      0.441159718388042,
      0.4447312316585501,
      0.43601956990283974,
      0.3773425328149673,
      0.4289339057257233,
      0.4308701234607979,
      0.41706691628382986,
      0.346846293528009,
      0.4619936981207071,
      0.2792652248003502,
      0.42012864861778154,
      0.4422117994385193,
      0.39612735990413306,
      0.42558992521393413,
      0.45640442842757256,
      0.4221249793375974,
      0.3442342314546285,
      0.3463186041940698,
      0.0,
      0.3825256811161091,
      0.29331622679493585,
      0.40049807870668674,
      0.3888312777994143,
      0.35588558105341184,
      0.40394012955731484,
      0.37417177728895057,
      0.3361453100142542
    ],
    [
      0.4247233461032043,
      0.4988091097768892,
      0.551272603750298,
      0.469105501774014,
      0.5153093093879775,
      0.48989633473959304,
      0.48051720042854607,
      0.4704921639007005,
      0.5378853430672506,
      0.38811699525436927,
      0.5782109052149829,
      0.29541165185538887,
      0.4784155111085482,
      0.4993359225656202,
      0.6016268175101485,
      0.49844897002461996,
      0.4342345716325544,
      0.5189421779652454,
      0.4942037534029937,
      0.5452842105375539,
      0.44850379429693565,
      0.0,
      0.3824188073472148,
      0.43559296334335285,
      0.3861028056457252,
      0.3863792775578978,
      0.5357016019139891,
      0.3943993777962722,
      0.4553968625715432
    ],
    [
      0.333987720088766,
      0.3670820809035571,
      0.41416841973514495,
      0.40561867197531765,
      0.42176274311478346,
      0.3979377661561114,
      0.4277975948853363,
      0.3538148750933583,
      0.38220356385129084,
      0.3849842970528916,
      0.4130447022016328,
      0.3001822597255386,
      0.3912832498679111,
      0.4542273849431657,
      0.45180851104646735,
      0.4388057755304464,
      0.3476366171659633,
      0.39784061079828215,
      0.34694668725138245,
      0.41922519442892847,
      0.3863551732622499,
      0.4101396236577186,
      0.0,
      0.37092534235822594,
      0.3531110738046408,
      0.3621615995462728,
      0.41885889882100513,
      0.38857218423312867,
      0.3787922252106719
    ],
    [
      0.41446742668744974,
      0.5108778394513509,
      0.5384542723213499,
      0.526051836504885,
      0.4930584879814932,
      0.4889989608236667,
      0.5373687234088005,
      0.47557876471892535,
      0.4946083143788529,
      0.440666479386995,
      0.5443304191263676,
      0.3134034943498589,
      0.534039328686833,
      0.5383156948869865,
      0.49245218624644593,
      0.5138693839605579,
      0.51597431832092,
      0.4872554064021588,
      0.4642352024173937,
      0.47820596628332,
      0.4984095607348742,
      0.44645602737046386,
      0.3577668924070292,
      0.0,
      0.45570916210409007,
      0.39559225464786163,
      0.48684010533334376,
      0.47012492462645183,
      0.4350404887298305
    ],
    [
      0.2894525750313306,
      0.3229010989777108,
      0.38387580756171813,
      0.3267816538016495,
      0.4085542672130942,
      0.27041685826109263,
      0.3407234492942395,
      0.35890690736147124,
      0.30635627224024176,
      0.37585064554099556,
      0.32782281143136305,
      0.23530577189846924,
      0.3307752912556152,
      0.32310394039722423,
      0.3157623123357933,
      0.3223784225329427,
      0.4091442808396626,
      0.3344903835615043,
      0.2996548211379382,
      0.29644052405249055,
      0.3751550717035319,
      0.32681228420698183,
      0.2965362571475787,
      0.31857116214646997,
      0.0,
      0.3109057582007537,
      0.3541161640738866,
      0.3610328379335588,
      0.3102771148471555
    ],
    [
      0.40632823339659785,
      0.5105584108582681,
      0.4805671583715543,
      0.4911150886275495,
      0.4689761365958762,
      0.47439696541989007,
      0.5007165656967729,
      0.43996740113339694,
      0.4589728180832793,
      0.42846568077903924,
      0.4880606921557029,
      0.3323768245697134,
      0.42295934864484863,
      0.5017980400815261,
      0.42145468946524933,
      0.4818384851208677,
      0.44388700643431833,
      0.4678060770970762,
      0.4040583346807032,
      0.4702510339671464,
      0.4415791124936319,
      0.44969432049352065,
      0.33196967869841365,
      0.41365783745410734,
      0.3597257770275726,
      0.0,
      0.4187037670948892,
      0.35234519104085793,
      0.40739400938023973
    ],
    [
      0.40969090966261223,
      0.49939237744727616,
      0.5025942089706872,
      0.505272650592179,
      0.4996666161211638,
      0.5006280437925592,
      0.48923178890517827,
      0.4190001201032103,
      0.4835689530757681,
      0.38698003702113337,
      0.5292179262297185,
      0.28545784359566473,
      0.47500112694983443,
      0.45248199832302105,
      0.47559482653925955,
      0.4757764963513247,
      0.42197005232665274,
      0.48996149135603106,
      0.4662149767369623,
      0.44337567087969965,
      0.46712602996779906,
      0.49373483980066957,
      0.4011553190243975,
      0.4281280693603917,
      0.40533202680451685,
      0.3567487112631804,
      0.0,
      0.4051140671674429,
      0.44869768481401495
    ],
    [
      0.3795328627552683,
      0.5187392455027444,
      0.5441659937976415,
      0.5430596579772404,
      0.4606706747544096,
      0.44685468835599074,
      0.4599828167336246,
      0.4459251352404765,
      0.4584246486334842,
      0.4172301477078473,
      0.5128244307610192,
      0.3165214171401971,
      0.4252065982360471,
      0.4789862480177929,
      0.5159857035475321,
      0.4904513420977419,
      0.4478984544145286,
      0.46609316593862204,
      0.42202128024747565,
      0.4366602682130962,
      0.43719019293457895,
      0.4358578503126993,
      0.41422723580807963,
      0.43140131597065734,
      0.47597252160203785,
      0.4161159063359452,
      0.4582351749602047,
      0.0,
      0.39950637020268953
    ],
    [
      0.4307218796736536,
      0.4522493776831362,
      0.48576742772567694,
      0.4417789219098982,
      0.5239009060169648,
      0.492087593801676,
      0.4993422800925462,
      0.4152834773072518,
      0.5315278481818999,
      0.35183278487981684,
      0.4752649732396743,
      0.32919618629685177,
      0.48033863093393836,
      0.501689837786025,
      0.5020437121966259,
      0.46359825949207667,
      0.4265816654553085,
      0.5300187531509348,
      0.4759425211379402,
      0.5233973010132933,
      0.45753513651220756,
      0.5122550330405424,
      0.38435019359593237,
      0.45091108751035036,
      0.4010369722834506,
      0.382201040720088,
      0.48118493716791577,
      0.40278809860476184,
      0.0
    ]
  ],
  "row_avgs": [
    0.2510762260684486,
    0.49091257412791517,
    0.4213875218931761,
    0.4208534639717984,
    0.45812021828185506,
    0.41256346816539013,
    0.3977367805318127,
    0.44200866139507866,
    0.4907583018503617,
    0.3449717475949142,
    0.5038788711117446,
    0.28135931654002816,
    0.4513482006337525,
    0.5186224785019312,
    0.4983484746316619,
    0.43033616840143196,
    0.38791640647463277,
    0.4643179804911304,
    0.36895689043561425,
    0.4359495809245283,
    0.39651591133397796,
    0.4712406389454796,
    0.38997410166822105,
    0.47671971151066267,
    0.32971802660665944,
    0.4382008816022361,
    0.4506112451136553,
    0.4519907624357026,
    0.45731524419322983
  ],
  "col_avgs": [
    0.37421470341675744,
    0.46967673502632296,
    0.4873760533288155,
    0.4680187311139767,
    0.4584371601557698,
    0.4430310898367501,
    0.46642169644614956,
    0.43398491256408367,
    0.46964629443834977,
    0.3832334453033038,
    0.4904485999201253,
    0.2849115442791063,
    0.43365625794834134,
    0.46322392269792595,
    0.4548022753884712,
    0.4621288657093169,
    0.42715972310964695,
    0.4370310367288153,
    0.4125912886136671,
    0.4501814733932678,
    0.41592898185813204,
    0.431051047530024,
    0.33967217436522806,
    0.40911404237621884,
    0.38047325759672496,
    0.37140850445927853,
    0.4338872668296917,
    0.3913492089786426,
    0.3906495620241274
  ],
  "combined_avgs": [
    0.312645464742603,
    0.48029465457711906,
    0.4543817876109958,
    0.44443609754288754,
    0.45827868921881243,
    0.4277972790010701,
    0.4320792384889811,
    0.43799678697958117,
    0.4802022981443557,
    0.36410259644910903,
    0.49716373551593496,
    0.28313543040956723,
    0.44250222929104693,
    0.49092320059992856,
    0.4765753750100665,
    0.4462325170553744,
    0.40753806479213983,
    0.4506745086099728,
    0.39077408952464066,
    0.443065527158898,
    0.406222446596055,
    0.45114584323775175,
    0.36482313801672456,
    0.44291687694344073,
    0.3550956421016922,
    0.4048046930307573,
    0.4422492559716735,
    0.4216699857071726,
    0.42398240310867863
  ],
  "gppm": [
    599.2241201955455,
    577.5999425061597,
    569.2680953385277,
    577.6630061347587,
    582.4908930866172,
    586.3958748061276,
    578.6534943435281,
    592.8772678124232,
    577.9788780580468,
    617.249476203392,
    570.5477161651428,
    661.2429483824125,
    597.0108361254325,
    583.5250772465002,
    585.5082279837899,
    582.3480077446997,
    595.7546272018932,
    594.8731638377354,
    600.0813369919794,
    586.4838561895378,
    602.3981329555594,
    596.7592164219739,
    636.8883903321077,
    607.9548131656815,
    619.1002717194741,
    619.1655986005965,
    596.0221174052066,
    615.5201428498737,
    617.3584612532095
  ],
  "gppm_normalized": [
    1.3299116215630613,
    1.3497823740658703,
    1.3387832922982805,
    1.346394286343934,
    1.3581538010006737,
    1.3662998132809914,
    1.3533773433159302,
    1.3726024190132704,
    1.345155740506512,
    1.4327853271928463,
    1.322901504515961,
    1.534005666439111,
    1.3894301693056725,
    1.3549430414379875,
    1.3603615620371643,
    1.351289231006864,
    1.3834448656169855,
    1.3833368824729912,
    1.3922087401667258,
    1.368901495070795,
    1.3936304819697831,
    1.3810519005373774,
    1.4717822554396736,
    1.4082003895118114,
    1.4370263944057708,
    1.4312563871068973,
    1.383723129116687,
    1.4288262410161043,
    1.4276781342513838
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409,
    557,
    395,
    451,
    431,
    438,
    437,
    417,
    383,
    398,
    374,
    428,
    387,
    416,
    435,
    406,
    409,
    457,
    395,
    425,
    379,
    415,
    441,
    340,
    408,
    417,
    426,
    414,
    438,
    335,
    468,
    442,
    473,
    474,
    383,
    401,
    465,
    446,
    413,
    388,
    399,
    481,
    480,
    453,
    411,
    415,
    389,
    446,
    406,
    459,
    411,
    398,
    367,
    435,
    415,
    350,
    346,
    429,
    364,
    533,
    424,
    479,
    491,
    712,
    431,
    444,
    521,
    415,
    476,
    408,
    564,
    485,
    428,
    449,
    470,
    420,
    441,
    430,
    446,
    447,
    421,
    459,
    433,
    414,
    473,
    353,
    452,
    397,
    650,
    429,
    489,
    455,
    450,
    445,
    432,
    435,
    464,
    421,
    395,
    489,
    392,
    476,
    454,
    450,
    376,
    391,
    389,
    441,
    381,
    413,
    421,
    418,
    441,
    428,
    410,
    396,
    417,
    1943,
    416,
    433,
    407,
    650,
    411,
    434,
    376,
    416,
    422,
    408,
    502,
    428,
    414,
    429,
    420,
    352,
    404,
    352,
    439,
    417,
    355,
    372,
    425,
    401,
    534,
    404,
    461,
    349,
    539,
    434,
    448,
    462,
    435,
    468,
    471,
    457,
    428,
    459,
    392,
    455,
    380,
    417,
    420,
    425,
    418,
    379,
    391,
    487,
    375,
    426,
    378,
    397,
    363,
    392,
    405,
    407,
    337,
    473,
    419,
    495,
    432,
    470,
    474,
    380,
    373,
    422,
    479,
    404,
    457,
    444,
    433,
    437,
    416,
    423,
    392,
    464,
    456,
    432,
    385,
    405,
    480,
    428,
    438,
    447,
    512,
    369,
    548,
    444,
    472,
    485,
    427,
    474,
    473,
    415,
    410,
    430,
    430,
    487,
    409,
    392,
    429,
    460,
    396,
    399,
    475,
    446,
    421,
    438,
    387,
    384,
    422,
    399,
    414,
    378,
    371
  ],
  "response_lengths": [
    2552,
    2437,
    2589,
    2689,
    2432,
    2688,
    2656,
    2244,
    2330,
    2457,
    2346,
    2787,
    2368,
    2202,
    2328,
    2618,
    2188,
    2243,
    2665,
    2364,
    2415,
    2286,
    2139,
    2199,
    2336,
    2268,
    2357,
    2104,
    2064
  ]
}